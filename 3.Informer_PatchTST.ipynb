{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.helper import extract_metrics_from_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connecting to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on GPU, because running it on CPU will cost a lot of time.\n",
    "\n",
    "\n",
    "I do not recommend to run it in Google Colab, because it interrupts training process.\n",
    "\n",
    "If you are not going to use remote servers with multiple GPUs, skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# For CUDA making it available this works:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of available GPUs:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the GPU you want to use (e.g., 0, 1, 2, etc.)\n",
    "# Choose that one that is not used by other processes\n",
    "gpu_index = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/main_experiments\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "seq_len = \"96\"\n",
    "model = \"Informer\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 2\n",
    "d_layers = 1\n",
    "\n",
    "# List to store the results\n",
    "informer_results = []\n",
    "\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "  for i, country in enumerate(countries):\n",
    "\n",
    "    log_file.write(f\"\\n=== Starting experiments for country: {country} ===\\n\")\n",
    "\n",
    "    for pred_len in pred_lens:\n",
    "      model_id = f\"{country}_{seq_len}_{pred_len}\"\n",
    "      dataset = f\"{country}_data.csv\"\n",
    "      \n",
    "      # Arguments for the command\n",
    "      command = f\"\"\"\n",
    "      python {script_path} \\\n",
    "        --random_seed 2021 \\\n",
    "        --is_training 1 \\\n",
    "        --root_path \"{data_path}\" \\\n",
    "        --data_path \"{dataset}\" \\\n",
    "        --model_id {model_id} \\\n",
    "        --model \"{model}\" \\\n",
    "        --data \"custom\" \\\n",
    "        --features M \\\n",
    "        --seq_len {seq_len} \\\n",
    "        --label_len 5 \\\n",
    "        --pred_len {pred_len} \\\n",
    "        --e_layers 2 \\\n",
    "        --d_layers 1 \\\n",
    "        --factor 5 \\\n",
    "        --enc_in \"{num_cols[i]}\" \\\n",
    "        --dec_in \"{num_cols[i]}\" \\\n",
    "        --c_out \"{num_cols[i]}\" \\\n",
    "        --des 'Exp' \\\n",
    "        --train_epochs 10 \\\n",
    "        --patience 3 \\\n",
    "        --n_heads {n_heads} \\\n",
    "        --overlapping_windows \\\n",
    "        --itr 1 --batch_size 32 --learning_rate {lr}\n",
    "      \"\"\"\n",
    "      # Log the country and prediction length\n",
    "      log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "      # Run the command and capture the output\n",
    "      process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "      # Capture the output in real-time\n",
    "      output = []\n",
    "      for line in process.stdout:\n",
    "          output.append(line)\n",
    "          print(line, end='')  # Print in the .ipynb cell\n",
    "          log_file.write(line)  # Write to the log file\n",
    "\n",
    "      # Wait for the process to complete\n",
    "      process.wait()\n",
    "\n",
    "      # Extract metrics from the captured output\n",
    "      mse, mae = extract_metrics_from_output(output)\n",
    "      \n",
    "      # Delete the checkpoints folder and all its contents\n",
    "      shutil.rmtree('./checkpoints' )\n",
    "\n",
    "      # Log the extracted metrics\n",
    "      log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}:\\n\")\n",
    "      log_file.write(f\"MSE: {mse}, MAE: {mae}\\n\")\n",
    "\n",
    "      # Append the results to the informer_results list\n",
    "      informer_results.append({\n",
    "          'Country': country,\n",
    "          'Pred_len': pred_len,\n",
    "          'MSE': mse,\n",
    "          'MAE': mae\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the collected data into DataFrame\n",
    "informer_df = pd.DataFrame(informer_results)\n",
    "\n",
    "# Set multi-index \n",
    "informer_df.set_index(['Country', 'Pred_len'], inplace=True)\n",
    "informer_df = informer_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST\n",
    "\n",
    "We separated PatchTST from Informer, because it has additional arguments. It is not so easy to modify f-string (as e. g. distionary) to unpack some arguments with if statement. Moreover, it has different parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "seq_len = \"336\"\n",
    "model = \"PatchTST\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.00001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "  for i, country in enumerate(countries):\n",
    "\n",
    "    log_file.write(f\"\\n=== Starting experiments for country: {country} ===\\n\")\n",
    "\n",
    "    for pred_len in pred_lens:\n",
    "      model_id = f\"{country}_{seq_len}_{pred_len}\"\n",
    "      dataset = f\"{country}_data.csv\"\n",
    "\n",
    "      # Arguments for the command\n",
    "      command = f\"\"\"\n",
    "      python {script_path} \\\n",
    "        --random_seed 2021 \\\n",
    "        --is_training 1 \\\n",
    "        --root_path \"{data_path}\" \\\n",
    "        --data_path \"{dataset}\" \\\n",
    "        --model_id {model_id} \\\n",
    "        --model \"{model}\" \\\n",
    "        --data \"custom\" \\\n",
    "        --features M \\\n",
    "        --seq_len {seq_len} \\\n",
    "        --label_len 5 \\\n",
    "        --pred_len {pred_len} \\\n",
    "        --e_layers 3 \\\n",
    "        --factor 5 \\\n",
    "        --enc_in \"{num_cols[i]}\" \\\n",
    "        --dec_in \"{num_cols[i]}\" \\\n",
    "        --c_out \"{num_cols[i]}\" \\\n",
    "        --des 'Exp' \\\n",
    "        --train_epochs 20 \\\n",
    "        --patience 3 \\\n",
    "        --n_heads {n_heads} \\\n",
    "        --patch_len 32 \\\n",
    "        --stride 16 \\\n",
    "        --overlapping_windows \\\n",
    "        --itr 1 --batch_size 64 --learning_rate {lr}\n",
    "      \"\"\"\n",
    "      # Log the country and prediction length\n",
    "      log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "      # Run the command and capture the output\n",
    "      process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "      # Capture the output in real-time\n",
    "      output = []\n",
    "      for line in process.stdout:\n",
    "          output.append(line)\n",
    "          print(line, end='')  # Print in the .ipynb cell\n",
    "          log_file.write(line)  # Write to the log file\n",
    "\n",
    "      # Wait for the process to complete\n",
    "      process.wait()\n",
    "\n",
    "      # Extract metrics from the captured output\n",
    "      mse, mae = extract_metrics_from_output(output)\n",
    "\n",
    "      # Log the extracted metrics\n",
    "      log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}:\\n\")\n",
    "      log_file.write(f\"MSE: {mse}, MAE: {mae}\\n\")\n",
    "\n",
    "      # Append the results to the list\n",
    "      patchtst_results.append({\n",
    "          'Country': country,\n",
    "          'Pred_len': pred_len,\n",
    "          'MSE': mse,\n",
    "          'MAE': mae\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the collected data into DataFrame\n",
    "patchtst_df = pd.DataFrame(patchtst_results)\n",
    "\n",
    "# Set multi-index \n",
    "patchtst_df.set_index(['Country', 'Pred_len'], inplace=True)\n",
    "patchtst_df = patchtst_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
