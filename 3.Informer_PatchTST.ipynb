{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. Connecting to CUDA](#1-connecting-to-cuda)\n",
    "- [2. Informer](#2-informer)\n",
    "- [3. PatchTST 168](#3-patchtst-168)\n",
    "- [4. PatchTST 336](#4-patchtst-336)\n",
    "- [5. PatchTST 512](#5-patchtst-512)\n",
    "\n",
    "\n",
    "Script with Informer and PatchTST (default parameters). PatchTST with input look-back windows =168, 336 and 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from utils.helper import extract_metrics_from_output, running_time, convert_results_into_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connecting to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on GPU, because running it on CPU will cost a lot of time.\n",
    "\n",
    "\n",
    "I do not recommend to run it in Google Colab, because it interrupts training process.\n",
    "\n",
    "If you are not going to use remote servers with multiple GPUs, skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# For CUDA making it available this works:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "# Check the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of available GPUs:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the GPU you want to use (e.g., 0, 1, 2, etc.)\n",
    "# Choose that one that is not used by other processes\n",
    "cuda_device = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/informer/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "seq_len = 96\n",
    "model = \"Informer\"\n",
    "itr = 1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.0001\n",
    "#n_heads = 16\n",
    "e_layers = 2\n",
    "d_layers = 1\n",
    "loss = \"MSE\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Lists to store the results\n",
    "informer_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_96_24_DE', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_96_24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0844291\n",
      "\tspeed: 0.1427s/iter; left time: 2572.1345s\n",
      "\titers: 200, epoch: 1 | loss: 0.0693888\n",
      "\tspeed: 0.0494s/iter; left time: 885.8380s\n",
      "\titers: 300, epoch: 1 | loss: 0.0655493\n",
      "\tspeed: 0.0499s/iter; left time: 890.1312s\n",
      "\titers: 400, epoch: 1 | loss: 0.0564399\n",
      "\tspeed: 0.0496s/iter; left time: 879.7680s\n",
      "\titers: 500, epoch: 1 | loss: 0.0501456\n",
      "\tspeed: 0.0501s/iter; left time: 882.9455s\n",
      "\titers: 600, epoch: 1 | loss: 0.0479440\n",
      "\tspeed: 0.0499s/iter; left time: 873.8236s\n",
      "\titers: 700, epoch: 1 | loss: 0.0540418\n",
      "\tspeed: 0.0497s/iter; left time: 866.1543s\n",
      "\titers: 800, epoch: 1 | loss: 0.0417317\n",
      "\tspeed: 0.0496s/iter; left time: 859.8567s\n",
      "\titers: 900, epoch: 1 | loss: 0.0365065\n",
      "\tspeed: 0.0493s/iter; left time: 848.1437s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:52.17s\n",
      "Steps: 906 | Train Loss: 0.0620887 Vali Loss: 0.0407531 Test Loss: 0.0445114\n",
      "Validation loss decreased (inf --> 0.040753).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0251532\n",
      "\tspeed: 0.1083s/iter; left time: 1854.3454s\n",
      "\titers: 200, epoch: 2 | loss: 0.0322412\n",
      "\tspeed: 0.0502s/iter; left time: 854.2099s\n",
      "\titers: 300, epoch: 2 | loss: 0.0245991\n",
      "\tspeed: 0.0499s/iter; left time: 844.4689s\n",
      "\titers: 400, epoch: 2 | loss: 0.0171704\n",
      "\tspeed: 0.0499s/iter; left time: 838.3457s\n",
      "\titers: 500, epoch: 2 | loss: 0.0195076\n",
      "\tspeed: 0.0500s/iter; left time: 836.3813s\n",
      "\titers: 600, epoch: 2 | loss: 0.0165113\n",
      "\tspeed: 0.0496s/iter; left time: 823.7032s\n",
      "\titers: 700, epoch: 2 | loss: 0.0190108\n",
      "\tspeed: 0.0496s/iter; left time: 818.9604s\n",
      "\titers: 800, epoch: 2 | loss: 0.0165992\n",
      "\tspeed: 0.0496s/iter; left time: 813.3686s\n",
      "\titers: 900, epoch: 2 | loss: 0.0153506\n",
      "\tspeed: 0.0496s/iter; left time: 809.8884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:45.41s\n",
      "Steps: 906 | Train Loss: 0.0211746 Vali Loss: 0.0227766 Test Loss: 0.0249225\n",
      "Validation loss decreased (0.040753 --> 0.022777).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0191735\n",
      "\tspeed: 0.1052s/iter; left time: 1704.4077s\n",
      "\titers: 200, epoch: 3 | loss: 0.0155110\n",
      "\tspeed: 0.0499s/iter; left time: 803.6381s\n",
      "\titers: 300, epoch: 3 | loss: 0.0156315\n",
      "\tspeed: 0.0503s/iter; left time: 804.9803s\n",
      "\titers: 400, epoch: 3 | loss: 0.0145043\n",
      "\tspeed: 0.0496s/iter; left time: 788.9447s\n",
      "\titers: 500, epoch: 3 | loss: 0.0143492\n",
      "\tspeed: 0.0497s/iter; left time: 785.2020s\n",
      "\titers: 600, epoch: 3 | loss: 0.0139118\n",
      "\tspeed: 0.0495s/iter; left time: 777.8266s\n",
      "\titers: 700, epoch: 3 | loss: 0.0120604\n",
      "\tspeed: 0.0496s/iter; left time: 774.9063s\n",
      "\titers: 800, epoch: 3 | loss: 0.0160756\n",
      "\tspeed: 0.0487s/iter; left time: 755.4104s\n",
      "\titers: 900, epoch: 3 | loss: 0.0143501\n",
      "\tspeed: 0.0495s/iter; left time: 762.4773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:45.15s\n",
      "Steps: 906 | Train Loss: 0.0150597 Vali Loss: 0.0232971 Test Loss: 0.0252371\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0124714\n",
      "\tspeed: 0.1032s/iter; left time: 1579.5266s\n",
      "\titers: 200, epoch: 4 | loss: 0.0135723\n",
      "\tspeed: 0.0499s/iter; left time: 757.9602s\n",
      "\titers: 300, epoch: 4 | loss: 0.0133752\n",
      "\tspeed: 0.0499s/iter; left time: 753.9695s\n",
      "\titers: 400, epoch: 4 | loss: 0.0128325\n",
      "\tspeed: 0.0499s/iter; left time: 748.0100s\n",
      "\titers: 500, epoch: 4 | loss: 0.0170870\n",
      "\tspeed: 0.0499s/iter; left time: 743.4145s\n",
      "\titers: 600, epoch: 4 | loss: 0.0159228\n",
      "\tspeed: 0.0500s/iter; left time: 739.7313s\n",
      "\titers: 700, epoch: 4 | loss: 0.0160074\n",
      "\tspeed: 0.0500s/iter; left time: 734.7973s\n",
      "\titers: 800, epoch: 4 | loss: 0.0128487\n",
      "\tspeed: 0.0500s/iter; left time: 729.5119s\n",
      "\titers: 900, epoch: 4 | loss: 0.0095744\n",
      "\tspeed: 0.0498s/iter; left time: 722.5718s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:45.44s\n",
      "Steps: 906 | Train Loss: 0.0137407 Vali Loss: 0.0219377 Test Loss: 0.0240274\n",
      "Validation loss decreased (0.022777 --> 0.021938).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0123596\n",
      "\tspeed: 0.1080s/iter; left time: 1555.5584s\n",
      "\titers: 200, epoch: 5 | loss: 0.0123557\n",
      "\tspeed: 0.0504s/iter; left time: 720.7002s\n",
      "\titers: 300, epoch: 5 | loss: 0.0114830\n",
      "\tspeed: 0.0501s/iter; left time: 711.9341s\n",
      "\titers: 400, epoch: 5 | loss: 0.0166790\n",
      "\tspeed: 0.0503s/iter; left time: 708.9998s\n",
      "\titers: 500, epoch: 5 | loss: 0.0130099\n",
      "\tspeed: 0.0496s/iter; left time: 694.4080s\n",
      "\titers: 600, epoch: 5 | loss: 0.0162379\n",
      "\tspeed: 0.0502s/iter; left time: 697.7440s\n",
      "\titers: 700, epoch: 5 | loss: 0.0147405\n",
      "\tspeed: 0.0502s/iter; left time: 692.5045s\n",
      "\titers: 800, epoch: 5 | loss: 0.0135671\n",
      "\tspeed: 0.0504s/iter; left time: 690.7652s\n",
      "\titers: 900, epoch: 5 | loss: 0.0163193\n",
      "\tspeed: 0.0503s/iter; left time: 684.3252s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:45.74s\n",
      "Steps: 906 | Train Loss: 0.0126119 Vali Loss: 0.0216679 Test Loss: 0.0243225\n",
      "Validation loss decreased (0.021938 --> 0.021668).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0116733\n",
      "\tspeed: 0.1058s/iter; left time: 1427.9170s\n",
      "\titers: 200, epoch: 6 | loss: 0.0101081\n",
      "\tspeed: 0.0496s/iter; left time: 664.2453s\n",
      "\titers: 300, epoch: 6 | loss: 0.0103430\n",
      "\tspeed: 0.0501s/iter; left time: 666.2620s\n",
      "\titers: 400, epoch: 6 | loss: 0.0111628\n",
      "\tspeed: 0.0495s/iter; left time: 653.5656s\n",
      "\titers: 500, epoch: 6 | loss: 0.0122635\n",
      "\tspeed: 0.0500s/iter; left time: 654.5210s\n",
      "\titers: 600, epoch: 6 | loss: 0.0087395\n",
      "\tspeed: 0.0500s/iter; left time: 649.0961s\n",
      "\titers: 700, epoch: 6 | loss: 0.0120656\n",
      "\tspeed: 0.0501s/iter; left time: 645.6841s\n",
      "\titers: 800, epoch: 6 | loss: 0.0125768\n",
      "\tspeed: 0.0498s/iter; left time: 636.7036s\n",
      "\titers: 900, epoch: 6 | loss: 0.0099931\n",
      "\tspeed: 0.0496s/iter; left time: 629.8804s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:45.33s\n",
      "Steps: 906 | Train Loss: 0.0116739 Vali Loss: 0.0211125 Test Loss: 0.0254185\n",
      "Validation loss decreased (0.021668 --> 0.021113).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0096771\n",
      "\tspeed: 0.1062s/iter; left time: 1336.8479s\n",
      "\titers: 200, epoch: 7 | loss: 0.0094396\n",
      "\tspeed: 0.0499s/iter; left time: 623.2537s\n",
      "\titers: 300, epoch: 7 | loss: 0.0114610\n",
      "\tspeed: 0.0496s/iter; left time: 614.8298s\n",
      "\titers: 400, epoch: 7 | loss: 0.0119442\n",
      "\tspeed: 0.0491s/iter; left time: 603.5330s\n",
      "\titers: 500, epoch: 7 | loss: 0.0132686\n",
      "\tspeed: 0.0493s/iter; left time: 600.8634s\n",
      "\titers: 600, epoch: 7 | loss: 0.0079299\n",
      "\tspeed: 0.0478s/iter; left time: 577.4773s\n",
      "\titers: 700, epoch: 7 | loss: 0.0124909\n",
      "\tspeed: 0.0480s/iter; left time: 575.4539s\n",
      "\titers: 800, epoch: 7 | loss: 0.0076778\n",
      "\tspeed: 0.0465s/iter; left time: 552.8320s\n",
      "\titers: 900, epoch: 7 | loss: 0.0091967\n",
      "\tspeed: 0.0422s/iter; left time: 496.8931s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:43.72s\n",
      "Steps: 906 | Train Loss: 0.0107831 Vali Loss: 0.0222866 Test Loss: 0.0263801\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0086997\n",
      "\tspeed: 0.0981s/iter; left time: 1145.6847s\n",
      "\titers: 200, epoch: 8 | loss: 0.0115932\n",
      "\tspeed: 0.0399s/iter; left time: 461.9744s\n",
      "\titers: 300, epoch: 8 | loss: 0.0087272\n",
      "\tspeed: 0.0488s/iter; left time: 560.3632s\n",
      "\titers: 400, epoch: 8 | loss: 0.0111578\n",
      "\tspeed: 0.0463s/iter; left time: 527.2175s\n",
      "\titers: 500, epoch: 8 | loss: 0.0074500\n",
      "\tspeed: 0.0481s/iter; left time: 542.4308s\n",
      "\titers: 600, epoch: 8 | loss: 0.0114486\n",
      "\tspeed: 0.0416s/iter; left time: 464.6558s\n",
      "\titers: 700, epoch: 8 | loss: 0.0086220\n",
      "\tspeed: 0.0411s/iter; left time: 455.1277s\n",
      "\titers: 800, epoch: 8 | loss: 0.0097864\n",
      "\tspeed: 0.0511s/iter; left time: 561.1245s\n",
      "\titers: 900, epoch: 8 | loss: 0.0122829\n",
      "\tspeed: 0.0441s/iter; left time: 479.5812s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:41.10s\n",
      "Steps: 906 | Train Loss: 0.0098266 Vali Loss: 0.0233274 Test Loss: 0.0263052\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0102292\n",
      "\tspeed: 0.1046s/iter; left time: 1126.3727s\n",
      "\titers: 200, epoch: 9 | loss: 0.0096972\n",
      "\tspeed: 0.0450s/iter; left time: 480.0894s\n",
      "\titers: 300, epoch: 9 | loss: 0.0081635\n",
      "\tspeed: 0.0453s/iter; left time: 479.3784s\n",
      "\titers: 400, epoch: 9 | loss: 0.0104667\n",
      "\tspeed: 0.0503s/iter; left time: 527.2788s\n",
      "\titers: 500, epoch: 9 | loss: 0.0084879\n",
      "\tspeed: 0.0438s/iter; left time: 454.4248s\n",
      "\titers: 600, epoch: 9 | loss: 0.0097039\n",
      "\tspeed: 0.0484s/iter; left time: 496.8385s\n",
      "\titers: 700, epoch: 9 | loss: 0.0099554\n",
      "\tspeed: 0.0495s/iter; left time: 504.0006s\n",
      "\titers: 800, epoch: 9 | loss: 0.0107502\n",
      "\tspeed: 0.0443s/iter; left time: 445.7749s\n",
      "\titers: 900, epoch: 9 | loss: 0.0077211\n",
      "\tspeed: 0.0443s/iter; left time: 441.4584s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:42.41s\n",
      "Steps: 906 | Train Loss: 0.0090420 Vali Loss: 0.0235619 Test Loss: 0.0281686\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0089565\n",
      "\tspeed: 0.1001s/iter; left time: 987.2302s\n",
      "\titers: 200, epoch: 10 | loss: 0.0086324\n",
      "\tspeed: 0.0488s/iter; left time: 476.6851s\n",
      "\titers: 300, epoch: 10 | loss: 0.0090554\n",
      "\tspeed: 0.0433s/iter; left time: 418.2883s\n",
      "\titers: 400, epoch: 10 | loss: 0.0098368\n",
      "\tspeed: 0.0487s/iter; left time: 465.9023s\n",
      "\titers: 500, epoch: 10 | loss: 0.0081708\n",
      "\tspeed: 0.0492s/iter; left time: 465.5845s\n",
      "\titers: 600, epoch: 10 | loss: 0.0068844\n",
      "\tspeed: 0.0465s/iter; left time: 435.7801s\n",
      "\titers: 700, epoch: 10 | loss: 0.0071657\n",
      "\tspeed: 0.0452s/iter; left time: 418.6769s\n",
      "\titers: 800, epoch: 10 | loss: 0.0073063\n",
      "\tspeed: 0.0499s/iter; left time: 457.5685s\n",
      "\titers: 900, epoch: 10 | loss: 0.0070751\n",
      "\tspeed: 0.0434s/iter; left time: 393.2744s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:42.52s\n",
      "Steps: 906 | Train Loss: 0.0083348 Vali Loss: 0.0225941 Test Loss: 0.0268508\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0080009\n",
      "\tspeed: 0.0976s/iter; left time: 874.8162s\n",
      "\titers: 200, epoch: 11 | loss: 0.0075677\n",
      "\tspeed: 0.0495s/iter; left time: 438.4793s\n",
      "\titers: 300, epoch: 11 | loss: 0.0058150\n",
      "\tspeed: 0.0471s/iter; left time: 412.7264s\n",
      "\titers: 400, epoch: 11 | loss: 0.0062782\n",
      "\tspeed: 0.0446s/iter; left time: 386.6124s\n",
      "\titers: 500, epoch: 11 | loss: 0.0092325\n",
      "\tspeed: 0.0475s/iter; left time: 406.2238s\n",
      "\titers: 600, epoch: 11 | loss: 0.0077828\n",
      "\tspeed: 0.0467s/iter; left time: 395.0152s\n",
      "\titers: 700, epoch: 11 | loss: 0.0077392\n",
      "\tspeed: 0.0478s/iter; left time: 399.9426s\n",
      "\titers: 800, epoch: 11 | loss: 0.0067178\n",
      "\tspeed: 0.0462s/iter; left time: 381.7836s\n",
      "\titers: 900, epoch: 11 | loss: 0.0054118\n",
      "\tspeed: 0.0460s/iter; left time: 375.4481s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:42.28s\n",
      "Steps: 906 | Train Loss: 0.0075969 Vali Loss: 0.0233432 Test Loss: 0.0285462\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_96_24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02549596317112446, rmse:0.15967455506324768, mae:0.10630149394273758, rse:0.5638949871063232\n",
      "Intermediate time for DE and pred_len 24: 00h:09m:19.20s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_96_96_DE', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_96_96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0908792\n",
      "\tspeed: 0.0750s/iter; left time: 1348.1348s\n",
      "\titers: 200, epoch: 1 | loss: 0.0826019\n",
      "\tspeed: 0.0508s/iter; left time: 907.8065s\n",
      "\titers: 300, epoch: 1 | loss: 0.0739485\n",
      "\tspeed: 0.0444s/iter; left time: 788.6712s\n",
      "\titers: 400, epoch: 1 | loss: 0.0652045\n",
      "\tspeed: 0.0461s/iter; left time: 814.3695s\n",
      "\titers: 500, epoch: 1 | loss: 0.0608115\n",
      "\tspeed: 0.0443s/iter; left time: 779.4635s\n",
      "\titers: 600, epoch: 1 | loss: 0.0582726\n",
      "\tspeed: 0.0516s/iter; left time: 902.3502s\n",
      "\titers: 700, epoch: 1 | loss: 0.0530457\n",
      "\tspeed: 0.0436s/iter; left time: 757.0509s\n",
      "\titers: 800, epoch: 1 | loss: 0.0556172\n",
      "\tspeed: 0.0423s/iter; left time: 731.2912s\n",
      "\titers: 900, epoch: 1 | loss: 0.0533727\n",
      "\tspeed: 0.0443s/iter; left time: 761.0885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:42.39s\n",
      "Steps: 904 | Train Loss: 0.0693695 Vali Loss: 0.0569431 Test Loss: 0.0726330\n",
      "Validation loss decreased (inf --> 0.056943).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0445395\n",
      "\tspeed: 0.1197s/iter; left time: 2043.4044s\n",
      "\titers: 200, epoch: 2 | loss: 0.0347430\n",
      "\tspeed: 0.0511s/iter; left time: 867.8837s\n",
      "\titers: 300, epoch: 2 | loss: 0.0310663\n",
      "\tspeed: 0.0486s/iter; left time: 819.5342s\n",
      "\titers: 400, epoch: 2 | loss: 0.0340965\n",
      "\tspeed: 0.0504s/iter; left time: 846.3156s\n",
      "\titers: 500, epoch: 2 | loss: 0.0340284\n",
      "\tspeed: 0.0494s/iter; left time: 823.8322s\n",
      "\titers: 600, epoch: 2 | loss: 0.0291398\n",
      "\tspeed: 0.0512s/iter; left time: 848.4981s\n",
      "\titers: 700, epoch: 2 | loss: 0.0287280\n",
      "\tspeed: 0.0428s/iter; left time: 705.9979s\n",
      "\titers: 800, epoch: 2 | loss: 0.0285814\n",
      "\tspeed: 0.0439s/iter; left time: 719.2027s\n",
      "\titers: 900, epoch: 2 | loss: 0.0318606\n",
      "\tspeed: 0.0492s/iter; left time: 800.0778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:44.24s\n",
      "Steps: 904 | Train Loss: 0.0336223 Vali Loss: 0.0370270 Test Loss: 0.0475181\n",
      "Validation loss decreased (0.056943 --> 0.037027).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0267080\n",
      "\tspeed: 0.1167s/iter; left time: 1886.8275s\n",
      "\titers: 200, epoch: 3 | loss: 0.0226177\n",
      "\tspeed: 0.0514s/iter; left time: 825.4768s\n",
      "\titers: 300, epoch: 3 | loss: 0.0249391\n",
      "\tspeed: 0.0453s/iter; left time: 723.7616s\n",
      "\titers: 400, epoch: 3 | loss: 0.0226962\n",
      "\tspeed: 0.0489s/iter; left time: 775.8878s\n",
      "\titers: 500, epoch: 3 | loss: 0.0232413\n",
      "\tspeed: 0.0469s/iter; left time: 739.4723s\n",
      "\titers: 600, epoch: 3 | loss: 0.0222054\n",
      "\tspeed: 0.0441s/iter; left time: 690.6065s\n",
      "\titers: 700, epoch: 3 | loss: 0.0253964\n",
      "\tspeed: 0.0474s/iter; left time: 738.2745s\n",
      "\titers: 800, epoch: 3 | loss: 0.0199290\n",
      "\tspeed: 0.0436s/iter; left time: 674.9476s\n",
      "\titers: 900, epoch: 3 | loss: 0.0254071\n",
      "\tspeed: 0.0483s/iter; left time: 743.2619s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:42.83s\n",
      "Steps: 904 | Train Loss: 0.0236946 Vali Loss: 0.0336329 Test Loss: 0.0415587\n",
      "Validation loss decreased (0.037027 --> 0.033633).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0275110\n",
      "\tspeed: 0.1120s/iter; left time: 1709.5452s\n",
      "\titers: 200, epoch: 4 | loss: 0.0187502\n",
      "\tspeed: 0.0423s/iter; left time: 641.2553s\n",
      "\titers: 300, epoch: 4 | loss: 0.0265851\n",
      "\tspeed: 0.0341s/iter; left time: 513.1235s\n",
      "\titers: 400, epoch: 4 | loss: 0.0211057\n",
      "\tspeed: 0.0345s/iter; left time: 516.0901s\n",
      "\titers: 500, epoch: 4 | loss: 0.0217084\n",
      "\tspeed: 0.0399s/iter; left time: 593.4692s\n",
      "\titers: 600, epoch: 4 | loss: 0.0198707\n",
      "\tspeed: 0.0437s/iter; left time: 644.8752s\n",
      "\titers: 700, epoch: 4 | loss: 0.0226085\n",
      "\tspeed: 0.0348s/iter; left time: 510.1822s\n",
      "\titers: 800, epoch: 4 | loss: 0.0182402\n",
      "\tspeed: 0.0353s/iter; left time: 513.8942s\n",
      "\titers: 900, epoch: 4 | loss: 0.0187977\n",
      "\tspeed: 0.0350s/iter; left time: 506.9563s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:35.17s\n",
      "Steps: 904 | Train Loss: 0.0206394 Vali Loss: 0.0352910 Test Loss: 0.0430563\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0173709\n",
      "\tspeed: 0.0985s/iter; left time: 1415.4455s\n",
      "\titers: 200, epoch: 5 | loss: 0.0166946\n",
      "\tspeed: 0.0342s/iter; left time: 488.1990s\n",
      "\titers: 300, epoch: 5 | loss: 0.0180556\n",
      "\tspeed: 0.0351s/iter; left time: 496.6928s\n",
      "\titers: 400, epoch: 5 | loss: 0.0225047\n",
      "\tspeed: 0.0345s/iter; left time: 484.8603s\n",
      "\titers: 500, epoch: 5 | loss: 0.0165685\n",
      "\tspeed: 0.0339s/iter; left time: 473.2486s\n",
      "\titers: 600, epoch: 5 | loss: 0.0213444\n",
      "\tspeed: 0.0334s/iter; left time: 462.5078s\n",
      "\titers: 700, epoch: 5 | loss: 0.0211287\n",
      "\tspeed: 0.0345s/iter; left time: 474.7769s\n",
      "\titers: 800, epoch: 5 | loss: 0.0154284\n",
      "\tspeed: 0.0343s/iter; left time: 468.3148s\n",
      "\titers: 900, epoch: 5 | loss: 0.0167327\n",
      "\tspeed: 0.0347s/iter; left time: 470.5814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:31.45s\n",
      "Steps: 904 | Train Loss: 0.0181818 Vali Loss: 0.0360216 Test Loss: 0.0486139\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0162390\n",
      "\tspeed: 0.0993s/iter; left time: 1337.0378s\n",
      "\titers: 200, epoch: 6 | loss: 0.0149850\n",
      "\tspeed: 0.0348s/iter; left time: 465.5123s\n",
      "\titers: 300, epoch: 6 | loss: 0.0161773\n",
      "\tspeed: 0.0377s/iter; left time: 499.4532s\n",
      "\titers: 400, epoch: 6 | loss: 0.0161455\n",
      "\tspeed: 0.0350s/iter; left time: 460.4346s\n",
      "\titers: 500, epoch: 6 | loss: 0.0159038\n",
      "\tspeed: 0.0371s/iter; left time: 484.9915s\n",
      "\titers: 600, epoch: 6 | loss: 0.0155437\n",
      "\tspeed: 0.0375s/iter; left time: 486.3731s\n",
      "\titers: 700, epoch: 6 | loss: 0.0135126\n",
      "\tspeed: 0.0360s/iter; left time: 462.8306s\n",
      "\titers: 800, epoch: 6 | loss: 0.0161070\n",
      "\tspeed: 0.0370s/iter; left time: 472.5303s\n",
      "\titers: 900, epoch: 6 | loss: 0.0155187\n",
      "\tspeed: 0.0366s/iter; left time: 463.9396s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:33.27s\n",
      "Steps: 904 | Train Loss: 0.0158869 Vali Loss: 0.0358567 Test Loss: 0.0487426\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0146865\n",
      "\tspeed: 0.1068s/iter; left time: 1341.2589s\n",
      "\titers: 200, epoch: 7 | loss: 0.0132540\n",
      "\tspeed: 0.0346s/iter; left time: 430.8277s\n",
      "\titers: 300, epoch: 7 | loss: 0.0136056\n",
      "\tspeed: 0.0371s/iter; left time: 457.8333s\n",
      "\titers: 400, epoch: 7 | loss: 0.0151962\n",
      "\tspeed: 0.0350s/iter; left time: 429.5408s\n",
      "\titers: 500, epoch: 7 | loss: 0.0126808\n",
      "\tspeed: 0.0350s/iter; left time: 425.5842s\n",
      "\titers: 600, epoch: 7 | loss: 0.0143357\n",
      "\tspeed: 0.0384s/iter; left time: 462.6141s\n",
      "\titers: 700, epoch: 7 | loss: 0.0140345\n",
      "\tspeed: 0.0367s/iter; left time: 439.3212s\n",
      "\titers: 800, epoch: 7 | loss: 0.0124623\n",
      "\tspeed: 0.0337s/iter; left time: 399.1144s\n",
      "\titers: 900, epoch: 7 | loss: 0.0136355\n",
      "\tspeed: 0.0375s/iter; left time: 440.9399s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:33.20s\n",
      "Steps: 904 | Train Loss: 0.0139272 Vali Loss: 0.0363626 Test Loss: 0.0491312\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0122056\n",
      "\tspeed: 0.1078s/iter; left time: 1256.2333s\n",
      "\titers: 200, epoch: 8 | loss: 0.0113194\n",
      "\tspeed: 0.0339s/iter; left time: 391.4538s\n",
      "\titers: 300, epoch: 8 | loss: 0.0139114\n",
      "\tspeed: 0.0348s/iter; left time: 398.0912s\n",
      "\titers: 400, epoch: 8 | loss: 0.0119940\n",
      "\tspeed: 0.0375s/iter; left time: 425.5855s\n",
      "\titers: 500, epoch: 8 | loss: 0.0131242\n",
      "\tspeed: 0.0349s/iter; left time: 392.5505s\n",
      "\titers: 600, epoch: 8 | loss: 0.0141879\n",
      "\tspeed: 0.0370s/iter; left time: 412.4413s\n",
      "\titers: 700, epoch: 8 | loss: 0.0130758\n",
      "\tspeed: 0.0368s/iter; left time: 407.2791s\n",
      "\titers: 800, epoch: 8 | loss: 0.0103310\n",
      "\tspeed: 0.0347s/iter; left time: 379.9474s\n",
      "\titers: 900, epoch: 8 | loss: 0.0104962\n",
      "\tspeed: 0.0370s/iter; left time: 401.4193s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:33.20s\n",
      "Steps: 904 | Train Loss: 0.0124156 Vali Loss: 0.0373577 Test Loss: 0.0492265\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_96_96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04156292974948883, rmse:0.20386987924575806, mae:0.14678193628787994, rse:0.7219445109367371\n",
      "Intermediate time for DE and pred_len 96: 00h:05m:58.92s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_96_168_DE', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_96_168_DE_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0861944\n",
      "\tspeed: 0.1069s/iter; left time: 1917.8637s\n",
      "\titers: 200, epoch: 1 | loss: 0.0816551\n",
      "\tspeed: 0.0668s/iter; left time: 1192.5272s\n",
      "\titers: 300, epoch: 1 | loss: 0.0688623\n",
      "\tspeed: 0.0612s/iter; left time: 1086.2980s\n",
      "\titers: 400, epoch: 1 | loss: 0.0678409\n",
      "\tspeed: 0.0637s/iter; left time: 1124.2229s\n",
      "\titers: 500, epoch: 1 | loss: 0.0633130\n",
      "\tspeed: 0.0704s/iter; left time: 1234.9969s\n",
      "\titers: 600, epoch: 1 | loss: 0.0644796\n",
      "\tspeed: 0.0585s/iter; left time: 1021.1471s\n",
      "\titers: 700, epoch: 1 | loss: 0.0581975\n",
      "\tspeed: 0.0669s/iter; left time: 1159.7102s\n",
      "\titers: 800, epoch: 1 | loss: 0.0562265\n",
      "\tspeed: 0.0696s/iter; left time: 1200.3932s\n",
      "\titers: 900, epoch: 1 | loss: 0.0581804\n",
      "\tspeed: 0.0579s/iter; left time: 992.7731s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:58.85s\n",
      "Steps: 902 | Train Loss: 0.0709426 Vali Loss: 0.0584743 Test Loss: 0.0763278\n",
      "Validation loss decreased (inf --> 0.058474).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0473072\n",
      "\tspeed: 0.2372s/iter; left time: 4042.0005s\n",
      "\titers: 200, epoch: 2 | loss: 0.0385538\n",
      "\tspeed: 0.0639s/iter; left time: 1082.8934s\n",
      "\titers: 300, epoch: 2 | loss: 0.0370987\n",
      "\tspeed: 0.0623s/iter; left time: 1048.9279s\n",
      "\titers: 400, epoch: 2 | loss: 0.0379711\n",
      "\tspeed: 0.0719s/iter; left time: 1203.0546s\n",
      "\titers: 500, epoch: 2 | loss: 0.0324598\n",
      "\tspeed: 0.0607s/iter; left time: 1010.2992s\n",
      "\titers: 600, epoch: 2 | loss: 0.0304995\n",
      "\tspeed: 0.0650s/iter; left time: 1075.7078s\n",
      "\titers: 700, epoch: 2 | loss: 0.0285279\n",
      "\tspeed: 0.0646s/iter; left time: 1061.9442s\n",
      "\titers: 800, epoch: 2 | loss: 0.0329162\n",
      "\tspeed: 0.0602s/iter; left time: 983.4328s\n",
      "\titers: 900, epoch: 2 | loss: 0.0283897\n",
      "\tspeed: 0.0712s/iter; left time: 1155.9906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:58.96s\n",
      "Steps: 902 | Train Loss: 0.0370804 Vali Loss: 0.0431616 Test Loss: 0.0524194\n",
      "Validation loss decreased (0.058474 --> 0.043162).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0306641\n",
      "\tspeed: 0.2314s/iter; left time: 3733.8028s\n",
      "\titers: 200, epoch: 3 | loss: 0.0285753\n",
      "\tspeed: 0.0554s/iter; left time: 888.4296s\n",
      "\titers: 300, epoch: 3 | loss: 0.0272127\n",
      "\tspeed: 0.0618s/iter; left time: 984.2828s\n",
      "\titers: 400, epoch: 3 | loss: 0.0306366\n",
      "\tspeed: 0.0478s/iter; left time: 756.5741s\n",
      "\titers: 500, epoch: 3 | loss: 0.0275743\n",
      "\tspeed: 0.0383s/iter; left time: 602.2215s\n",
      "\titers: 600, epoch: 3 | loss: 0.0266086\n",
      "\tspeed: 0.0492s/iter; left time: 770.0451s\n",
      "\titers: 700, epoch: 3 | loss: 0.0248030\n",
      "\tspeed: 0.0595s/iter; left time: 924.1043s\n",
      "\titers: 800, epoch: 3 | loss: 0.0238694\n",
      "\tspeed: 0.0686s/iter; left time: 1058.7800s\n",
      "\titers: 900, epoch: 3 | loss: 0.0242699\n",
      "\tspeed: 0.0580s/iter; left time: 890.3062s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:50.26s\n",
      "Steps: 902 | Train Loss: 0.0274520 Vali Loss: 0.0379027 Test Loss: 0.0459632\n",
      "Validation loss decreased (0.043162 --> 0.037903).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0232967\n",
      "\tspeed: 0.2251s/iter; left time: 3429.1374s\n",
      "\titers: 200, epoch: 4 | loss: 0.0210634\n",
      "\tspeed: 0.0673s/iter; left time: 1018.4676s\n",
      "\titers: 300, epoch: 4 | loss: 0.0214336\n",
      "\tspeed: 0.0544s/iter; left time: 817.6585s\n",
      "\titers: 400, epoch: 4 | loss: 0.0200201\n",
      "\tspeed: 0.0591s/iter; left time: 882.9815s\n",
      "\titers: 500, epoch: 4 | loss: 0.0191740\n",
      "\tspeed: 0.0609s/iter; left time: 902.8600s\n",
      "\titers: 600, epoch: 4 | loss: 0.0223153\n",
      "\tspeed: 0.0554s/iter; left time: 816.0936s\n",
      "\titers: 700, epoch: 4 | loss: 0.0197920\n",
      "\tspeed: 0.0577s/iter; left time: 844.4934s\n",
      "\titers: 800, epoch: 4 | loss: 0.0204711\n",
      "\tspeed: 0.0554s/iter; left time: 804.8677s\n",
      "\titers: 900, epoch: 4 | loss: 0.0179707\n",
      "\tspeed: 0.0643s/iter; left time: 927.9186s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:53.81s\n",
      "Steps: 902 | Train Loss: 0.0220058 Vali Loss: 0.0428637 Test Loss: 0.0473708\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0184987\n",
      "\tspeed: 0.2127s/iter; left time: 3048.9004s\n",
      "\titers: 200, epoch: 5 | loss: 0.0188701\n",
      "\tspeed: 0.0567s/iter; left time: 807.6322s\n",
      "\titers: 300, epoch: 5 | loss: 0.0192380\n",
      "\tspeed: 0.0575s/iter; left time: 812.2377s\n",
      "\titers: 400, epoch: 5 | loss: 0.0198872\n",
      "\tspeed: 0.0606s/iter; left time: 850.8197s\n",
      "\titers: 500, epoch: 5 | loss: 0.0168683\n",
      "\tspeed: 0.0547s/iter; left time: 762.8055s\n",
      "\titers: 600, epoch: 5 | loss: 0.0176144\n",
      "\tspeed: 0.0613s/iter; left time: 847.7396s\n",
      "\titers: 700, epoch: 5 | loss: 0.0178311\n",
      "\tspeed: 0.0565s/iter; left time: 775.6462s\n",
      "\titers: 800, epoch: 5 | loss: 0.0204910\n",
      "\tspeed: 0.0577s/iter; left time: 786.5932s\n",
      "\titers: 900, epoch: 5 | loss: 0.0178536\n",
      "\tspeed: 0.0523s/iter; left time: 708.2676s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:52.21s\n",
      "Steps: 902 | Train Loss: 0.0191415 Vali Loss: 0.0384142 Test Loss: 0.0487048\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0186351\n",
      "\tspeed: 0.2125s/iter; left time: 2853.5858s\n",
      "\titers: 200, epoch: 6 | loss: 0.0176800\n",
      "\tspeed: 0.0600s/iter; left time: 799.9818s\n",
      "\titers: 300, epoch: 6 | loss: 0.0170789\n",
      "\tspeed: 0.0594s/iter; left time: 786.0428s\n",
      "\titers: 400, epoch: 6 | loss: 0.0206994\n",
      "\tspeed: 0.0566s/iter; left time: 743.8695s\n",
      "\titers: 500, epoch: 6 | loss: 0.0190296\n",
      "\tspeed: 0.0539s/iter; left time: 701.9603s\n",
      "\titers: 600, epoch: 6 | loss: 0.0164793\n",
      "\tspeed: 0.0578s/iter; left time: 747.4678s\n",
      "\titers: 700, epoch: 6 | loss: 0.0139499\n",
      "\tspeed: 0.0585s/iter; left time: 750.8084s\n",
      "\titers: 800, epoch: 6 | loss: 0.0153553\n",
      "\tspeed: 0.0571s/iter; left time: 726.3704s\n",
      "\titers: 900, epoch: 6 | loss: 0.0163441\n",
      "\tspeed: 0.0567s/iter; left time: 716.2719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:51.82s\n",
      "Steps: 902 | Train Loss: 0.0168664 Vali Loss: 0.0399395 Test Loss: 0.0500806\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0141258\n",
      "\tspeed: 0.2079s/iter; left time: 2604.2335s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150832\n",
      "\tspeed: 0.0543s/iter; left time: 675.0986s\n",
      "\titers: 300, epoch: 7 | loss: 0.0154001\n",
      "\tspeed: 0.0515s/iter; left time: 635.4854s\n",
      "\titers: 400, epoch: 7 | loss: 0.0129656\n",
      "\tspeed: 0.0559s/iter; left time: 683.3631s\n",
      "\titers: 500, epoch: 7 | loss: 0.0154558\n",
      "\tspeed: 0.0582s/iter; left time: 705.4288s\n",
      "\titers: 600, epoch: 7 | loss: 0.0135689\n",
      "\tspeed: 0.0548s/iter; left time: 658.6908s\n",
      "\titers: 700, epoch: 7 | loss: 0.0144427\n",
      "\tspeed: 0.0514s/iter; left time: 613.6999s\n",
      "\titers: 800, epoch: 7 | loss: 0.0146086\n",
      "\tspeed: 0.0567s/iter; left time: 670.1602s\n",
      "\titers: 900, epoch: 7 | loss: 0.0156404\n",
      "\tspeed: 0.0569s/iter; left time: 667.0137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:50.21s\n",
      "Steps: 902 | Train Loss: 0.0148521 Vali Loss: 0.0405050 Test Loss: 0.0519974\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0135467\n",
      "\tspeed: 0.2082s/iter; left time: 2420.1803s\n",
      "\titers: 200, epoch: 8 | loss: 0.0147633\n",
      "\tspeed: 0.0561s/iter; left time: 646.7043s\n",
      "\titers: 300, epoch: 8 | loss: 0.0138894\n",
      "\tspeed: 0.0584s/iter; left time: 667.0506s\n",
      "\titers: 400, epoch: 8 | loss: 0.0129612\n",
      "\tspeed: 0.0528s/iter; left time: 598.4363s\n",
      "\titers: 500, epoch: 8 | loss: 0.0127115\n",
      "\tspeed: 0.0560s/iter; left time: 628.6022s\n",
      "\titers: 600, epoch: 8 | loss: 0.0125061\n",
      "\tspeed: 0.0585s/iter; left time: 651.3853s\n",
      "\titers: 700, epoch: 8 | loss: 0.0132125\n",
      "\tspeed: 0.0555s/iter; left time: 611.8251s\n",
      "\titers: 800, epoch: 8 | loss: 0.0115913\n",
      "\tspeed: 0.0551s/iter; left time: 602.2714s\n",
      "\titers: 900, epoch: 8 | loss: 0.0115792\n",
      "\tspeed: 0.0537s/iter; left time: 581.9366s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:50.34s\n",
      "Steps: 902 | Train Loss: 0.0132287 Vali Loss: 0.0421177 Test Loss: 0.0539093\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_96_168_DE_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04590200260281563, rmse:0.21424752473831177, mae:0.15394257009029388, rse:0.7590144276618958\n",
      "Intermediate time for DE and pred_len 168: 00h:09m:31.89s\n",
      "Intermediate time for DE: 00h:24m:50.01s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_24_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0764679\n",
      "\tspeed: 0.0622s/iter; left time: 1120.8065s\n",
      "\titers: 200, epoch: 1 | loss: 0.0657391\n",
      "\tspeed: 0.0263s/iter; left time: 471.5337s\n",
      "\titers: 300, epoch: 1 | loss: 0.0562565\n",
      "\tspeed: 0.0266s/iter; left time: 474.1467s\n",
      "\titers: 400, epoch: 1 | loss: 0.0548274\n",
      "\tspeed: 0.0265s/iter; left time: 470.2608s\n",
      "\titers: 500, epoch: 1 | loss: 0.0477097\n",
      "\tspeed: 0.0260s/iter; left time: 458.0102s\n",
      "\titers: 600, epoch: 1 | loss: 0.0463367\n",
      "\tspeed: 0.0260s/iter; left time: 455.9176s\n",
      "\titers: 700, epoch: 1 | loss: 0.0439485\n",
      "\tspeed: 0.0279s/iter; left time: 485.8104s\n",
      "\titers: 800, epoch: 1 | loss: 0.0382239\n",
      "\tspeed: 0.0271s/iter; left time: 469.7998s\n",
      "\titers: 900, epoch: 1 | loss: 0.0366430\n",
      "\tspeed: 0.0320s/iter; left time: 551.9255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:25.68s\n",
      "Steps: 906 | Train Loss: 0.0594367 Vali Loss: 0.0429403 Test Loss: 0.0600197\n",
      "Validation loss decreased (inf --> 0.042940).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0303652\n",
      "\tspeed: 0.1016s/iter; left time: 1738.7253s\n",
      "\titers: 200, epoch: 2 | loss: 0.0261540\n",
      "\tspeed: 0.0343s/iter; left time: 583.0645s\n",
      "\titers: 300, epoch: 2 | loss: 0.0150766\n",
      "\tspeed: 0.0326s/iter; left time: 551.3112s\n",
      "\titers: 400, epoch: 2 | loss: 0.0167487\n",
      "\tspeed: 0.0360s/iter; left time: 606.0696s\n",
      "\titers: 500, epoch: 2 | loss: 0.0202715\n",
      "\tspeed: 0.0336s/iter; left time: 561.2670s\n",
      "\titers: 600, epoch: 2 | loss: 0.0124757\n",
      "\tspeed: 0.0326s/iter; left time: 542.4203s\n",
      "\titers: 700, epoch: 2 | loss: 0.0234306\n",
      "\tspeed: 0.0325s/iter; left time: 536.8086s\n",
      "\titers: 800, epoch: 2 | loss: 0.0192762\n",
      "\tspeed: 0.0339s/iter; left time: 557.0487s\n",
      "\titers: 900, epoch: 2 | loss: 0.0130124\n",
      "\tspeed: 0.0371s/iter; left time: 605.7242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:31.11s\n",
      "Steps: 906 | Train Loss: 0.0197113 Vali Loss: 0.0220081 Test Loss: 0.0308800\n",
      "Validation loss decreased (0.042940 --> 0.022008).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0158460\n",
      "\tspeed: 0.1025s/iter; left time: 1661.7703s\n",
      "\titers: 200, epoch: 3 | loss: 0.0151482\n",
      "\tspeed: 0.0334s/iter; left time: 537.9538s\n",
      "\titers: 300, epoch: 3 | loss: 0.0177428\n",
      "\tspeed: 0.0329s/iter; left time: 526.2997s\n",
      "\titers: 400, epoch: 3 | loss: 0.0127546\n",
      "\tspeed: 0.0359s/iter; left time: 571.2037s\n",
      "\titers: 500, epoch: 3 | loss: 0.0152168\n",
      "\tspeed: 0.0336s/iter; left time: 531.5048s\n",
      "\titers: 600, epoch: 3 | loss: 0.0130471\n",
      "\tspeed: 0.0318s/iter; left time: 499.9435s\n",
      "\titers: 700, epoch: 3 | loss: 0.0149760\n",
      "\tspeed: 0.0327s/iter; left time: 510.8295s\n",
      "\titers: 800, epoch: 3 | loss: 0.0108637\n",
      "\tspeed: 0.0325s/iter; left time: 503.8202s\n",
      "\titers: 900, epoch: 3 | loss: 0.0151149\n",
      "\tspeed: 0.0329s/iter; left time: 506.5198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:30.45s\n",
      "Steps: 906 | Train Loss: 0.0149118 Vali Loss: 0.0218676 Test Loss: 0.0311270\n",
      "Validation loss decreased (0.022008 --> 0.021868).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0119106\n",
      "\tspeed: 0.1149s/iter; left time: 1758.8284s\n",
      "\titers: 200, epoch: 4 | loss: 0.0157133\n",
      "\tspeed: 0.0332s/iter; left time: 505.2567s\n",
      "\titers: 300, epoch: 4 | loss: 0.0150046\n",
      "\tspeed: 0.0327s/iter; left time: 493.1885s\n",
      "\titers: 400, epoch: 4 | loss: 0.0168810\n",
      "\tspeed: 0.0328s/iter; left time: 492.3720s\n",
      "\titers: 500, epoch: 4 | loss: 0.0203729\n",
      "\tspeed: 0.0368s/iter; left time: 547.9523s\n",
      "\titers: 600, epoch: 4 | loss: 0.0154422\n",
      "\tspeed: 0.0347s/iter; left time: 513.6878s\n",
      "\titers: 700, epoch: 4 | loss: 0.0118690\n",
      "\tspeed: 0.0328s/iter; left time: 481.6657s\n",
      "\titers: 800, epoch: 4 | loss: 0.0124499\n",
      "\tspeed: 0.0328s/iter; left time: 479.6343s\n",
      "\titers: 900, epoch: 4 | loss: 0.0121385\n",
      "\tspeed: 0.0330s/iter; left time: 478.8186s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:30.91s\n",
      "Steps: 906 | Train Loss: 0.0138662 Vali Loss: 0.0221879 Test Loss: 0.0294972\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0200112\n",
      "\tspeed: 0.1002s/iter; left time: 1442.6273s\n",
      "\titers: 200, epoch: 5 | loss: 0.0109799\n",
      "\tspeed: 0.0334s/iter; left time: 476.8437s\n",
      "\titers: 300, epoch: 5 | loss: 0.0117807\n",
      "\tspeed: 0.0333s/iter; left time: 472.6238s\n",
      "\titers: 400, epoch: 5 | loss: 0.0172931\n",
      "\tspeed: 0.0329s/iter; left time: 464.0789s\n",
      "\titers: 500, epoch: 5 | loss: 0.0109414\n",
      "\tspeed: 0.0317s/iter; left time: 443.0067s\n",
      "\titers: 600, epoch: 5 | loss: 0.0122645\n",
      "\tspeed: 0.0343s/iter; left time: 476.0033s\n",
      "\titers: 700, epoch: 5 | loss: 0.0151734\n",
      "\tspeed: 0.0378s/iter; left time: 522.1190s\n",
      "\titers: 800, epoch: 5 | loss: 0.0145623\n",
      "\tspeed: 0.0339s/iter; left time: 464.9881s\n",
      "\titers: 900, epoch: 5 | loss: 0.0112677\n",
      "\tspeed: 0.0324s/iter; left time: 441.1428s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:31.08s\n",
      "Steps: 906 | Train Loss: 0.0130095 Vali Loss: 0.0240887 Test Loss: 0.0324934\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0120146\n",
      "\tspeed: 0.0978s/iter; left time: 1319.7265s\n",
      "\titers: 200, epoch: 6 | loss: 0.0134536\n",
      "\tspeed: 0.0364s/iter; left time: 487.1294s\n",
      "\titers: 300, epoch: 6 | loss: 0.0109227\n",
      "\tspeed: 0.0348s/iter; left time: 462.4457s\n",
      "\titers: 400, epoch: 6 | loss: 0.0137217\n",
      "\tspeed: 0.0324s/iter; left time: 427.9542s\n",
      "\titers: 500, epoch: 6 | loss: 0.0131301\n",
      "\tspeed: 0.0335s/iter; left time: 438.4563s\n",
      "\titers: 600, epoch: 6 | loss: 0.0103798\n",
      "\tspeed: 0.0325s/iter; left time: 422.7400s\n",
      "\titers: 700, epoch: 6 | loss: 0.0114151\n",
      "\tspeed: 0.0341s/iter; left time: 439.9907s\n",
      "\titers: 800, epoch: 6 | loss: 0.0096706\n",
      "\tspeed: 0.0355s/iter; left time: 453.7663s\n",
      "\titers: 900, epoch: 6 | loss: 0.0158617\n",
      "\tspeed: 0.0332s/iter; left time: 421.6472s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:31.19s\n",
      "Steps: 906 | Train Loss: 0.0120912 Vali Loss: 0.0258931 Test Loss: 0.0350914\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0107834\n",
      "\tspeed: 0.0948s/iter; left time: 1193.5660s\n",
      "\titers: 200, epoch: 7 | loss: 0.0110156\n",
      "\tspeed: 0.0343s/iter; left time: 427.8659s\n",
      "\titers: 300, epoch: 7 | loss: 0.0107683\n",
      "\tspeed: 0.0339s/iter; left time: 420.1432s\n",
      "\titers: 400, epoch: 7 | loss: 0.0116925\n",
      "\tspeed: 0.0339s/iter; left time: 416.6181s\n",
      "\titers: 500, epoch: 7 | loss: 0.0104368\n",
      "\tspeed: 0.0335s/iter; left time: 408.6290s\n",
      "\titers: 600, epoch: 7 | loss: 0.0069574\n",
      "\tspeed: 0.0320s/iter; left time: 386.8675s\n",
      "\titers: 700, epoch: 7 | loss: 0.0106184\n",
      "\tspeed: 0.0337s/iter; left time: 403.7701s\n",
      "\titers: 800, epoch: 7 | loss: 0.0090532\n",
      "\tspeed: 0.0395s/iter; left time: 468.8837s\n",
      "\titers: 900, epoch: 7 | loss: 0.0105725\n",
      "\tspeed: 0.0353s/iter; left time: 416.0763s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:31.55s\n",
      "Steps: 906 | Train Loss: 0.0112189 Vali Loss: 0.0227329 Test Loss: 0.0320340\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0106522\n",
      "\tspeed: 0.0991s/iter; left time: 1157.1982s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081547\n",
      "\tspeed: 0.0356s/iter; left time: 412.3166s\n",
      "\titers: 300, epoch: 8 | loss: 0.0101518\n",
      "\tspeed: 0.0375s/iter; left time: 430.7536s\n",
      "\titers: 400, epoch: 8 | loss: 0.0103827\n",
      "\tspeed: 0.0348s/iter; left time: 395.7110s\n",
      "\titers: 500, epoch: 8 | loss: 0.0097602\n",
      "\tspeed: 0.0319s/iter; left time: 359.5612s\n",
      "\titers: 600, epoch: 8 | loss: 0.0106588\n",
      "\tspeed: 0.0348s/iter; left time: 388.9154s\n",
      "\titers: 700, epoch: 8 | loss: 0.0106670\n",
      "\tspeed: 0.0316s/iter; left time: 350.3074s\n",
      "\titers: 800, epoch: 8 | loss: 0.0084209\n",
      "\tspeed: 0.0306s/iter; left time: 335.8359s\n",
      "\titers: 900, epoch: 8 | loss: 0.0084942\n",
      "\tspeed: 0.0342s/iter; left time: 372.2045s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:31.14s\n",
      "Steps: 906 | Train Loss: 0.0102566 Vali Loss: 0.0264436 Test Loss: 0.0365799\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.031130269169807434, rmse:0.17643772065639496, mae:0.12031001597642899, rse:0.6083531379699707\n",
      "Intermediate time for GB and pred_len 24: 00h:05m:07.85s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_96_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0893760\n",
      "\tspeed: 0.0890s/iter; left time: 1600.4652s\n",
      "\titers: 200, epoch: 1 | loss: 0.0699951\n",
      "\tspeed: 0.0353s/iter; left time: 631.7274s\n",
      "\titers: 300, epoch: 1 | loss: 0.0674433\n",
      "\tspeed: 0.0370s/iter; left time: 657.6954s\n",
      "\titers: 400, epoch: 1 | loss: 0.0645395\n",
      "\tspeed: 0.0344s/iter; left time: 608.3971s\n",
      "\titers: 500, epoch: 1 | loss: 0.0576673\n",
      "\tspeed: 0.0359s/iter; left time: 631.2129s\n",
      "\titers: 600, epoch: 1 | loss: 0.0536212\n",
      "\tspeed: 0.0369s/iter; left time: 644.6308s\n",
      "\titers: 700, epoch: 1 | loss: 0.0497722\n",
      "\tspeed: 0.0355s/iter; left time: 616.4758s\n",
      "\titers: 800, epoch: 1 | loss: 0.0523500\n",
      "\tspeed: 0.0353s/iter; left time: 610.4711s\n",
      "\titers: 900, epoch: 1 | loss: 0.0492230\n",
      "\tspeed: 0.0374s/iter; left time: 642.6733s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:33.68s\n",
      "Steps: 904 | Train Loss: 0.0649915 Vali Loss: 0.0547138 Test Loss: 0.0850027\n",
      "Validation loss decreased (inf --> 0.054714).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0341445\n",
      "\tspeed: 0.1066s/iter; left time: 1821.2269s\n",
      "\titers: 200, epoch: 2 | loss: 0.0328115\n",
      "\tspeed: 0.0363s/iter; left time: 617.0372s\n",
      "\titers: 300, epoch: 2 | loss: 0.0280633\n",
      "\tspeed: 0.0364s/iter; left time: 614.5451s\n",
      "\titers: 400, epoch: 2 | loss: 0.0278533\n",
      "\tspeed: 0.0356s/iter; left time: 597.7265s\n",
      "\titers: 500, epoch: 2 | loss: 0.0259066\n",
      "\tspeed: 0.0400s/iter; left time: 666.6249s\n",
      "\titers: 600, epoch: 2 | loss: 0.0295652\n",
      "\tspeed: 0.0410s/iter; left time: 678.9570s\n",
      "\titers: 700, epoch: 2 | loss: 0.0231918\n",
      "\tspeed: 0.0354s/iter; left time: 584.1065s\n",
      "\titers: 800, epoch: 2 | loss: 0.0256015\n",
      "\tspeed: 0.0354s/iter; left time: 579.1183s\n",
      "\titers: 900, epoch: 2 | loss: 0.0304700\n",
      "\tspeed: 0.0352s/iter; left time: 573.0807s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:33.59s\n",
      "Steps: 904 | Train Loss: 0.0301226 Vali Loss: 0.0388117 Test Loss: 0.0579192\n",
      "Validation loss decreased (0.054714 --> 0.038812).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0234212\n",
      "\tspeed: 0.1133s/iter; left time: 1833.1627s\n",
      "\titers: 200, epoch: 3 | loss: 0.0262930\n",
      "\tspeed: 0.0370s/iter; left time: 594.2860s\n",
      "\titers: 300, epoch: 3 | loss: 0.0239697\n",
      "\tspeed: 0.0393s/iter; left time: 627.1740s\n",
      "\titers: 400, epoch: 3 | loss: 0.0237850\n",
      "\tspeed: 0.0374s/iter; left time: 593.8077s\n",
      "\titers: 500, epoch: 3 | loss: 0.0224860\n",
      "\tspeed: 0.0362s/iter; left time: 570.2314s\n",
      "\titers: 600, epoch: 3 | loss: 0.0251826\n",
      "\tspeed: 0.0397s/iter; left time: 622.6229s\n",
      "\titers: 700, epoch: 3 | loss: 0.0222447\n",
      "\tspeed: 0.0379s/iter; left time: 589.9097s\n",
      "\titers: 800, epoch: 3 | loss: 0.0206792\n",
      "\tspeed: 0.0352s/iter; left time: 544.3801s\n",
      "\titers: 900, epoch: 3 | loss: 0.0221146\n",
      "\tspeed: 0.0379s/iter; left time: 581.9649s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:34.18s\n",
      "Steps: 904 | Train Loss: 0.0232364 Vali Loss: 0.0331869 Test Loss: 0.0506150\n",
      "Validation loss decreased (0.038812 --> 0.033187).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0239859\n",
      "\tspeed: 0.1201s/iter; left time: 1833.1005s\n",
      "\titers: 200, epoch: 4 | loss: 0.0222580\n",
      "\tspeed: 0.0373s/iter; left time: 565.4085s\n",
      "\titers: 300, epoch: 4 | loss: 0.0253674\n",
      "\tspeed: 0.0373s/iter; left time: 562.4477s\n",
      "\titers: 400, epoch: 4 | loss: 0.0194161\n",
      "\tspeed: 0.0419s/iter; left time: 626.6843s\n",
      "\titers: 500, epoch: 4 | loss: 0.0188118\n",
      "\tspeed: 0.0406s/iter; left time: 603.6536s\n",
      "\titers: 600, epoch: 4 | loss: 0.0208789\n",
      "\tspeed: 0.0347s/iter; left time: 511.8923s\n",
      "\titers: 700, epoch: 4 | loss: 0.0178792\n",
      "\tspeed: 0.0341s/iter; left time: 500.1203s\n",
      "\titers: 800, epoch: 4 | loss: 0.0212071\n",
      "\tspeed: 0.0384s/iter; left time: 559.4364s\n",
      "\titers: 900, epoch: 4 | loss: 0.0216732\n",
      "\tspeed: 0.0377s/iter; left time: 545.2422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:34.57s\n",
      "Steps: 904 | Train Loss: 0.0205480 Vali Loss: 0.0346284 Test Loss: 0.0530353\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0180327\n",
      "\tspeed: 0.1114s/iter; left time: 1599.6383s\n",
      "\titers: 200, epoch: 5 | loss: 0.0200579\n",
      "\tspeed: 0.0361s/iter; left time: 515.0310s\n",
      "\titers: 300, epoch: 5 | loss: 0.0168916\n",
      "\tspeed: 0.0371s/iter; left time: 525.8882s\n",
      "\titers: 400, epoch: 5 | loss: 0.0205630\n",
      "\tspeed: 0.0366s/iter; left time: 514.1776s\n",
      "\titers: 500, epoch: 5 | loss: 0.0182274\n",
      "\tspeed: 0.0369s/iter; left time: 515.0482s\n",
      "\titers: 600, epoch: 5 | loss: 0.0207174\n",
      "\tspeed: 0.0364s/iter; left time: 504.4256s\n",
      "\titers: 700, epoch: 5 | loss: 0.0234901\n",
      "\tspeed: 0.0371s/iter; left time: 510.9034s\n",
      "\titers: 800, epoch: 5 | loss: 0.0209549\n",
      "\tspeed: 0.0372s/iter; left time: 508.4176s\n",
      "\titers: 900, epoch: 5 | loss: 0.0205995\n",
      "\tspeed: 0.0345s/iter; left time: 468.4577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:33.47s\n",
      "Steps: 904 | Train Loss: 0.0186726 Vali Loss: 0.0334696 Test Loss: 0.0521068\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0173234\n",
      "\tspeed: 0.1222s/iter; left time: 1644.4529s\n",
      "\titers: 200, epoch: 6 | loss: 0.0156628\n",
      "\tspeed: 0.0343s/iter; left time: 458.0558s\n",
      "\titers: 300, epoch: 6 | loss: 0.0159969\n",
      "\tspeed: 0.0371s/iter; left time: 491.6261s\n",
      "\titers: 400, epoch: 6 | loss: 0.0178492\n",
      "\tspeed: 0.0379s/iter; left time: 498.2209s\n",
      "\titers: 500, epoch: 6 | loss: 0.0150308\n",
      "\tspeed: 0.0372s/iter; left time: 486.0870s\n",
      "\titers: 600, epoch: 6 | loss: 0.0172063\n",
      "\tspeed: 0.0357s/iter; left time: 463.3490s\n",
      "\titers: 700, epoch: 6 | loss: 0.0152361\n",
      "\tspeed: 0.0389s/iter; left time: 500.8529s\n",
      "\titers: 800, epoch: 6 | loss: 0.0163435\n",
      "\tspeed: 0.0356s/iter; left time: 454.7557s\n",
      "\titers: 900, epoch: 6 | loss: 0.0171424\n",
      "\tspeed: 0.0365s/iter; left time: 461.8533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:34.25s\n",
      "Steps: 904 | Train Loss: 0.0166152 Vali Loss: 0.0380499 Test Loss: 0.0581647\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0139970\n",
      "\tspeed: 0.1110s/iter; left time: 1393.2783s\n",
      "\titers: 200, epoch: 7 | loss: 0.0148535\n",
      "\tspeed: 0.0354s/iter; left time: 441.3687s\n",
      "\titers: 300, epoch: 7 | loss: 0.0137906\n",
      "\tspeed: 0.0384s/iter; left time: 474.0033s\n",
      "\titers: 400, epoch: 7 | loss: 0.0162590\n",
      "\tspeed: 0.0375s/iter; left time: 460.1537s\n",
      "\titers: 500, epoch: 7 | loss: 0.0165481\n",
      "\tspeed: 0.0358s/iter; left time: 434.7658s\n",
      "\titers: 600, epoch: 7 | loss: 0.0164570\n",
      "\tspeed: 0.0358s/iter; left time: 432.0143s\n",
      "\titers: 700, epoch: 7 | loss: 0.0176711\n",
      "\tspeed: 0.0408s/iter; left time: 487.8500s\n",
      "\titers: 800, epoch: 7 | loss: 0.0125647\n",
      "\tspeed: 0.0395s/iter; left time: 467.8862s\n",
      "\titers: 900, epoch: 7 | loss: 0.0137664\n",
      "\tspeed: 0.0377s/iter; left time: 443.7280s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.39s\n",
      "Steps: 904 | Train Loss: 0.0148866 Vali Loss: 0.0356738 Test Loss: 0.0604467\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0133834\n",
      "\tspeed: 0.1097s/iter; left time: 1277.9865s\n",
      "\titers: 200, epoch: 8 | loss: 0.0131127\n",
      "\tspeed: 0.0397s/iter; left time: 459.0377s\n",
      "\titers: 300, epoch: 8 | loss: 0.0137572\n",
      "\tspeed: 0.0386s/iter; left time: 441.5802s\n",
      "\titers: 400, epoch: 8 | loss: 0.0119892\n",
      "\tspeed: 0.0371s/iter; left time: 420.7596s\n",
      "\titers: 500, epoch: 8 | loss: 0.0147413\n",
      "\tspeed: 0.0351s/iter; left time: 394.5534s\n",
      "\titers: 600, epoch: 8 | loss: 0.0135280\n",
      "\tspeed: 0.0399s/iter; left time: 445.4019s\n",
      "\titers: 700, epoch: 8 | loss: 0.0128642\n",
      "\tspeed: 0.0363s/iter; left time: 400.7349s\n",
      "\titers: 800, epoch: 8 | loss: 0.0143640\n",
      "\tspeed: 0.0351s/iter; left time: 384.6032s\n",
      "\titers: 900, epoch: 8 | loss: 0.0136676\n",
      "\tspeed: 0.0356s/iter; left time: 385.9814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.00s\n",
      "Steps: 904 | Train Loss: 0.0132417 Vali Loss: 0.0376644 Test Loss: 0.0605138\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.05061672627925873, rmse:0.22498160600662231, mae:0.1600819081068039, rse:0.7780177593231201\n",
      "Intermediate time for GB and pred_len 96: 00h:05m:46.56s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_168_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0828232\n",
      "\tspeed: 0.0940s/iter; left time: 1687.0031s\n",
      "\titers: 200, epoch: 1 | loss: 0.0749127\n",
      "\tspeed: 0.0583s/iter; left time: 1039.7827s\n",
      "\titers: 300, epoch: 1 | loss: 0.0620970\n",
      "\tspeed: 0.0765s/iter; left time: 1356.7755s\n",
      "\titers: 400, epoch: 1 | loss: 0.0595553\n",
      "\tspeed: 0.0761s/iter; left time: 1343.2923s\n",
      "\titers: 500, epoch: 1 | loss: 0.0616825\n",
      "\tspeed: 0.0609s/iter; left time: 1067.5064s\n",
      "\titers: 600, epoch: 1 | loss: 0.0584624\n",
      "\tspeed: 0.0646s/iter; left time: 1126.7998s\n",
      "\titers: 700, epoch: 1 | loss: 0.0525494\n",
      "\tspeed: 0.0737s/iter; left time: 1278.3715s\n",
      "\titers: 800, epoch: 1 | loss: 0.0509944\n",
      "\tspeed: 0.0730s/iter; left time: 1257.7551s\n",
      "\titers: 900, epoch: 1 | loss: 0.0496514\n",
      "\tspeed: 0.0648s/iter; left time: 1110.3270s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:01m:02.92s\n",
      "Steps: 902 | Train Loss: 0.0657342 Vali Loss: 0.0569065 Test Loss: 0.0867981\n",
      "Validation loss decreased (inf --> 0.056906).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0415353\n",
      "\tspeed: 0.1859s/iter; left time: 3167.8152s\n",
      "\titers: 200, epoch: 2 | loss: 0.0353375\n",
      "\tspeed: 0.0495s/iter; left time: 838.0543s\n",
      "\titers: 300, epoch: 2 | loss: 0.0351734\n",
      "\tspeed: 0.0619s/iter; left time: 1042.4786s\n",
      "\titers: 400, epoch: 2 | loss: 0.0284817\n",
      "\tspeed: 0.0595s/iter; left time: 996.4829s\n",
      "\titers: 500, epoch: 2 | loss: 0.0283058\n",
      "\tspeed: 0.0610s/iter; left time: 1014.1560s\n",
      "\titers: 600, epoch: 2 | loss: 0.0289276\n",
      "\tspeed: 0.0709s/iter; left time: 1172.4229s\n",
      "\titers: 700, epoch: 2 | loss: 0.0289890\n",
      "\tspeed: 0.0597s/iter; left time: 981.7381s\n",
      "\titers: 800, epoch: 2 | loss: 0.0284246\n",
      "\tspeed: 0.0603s/iter; left time: 985.5887s\n",
      "\titers: 900, epoch: 2 | loss: 0.0308674\n",
      "\tspeed: 0.0676s/iter; left time: 1097.9624s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:54.22s\n",
      "Steps: 902 | Train Loss: 0.0330005 Vali Loss: 0.0403766 Test Loss: 0.0631403\n",
      "Validation loss decreased (0.056906 --> 0.040377).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0281291\n",
      "\tspeed: 0.2374s/iter; left time: 3831.2092s\n",
      "\titers: 200, epoch: 3 | loss: 0.0253809\n",
      "\tspeed: 0.0672s/iter; left time: 1077.3620s\n",
      "\titers: 300, epoch: 3 | loss: 0.0304348\n",
      "\tspeed: 0.0613s/iter; left time: 977.0379s\n",
      "\titers: 400, epoch: 3 | loss: 0.0270059\n",
      "\tspeed: 0.0652s/iter; left time: 1032.7159s\n",
      "\titers: 500, epoch: 3 | loss: 0.0259681\n",
      "\tspeed: 0.0676s/iter; left time: 1063.8254s\n",
      "\titers: 600, epoch: 3 | loss: 0.0313333\n",
      "\tspeed: 0.0629s/iter; left time: 983.2662s\n",
      "\titers: 700, epoch: 3 | loss: 0.0257676\n",
      "\tspeed: 0.0581s/iter; left time: 903.0328s\n",
      "\titers: 800, epoch: 3 | loss: 0.0252682\n",
      "\tspeed: 0.0747s/iter; left time: 1153.7661s\n",
      "\titers: 900, epoch: 3 | loss: 0.0248985\n",
      "\tspeed: 0.0580s/iter; left time: 889.6014s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:58.65s\n",
      "Steps: 902 | Train Loss: 0.0268744 Vali Loss: 0.0439434 Test Loss: 0.0653365\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0279412\n",
      "\tspeed: 0.2363s/iter; left time: 3599.4929s\n",
      "\titers: 200, epoch: 4 | loss: 0.0242079\n",
      "\tspeed: 0.0591s/iter; left time: 894.1670s\n",
      "\titers: 300, epoch: 4 | loss: 0.0269835\n",
      "\tspeed: 0.0691s/iter; left time: 1039.1803s\n",
      "\titers: 400, epoch: 4 | loss: 0.0255507\n",
      "\tspeed: 0.0643s/iter; left time: 959.9342s\n",
      "\titers: 500, epoch: 4 | loss: 0.0225070\n",
      "\tspeed: 0.0615s/iter; left time: 911.7824s\n",
      "\titers: 600, epoch: 4 | loss: 0.0230421\n",
      "\tspeed: 0.0593s/iter; left time: 873.1800s\n",
      "\titers: 700, epoch: 4 | loss: 0.0248679\n",
      "\tspeed: 0.0755s/iter; left time: 1105.1853s\n",
      "\titers: 800, epoch: 4 | loss: 0.0225724\n",
      "\tspeed: 0.0589s/iter; left time: 855.9364s\n",
      "\titers: 900, epoch: 4 | loss: 0.0205266\n",
      "\tspeed: 0.0608s/iter; left time: 878.2315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:57.81s\n",
      "Steps: 902 | Train Loss: 0.0245145 Vali Loss: 0.0396597 Test Loss: 0.0621735\n",
      "Validation loss decreased (0.040377 --> 0.039660).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0214768\n",
      "\tspeed: 0.2413s/iter; left time: 3458.7244s\n",
      "\titers: 200, epoch: 5 | loss: 0.0234881\n",
      "\tspeed: 0.0667s/iter; left time: 948.9383s\n",
      "\titers: 300, epoch: 5 | loss: 0.0215285\n",
      "\tspeed: 0.0667s/iter; left time: 942.9494s\n",
      "\titers: 400, epoch: 5 | loss: 0.0217437\n",
      "\tspeed: 0.0488s/iter; left time: 684.8049s\n",
      "\titers: 500, epoch: 5 | loss: 0.0214513\n",
      "\tspeed: 0.0480s/iter; left time: 669.0579s\n",
      "\titers: 600, epoch: 5 | loss: 0.0207977\n",
      "\tspeed: 0.0536s/iter; left time: 741.1752s\n",
      "\titers: 700, epoch: 5 | loss: 0.0178466\n",
      "\tspeed: 0.0635s/iter; left time: 872.1859s\n",
      "\titers: 800, epoch: 5 | loss: 0.0219532\n",
      "\tspeed: 0.0597s/iter; left time: 814.2188s\n",
      "\titers: 900, epoch: 5 | loss: 0.0201745\n",
      "\tspeed: 0.0626s/iter; left time: 846.9751s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:53.43s\n",
      "Steps: 902 | Train Loss: 0.0214388 Vali Loss: 0.0403047 Test Loss: 0.0659283\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0192441\n",
      "\tspeed: 0.2269s/iter; left time: 3047.5116s\n",
      "\titers: 200, epoch: 6 | loss: 0.0194779\n",
      "\tspeed: 0.0598s/iter; left time: 796.6182s\n",
      "\titers: 300, epoch: 6 | loss: 0.0164167\n",
      "\tspeed: 0.0629s/iter; left time: 832.8238s\n",
      "\titers: 400, epoch: 6 | loss: 0.0161506\n",
      "\tspeed: 0.0628s/iter; left time: 824.2074s\n",
      "\titers: 500, epoch: 6 | loss: 0.0196075\n",
      "\tspeed: 0.0609s/iter; left time: 793.4357s\n",
      "\titers: 600, epoch: 6 | loss: 0.0166038\n",
      "\tspeed: 0.0624s/iter; left time: 806.8177s\n",
      "\titers: 700, epoch: 6 | loss: 0.0136157\n",
      "\tspeed: 0.0640s/iter; left time: 820.9817s\n",
      "\titers: 800, epoch: 6 | loss: 0.0160816\n",
      "\tspeed: 0.0600s/iter; left time: 763.5863s\n",
      "\titers: 900, epoch: 6 | loss: 0.0160592\n",
      "\tspeed: 0.0590s/iter; left time: 744.6866s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:55.54s\n",
      "Steps: 902 | Train Loss: 0.0170734 Vali Loss: 0.0388602 Test Loss: 0.0664243\n",
      "Validation loss decreased (0.039660 --> 0.038860).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0169348\n",
      "\tspeed: 0.2222s/iter; left time: 2784.4466s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150197\n",
      "\tspeed: 0.0596s/iter; left time: 741.0774s\n",
      "\titers: 300, epoch: 7 | loss: 0.0177164\n",
      "\tspeed: 0.0533s/iter; left time: 656.8524s\n",
      "\titers: 400, epoch: 7 | loss: 0.0144806\n",
      "\tspeed: 0.0598s/iter; left time: 731.5548s\n",
      "\titers: 500, epoch: 7 | loss: 0.0147240\n",
      "\tspeed: 0.0590s/iter; left time: 716.1321s\n",
      "\titers: 600, epoch: 7 | loss: 0.0131048\n",
      "\tspeed: 0.0551s/iter; left time: 662.9405s\n",
      "\titers: 700, epoch: 7 | loss: 0.0154541\n",
      "\tspeed: 0.0537s/iter; left time: 641.0155s\n",
      "\titers: 800, epoch: 7 | loss: 0.0132857\n",
      "\tspeed: 0.0591s/iter; left time: 699.3555s\n",
      "\titers: 900, epoch: 7 | loss: 0.0172589\n",
      "\tspeed: 0.0562s/iter; left time: 659.3353s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:51.92s\n",
      "Steps: 902 | Train Loss: 0.0149902 Vali Loss: 0.0377542 Test Loss: 0.0672273\n",
      "Validation loss decreased (0.038860 --> 0.037754).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0124112\n",
      "\tspeed: 0.2127s/iter; left time: 2472.9848s\n",
      "\titers: 200, epoch: 8 | loss: 0.0133382\n",
      "\tspeed: 0.0589s/iter; left time: 678.5519s\n",
      "\titers: 300, epoch: 8 | loss: 0.0144903\n",
      "\tspeed: 0.0543s/iter; left time: 620.8336s\n",
      "\titers: 400, epoch: 8 | loss: 0.0129757\n",
      "\tspeed: 0.0560s/iter; left time: 633.8268s\n",
      "\titers: 500, epoch: 8 | loss: 0.0129781\n",
      "\tspeed: 0.0501s/iter; left time: 562.5969s\n",
      "\titers: 600, epoch: 8 | loss: 0.0131203\n",
      "\tspeed: 0.0585s/iter; left time: 650.8065s\n",
      "\titers: 700, epoch: 8 | loss: 0.0113418\n",
      "\tspeed: 0.0575s/iter; left time: 634.1865s\n",
      "\titers: 800, epoch: 8 | loss: 0.0117694\n",
      "\tspeed: 0.0530s/iter; left time: 578.9685s\n",
      "\titers: 900, epoch: 8 | loss: 0.0125589\n",
      "\tspeed: 0.0528s/iter; left time: 571.6851s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:49.93s\n",
      "Steps: 902 | Train Loss: 0.0136115 Vali Loss: 0.0394928 Test Loss: 0.0707811\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0127657\n",
      "\tspeed: 0.2052s/iter; left time: 2200.9914s\n",
      "\titers: 200, epoch: 9 | loss: 0.0124597\n",
      "\tspeed: 0.0584s/iter; left time: 620.5246s\n",
      "\titers: 300, epoch: 9 | loss: 0.0134299\n",
      "\tspeed: 0.0511s/iter; left time: 538.0723s\n",
      "\titers: 400, epoch: 9 | loss: 0.0110699\n",
      "\tspeed: 0.0564s/iter; left time: 588.4007s\n",
      "\titers: 500, epoch: 9 | loss: 0.0132655\n",
      "\tspeed: 0.0568s/iter; left time: 586.7083s\n",
      "\titers: 600, epoch: 9 | loss: 0.0112603\n",
      "\tspeed: 0.0544s/iter; left time: 556.6322s\n",
      "\titers: 700, epoch: 9 | loss: 0.0130516\n",
      "\tspeed: 0.0547s/iter; left time: 554.1436s\n",
      "\titers: 800, epoch: 9 | loss: 0.0126161\n",
      "\tspeed: 0.0520s/iter; left time: 521.2077s\n",
      "\titers: 900, epoch: 9 | loss: 0.0119859\n",
      "\tspeed: 0.0601s/iter; left time: 595.9974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:49.91s\n",
      "Steps: 902 | Train Loss: 0.0123658 Vali Loss: 0.0383595 Test Loss: 0.0662519\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0107819\n",
      "\tspeed: 0.2077s/iter; left time: 2040.0214s\n",
      "\titers: 200, epoch: 10 | loss: 0.0127241\n",
      "\tspeed: 0.0590s/iter; left time: 574.0561s\n",
      "\titers: 300, epoch: 10 | loss: 0.0123100\n",
      "\tspeed: 0.0543s/iter; left time: 522.4042s\n",
      "\titers: 400, epoch: 10 | loss: 0.0109170\n",
      "\tspeed: 0.0593s/iter; left time: 564.8281s\n",
      "\titers: 500, epoch: 10 | loss: 0.0114692\n",
      "\tspeed: 0.0560s/iter; left time: 527.7626s\n",
      "\titers: 600, epoch: 10 | loss: 0.0110846\n",
      "\tspeed: 0.0552s/iter; left time: 514.3282s\n",
      "\titers: 700, epoch: 10 | loss: 0.0117878\n",
      "\tspeed: 0.0591s/iter; left time: 545.1368s\n",
      "\titers: 800, epoch: 10 | loss: 0.0111483\n",
      "\tspeed: 0.0543s/iter; left time: 495.7798s\n",
      "\titers: 900, epoch: 10 | loss: 0.0121212\n",
      "\tspeed: 0.0466s/iter; left time: 420.1667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:50.50s\n",
      "Steps: 902 | Train Loss: 0.0112837 Vali Loss: 0.0393359 Test Loss: 0.0676974\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0106947\n",
      "\tspeed: 0.1719s/iter; left time: 1533.6289s\n",
      "\titers: 200, epoch: 11 | loss: 0.0114429\n",
      "\tspeed: 0.0659s/iter; left time: 581.0995s\n",
      "\titers: 300, epoch: 11 | loss: 0.0102906\n",
      "\tspeed: 0.0583s/iter; left time: 508.2840s\n",
      "\titers: 400, epoch: 11 | loss: 0.0102502\n",
      "\tspeed: 0.0627s/iter; left time: 540.9003s\n",
      "\titers: 500, epoch: 11 | loss: 0.0127098\n",
      "\tspeed: 0.0571s/iter; left time: 486.2570s\n",
      "\titers: 600, epoch: 11 | loss: 0.0100624\n",
      "\tspeed: 0.0602s/iter; left time: 506.6048s\n",
      "\titers: 700, epoch: 11 | loss: 0.0098379\n",
      "\tspeed: 0.0580s/iter; left time: 482.4003s\n",
      "\titers: 800, epoch: 11 | loss: 0.0095954\n",
      "\tspeed: 0.0608s/iter; left time: 500.2441s\n",
      "\titers: 900, epoch: 11 | loss: 0.0092578\n",
      "\tspeed: 0.0633s/iter; left time: 514.1071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:54.45s\n",
      "Steps: 902 | Train Loss: 0.0103650 Vali Loss: 0.0400552 Test Loss: 0.0669268\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0088414\n",
      "\tspeed: 0.2134s/iter; left time: 1711.1095s\n",
      "\titers: 200, epoch: 12 | loss: 0.0110274\n",
      "\tspeed: 0.0509s/iter; left time: 403.3595s\n",
      "\titers: 300, epoch: 12 | loss: 0.0086307\n",
      "\tspeed: 0.0469s/iter; left time: 366.9381s\n",
      "\titers: 400, epoch: 12 | loss: 0.0093599\n",
      "\tspeed: 0.0633s/iter; left time: 488.9164s\n",
      "\titers: 500, epoch: 12 | loss: 0.0102409\n",
      "\tspeed: 0.0534s/iter; left time: 407.1776s\n",
      "\titers: 600, epoch: 12 | loss: 0.0096047\n",
      "\tspeed: 0.0603s/iter; left time: 453.0206s\n",
      "\titers: 700, epoch: 12 | loss: 0.0099307\n",
      "\tspeed: 0.0706s/iter; left time: 524.1401s\n",
      "\titers: 800, epoch: 12 | loss: 0.0111330\n",
      "\tspeed: 0.0614s/iter; left time: 449.0882s\n",
      "\titers: 900, epoch: 12 | loss: 0.0091989\n",
      "\tspeed: 0.0565s/iter; left time: 407.9592s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:52.76s\n",
      "Steps: 902 | Train Loss: 0.0096406 Vali Loss: 0.0401528 Test Loss: 0.0665900\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.06714829802513123, rmse:0.2591298818588257, mae:0.18294008076190948, rse:0.8983698487281799\n",
      "Intermediate time for GB and pred_len 168: 00h:14m:09.79s\n",
      "Intermediate time for GB: 00h:25m:04.20s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_24_ES', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0857679\n",
      "\tspeed: 0.0440s/iter; left time: 792.6031s\n",
      "\titers: 200, epoch: 1 | loss: 0.0807853\n",
      "\tspeed: 0.0263s/iter; left time: 471.0709s\n",
      "\titers: 300, epoch: 1 | loss: 0.0553400\n",
      "\tspeed: 0.0292s/iter; left time: 520.9487s\n",
      "\titers: 400, epoch: 1 | loss: 0.0538047\n",
      "\tspeed: 0.0293s/iter; left time: 519.2219s\n",
      "\titers: 500, epoch: 1 | loss: 0.0488718\n",
      "\tspeed: 0.0275s/iter; left time: 483.8720s\n",
      "\titers: 600, epoch: 1 | loss: 0.0414990\n",
      "\tspeed: 0.0258s/iter; left time: 452.0399s\n",
      "\titers: 700, epoch: 1 | loss: 0.0401268\n",
      "\tspeed: 0.0286s/iter; left time: 499.0068s\n",
      "\titers: 800, epoch: 1 | loss: 0.0366052\n",
      "\tspeed: 0.0256s/iter; left time: 444.0711s\n",
      "\titers: 900, epoch: 1 | loss: 0.0330763\n",
      "\tspeed: 0.0278s/iter; left time: 479.4301s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:25.76s\n",
      "Steps: 906 | Train Loss: 0.0567425 Vali Loss: 0.0218313 Test Loss: 0.0343488\n",
      "Validation loss decreased (inf --> 0.021831).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0236104\n",
      "\tspeed: 0.0970s/iter; left time: 1660.0509s\n",
      "\titers: 200, epoch: 2 | loss: 0.0223391\n",
      "\tspeed: 0.0264s/iter; left time: 449.4381s\n",
      "\titers: 300, epoch: 2 | loss: 0.0169560\n",
      "\tspeed: 0.0258s/iter; left time: 435.7617s\n",
      "\titers: 400, epoch: 2 | loss: 0.0164908\n",
      "\tspeed: 0.0261s/iter; left time: 439.6817s\n",
      "\titers: 500, epoch: 2 | loss: 0.0154636\n",
      "\tspeed: 0.0275s/iter; left time: 459.9297s\n",
      "\titers: 600, epoch: 2 | loss: 0.0176749\n",
      "\tspeed: 0.0276s/iter; left time: 458.2161s\n",
      "\titers: 700, epoch: 2 | loss: 0.0140468\n",
      "\tspeed: 0.0282s/iter; left time: 465.8257s\n",
      "\titers: 800, epoch: 2 | loss: 0.0139205\n",
      "\tspeed: 0.0292s/iter; left time: 478.6710s\n",
      "\titers: 900, epoch: 2 | loss: 0.0108507\n",
      "\tspeed: 0.0286s/iter; left time: 466.0723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.24s\n",
      "Steps: 906 | Train Loss: 0.0172144 Vali Loss: 0.0128167 Test Loss: 0.0295072\n",
      "Validation loss decreased (0.021831 --> 0.012817).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0121302\n",
      "\tspeed: 0.0845s/iter; left time: 1370.4653s\n",
      "\titers: 200, epoch: 3 | loss: 0.0131790\n",
      "\tspeed: 0.0270s/iter; left time: 435.1248s\n",
      "\titers: 300, epoch: 3 | loss: 0.0119967\n",
      "\tspeed: 0.0269s/iter; left time: 430.3373s\n",
      "\titers: 400, epoch: 3 | loss: 0.0101148\n",
      "\tspeed: 0.0277s/iter; left time: 440.4781s\n",
      "\titers: 500, epoch: 3 | loss: 0.0126973\n",
      "\tspeed: 0.0275s/iter; left time: 434.4107s\n",
      "\titers: 600, epoch: 3 | loss: 0.0094017\n",
      "\tspeed: 0.0278s/iter; left time: 436.3351s\n",
      "\titers: 700, epoch: 3 | loss: 0.0109511\n",
      "\tspeed: 0.0273s/iter; left time: 426.7283s\n",
      "\titers: 800, epoch: 3 | loss: 0.0129451\n",
      "\tspeed: 0.0257s/iter; left time: 399.3274s\n",
      "\titers: 900, epoch: 3 | loss: 0.0106098\n",
      "\tspeed: 0.0269s/iter; left time: 414.4546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:24.96s\n",
      "Steps: 906 | Train Loss: 0.0110201 Vali Loss: 0.0114797 Test Loss: 0.0272940\n",
      "Validation loss decreased (0.012817 --> 0.011480).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0107610\n",
      "\tspeed: 0.0820s/iter; left time: 1255.4961s\n",
      "\titers: 200, epoch: 4 | loss: 0.0116187\n",
      "\tspeed: 0.0300s/iter; left time: 456.7480s\n",
      "\titers: 300, epoch: 4 | loss: 0.0090415\n",
      "\tspeed: 0.0267s/iter; left time: 403.5439s\n",
      "\titers: 400, epoch: 4 | loss: 0.0098316\n",
      "\tspeed: 0.0258s/iter; left time: 386.5591s\n",
      "\titers: 500, epoch: 4 | loss: 0.0075964\n",
      "\tspeed: 0.0281s/iter; left time: 418.6949s\n",
      "\titers: 600, epoch: 4 | loss: 0.0088572\n",
      "\tspeed: 0.0251s/iter; left time: 371.7086s\n",
      "\titers: 700, epoch: 4 | loss: 0.0112044\n",
      "\tspeed: 0.0268s/iter; left time: 394.2140s\n",
      "\titers: 800, epoch: 4 | loss: 0.0099376\n",
      "\tspeed: 0.0270s/iter; left time: 394.9372s\n",
      "\titers: 900, epoch: 4 | loss: 0.0107191\n",
      "\tspeed: 0.0285s/iter; left time: 412.9988s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:25.27s\n",
      "Steps: 906 | Train Loss: 0.0096495 Vali Loss: 0.0106362 Test Loss: 0.0251008\n",
      "Validation loss decreased (0.011480 --> 0.010636).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0096470\n",
      "\tspeed: 0.0909s/iter; left time: 1308.1322s\n",
      "\titers: 200, epoch: 5 | loss: 0.0094309\n",
      "\tspeed: 0.0268s/iter; left time: 382.9182s\n",
      "\titers: 300, epoch: 5 | loss: 0.0075118\n",
      "\tspeed: 0.0283s/iter; left time: 401.2549s\n",
      "\titers: 400, epoch: 5 | loss: 0.0097770\n",
      "\tspeed: 0.0263s/iter; left time: 370.2591s\n",
      "\titers: 500, epoch: 5 | loss: 0.0081016\n",
      "\tspeed: 0.0281s/iter; left time: 393.7292s\n",
      "\titers: 600, epoch: 5 | loss: 0.0088815\n",
      "\tspeed: 0.0308s/iter; left time: 427.9773s\n",
      "\titers: 700, epoch: 5 | loss: 0.0106172\n",
      "\tspeed: 0.0300s/iter; left time: 414.0815s\n",
      "\titers: 800, epoch: 5 | loss: 0.0077928\n",
      "\tspeed: 0.0264s/iter; left time: 361.5377s\n",
      "\titers: 900, epoch: 5 | loss: 0.0090509\n",
      "\tspeed: 0.0274s/iter; left time: 372.7753s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.65s\n",
      "Steps: 906 | Train Loss: 0.0087972 Vali Loss: 0.0097689 Test Loss: 0.0239744\n",
      "Validation loss decreased (0.010636 --> 0.009769).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0091501\n",
      "\tspeed: 0.0852s/iter; left time: 1148.8839s\n",
      "\titers: 200, epoch: 6 | loss: 0.0064490\n",
      "\tspeed: 0.0305s/iter; left time: 408.4833s\n",
      "\titers: 300, epoch: 6 | loss: 0.0077124\n",
      "\tspeed: 0.0330s/iter; left time: 439.1970s\n",
      "\titers: 400, epoch: 6 | loss: 0.0087460\n",
      "\tspeed: 0.0324s/iter; left time: 427.0396s\n",
      "\titers: 500, epoch: 6 | loss: 0.0080305\n",
      "\tspeed: 0.0325s/iter; left time: 425.0831s\n",
      "\titers: 600, epoch: 6 | loss: 0.0103331\n",
      "\tspeed: 0.0321s/iter; left time: 416.6597s\n",
      "\titers: 700, epoch: 6 | loss: 0.0072778\n",
      "\tspeed: 0.0320s/iter; left time: 412.6148s\n",
      "\titers: 800, epoch: 6 | loss: 0.0095548\n",
      "\tspeed: 0.0358s/iter; left time: 457.8767s\n",
      "\titers: 900, epoch: 6 | loss: 0.0093930\n",
      "\tspeed: 0.0359s/iter; left time: 455.9414s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:30.17s\n",
      "Steps: 906 | Train Loss: 0.0081579 Vali Loss: 0.0109167 Test Loss: 0.0256032\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0074642\n",
      "\tspeed: 0.0976s/iter; left time: 1227.8833s\n",
      "\titers: 200, epoch: 7 | loss: 0.0078414\n",
      "\tspeed: 0.0311s/iter; left time: 388.0548s\n",
      "\titers: 300, epoch: 7 | loss: 0.0074884\n",
      "\tspeed: 0.0335s/iter; left time: 415.2346s\n",
      "\titers: 400, epoch: 7 | loss: 0.0082921\n",
      "\tspeed: 0.0377s/iter; left time: 463.6036s\n",
      "\titers: 500, epoch: 7 | loss: 0.0081943\n",
      "\tspeed: 0.0353s/iter; left time: 430.4170s\n",
      "\titers: 600, epoch: 7 | loss: 0.0072786\n",
      "\tspeed: 0.0311s/iter; left time: 375.3918s\n",
      "\titers: 700, epoch: 7 | loss: 0.0069414\n",
      "\tspeed: 0.0315s/iter; left time: 377.3526s\n",
      "\titers: 800, epoch: 7 | loss: 0.0066637\n",
      "\tspeed: 0.0317s/iter; left time: 376.2685s\n",
      "\titers: 900, epoch: 7 | loss: 0.0079832\n",
      "\tspeed: 0.0321s/iter; left time: 378.8581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:30.41s\n",
      "Steps: 906 | Train Loss: 0.0076666 Vali Loss: 0.0096232 Test Loss: 0.0262587\n",
      "Validation loss decreased (0.009769 --> 0.009623).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0069152\n",
      "\tspeed: 0.1117s/iter; left time: 1305.1047s\n",
      "\titers: 200, epoch: 8 | loss: 0.0064987\n",
      "\tspeed: 0.0326s/iter; left time: 377.2986s\n",
      "\titers: 300, epoch: 8 | loss: 0.0070823\n",
      "\tspeed: 0.0325s/iter; left time: 372.9281s\n",
      "\titers: 400, epoch: 8 | loss: 0.0069094\n",
      "\tspeed: 0.0328s/iter; left time: 372.9548s\n",
      "\titers: 500, epoch: 8 | loss: 0.0070434\n",
      "\tspeed: 0.0330s/iter; left time: 372.3254s\n",
      "\titers: 600, epoch: 8 | loss: 0.0063723\n",
      "\tspeed: 0.0345s/iter; left time: 385.1761s\n",
      "\titers: 700, epoch: 8 | loss: 0.0084175\n",
      "\tspeed: 0.0319s/iter; left time: 353.6095s\n",
      "\titers: 800, epoch: 8 | loss: 0.0076559\n",
      "\tspeed: 0.0326s/iter; left time: 357.4149s\n",
      "\titers: 900, epoch: 8 | loss: 0.0077767\n",
      "\tspeed: 0.0335s/iter; left time: 364.8199s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:30.35s\n",
      "Steps: 906 | Train Loss: 0.0071775 Vali Loss: 0.0098634 Test Loss: 0.0328440\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0059583\n",
      "\tspeed: 0.0977s/iter; left time: 1052.2104s\n",
      "\titers: 200, epoch: 9 | loss: 0.0067932\n",
      "\tspeed: 0.0330s/iter; left time: 352.0814s\n",
      "\titers: 300, epoch: 9 | loss: 0.0063904\n",
      "\tspeed: 0.0331s/iter; left time: 349.9304s\n",
      "\titers: 400, epoch: 9 | loss: 0.0078322\n",
      "\tspeed: 0.0341s/iter; left time: 357.0860s\n",
      "\titers: 500, epoch: 9 | loss: 0.0069900\n",
      "\tspeed: 0.0316s/iter; left time: 327.8151s\n",
      "\titers: 600, epoch: 9 | loss: 0.0074752\n",
      "\tspeed: 0.0308s/iter; left time: 315.9255s\n",
      "\titers: 700, epoch: 9 | loss: 0.0064412\n",
      "\tspeed: 0.0308s/iter; left time: 312.9351s\n",
      "\titers: 800, epoch: 9 | loss: 0.0069325\n",
      "\tspeed: 0.0338s/iter; left time: 340.1513s\n",
      "\titers: 900, epoch: 9 | loss: 0.0047393\n",
      "\tspeed: 0.0335s/iter; left time: 333.9166s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:30.14s\n",
      "Steps: 906 | Train Loss: 0.0067945 Vali Loss: 0.0092748 Test Loss: 0.0247219\n",
      "Validation loss decreased (0.009623 --> 0.009275).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0068811\n",
      "\tspeed: 0.0990s/iter; left time: 976.8156s\n",
      "\titers: 200, epoch: 10 | loss: 0.0062535\n",
      "\tspeed: 0.0352s/iter; left time: 343.5092s\n",
      "\titers: 300, epoch: 10 | loss: 0.0066703\n",
      "\tspeed: 0.0371s/iter; left time: 358.5688s\n",
      "\titers: 400, epoch: 10 | loss: 0.0085733\n",
      "\tspeed: 0.0336s/iter; left time: 321.7765s\n",
      "\titers: 500, epoch: 10 | loss: 0.0066388\n",
      "\tspeed: 0.0311s/iter; left time: 294.8764s\n",
      "\titers: 600, epoch: 10 | loss: 0.0059668\n",
      "\tspeed: 0.0322s/iter; left time: 301.6838s\n",
      "\titers: 700, epoch: 10 | loss: 0.0079732\n",
      "\tspeed: 0.0337s/iter; left time: 311.8872s\n",
      "\titers: 800, epoch: 10 | loss: 0.0046471\n",
      "\tspeed: 0.0347s/iter; left time: 317.9309s\n",
      "\titers: 900, epoch: 10 | loss: 0.0063734\n",
      "\tspeed: 0.0345s/iter; left time: 312.3733s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:31.26s\n",
      "Steps: 906 | Train Loss: 0.0063905 Vali Loss: 0.0091948 Test Loss: 0.0303363\n",
      "Validation loss decreased (0.009275 --> 0.009195).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0062151\n",
      "\tspeed: 0.1037s/iter; left time: 928.8390s\n",
      "\titers: 200, epoch: 11 | loss: 0.0057975\n",
      "\tspeed: 0.0353s/iter; left time: 313.0500s\n",
      "\titers: 300, epoch: 11 | loss: 0.0060051\n",
      "\tspeed: 0.0332s/iter; left time: 291.0262s\n",
      "\titers: 400, epoch: 11 | loss: 0.0057208\n",
      "\tspeed: 0.0342s/iter; left time: 296.2042s\n",
      "\titers: 500, epoch: 11 | loss: 0.0063676\n",
      "\tspeed: 0.0336s/iter; left time: 287.3652s\n",
      "\titers: 600, epoch: 11 | loss: 0.0069670\n",
      "\tspeed: 0.0345s/iter; left time: 291.6189s\n",
      "\titers: 700, epoch: 11 | loss: 0.0049594\n",
      "\tspeed: 0.0331s/iter; left time: 277.0729s\n",
      "\titers: 800, epoch: 11 | loss: 0.0060550\n",
      "\tspeed: 0.0323s/iter; left time: 267.2256s\n",
      "\titers: 900, epoch: 11 | loss: 0.0063580\n",
      "\tspeed: 0.0348s/iter; left time: 283.9509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:31.00s\n",
      "Steps: 906 | Train Loss: 0.0061548 Vali Loss: 0.0094559 Test Loss: 0.0314645\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0077157\n",
      "\tspeed: 0.1027s/iter; left time: 827.3844s\n",
      "\titers: 200, epoch: 12 | loss: 0.0057735\n",
      "\tspeed: 0.0362s/iter; left time: 287.8489s\n",
      "\titers: 300, epoch: 12 | loss: 0.0053997\n",
      "\tspeed: 0.0408s/iter; left time: 320.3952s\n",
      "\titers: 400, epoch: 12 | loss: 0.0082485\n",
      "\tspeed: 0.0447s/iter; left time: 346.9352s\n",
      "\titers: 500, epoch: 12 | loss: 0.0067849\n",
      "\tspeed: 0.0323s/iter; left time: 246.9750s\n",
      "\titers: 600, epoch: 12 | loss: 0.0046099\n",
      "\tspeed: 0.0305s/iter; left time: 230.3371s\n",
      "\titers: 700, epoch: 12 | loss: 0.0044747\n",
      "\tspeed: 0.0327s/iter; left time: 243.7367s\n",
      "\titers: 800, epoch: 12 | loss: 0.0054506\n",
      "\tspeed: 0.0316s/iter; left time: 232.5769s\n",
      "\titers: 900, epoch: 12 | loss: 0.0053148\n",
      "\tspeed: 0.0355s/iter; left time: 257.3868s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:32.83s\n",
      "Steps: 906 | Train Loss: 0.0057593 Vali Loss: 0.0099598 Test Loss: 0.0384844\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0054433\n",
      "\tspeed: 0.1002s/iter; left time: 716.3370s\n",
      "\titers: 200, epoch: 13 | loss: 0.0058109\n",
      "\tspeed: 0.0316s/iter; left time: 223.0693s\n",
      "\titers: 300, epoch: 13 | loss: 0.0062376\n",
      "\tspeed: 0.0321s/iter; left time: 223.0013s\n",
      "\titers: 400, epoch: 13 | loss: 0.0048624\n",
      "\tspeed: 0.0314s/iter; left time: 214.9900s\n",
      "\titers: 500, epoch: 13 | loss: 0.0055492\n",
      "\tspeed: 0.0328s/iter; left time: 221.1479s\n",
      "\titers: 600, epoch: 13 | loss: 0.0049843\n",
      "\tspeed: 0.0328s/iter; left time: 217.8897s\n",
      "\titers: 700, epoch: 13 | loss: 0.0053882\n",
      "\tspeed: 0.0329s/iter; left time: 215.3787s\n",
      "\titers: 800, epoch: 13 | loss: 0.0060933\n",
      "\tspeed: 0.0353s/iter; left time: 227.9628s\n",
      "\titers: 900, epoch: 13 | loss: 0.0047877\n",
      "\tspeed: 0.0316s/iter; left time: 200.8906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:29.88s\n",
      "Steps: 906 | Train Loss: 0.0054440 Vali Loss: 0.0095827 Test Loss: 0.0370335\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0048743\n",
      "\tspeed: 0.0940s/iter; left time: 587.0656s\n",
      "\titers: 200, epoch: 14 | loss: 0.0055140\n",
      "\tspeed: 0.0357s/iter; left time: 219.1526s\n",
      "\titers: 300, epoch: 14 | loss: 0.0043491\n",
      "\tspeed: 0.0362s/iter; left time: 218.5411s\n",
      "\titers: 400, epoch: 14 | loss: 0.0053514\n",
      "\tspeed: 0.0334s/iter; left time: 198.4606s\n",
      "\titers: 500, epoch: 14 | loss: 0.0046246\n",
      "\tspeed: 0.0315s/iter; left time: 184.0991s\n",
      "\titers: 600, epoch: 14 | loss: 0.0043639\n",
      "\tspeed: 0.0319s/iter; left time: 183.2888s\n",
      "\titers: 700, epoch: 14 | loss: 0.0058698\n",
      "\tspeed: 0.0322s/iter; left time: 181.8568s\n",
      "\titers: 800, epoch: 14 | loss: 0.0070508\n",
      "\tspeed: 0.0340s/iter; left time: 188.2183s\n",
      "\titers: 900, epoch: 14 | loss: 0.0055367\n",
      "\tspeed: 0.0333s/iter; left time: 181.2321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:30.61s\n",
      "Steps: 906 | Train Loss: 0.0051721 Vali Loss: 0.0094972 Test Loss: 0.0370448\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0048664\n",
      "\tspeed: 0.0982s/iter; left time: 524.2248s\n",
      "\titers: 200, epoch: 15 | loss: 0.0045329\n",
      "\tspeed: 0.0320s/iter; left time: 167.5370s\n",
      "\titers: 300, epoch: 15 | loss: 0.0051289\n",
      "\tspeed: 0.0330s/iter; left time: 169.7455s\n",
      "\titers: 400, epoch: 15 | loss: 0.0048189\n",
      "\tspeed: 0.0313s/iter; left time: 157.8040s\n",
      "\titers: 500, epoch: 15 | loss: 0.0052685\n",
      "\tspeed: 0.0318s/iter; left time: 156.8851s\n",
      "\titers: 600, epoch: 15 | loss: 0.0050529\n",
      "\tspeed: 0.0329s/iter; left time: 159.3255s\n",
      "\titers: 700, epoch: 15 | loss: 0.0045252\n",
      "\tspeed: 0.0330s/iter; left time: 156.5202s\n",
      "\titers: 800, epoch: 15 | loss: 0.0036254\n",
      "\tspeed: 0.0329s/iter; left time: 152.6523s\n",
      "\titers: 900, epoch: 15 | loss: 0.0046330\n",
      "\tspeed: 0.0321s/iter; left time: 145.6334s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:29.80s\n",
      "Steps: 906 | Train Loss: 0.0049421 Vali Loss: 0.0096856 Test Loss: 0.0405013\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.03036285936832428, rmse:0.17424941062927246, mae:0.10449937731027603, rse:0.5119974613189697\n",
      "Intermediate time for ES and pred_len 24: 00h:08m:51.92s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_96_ES', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1013976\n",
      "\tspeed: 0.0739s/iter; left time: 1329.3030s\n",
      "\titers: 200, epoch: 1 | loss: 0.0771578\n",
      "\tspeed: 0.0389s/iter; left time: 695.0236s\n",
      "\titers: 300, epoch: 1 | loss: 0.0797359\n",
      "\tspeed: 0.0348s/iter; left time: 618.2741s\n",
      "\titers: 400, epoch: 1 | loss: 0.0694062\n",
      "\tspeed: 0.0362s/iter; left time: 639.2349s\n",
      "\titers: 500, epoch: 1 | loss: 0.0591974\n",
      "\tspeed: 0.0351s/iter; left time: 617.2378s\n",
      "\titers: 600, epoch: 1 | loss: 0.0565173\n",
      "\tspeed: 0.0362s/iter; left time: 633.2192s\n",
      "\titers: 700, epoch: 1 | loss: 0.0575356\n",
      "\tspeed: 0.0380s/iter; left time: 660.0583s\n",
      "\titers: 800, epoch: 1 | loss: 0.0524184\n",
      "\tspeed: 0.0356s/iter; left time: 615.0854s\n",
      "\titers: 900, epoch: 1 | loss: 0.0508521\n",
      "\tspeed: 0.0366s/iter; left time: 629.2359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:34.01s\n",
      "Steps: 904 | Train Loss: 0.0714525 Vali Loss: 0.0426748 Test Loss: 0.0774080\n",
      "Validation loss decreased (inf --> 0.042675).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0342556\n",
      "\tspeed: 0.1143s/iter; left time: 1952.5424s\n",
      "\titers: 200, epoch: 2 | loss: 0.0295202\n",
      "\tspeed: 0.0349s/iter; left time: 593.2911s\n",
      "\titers: 300, epoch: 2 | loss: 0.0292581\n",
      "\tspeed: 0.0350s/iter; left time: 590.8787s\n",
      "\titers: 400, epoch: 2 | loss: 0.0218068\n",
      "\tspeed: 0.0402s/iter; left time: 673.6128s\n",
      "\titers: 500, epoch: 2 | loss: 0.0203914\n",
      "\tspeed: 0.0398s/iter; left time: 662.9595s\n",
      "\titers: 600, epoch: 2 | loss: 0.0206816\n",
      "\tspeed: 0.0354s/iter; left time: 587.2539s\n",
      "\titers: 700, epoch: 2 | loss: 0.0191458\n",
      "\tspeed: 0.0361s/iter; left time: 594.4477s\n",
      "\titers: 800, epoch: 2 | loss: 0.0165509\n",
      "\tspeed: 0.0359s/iter; left time: 588.2600s\n",
      "\titers: 900, epoch: 2 | loss: 0.0190492\n",
      "\tspeed: 0.0346s/iter; left time: 563.9956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:33.41s\n",
      "Steps: 904 | Train Loss: 0.0254755 Vali Loss: 0.0198806 Test Loss: 0.0384178\n",
      "Validation loss decreased (0.042675 --> 0.019881).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0185786\n",
      "\tspeed: 0.1099s/iter; left time: 1776.8290s\n",
      "\titers: 200, epoch: 3 | loss: 0.0157663\n",
      "\tspeed: 0.0384s/iter; left time: 617.6594s\n",
      "\titers: 300, epoch: 3 | loss: 0.0181196\n",
      "\tspeed: 0.0353s/iter; left time: 564.4243s\n",
      "\titers: 400, epoch: 3 | loss: 0.0181286\n",
      "\tspeed: 0.0357s/iter; left time: 566.4197s\n",
      "\titers: 500, epoch: 3 | loss: 0.0144538\n",
      "\tspeed: 0.0361s/iter; left time: 569.2958s\n",
      "\titers: 600, epoch: 3 | loss: 0.0161840\n",
      "\tspeed: 0.0379s/iter; left time: 594.0816s\n",
      "\titers: 700, epoch: 3 | loss: 0.0171365\n",
      "\tspeed: 0.0372s/iter; left time: 579.3469s\n",
      "\titers: 800, epoch: 3 | loss: 0.0168469\n",
      "\tspeed: 0.0355s/iter; left time: 548.8587s\n",
      "\titers: 900, epoch: 3 | loss: 0.0152578\n",
      "\tspeed: 0.0368s/iter; left time: 566.0448s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:33.60s\n",
      "Steps: 904 | Train Loss: 0.0169028 Vali Loss: 0.0177613 Test Loss: 0.0432140\n",
      "Validation loss decreased (0.019881 --> 0.017761).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0139807\n",
      "\tspeed: 0.1156s/iter; left time: 1764.6867s\n",
      "\titers: 200, epoch: 4 | loss: 0.0142875\n",
      "\tspeed: 0.0371s/iter; left time: 562.8297s\n",
      "\titers: 300, epoch: 4 | loss: 0.0153466\n",
      "\tspeed: 0.0365s/iter; left time: 550.6426s\n",
      "\titers: 400, epoch: 4 | loss: 0.0136019\n",
      "\tspeed: 0.0378s/iter; left time: 565.5678s\n",
      "\titers: 500, epoch: 4 | loss: 0.0160778\n",
      "\tspeed: 0.0380s/iter; left time: 565.2504s\n",
      "\titers: 600, epoch: 4 | loss: 0.0141800\n",
      "\tspeed: 0.0352s/iter; left time: 519.9266s\n",
      "\titers: 700, epoch: 4 | loss: 0.0135171\n",
      "\tspeed: 0.0367s/iter; left time: 538.5034s\n",
      "\titers: 800, epoch: 4 | loss: 0.0128371\n",
      "\tspeed: 0.0357s/iter; left time: 519.4828s\n",
      "\titers: 900, epoch: 4 | loss: 0.0160632\n",
      "\tspeed: 0.0387s/iter; left time: 559.5821s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:33.96s\n",
      "Steps: 904 | Train Loss: 0.0154550 Vali Loss: 0.0175591 Test Loss: 0.0405319\n",
      "Validation loss decreased (0.017761 --> 0.017559).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0152498\n",
      "\tspeed: 0.1116s/iter; left time: 1603.0667s\n",
      "\titers: 200, epoch: 5 | loss: 0.0144196\n",
      "\tspeed: 0.0339s/iter; left time: 482.9221s\n",
      "\titers: 300, epoch: 5 | loss: 0.0148976\n",
      "\tspeed: 0.0347s/iter; left time: 491.2748s\n",
      "\titers: 400, epoch: 5 | loss: 0.0138593\n",
      "\tspeed: 0.0345s/iter; left time: 485.3613s\n",
      "\titers: 500, epoch: 5 | loss: 0.0134832\n",
      "\tspeed: 0.0383s/iter; left time: 534.3952s\n",
      "\titers: 600, epoch: 5 | loss: 0.0133692\n",
      "\tspeed: 0.0350s/iter; left time: 485.7507s\n",
      "\titers: 700, epoch: 5 | loss: 0.0127165\n",
      "\tspeed: 0.0346s/iter; left time: 476.9180s\n",
      "\titers: 800, epoch: 5 | loss: 0.0143193\n",
      "\tspeed: 0.0356s/iter; left time: 486.0689s\n",
      "\titers: 900, epoch: 5 | loss: 0.0145045\n",
      "\tspeed: 0.0369s/iter; left time: 500.7456s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:32.33s\n",
      "Steps: 904 | Train Loss: 0.0142620 Vali Loss: 0.0170134 Test Loss: 0.0450900\n",
      "Validation loss decreased (0.017559 --> 0.017013).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0141830\n",
      "\tspeed: 0.1128s/iter; left time: 1518.8698s\n",
      "\titers: 200, epoch: 6 | loss: 0.0146969\n",
      "\tspeed: 0.0366s/iter; left time: 489.3171s\n",
      "\titers: 300, epoch: 6 | loss: 0.0133638\n",
      "\tspeed: 0.0355s/iter; left time: 470.9691s\n",
      "\titers: 400, epoch: 6 | loss: 0.0126833\n",
      "\tspeed: 0.0351s/iter; left time: 462.0429s\n",
      "\titers: 500, epoch: 6 | loss: 0.0126383\n",
      "\tspeed: 0.0353s/iter; left time: 461.1625s\n",
      "\titers: 600, epoch: 6 | loss: 0.0127252\n",
      "\tspeed: 0.0348s/iter; left time: 451.1328s\n",
      "\titers: 700, epoch: 6 | loss: 0.0139683\n",
      "\tspeed: 0.0364s/iter; left time: 468.7590s\n",
      "\titers: 800, epoch: 6 | loss: 0.0128622\n",
      "\tspeed: 0.0348s/iter; left time: 444.4484s\n",
      "\titers: 900, epoch: 6 | loss: 0.0116518\n",
      "\tspeed: 0.0346s/iter; left time: 438.5653s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:32.51s\n",
      "Steps: 904 | Train Loss: 0.0132401 Vali Loss: 0.0178354 Test Loss: 0.0533979\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0123468\n",
      "\tspeed: 0.1065s/iter; left time: 1336.8369s\n",
      "\titers: 200, epoch: 7 | loss: 0.0132131\n",
      "\tspeed: 0.0370s/iter; left time: 461.0177s\n",
      "\titers: 300, epoch: 7 | loss: 0.0143868\n",
      "\tspeed: 0.0357s/iter; left time: 440.8902s\n",
      "\titers: 400, epoch: 7 | loss: 0.0135288\n",
      "\tspeed: 0.0342s/iter; left time: 418.6562s\n",
      "\titers: 500, epoch: 7 | loss: 0.0111989\n",
      "\tspeed: 0.0363s/iter; left time: 440.7803s\n",
      "\titers: 600, epoch: 7 | loss: 0.0100680\n",
      "\tspeed: 0.0354s/iter; left time: 426.9317s\n",
      "\titers: 700, epoch: 7 | loss: 0.0116993\n",
      "\tspeed: 0.0342s/iter; left time: 408.7937s\n",
      "\titers: 800, epoch: 7 | loss: 0.0126507\n",
      "\tspeed: 0.0359s/iter; left time: 425.4187s\n",
      "\titers: 900, epoch: 7 | loss: 0.0127079\n",
      "\tspeed: 0.0350s/iter; left time: 411.5474s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:32.44s\n",
      "Steps: 904 | Train Loss: 0.0122550 Vali Loss: 0.0170722 Test Loss: 0.0502459\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0137502\n",
      "\tspeed: 0.1077s/iter; left time: 1254.6236s\n",
      "\titers: 200, epoch: 8 | loss: 0.0107854\n",
      "\tspeed: 0.0343s/iter; left time: 396.0330s\n",
      "\titers: 300, epoch: 8 | loss: 0.0110221\n",
      "\tspeed: 0.0367s/iter; left time: 420.8695s\n",
      "\titers: 400, epoch: 8 | loss: 0.0107047\n",
      "\tspeed: 0.0376s/iter; left time: 426.8505s\n",
      "\titers: 500, epoch: 8 | loss: 0.0104447\n",
      "\tspeed: 0.0348s/iter; left time: 392.0074s\n",
      "\titers: 600, epoch: 8 | loss: 0.0098792\n",
      "\tspeed: 0.0377s/iter; left time: 420.9666s\n",
      "\titers: 700, epoch: 8 | loss: 0.0090191\n",
      "\tspeed: 0.0355s/iter; left time: 392.7931s\n",
      "\titers: 800, epoch: 8 | loss: 0.0116056\n",
      "\tspeed: 0.0341s/iter; left time: 373.3702s\n",
      "\titers: 900, epoch: 8 | loss: 0.0101424\n",
      "\tspeed: 0.0372s/iter; left time: 403.2423s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:32.96s\n",
      "Steps: 904 | Train Loss: 0.0111770 Vali Loss: 0.0175536 Test Loss: 0.0653295\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0100417\n",
      "\tspeed: 0.1080s/iter; left time: 1160.3693s\n",
      "\titers: 200, epoch: 9 | loss: 0.0109558\n",
      "\tspeed: 0.0346s/iter; left time: 367.9681s\n",
      "\titers: 300, epoch: 9 | loss: 0.0106716\n",
      "\tspeed: 0.0353s/iter; left time: 372.1049s\n",
      "\titers: 400, epoch: 9 | loss: 0.0099881\n",
      "\tspeed: 0.0373s/iter; left time: 389.6040s\n",
      "\titers: 500, epoch: 9 | loss: 0.0102421\n",
      "\tspeed: 0.0362s/iter; left time: 374.7245s\n",
      "\titers: 600, epoch: 9 | loss: 0.0116103\n",
      "\tspeed: 0.0347s/iter; left time: 355.8872s\n",
      "\titers: 700, epoch: 9 | loss: 0.0098578\n",
      "\tspeed: 0.0360s/iter; left time: 365.4718s\n",
      "\titers: 800, epoch: 9 | loss: 0.0105738\n",
      "\tspeed: 0.0362s/iter; left time: 364.2395s\n",
      "\titers: 900, epoch: 9 | loss: 0.0112765\n",
      "\tspeed: 0.0351s/iter; left time: 349.3016s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:32.89s\n",
      "Steps: 904 | Train Loss: 0.0102351 Vali Loss: 0.0179571 Test Loss: 0.0696844\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0096858\n",
      "\tspeed: 0.1002s/iter; left time: 986.7954s\n",
      "\titers: 200, epoch: 10 | loss: 0.0088900\n",
      "\tspeed: 0.0350s/iter; left time: 340.8842s\n",
      "\titers: 300, epoch: 10 | loss: 0.0082502\n",
      "\tspeed: 0.0373s/iter; left time: 359.6288s\n",
      "\titers: 400, epoch: 10 | loss: 0.0097051\n",
      "\tspeed: 0.0379s/iter; left time: 361.3709s\n",
      "\titers: 500, epoch: 10 | loss: 0.0085401\n",
      "\tspeed: 0.0376s/iter; left time: 354.9389s\n",
      "\titers: 600, epoch: 10 | loss: 0.0089890\n",
      "\tspeed: 0.0348s/iter; left time: 325.5696s\n",
      "\titers: 700, epoch: 10 | loss: 0.0098679\n",
      "\tspeed: 0.0359s/iter; left time: 331.9615s\n",
      "\titers: 800, epoch: 10 | loss: 0.0080299\n",
      "\tspeed: 0.0376s/iter; left time: 343.7813s\n",
      "\titers: 900, epoch: 10 | loss: 0.0081632\n",
      "\tspeed: 0.0358s/iter; left time: 323.5646s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:33.02s\n",
      "Steps: 904 | Train Loss: 0.0094014 Vali Loss: 0.0173877 Test Loss: 0.0682939\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04509866237640381, rmse:0.21236445009708405, mae:0.13736702501773834, rse:0.6238628625869751\n",
      "Intermediate time for ES and pred_len 96: 00h:06m:56.75s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_168_ES', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_168_ES_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1076604\n",
      "\tspeed: 0.0754s/iter; left time: 1352.2940s\n",
      "\titers: 200, epoch: 1 | loss: 0.0817696\n",
      "\tspeed: 0.0587s/iter; left time: 1047.8419s\n",
      "\titers: 300, epoch: 1 | loss: 0.0782555\n",
      "\tspeed: 0.0534s/iter; left time: 947.6364s\n",
      "\titers: 400, epoch: 1 | loss: 0.0722798\n",
      "\tspeed: 0.0514s/iter; left time: 906.7635s\n",
      "\titers: 500, epoch: 1 | loss: 0.0666439\n",
      "\tspeed: 0.0596s/iter; left time: 1045.0301s\n",
      "\titers: 600, epoch: 1 | loss: 0.0624405\n",
      "\tspeed: 0.0515s/iter; left time: 898.6392s\n",
      "\titers: 700, epoch: 1 | loss: 0.0599549\n",
      "\tspeed: 0.0561s/iter; left time: 972.6526s\n",
      "\titers: 800, epoch: 1 | loss: 0.0559474\n",
      "\tspeed: 0.0576s/iter; left time: 992.9466s\n",
      "\titers: 900, epoch: 1 | loss: 0.0540454\n",
      "\tspeed: 0.0513s/iter; left time: 879.0107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:50.27s\n",
      "Steps: 902 | Train Loss: 0.0748573 Vali Loss: 0.0524881 Test Loss: 0.0967396\n",
      "Validation loss decreased (inf --> 0.052488).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0421625\n",
      "\tspeed: 0.1912s/iter; left time: 3257.8479s\n",
      "\titers: 200, epoch: 2 | loss: 0.0393575\n",
      "\tspeed: 0.0557s/iter; left time: 943.2098s\n",
      "\titers: 300, epoch: 2 | loss: 0.0356493\n",
      "\tspeed: 0.0556s/iter; left time: 936.4896s\n",
      "\titers: 400, epoch: 2 | loss: 0.0319433\n",
      "\tspeed: 0.0543s/iter; left time: 909.6275s\n",
      "\titers: 500, epoch: 2 | loss: 0.0246841\n",
      "\tspeed: 0.0526s/iter; left time: 874.6119s\n",
      "\titers: 600, epoch: 2 | loss: 0.0226763\n",
      "\tspeed: 0.0565s/iter; left time: 934.5025s\n",
      "\titers: 700, epoch: 2 | loss: 0.0219264\n",
      "\tspeed: 0.0549s/iter; left time: 902.6796s\n",
      "\titers: 800, epoch: 2 | loss: 0.0208745\n",
      "\tspeed: 0.0521s/iter; left time: 851.1553s\n",
      "\titers: 900, epoch: 2 | loss: 0.0186008\n",
      "\tspeed: 0.0553s/iter; left time: 898.4655s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:49.59s\n",
      "Steps: 902 | Train Loss: 0.0303606 Vali Loss: 0.0206722 Test Loss: 0.0529895\n",
      "Validation loss decreased (0.052488 --> 0.020672).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0201931\n",
      "\tspeed: 0.1919s/iter; left time: 3096.9220s\n",
      "\titers: 200, epoch: 3 | loss: 0.0177573\n",
      "\tspeed: 0.0515s/iter; left time: 825.5585s\n",
      "\titers: 300, epoch: 3 | loss: 0.0159174\n",
      "\tspeed: 0.0557s/iter; left time: 886.9390s\n",
      "\titers: 400, epoch: 3 | loss: 0.0174967\n",
      "\tspeed: 0.0542s/iter; left time: 857.6107s\n",
      "\titers: 500, epoch: 3 | loss: 0.0174612\n",
      "\tspeed: 0.0539s/iter; left time: 848.5347s\n",
      "\titers: 600, epoch: 3 | loss: 0.0209598\n",
      "\tspeed: 0.0563s/iter; left time: 879.7305s\n",
      "\titers: 700, epoch: 3 | loss: 0.0185944\n",
      "\tspeed: 0.0566s/iter; left time: 879.9788s\n",
      "\titers: 800, epoch: 3 | loss: 0.0170346\n",
      "\tspeed: 0.0503s/iter; left time: 777.0366s\n",
      "\titers: 900, epoch: 3 | loss: 0.0172354\n",
      "\tspeed: 0.0542s/iter; left time: 830.5289s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:49.53s\n",
      "Steps: 902 | Train Loss: 0.0183554 Vali Loss: 0.0204376 Test Loss: 0.0526916\n",
      "Validation loss decreased (0.020672 --> 0.020438).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0171463\n",
      "\tspeed: 0.1866s/iter; left time: 2843.0165s\n",
      "\titers: 200, epoch: 4 | loss: 0.0193084\n",
      "\tspeed: 0.0514s/iter; left time: 778.6349s\n",
      "\titers: 300, epoch: 4 | loss: 0.0158545\n",
      "\tspeed: 0.0521s/iter; left time: 782.9082s\n",
      "\titers: 400, epoch: 4 | loss: 0.0169759\n",
      "\tspeed: 0.0515s/iter; left time: 769.7277s\n",
      "\titers: 500, epoch: 4 | loss: 0.0165341\n",
      "\tspeed: 0.0514s/iter; left time: 763.0252s\n",
      "\titers: 600, epoch: 4 | loss: 0.0156424\n",
      "\tspeed: 0.0491s/iter; left time: 723.6069s\n",
      "\titers: 700, epoch: 4 | loss: 0.0156201\n",
      "\tspeed: 0.0495s/iter; left time: 724.5851s\n",
      "\titers: 800, epoch: 4 | loss: 0.0194296\n",
      "\tspeed: 0.0518s/iter; left time: 752.6211s\n",
      "\titers: 900, epoch: 4 | loss: 0.0168761\n",
      "\tspeed: 0.0499s/iter; left time: 720.8929s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:46.49s\n",
      "Steps: 902 | Train Loss: 0.0168468 Vali Loss: 0.0181415 Test Loss: 0.0457093\n",
      "Validation loss decreased (0.020438 --> 0.018142).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0150640\n",
      "\tspeed: 0.1748s/iter; left time: 2504.8431s\n",
      "\titers: 200, epoch: 5 | loss: 0.0157095\n",
      "\tspeed: 0.0476s/iter; left time: 676.8551s\n",
      "\titers: 300, epoch: 5 | loss: 0.0159143\n",
      "\tspeed: 0.0483s/iter; left time: 681.9523s\n",
      "\titers: 400, epoch: 5 | loss: 0.0148808\n",
      "\tspeed: 0.0504s/iter; left time: 706.6997s\n",
      "\titers: 500, epoch: 5 | loss: 0.0149314\n",
      "\tspeed: 0.0445s/iter; left time: 620.3126s\n",
      "\titers: 600, epoch: 5 | loss: 0.0147398\n",
      "\tspeed: 0.0397s/iter; left time: 549.4337s\n",
      "\titers: 700, epoch: 5 | loss: 0.0151090\n",
      "\tspeed: 0.0437s/iter; left time: 600.0459s\n",
      "\titers: 800, epoch: 5 | loss: 0.0155658\n",
      "\tspeed: 0.0469s/iter; left time: 640.0667s\n",
      "\titers: 900, epoch: 5 | loss: 0.0139933\n",
      "\tspeed: 0.0557s/iter; left time: 754.2638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:43.37s\n",
      "Steps: 902 | Train Loss: 0.0154849 Vali Loss: 0.0210057 Test Loss: 0.0575928\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0153980\n",
      "\tspeed: 0.1915s/iter; left time: 2572.5208s\n",
      "\titers: 200, epoch: 6 | loss: 0.0163318\n",
      "\tspeed: 0.0559s/iter; left time: 745.6963s\n",
      "\titers: 300, epoch: 6 | loss: 0.0161950\n",
      "\tspeed: 0.0531s/iter; left time: 702.8263s\n",
      "\titers: 400, epoch: 6 | loss: 0.0137966\n",
      "\tspeed: 0.0571s/iter; left time: 750.3720s\n",
      "\titers: 500, epoch: 6 | loss: 0.0135618\n",
      "\tspeed: 0.0593s/iter; left time: 772.4411s\n",
      "\titers: 600, epoch: 6 | loss: 0.0140798\n",
      "\tspeed: 0.0534s/iter; left time: 690.6953s\n",
      "\titers: 700, epoch: 6 | loss: 0.0140824\n",
      "\tspeed: 0.0564s/iter; left time: 723.5259s\n",
      "\titers: 800, epoch: 6 | loss: 0.0144318\n",
      "\tspeed: 0.0592s/iter; left time: 753.6689s\n",
      "\titers: 900, epoch: 6 | loss: 0.0141567\n",
      "\tspeed: 0.0452s/iter; left time: 570.7096s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:50.19s\n",
      "Steps: 902 | Train Loss: 0.0142061 Vali Loss: 0.0200412 Test Loss: 0.0589927\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0129912\n",
      "\tspeed: 0.1652s/iter; left time: 2069.3153s\n",
      "\titers: 200, epoch: 7 | loss: 0.0121356\n",
      "\tspeed: 0.0595s/iter; left time: 739.4595s\n",
      "\titers: 300, epoch: 7 | loss: 0.0127763\n",
      "\tspeed: 0.0588s/iter; left time: 724.9347s\n",
      "\titers: 400, epoch: 7 | loss: 0.0125835\n",
      "\tspeed: 0.0611s/iter; left time: 747.7475s\n",
      "\titers: 500, epoch: 7 | loss: 0.0129308\n",
      "\tspeed: 0.0646s/iter; left time: 783.0269s\n",
      "\titers: 600, epoch: 7 | loss: 0.0124214\n",
      "\tspeed: 0.0534s/iter; left time: 642.1967s\n",
      "\titers: 700, epoch: 7 | loss: 0.0119898\n",
      "\tspeed: 0.0599s/iter; left time: 714.2870s\n",
      "\titers: 800, epoch: 7 | loss: 0.0121501\n",
      "\tspeed: 0.0656s/iter; left time: 776.2387s\n",
      "\titers: 900, epoch: 7 | loss: 0.0122737\n",
      "\tspeed: 0.0561s/iter; left time: 658.0494s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:54.64s\n",
      "Steps: 902 | Train Loss: 0.0129999 Vali Loss: 0.0207440 Test Loss: 0.0627964\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0124242\n",
      "\tspeed: 0.2055s/iter; left time: 2389.6126s\n",
      "\titers: 200, epoch: 8 | loss: 0.0118248\n",
      "\tspeed: 0.0526s/iter; left time: 606.7573s\n",
      "\titers: 300, epoch: 8 | loss: 0.0117292\n",
      "\tspeed: 0.0564s/iter; left time: 644.5613s\n",
      "\titers: 400, epoch: 8 | loss: 0.0130134\n",
      "\tspeed: 0.0665s/iter; left time: 753.2220s\n",
      "\titers: 500, epoch: 8 | loss: 0.0130490\n",
      "\tspeed: 0.0596s/iter; left time: 669.5602s\n",
      "\titers: 600, epoch: 8 | loss: 0.0109687\n",
      "\tspeed: 0.0583s/iter; left time: 648.2929s\n",
      "\titers: 700, epoch: 8 | loss: 0.0122383\n",
      "\tspeed: 0.0571s/iter; left time: 630.1158s\n",
      "\titers: 800, epoch: 8 | loss: 0.0117130\n",
      "\tspeed: 0.0663s/iter; left time: 724.1992s\n",
      "\titers: 900, epoch: 8 | loss: 0.0100833\n",
      "\tspeed: 0.0572s/iter; left time: 619.3603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:54.08s\n",
      "Steps: 902 | Train Loss: 0.0118847 Vali Loss: 0.0210125 Test Loss: 0.0733620\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0103930\n",
      "\tspeed: 0.2014s/iter; left time: 2159.8774s\n",
      "\titers: 200, epoch: 9 | loss: 0.0112016\n",
      "\tspeed: 0.0558s/iter; left time: 592.9863s\n",
      "\titers: 300, epoch: 9 | loss: 0.0108396\n",
      "\tspeed: 0.0565s/iter; left time: 594.7679s\n",
      "\titers: 400, epoch: 9 | loss: 0.0120394\n",
      "\tspeed: 0.0646s/iter; left time: 673.4253s\n",
      "\titers: 500, epoch: 9 | loss: 0.0112179\n",
      "\tspeed: 0.0599s/iter; left time: 618.2037s\n",
      "\titers: 600, epoch: 9 | loss: 0.0110158\n",
      "\tspeed: 0.0553s/iter; left time: 565.7573s\n",
      "\titers: 700, epoch: 9 | loss: 0.0111671\n",
      "\tspeed: 0.0625s/iter; left time: 632.9769s\n",
      "\titers: 800, epoch: 9 | loss: 0.0114187\n",
      "\tspeed: 0.0552s/iter; left time: 553.5929s\n",
      "\titers: 900, epoch: 9 | loss: 0.0102199\n",
      "\tspeed: 0.0608s/iter; left time: 603.9255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:54.30s\n",
      "Steps: 902 | Train Loss: 0.0108681 Vali Loss: 0.0215595 Test Loss: 0.0831135\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_168_ES_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04569759592413902, rmse:0.21376995742321014, mae:0.14075864851474762, rse:0.6279107928276062\n",
      "Intermediate time for ES and pred_len 168: 00h:09m:44.87s\n",
      "Intermediate time for ES: 00h:25m:33.53s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_96_24_FR', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_96_24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0557040\n",
      "\tspeed: 0.0888s/iter; left time: 1599.3656s\n",
      "\titers: 200, epoch: 1 | loss: 0.0566567\n",
      "\tspeed: 0.0361s/iter; left time: 646.6672s\n",
      "\titers: 300, epoch: 1 | loss: 0.0498068\n",
      "\tspeed: 0.0350s/iter; left time: 623.6590s\n",
      "\titers: 400, epoch: 1 | loss: 0.0433728\n",
      "\tspeed: 0.0338s/iter; left time: 598.4894s\n",
      "\titers: 500, epoch: 1 | loss: 0.0398614\n",
      "\tspeed: 0.0324s/iter; left time: 571.7837s\n",
      "\titers: 600, epoch: 1 | loss: 0.0323519\n",
      "\tspeed: 0.0313s/iter; left time: 548.8695s\n",
      "\titers: 700, epoch: 1 | loss: 0.0296219\n",
      "\tspeed: 0.0318s/iter; left time: 554.8368s\n",
      "\titers: 800, epoch: 1 | loss: 0.0302265\n",
      "\tspeed: 0.0329s/iter; left time: 570.3510s\n",
      "\titers: 900, epoch: 1 | loss: 0.0270551\n",
      "\tspeed: 0.0343s/iter; left time: 590.1721s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:32.06s\n",
      "Steps: 906 | Train Loss: 0.0440174 Vali Loss: 0.0216729 Test Loss: 0.0262815\n",
      "Validation loss decreased (inf --> 0.021673).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0230959\n",
      "\tspeed: 0.1107s/iter; left time: 1893.7844s\n",
      "\titers: 200, epoch: 2 | loss: 0.0100425\n",
      "\tspeed: 0.0361s/iter; left time: 613.6718s\n",
      "\titers: 300, epoch: 2 | loss: 0.0092201\n",
      "\tspeed: 0.0320s/iter; left time: 541.9046s\n",
      "\titers: 400, epoch: 2 | loss: 0.0095029\n",
      "\tspeed: 0.0325s/iter; left time: 547.2760s\n",
      "\titers: 500, epoch: 2 | loss: 0.0086843\n",
      "\tspeed: 0.0350s/iter; left time: 585.3725s\n",
      "\titers: 600, epoch: 2 | loss: 0.0133543\n",
      "\tspeed: 0.0371s/iter; left time: 615.8823s\n",
      "\titers: 700, epoch: 2 | loss: 0.0075673\n",
      "\tspeed: 0.0322s/iter; left time: 532.4580s\n",
      "\titers: 800, epoch: 2 | loss: 0.0105873\n",
      "\tspeed: 0.0334s/iter; left time: 547.8638s\n",
      "\titers: 900, epoch: 2 | loss: 0.0098104\n",
      "\tspeed: 0.0329s/iter; left time: 536.0198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:31.25s\n",
      "Steps: 906 | Train Loss: 0.0122258 Vali Loss: 0.0109184 Test Loss: 0.0141849\n",
      "Validation loss decreased (0.021673 --> 0.010918).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0077783\n",
      "\tspeed: 0.1134s/iter; left time: 1837.3004s\n",
      "\titers: 200, epoch: 3 | loss: 0.0066038\n",
      "\tspeed: 0.0316s/iter; left time: 509.1545s\n",
      "\titers: 300, epoch: 3 | loss: 0.0083415\n",
      "\tspeed: 0.0322s/iter; left time: 515.3240s\n",
      "\titers: 400, epoch: 3 | loss: 0.0072368\n",
      "\tspeed: 0.0316s/iter; left time: 503.4924s\n",
      "\titers: 500, epoch: 3 | loss: 0.0091273\n",
      "\tspeed: 0.0325s/iter; left time: 513.0264s\n",
      "\titers: 600, epoch: 3 | loss: 0.0085347\n",
      "\tspeed: 0.0334s/iter; left time: 523.9644s\n",
      "\titers: 700, epoch: 3 | loss: 0.0069173\n",
      "\tspeed: 0.0355s/iter; left time: 554.4338s\n",
      "\titers: 800, epoch: 3 | loss: 0.0051648\n",
      "\tspeed: 0.0363s/iter; left time: 562.5077s\n",
      "\titers: 900, epoch: 3 | loss: 0.0051273\n",
      "\tspeed: 0.0330s/iter; left time: 508.7869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:30.63s\n",
      "Steps: 906 | Train Loss: 0.0077618 Vali Loss: 0.0107251 Test Loss: 0.0137270\n",
      "Validation loss decreased (0.010918 --> 0.010725).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0063474\n",
      "\tspeed: 0.1108s/iter; left time: 1695.6758s\n",
      "\titers: 200, epoch: 4 | loss: 0.0053195\n",
      "\tspeed: 0.0361s/iter; left time: 548.4656s\n",
      "\titers: 300, epoch: 4 | loss: 0.0081236\n",
      "\tspeed: 0.0322s/iter; left time: 486.0109s\n",
      "\titers: 400, epoch: 4 | loss: 0.0075084\n",
      "\tspeed: 0.0327s/iter; left time: 490.0030s\n",
      "\titers: 500, epoch: 4 | loss: 0.0051806\n",
      "\tspeed: 0.0315s/iter; left time: 468.8650s\n",
      "\titers: 600, epoch: 4 | loss: 0.0053774\n",
      "\tspeed: 0.0331s/iter; left time: 490.6987s\n",
      "\titers: 700, epoch: 4 | loss: 0.0082503\n",
      "\tspeed: 0.0343s/iter; left time: 504.4440s\n",
      "\titers: 800, epoch: 4 | loss: 0.0073275\n",
      "\tspeed: 0.0333s/iter; left time: 485.9623s\n",
      "\titers: 900, epoch: 4 | loss: 0.0056999\n",
      "\tspeed: 0.0336s/iter; left time: 486.8531s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:31.17s\n",
      "Steps: 906 | Train Loss: 0.0069536 Vali Loss: 0.0111535 Test Loss: 0.0136705\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0055979\n",
      "\tspeed: 0.1076s/iter; left time: 1549.7699s\n",
      "\titers: 200, epoch: 5 | loss: 0.0067906\n",
      "\tspeed: 0.0343s/iter; left time: 489.8944s\n",
      "\titers: 300, epoch: 5 | loss: 0.0069095\n",
      "\tspeed: 0.0328s/iter; left time: 465.0075s\n",
      "\titers: 400, epoch: 5 | loss: 0.0069703\n",
      "\tspeed: 0.0371s/iter; left time: 523.4686s\n",
      "\titers: 500, epoch: 5 | loss: 0.0061679\n",
      "\tspeed: 0.0372s/iter; left time: 521.2084s\n",
      "\titers: 600, epoch: 5 | loss: 0.0086047\n",
      "\tspeed: 0.0351s/iter; left time: 488.3821s\n",
      "\titers: 700, epoch: 5 | loss: 0.0074374\n",
      "\tspeed: 0.0333s/iter; left time: 459.9522s\n",
      "\titers: 800, epoch: 5 | loss: 0.0054916\n",
      "\tspeed: 0.0350s/iter; left time: 480.0014s\n",
      "\titers: 900, epoch: 5 | loss: 0.0056452\n",
      "\tspeed: 0.0350s/iter; left time: 475.9066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:32.48s\n",
      "Steps: 906 | Train Loss: 0.0064846 Vali Loss: 0.0108404 Test Loss: 0.0129843\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0053115\n",
      "\tspeed: 0.1062s/iter; left time: 1432.2901s\n",
      "\titers: 200, epoch: 6 | loss: 0.0040077\n",
      "\tspeed: 0.0338s/iter; left time: 451.9645s\n",
      "\titers: 300, epoch: 6 | loss: 0.0047869\n",
      "\tspeed: 0.0378s/iter; left time: 502.1505s\n",
      "\titers: 400, epoch: 6 | loss: 0.0080178\n",
      "\tspeed: 0.0374s/iter; left time: 493.2560s\n",
      "\titers: 500, epoch: 6 | loss: 0.0051984\n",
      "\tspeed: 0.0337s/iter; left time: 440.9629s\n",
      "\titers: 600, epoch: 6 | loss: 0.0075436\n",
      "\tspeed: 0.0337s/iter; left time: 438.4126s\n",
      "\titers: 700, epoch: 6 | loss: 0.0052424\n",
      "\tspeed: 0.0327s/iter; left time: 421.9993s\n",
      "\titers: 800, epoch: 6 | loss: 0.0052196\n",
      "\tspeed: 0.0373s/iter; left time: 476.7730s\n",
      "\titers: 900, epoch: 6 | loss: 0.0064065\n",
      "\tspeed: 0.0367s/iter; left time: 465.4946s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:32.43s\n",
      "Steps: 906 | Train Loss: 0.0059796 Vali Loss: 0.0110588 Test Loss: 0.0135888\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0043378\n",
      "\tspeed: 0.0995s/iter; left time: 1252.8248s\n",
      "\titers: 200, epoch: 7 | loss: 0.0085250\n",
      "\tspeed: 0.0355s/iter; left time: 442.8052s\n",
      "\titers: 300, epoch: 7 | loss: 0.0049057\n",
      "\tspeed: 0.0362s/iter; left time: 447.9070s\n",
      "\titers: 400, epoch: 7 | loss: 0.0061337\n",
      "\tspeed: 0.0328s/iter; left time: 403.3432s\n",
      "\titers: 500, epoch: 7 | loss: 0.0045658\n",
      "\tspeed: 0.0333s/iter; left time: 406.0233s\n",
      "\titers: 600, epoch: 7 | loss: 0.0041116\n",
      "\tspeed: 0.0343s/iter; left time: 414.9597s\n",
      "\titers: 700, epoch: 7 | loss: 0.0054256\n",
      "\tspeed: 0.0322s/iter; left time: 385.3954s\n",
      "\titers: 800, epoch: 7 | loss: 0.0048906\n",
      "\tspeed: 0.0348s/iter; left time: 413.4079s\n",
      "\titers: 900, epoch: 7 | loss: 0.0044429\n",
      "\tspeed: 0.0344s/iter; left time: 405.0318s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:31.16s\n",
      "Steps: 906 | Train Loss: 0.0055171 Vali Loss: 0.0111075 Test Loss: 0.0134708\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0040176\n",
      "\tspeed: 0.1035s/iter; left time: 1208.9279s\n",
      "\titers: 200, epoch: 8 | loss: 0.0060926\n",
      "\tspeed: 0.0348s/iter; left time: 403.1421s\n",
      "\titers: 300, epoch: 8 | loss: 0.0042525\n",
      "\tspeed: 0.0335s/iter; left time: 384.8762s\n",
      "\titers: 400, epoch: 8 | loss: 0.0058098\n",
      "\tspeed: 0.0378s/iter; left time: 430.5484s\n",
      "\titers: 500, epoch: 8 | loss: 0.0045941\n",
      "\tspeed: 0.0321s/iter; left time: 361.9506s\n",
      "\titers: 600, epoch: 8 | loss: 0.0044146\n",
      "\tspeed: 0.0324s/iter; left time: 361.9939s\n",
      "\titers: 700, epoch: 8 | loss: 0.0051017\n",
      "\tspeed: 0.0335s/iter; left time: 371.4275s\n",
      "\titers: 800, epoch: 8 | loss: 0.0055601\n",
      "\tspeed: 0.0380s/iter; left time: 416.6687s\n",
      "\titers: 900, epoch: 8 | loss: 0.0048369\n",
      "\tspeed: 0.0345s/iter; left time: 375.0483s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:31.78s\n",
      "Steps: 906 | Train Loss: 0.0050730 Vali Loss: 0.0111505 Test Loss: 0.0134428\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_96_24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.013713154941797256, rmse:0.11710318177938461, mae:0.07169971615076065, rse:0.45187991857528687\n",
      "Intermediate time for FR and pred_len 24: 00h:05m:24.59s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_96_96_FR', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_96_96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0713282\n",
      "\tspeed: 0.0858s/iter; left time: 1542.7723s\n",
      "\titers: 200, epoch: 1 | loss: 0.0532927\n",
      "\tspeed: 0.0373s/iter; left time: 667.1232s\n",
      "\titers: 300, epoch: 1 | loss: 0.0547653\n",
      "\tspeed: 0.0354s/iter; left time: 629.6149s\n",
      "\titers: 400, epoch: 1 | loss: 0.0498358\n",
      "\tspeed: 0.0401s/iter; left time: 708.5283s\n",
      "\titers: 500, epoch: 1 | loss: 0.0470307\n",
      "\tspeed: 0.0368s/iter; left time: 646.3491s\n",
      "\titers: 600, epoch: 1 | loss: 0.0438069\n",
      "\tspeed: 0.0353s/iter; left time: 616.6241s\n",
      "\titers: 700, epoch: 1 | loss: 0.0451446\n",
      "\tspeed: 0.0341s/iter; left time: 592.4182s\n",
      "\titers: 800, epoch: 1 | loss: 0.0392235\n",
      "\tspeed: 0.0370s/iter; left time: 638.5739s\n",
      "\titers: 900, epoch: 1 | loss: 0.0377153\n",
      "\tspeed: 0.0364s/iter; left time: 625.8237s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:34.75s\n",
      "Steps: 904 | Train Loss: 0.0528629 Vali Loss: 0.0476110 Test Loss: 0.0643863\n",
      "Validation loss decreased (inf --> 0.047611).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0264039\n",
      "\tspeed: 0.1191s/iter; left time: 2033.7781s\n",
      "\titers: 200, epoch: 2 | loss: 0.0236852\n",
      "\tspeed: 0.0359s/iter; left time: 609.2975s\n",
      "\titers: 300, epoch: 2 | loss: 0.0163798\n",
      "\tspeed: 0.0402s/iter; left time: 677.7918s\n",
      "\titers: 400, epoch: 2 | loss: 0.0185473\n",
      "\tspeed: 0.0365s/iter; left time: 613.1493s\n",
      "\titers: 500, epoch: 2 | loss: 0.0154308\n",
      "\tspeed: 0.0339s/iter; left time: 565.4403s\n",
      "\titers: 600, epoch: 2 | loss: 0.0147546\n",
      "\tspeed: 0.0342s/iter; left time: 566.7933s\n",
      "\titers: 700, epoch: 2 | loss: 0.0153008\n",
      "\tspeed: 0.0357s/iter; left time: 588.9725s\n",
      "\titers: 800, epoch: 2 | loss: 0.0102496\n",
      "\tspeed: 0.0373s/iter; left time: 610.6545s\n",
      "\titers: 900, epoch: 2 | loss: 0.0147741\n",
      "\tspeed: 0.0355s/iter; left time: 577.4771s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:33.26s\n",
      "Steps: 904 | Train Loss: 0.0189365 Vali Loss: 0.0191721 Test Loss: 0.0245784\n",
      "Validation loss decreased (0.047611 --> 0.019172).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0127275\n",
      "\tspeed: 0.1142s/iter; left time: 1846.5062s\n",
      "\titers: 200, epoch: 3 | loss: 0.0125322\n",
      "\tspeed: 0.0351s/iter; left time: 563.8656s\n",
      "\titers: 300, epoch: 3 | loss: 0.0125635\n",
      "\tspeed: 0.0367s/iter; left time: 585.4402s\n",
      "\titers: 400, epoch: 3 | loss: 0.0092649\n",
      "\tspeed: 0.0372s/iter; left time: 590.9637s\n",
      "\titers: 500, epoch: 3 | loss: 0.0118248\n",
      "\tspeed: 0.0354s/iter; left time: 558.4883s\n",
      "\titers: 600, epoch: 3 | loss: 0.0097686\n",
      "\tspeed: 0.0368s/iter; left time: 576.4442s\n",
      "\titers: 700, epoch: 3 | loss: 0.0104057\n",
      "\tspeed: 0.0381s/iter; left time: 594.0925s\n",
      "\titers: 800, epoch: 3 | loss: 0.0134165\n",
      "\tspeed: 0.0348s/iter; left time: 538.1807s\n",
      "\titers: 900, epoch: 3 | loss: 0.0104153\n",
      "\tspeed: 0.0356s/iter; left time: 547.8976s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:33.26s\n",
      "Steps: 904 | Train Loss: 0.0116075 Vali Loss: 0.0172378 Test Loss: 0.0219515\n",
      "Validation loss decreased (0.019172 --> 0.017238).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0087241\n",
      "\tspeed: 0.1169s/iter; left time: 1784.9254s\n",
      "\titers: 200, epoch: 4 | loss: 0.0109887\n",
      "\tspeed: 0.0369s/iter; left time: 560.4141s\n",
      "\titers: 300, epoch: 4 | loss: 0.0094588\n",
      "\tspeed: 0.0385s/iter; left time: 580.4079s\n",
      "\titers: 400, epoch: 4 | loss: 0.0108194\n",
      "\tspeed: 0.0354s/iter; left time: 529.1731s\n",
      "\titers: 500, epoch: 4 | loss: 0.0091312\n",
      "\tspeed: 0.0374s/iter; left time: 556.7945s\n",
      "\titers: 600, epoch: 4 | loss: 0.0099830\n",
      "\tspeed: 0.0362s/iter; left time: 534.5328s\n",
      "\titers: 700, epoch: 4 | loss: 0.0088046\n",
      "\tspeed: 0.0358s/iter; left time: 524.6948s\n",
      "\titers: 800, epoch: 4 | loss: 0.0083616\n",
      "\tspeed: 0.0372s/iter; left time: 542.6594s\n",
      "\titers: 900, epoch: 4 | loss: 0.0098428\n",
      "\tspeed: 0.0357s/iter; left time: 516.2106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:33.57s\n",
      "Steps: 904 | Train Loss: 0.0103794 Vali Loss: 0.0177083 Test Loss: 0.0204828\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0101264\n",
      "\tspeed: 0.1086s/iter; left time: 1559.6593s\n",
      "\titers: 200, epoch: 5 | loss: 0.0090347\n",
      "\tspeed: 0.0357s/iter; left time: 509.6380s\n",
      "\titers: 300, epoch: 5 | loss: 0.0106993\n",
      "\tspeed: 0.0355s/iter; left time: 503.3038s\n",
      "\titers: 400, epoch: 5 | loss: 0.0086455\n",
      "\tspeed: 0.0369s/iter; left time: 519.1749s\n",
      "\titers: 500, epoch: 5 | loss: 0.0094088\n",
      "\tspeed: 0.0385s/iter; left time: 538.2974s\n",
      "\titers: 600, epoch: 5 | loss: 0.0096442\n",
      "\tspeed: 0.0361s/iter; left time: 500.2638s\n",
      "\titers: 700, epoch: 5 | loss: 0.0101340\n",
      "\tspeed: 0.0370s/iter; left time: 509.9565s\n",
      "\titers: 800, epoch: 5 | loss: 0.0093720\n",
      "\tspeed: 0.0372s/iter; left time: 508.3056s\n",
      "\titers: 900, epoch: 5 | loss: 0.0095146\n",
      "\tspeed: 0.0364s/iter; left time: 493.3894s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:33.73s\n",
      "Steps: 904 | Train Loss: 0.0092743 Vali Loss: 0.0178018 Test Loss: 0.0213532\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0089636\n",
      "\tspeed: 0.1105s/iter; left time: 1487.9342s\n",
      "\titers: 200, epoch: 6 | loss: 0.0083519\n",
      "\tspeed: 0.0353s/iter; left time: 471.9524s\n",
      "\titers: 300, epoch: 6 | loss: 0.0084763\n",
      "\tspeed: 0.0375s/iter; left time: 496.9349s\n",
      "\titers: 400, epoch: 6 | loss: 0.0079713\n",
      "\tspeed: 0.0370s/iter; left time: 486.4529s\n",
      "\titers: 500, epoch: 6 | loss: 0.0071507\n",
      "\tspeed: 0.0357s/iter; left time: 466.3746s\n",
      "\titers: 600, epoch: 6 | loss: 0.0079912\n",
      "\tspeed: 0.0374s/iter; left time: 484.7501s\n",
      "\titers: 700, epoch: 6 | loss: 0.0075933\n",
      "\tspeed: 0.0368s/iter; left time: 473.3324s\n",
      "\titers: 800, epoch: 6 | loss: 0.0071945\n",
      "\tspeed: 0.0341s/iter; left time: 434.7576s\n",
      "\titers: 900, epoch: 6 | loss: 0.0090047\n",
      "\tspeed: 0.0343s/iter; left time: 434.1977s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:33.05s\n",
      "Steps: 904 | Train Loss: 0.0083276 Vali Loss: 0.0188322 Test Loss: 0.0227438\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0072072\n",
      "\tspeed: 0.1073s/iter; left time: 1346.9943s\n",
      "\titers: 200, epoch: 7 | loss: 0.0083546\n",
      "\tspeed: 0.0374s/iter; left time: 466.0270s\n",
      "\titers: 300, epoch: 7 | loss: 0.0075572\n",
      "\tspeed: 0.0379s/iter; left time: 467.7444s\n",
      "\titers: 400, epoch: 7 | loss: 0.0066608\n",
      "\tspeed: 0.0361s/iter; left time: 442.1268s\n",
      "\titers: 500, epoch: 7 | loss: 0.0066593\n",
      "\tspeed: 0.0379s/iter; left time: 460.6477s\n",
      "\titers: 600, epoch: 7 | loss: 0.0058861\n",
      "\tspeed: 0.0361s/iter; left time: 435.1961s\n",
      "\titers: 700, epoch: 7 | loss: 0.0074150\n",
      "\tspeed: 0.0374s/iter; left time: 446.6572s\n",
      "\titers: 800, epoch: 7 | loss: 0.0077773\n",
      "\tspeed: 0.0376s/iter; left time: 445.2857s\n",
      "\titers: 900, epoch: 7 | loss: 0.0078835\n",
      "\tspeed: 0.0363s/iter; left time: 426.6413s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.13s\n",
      "Steps: 904 | Train Loss: 0.0075487 Vali Loss: 0.0184792 Test Loss: 0.0213013\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0079015\n",
      "\tspeed: 0.1124s/iter; left time: 1309.9769s\n",
      "\titers: 200, epoch: 8 | loss: 0.0070421\n",
      "\tspeed: 0.0361s/iter; left time: 417.4671s\n",
      "\titers: 300, epoch: 8 | loss: 0.0068618\n",
      "\tspeed: 0.0377s/iter; left time: 431.6196s\n",
      "\titers: 400, epoch: 8 | loss: 0.0055678\n",
      "\tspeed: 0.0368s/iter; left time: 418.0457s\n",
      "\titers: 500, epoch: 8 | loss: 0.0061512\n",
      "\tspeed: 0.0377s/iter; left time: 424.6130s\n",
      "\titers: 600, epoch: 8 | loss: 0.0059672\n",
      "\tspeed: 0.0370s/iter; left time: 412.6894s\n",
      "\titers: 700, epoch: 8 | loss: 0.0058989\n",
      "\tspeed: 0.0354s/iter; left time: 391.2519s\n",
      "\titers: 800, epoch: 8 | loss: 0.0069831\n",
      "\tspeed: 0.0377s/iter; left time: 413.4566s\n",
      "\titers: 900, epoch: 8 | loss: 0.0054988\n",
      "\tspeed: 0.0385s/iter; left time: 417.7743s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:33.91s\n",
      "Steps: 904 | Train Loss: 0.0068911 Vali Loss: 0.0190678 Test Loss: 0.0223106\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_96_96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.021918583661317825, rmse:0.1480492651462555, mae:0.09203358739614487, rse:0.5726938843727112\n",
      "Intermediate time for FR and pred_len 96: 00h:05m:44.59s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_96_168_FR', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_96_168_FR_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0712789\n",
      "\tspeed: 0.0921s/iter; left time: 1652.3474s\n",
      "\titers: 200, epoch: 1 | loss: 0.0564459\n",
      "\tspeed: 0.0514s/iter; left time: 916.5102s\n",
      "\titers: 300, epoch: 1 | loss: 0.0524226\n",
      "\tspeed: 0.0510s/iter; left time: 903.9751s\n",
      "\titers: 400, epoch: 1 | loss: 0.0486759\n",
      "\tspeed: 0.0515s/iter; left time: 908.4272s\n",
      "\titers: 500, epoch: 1 | loss: 0.0526598\n",
      "\tspeed: 0.0411s/iter; left time: 721.2913s\n",
      "\titers: 600, epoch: 1 | loss: 0.0478596\n",
      "\tspeed: 0.0528s/iter; left time: 921.4439s\n",
      "\titers: 700, epoch: 1 | loss: 0.0439855\n",
      "\tspeed: 0.0506s/iter; left time: 878.1430s\n",
      "\titers: 800, epoch: 1 | loss: 0.0431464\n",
      "\tspeed: 0.0573s/iter; left time: 988.1612s\n",
      "\titers: 900, epoch: 1 | loss: 0.0428041\n",
      "\tspeed: 0.0543s/iter; left time: 929.9780s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:47.93s\n",
      "Steps: 902 | Train Loss: 0.0537082 Vali Loss: 0.0510528 Test Loss: 0.0686685\n",
      "Validation loss decreased (inf --> 0.051053).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0337284\n",
      "\tspeed: 0.1859s/iter; left time: 3167.7391s\n",
      "\titers: 200, epoch: 2 | loss: 0.0246603\n",
      "\tspeed: 0.0531s/iter; left time: 899.5291s\n",
      "\titers: 300, epoch: 2 | loss: 0.0220720\n",
      "\tspeed: 0.0530s/iter; left time: 891.9195s\n",
      "\titers: 400, epoch: 2 | loss: 0.0226117\n",
      "\tspeed: 0.0538s/iter; left time: 901.3671s\n",
      "\titers: 500, epoch: 2 | loss: 0.0250630\n",
      "\tspeed: 0.0549s/iter; left time: 913.7793s\n",
      "\titers: 600, epoch: 2 | loss: 0.0193863\n",
      "\tspeed: 0.0523s/iter; left time: 864.5525s\n",
      "\titers: 700, epoch: 2 | loss: 0.0175116\n",
      "\tspeed: 0.0524s/iter; left time: 861.0281s\n",
      "\titers: 800, epoch: 2 | loss: 0.0138841\n",
      "\tspeed: 0.0573s/iter; left time: 936.4079s\n",
      "\titers: 900, epoch: 2 | loss: 0.0140103\n",
      "\tspeed: 0.0487s/iter; left time: 790.6525s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:48.67s\n",
      "Steps: 902 | Train Loss: 0.0229412 Vali Loss: 0.0200941 Test Loss: 0.0258313\n",
      "Validation loss decreased (0.051053 --> 0.020094).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0138645\n",
      "\tspeed: 0.1628s/iter; left time: 2627.1203s\n",
      "\titers: 200, epoch: 3 | loss: 0.0152363\n",
      "\tspeed: 0.0580s/iter; left time: 930.6524s\n",
      "\titers: 300, epoch: 3 | loss: 0.0125207\n",
      "\tspeed: 0.0582s/iter; left time: 927.8613s\n",
      "\titers: 400, epoch: 3 | loss: 0.0109847\n",
      "\tspeed: 0.0609s/iter; left time: 965.1406s\n",
      "\titers: 500, epoch: 3 | loss: 0.0132744\n",
      "\tspeed: 0.0544s/iter; left time: 855.3827s\n",
      "\titers: 600, epoch: 3 | loss: 0.0139970\n",
      "\tspeed: 0.0534s/iter; left time: 834.4592s\n",
      "\titers: 700, epoch: 3 | loss: 0.0113928\n",
      "\tspeed: 0.0589s/iter; left time: 915.1318s\n",
      "\titers: 800, epoch: 3 | loss: 0.0096550\n",
      "\tspeed: 0.0585s/iter; left time: 903.6271s\n",
      "\titers: 900, epoch: 3 | loss: 0.0107050\n",
      "\tspeed: 0.0584s/iter; left time: 895.0547s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:51.87s\n",
      "Steps: 902 | Train Loss: 0.0128087 Vali Loss: 0.0184778 Test Loss: 0.0229411\n",
      "Validation loss decreased (0.020094 --> 0.018478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120290\n",
      "\tspeed: 0.1984s/iter; left time: 3021.8811s\n",
      "\titers: 200, epoch: 4 | loss: 0.0122949\n",
      "\tspeed: 0.0591s/iter; left time: 894.9164s\n",
      "\titers: 300, epoch: 4 | loss: 0.0100306\n",
      "\tspeed: 0.0508s/iter; left time: 763.9291s\n",
      "\titers: 400, epoch: 4 | loss: 0.0128027\n",
      "\tspeed: 0.0524s/iter; left time: 783.2639s\n",
      "\titers: 500, epoch: 4 | loss: 0.0108234\n",
      "\tspeed: 0.0638s/iter; left time: 946.7804s\n",
      "\titers: 600, epoch: 4 | loss: 0.0116764\n",
      "\tspeed: 0.0585s/iter; left time: 862.2161s\n",
      "\titers: 700, epoch: 4 | loss: 0.0110869\n",
      "\tspeed: 0.0514s/iter; left time: 751.8002s\n",
      "\titers: 800, epoch: 4 | loss: 0.0106050\n",
      "\tspeed: 0.0563s/iter; left time: 818.6393s\n",
      "\titers: 900, epoch: 4 | loss: 0.0101593\n",
      "\tspeed: 0.0607s/iter; left time: 876.5660s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:51.83s\n",
      "Steps: 902 | Train Loss: 0.0114168 Vali Loss: 0.0190368 Test Loss: 0.0244393\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0101887\n",
      "\tspeed: 0.1833s/iter; left time: 2626.8380s\n",
      "\titers: 200, epoch: 5 | loss: 0.0095769\n",
      "\tspeed: 0.0600s/iter; left time: 854.1281s\n",
      "\titers: 300, epoch: 5 | loss: 0.0090418\n",
      "\tspeed: 0.0576s/iter; left time: 814.4633s\n",
      "\titers: 400, epoch: 5 | loss: 0.0099594\n",
      "\tspeed: 0.0504s/iter; left time: 706.6110s\n",
      "\titers: 500, epoch: 5 | loss: 0.0107698\n",
      "\tspeed: 0.0516s/iter; left time: 718.3115s\n",
      "\titers: 600, epoch: 5 | loss: 0.0110696\n",
      "\tspeed: 0.0602s/iter; left time: 832.9217s\n",
      "\titers: 700, epoch: 5 | loss: 0.0097897\n",
      "\tspeed: 0.0586s/iter; left time: 804.6928s\n",
      "\titers: 800, epoch: 5 | loss: 0.0107613\n",
      "\tspeed: 0.0507s/iter; left time: 691.5241s\n",
      "\titers: 900, epoch: 5 | loss: 0.0091366\n",
      "\tspeed: 0.0565s/iter; left time: 764.6654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:50.25s\n",
      "Steps: 902 | Train Loss: 0.0102130 Vali Loss: 0.0209971 Test Loss: 0.0247405\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0088060\n",
      "\tspeed: 0.1894s/iter; left time: 2543.3506s\n",
      "\titers: 200, epoch: 6 | loss: 0.0118641\n",
      "\tspeed: 0.0590s/iter; left time: 785.8691s\n",
      "\titers: 300, epoch: 6 | loss: 0.0103403\n",
      "\tspeed: 0.0499s/iter; left time: 660.3317s\n",
      "\titers: 400, epoch: 6 | loss: 0.0086813\n",
      "\tspeed: 0.0567s/iter; left time: 744.4907s\n",
      "\titers: 500, epoch: 6 | loss: 0.0089356\n",
      "\tspeed: 0.0615s/iter; left time: 801.6109s\n",
      "\titers: 600, epoch: 6 | loss: 0.0079076\n",
      "\tspeed: 0.0509s/iter; left time: 657.7054s\n",
      "\titers: 700, epoch: 6 | loss: 0.0084999\n",
      "\tspeed: 0.0416s/iter; left time: 533.1942s\n",
      "\titers: 800, epoch: 6 | loss: 0.0104291\n",
      "\tspeed: 0.0393s/iter; left time: 499.9840s\n",
      "\titers: 900, epoch: 6 | loss: 0.0085110\n",
      "\tspeed: 0.0495s/iter; left time: 625.8533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:46.81s\n",
      "Steps: 902 | Train Loss: 0.0091048 Vali Loss: 0.0218596 Test Loss: 0.0251613\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0081512\n",
      "\tspeed: 0.1920s/iter; left time: 2405.6479s\n",
      "\titers: 200, epoch: 7 | loss: 0.0086582\n",
      "\tspeed: 0.0495s/iter; left time: 615.6133s\n",
      "\titers: 300, epoch: 7 | loss: 0.0078223\n",
      "\tspeed: 0.0554s/iter; left time: 683.4369s\n",
      "\titers: 400, epoch: 7 | loss: 0.0082433\n",
      "\tspeed: 0.0590s/iter; left time: 722.0930s\n",
      "\titers: 500, epoch: 7 | loss: 0.0077983\n",
      "\tspeed: 0.0543s/iter; left time: 658.1248s\n",
      "\titers: 600, epoch: 7 | loss: 0.0094143\n",
      "\tspeed: 0.0539s/iter; left time: 647.9025s\n",
      "\titers: 700, epoch: 7 | loss: 0.0069718\n",
      "\tspeed: 0.0529s/iter; left time: 631.1758s\n",
      "\titers: 800, epoch: 7 | loss: 0.0072303\n",
      "\tspeed: 0.0539s/iter; left time: 637.9092s\n",
      "\titers: 900, epoch: 7 | loss: 0.0069558\n",
      "\tspeed: 0.0574s/iter; left time: 673.1431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:49.92s\n",
      "Steps: 902 | Train Loss: 0.0082977 Vali Loss: 0.0210507 Test Loss: 0.0243219\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0077253\n",
      "\tspeed: 0.1842s/iter; left time: 2142.2477s\n",
      "\titers: 200, epoch: 8 | loss: 0.0073574\n",
      "\tspeed: 0.0517s/iter; left time: 595.8668s\n",
      "\titers: 300, epoch: 8 | loss: 0.0081805\n",
      "\tspeed: 0.0603s/iter; left time: 689.1393s\n",
      "\titers: 400, epoch: 8 | loss: 0.0069221\n",
      "\tspeed: 0.0559s/iter; left time: 633.4095s\n",
      "\titers: 500, epoch: 8 | loss: 0.0077838\n",
      "\tspeed: 0.0483s/iter; left time: 542.0257s\n",
      "\titers: 600, epoch: 8 | loss: 0.0065794\n",
      "\tspeed: 0.0556s/iter; left time: 619.2167s\n",
      "\titers: 700, epoch: 8 | loss: 0.0078354\n",
      "\tspeed: 0.0608s/iter; left time: 670.4803s\n",
      "\titers: 800, epoch: 8 | loss: 0.0080798\n",
      "\tspeed: 0.0532s/iter; left time: 581.5962s\n",
      "\titers: 900, epoch: 8 | loss: 0.0071020\n",
      "\tspeed: 0.0536s/iter; left time: 579.9905s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:49.86s\n",
      "Steps: 902 | Train Loss: 0.0075413 Vali Loss: 0.0216812 Test Loss: 0.0243932\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_96_168_FR_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02291427180171013, rmse:0.15137460827827454, mae:0.09534455090761185, rse:0.5862637162208557\n",
      "Intermediate time for FR and pred_len 168: 00h:08m:36.11s\n",
      "Intermediate time for FR: 00h:19m:45.29s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_24_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0957632\n",
      "\tspeed: 0.0709s/iter; left time: 1277.4722s\n",
      "\titers: 200, epoch: 1 | loss: 0.0817575\n",
      "\tspeed: 0.0338s/iter; left time: 606.5024s\n",
      "\titers: 300, epoch: 1 | loss: 0.0646085\n",
      "\tspeed: 0.0354s/iter; left time: 631.2071s\n",
      "\titers: 400, epoch: 1 | loss: 0.0612782\n",
      "\tspeed: 0.0329s/iter; left time: 583.1109s\n",
      "\titers: 500, epoch: 1 | loss: 0.0484514\n",
      "\tspeed: 0.0353s/iter; left time: 622.7230s\n",
      "\titers: 600, epoch: 1 | loss: 0.0481590\n",
      "\tspeed: 0.0338s/iter; left time: 592.0886s\n",
      "\titers: 700, epoch: 1 | loss: 0.0343953\n",
      "\tspeed: 0.0346s/iter; left time: 602.4717s\n",
      "\titers: 800, epoch: 1 | loss: 0.0335397\n",
      "\tspeed: 0.0377s/iter; left time: 653.3001s\n",
      "\titers: 900, epoch: 1 | loss: 0.0383372\n",
      "\tspeed: 0.0374s/iter; left time: 644.1736s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:33.47s\n",
      "Steps: 906 | Train Loss: 0.0605991 Vali Loss: 0.0236095 Test Loss: 0.0250904\n",
      "Validation loss decreased (inf --> 0.023610).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0261842\n",
      "\tspeed: 0.1100s/iter; left time: 1882.6684s\n",
      "\titers: 200, epoch: 2 | loss: 0.0226637\n",
      "\tspeed: 0.0372s/iter; left time: 632.8492s\n",
      "\titers: 300, epoch: 2 | loss: 0.0147367\n",
      "\tspeed: 0.0363s/iter; left time: 613.5112s\n",
      "\titers: 400, epoch: 2 | loss: 0.0144873\n",
      "\tspeed: 0.0345s/iter; left time: 579.4634s\n",
      "\titers: 500, epoch: 2 | loss: 0.0160467\n",
      "\tspeed: 0.0352s/iter; left time: 589.1437s\n",
      "\titers: 600, epoch: 2 | loss: 0.0193212\n",
      "\tspeed: 0.0358s/iter; left time: 594.4283s\n",
      "\titers: 700, epoch: 2 | loss: 0.0140467\n",
      "\tspeed: 0.0343s/iter; left time: 565.8549s\n",
      "\titers: 800, epoch: 2 | loss: 0.0140468\n",
      "\tspeed: 0.0359s/iter; left time: 588.7617s\n",
      "\titers: 900, epoch: 2 | loss: 0.0126574\n",
      "\tspeed: 0.0370s/iter; left time: 604.0885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:32.84s\n",
      "Steps: 906 | Train Loss: 0.0177892 Vali Loss: 0.0124438 Test Loss: 0.0138910\n",
      "Validation loss decreased (0.023610 --> 0.012444).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0111033\n",
      "\tspeed: 0.1100s/iter; left time: 1783.7594s\n",
      "\titers: 200, epoch: 3 | loss: 0.0115581\n",
      "\tspeed: 0.0366s/iter; left time: 589.7373s\n",
      "\titers: 300, epoch: 3 | loss: 0.0118196\n",
      "\tspeed: 0.0381s/iter; left time: 609.2384s\n",
      "\titers: 400, epoch: 3 | loss: 0.0087795\n",
      "\tspeed: 0.0354s/iter; left time: 563.6478s\n",
      "\titers: 500, epoch: 3 | loss: 0.0111346\n",
      "\tspeed: 0.0329s/iter; left time: 520.6840s\n",
      "\titers: 600, epoch: 3 | loss: 0.0131450\n",
      "\tspeed: 0.0349s/iter; left time: 548.0488s\n",
      "\titers: 700, epoch: 3 | loss: 0.0124399\n",
      "\tspeed: 0.0367s/iter; left time: 572.1388s\n",
      "\titers: 800, epoch: 3 | loss: 0.0134451\n",
      "\tspeed: 0.0357s/iter; left time: 553.1430s\n",
      "\titers: 900, epoch: 3 | loss: 0.0096626\n",
      "\tspeed: 0.0325s/iter; left time: 500.4454s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:32.34s\n",
      "Steps: 906 | Train Loss: 0.0118055 Vali Loss: 0.0124286 Test Loss: 0.0138577\n",
      "Validation loss decreased (0.012444 --> 0.012429).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0127130\n",
      "\tspeed: 0.0973s/iter; left time: 1489.5018s\n",
      "\titers: 200, epoch: 4 | loss: 0.0096245\n",
      "\tspeed: 0.0319s/iter; left time: 484.3129s\n",
      "\titers: 300, epoch: 4 | loss: 0.0105408\n",
      "\tspeed: 0.0348s/iter; left time: 526.0765s\n",
      "\titers: 400, epoch: 4 | loss: 0.0131704\n",
      "\tspeed: 0.0346s/iter; left time: 518.8553s\n",
      "\titers: 500, epoch: 4 | loss: 0.0106625\n",
      "\tspeed: 0.0351s/iter; left time: 523.1579s\n",
      "\titers: 600, epoch: 4 | loss: 0.0090194\n",
      "\tspeed: 0.0365s/iter; left time: 540.1050s\n",
      "\titers: 700, epoch: 4 | loss: 0.0096538\n",
      "\tspeed: 0.0351s/iter; left time: 515.4415s\n",
      "\titers: 800, epoch: 4 | loss: 0.0114234\n",
      "\tspeed: 0.0330s/iter; left time: 481.6343s\n",
      "\titers: 900, epoch: 4 | loss: 0.0113872\n",
      "\tspeed: 0.0319s/iter; left time: 462.5542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:31.24s\n",
      "Steps: 906 | Train Loss: 0.0106975 Vali Loss: 0.0112019 Test Loss: 0.0123353\n",
      "Validation loss decreased (0.012429 --> 0.011202).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0091417\n",
      "\tspeed: 0.1134s/iter; left time: 1632.2343s\n",
      "\titers: 200, epoch: 5 | loss: 0.0132211\n",
      "\tspeed: 0.0334s/iter; left time: 477.1699s\n",
      "\titers: 300, epoch: 5 | loss: 0.0108337\n",
      "\tspeed: 0.0342s/iter; left time: 484.9021s\n",
      "\titers: 400, epoch: 5 | loss: 0.0088053\n",
      "\tspeed: 0.0329s/iter; left time: 464.2612s\n",
      "\titers: 500, epoch: 5 | loss: 0.0094521\n",
      "\tspeed: 0.0351s/iter; left time: 491.2856s\n",
      "\titers: 600, epoch: 5 | loss: 0.0083584\n",
      "\tspeed: 0.0361s/iter; left time: 502.3690s\n",
      "\titers: 700, epoch: 5 | loss: 0.0135315\n",
      "\tspeed: 0.0335s/iter; left time: 462.7512s\n",
      "\titers: 800, epoch: 5 | loss: 0.0068324\n",
      "\tspeed: 0.0321s/iter; left time: 439.3836s\n",
      "\titers: 900, epoch: 5 | loss: 0.0104479\n",
      "\tspeed: 0.0327s/iter; left time: 444.6534s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:31.15s\n",
      "Steps: 906 | Train Loss: 0.0097477 Vali Loss: 0.0103169 Test Loss: 0.0118225\n",
      "Validation loss decreased (0.011202 --> 0.010317).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0084765\n",
      "\tspeed: 0.1093s/iter; left time: 1473.9824s\n",
      "\titers: 200, epoch: 6 | loss: 0.0092567\n",
      "\tspeed: 0.0334s/iter; left time: 447.7382s\n",
      "\titers: 300, epoch: 6 | loss: 0.0127090\n",
      "\tspeed: 0.0331s/iter; left time: 440.3307s\n",
      "\titers: 400, epoch: 6 | loss: 0.0078653\n",
      "\tspeed: 0.0324s/iter; left time: 427.4558s\n",
      "\titers: 500, epoch: 6 | loss: 0.0061968\n",
      "\tspeed: 0.0346s/iter; left time: 452.8788s\n",
      "\titers: 600, epoch: 6 | loss: 0.0094398\n",
      "\tspeed: 0.0362s/iter; left time: 470.1577s\n",
      "\titers: 700, epoch: 6 | loss: 0.0095741\n",
      "\tspeed: 0.0332s/iter; left time: 427.3429s\n",
      "\titers: 800, epoch: 6 | loss: 0.0084911\n",
      "\tspeed: 0.0320s/iter; left time: 409.7873s\n",
      "\titers: 900, epoch: 6 | loss: 0.0074697\n",
      "\tspeed: 0.0329s/iter; left time: 418.1478s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:30.93s\n",
      "Steps: 906 | Train Loss: 0.0089266 Vali Loss: 0.0098866 Test Loss: 0.0116170\n",
      "Validation loss decreased (0.010317 --> 0.009887).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0078370\n",
      "\tspeed: 0.1131s/iter; left time: 1423.2426s\n",
      "\titers: 200, epoch: 7 | loss: 0.0097201\n",
      "\tspeed: 0.0347s/iter; left time: 433.6008s\n",
      "\titers: 300, epoch: 7 | loss: 0.0068884\n",
      "\tspeed: 0.0329s/iter; left time: 408.0298s\n",
      "\titers: 400, epoch: 7 | loss: 0.0094338\n",
      "\tspeed: 0.0340s/iter; left time: 418.0453s\n",
      "\titers: 500, epoch: 7 | loss: 0.0086338\n",
      "\tspeed: 0.0347s/iter; left time: 422.8576s\n",
      "\titers: 600, epoch: 7 | loss: 0.0053967\n",
      "\tspeed: 0.0341s/iter; left time: 412.4029s\n",
      "\titers: 700, epoch: 7 | loss: 0.0085320\n",
      "\tspeed: 0.0329s/iter; left time: 393.8772s\n",
      "\titers: 800, epoch: 7 | loss: 0.0095296\n",
      "\tspeed: 0.0314s/iter; left time: 373.6059s\n",
      "\titers: 900, epoch: 7 | loss: 0.0069363\n",
      "\tspeed: 0.0331s/iter; left time: 389.5835s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:30.78s\n",
      "Steps: 906 | Train Loss: 0.0083970 Vali Loss: 0.0095849 Test Loss: 0.0113854\n",
      "Validation loss decreased (0.009887 --> 0.009585).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0068547\n",
      "\tspeed: 0.1201s/iter; left time: 1403.0021s\n",
      "\titers: 200, epoch: 8 | loss: 0.0062880\n",
      "\tspeed: 0.0327s/iter; left time: 378.1468s\n",
      "\titers: 300, epoch: 8 | loss: 0.0063780\n",
      "\tspeed: 0.0362s/iter; left time: 414.9861s\n",
      "\titers: 400, epoch: 8 | loss: 0.0076308\n",
      "\tspeed: 0.0342s/iter; left time: 388.6866s\n",
      "\titers: 500, epoch: 8 | loss: 0.0084253\n",
      "\tspeed: 0.0334s/iter; left time: 376.4441s\n",
      "\titers: 600, epoch: 8 | loss: 0.0079748\n",
      "\tspeed: 0.0329s/iter; left time: 367.5133s\n",
      "\titers: 700, epoch: 8 | loss: 0.0071999\n",
      "\tspeed: 0.0372s/iter; left time: 412.4207s\n",
      "\titers: 800, epoch: 8 | loss: 0.0082351\n",
      "\tspeed: 0.0326s/iter; left time: 357.8190s\n",
      "\titers: 900, epoch: 8 | loss: 0.0076345\n",
      "\tspeed: 0.0357s/iter; left time: 388.4410s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:31.38s\n",
      "Steps: 906 | Train Loss: 0.0078698 Vali Loss: 0.0095649 Test Loss: 0.0113864\n",
      "Validation loss decreased (0.009585 --> 0.009565).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0054177\n",
      "\tspeed: 0.1082s/iter; left time: 1165.8282s\n",
      "\titers: 200, epoch: 9 | loss: 0.0087093\n",
      "\tspeed: 0.0327s/iter; left time: 348.4798s\n",
      "\titers: 300, epoch: 9 | loss: 0.0088883\n",
      "\tspeed: 0.0364s/iter; left time: 384.6554s\n",
      "\titers: 400, epoch: 9 | loss: 0.0067530\n",
      "\tspeed: 0.0352s/iter; left time: 368.7421s\n",
      "\titers: 500, epoch: 9 | loss: 0.0068062\n",
      "\tspeed: 0.0316s/iter; left time: 327.8233s\n",
      "\titers: 600, epoch: 9 | loss: 0.0070813\n",
      "\tspeed: 0.0325s/iter; left time: 333.4549s\n",
      "\titers: 700, epoch: 9 | loss: 0.0077914\n",
      "\tspeed: 0.0356s/iter; left time: 362.5394s\n",
      "\titers: 800, epoch: 9 | loss: 0.0067969\n",
      "\tspeed: 0.0348s/iter; left time: 350.3827s\n",
      "\titers: 900, epoch: 9 | loss: 0.0065149\n",
      "\tspeed: 0.0340s/iter; left time: 339.3087s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:31.19s\n",
      "Steps: 906 | Train Loss: 0.0073633 Vali Loss: 0.0106109 Test Loss: 0.0119681\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0078787\n",
      "\tspeed: 0.0947s/iter; left time: 933.9321s\n",
      "\titers: 200, epoch: 10 | loss: 0.0073015\n",
      "\tspeed: 0.0334s/iter; left time: 326.6346s\n",
      "\titers: 300, epoch: 10 | loss: 0.0071232\n",
      "\tspeed: 0.0311s/iter; left time: 300.8265s\n",
      "\titers: 400, epoch: 10 | loss: 0.0069564\n",
      "\tspeed: 0.0325s/iter; left time: 310.9176s\n",
      "\titers: 500, epoch: 10 | loss: 0.0055014\n",
      "\tspeed: 0.0346s/iter; left time: 327.3613s\n",
      "\titers: 600, epoch: 10 | loss: 0.0067581\n",
      "\tspeed: 0.0330s/iter; left time: 309.2437s\n",
      "\titers: 700, epoch: 10 | loss: 0.0056662\n",
      "\tspeed: 0.0327s/iter; left time: 303.4657s\n",
      "\titers: 800, epoch: 10 | loss: 0.0064611\n",
      "\tspeed: 0.0332s/iter; left time: 303.9435s\n",
      "\titers: 900, epoch: 10 | loss: 0.0061233\n",
      "\tspeed: 0.0347s/iter; left time: 314.2948s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:30.39s\n",
      "Steps: 906 | Train Loss: 0.0069422 Vali Loss: 0.0097545 Test Loss: 0.0121330\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0067805\n",
      "\tspeed: 0.1014s/iter; left time: 908.4260s\n",
      "\titers: 200, epoch: 11 | loss: 0.0054183\n",
      "\tspeed: 0.0323s/iter; left time: 286.2677s\n",
      "\titers: 300, epoch: 11 | loss: 0.0069844\n",
      "\tspeed: 0.0318s/iter; left time: 278.5425s\n",
      "\titers: 400, epoch: 11 | loss: 0.0054563\n",
      "\tspeed: 0.0348s/iter; left time: 301.7368s\n",
      "\titers: 500, epoch: 11 | loss: 0.0078667\n",
      "\tspeed: 0.0345s/iter; left time: 295.5915s\n",
      "\titers: 600, epoch: 11 | loss: 0.0084661\n",
      "\tspeed: 0.0318s/iter; left time: 269.2762s\n",
      "\titers: 700, epoch: 11 | loss: 0.0064551\n",
      "\tspeed: 0.0317s/iter; left time: 265.2217s\n",
      "\titers: 800, epoch: 11 | loss: 0.0078192\n",
      "\tspeed: 0.0353s/iter; left time: 291.3421s\n",
      "\titers: 900, epoch: 11 | loss: 0.0064657\n",
      "\tspeed: 0.0352s/iter; left time: 287.3652s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:31.04s\n",
      "Steps: 906 | Train Loss: 0.0065644 Vali Loss: 0.0102390 Test Loss: 0.0123357\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0049833\n",
      "\tspeed: 0.0992s/iter; left time: 799.1754s\n",
      "\titers: 200, epoch: 12 | loss: 0.0075870\n",
      "\tspeed: 0.0328s/iter; left time: 260.8206s\n",
      "\titers: 300, epoch: 12 | loss: 0.0057872\n",
      "\tspeed: 0.0325s/iter; left time: 255.2413s\n",
      "\titers: 400, epoch: 12 | loss: 0.0063423\n",
      "\tspeed: 0.0342s/iter; left time: 265.5325s\n",
      "\titers: 500, epoch: 12 | loss: 0.0083102\n",
      "\tspeed: 0.0329s/iter; left time: 252.0987s\n",
      "\titers: 600, epoch: 12 | loss: 0.0066547\n",
      "\tspeed: 0.0330s/iter; left time: 249.4184s\n",
      "\titers: 700, epoch: 12 | loss: 0.0055785\n",
      "\tspeed: 0.0330s/iter; left time: 245.9778s\n",
      "\titers: 800, epoch: 12 | loss: 0.0077896\n",
      "\tspeed: 0.0340s/iter; left time: 250.0052s\n",
      "\titers: 900, epoch: 12 | loss: 0.0061444\n",
      "\tspeed: 0.0318s/iter; left time: 230.3781s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:30.70s\n",
      "Steps: 906 | Train Loss: 0.0062224 Vali Loss: 0.0102077 Test Loss: 0.0127671\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0060006\n",
      "\tspeed: 0.1032s/iter; left time: 737.7209s\n",
      "\titers: 200, epoch: 13 | loss: 0.0065340\n",
      "\tspeed: 0.0312s/iter; left time: 220.2470s\n",
      "\titers: 300, epoch: 13 | loss: 0.0052825\n",
      "\tspeed: 0.0312s/iter; left time: 217.1292s\n",
      "\titers: 400, epoch: 13 | loss: 0.0075953\n",
      "\tspeed: 0.0333s/iter; left time: 228.3622s\n",
      "\titers: 500, epoch: 13 | loss: 0.0046650\n",
      "\tspeed: 0.0352s/iter; left time: 237.7721s\n",
      "\titers: 600, epoch: 13 | loss: 0.0054308\n",
      "\tspeed: 0.0305s/iter; left time: 202.9122s\n",
      "\titers: 700, epoch: 13 | loss: 0.0058771\n",
      "\tspeed: 0.0326s/iter; left time: 213.7939s\n",
      "\titers: 800, epoch: 13 | loss: 0.0050994\n",
      "\tspeed: 0.0357s/iter; left time: 230.4652s\n",
      "\titers: 900, epoch: 13 | loss: 0.0066943\n",
      "\tspeed: 0.0329s/iter; left time: 208.6631s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:30.50s\n",
      "Steps: 906 | Train Loss: 0.0058265 Vali Loss: 0.0103207 Test Loss: 0.0131232\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01137869618833065, rmse:0.10667096823453903, mae:0.06598816812038422, rse:0.40311703085899353\n",
      "Intermediate time for IT and pred_len 24: 00h:08m:25.79s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_96_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1052735\n",
      "\tspeed: 0.0850s/iter; left time: 1527.9730s\n",
      "\titers: 200, epoch: 1 | loss: 0.0845075\n",
      "\tspeed: 0.0365s/iter; left time: 652.0613s\n",
      "\titers: 300, epoch: 1 | loss: 0.0778775\n",
      "\tspeed: 0.0352s/iter; left time: 626.1546s\n",
      "\titers: 400, epoch: 1 | loss: 0.0722261\n",
      "\tspeed: 0.0364s/iter; left time: 643.2556s\n",
      "\titers: 500, epoch: 1 | loss: 0.0733849\n",
      "\tspeed: 0.0368s/iter; left time: 647.2547s\n",
      "\titers: 600, epoch: 1 | loss: 0.0649374\n",
      "\tspeed: 0.0348s/iter; left time: 608.9235s\n",
      "\titers: 700, epoch: 1 | loss: 0.0627303\n",
      "\tspeed: 0.0355s/iter; left time: 617.1511s\n",
      "\titers: 800, epoch: 1 | loss: 0.0568795\n",
      "\tspeed: 0.0373s/iter; left time: 644.4195s\n",
      "\titers: 900, epoch: 1 | loss: 0.0588607\n",
      "\tspeed: 0.0374s/iter; left time: 642.6338s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:34.10s\n",
      "Steps: 904 | Train Loss: 0.0770370 Vali Loss: 0.0422612 Test Loss: 0.0492609\n",
      "Validation loss decreased (inf --> 0.042261).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0423253\n",
      "\tspeed: 0.1105s/iter; left time: 1886.8542s\n",
      "\titers: 200, epoch: 2 | loss: 0.0328577\n",
      "\tspeed: 0.0348s/iter; left time: 591.3248s\n",
      "\titers: 300, epoch: 2 | loss: 0.0310498\n",
      "\tspeed: 0.0361s/iter; left time: 608.5395s\n",
      "\titers: 400, epoch: 2 | loss: 0.0267299\n",
      "\tspeed: 0.0373s/iter; left time: 625.1138s\n",
      "\titers: 500, epoch: 2 | loss: 0.0250892\n",
      "\tspeed: 0.0360s/iter; left time: 600.0345s\n",
      "\titers: 600, epoch: 2 | loss: 0.0215443\n",
      "\tspeed: 0.0369s/iter; left time: 612.4137s\n",
      "\titers: 700, epoch: 2 | loss: 0.0228624\n",
      "\tspeed: 0.0377s/iter; left time: 620.7616s\n",
      "\titers: 800, epoch: 2 | loss: 0.0199976\n",
      "\tspeed: 0.0432s/iter; left time: 708.0240s\n",
      "\titers: 900, epoch: 2 | loss: 0.0192713\n",
      "\tspeed: 0.0431s/iter; left time: 701.2421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:34.80s\n",
      "Steps: 904 | Train Loss: 0.0281857 Vali Loss: 0.0192857 Test Loss: 0.0213712\n",
      "Validation loss decreased (0.042261 --> 0.019286).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0187188\n",
      "\tspeed: 0.1024s/iter; left time: 1655.7202s\n",
      "\titers: 200, epoch: 3 | loss: 0.0193838\n",
      "\tspeed: 0.0351s/iter; left time: 563.7247s\n",
      "\titers: 300, epoch: 3 | loss: 0.0201494\n",
      "\tspeed: 0.0340s/iter; left time: 542.3290s\n",
      "\titers: 400, epoch: 3 | loss: 0.0181761\n",
      "\tspeed: 0.0344s/iter; left time: 546.6556s\n",
      "\titers: 500, epoch: 3 | loss: 0.0159714\n",
      "\tspeed: 0.0340s/iter; left time: 535.8257s\n",
      "\titers: 600, epoch: 3 | loss: 0.0182038\n",
      "\tspeed: 0.0342s/iter; left time: 535.2584s\n",
      "\titers: 700, epoch: 3 | loss: 0.0159824\n",
      "\tspeed: 0.0342s/iter; left time: 532.9489s\n",
      "\titers: 800, epoch: 3 | loss: 0.0199805\n",
      "\tspeed: 0.0354s/iter; left time: 548.0319s\n",
      "\titers: 900, epoch: 3 | loss: 0.0154733\n",
      "\tspeed: 0.0338s/iter; left time: 519.7243s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:31.45s\n",
      "Steps: 904 | Train Loss: 0.0183871 Vali Loss: 0.0164777 Test Loss: 0.0187503\n",
      "Validation loss decreased (0.019286 --> 0.016478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0155618\n",
      "\tspeed: 0.1050s/iter; left time: 1603.6096s\n",
      "\titers: 200, epoch: 4 | loss: 0.0181367\n",
      "\tspeed: 0.0342s/iter; left time: 518.7999s\n",
      "\titers: 300, epoch: 4 | loss: 0.0170885\n",
      "\tspeed: 0.0342s/iter; left time: 515.9864s\n",
      "\titers: 400, epoch: 4 | loss: 0.0159648\n",
      "\tspeed: 0.0341s/iter; left time: 510.8793s\n",
      "\titers: 500, epoch: 4 | loss: 0.0150804\n",
      "\tspeed: 0.0340s/iter; left time: 505.8713s\n",
      "\titers: 600, epoch: 4 | loss: 0.0142211\n",
      "\tspeed: 0.0334s/iter; left time: 493.0794s\n",
      "\titers: 700, epoch: 4 | loss: 0.0189483\n",
      "\tspeed: 0.0350s/iter; left time: 513.6063s\n",
      "\titers: 800, epoch: 4 | loss: 0.0140306\n",
      "\tspeed: 0.0349s/iter; left time: 508.0224s\n",
      "\titers: 900, epoch: 4 | loss: 0.0153686\n",
      "\tspeed: 0.0340s/iter; left time: 491.4557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:31.40s\n",
      "Steps: 904 | Train Loss: 0.0166229 Vali Loss: 0.0166674 Test Loss: 0.0183428\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0166662\n",
      "\tspeed: 0.0984s/iter; left time: 1413.9751s\n",
      "\titers: 200, epoch: 5 | loss: 0.0135682\n",
      "\tspeed: 0.0341s/iter; left time: 485.7783s\n",
      "\titers: 300, epoch: 5 | loss: 0.0158774\n",
      "\tspeed: 0.0338s/iter; left time: 478.6189s\n",
      "\titers: 400, epoch: 5 | loss: 0.0165986\n",
      "\tspeed: 0.0331s/iter; left time: 465.6397s\n",
      "\titers: 500, epoch: 5 | loss: 0.0149370\n",
      "\tspeed: 0.0350s/iter; left time: 488.1823s\n",
      "\titers: 600, epoch: 5 | loss: 0.0161001\n",
      "\tspeed: 0.0340s/iter; left time: 471.1151s\n",
      "\titers: 700, epoch: 5 | loss: 0.0142459\n",
      "\tspeed: 0.0354s/iter; left time: 487.4808s\n",
      "\titers: 800, epoch: 5 | loss: 0.0150630\n",
      "\tspeed: 0.0343s/iter; left time: 468.1941s\n",
      "\titers: 900, epoch: 5 | loss: 0.0157289\n",
      "\tspeed: 0.0361s/iter; left time: 489.2616s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:31.54s\n",
      "Steps: 904 | Train Loss: 0.0152686 Vali Loss: 0.0166363 Test Loss: 0.0196131\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0172512\n",
      "\tspeed: 0.0995s/iter; left time: 1339.8299s\n",
      "\titers: 200, epoch: 6 | loss: 0.0152996\n",
      "\tspeed: 0.0336s/iter; left time: 448.5455s\n",
      "\titers: 300, epoch: 6 | loss: 0.0147258\n",
      "\tspeed: 0.0342s/iter; left time: 454.0363s\n",
      "\titers: 400, epoch: 6 | loss: 0.0128632\n",
      "\tspeed: 0.0347s/iter; left time: 456.7381s\n",
      "\titers: 500, epoch: 6 | loss: 0.0138966\n",
      "\tspeed: 0.0338s/iter; left time: 440.9587s\n",
      "\titers: 600, epoch: 6 | loss: 0.0140565\n",
      "\tspeed: 0.0350s/iter; left time: 453.9571s\n",
      "\titers: 700, epoch: 6 | loss: 0.0149950\n",
      "\tspeed: 0.0340s/iter; left time: 437.2851s\n",
      "\titers: 800, epoch: 6 | loss: 0.0133245\n",
      "\tspeed: 0.0344s/iter; left time: 438.4345s\n",
      "\titers: 900, epoch: 6 | loss: 0.0130404\n",
      "\tspeed: 0.0341s/iter; left time: 432.1065s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:31.41s\n",
      "Steps: 904 | Train Loss: 0.0141668 Vali Loss: 0.0181570 Test Loss: 0.0206778\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0123304\n",
      "\tspeed: 0.1006s/iter; left time: 1263.1123s\n",
      "\titers: 200, epoch: 7 | loss: 0.0136654\n",
      "\tspeed: 0.0351s/iter; left time: 437.7539s\n",
      "\titers: 300, epoch: 7 | loss: 0.0142162\n",
      "\tspeed: 0.0339s/iter; left time: 418.6839s\n",
      "\titers: 400, epoch: 7 | loss: 0.0128690\n",
      "\tspeed: 0.0376s/iter; left time: 460.7830s\n",
      "\titers: 500, epoch: 7 | loss: 0.0128486\n",
      "\tspeed: 0.0441s/iter; left time: 535.7995s\n",
      "\titers: 600, epoch: 7 | loss: 0.0104035\n",
      "\tspeed: 0.0470s/iter; left time: 566.8150s\n",
      "\titers: 700, epoch: 7 | loss: 0.0125170\n",
      "\tspeed: 0.0354s/iter; left time: 423.2105s\n",
      "\titers: 800, epoch: 7 | loss: 0.0103865\n",
      "\tspeed: 0.0335s/iter; left time: 397.0982s\n",
      "\titers: 900, epoch: 7 | loss: 0.0143755\n",
      "\tspeed: 0.0352s/iter; left time: 413.5079s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.22s\n",
      "Steps: 904 | Train Loss: 0.0130236 Vali Loss: 0.0173678 Test Loss: 0.0208934\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0150618\n",
      "\tspeed: 0.1098s/iter; left time: 1279.4604s\n",
      "\titers: 200, epoch: 8 | loss: 0.0120275\n",
      "\tspeed: 0.0351s/iter; left time: 405.2959s\n",
      "\titers: 300, epoch: 8 | loss: 0.0121574\n",
      "\tspeed: 0.0347s/iter; left time: 396.9476s\n",
      "\titers: 400, epoch: 8 | loss: 0.0106792\n",
      "\tspeed: 0.0338s/iter; left time: 383.5012s\n",
      "\titers: 500, epoch: 8 | loss: 0.0111393\n",
      "\tspeed: 0.0341s/iter; left time: 383.3223s\n",
      "\titers: 600, epoch: 8 | loss: 0.0108992\n",
      "\tspeed: 0.0357s/iter; left time: 397.9531s\n",
      "\titers: 700, epoch: 8 | loss: 0.0118494\n",
      "\tspeed: 0.0349s/iter; left time: 386.1100s\n",
      "\titers: 800, epoch: 8 | loss: 0.0115658\n",
      "\tspeed: 0.0349s/iter; left time: 382.1861s\n",
      "\titers: 900, epoch: 8 | loss: 0.0098825\n",
      "\tspeed: 0.0345s/iter; left time: 374.1554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:32.77s\n",
      "Steps: 904 | Train Loss: 0.0118994 Vali Loss: 0.0178741 Test Loss: 0.0224808\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018757950514554977, rmse:0.13695967197418213, mae:0.09164635092020035, rse:0.5178592801094055\n",
      "Intermediate time for IT and pred_len 96: 00h:05m:30.33s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_168_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_168_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1080960\n",
      "\tspeed: 0.0668s/iter; left time: 1198.2046s\n",
      "\titers: 200, epoch: 1 | loss: 0.0824163\n",
      "\tspeed: 0.0410s/iter; left time: 730.6883s\n",
      "\titers: 300, epoch: 1 | loss: 0.0842993\n",
      "\tspeed: 0.0466s/iter; left time: 826.2555s\n",
      "\titers: 400, epoch: 1 | loss: 0.0736092\n",
      "\tspeed: 0.0436s/iter; left time: 768.7929s\n",
      "\titers: 500, epoch: 1 | loss: 0.0733240\n",
      "\tspeed: 0.0486s/iter; left time: 851.9585s\n",
      "\titers: 600, epoch: 1 | loss: 0.0706275\n",
      "\tspeed: 0.0447s/iter; left time: 780.0874s\n",
      "\titers: 700, epoch: 1 | loss: 0.0656435\n",
      "\tspeed: 0.0461s/iter; left time: 799.3103s\n",
      "\titers: 800, epoch: 1 | loss: 0.0658726\n",
      "\tspeed: 0.0491s/iter; left time: 847.3773s\n",
      "\titers: 900, epoch: 1 | loss: 0.0630696\n",
      "\tspeed: 0.0458s/iter; left time: 785.7586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:41.95s\n",
      "Steps: 902 | Train Loss: 0.0786550 Vali Loss: 0.0498637 Test Loss: 0.0583538\n",
      "Validation loss decreased (inf --> 0.049864).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0482759\n",
      "\tspeed: 0.1476s/iter; left time: 2514.7456s\n",
      "\titers: 200, epoch: 2 | loss: 0.0421801\n",
      "\tspeed: 0.0418s/iter; left time: 708.1329s\n",
      "\titers: 300, epoch: 2 | loss: 0.0385429\n",
      "\tspeed: 0.0454s/iter; left time: 764.3878s\n",
      "\titers: 400, epoch: 2 | loss: 0.0367902\n",
      "\tspeed: 0.0408s/iter; left time: 682.9286s\n",
      "\titers: 500, epoch: 2 | loss: 0.0308838\n",
      "\tspeed: 0.0421s/iter; left time: 701.2395s\n",
      "\titers: 600, epoch: 2 | loss: 0.0255439\n",
      "\tspeed: 0.0423s/iter; left time: 699.4867s\n",
      "\titers: 700, epoch: 2 | loss: 0.0225206\n",
      "\tspeed: 0.0465s/iter; left time: 764.8655s\n",
      "\titers: 800, epoch: 2 | loss: 0.0219971\n",
      "\tspeed: 0.0484s/iter; left time: 790.0311s\n",
      "\titers: 900, epoch: 2 | loss: 0.0218719\n",
      "\tspeed: 0.0456s/iter; left time: 740.5256s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:40.16s\n",
      "Steps: 902 | Train Loss: 0.0340902 Vali Loss: 0.0191990 Test Loss: 0.0217300\n",
      "Validation loss decreased (0.049864 --> 0.019199).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0235283\n",
      "\tspeed: 0.1423s/iter; left time: 2296.4848s\n",
      "\titers: 200, epoch: 3 | loss: 0.0206867\n",
      "\tspeed: 0.0407s/iter; left time: 653.2566s\n",
      "\titers: 300, epoch: 3 | loss: 0.0206621\n",
      "\tspeed: 0.0485s/iter; left time: 773.1418s\n",
      "\titers: 400, epoch: 3 | loss: 0.0177734\n",
      "\tspeed: 0.0419s/iter; left time: 663.6464s\n",
      "\titers: 500, epoch: 3 | loss: 0.0205937\n",
      "\tspeed: 0.0455s/iter; left time: 716.7677s\n",
      "\titers: 600, epoch: 3 | loss: 0.0205176\n",
      "\tspeed: 0.0456s/iter; left time: 713.0480s\n",
      "\titers: 700, epoch: 3 | loss: 0.0188191\n",
      "\tspeed: 0.0452s/iter; left time: 701.6847s\n",
      "\titers: 800, epoch: 3 | loss: 0.0180016\n",
      "\tspeed: 0.0459s/iter; left time: 708.7676s\n",
      "\titers: 900, epoch: 3 | loss: 0.0204296\n",
      "\tspeed: 0.0446s/iter; left time: 683.8174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:40.36s\n",
      "Steps: 902 | Train Loss: 0.0200820 Vali Loss: 0.0191245 Test Loss: 0.0210825\n",
      "Validation loss decreased (0.019199 --> 0.019124).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0196903\n",
      "\tspeed: 0.1273s/iter; left time: 1940.0853s\n",
      "\titers: 200, epoch: 4 | loss: 0.0173575\n",
      "\tspeed: 0.0442s/iter; left time: 668.9377s\n",
      "\titers: 300, epoch: 4 | loss: 0.0186932\n",
      "\tspeed: 0.0444s/iter; left time: 667.0606s\n",
      "\titers: 400, epoch: 4 | loss: 0.0181120\n",
      "\tspeed: 0.0450s/iter; left time: 671.6515s\n",
      "\titers: 500, epoch: 4 | loss: 0.0186082\n",
      "\tspeed: 0.0441s/iter; left time: 654.0079s\n",
      "\titers: 600, epoch: 4 | loss: 0.0175178\n",
      "\tspeed: 0.0495s/iter; left time: 728.8358s\n",
      "\titers: 700, epoch: 4 | loss: 0.0184030\n",
      "\tspeed: 0.0444s/iter; left time: 650.0754s\n",
      "\titers: 800, epoch: 4 | loss: 0.0189094\n",
      "\tspeed: 0.0421s/iter; left time: 611.4035s\n",
      "\titers: 900, epoch: 4 | loss: 0.0182288\n",
      "\tspeed: 0.0467s/iter; left time: 673.4291s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:41.26s\n",
      "Steps: 902 | Train Loss: 0.0183361 Vali Loss: 0.0185760 Test Loss: 0.0211089\n",
      "Validation loss decreased (0.019124 --> 0.018576).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0169285\n",
      "\tspeed: 0.1335s/iter; left time: 1913.8118s\n",
      "\titers: 200, epoch: 5 | loss: 0.0162984\n",
      "\tspeed: 0.0455s/iter; left time: 647.1148s\n",
      "\titers: 300, epoch: 5 | loss: 0.0175885\n",
      "\tspeed: 0.0444s/iter; left time: 627.6361s\n",
      "\titers: 400, epoch: 5 | loss: 0.0168064\n",
      "\tspeed: 0.0421s/iter; left time: 590.5830s\n",
      "\titers: 500, epoch: 5 | loss: 0.0158014\n",
      "\tspeed: 0.0451s/iter; left time: 628.0847s\n",
      "\titers: 600, epoch: 5 | loss: 0.0175041\n",
      "\tspeed: 0.0419s/iter; left time: 579.9780s\n",
      "\titers: 700, epoch: 5 | loss: 0.0175251\n",
      "\tspeed: 0.0464s/iter; left time: 636.5337s\n",
      "\titers: 800, epoch: 5 | loss: 0.0160738\n",
      "\tspeed: 0.0449s/iter; left time: 612.4442s\n",
      "\titers: 900, epoch: 5 | loss: 0.0180332\n",
      "\tspeed: 0.0436s/iter; left time: 590.5206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:40.40s\n",
      "Steps: 902 | Train Loss: 0.0168270 Vali Loss: 0.0191929 Test Loss: 0.0207321\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0170055\n",
      "\tspeed: 0.1245s/iter; left time: 1672.5048s\n",
      "\titers: 200, epoch: 6 | loss: 0.0187532\n",
      "\tspeed: 0.0473s/iter; left time: 630.4814s\n",
      "\titers: 300, epoch: 6 | loss: 0.0166333\n",
      "\tspeed: 0.0445s/iter; left time: 588.8341s\n",
      "\titers: 400, epoch: 6 | loss: 0.0146327\n",
      "\tspeed: 0.0452s/iter; left time: 594.0151s\n",
      "\titers: 500, epoch: 6 | loss: 0.0151519\n",
      "\tspeed: 0.0455s/iter; left time: 593.2302s\n",
      "\titers: 600, epoch: 6 | loss: 0.0148442\n",
      "\tspeed: 0.0422s/iter; left time: 546.0029s\n",
      "\titers: 700, epoch: 6 | loss: 0.0146263\n",
      "\tspeed: 0.0452s/iter; left time: 579.5969s\n",
      "\titers: 800, epoch: 6 | loss: 0.0176119\n",
      "\tspeed: 0.0457s/iter; left time: 582.4178s\n",
      "\titers: 900, epoch: 6 | loss: 0.0159310\n",
      "\tspeed: 0.0448s/iter; left time: 565.2726s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:41.05s\n",
      "Steps: 902 | Train Loss: 0.0153580 Vali Loss: 0.0195957 Test Loss: 0.0217750\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0179181\n",
      "\tspeed: 0.1228s/iter; left time: 1538.5915s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150460\n",
      "\tspeed: 0.0460s/iter; left time: 571.9600s\n",
      "\titers: 300, epoch: 7 | loss: 0.0146198\n",
      "\tspeed: 0.0449s/iter; left time: 553.1517s\n",
      "\titers: 400, epoch: 7 | loss: 0.0149516\n",
      "\tspeed: 0.0448s/iter; left time: 547.4800s\n",
      "\titers: 500, epoch: 7 | loss: 0.0124086\n",
      "\tspeed: 0.0439s/iter; left time: 532.2308s\n",
      "\titers: 600, epoch: 7 | loss: 0.0153495\n",
      "\tspeed: 0.0469s/iter; left time: 564.0056s\n",
      "\titers: 700, epoch: 7 | loss: 0.0124983\n",
      "\tspeed: 0.0521s/iter; left time: 621.8838s\n",
      "\titers: 800, epoch: 7 | loss: 0.0126885\n",
      "\tspeed: 0.0520s/iter; left time: 615.5218s\n",
      "\titers: 900, epoch: 7 | loss: 0.0129139\n",
      "\tspeed: 0.0520s/iter; left time: 609.3312s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:43.18s\n",
      "Steps: 902 | Train Loss: 0.0140290 Vali Loss: 0.0201149 Test Loss: 0.0227018\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0139799\n",
      "\tspeed: 0.1212s/iter; left time: 1409.4176s\n",
      "\titers: 200, epoch: 8 | loss: 0.0130178\n",
      "\tspeed: 0.0517s/iter; left time: 595.7647s\n",
      "\titers: 300, epoch: 8 | loss: 0.0146793\n",
      "\tspeed: 0.0519s/iter; left time: 592.8671s\n",
      "\titers: 400, epoch: 8 | loss: 0.0124940\n",
      "\tspeed: 0.0517s/iter; left time: 585.7752s\n",
      "\titers: 500, epoch: 8 | loss: 0.0133813\n",
      "\tspeed: 0.0523s/iter; left time: 586.9921s\n",
      "\titers: 600, epoch: 8 | loss: 0.0127013\n",
      "\tspeed: 0.0521s/iter; left time: 579.8516s\n",
      "\titers: 700, epoch: 8 | loss: 0.0136042\n",
      "\tspeed: 0.0519s/iter; left time: 572.0377s\n",
      "\titers: 800, epoch: 8 | loss: 0.0127588\n",
      "\tspeed: 0.0519s/iter; left time: 566.5931s\n",
      "\titers: 900, epoch: 8 | loss: 0.0109258\n",
      "\tspeed: 0.0521s/iter; left time: 563.6564s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:47.13s\n",
      "Steps: 902 | Train Loss: 0.0127591 Vali Loss: 0.0202601 Test Loss: 0.0218758\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0109588\n",
      "\tspeed: 0.1229s/iter; left time: 1318.4384s\n",
      "\titers: 200, epoch: 9 | loss: 0.0122424\n",
      "\tspeed: 0.0508s/iter; left time: 539.5256s\n",
      "\titers: 300, epoch: 9 | loss: 0.0136600\n",
      "\tspeed: 0.0522s/iter; left time: 549.4978s\n",
      "\titers: 400, epoch: 9 | loss: 0.0114705\n",
      "\tspeed: 0.0499s/iter; left time: 520.0693s\n",
      "\titers: 500, epoch: 9 | loss: 0.0127231\n",
      "\tspeed: 0.0520s/iter; left time: 537.2253s\n",
      "\titers: 600, epoch: 9 | loss: 0.0126706\n",
      "\tspeed: 0.0514s/iter; left time: 525.9851s\n",
      "\titers: 700, epoch: 9 | loss: 0.0122179\n",
      "\tspeed: 0.0521s/iter; left time: 527.4878s\n",
      "\titers: 800, epoch: 9 | loss: 0.0115100\n",
      "\tspeed: 0.0519s/iter; left time: 519.8266s\n",
      "\titers: 900, epoch: 9 | loss: 0.0118221\n",
      "\tspeed: 0.0519s/iter; left time: 514.6659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:46.90s\n",
      "Steps: 902 | Train Loss: 0.0116011 Vali Loss: 0.0210053 Test Loss: 0.0222173\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_168_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.021097851917147636, rmse:0.14525099098682404, mae:0.09748273342847824, rse:0.5495890974998474\n",
      "Intermediate time for IT and pred_len 168: 00h:07m:45.72s\n",
      "Intermediate time for IT: 00h:21m:41.84s\n",
      "Total time: 01h:56m:54.90s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --label_len 48 \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --d_layers {d_layers} \\\n",
    "              --factor 5 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --dec_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --dropout 0.1 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --overlapping_windows \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                informer_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Informer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.1597</td>\n",
       "      <td>0.1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.2039</td>\n",
       "      <td>0.1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.1539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.1764</td>\n",
       "      <td>0.1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.2591</td>\n",
       "      <td>0.1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.1374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>0.1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.0920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.0953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            Informer                \n",
       "Metrics               MSE    RMSE     MAE\n",
       "Country Pred_len                         \n",
       "DE      24         0.0255  0.1597  0.1063\n",
       "        96         0.0416  0.2039  0.1468\n",
       "        168        0.0459  0.2142  0.1539\n",
       "GB      24         0.0311  0.1764  0.1203\n",
       "        96         0.0506  0.2250  0.1601\n",
       "        168        0.0671  0.2591  0.1829\n",
       "ES      24         0.0304  0.1742  0.1045\n",
       "        96         0.0451  0.2124  0.1374\n",
       "        168        0.0457  0.2138  0.1408\n",
       "FR      24         0.0137  0.1171  0.0717\n",
       "        96         0.0219  0.1480  0.0920\n",
       "        168        0.0229  0.1514  0.0953\n",
       "IT      24         0.0114  0.1067  0.0660\n",
       "        96         0.0188  0.1370  0.0916\n",
       "        168        0.0211  0.1453  0.0975"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/informer'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "informer_df = convert_results_into_df(informer_results, if_loss_fnc=False, itr=1)\n",
    "informer_df.drop(columns=['Iteration'], inplace=True)\n",
    "\n",
    "# Final DF\n",
    "informer_df.columns = pd.MultiIndex.from_product([['Informer'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "informer_df.to_csv(os.path.join(path, 'informer.csv'))\n",
    "informer_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PatchTST 168\n",
    "\n",
    "We separated PatchTST from Informer, because it has additional arguments. It is not so easy to modify f-string (as e. g. distionary) to unpack some arguments with if statement. Moreover, it has different parameter values.\n",
    "\n",
    "Again, we separated all look-back windows into different cells, to settle up training on remote servers. Here is first with look-back 168 time steps, the next will be 336 and finally 512. In the following 3 parts all arguments are the same except seq_lem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 168\n",
    "model = \"PatchTST\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_168.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128 \n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0383199\n",
      "\tspeed: 0.0538s/iter; left time: 1210.6422s\n",
      "\titers: 200, epoch: 1 | loss: 0.0312549\n",
      "\tspeed: 0.0263s/iter; left time: 590.0308s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 226 | Train Loss: 0.0377966 Vali Loss: 0.0340360 Test Loss: 0.0390400\n",
      "Validation loss decreased (inf --> 0.034036).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0153823\n",
      "\tspeed: 0.0444s/iter; left time: 988.6494s\n",
      "\titers: 200, epoch: 2 | loss: 0.0156625\n",
      "\tspeed: 0.0235s/iter; left time: 521.2420s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.16s\n",
      "Steps: 226 | Train Loss: 0.0177919 Vali Loss: 0.0212845 Test Loss: 0.0229115\n",
      "Validation loss decreased (0.034036 --> 0.021285).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0143448\n",
      "\tspeed: 0.0464s/iter; left time: 1022.2165s\n",
      "\titers: 200, epoch: 3 | loss: 0.0157216\n",
      "\tspeed: 0.0227s/iter; left time: 499.3058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.64s\n",
      "Steps: 226 | Train Loss: 0.0150513 Vali Loss: 0.0200835 Test Loss: 0.0220109\n",
      "Validation loss decreased (0.021285 --> 0.020084).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0155801\n",
      "\tspeed: 0.0444s/iter; left time: 968.1773s\n",
      "\titers: 200, epoch: 4 | loss: 0.0138919\n",
      "\tspeed: 0.0219s/iter; left time: 475.6993s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.24s\n",
      "Steps: 226 | Train Loss: 0.0143696 Vali Loss: 0.0197017 Test Loss: 0.0216145\n",
      "Validation loss decreased (0.020084 --> 0.019702).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0155787\n",
      "\tspeed: 0.0520s/iter; left time: 1123.7833s\n",
      "\titers: 200, epoch: 5 | loss: 0.0120253\n",
      "\tspeed: 0.0255s/iter; left time: 547.2439s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.04s\n",
      "Steps: 226 | Train Loss: 0.0140176 Vali Loss: 0.0197954 Test Loss: 0.0217132\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0116973\n",
      "\tspeed: 0.0473s/iter; left time: 1010.2116s\n",
      "\titers: 200, epoch: 6 | loss: 0.0129933\n",
      "\tspeed: 0.0275s/iter; left time: 584.2606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.13s\n",
      "Steps: 226 | Train Loss: 0.0137816 Vali Loss: 0.0194198 Test Loss: 0.0212227\n",
      "Validation loss decreased (0.019702 --> 0.019420).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0148134\n",
      "\tspeed: 0.0510s/iter; left time: 1077.4099s\n",
      "\titers: 200, epoch: 7 | loss: 0.0137267\n",
      "\tspeed: 0.0265s/iter; left time: 558.1370s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 226 | Train Loss: 0.0136141 Vali Loss: 0.0193366 Test Loss: 0.0213316\n",
      "Validation loss decreased (0.019420 --> 0.019337).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0116126\n",
      "\tspeed: 0.0460s/iter; left time: 961.8929s\n",
      "\titers: 200, epoch: 8 | loss: 0.0123869\n",
      "\tspeed: 0.0185s/iter; left time: 385.2594s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.85s\n",
      "Steps: 226 | Train Loss: 0.0135009 Vali Loss: 0.0191222 Test Loss: 0.0213271\n",
      "Validation loss decreased (0.019337 --> 0.019122).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0126129\n",
      "\tspeed: 0.0480s/iter; left time: 994.0540s\n",
      "\titers: 200, epoch: 9 | loss: 0.0113241\n",
      "\tspeed: 0.0280s/iter; left time: 575.9233s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 226 | Train Loss: 0.0133891 Vali Loss: 0.0191334 Test Loss: 0.0211914\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0143735\n",
      "\tspeed: 0.0497s/iter; left time: 1017.7869s\n",
      "\titers: 200, epoch: 10 | loss: 0.0132874\n",
      "\tspeed: 0.0210s/iter; left time: 427.3026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.39s\n",
      "Steps: 226 | Train Loss: 0.0132893 Vali Loss: 0.0191786 Test Loss: 0.0212638\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0139704\n",
      "\tspeed: 0.0473s/iter; left time: 957.6863s\n",
      "\titers: 200, epoch: 11 | loss: 0.0125956\n",
      "\tspeed: 0.0253s/iter; left time: 509.4556s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.07s\n",
      "Steps: 226 | Train Loss: 0.0132077 Vali Loss: 0.0191151 Test Loss: 0.0212688\n",
      "Validation loss decreased (0.019122 --> 0.019115).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0130513\n",
      "\tspeed: 0.0502s/iter; left time: 1004.8599s\n",
      "\titers: 200, epoch: 12 | loss: 0.0148896\n",
      "\tspeed: 0.0255s/iter; left time: 508.3369s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.10s\n",
      "Steps: 226 | Train Loss: 0.0131658 Vali Loss: 0.0190011 Test Loss: 0.0211472\n",
      "Validation loss decreased (0.019115 --> 0.019001).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0128899\n",
      "\tspeed: 0.0478s/iter; left time: 946.5364s\n",
      "\titers: 200, epoch: 13 | loss: 0.0118731\n",
      "\tspeed: 0.0253s/iter; left time: 498.1929s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.02s\n",
      "Steps: 226 | Train Loss: 0.0131046 Vali Loss: 0.0189218 Test Loss: 0.0210358\n",
      "Validation loss decreased (0.019001 --> 0.018922).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0133532\n",
      "\tspeed: 0.0469s/iter; left time: 917.2947s\n",
      "\titers: 200, epoch: 14 | loss: 0.0144624\n",
      "\tspeed: 0.0254s/iter; left time: 494.8066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.74s\n",
      "Steps: 226 | Train Loss: 0.0130366 Vali Loss: 0.0189040 Test Loss: 0.0210546\n",
      "Validation loss decreased (0.018922 --> 0.018904).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0130911\n",
      "\tspeed: 0.0488s/iter; left time: 943.7265s\n",
      "\titers: 200, epoch: 15 | loss: 0.0122813\n",
      "\tspeed: 0.0263s/iter; left time: 506.8760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 226 | Train Loss: 0.0129929 Vali Loss: 0.0188822 Test Loss: 0.0209941\n",
      "Validation loss decreased (0.018904 --> 0.018882).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0127345\n",
      "\tspeed: 0.0504s/iter; left time: 963.2289s\n",
      "\titers: 200, epoch: 16 | loss: 0.0142818\n",
      "\tspeed: 0.0271s/iter; left time: 514.6393s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 226 | Train Loss: 0.0129400 Vali Loss: 0.0188522 Test Loss: 0.0209537\n",
      "Validation loss decreased (0.018882 --> 0.018852).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0127672\n",
      "\tspeed: 0.0469s/iter; left time: 886.4343s\n",
      "\titers: 200, epoch: 17 | loss: 0.0128871\n",
      "\tspeed: 0.0204s/iter; left time: 382.6192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 226 | Train Loss: 0.0129139 Vali Loss: 0.0188519 Test Loss: 0.0210161\n",
      "Validation loss decreased (0.018852 --> 0.018852).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0121936\n",
      "\tspeed: 0.0476s/iter; left time: 887.5064s\n",
      "\titers: 200, epoch: 18 | loss: 0.0122830\n",
      "\tspeed: 0.0253s/iter; left time: 469.2159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.08s\n",
      "Steps: 226 | Train Loss: 0.0128742 Vali Loss: 0.0188405 Test Loss: 0.0209885\n",
      "Validation loss decreased (0.018852 --> 0.018841).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0152183\n",
      "\tspeed: 0.0461s/iter; left time: 848.9115s\n",
      "\titers: 200, epoch: 19 | loss: 0.0130465\n",
      "\tspeed: 0.0257s/iter; left time: 470.2621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.74s\n",
      "Steps: 226 | Train Loss: 0.0128390 Vali Loss: 0.0188048 Test Loss: 0.0209550\n",
      "Validation loss decreased (0.018841 --> 0.018805).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0138098\n",
      "\tspeed: 0.0483s/iter; left time: 879.3514s\n",
      "\titers: 200, epoch: 20 | loss: 0.0152119\n",
      "\tspeed: 0.0266s/iter; left time: 480.9686s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 226 | Train Loss: 0.0128229 Vali Loss: 0.0186983 Test Loss: 0.0209512\n",
      "Validation loss decreased (0.018805 --> 0.018698).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0121355\n",
      "\tspeed: 0.0476s/iter; left time: 855.5641s\n",
      "\titers: 200, epoch: 21 | loss: 0.0112943\n",
      "\tspeed: 0.0263s/iter; left time: 470.2271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 226 | Train Loss: 0.0127835 Vali Loss: 0.0188119 Test Loss: 0.0209863\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0121620\n",
      "\tspeed: 0.0465s/iter; left time: 826.2803s\n",
      "\titers: 200, epoch: 22 | loss: 0.0122019\n",
      "\tspeed: 0.0278s/iter; left time: 490.1601s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:06.13s\n",
      "Steps: 226 | Train Loss: 0.0127699 Vali Loss: 0.0188123 Test Loss: 0.0210200\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0129834\n",
      "\tspeed: 0.0496s/iter; left time: 869.7931s\n",
      "\titers: 200, epoch: 23 | loss: 0.0121842\n",
      "\tspeed: 0.0200s/iter; left time: 348.6718s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.32s\n",
      "Steps: 226 | Train Loss: 0.0127693 Vali Loss: 0.0187909 Test Loss: 0.0210002\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0110590\n",
      "\tspeed: 0.0414s/iter; left time: 717.1757s\n",
      "\titers: 200, epoch: 24 | loss: 0.0139926\n",
      "\tspeed: 0.0188s/iter; left time: 324.1019s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:04.58s\n",
      "Steps: 226 | Train Loss: 0.0127507 Vali Loss: 0.0187344 Test Loss: 0.0209447\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0128071\n",
      "\tspeed: 0.0464s/iter; left time: 791.6337s\n",
      "\titers: 200, epoch: 25 | loss: 0.0132529\n",
      "\tspeed: 0.0251s/iter; left time: 426.4795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 226 | Train Loss: 0.0127247 Vali Loss: 0.0188031 Test Loss: 0.0209953\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0114409\n",
      "\tspeed: 0.0433s/iter; left time: 729.4194s\n",
      "\titers: 200, epoch: 26 | loss: 0.0129284\n",
      "\tspeed: 0.0234s/iter; left time: 392.6331s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:05.34s\n",
      "Steps: 226 | Train Loss: 0.0127187 Vali Loss: 0.0188277 Test Loss: 0.0209686\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0145785\n",
      "\tspeed: 0.0469s/iter; left time: 780.0009s\n",
      "\titers: 200, epoch: 27 | loss: 0.0137299\n",
      "\tspeed: 0.0258s/iter; left time: 426.4586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 226 | Train Loss: 0.0127071 Vali Loss: 0.0187976 Test Loss: 0.0209546\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0120018\n",
      "\tspeed: 0.0482s/iter; left time: 791.0466s\n",
      "\titers: 200, epoch: 28 | loss: 0.0117700\n",
      "\tspeed: 0.0273s/iter; left time: 445.3811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0126744 Vali Loss: 0.0187394 Test Loss: 0.0209601\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0133241\n",
      "\tspeed: 0.0510s/iter; left time: 824.5815s\n",
      "\titers: 200, epoch: 29 | loss: 0.0133722\n",
      "\tspeed: 0.0264s/iter; left time: 424.8904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0126740 Vali Loss: 0.0187694 Test Loss: 0.0209734\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0131072\n",
      "\tspeed: 0.0468s/iter; left time: 746.9519s\n",
      "\titers: 200, epoch: 30 | loss: 0.0118285\n",
      "\tspeed: 0.0256s/iter; left time: 405.8730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 226 | Train Loss: 0.0126849 Vali Loss: 0.0186991 Test Loss: 0.0209547\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.020951194688677788, rmse:0.14474527537822723, mae:0.09142162650823593, rse:0.5108261108398438\n",
      "Intermediate time for DE and pred_len 24: 00h:03m:48.69s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0411841\n",
      "\tspeed: 0.0479s/iter; left time: 1072.1390s\n",
      "\titers: 200, epoch: 1 | loss: 0.0347563\n",
      "\tspeed: 0.0200s/iter; left time: 445.6445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 225 | Train Loss: 0.0428926 Vali Loss: 0.0408700 Test Loss: 0.0488379\n",
      "Validation loss decreased (inf --> 0.040870).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0280918\n",
      "\tspeed: 0.0529s/iter; left time: 1172.3212s\n",
      "\titers: 200, epoch: 2 | loss: 0.0259415\n",
      "\tspeed: 0.0274s/iter; left time: 604.1406s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0278961 Vali Loss: 0.0323799 Test Loss: 0.0383353\n",
      "Validation loss decreased (0.040870 --> 0.032380).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0256202\n",
      "\tspeed: 0.0543s/iter; left time: 1191.7391s\n",
      "\titers: 200, epoch: 3 | loss: 0.0249472\n",
      "\tspeed: 0.0268s/iter; left time: 586.5331s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 225 | Train Loss: 0.0248436 Vali Loss: 0.0313352 Test Loss: 0.0374697\n",
      "Validation loss decreased (0.032380 --> 0.031335).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0231897\n",
      "\tspeed: 0.0543s/iter; left time: 1179.5102s\n",
      "\titers: 200, epoch: 4 | loss: 0.0219180\n",
      "\tspeed: 0.0273s/iter; left time: 590.1255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 225 | Train Loss: 0.0240810 Vali Loss: 0.0309673 Test Loss: 0.0371241\n",
      "Validation loss decreased (0.031335 --> 0.030967).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0224635\n",
      "\tspeed: 0.0535s/iter; left time: 1150.8540s\n",
      "\titers: 200, epoch: 5 | loss: 0.0248532\n",
      "\tspeed: 0.0252s/iter; left time: 539.9441s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 225 | Train Loss: 0.0236383 Vali Loss: 0.0312279 Test Loss: 0.0370489\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0259781\n",
      "\tspeed: 0.0527s/iter; left time: 1120.8972s\n",
      "\titers: 200, epoch: 6 | loss: 0.0241365\n",
      "\tspeed: 0.0239s/iter; left time: 505.9149s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.25s\n",
      "Steps: 225 | Train Loss: 0.0232959 Vali Loss: 0.0309534 Test Loss: 0.0370188\n",
      "Validation loss decreased (0.030967 --> 0.030953).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0221646\n",
      "\tspeed: 0.0525s/iter; left time: 1105.9718s\n",
      "\titers: 200, epoch: 7 | loss: 0.0239294\n",
      "\tspeed: 0.0302s/iter; left time: 632.4346s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 225 | Train Loss: 0.0231015 Vali Loss: 0.0309000 Test Loss: 0.0367180\n",
      "Validation loss decreased (0.030953 --> 0.030900).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0237838\n",
      "\tspeed: 0.0575s/iter; left time: 1197.4413s\n",
      "\titers: 200, epoch: 8 | loss: 0.0253054\n",
      "\tspeed: 0.0306s/iter; left time: 633.5904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 225 | Train Loss: 0.0229007 Vali Loss: 0.0309551 Test Loss: 0.0369570\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0234542\n",
      "\tspeed: 0.0540s/iter; left time: 1112.7232s\n",
      "\titers: 200, epoch: 9 | loss: 0.0214983\n",
      "\tspeed: 0.0306s/iter; left time: 626.9084s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 225 | Train Loss: 0.0227605 Vali Loss: 0.0308411 Test Loss: 0.0365352\n",
      "Validation loss decreased (0.030900 --> 0.030841).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0219778\n",
      "\tspeed: 0.0546s/iter; left time: 1113.0700s\n",
      "\titers: 200, epoch: 10 | loss: 0.0220064\n",
      "\tspeed: 0.0286s/iter; left time: 578.9570s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.68s\n",
      "Steps: 225 | Train Loss: 0.0225765 Vali Loss: 0.0307082 Test Loss: 0.0365368\n",
      "Validation loss decreased (0.030841 --> 0.030708).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0229735\n",
      "\tspeed: 0.0555s/iter; left time: 1118.1702s\n",
      "\titers: 200, epoch: 11 | loss: 0.0225767\n",
      "\tspeed: 0.0293s/iter; left time: 586.9332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.75s\n",
      "Steps: 225 | Train Loss: 0.0224487 Vali Loss: 0.0306180 Test Loss: 0.0365732\n",
      "Validation loss decreased (0.030708 --> 0.030618).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0217478\n",
      "\tspeed: 0.0521s/iter; left time: 1038.9132s\n",
      "\titers: 200, epoch: 12 | loss: 0.0245087\n",
      "\tspeed: 0.0237s/iter; left time: 470.6473s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.71s\n",
      "Steps: 225 | Train Loss: 0.0223362 Vali Loss: 0.0307271 Test Loss: 0.0367705\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0234131\n",
      "\tspeed: 0.0498s/iter; left time: 980.9358s\n",
      "\titers: 200, epoch: 13 | loss: 0.0227327\n",
      "\tspeed: 0.0203s/iter; left time: 396.9530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.28s\n",
      "Steps: 225 | Train Loss: 0.0222529 Vali Loss: 0.0309070 Test Loss: 0.0368769\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0223071\n",
      "\tspeed: 0.0502s/iter; left time: 978.3189s\n",
      "\titers: 200, epoch: 14 | loss: 0.0235474\n",
      "\tspeed: 0.0263s/iter; left time: 509.9303s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 225 | Train Loss: 0.0221733 Vali Loss: 0.0307393 Test Loss: 0.0366288\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0220968\n",
      "\tspeed: 0.0534s/iter; left time: 1027.1092s\n",
      "\titers: 200, epoch: 15 | loss: 0.0209606\n",
      "\tspeed: 0.0284s/iter; left time: 544.7682s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.57s\n",
      "Steps: 225 | Train Loss: 0.0221032 Vali Loss: 0.0307878 Test Loss: 0.0366282\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0221824\n",
      "\tspeed: 0.0531s/iter; left time: 1011.0572s\n",
      "\titers: 200, epoch: 16 | loss: 0.0217833\n",
      "\tspeed: 0.0269s/iter; left time: 509.2177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.37s\n",
      "Steps: 225 | Train Loss: 0.0220159 Vali Loss: 0.0307263 Test Loss: 0.0364834\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0211384\n",
      "\tspeed: 0.0488s/iter; left time: 918.3960s\n",
      "\titers: 200, epoch: 17 | loss: 0.0233507\n",
      "\tspeed: 0.0268s/iter; left time: 501.8258s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 225 | Train Loss: 0.0219597 Vali Loss: 0.0307062 Test Loss: 0.0367133\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0248679\n",
      "\tspeed: 0.0519s/iter; left time: 963.8943s\n",
      "\titers: 200, epoch: 18 | loss: 0.0207510\n",
      "\tspeed: 0.0271s/iter; left time: 500.7151s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 225 | Train Loss: 0.0218721 Vali Loss: 0.0306500 Test Loss: 0.0365560\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0224404\n",
      "\tspeed: 0.0491s/iter; left time: 900.1339s\n",
      "\titers: 200, epoch: 19 | loss: 0.0215610\n",
      "\tspeed: 0.0243s/iter; left time: 444.1266s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.54s\n",
      "Steps: 225 | Train Loss: 0.0218504 Vali Loss: 0.0308245 Test Loss: 0.0366136\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0218998\n",
      "\tspeed: 0.0529s/iter; left time: 958.2390s\n",
      "\titers: 200, epoch: 20 | loss: 0.0231873\n",
      "\tspeed: 0.0268s/iter; left time: 483.1225s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.25s\n",
      "Steps: 225 | Train Loss: 0.0218139 Vali Loss: 0.0308543 Test Loss: 0.0368732\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0215853\n",
      "\tspeed: 0.0448s/iter; left time: 801.9054s\n",
      "\titers: 200, epoch: 21 | loss: 0.0210817\n",
      "\tspeed: 0.0195s/iter; left time: 346.5337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:04.66s\n",
      "Steps: 225 | Train Loss: 0.0217619 Vali Loss: 0.0308317 Test Loss: 0.0368701\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.036573249846696854, rmse:0.1912413388490677, mae:0.12977558374404907, rse:0.6772242784500122\n",
      "Intermediate time for DE and pred_len 96: 00h:02m:56.07s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0447109\n",
      "\tspeed: 0.0359s/iter; left time: 803.6922s\n",
      "\titers: 200, epoch: 1 | loss: 0.0350626\n",
      "\tspeed: 0.0223s/iter; left time: 497.7267s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.42s\n",
      "Steps: 225 | Train Loss: 0.0450947 Vali Loss: 0.0424763 Test Loss: 0.0511885\n",
      "Validation loss decreased (inf --> 0.042476).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0322759\n",
      "\tspeed: 0.0457s/iter; left time: 1013.7407s\n",
      "\titers: 200, epoch: 2 | loss: 0.0286798\n",
      "\tspeed: 0.0212s/iter; left time: 468.1730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 225 | Train Loss: 0.0307300 Vali Loss: 0.0340023 Test Loss: 0.0414040\n",
      "Validation loss decreased (0.042476 --> 0.034002).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0276802\n",
      "\tspeed: 0.0483s/iter; left time: 1060.7960s\n",
      "\titers: 200, epoch: 3 | loss: 0.0266256\n",
      "\tspeed: 0.0213s/iter; left time: 465.3776s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.07s\n",
      "Steps: 225 | Train Loss: 0.0274781 Vali Loss: 0.0334509 Test Loss: 0.0405938\n",
      "Validation loss decreased (0.034002 --> 0.033451).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0282453\n",
      "\tspeed: 0.0470s/iter; left time: 1020.1148s\n",
      "\titers: 200, epoch: 4 | loss: 0.0259354\n",
      "\tspeed: 0.0198s/iter; left time: 427.5512s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.74s\n",
      "Steps: 225 | Train Loss: 0.0266396 Vali Loss: 0.0331835 Test Loss: 0.0402118\n",
      "Validation loss decreased (0.033451 --> 0.033183).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0275930\n",
      "\tspeed: 0.0499s/iter; left time: 1073.3855s\n",
      "\titers: 200, epoch: 5 | loss: 0.0242258\n",
      "\tspeed: 0.0199s/iter; left time: 425.6363s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.84s\n",
      "Steps: 225 | Train Loss: 0.0261759 Vali Loss: 0.0331400 Test Loss: 0.0401583\n",
      "Validation loss decreased (0.033183 --> 0.033140).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0249907\n",
      "\tspeed: 0.0462s/iter; left time: 982.1340s\n",
      "\titers: 200, epoch: 6 | loss: 0.0265714\n",
      "\tspeed: 0.0209s/iter; left time: 442.0510s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 225 | Train Loss: 0.0258694 Vali Loss: 0.0327760 Test Loss: 0.0400483\n",
      "Validation loss decreased (0.033140 --> 0.032776).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0262237\n",
      "\tspeed: 0.0496s/iter; left time: 1044.8968s\n",
      "\titers: 200, epoch: 7 | loss: 0.0241183\n",
      "\tspeed: 0.0195s/iter; left time: 409.0684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.82s\n",
      "Steps: 225 | Train Loss: 0.0256120 Vali Loss: 0.0329920 Test Loss: 0.0398653\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0235487\n",
      "\tspeed: 0.0476s/iter; left time: 990.3748s\n",
      "\titers: 200, epoch: 8 | loss: 0.0245051\n",
      "\tspeed: 0.0235s/iter; left time: 486.1281s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 225 | Train Loss: 0.0254211 Vali Loss: 0.0331922 Test Loss: 0.0402371\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0269264\n",
      "\tspeed: 0.0531s/iter; left time: 1094.1857s\n",
      "\titers: 200, epoch: 9 | loss: 0.0250130\n",
      "\tspeed: 0.0251s/iter; left time: 514.2615s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 225 | Train Loss: 0.0252059 Vali Loss: 0.0330678 Test Loss: 0.0398848\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0238565\n",
      "\tspeed: 0.0691s/iter; left time: 1408.9169s\n",
      "\titers: 200, epoch: 10 | loss: 0.0237731\n",
      "\tspeed: 0.0285s/iter; left time: 577.4320s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 225 | Train Loss: 0.0250347 Vali Loss: 0.0327603 Test Loss: 0.0400813\n",
      "Validation loss decreased (0.032776 --> 0.032760).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0244635\n",
      "\tspeed: 0.0768s/iter; left time: 1547.0035s\n",
      "\titers: 200, epoch: 11 | loss: 0.0232718\n",
      "\tspeed: 0.0270s/iter; left time: 541.0004s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.68s\n",
      "Steps: 225 | Train Loss: 0.0248838 Vali Loss: 0.0326611 Test Loss: 0.0401495\n",
      "Validation loss decreased (0.032760 --> 0.032661).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0247994\n",
      "\tspeed: 0.0771s/iter; left time: 1536.4119s\n",
      "\titers: 200, epoch: 12 | loss: 0.0246821\n",
      "\tspeed: 0.0276s/iter; left time: 546.5131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.01s\n",
      "Steps: 225 | Train Loss: 0.0247419 Vali Loss: 0.0327078 Test Loss: 0.0401618\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0239694\n",
      "\tspeed: 0.0842s/iter; left time: 1659.3024s\n",
      "\titers: 200, epoch: 13 | loss: 0.0218429\n",
      "\tspeed: 0.0284s/iter; left time: 557.6432s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.93s\n",
      "Steps: 225 | Train Loss: 0.0246602 Vali Loss: 0.0327330 Test Loss: 0.0401319\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0255009\n",
      "\tspeed: 0.0678s/iter; left time: 1319.6675s\n",
      "\titers: 200, epoch: 14 | loss: 0.0252126\n",
      "\tspeed: 0.0275s/iter; left time: 533.3919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 225 | Train Loss: 0.0245515 Vali Loss: 0.0327835 Test Loss: 0.0401811\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0236850\n",
      "\tspeed: 0.0792s/iter; left time: 1525.1057s\n",
      "\titers: 200, epoch: 15 | loss: 0.0246431\n",
      "\tspeed: 0.0250s/iter; left time: 478.4968s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 225 | Train Loss: 0.0244478 Vali Loss: 0.0326965 Test Loss: 0.0404440\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0271170\n",
      "\tspeed: 0.0718s/iter; left time: 1366.7172s\n",
      "\titers: 200, epoch: 16 | loss: 0.0265224\n",
      "\tspeed: 0.0339s/iter; left time: 640.7791s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.61s\n",
      "Steps: 225 | Train Loss: 0.0243604 Vali Loss: 0.0327115 Test Loss: 0.0401209\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0254853\n",
      "\tspeed: 0.0750s/iter; left time: 1410.3246s\n",
      "\titers: 200, epoch: 17 | loss: 0.0264738\n",
      "\tspeed: 0.0269s/iter; left time: 503.3991s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 225 | Train Loss: 0.0242914 Vali Loss: 0.0326806 Test Loss: 0.0404772\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0232780\n",
      "\tspeed: 0.0693s/iter; left time: 1287.6609s\n",
      "\titers: 200, epoch: 18 | loss: 0.0237292\n",
      "\tspeed: 0.0306s/iter; left time: 564.5865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.06s\n",
      "Steps: 225 | Train Loss: 0.0242091 Vali Loss: 0.0327785 Test Loss: 0.0405320\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0236547\n",
      "\tspeed: 0.0724s/iter; left time: 1328.5391s\n",
      "\titers: 200, epoch: 19 | loss: 0.0253258\n",
      "\tspeed: 0.0304s/iter; left time: 554.3087s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.01s\n",
      "Steps: 225 | Train Loss: 0.0241632 Vali Loss: 0.0326887 Test Loss: 0.0406219\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0250084\n",
      "\tspeed: 0.0884s/iter; left time: 1602.4261s\n",
      "\titers: 200, epoch: 20 | loss: 0.0265045\n",
      "\tspeed: 0.0322s/iter; left time: 581.2436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.94s\n",
      "Steps: 225 | Train Loss: 0.0241166 Vali Loss: 0.0326053 Test Loss: 0.0405097\n",
      "Validation loss decreased (0.032661 --> 0.032605).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0238479\n",
      "\tspeed: 0.0786s/iter; left time: 1406.8111s\n",
      "\titers: 200, epoch: 21 | loss: 0.0227442\n",
      "\tspeed: 0.0281s/iter; left time: 499.5196s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.94s\n",
      "Steps: 225 | Train Loss: 0.0240373 Vali Loss: 0.0325985 Test Loss: 0.0406184\n",
      "Validation loss decreased (0.032605 --> 0.032598).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0242475\n",
      "\tspeed: 0.0833s/iter; left time: 1473.0018s\n",
      "\titers: 200, epoch: 22 | loss: 0.0240004\n",
      "\tspeed: 0.0474s/iter; left time: 833.9684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.32s\n",
      "Steps: 225 | Train Loss: 0.0240255 Vali Loss: 0.0326136 Test Loss: 0.0406383\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0233491\n",
      "\tspeed: 0.1110s/iter; left time: 1937.1116s\n",
      "\titers: 200, epoch: 23 | loss: 0.0215407\n",
      "\tspeed: 0.0453s/iter; left time: 786.4895s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:10.40s\n",
      "Steps: 225 | Train Loss: 0.0239283 Vali Loss: 0.0326419 Test Loss: 0.0406990\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0229299\n",
      "\tspeed: 0.1253s/iter; left time: 2158.4732s\n",
      "\titers: 200, epoch: 24 | loss: 0.0237968\n",
      "\tspeed: 0.0419s/iter; left time: 718.1554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:09.65s\n",
      "Steps: 225 | Train Loss: 0.0239189 Vali Loss: 0.0327209 Test Loss: 0.0407308\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0228200\n",
      "\tspeed: 0.1314s/iter; left time: 2233.7386s\n",
      "\titers: 200, epoch: 25 | loss: 0.0248429\n",
      "\tspeed: 0.0401s/iter; left time: 678.0096s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.18s\n",
      "Steps: 225 | Train Loss: 0.0238833 Vali Loss: 0.0326425 Test Loss: 0.0406959\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0235288\n",
      "\tspeed: 0.1557s/iter; left time: 2612.2294s\n",
      "\titers: 200, epoch: 26 | loss: 0.0227188\n",
      "\tspeed: 0.0514s/iter; left time: 857.3906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:13.29s\n",
      "Steps: 225 | Train Loss: 0.0238446 Vali Loss: 0.0327189 Test Loss: 0.0407183\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0231323\n",
      "\tspeed: 0.1150s/iter; left time: 1903.1002s\n",
      "\titers: 200, epoch: 27 | loss: 0.0251811\n",
      "\tspeed: 0.0458s/iter; left time: 753.6024s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:10.71s\n",
      "Steps: 225 | Train Loss: 0.0238494 Vali Loss: 0.0326425 Test Loss: 0.0406725\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0233792\n",
      "\tspeed: 0.1166s/iter; left time: 1903.0459s\n",
      "\titers: 200, epoch: 28 | loss: 0.0243124\n",
      "\tspeed: 0.0429s/iter; left time: 696.4843s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:09.82s\n",
      "Steps: 225 | Train Loss: 0.0238004 Vali Loss: 0.0327135 Test Loss: 0.0407996\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0253402\n",
      "\tspeed: 0.1110s/iter; left time: 1786.6988s\n",
      "\titers: 200, epoch: 29 | loss: 0.0235787\n",
      "\tspeed: 0.0437s/iter; left time: 698.7407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:09.97s\n",
      "Steps: 225 | Train Loss: 0.0237641 Vali Loss: 0.0326271 Test Loss: 0.0408671\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0221469\n",
      "\tspeed: 0.1325s/iter; left time: 2103.2824s\n",
      "\titers: 200, epoch: 30 | loss: 0.0262810\n",
      "\tspeed: 0.0643s/iter; left time: 1013.9191s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:14.16s\n",
      "Steps: 225 | Train Loss: 0.0237545 Vali Loss: 0.0326350 Test Loss: 0.0408822\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0240579\n",
      "\tspeed: 0.1174s/iter; left time: 1838.1702s\n",
      "\titers: 200, epoch: 31 | loss: 0.0251211\n",
      "\tspeed: 0.0414s/iter; left time: 643.1440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:10.38s\n",
      "Steps: 225 | Train Loss: 0.0237316 Vali Loss: 0.0327280 Test Loss: 0.0408680\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.0406184159219265, rmse:0.20154011249542236, mae:0.13821378350257874, rse:0.7138713002204895\n",
      "Intermediate time for DE and pred_len 168: 00h:06m:06.96s\n",
      "Intermediate time for DE: 00h:12m:51.73s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0306815\n",
      "\tspeed: 0.0681s/iter; left time: 1532.5494s\n",
      "\titers: 200, epoch: 1 | loss: 0.0266523\n",
      "\tspeed: 0.0423s/iter; left time: 946.7330s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.07s\n",
      "Steps: 226 | Train Loss: 0.0323873 Vali Loss: 0.0310920 Test Loss: 0.0430876\n",
      "Validation loss decreased (inf --> 0.031092).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0153087\n",
      "\tspeed: 0.1056s/iter; left time: 2352.3071s\n",
      "\titers: 200, epoch: 2 | loss: 0.0142865\n",
      "\tspeed: 0.0512s/iter; left time: 1135.3933s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.83s\n",
      "Steps: 226 | Train Loss: 0.0165819 Vali Loss: 0.0204234 Test Loss: 0.0264011\n",
      "Validation loss decreased (0.031092 --> 0.020423).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0170686\n",
      "\tspeed: 0.1340s/iter; left time: 2954.9373s\n",
      "\titers: 200, epoch: 3 | loss: 0.0158198\n",
      "\tspeed: 0.0500s/iter; left time: 1098.2466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.71s\n",
      "Steps: 226 | Train Loss: 0.0145620 Vali Loss: 0.0201542 Test Loss: 0.0261200\n",
      "Validation loss decreased (0.020423 --> 0.020154).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0144440\n",
      "\tspeed: 0.1015s/iter; left time: 2214.5517s\n",
      "\titers: 200, epoch: 4 | loss: 0.0145676\n",
      "\tspeed: 0.0391s/iter; left time: 849.9930s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.73s\n",
      "Steps: 226 | Train Loss: 0.0141880 Vali Loss: 0.0201375 Test Loss: 0.0256896\n",
      "Validation loss decreased (0.020154 --> 0.020138).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0139161\n",
      "\tspeed: 0.1047s/iter; left time: 2260.4077s\n",
      "\titers: 200, epoch: 5 | loss: 0.0156245\n",
      "\tspeed: 0.0447s/iter; left time: 960.8914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 226 | Train Loss: 0.0139841 Vali Loss: 0.0199274 Test Loss: 0.0255548\n",
      "Validation loss decreased (0.020138 --> 0.019927).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0134839\n",
      "\tspeed: 0.0952s/iter; left time: 2034.9297s\n",
      "\titers: 200, epoch: 6 | loss: 0.0130852\n",
      "\tspeed: 0.0457s/iter; left time: 971.8614s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.29s\n",
      "Steps: 226 | Train Loss: 0.0138393 Vali Loss: 0.0195793 Test Loss: 0.0252798\n",
      "Validation loss decreased (0.019927 --> 0.019579).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0130190\n",
      "\tspeed: 0.1178s/iter; left time: 2491.4614s\n",
      "\titers: 200, epoch: 7 | loss: 0.0126406\n",
      "\tspeed: 0.0563s/iter; left time: 1184.9396s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:13.02s\n",
      "Steps: 226 | Train Loss: 0.0137379 Vali Loss: 0.0195426 Test Loss: 0.0252673\n",
      "Validation loss decreased (0.019579 --> 0.019543).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0126214\n",
      "\tspeed: 0.1138s/iter; left time: 2380.7048s\n",
      "\titers: 200, epoch: 8 | loss: 0.0131759\n",
      "\tspeed: 0.0461s/iter; left time: 960.7264s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.29s\n",
      "Steps: 226 | Train Loss: 0.0136468 Vali Loss: 0.0195262 Test Loss: 0.0252110\n",
      "Validation loss decreased (0.019543 --> 0.019526).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0142638\n",
      "\tspeed: 0.0991s/iter; left time: 2050.6731s\n",
      "\titers: 200, epoch: 9 | loss: 0.0131716\n",
      "\tspeed: 0.0427s/iter; left time: 879.0659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.23s\n",
      "Steps: 226 | Train Loss: 0.0135563 Vali Loss: 0.0193729 Test Loss: 0.0252877\n",
      "Validation loss decreased (0.019526 --> 0.019373).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0127078\n",
      "\tspeed: 0.0971s/iter; left time: 1986.3346s\n",
      "\titers: 200, epoch: 10 | loss: 0.0139840\n",
      "\tspeed: 0.0447s/iter; left time: 910.7431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.41s\n",
      "Steps: 226 | Train Loss: 0.0134777 Vali Loss: 0.0194025 Test Loss: 0.0252130\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0161274\n",
      "\tspeed: 0.0958s/iter; left time: 1938.3375s\n",
      "\titers: 200, epoch: 11 | loss: 0.0138039\n",
      "\tspeed: 0.0550s/iter; left time: 1107.6860s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.81s\n",
      "Steps: 226 | Train Loss: 0.0134166 Vali Loss: 0.0192794 Test Loss: 0.0250856\n",
      "Validation loss decreased (0.019373 --> 0.019279).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0121306\n",
      "\tspeed: 0.1345s/iter; left time: 2691.7819s\n",
      "\titers: 200, epoch: 12 | loss: 0.0160302\n",
      "\tspeed: 0.0399s/iter; left time: 794.6016s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.08s\n",
      "Steps: 226 | Train Loss: 0.0133569 Vali Loss: 0.0192511 Test Loss: 0.0251192\n",
      "Validation loss decreased (0.019279 --> 0.019251).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0113891\n",
      "\tspeed: 0.1013s/iter; left time: 2005.0537s\n",
      "\titers: 200, epoch: 13 | loss: 0.0132807\n",
      "\tspeed: 0.0383s/iter; left time: 753.6980s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.07s\n",
      "Steps: 226 | Train Loss: 0.0133088 Vali Loss: 0.0192478 Test Loss: 0.0251219\n",
      "Validation loss decreased (0.019251 --> 0.019248).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0134227\n",
      "\tspeed: 0.0917s/iter; left time: 1793.1355s\n",
      "\titers: 200, epoch: 14 | loss: 0.0130552\n",
      "\tspeed: 0.0476s/iter; left time: 927.2188s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.11s\n",
      "Steps: 226 | Train Loss: 0.0132481 Vali Loss: 0.0193488 Test Loss: 0.0250907\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0139046\n",
      "\tspeed: 0.1020s/iter; left time: 1971.9664s\n",
      "\titers: 200, epoch: 15 | loss: 0.0139125\n",
      "\tspeed: 0.0410s/iter; left time: 789.0880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.09s\n",
      "Steps: 226 | Train Loss: 0.0132269 Vali Loss: 0.0191523 Test Loss: 0.0251043\n",
      "Validation loss decreased (0.019248 --> 0.019152).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0133044\n",
      "\tspeed: 0.1234s/iter; left time: 2357.4765s\n",
      "\titers: 200, epoch: 16 | loss: 0.0134255\n",
      "\tspeed: 0.0614s/iter; left time: 1167.6348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:13.21s\n",
      "Steps: 226 | Train Loss: 0.0131907 Vali Loss: 0.0191569 Test Loss: 0.0251692\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0126479\n",
      "\tspeed: 0.0948s/iter; left time: 1789.6101s\n",
      "\titers: 200, epoch: 17 | loss: 0.0132790\n",
      "\tspeed: 0.0392s/iter; left time: 737.2248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:09.07s\n",
      "Steps: 226 | Train Loss: 0.0131559 Vali Loss: 0.0192524 Test Loss: 0.0252377\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0124893\n",
      "\tspeed: 0.0968s/iter; left time: 1805.6232s\n",
      "\titers: 200, epoch: 18 | loss: 0.0135506\n",
      "\tspeed: 0.0469s/iter; left time: 870.9445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 226 | Train Loss: 0.0131313 Vali Loss: 0.0191961 Test Loss: 0.0251142\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0135661\n",
      "\tspeed: 0.0941s/iter; left time: 1734.4131s\n",
      "\titers: 200, epoch: 19 | loss: 0.0137399\n",
      "\tspeed: 0.0438s/iter; left time: 803.4638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.09s\n",
      "Steps: 226 | Train Loss: 0.0130996 Vali Loss: 0.0191331 Test Loss: 0.0251709\n",
      "Validation loss decreased (0.019152 --> 0.019133).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0151006\n",
      "\tspeed: 0.1026s/iter; left time: 1867.9672s\n",
      "\titers: 200, epoch: 20 | loss: 0.0137961\n",
      "\tspeed: 0.0575s/iter; left time: 1041.7981s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:12.65s\n",
      "Steps: 226 | Train Loss: 0.0130854 Vali Loss: 0.0191960 Test Loss: 0.0250267\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0132778\n",
      "\tspeed: 0.1176s/iter; left time: 2114.1918s\n",
      "\titers: 200, epoch: 21 | loss: 0.0140660\n",
      "\tspeed: 0.0434s/iter; left time: 776.0602s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:09.46s\n",
      "Steps: 226 | Train Loss: 0.0130569 Vali Loss: 0.0192014 Test Loss: 0.0251986\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0109714\n",
      "\tspeed: 0.0987s/iter; left time: 1753.1350s\n",
      "\titers: 200, epoch: 22 | loss: 0.0144550\n",
      "\tspeed: 0.0396s/iter; left time: 698.5400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:09.71s\n",
      "Steps: 226 | Train Loss: 0.0130485 Vali Loss: 0.0191108 Test Loss: 0.0251565\n",
      "Validation loss decreased (0.019133 --> 0.019111).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0115084\n",
      "\tspeed: 0.1004s/iter; left time: 1760.5756s\n",
      "\titers: 200, epoch: 23 | loss: 0.0116755\n",
      "\tspeed: 0.0515s/iter; left time: 898.2234s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:11.12s\n",
      "Steps: 226 | Train Loss: 0.0129933 Vali Loss: 0.0190995 Test Loss: 0.0251609\n",
      "Validation loss decreased (0.019111 --> 0.019099).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0130449\n",
      "\tspeed: 0.0924s/iter; left time: 1599.3736s\n",
      "\titers: 200, epoch: 24 | loss: 0.0129596\n",
      "\tspeed: 0.0521s/iter; left time: 896.8435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.54s\n",
      "Steps: 226 | Train Loss: 0.0129977 Vali Loss: 0.0191031 Test Loss: 0.0252141\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0124596\n",
      "\tspeed: 0.1359s/iter; left time: 2321.4060s\n",
      "\titers: 200, epoch: 25 | loss: 0.0103465\n",
      "\tspeed: 0.0510s/iter; left time: 865.7878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:12.36s\n",
      "Steps: 226 | Train Loss: 0.0129898 Vali Loss: 0.0190852 Test Loss: 0.0251680\n",
      "Validation loss decreased (0.019099 --> 0.019085).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0146096\n",
      "\tspeed: 0.0786s/iter; left time: 1325.1361s\n",
      "\titers: 200, epoch: 26 | loss: 0.0114260\n",
      "\tspeed: 0.0303s/iter; left time: 507.8375s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:06.61s\n",
      "Steps: 226 | Train Loss: 0.0129762 Vali Loss: 0.0191453 Test Loss: 0.0251956\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0148471\n",
      "\tspeed: 0.0625s/iter; left time: 1039.4186s\n",
      "\titers: 200, epoch: 27 | loss: 0.0142760\n",
      "\tspeed: 0.0299s/iter; left time: 494.3536s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:06.75s\n",
      "Steps: 226 | Train Loss: 0.0129704 Vali Loss: 0.0190854 Test Loss: 0.0251721\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0120537\n",
      "\tspeed: 0.0607s/iter; left time: 995.3985s\n",
      "\titers: 200, epoch: 28 | loss: 0.0137661\n",
      "\tspeed: 0.0343s/iter; left time: 559.7215s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:07.12s\n",
      "Steps: 226 | Train Loss: 0.0129575 Vali Loss: 0.0190943 Test Loss: 0.0251400\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0132743\n",
      "\tspeed: 0.0585s/iter; left time: 946.4002s\n",
      "\titers: 200, epoch: 29 | loss: 0.0129417\n",
      "\tspeed: 0.0220s/iter; left time: 353.7564s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 226 | Train Loss: 0.0129509 Vali Loss: 0.0191139 Test Loss: 0.0251515\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0116974\n",
      "\tspeed: 0.0734s/iter; left time: 1170.7012s\n",
      "\titers: 200, epoch: 30 | loss: 0.0134378\n",
      "\tspeed: 0.0296s/iter; left time: 469.1240s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.86s\n",
      "Steps: 226 | Train Loss: 0.0129296 Vali Loss: 0.0191253 Test Loss: 0.0252005\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0125019\n",
      "\tspeed: 0.0731s/iter; left time: 1149.3511s\n",
      "\titers: 200, epoch: 31 | loss: 0.0145108\n",
      "\tspeed: 0.0440s/iter; left time: 687.9966s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.96s\n",
      "Steps: 226 | Train Loss: 0.0129281 Vali Loss: 0.0191124 Test Loss: 0.0251992\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0113440\n",
      "\tspeed: 0.0676s/iter; left time: 1048.1478s\n",
      "\titers: 200, epoch: 32 | loss: 0.0121925\n",
      "\tspeed: 0.0273s/iter; left time: 420.5292s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.53s\n",
      "Steps: 226 | Train Loss: 0.0129249 Vali Loss: 0.0191041 Test Loss: 0.0251873\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0128363\n",
      "\tspeed: 0.0857s/iter; left time: 1309.2490s\n",
      "\titers: 200, epoch: 33 | loss: 0.0128919\n",
      "\tspeed: 0.0379s/iter; left time: 574.7631s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:09.43s\n",
      "Steps: 226 | Train Loss: 0.0129289 Vali Loss: 0.0190764 Test Loss: 0.0251854\n",
      "Validation loss decreased (0.019085 --> 0.019076).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0115401\n",
      "\tspeed: 0.0747s/iter; left time: 1123.5613s\n",
      "\titers: 200, epoch: 34 | loss: 0.0133728\n",
      "\tspeed: 0.0289s/iter; left time: 432.2049s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.05s\n",
      "Steps: 226 | Train Loss: 0.0129210 Vali Loss: 0.0190594 Test Loss: 0.0251820\n",
      "Validation loss decreased (0.019076 --> 0.019059).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0132487\n",
      "\tspeed: 0.0829s/iter; left time: 1227.9971s\n",
      "\titers: 200, epoch: 35 | loss: 0.0136201\n",
      "\tspeed: 0.0287s/iter; left time: 421.8932s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:07.57s\n",
      "Steps: 226 | Train Loss: 0.0129160 Vali Loss: 0.0190945 Test Loss: 0.0251921\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0129037\n",
      "\tspeed: 0.0651s/iter; left time: 949.3845s\n",
      "\titers: 200, epoch: 36 | loss: 0.0127028\n",
      "\tspeed: 0.0250s/iter; left time: 362.7945s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:05.90s\n",
      "Steps: 226 | Train Loss: 0.0129073 Vali Loss: 0.0190527 Test Loss: 0.0252187\n",
      "Validation loss decreased (0.019059 --> 0.019053).  Saving model ...\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0122460\n",
      "\tspeed: 0.0930s/iter; left time: 1335.6076s\n",
      "\titers: 200, epoch: 37 | loss: 0.0127358\n",
      "\tspeed: 0.0276s/iter; left time: 393.1703s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:08.31s\n",
      "Steps: 226 | Train Loss: 0.0128986 Vali Loss: 0.0190852 Test Loss: 0.0251907\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0129839\n",
      "\tspeed: 0.0619s/iter; left time: 875.1020s\n",
      "\titers: 200, epoch: 38 | loss: 0.0129735\n",
      "\tspeed: 0.0255s/iter; left time: 358.6629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:06.07s\n",
      "Steps: 226 | Train Loss: 0.0128992 Vali Loss: 0.0190683 Test Loss: 0.0251875\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0111598\n",
      "\tspeed: 0.0940s/iter; left time: 1307.5915s\n",
      "\titers: 200, epoch: 39 | loss: 0.0126943\n",
      "\tspeed: 0.0273s/iter; left time: 377.1097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:07.86s\n",
      "Steps: 226 | Train Loss: 0.0128922 Vali Loss: 0.0191263 Test Loss: 0.0252117\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0118141\n",
      "\tspeed: 0.0635s/iter; left time: 869.0862s\n",
      "\titers: 200, epoch: 40 | loss: 0.0135553\n",
      "\tspeed: 0.0310s/iter; left time: 421.7236s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:07.20s\n",
      "Steps: 226 | Train Loss: 0.0128836 Vali Loss: 0.0190870 Test Loss: 0.0252051\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0121245\n",
      "\tspeed: 0.0895s/iter; left time: 1204.1751s\n",
      "\titers: 200, epoch: 41 | loss: 0.0123877\n",
      "\tspeed: 0.0282s/iter; left time: 377.3275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 226 | Train Loss: 0.0128702 Vali Loss: 0.0190757 Test Loss: 0.0252135\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0133441\n",
      "\tspeed: 0.0663s/iter; left time: 877.9292s\n",
      "\titers: 200, epoch: 42 | loss: 0.0115185\n",
      "\tspeed: 0.0244s/iter; left time: 320.9594s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.08s\n",
      "Steps: 226 | Train Loss: 0.0128975 Vali Loss: 0.0190773 Test Loss: 0.0251964\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0139865\n",
      "\tspeed: 0.0925s/iter; left time: 1203.5689s\n",
      "\titers: 200, epoch: 43 | loss: 0.0128980\n",
      "\tspeed: 0.0291s/iter; left time: 375.6127s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 226 | Train Loss: 0.0128922 Vali Loss: 0.0190738 Test Loss: 0.0251991\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0123033\n",
      "\tspeed: 0.0670s/iter; left time: 856.5725s\n",
      "\titers: 200, epoch: 44 | loss: 0.0136023\n",
      "\tspeed: 0.0352s/iter; left time: 446.0972s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 226 | Train Loss: 0.0128689 Vali Loss: 0.0191055 Test Loss: 0.0252051\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0129680\n",
      "\tspeed: 0.0867s/iter; left time: 1089.1394s\n",
      "\titers: 200, epoch: 45 | loss: 0.0121593\n",
      "\tspeed: 0.0256s/iter; left time: 318.6889s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0128768 Vali Loss: 0.0191128 Test Loss: 0.0252108\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0136561\n",
      "\tspeed: 0.0652s/iter; left time: 804.3159s\n",
      "\titers: 200, epoch: 46 | loss: 0.0120851\n",
      "\tspeed: 0.0356s/iter; left time: 436.0138s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 226 | Train Loss: 0.0128930 Vali Loss: 0.0190865 Test Loss: 0.0252188\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02521873079240322, rmse:0.1588040590286255, mae:0.10287422686815262, rse:0.5478290319442749\n",
      "Intermediate time for GB and pred_len 24: 00h:10m:12.15s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0354775\n",
      "\tspeed: 0.0686s/iter; left time: 1537.7750s\n",
      "\titers: 200, epoch: 1 | loss: 0.0330818\n",
      "\tspeed: 0.0267s/iter; left time: 595.9722s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0363572 Vali Loss: 0.0369944 Test Loss: 0.0533514\n",
      "Validation loss decreased (inf --> 0.036994).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0240189\n",
      "\tspeed: 0.0687s/iter; left time: 1522.9947s\n",
      "\titers: 200, epoch: 2 | loss: 0.0249086\n",
      "\tspeed: 0.0230s/iter; left time: 507.3656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 225 | Train Loss: 0.0249381 Vali Loss: 0.0303441 Test Loss: 0.0435347\n",
      "Validation loss decreased (0.036994 --> 0.030344).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0232706\n",
      "\tspeed: 0.0965s/iter; left time: 2118.9901s\n",
      "\titers: 200, epoch: 3 | loss: 0.0220721\n",
      "\tspeed: 0.0259s/iter; left time: 566.2517s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 225 | Train Loss: 0.0230414 Vali Loss: 0.0300716 Test Loss: 0.0431415\n",
      "Validation loss decreased (0.030344 --> 0.030072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0217916\n",
      "\tspeed: 0.0679s/iter; left time: 1475.3727s\n",
      "\titers: 200, epoch: 4 | loss: 0.0222335\n",
      "\tspeed: 0.0246s/iter; left time: 531.0174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 225 | Train Loss: 0.0226242 Vali Loss: 0.0299814 Test Loss: 0.0439132\n",
      "Validation loss decreased (0.030072 --> 0.029981).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0231342\n",
      "\tspeed: 0.0942s/iter; left time: 2026.1295s\n",
      "\titers: 200, epoch: 5 | loss: 0.0215362\n",
      "\tspeed: 0.0238s/iter; left time: 508.9080s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.90s\n",
      "Steps: 225 | Train Loss: 0.0223468 Vali Loss: 0.0297319 Test Loss: 0.0439771\n",
      "Validation loss decreased (0.029981 --> 0.029732).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0237017\n",
      "\tspeed: 0.0668s/iter; left time: 1421.6003s\n",
      "\titers: 200, epoch: 6 | loss: 0.0222148\n",
      "\tspeed: 0.0246s/iter; left time: 521.7184s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.04s\n",
      "Steps: 225 | Train Loss: 0.0221197 Vali Loss: 0.0296820 Test Loss: 0.0441826\n",
      "Validation loss decreased (0.029732 --> 0.029682).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0203015\n",
      "\tspeed: 0.0928s/iter; left time: 1954.4820s\n",
      "\titers: 200, epoch: 7 | loss: 0.0225045\n",
      "\tspeed: 0.0318s/iter; left time: 665.6530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 225 | Train Loss: 0.0219527 Vali Loss: 0.0296131 Test Loss: 0.0441187\n",
      "Validation loss decreased (0.029682 --> 0.029613).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0205286\n",
      "\tspeed: 0.0717s/iter; left time: 1492.2687s\n",
      "\titers: 200, epoch: 8 | loss: 0.0234542\n",
      "\tspeed: 0.0284s/iter; left time: 588.1607s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 225 | Train Loss: 0.0218284 Vali Loss: 0.0296417 Test Loss: 0.0449920\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0223067\n",
      "\tspeed: 0.0972s/iter; left time: 2002.5576s\n",
      "\titers: 200, epoch: 9 | loss: 0.0220027\n",
      "\tspeed: 0.0262s/iter; left time: 536.1908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 225 | Train Loss: 0.0217147 Vali Loss: 0.0295708 Test Loss: 0.0446208\n",
      "Validation loss decreased (0.029613 --> 0.029571).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0218705\n",
      "\tspeed: 0.0752s/iter; left time: 1532.2417s\n",
      "\titers: 200, epoch: 10 | loss: 0.0221990\n",
      "\tspeed: 0.0290s/iter; left time: 587.0119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 225 | Train Loss: 0.0215910 Vali Loss: 0.0298560 Test Loss: 0.0454434\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0200138\n",
      "\tspeed: 0.0936s/iter; left time: 1886.5657s\n",
      "\titers: 200, epoch: 11 | loss: 0.0235559\n",
      "\tspeed: 0.0270s/iter; left time: 542.0653s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 225 | Train Loss: 0.0214932 Vali Loss: 0.0295766 Test Loss: 0.0446929\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0203710\n",
      "\tspeed: 0.0709s/iter; left time: 1412.6928s\n",
      "\titers: 200, epoch: 12 | loss: 0.0218972\n",
      "\tspeed: 0.0219s/iter; left time: 433.9851s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.04s\n",
      "Steps: 225 | Train Loss: 0.0214143 Vali Loss: 0.0297747 Test Loss: 0.0450747\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0206936\n",
      "\tspeed: 0.0502s/iter; left time: 989.9310s\n",
      "\titers: 200, epoch: 13 | loss: 0.0215404\n",
      "\tspeed: 0.0265s/iter; left time: 520.3283s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.07s\n",
      "Steps: 225 | Train Loss: 0.0213293 Vali Loss: 0.0299438 Test Loss: 0.0450182\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0214430\n",
      "\tspeed: 0.0658s/iter; left time: 1281.0194s\n",
      "\titers: 200, epoch: 14 | loss: 0.0220037\n",
      "\tspeed: 0.0329s/iter; left time: 637.8399s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.40s\n",
      "Steps: 225 | Train Loss: 0.0212775 Vali Loss: 0.0300351 Test Loss: 0.0455550\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0208235\n",
      "\tspeed: 0.1150s/iter; left time: 2214.7338s\n",
      "\titers: 200, epoch: 15 | loss: 0.0200474\n",
      "\tspeed: 0.0219s/iter; left time: 419.5546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 225 | Train Loss: 0.0211910 Vali Loss: 0.0299286 Test Loss: 0.0452272\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0191070\n",
      "\tspeed: 0.0546s/iter; left time: 1039.2616s\n",
      "\titers: 200, epoch: 16 | loss: 0.0219055\n",
      "\tspeed: 0.0220s/iter; left time: 416.4554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 225 | Train Loss: 0.0211475 Vali Loss: 0.0298784 Test Loss: 0.0450351\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0200941\n",
      "\tspeed: 0.0658s/iter; left time: 1237.6131s\n",
      "\titers: 200, epoch: 17 | loss: 0.0211507\n",
      "\tspeed: 0.0361s/iter; left time: 675.0819s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.91s\n",
      "Steps: 225 | Train Loss: 0.0210881 Vali Loss: 0.0300969 Test Loss: 0.0456566\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0223482\n",
      "\tspeed: 0.0953s/iter; left time: 1770.0382s\n",
      "\titers: 200, epoch: 18 | loss: 0.0198237\n",
      "\tspeed: 0.0341s/iter; left time: 629.4172s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:08.99s\n",
      "Steps: 225 | Train Loss: 0.0210304 Vali Loss: 0.0299435 Test Loss: 0.0452439\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0211381\n",
      "\tspeed: 0.0916s/iter; left time: 1681.6391s\n",
      "\titers: 200, epoch: 19 | loss: 0.0200907\n",
      "\tspeed: 0.0294s/iter; left time: 535.7005s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.15s\n",
      "Steps: 225 | Train Loss: 0.0209759 Vali Loss: 0.0300613 Test Loss: 0.0456903\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.0446208193898201, rmse:0.2112364023923874, mae:0.14619070291519165, rse:0.7304849028587341\n",
      "Intermediate time for GB and pred_len 96: 00h:03m:38.99s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0390243\n",
      "\tspeed: 0.0472s/iter; left time: 1057.3378s\n",
      "\titers: 200, epoch: 1 | loss: 0.0298407\n",
      "\tspeed: 0.0390s/iter; left time: 870.4603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.15s\n",
      "Steps: 225 | Train Loss: 0.0378106 Vali Loss: 0.0389916 Test Loss: 0.0558763\n",
      "Validation loss decreased (inf --> 0.038992).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0256391\n",
      "\tspeed: 0.1145s/iter; left time: 2540.2143s\n",
      "\titers: 200, epoch: 2 | loss: 0.0244381\n",
      "\tspeed: 0.0264s/iter; left time: 583.3081s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.73s\n",
      "Steps: 225 | Train Loss: 0.0269063 Vali Loss: 0.0326266 Test Loss: 0.0464468\n",
      "Validation loss decreased (0.038992 --> 0.032627).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0238225\n",
      "\tspeed: 0.0938s/iter; left time: 2058.0264s\n",
      "\titers: 200, epoch: 3 | loss: 0.0257555\n",
      "\tspeed: 0.0484s/iter; left time: 1056.7327s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.17s\n",
      "Steps: 225 | Train Loss: 0.0248508 Vali Loss: 0.0326770 Test Loss: 0.0472591\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0263294\n",
      "\tspeed: 0.0825s/iter; left time: 1792.8509s\n",
      "\titers: 200, epoch: 4 | loss: 0.0246007\n",
      "\tspeed: 0.0388s/iter; left time: 838.5596s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.92s\n",
      "Steps: 225 | Train Loss: 0.0244757 Vali Loss: 0.0326091 Test Loss: 0.0475681\n",
      "Validation loss decreased (0.032627 --> 0.032609).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0237154\n",
      "\tspeed: 0.0802s/iter; left time: 1723.6471s\n",
      "\titers: 200, epoch: 5 | loss: 0.0240872\n",
      "\tspeed: 0.0502s/iter; left time: 1073.5185s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.67s\n",
      "Steps: 225 | Train Loss: 0.0241992 Vali Loss: 0.0324431 Test Loss: 0.0476243\n",
      "Validation loss decreased (0.032609 --> 0.032443).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0234693\n",
      "\tspeed: 0.0744s/iter; left time: 1582.3650s\n",
      "\titers: 200, epoch: 6 | loss: 0.0254878\n",
      "\tspeed: 0.0304s/iter; left time: 643.9315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.37s\n",
      "Steps: 225 | Train Loss: 0.0239851 Vali Loss: 0.0322094 Test Loss: 0.0475896\n",
      "Validation loss decreased (0.032443 --> 0.032209).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0246653\n",
      "\tspeed: 0.1300s/iter; left time: 2736.5306s\n",
      "\titers: 200, epoch: 7 | loss: 0.0230549\n",
      "\tspeed: 0.0268s/iter; left time: 560.8158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.66s\n",
      "Steps: 225 | Train Loss: 0.0238235 Vali Loss: 0.0325111 Test Loss: 0.0482226\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0213825\n",
      "\tspeed: 0.0816s/iter; left time: 1699.7433s\n",
      "\titers: 200, epoch: 8 | loss: 0.0244884\n",
      "\tspeed: 0.0459s/iter; left time: 952.1830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.24s\n",
      "Steps: 225 | Train Loss: 0.0236538 Vali Loss: 0.0322481 Test Loss: 0.0477532\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0229579\n",
      "\tspeed: 0.0895s/iter; left time: 1844.4827s\n",
      "\titers: 200, epoch: 9 | loss: 0.0238026\n",
      "\tspeed: 0.0478s/iter; left time: 980.1436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.78s\n",
      "Steps: 225 | Train Loss: 0.0235430 Vali Loss: 0.0323157 Test Loss: 0.0479474\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0232745\n",
      "\tspeed: 0.0847s/iter; left time: 1725.6330s\n",
      "\titers: 200, epoch: 10 | loss: 0.0231921\n",
      "\tspeed: 0.0435s/iter; left time: 881.2897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.18s\n",
      "Steps: 225 | Train Loss: 0.0234294 Vali Loss: 0.0321824 Test Loss: 0.0477643\n",
      "Validation loss decreased (0.032209 --> 0.032182).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0238903\n",
      "\tspeed: 0.0957s/iter; left time: 1927.6426s\n",
      "\titers: 200, epoch: 11 | loss: 0.0227449\n",
      "\tspeed: 0.0348s/iter; left time: 697.2737s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.91s\n",
      "Steps: 225 | Train Loss: 0.0233082 Vali Loss: 0.0324118 Test Loss: 0.0482034\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0227333\n",
      "\tspeed: 0.1300s/iter; left time: 2589.4023s\n",
      "\titers: 200, epoch: 12 | loss: 0.0233488\n",
      "\tspeed: 0.0328s/iter; left time: 649.5956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.45s\n",
      "Steps: 225 | Train Loss: 0.0232158 Vali Loss: 0.0323680 Test Loss: 0.0480370\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0221179\n",
      "\tspeed: 0.0814s/iter; left time: 1604.5093s\n",
      "\titers: 200, epoch: 13 | loss: 0.0217740\n",
      "\tspeed: 0.0567s/iter; left time: 1110.6467s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 225 | Train Loss: 0.0231166 Vali Loss: 0.0324860 Test Loss: 0.0483295\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0239112\n",
      "\tspeed: 0.0686s/iter; left time: 1336.9318s\n",
      "\titers: 200, epoch: 14 | loss: 0.0226446\n",
      "\tspeed: 0.0337s/iter; left time: 652.8516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.72s\n",
      "Steps: 225 | Train Loss: 0.0230206 Vali Loss: 0.0327152 Test Loss: 0.0488690\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0224236\n",
      "\tspeed: 0.0834s/iter; left time: 1604.9443s\n",
      "\titers: 200, epoch: 15 | loss: 0.0222827\n",
      "\tspeed: 0.0227s/iter; left time: 434.2060s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.02s\n",
      "Steps: 225 | Train Loss: 0.0229603 Vali Loss: 0.0326154 Test Loss: 0.0486470\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0238628\n",
      "\tspeed: 0.0495s/iter; left time: 942.7203s\n",
      "\titers: 200, epoch: 16 | loss: 0.0237833\n",
      "\tspeed: 0.0212s/iter; left time: 400.9043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.42s\n",
      "Steps: 225 | Train Loss: 0.0228710 Vali Loss: 0.0326586 Test Loss: 0.0487534\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0247075\n",
      "\tspeed: 0.0836s/iter; left time: 1570.8369s\n",
      "\titers: 200, epoch: 17 | loss: 0.0234942\n",
      "\tspeed: 0.0478s/iter; left time: 893.5717s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.22s\n",
      "Steps: 225 | Train Loss: 0.0228217 Vali Loss: 0.0326492 Test Loss: 0.0486032\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0229605\n",
      "\tspeed: 0.1296s/iter; left time: 2407.8405s\n",
      "\titers: 200, epoch: 18 | loss: 0.0239278\n",
      "\tspeed: 0.0473s/iter; left time: 873.8104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 225 | Train Loss: 0.0227531 Vali Loss: 0.0326073 Test Loss: 0.0483531\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0222851\n",
      "\tspeed: 0.1721s/iter; left time: 3158.0260s\n",
      "\titers: 200, epoch: 19 | loss: 0.0235539\n",
      "\tspeed: 0.0485s/iter; left time: 886.0591s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.88s\n",
      "Steps: 225 | Train Loss: 0.0227298 Vali Loss: 0.0328842 Test Loss: 0.0493051\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0239972\n",
      "\tspeed: 0.1419s/iter; left time: 2572.1804s\n",
      "\titers: 200, epoch: 20 | loss: 0.0237402\n",
      "\tspeed: 0.0644s/iter; left time: 1160.7873s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:15.44s\n",
      "Steps: 225 | Train Loss: 0.0226613 Vali Loss: 0.0329440 Test Loss: 0.0493550\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04776432737708092, rmse:0.21855051815509796, mae:0.1530516892671585, rse:0.7577460408210754\n",
      "Intermediate time for GB and pred_len 168: 00h:04m:48.85s\n",
      "Intermediate time for GB: 00h:18m:39.98s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0344858\n",
      "\tspeed: 0.0586s/iter; left time: 1317.9860s\n",
      "\titers: 200, epoch: 1 | loss: 0.0248702\n",
      "\tspeed: 0.0331s/iter; left time: 742.0129s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.54s\n",
      "Steps: 226 | Train Loss: 0.0371519 Vali Loss: 0.0223229 Test Loss: 0.0300770\n",
      "Validation loss decreased (inf --> 0.022323).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0126206\n",
      "\tspeed: 0.0825s/iter; left time: 1837.7030s\n",
      "\titers: 200, epoch: 2 | loss: 0.0100498\n",
      "\tspeed: 0.0363s/iter; left time: 805.0051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.25s\n",
      "Steps: 226 | Train Loss: 0.0132151 Vali Loss: 0.0098500 Test Loss: 0.0125102\n",
      "Validation loss decreased (0.022323 --> 0.009850).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0103429\n",
      "\tspeed: 0.1038s/iter; left time: 2288.8369s\n",
      "\titers: 200, epoch: 3 | loss: 0.0100795\n",
      "\tspeed: 0.0363s/iter; left time: 796.5311s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.20s\n",
      "Steps: 226 | Train Loss: 0.0103717 Vali Loss: 0.0093203 Test Loss: 0.0118425\n",
      "Validation loss decreased (0.009850 --> 0.009320).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0096413\n",
      "\tspeed: 0.0780s/iter; left time: 1702.4721s\n",
      "\titers: 200, epoch: 4 | loss: 0.0083005\n",
      "\tspeed: 0.0362s/iter; left time: 787.0372s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 226 | Train Loss: 0.0097180 Vali Loss: 0.0089005 Test Loss: 0.0113179\n",
      "Validation loss decreased (0.009320 --> 0.008900).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0097897\n",
      "\tspeed: 0.1184s/iter; left time: 2556.5842s\n",
      "\titers: 200, epoch: 5 | loss: 0.0100584\n",
      "\tspeed: 0.0438s/iter; left time: 942.4198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.74s\n",
      "Steps: 226 | Train Loss: 0.0092799 Vali Loss: 0.0086677 Test Loss: 0.0109406\n",
      "Validation loss decreased (0.008900 --> 0.008668).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0089060\n",
      "\tspeed: 0.0910s/iter; left time: 1944.6028s\n",
      "\titers: 200, epoch: 6 | loss: 0.0081647\n",
      "\tspeed: 0.0494s/iter; left time: 1050.5381s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.89s\n",
      "Steps: 226 | Train Loss: 0.0089618 Vali Loss: 0.0084586 Test Loss: 0.0106541\n",
      "Validation loss decreased (0.008668 --> 0.008459).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0075015\n",
      "\tspeed: 0.0816s/iter; left time: 1724.9503s\n",
      "\titers: 200, epoch: 7 | loss: 0.0088196\n",
      "\tspeed: 0.0356s/iter; left time: 749.9036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.21s\n",
      "Steps: 226 | Train Loss: 0.0087471 Vali Loss: 0.0083825 Test Loss: 0.0105173\n",
      "Validation loss decreased (0.008459 --> 0.008382).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0080909\n",
      "\tspeed: 0.0974s/iter; left time: 2036.4737s\n",
      "\titers: 200, epoch: 8 | loss: 0.0087270\n",
      "\tspeed: 0.0367s/iter; left time: 764.9337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.05s\n",
      "Steps: 226 | Train Loss: 0.0085740 Vali Loss: 0.0082854 Test Loss: 0.0104146\n",
      "Validation loss decreased (0.008382 --> 0.008285).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0079280\n",
      "\tspeed: 0.0860s/iter; left time: 1778.7867s\n",
      "\titers: 200, epoch: 9 | loss: 0.0090820\n",
      "\tspeed: 0.0367s/iter; left time: 755.8939s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:09.18s\n",
      "Steps: 226 | Train Loss: 0.0084567 Vali Loss: 0.0082317 Test Loss: 0.0103206\n",
      "Validation loss decreased (0.008285 --> 0.008232).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0085438\n",
      "\tspeed: 0.0975s/iter; left time: 1995.1632s\n",
      "\titers: 200, epoch: 10 | loss: 0.0085556\n",
      "\tspeed: 0.0371s/iter; left time: 756.1211s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.44s\n",
      "Steps: 226 | Train Loss: 0.0083482 Vali Loss: 0.0081226 Test Loss: 0.0101976\n",
      "Validation loss decreased (0.008232 --> 0.008123).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0075712\n",
      "\tspeed: 0.1200s/iter; left time: 2428.1962s\n",
      "\titers: 200, epoch: 11 | loss: 0.0083323\n",
      "\tspeed: 0.0532s/iter; left time: 1071.0822s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.94s\n",
      "Steps: 226 | Train Loss: 0.0082553 Vali Loss: 0.0080851 Test Loss: 0.0101538\n",
      "Validation loss decreased (0.008123 --> 0.008085).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0083999\n",
      "\tspeed: 0.0765s/iter; left time: 1531.4764s\n",
      "\titers: 200, epoch: 12 | loss: 0.0088925\n",
      "\tspeed: 0.0375s/iter; left time: 747.2862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.31s\n",
      "Steps: 226 | Train Loss: 0.0081682 Vali Loss: 0.0080263 Test Loss: 0.0100752\n",
      "Validation loss decreased (0.008085 --> 0.008026).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0076128\n",
      "\tspeed: 0.1027s/iter; left time: 2031.5328s\n",
      "\titers: 200, epoch: 13 | loss: 0.0087484\n",
      "\tspeed: 0.0342s/iter; left time: 672.8203s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:09.51s\n",
      "Steps: 226 | Train Loss: 0.0080892 Vali Loss: 0.0079926 Test Loss: 0.0100276\n",
      "Validation loss decreased (0.008026 --> 0.007993).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0075470\n",
      "\tspeed: 0.0834s/iter; left time: 1632.1813s\n",
      "\titers: 200, epoch: 14 | loss: 0.0081287\n",
      "\tspeed: 0.0356s/iter; left time: 692.3757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:09.18s\n",
      "Steps: 226 | Train Loss: 0.0080240 Vali Loss: 0.0079453 Test Loss: 0.0099897\n",
      "Validation loss decreased (0.007993 --> 0.007945).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0072469\n",
      "\tspeed: 0.1014s/iter; left time: 1961.2359s\n",
      "\titers: 200, epoch: 15 | loss: 0.0084926\n",
      "\tspeed: 0.0315s/iter; left time: 605.9885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 226 | Train Loss: 0.0079678 Vali Loss: 0.0079108 Test Loss: 0.0099600\n",
      "Validation loss decreased (0.007945 --> 0.007911).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0075845\n",
      "\tspeed: 0.0880s/iter; left time: 1682.6505s\n",
      "\titers: 200, epoch: 16 | loss: 0.0070598\n",
      "\tspeed: 0.0599s/iter; left time: 1137.8521s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:12.37s\n",
      "Steps: 226 | Train Loss: 0.0079283 Vali Loss: 0.0079144 Test Loss: 0.0099446\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0067384\n",
      "\tspeed: 0.1023s/iter; left time: 1932.4217s\n",
      "\titers: 200, epoch: 17 | loss: 0.0072390\n",
      "\tspeed: 0.0337s/iter; left time: 632.5663s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 226 | Train Loss: 0.0078908 Vali Loss: 0.0078561 Test Loss: 0.0099173\n",
      "Validation loss decreased (0.007911 --> 0.007856).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0073018\n",
      "\tspeed: 0.0968s/iter; left time: 1806.9276s\n",
      "\titers: 200, epoch: 18 | loss: 0.0074447\n",
      "\tspeed: 0.0376s/iter; left time: 698.1296s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.40s\n",
      "Steps: 226 | Train Loss: 0.0078563 Vali Loss: 0.0078118 Test Loss: 0.0098345\n",
      "Validation loss decreased (0.007856 --> 0.007812).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0085467\n",
      "\tspeed: 0.0538s/iter; left time: 990.9492s\n",
      "\titers: 200, epoch: 19 | loss: 0.0077147\n",
      "\tspeed: 0.0182s/iter; left time: 333.3859s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0078181 Vali Loss: 0.0077873 Test Loss: 0.0098552\n",
      "Validation loss decreased (0.007812 --> 0.007787).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0075294\n",
      "\tspeed: 0.0468s/iter; left time: 851.3470s\n",
      "\titers: 200, epoch: 20 | loss: 0.0082452\n",
      "\tspeed: 0.0281s/iter; left time: 508.9106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.86s\n",
      "Steps: 226 | Train Loss: 0.0078004 Vali Loss: 0.0077559 Test Loss: 0.0098293\n",
      "Validation loss decreased (0.007787 --> 0.007756).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0079917\n",
      "\tspeed: 0.0842s/iter; left time: 1513.5368s\n",
      "\titers: 200, epoch: 21 | loss: 0.0068666\n",
      "\tspeed: 0.0323s/iter; left time: 576.8380s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.05s\n",
      "Steps: 226 | Train Loss: 0.0077771 Vali Loss: 0.0078368 Test Loss: 0.0098692\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0077353\n",
      "\tspeed: 0.0889s/iter; left time: 1577.8287s\n",
      "\titers: 200, epoch: 22 | loss: 0.0068657\n",
      "\tspeed: 0.0471s/iter; left time: 831.0285s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.17s\n",
      "Steps: 226 | Train Loss: 0.0077436 Vali Loss: 0.0077783 Test Loss: 0.0098262\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0081617\n",
      "\tspeed: 0.1120s/iter; left time: 1962.9438s\n",
      "\titers: 200, epoch: 23 | loss: 0.0074716\n",
      "\tspeed: 0.0316s/iter; left time: 550.7875s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:09.12s\n",
      "Steps: 226 | Train Loss: 0.0077267 Vali Loss: 0.0078055 Test Loss: 0.0098338\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0069516\n",
      "\tspeed: 0.0802s/iter; left time: 1387.4038s\n",
      "\titers: 200, epoch: 24 | loss: 0.0089964\n",
      "\tspeed: 0.0335s/iter; left time: 576.6403s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:08.16s\n",
      "Steps: 226 | Train Loss: 0.0077165 Vali Loss: 0.0077400 Test Loss: 0.0097997\n",
      "Validation loss decreased (0.007756 --> 0.007740).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0072426\n",
      "\tspeed: 0.0914s/iter; left time: 1560.7434s\n",
      "\titers: 200, epoch: 25 | loss: 0.0081069\n",
      "\tspeed: 0.0356s/iter; left time: 605.0396s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:08.11s\n",
      "Steps: 226 | Train Loss: 0.0076998 Vali Loss: 0.0077898 Test Loss: 0.0098343\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0074234\n",
      "\tspeed: 0.0812s/iter; left time: 1367.8907s\n",
      "\titers: 200, epoch: 26 | loss: 0.0075612\n",
      "\tspeed: 0.0378s/iter; left time: 633.6596s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:08.70s\n",
      "Steps: 226 | Train Loss: 0.0076881 Vali Loss: 0.0077471 Test Loss: 0.0097822\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0090928\n",
      "\tspeed: 0.0893s/iter; left time: 1483.8526s\n",
      "\titers: 200, epoch: 27 | loss: 0.0066571\n",
      "\tspeed: 0.0363s/iter; left time: 599.9501s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.69s\n",
      "Steps: 226 | Train Loss: 0.0076465 Vali Loss: 0.0077481 Test Loss: 0.0098086\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0076121\n",
      "\tspeed: 0.0916s/iter; left time: 1502.1085s\n",
      "\titers: 200, epoch: 28 | loss: 0.0068969\n",
      "\tspeed: 0.0433s/iter; left time: 705.7046s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:10.38s\n",
      "Steps: 226 | Train Loss: 0.0076703 Vali Loss: 0.0077220 Test Loss: 0.0097674\n",
      "Validation loss decreased (0.007740 --> 0.007722).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0082714\n",
      "\tspeed: 0.0915s/iter; left time: 1480.2814s\n",
      "\titers: 200, epoch: 29 | loss: 0.0075957\n",
      "\tspeed: 0.0354s/iter; left time: 568.5146s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.94s\n",
      "Steps: 226 | Train Loss: 0.0076475 Vali Loss: 0.0077353 Test Loss: 0.0097849\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0068515\n",
      "\tspeed: 0.0765s/iter; left time: 1220.2614s\n",
      "\titers: 200, epoch: 30 | loss: 0.0069599\n",
      "\tspeed: 0.0322s/iter; left time: 510.8528s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.49s\n",
      "Steps: 226 | Train Loss: 0.0076292 Vali Loss: 0.0077237 Test Loss: 0.0097906\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0075222\n",
      "\tspeed: 0.0819s/iter; left time: 1287.5022s\n",
      "\titers: 200, epoch: 31 | loss: 0.0075825\n",
      "\tspeed: 0.0348s/iter; left time: 543.7270s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.12s\n",
      "Steps: 226 | Train Loss: 0.0076272 Vali Loss: 0.0077313 Test Loss: 0.0097910\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0078725\n",
      "\tspeed: 0.0743s/iter; left time: 1150.6650s\n",
      "\titers: 200, epoch: 32 | loss: 0.0086332\n",
      "\tspeed: 0.0293s/iter; left time: 450.9699s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:07.28s\n",
      "Steps: 226 | Train Loss: 0.0076108 Vali Loss: 0.0077107 Test Loss: 0.0097691\n",
      "Validation loss decreased (0.007722 --> 0.007711).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0079413\n",
      "\tspeed: 0.0801s/iter; left time: 1223.1830s\n",
      "\titers: 200, epoch: 33 | loss: 0.0084611\n",
      "\tspeed: 0.0341s/iter; left time: 517.1612s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:08.25s\n",
      "Steps: 226 | Train Loss: 0.0076104 Vali Loss: 0.0077472 Test Loss: 0.0097762\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0080443\n",
      "\tspeed: 0.0894s/iter; left time: 1344.5777s\n",
      "\titers: 200, epoch: 34 | loss: 0.0076971\n",
      "\tspeed: 0.0405s/iter; left time: 605.0621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:09.29s\n",
      "Steps: 226 | Train Loss: 0.0076043 Vali Loss: 0.0077256 Test Loss: 0.0097612\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0085065\n",
      "\tspeed: 0.0728s/iter; left time: 1078.0853s\n",
      "\titers: 200, epoch: 35 | loss: 0.0081995\n",
      "\tspeed: 0.0331s/iter; left time: 487.7798s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 226 | Train Loss: 0.0075912 Vali Loss: 0.0077143 Test Loss: 0.0097552\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0068593\n",
      "\tspeed: 0.0725s/iter; left time: 1057.8870s\n",
      "\titers: 200, epoch: 36 | loss: 0.0077889\n",
      "\tspeed: 0.0355s/iter; left time: 514.4121s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:07.55s\n",
      "Steps: 226 | Train Loss: 0.0075942 Vali Loss: 0.0077039 Test Loss: 0.0097492\n",
      "Validation loss decreased (0.007711 --> 0.007704).  Saving model ...\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0083970\n",
      "\tspeed: 0.0668s/iter; left time: 960.0787s\n",
      "\titers: 200, epoch: 37 | loss: 0.0081276\n",
      "\tspeed: 0.0272s/iter; left time: 387.4390s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:06.36s\n",
      "Steps: 226 | Train Loss: 0.0075800 Vali Loss: 0.0077205 Test Loss: 0.0097417\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0066566\n",
      "\tspeed: 0.0689s/iter; left time: 973.6672s\n",
      "\titers: 200, epoch: 38 | loss: 0.0076736\n",
      "\tspeed: 0.0346s/iter; left time: 485.8614s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 226 | Train Loss: 0.0075843 Vali Loss: 0.0077153 Test Loss: 0.0097522\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0073878\n",
      "\tspeed: 0.0648s/iter; left time: 900.9520s\n",
      "\titers: 200, epoch: 39 | loss: 0.0068948\n",
      "\tspeed: 0.0268s/iter; left time: 370.4303s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:06.64s\n",
      "Steps: 226 | Train Loss: 0.0075843 Vali Loss: 0.0077071 Test Loss: 0.0097395\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0067716\n",
      "\tspeed: 0.0828s/iter; left time: 1132.7915s\n",
      "\titers: 200, epoch: 40 | loss: 0.0069292\n",
      "\tspeed: 0.0422s/iter; left time: 572.9846s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:09.99s\n",
      "Steps: 226 | Train Loss: 0.0075804 Vali Loss: 0.0077128 Test Loss: 0.0097463\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0087412\n",
      "\tspeed: 0.0829s/iter; left time: 1116.0052s\n",
      "\titers: 200, epoch: 41 | loss: 0.0085838\n",
      "\tspeed: 0.0267s/iter; left time: 356.7318s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 226 | Train Loss: 0.0075678 Vali Loss: 0.0076696 Test Loss: 0.0097519\n",
      "Validation loss decreased (0.007704 --> 0.007670).  Saving model ...\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0066021\n",
      "\tspeed: 0.0680s/iter; left time: 899.8087s\n",
      "\titers: 200, epoch: 42 | loss: 0.0081806\n",
      "\tspeed: 0.0253s/iter; left time: 331.9343s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.38s\n",
      "Steps: 226 | Train Loss: 0.0075720 Vali Loss: 0.0076917 Test Loss: 0.0097417\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0064914\n",
      "\tspeed: 0.0693s/iter; left time: 901.4095s\n",
      "\titers: 200, epoch: 43 | loss: 0.0077684\n",
      "\tspeed: 0.0323s/iter; left time: 417.3040s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 226 | Train Loss: 0.0075833 Vali Loss: 0.0077075 Test Loss: 0.0097418\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0076977\n",
      "\tspeed: 0.0715s/iter; left time: 914.5500s\n",
      "\titers: 200, epoch: 44 | loss: 0.0064298\n",
      "\tspeed: 0.0252s/iter; left time: 319.6574s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:06.26s\n",
      "Steps: 226 | Train Loss: 0.0075701 Vali Loss: 0.0076917 Test Loss: 0.0097360\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0082549\n",
      "\tspeed: 0.0646s/iter; left time: 811.6309s\n",
      "\titers: 200, epoch: 45 | loss: 0.0081084\n",
      "\tspeed: 0.0309s/iter; left time: 385.3936s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 226 | Train Loss: 0.0075637 Vali Loss: 0.0077136 Test Loss: 0.0097361\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0077726\n",
      "\tspeed: 0.0839s/iter; left time: 1034.6099s\n",
      "\titers: 200, epoch: 46 | loss: 0.0061236\n",
      "\tspeed: 0.0348s/iter; left time: 425.2131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:08.18s\n",
      "Steps: 226 | Train Loss: 0.0075685 Vali Loss: 0.0076890 Test Loss: 0.0097379\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0080226\n",
      "\tspeed: 0.0846s/iter; left time: 1024.1546s\n",
      "\titers: 200, epoch: 47 | loss: 0.0073111\n",
      "\tspeed: 0.0332s/iter; left time: 398.0284s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:08.09s\n",
      "Steps: 226 | Train Loss: 0.0075626 Vali Loss: 0.0076897 Test Loss: 0.0097352\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0077425\n",
      "\tspeed: 0.0723s/iter; left time: 858.9998s\n",
      "\titers: 200, epoch: 48 | loss: 0.0077390\n",
      "\tspeed: 0.0303s/iter; left time: 356.9544s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 226 | Train Loss: 0.0075654 Vali Loss: 0.0077054 Test Loss: 0.0097400\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0079935\n",
      "\tspeed: 0.0697s/iter; left time: 812.1396s\n",
      "\titers: 200, epoch: 49 | loss: 0.0078852\n",
      "\tspeed: 0.0286s/iter; left time: 330.9500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:06.99s\n",
      "Steps: 226 | Train Loss: 0.0075531 Vali Loss: 0.0077219 Test Loss: 0.0097373\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0076052\n",
      "\tspeed: 0.0638s/iter; left time: 729.0553s\n",
      "\titers: 200, epoch: 50 | loss: 0.0080884\n",
      "\tspeed: 0.0307s/iter; left time: 347.2256s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:06.90s\n",
      "Steps: 226 | Train Loss: 0.0075651 Vali Loss: 0.0077060 Test Loss: 0.0097415\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0076148\n",
      "\tspeed: 0.0749s/iter; left time: 838.4543s\n",
      "\titers: 200, epoch: 51 | loss: 0.0078435\n",
      "\tspeed: 0.0270s/iter; left time: 299.3974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 226 | Train Loss: 0.0075558 Vali Loss: 0.0077239 Test Loss: 0.0097382\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009751889854669571, rmse:0.09875165671110153, mae:0.06079358235001564, rse:0.29061415791511536\n",
      "Intermediate time for ES and pred_len 24: 00h:10m:14.78s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0385214\n",
      "\tspeed: 0.0833s/iter; left time: 1866.7780s\n",
      "\titers: 200, epoch: 1 | loss: 0.0321006\n",
      "\tspeed: 0.0363s/iter; left time: 809.1241s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.77s\n",
      "Steps: 225 | Train Loss: 0.0402985 Vali Loss: 0.0268622 Test Loss: 0.0364986\n",
      "Validation loss decreased (inf --> 0.026862).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0202941\n",
      "\tspeed: 0.0986s/iter; left time: 2186.6123s\n",
      "\titers: 200, epoch: 2 | loss: 0.0176809\n",
      "\tspeed: 0.0383s/iter; left time: 846.5764s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 225 | Train Loss: 0.0199497 Vali Loss: 0.0171759 Test Loss: 0.0222667\n",
      "Validation loss decreased (0.026862 --> 0.017176).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0164886\n",
      "\tspeed: 0.0988s/iter; left time: 2169.5566s\n",
      "\titers: 200, epoch: 3 | loss: 0.0159799\n",
      "\tspeed: 0.0399s/iter; left time: 870.8837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.00s\n",
      "Steps: 225 | Train Loss: 0.0171304 Vali Loss: 0.0158539 Test Loss: 0.0206034\n",
      "Validation loss decreased (0.017176 --> 0.015854).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0139721\n",
      "\tspeed: 0.0899s/iter; left time: 1952.2639s\n",
      "\titers: 200, epoch: 4 | loss: 0.0155888\n",
      "\tspeed: 0.0408s/iter; left time: 882.6002s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.26s\n",
      "Steps: 225 | Train Loss: 0.0162038 Vali Loss: 0.0155706 Test Loss: 0.0199895\n",
      "Validation loss decreased (0.015854 --> 0.015571).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0160721\n",
      "\tspeed: 0.1031s/iter; left time: 2215.6828s\n",
      "\titers: 200, epoch: 5 | loss: 0.0148397\n",
      "\tspeed: 0.0516s/iter; left time: 1104.7086s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.61s\n",
      "Steps: 225 | Train Loss: 0.0157221 Vali Loss: 0.0152692 Test Loss: 0.0195936\n",
      "Validation loss decreased (0.015571 --> 0.015269).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0146115\n",
      "\tspeed: 0.1357s/iter; left time: 2888.1024s\n",
      "\titers: 200, epoch: 6 | loss: 0.0144678\n",
      "\tspeed: 0.0323s/iter; left time: 683.7279s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.32s\n",
      "Steps: 225 | Train Loss: 0.0153876 Vali Loss: 0.0152303 Test Loss: 0.0194437\n",
      "Validation loss decreased (0.015269 --> 0.015230).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0142163\n",
      "\tspeed: 0.0478s/iter; left time: 1005.6234s\n",
      "\titers: 200, epoch: 7 | loss: 0.0141031\n",
      "\tspeed: 0.0247s/iter; left time: 517.0438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 225 | Train Loss: 0.0151415 Vali Loss: 0.0150768 Test Loss: 0.0191790\n",
      "Validation loss decreased (0.015230 --> 0.015077).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0140387\n",
      "\tspeed: 0.0919s/iter; left time: 1913.4497s\n",
      "\titers: 200, epoch: 8 | loss: 0.0141506\n",
      "\tspeed: 0.0341s/iter; left time: 706.4841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.76s\n",
      "Steps: 225 | Train Loss: 0.0149555 Vali Loss: 0.0150878 Test Loss: 0.0191487\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0142673\n",
      "\tspeed: 0.1078s/iter; left time: 2221.1984s\n",
      "\titers: 200, epoch: 9 | loss: 0.0143092\n",
      "\tspeed: 0.0485s/iter; left time: 994.0081s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.68s\n",
      "Steps: 225 | Train Loss: 0.0148236 Vali Loss: 0.0148663 Test Loss: 0.0189639\n",
      "Validation loss decreased (0.015077 --> 0.014866).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0139251\n",
      "\tspeed: 0.0873s/iter; left time: 1779.8181s\n",
      "\titers: 200, epoch: 10 | loss: 0.0166310\n",
      "\tspeed: 0.0547s/iter; left time: 1109.0509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.05s\n",
      "Steps: 225 | Train Loss: 0.0146813 Vali Loss: 0.0150752 Test Loss: 0.0190709\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0141909\n",
      "\tspeed: 0.1272s/iter; left time: 2563.4074s\n",
      "\titers: 200, epoch: 11 | loss: 0.0153536\n",
      "\tspeed: 0.0197s/iter; left time: 394.5142s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.99s\n",
      "Steps: 225 | Train Loss: 0.0145726 Vali Loss: 0.0148738 Test Loss: 0.0189073\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0141518\n",
      "\tspeed: 0.0453s/iter; left time: 902.9086s\n",
      "\titers: 200, epoch: 12 | loss: 0.0146643\n",
      "\tspeed: 0.0243s/iter; left time: 482.5459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0144563 Vali Loss: 0.0147936 Test Loss: 0.0188262\n",
      "Validation loss decreased (0.014866 --> 0.014794).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0127954\n",
      "\tspeed: 0.0488s/iter; left time: 961.9703s\n",
      "\titers: 200, epoch: 13 | loss: 0.0135295\n",
      "\tspeed: 0.0303s/iter; left time: 593.0234s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 225 | Train Loss: 0.0143845 Vali Loss: 0.0148190 Test Loss: 0.0189337\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0153766\n",
      "\tspeed: 0.0791s/iter; left time: 1541.4361s\n",
      "\titers: 200, epoch: 14 | loss: 0.0150153\n",
      "\tspeed: 0.0212s/iter; left time: 409.8173s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.33s\n",
      "Steps: 225 | Train Loss: 0.0143134 Vali Loss: 0.0149541 Test Loss: 0.0189741\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0134031\n",
      "\tspeed: 0.0456s/iter; left time: 877.1835s\n",
      "\titers: 200, epoch: 15 | loss: 0.0160063\n",
      "\tspeed: 0.0202s/iter; left time: 387.6431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:04.71s\n",
      "Steps: 225 | Train Loss: 0.0142359 Vali Loss: 0.0148774 Test Loss: 0.0189182\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0137937\n",
      "\tspeed: 0.1028s/iter; left time: 1954.9851s\n",
      "\titers: 200, epoch: 16 | loss: 0.0153615\n",
      "\tspeed: 0.0205s/iter; left time: 388.5795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 225 | Train Loss: 0.0141616 Vali Loss: 0.0148836 Test Loss: 0.0189698\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0141673\n",
      "\tspeed: 0.0435s/iter; left time: 817.5058s\n",
      "\titers: 200, epoch: 17 | loss: 0.0134428\n",
      "\tspeed: 0.0214s/iter; left time: 399.2837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.71s\n",
      "Steps: 225 | Train Loss: 0.0140956 Vali Loss: 0.0147340 Test Loss: 0.0188355\n",
      "Validation loss decreased (0.014794 --> 0.014734).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0139518\n",
      "\tspeed: 0.0526s/iter; left time: 978.0026s\n",
      "\titers: 200, epoch: 18 | loss: 0.0137749\n",
      "\tspeed: 0.0361s/iter; left time: 666.3086s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.38s\n",
      "Steps: 225 | Train Loss: 0.0140458 Vali Loss: 0.0148421 Test Loss: 0.0189237\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0140153\n",
      "\tspeed: 0.0714s/iter; left time: 1309.4177s\n",
      "\titers: 200, epoch: 19 | loss: 0.0141719\n",
      "\tspeed: 0.0222s/iter; left time: 404.2952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 225 | Train Loss: 0.0139988 Vali Loss: 0.0146938 Test Loss: 0.0188097\n",
      "Validation loss decreased (0.014734 --> 0.014694).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0145078\n",
      "\tspeed: 0.0592s/iter; left time: 1072.6189s\n",
      "\titers: 200, epoch: 20 | loss: 0.0140767\n",
      "\tspeed: 0.0204s/iter; left time: 367.0104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.93s\n",
      "Steps: 225 | Train Loss: 0.0139585 Vali Loss: 0.0147689 Test Loss: 0.0188606\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0134230\n",
      "\tspeed: 0.0571s/iter; left time: 1021.9898s\n",
      "\titers: 200, epoch: 21 | loss: 0.0123201\n",
      "\tspeed: 0.0291s/iter; left time: 517.6158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 225 | Train Loss: 0.0139100 Vali Loss: 0.0148057 Test Loss: 0.0188332\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0146732\n",
      "\tspeed: 0.0608s/iter; left time: 1075.5546s\n",
      "\titers: 200, epoch: 22 | loss: 0.0162446\n",
      "\tspeed: 0.0191s/iter; left time: 336.5628s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:05.53s\n",
      "Steps: 225 | Train Loss: 0.0138737 Vali Loss: 0.0148652 Test Loss: 0.0188741\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0149700\n",
      "\tspeed: 0.0469s/iter; left time: 818.4077s\n",
      "\titers: 200, epoch: 23 | loss: 0.0135929\n",
      "\tspeed: 0.0218s/iter; left time: 378.8562s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.67s\n",
      "Steps: 225 | Train Loss: 0.0138603 Vali Loss: 0.0147573 Test Loss: 0.0188534\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0130955\n",
      "\tspeed: 0.1010s/iter; left time: 1740.1626s\n",
      "\titers: 200, epoch: 24 | loss: 0.0129737\n",
      "\tspeed: 0.0196s/iter; left time: 334.8176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:06.13s\n",
      "Steps: 225 | Train Loss: 0.0138433 Vali Loss: 0.0147270 Test Loss: 0.0188300\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0143310\n",
      "\tspeed: 0.0431s/iter; left time: 733.4137s\n",
      "\titers: 200, epoch: 25 | loss: 0.0139837\n",
      "\tspeed: 0.0193s/iter; left time: 327.0212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0137946 Vali Loss: 0.0148280 Test Loss: 0.0188872\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0130300\n",
      "\tspeed: 0.0537s/iter; left time: 901.2312s\n",
      "\titers: 200, epoch: 26 | loss: 0.0136986\n",
      "\tspeed: 0.0490s/iter; left time: 817.6382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:08.94s\n",
      "Steps: 225 | Train Loss: 0.0137725 Vali Loss: 0.0147710 Test Loss: 0.0188353\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0146367\n",
      "\tspeed: 0.0541s/iter; left time: 895.3023s\n",
      "\titers: 200, epoch: 27 | loss: 0.0136573\n",
      "\tspeed: 0.0217s/iter; left time: 357.0046s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:04.83s\n",
      "Steps: 225 | Train Loss: 0.0137646 Vali Loss: 0.0147977 Test Loss: 0.0189136\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0146420\n",
      "\tspeed: 0.0509s/iter; left time: 830.3653s\n",
      "\titers: 200, epoch: 28 | loss: 0.0124258\n",
      "\tspeed: 0.0311s/iter; left time: 503.9161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.29s\n",
      "Steps: 225 | Train Loss: 0.0137562 Vali Loss: 0.0147614 Test Loss: 0.0188549\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0153049\n",
      "\tspeed: 0.0791s/iter; left time: 1273.9197s\n",
      "\titers: 200, epoch: 29 | loss: 0.0134878\n",
      "\tspeed: 0.0185s/iter; left time: 296.3427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.19s\n",
      "Steps: 225 | Train Loss: 0.0137445 Vali Loss: 0.0147277 Test Loss: 0.0188331\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018809732049703598, rmse:0.13714857399463654, mae:0.08826389908790588, rse:0.40290123224258423\n",
      "Intermediate time for ES and pred_len 96: 00h:05m:14.75s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0418318\n",
      "\tspeed: 0.0686s/iter; left time: 1535.7814s\n",
      "\titers: 200, epoch: 1 | loss: 0.0310877\n",
      "\tspeed: 0.0289s/iter; left time: 645.2031s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.70s\n",
      "Steps: 225 | Train Loss: 0.0424433 Vali Loss: 0.0290079 Test Loss: 0.0391881\n",
      "Validation loss decreased (inf --> 0.029008).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0218836\n",
      "\tspeed: 0.0559s/iter; left time: 1240.2360s\n",
      "\titers: 200, epoch: 2 | loss: 0.0210375\n",
      "\tspeed: 0.0226s/iter; left time: 499.5937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 225 | Train Loss: 0.0218081 Vali Loss: 0.0196624 Test Loss: 0.0247310\n",
      "Validation loss decreased (0.029008 --> 0.019662).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0185089\n",
      "\tspeed: 0.1102s/iter; left time: 2418.0889s\n",
      "\titers: 200, epoch: 3 | loss: 0.0187048\n",
      "\tspeed: 0.0242s/iter; left time: 529.7523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.12s\n",
      "Steps: 225 | Train Loss: 0.0190434 Vali Loss: 0.0182646 Test Loss: 0.0228481\n",
      "Validation loss decreased (0.019662 --> 0.018265).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0182760\n",
      "\tspeed: 0.0615s/iter; left time: 1335.8476s\n",
      "\titers: 200, epoch: 4 | loss: 0.0178577\n",
      "\tspeed: 0.0228s/iter; left time: 493.3402s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 225 | Train Loss: 0.0180016 Vali Loss: 0.0176468 Test Loss: 0.0218411\n",
      "Validation loss decreased (0.018265 --> 0.017647).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0167117\n",
      "\tspeed: 0.1185s/iter; left time: 2547.2858s\n",
      "\titers: 200, epoch: 5 | loss: 0.0163538\n",
      "\tspeed: 0.0213s/iter; left time: 455.9758s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.10s\n",
      "Steps: 225 | Train Loss: 0.0174683 Vali Loss: 0.0175372 Test Loss: 0.0215410\n",
      "Validation loss decreased (0.017647 --> 0.017537).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0154899\n",
      "\tspeed: 0.0576s/iter; left time: 1226.5511s\n",
      "\titers: 200, epoch: 6 | loss: 0.0176745\n",
      "\tspeed: 0.0259s/iter; left time: 547.5189s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 225 | Train Loss: 0.0171330 Vali Loss: 0.0173297 Test Loss: 0.0213197\n",
      "Validation loss decreased (0.017537 --> 0.017330).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0173982\n",
      "\tspeed: 0.1044s/iter; left time: 2197.5805s\n",
      "\titers: 200, epoch: 7 | loss: 0.0163391\n",
      "\tspeed: 0.0251s/iter; left time: 525.8038s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.43s\n",
      "Steps: 225 | Train Loss: 0.0168822 Vali Loss: 0.0172685 Test Loss: 0.0212768\n",
      "Validation loss decreased (0.017330 --> 0.017269).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0175235\n",
      "\tspeed: 0.0601s/iter; left time: 1251.3915s\n",
      "\titers: 200, epoch: 8 | loss: 0.0173391\n",
      "\tspeed: 0.0359s/iter; left time: 744.8956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.12s\n",
      "Steps: 225 | Train Loss: 0.0166973 Vali Loss: 0.0172033 Test Loss: 0.0212305\n",
      "Validation loss decreased (0.017269 --> 0.017203).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0174077\n",
      "\tspeed: 0.0742s/iter; left time: 1528.0674s\n",
      "\titers: 200, epoch: 9 | loss: 0.0177108\n",
      "\tspeed: 0.0291s/iter; left time: 596.6557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.81s\n",
      "Steps: 225 | Train Loss: 0.0165517 Vali Loss: 0.0170777 Test Loss: 0.0211148\n",
      "Validation loss decreased (0.017203 --> 0.017078).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0160374\n",
      "\tspeed: 0.0759s/iter; left time: 1546.4363s\n",
      "\titers: 200, epoch: 10 | loss: 0.0152154\n",
      "\tspeed: 0.0259s/iter; left time: 524.8310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 225 | Train Loss: 0.0164015 Vali Loss: 0.0172799 Test Loss: 0.0212429\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0165146\n",
      "\tspeed: 0.0697s/iter; left time: 1404.6440s\n",
      "\titers: 200, epoch: 11 | loss: 0.0169340\n",
      "\tspeed: 0.0366s/iter; left time: 733.3366s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.95s\n",
      "Steps: 225 | Train Loss: 0.0162799 Vali Loss: 0.0170245 Test Loss: 0.0211387\n",
      "Validation loss decreased (0.017078 --> 0.017025).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0174235\n",
      "\tspeed: 0.0649s/iter; left time: 1292.3864s\n",
      "\titers: 200, epoch: 12 | loss: 0.0168405\n",
      "\tspeed: 0.0163s/iter; left time: 322.4914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.55s\n",
      "Steps: 225 | Train Loss: 0.0161901 Vali Loss: 0.0170253 Test Loss: 0.0209918\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0157391\n",
      "\tspeed: 0.0381s/iter; left time: 750.4034s\n",
      "\titers: 200, epoch: 13 | loss: 0.0155118\n",
      "\tspeed: 0.0171s/iter; left time: 335.4348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.31s\n",
      "Steps: 225 | Train Loss: 0.0160766 Vali Loss: 0.0171959 Test Loss: 0.0210651\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0160115\n",
      "\tspeed: 0.0866s/iter; left time: 1687.1148s\n",
      "\titers: 200, epoch: 14 | loss: 0.0140521\n",
      "\tspeed: 0.0268s/iter; left time: 518.8363s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.66s\n",
      "Steps: 225 | Train Loss: 0.0159977 Vali Loss: 0.0171280 Test Loss: 0.0211238\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0158797\n",
      "\tspeed: 0.0711s/iter; left time: 1369.6018s\n",
      "\titers: 200, epoch: 15 | loss: 0.0149746\n",
      "\tspeed: 0.0553s/iter; left time: 1059.9531s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.05s\n",
      "Steps: 225 | Train Loss: 0.0159293 Vali Loss: 0.0171289 Test Loss: 0.0210804\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0157262\n",
      "\tspeed: 0.0535s/iter; left time: 1018.3351s\n",
      "\titers: 200, epoch: 16 | loss: 0.0152471\n",
      "\tspeed: 0.0257s/iter; left time: 486.7917s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 225 | Train Loss: 0.0158684 Vali Loss: 0.0171377 Test Loss: 0.0211543\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0158248\n",
      "\tspeed: 0.0782s/iter; left time: 1469.9721s\n",
      "\titers: 200, epoch: 17 | loss: 0.0167456\n",
      "\tspeed: 0.0431s/iter; left time: 805.9284s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.15s\n",
      "Steps: 225 | Train Loss: 0.0157851 Vali Loss: 0.0171424 Test Loss: 0.0211536\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0142377\n",
      "\tspeed: 0.0663s/iter; left time: 1231.6919s\n",
      "\titers: 200, epoch: 18 | loss: 0.0154455\n",
      "\tspeed: 0.0357s/iter; left time: 659.3945s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 225 | Train Loss: 0.0157332 Vali Loss: 0.0171524 Test Loss: 0.0210933\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0156443\n",
      "\tspeed: 0.0726s/iter; left time: 1332.6308s\n",
      "\titers: 200, epoch: 19 | loss: 0.0168023\n",
      "\tspeed: 0.0464s/iter; left time: 846.2397s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.91s\n",
      "Steps: 225 | Train Loss: 0.0156895 Vali Loss: 0.0171591 Test Loss: 0.0211719\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0153335\n",
      "\tspeed: 0.0819s/iter; left time: 1483.7848s\n",
      "\titers: 200, epoch: 20 | loss: 0.0163019\n",
      "\tspeed: 0.0254s/iter; left time: 458.3233s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 225 | Train Loss: 0.0156480 Vali Loss: 0.0171658 Test Loss: 0.0211469\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0144234\n",
      "\tspeed: 0.0640s/iter; left time: 1145.1888s\n",
      "\titers: 200, epoch: 21 | loss: 0.0156189\n",
      "\tspeed: 0.0545s/iter; left time: 970.2857s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.12s\n",
      "Steps: 225 | Train Loss: 0.0156178 Vali Loss: 0.0171672 Test Loss: 0.0212086\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.021138742566108704, rmse:0.14539168775081635, mae:0.0951080247759819, rse:0.42714768648147583\n",
      "Intermediate time for ES and pred_len 168: 00h:03m:52.53s\n",
      "Intermediate time for ES: 00h:19m:22.06s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0214014\n",
      "\tspeed: 0.0465s/iter; left time: 1045.5679s\n",
      "\titers: 200, epoch: 1 | loss: 0.0145572\n",
      "\tspeed: 0.0183s/iter; left time: 410.2439s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.07s\n",
      "Steps: 226 | Train Loss: 0.0225206 Vali Loss: 0.0184396 Test Loss: 0.0229489\n",
      "Validation loss decreased (inf --> 0.018440).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0084885\n",
      "\tspeed: 0.0521s/iter; left time: 1161.2013s\n",
      "\titers: 200, epoch: 2 | loss: 0.0069146\n",
      "\tspeed: 0.0274s/iter; left time: 607.0213s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0085904 Vali Loss: 0.0098390 Test Loss: 0.0112448\n",
      "Validation loss decreased (0.018440 --> 0.009839).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0067327\n",
      "\tspeed: 0.0826s/iter; left time: 1820.3676s\n",
      "\titers: 200, epoch: 3 | loss: 0.0059661\n",
      "\tspeed: 0.0283s/iter; left time: 620.1351s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 226 | Train Loss: 0.0070147 Vali Loss: 0.0093701 Test Loss: 0.0107432\n",
      "Validation loss decreased (0.009839 --> 0.009370).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0073707\n",
      "\tspeed: 0.0612s/iter; left time: 1336.4966s\n",
      "\titers: 200, epoch: 4 | loss: 0.0069606\n",
      "\tspeed: 0.0306s/iter; left time: 663.6817s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.98s\n",
      "Steps: 226 | Train Loss: 0.0066252 Vali Loss: 0.0089922 Test Loss: 0.0105075\n",
      "Validation loss decreased (0.009370 --> 0.008992).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059994\n",
      "\tspeed: 0.0626s/iter; left time: 1352.6284s\n",
      "\titers: 200, epoch: 5 | loss: 0.0052356\n",
      "\tspeed: 0.0334s/iter; left time: 718.8642s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 226 | Train Loss: 0.0063918 Vali Loss: 0.0088446 Test Loss: 0.0103435\n",
      "Validation loss decreased (0.008992 --> 0.008845).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0059743\n",
      "\tspeed: 0.0718s/iter; left time: 1534.6728s\n",
      "\titers: 200, epoch: 6 | loss: 0.0054268\n",
      "\tspeed: 0.0243s/iter; left time: 516.0893s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.41s\n",
      "Steps: 226 | Train Loss: 0.0062265 Vali Loss: 0.0086468 Test Loss: 0.0102027\n",
      "Validation loss decreased (0.008845 --> 0.008647).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064023\n",
      "\tspeed: 0.0604s/iter; left time: 1277.7380s\n",
      "\titers: 200, epoch: 7 | loss: 0.0064050\n",
      "\tspeed: 0.0283s/iter; left time: 595.7747s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.02s\n",
      "Steps: 226 | Train Loss: 0.0061115 Vali Loss: 0.0086640 Test Loss: 0.0102161\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0059183\n",
      "\tspeed: 0.0922s/iter; left time: 1928.2814s\n",
      "\titers: 200, epoch: 8 | loss: 0.0059890\n",
      "\tspeed: 0.0218s/iter; left time: 453.4298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.35s\n",
      "Steps: 226 | Train Loss: 0.0060151 Vali Loss: 0.0086641 Test Loss: 0.0103460\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0052437\n",
      "\tspeed: 0.0549s/iter; left time: 1135.2678s\n",
      "\titers: 200, epoch: 9 | loss: 0.0057425\n",
      "\tspeed: 0.0302s/iter; left time: 621.5328s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 226 | Train Loss: 0.0059436 Vali Loss: 0.0084395 Test Loss: 0.0101464\n",
      "Validation loss decreased (0.008647 --> 0.008440).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0057585\n",
      "\tspeed: 0.0763s/iter; left time: 1562.2544s\n",
      "\titers: 200, epoch: 10 | loss: 0.0061389\n",
      "\tspeed: 0.0331s/iter; left time: 674.5683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.64s\n",
      "Steps: 226 | Train Loss: 0.0058756 Vali Loss: 0.0084618 Test Loss: 0.0100039\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0055120\n",
      "\tspeed: 0.0616s/iter; left time: 1247.4301s\n",
      "\titers: 200, epoch: 11 | loss: 0.0054122\n",
      "\tspeed: 0.0354s/iter; left time: 713.9236s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.03s\n",
      "Steps: 226 | Train Loss: 0.0058268 Vali Loss: 0.0084441 Test Loss: 0.0100350\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0066425\n",
      "\tspeed: 0.0534s/iter; left time: 1068.2266s\n",
      "\titers: 200, epoch: 12 | loss: 0.0056953\n",
      "\tspeed: 0.0302s/iter; left time: 601.3445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0057755 Vali Loss: 0.0083846 Test Loss: 0.0100768\n",
      "Validation loss decreased (0.008440 --> 0.008385).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0049884\n",
      "\tspeed: 0.0771s/iter; left time: 1526.0992s\n",
      "\titers: 200, epoch: 13 | loss: 0.0064530\n",
      "\tspeed: 0.0241s/iter; left time: 474.2926s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.66s\n",
      "Steps: 226 | Train Loss: 0.0057345 Vali Loss: 0.0083272 Test Loss: 0.0099970\n",
      "Validation loss decreased (0.008385 --> 0.008327).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0061710\n",
      "\tspeed: 0.0577s/iter; left time: 1128.0379s\n",
      "\titers: 200, epoch: 14 | loss: 0.0062265\n",
      "\tspeed: 0.0245s/iter; left time: 476.2506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0056927 Vali Loss: 0.0083816 Test Loss: 0.0100558\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0053974\n",
      "\tspeed: 0.1023s/iter; left time: 1977.2106s\n",
      "\titers: 200, epoch: 15 | loss: 0.0045167\n",
      "\tspeed: 0.0249s/iter; left time: 478.5829s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 226 | Train Loss: 0.0056677 Vali Loss: 0.0083361 Test Loss: 0.0100202\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0060043\n",
      "\tspeed: 0.0570s/iter; left time: 1089.8463s\n",
      "\titers: 200, epoch: 16 | loss: 0.0055771\n",
      "\tspeed: 0.0272s/iter; left time: 516.3875s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.25s\n",
      "Steps: 226 | Train Loss: 0.0056243 Vali Loss: 0.0083715 Test Loss: 0.0099776\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0055168\n",
      "\tspeed: 0.0740s/iter; left time: 1397.7394s\n",
      "\titers: 200, epoch: 17 | loss: 0.0054286\n",
      "\tspeed: 0.0322s/iter; left time: 604.7882s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.32s\n",
      "Steps: 226 | Train Loss: 0.0056142 Vali Loss: 0.0083595 Test Loss: 0.0100348\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0055352\n",
      "\tspeed: 0.0597s/iter; left time: 1114.3097s\n",
      "\titers: 200, epoch: 18 | loss: 0.0051048\n",
      "\tspeed: 0.0298s/iter; left time: 552.6135s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.65s\n",
      "Steps: 226 | Train Loss: 0.0055779 Vali Loss: 0.0083256 Test Loss: 0.0100114\n",
      "Validation loss decreased (0.008327 --> 0.008326).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0057776\n",
      "\tspeed: 0.0624s/iter; left time: 1151.0329s\n",
      "\titers: 200, epoch: 19 | loss: 0.0059579\n",
      "\tspeed: 0.0235s/iter; left time: 431.1075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.75s\n",
      "Steps: 226 | Train Loss: 0.0055696 Vali Loss: 0.0082678 Test Loss: 0.0099987\n",
      "Validation loss decreased (0.008326 --> 0.008268).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0063669\n",
      "\tspeed: 0.0811s/iter; left time: 1476.2399s\n",
      "\titers: 200, epoch: 20 | loss: 0.0077393\n",
      "\tspeed: 0.0331s/iter; left time: 599.5057s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.51s\n",
      "Steps: 226 | Train Loss: 0.0055387 Vali Loss: 0.0082505 Test Loss: 0.0099721\n",
      "Validation loss decreased (0.008268 --> 0.008250).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0056185\n",
      "\tspeed: 0.0568s/iter; left time: 1021.5923s\n",
      "\titers: 200, epoch: 21 | loss: 0.0054805\n",
      "\tspeed: 0.0220s/iter; left time: 392.9766s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 226 | Train Loss: 0.0055185 Vali Loss: 0.0083284 Test Loss: 0.0100105\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0052375\n",
      "\tspeed: 0.0749s/iter; left time: 1329.5299s\n",
      "\titers: 200, epoch: 22 | loss: 0.0058114\n",
      "\tspeed: 0.0356s/iter; left time: 628.6617s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.65s\n",
      "Steps: 226 | Train Loss: 0.0055146 Vali Loss: 0.0082659 Test Loss: 0.0099952\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0055529\n",
      "\tspeed: 0.0495s/iter; left time: 868.3212s\n",
      "\titers: 200, epoch: 23 | loss: 0.0053774\n",
      "\tspeed: 0.0188s/iter; left time: 328.2097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0055039 Vali Loss: 0.0082733 Test Loss: 0.0099869\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0058973\n",
      "\tspeed: 0.0514s/iter; left time: 889.0240s\n",
      "\titers: 200, epoch: 24 | loss: 0.0055044\n",
      "\tspeed: 0.0266s/iter; left time: 458.3838s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 226 | Train Loss: 0.0054934 Vali Loss: 0.0082724 Test Loss: 0.0099753\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0055101\n",
      "\tspeed: 0.0631s/iter; left time: 1078.0938s\n",
      "\titers: 200, epoch: 25 | loss: 0.0062767\n",
      "\tspeed: 0.0336s/iter; left time: 571.0552s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:07.07s\n",
      "Steps: 226 | Train Loss: 0.0054823 Vali Loss: 0.0082438 Test Loss: 0.0099522\n",
      "Validation loss decreased (0.008250 --> 0.008244).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0058566\n",
      "\tspeed: 0.0744s/iter; left time: 1254.3010s\n",
      "\titers: 200, epoch: 26 | loss: 0.0056711\n",
      "\tspeed: 0.0246s/iter; left time: 412.1057s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:06.08s\n",
      "Steps: 226 | Train Loss: 0.0054782 Vali Loss: 0.0082551 Test Loss: 0.0099774\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0063704\n",
      "\tspeed: 0.0658s/iter; left time: 1094.6314s\n",
      "\titers: 200, epoch: 27 | loss: 0.0056807\n",
      "\tspeed: 0.0357s/iter; left time: 590.3216s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:07.35s\n",
      "Steps: 226 | Train Loss: 0.0054562 Vali Loss: 0.0082273 Test Loss: 0.0099577\n",
      "Validation loss decreased (0.008244 --> 0.008227).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0051593\n",
      "\tspeed: 0.0674s/iter; left time: 1104.8162s\n",
      "\titers: 200, epoch: 28 | loss: 0.0049602\n",
      "\tspeed: 0.0292s/iter; left time: 475.7376s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 226 | Train Loss: 0.0054514 Vali Loss: 0.0082629 Test Loss: 0.0099501\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0059183\n",
      "\tspeed: 0.0806s/iter; left time: 1302.7991s\n",
      "\titers: 200, epoch: 29 | loss: 0.0055221\n",
      "\tspeed: 0.0282s/iter; left time: 453.7541s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.77s\n",
      "Steps: 226 | Train Loss: 0.0054394 Vali Loss: 0.0082124 Test Loss: 0.0099311\n",
      "Validation loss decreased (0.008227 --> 0.008212).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0048695\n",
      "\tspeed: 0.0679s/iter; left time: 1082.1081s\n",
      "\titers: 200, epoch: 30 | loss: 0.0057108\n",
      "\tspeed: 0.0248s/iter; left time: 392.3202s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 226 | Train Loss: 0.0054387 Vali Loss: 0.0082322 Test Loss: 0.0099606\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0055624\n",
      "\tspeed: 0.0740s/iter; left time: 1163.9512s\n",
      "\titers: 200, epoch: 31 | loss: 0.0064698\n",
      "\tspeed: 0.0255s/iter; left time: 397.9365s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:06.72s\n",
      "Steps: 226 | Train Loss: 0.0054343 Vali Loss: 0.0082022 Test Loss: 0.0099584\n",
      "Validation loss decreased (0.008212 --> 0.008202).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0054912\n",
      "\tspeed: 0.0617s/iter; left time: 955.7692s\n",
      "\titers: 200, epoch: 32 | loss: 0.0051729\n",
      "\tspeed: 0.0339s/iter; left time: 521.8560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:07.45s\n",
      "Steps: 226 | Train Loss: 0.0054356 Vali Loss: 0.0082200 Test Loss: 0.0099480\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0066934\n",
      "\tspeed: 0.0711s/iter; left time: 1084.9728s\n",
      "\titers: 200, epoch: 33 | loss: 0.0061226\n",
      "\tspeed: 0.0178s/iter; left time: 269.5879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.07s\n",
      "Steps: 226 | Train Loss: 0.0054288 Vali Loss: 0.0082018 Test Loss: 0.0099535\n",
      "Validation loss decreased (0.008202 --> 0.008202).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0048274\n",
      "\tspeed: 0.0417s/iter; left time: 627.2623s\n",
      "\titers: 200, epoch: 34 | loss: 0.0058461\n",
      "\tspeed: 0.0167s/iter; left time: 250.1951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:04.24s\n",
      "Steps: 226 | Train Loss: 0.0054293 Vali Loss: 0.0082111 Test Loss: 0.0099458\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0048740\n",
      "\tspeed: 0.0500s/iter; left time: 740.2860s\n",
      "\titers: 200, epoch: 35 | loss: 0.0052270\n",
      "\tspeed: 0.0274s/iter; left time: 402.8505s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 226 | Train Loss: 0.0054257 Vali Loss: 0.0082203 Test Loss: 0.0099352\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0060480\n",
      "\tspeed: 0.0671s/iter; left time: 978.6669s\n",
      "\titers: 200, epoch: 36 | loss: 0.0057200\n",
      "\tspeed: 0.0268s/iter; left time: 388.6075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.56s\n",
      "Steps: 226 | Train Loss: 0.0054139 Vali Loss: 0.0082122 Test Loss: 0.0099516\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0050441\n",
      "\tspeed: 0.0637s/iter; left time: 914.9044s\n",
      "\titers: 200, epoch: 37 | loss: 0.0059495\n",
      "\tspeed: 0.0318s/iter; left time: 453.1156s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:07.19s\n",
      "Steps: 226 | Train Loss: 0.0054095 Vali Loss: 0.0082246 Test Loss: 0.0099409\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0051967\n",
      "\tspeed: 0.0659s/iter; left time: 932.4604s\n",
      "\titers: 200, epoch: 38 | loss: 0.0059897\n",
      "\tspeed: 0.0320s/iter; left time: 448.6926s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 226 | Train Loss: 0.0053976 Vali Loss: 0.0082012 Test Loss: 0.0099508\n",
      "Validation loss decreased (0.008202 --> 0.008201).  Saving model ...\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0052787\n",
      "\tspeed: 0.0695s/iter; left time: 966.4156s\n",
      "\titers: 200, epoch: 39 | loss: 0.0047258\n",
      "\tspeed: 0.0318s/iter; left time: 439.5298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 226 | Train Loss: 0.0054121 Vali Loss: 0.0082163 Test Loss: 0.0099372\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0047600\n",
      "\tspeed: 0.0667s/iter; left time: 913.5721s\n",
      "\titers: 200, epoch: 40 | loss: 0.0060419\n",
      "\tspeed: 0.0279s/iter; left time: 379.6164s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:06.92s\n",
      "Steps: 226 | Train Loss: 0.0054035 Vali Loss: 0.0082144 Test Loss: 0.0099391\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0053946\n",
      "\tspeed: 0.0674s/iter; left time: 907.4149s\n",
      "\titers: 200, epoch: 41 | loss: 0.0048580\n",
      "\tspeed: 0.0308s/iter; left time: 411.4795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:07.40s\n",
      "Steps: 226 | Train Loss: 0.0054028 Vali Loss: 0.0082137 Test Loss: 0.0099513\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0053619\n",
      "\tspeed: 0.0728s/iter; left time: 963.1081s\n",
      "\titers: 200, epoch: 42 | loss: 0.0055491\n",
      "\tspeed: 0.0300s/iter; left time: 394.1669s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:07.10s\n",
      "Steps: 226 | Train Loss: 0.0054027 Vali Loss: 0.0081900 Test Loss: 0.0099411\n",
      "Validation loss decreased (0.008201 --> 0.008190).  Saving model ...\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0055729\n",
      "\tspeed: 0.0746s/iter; left time: 971.0289s\n",
      "\titers: 200, epoch: 43 | loss: 0.0051776\n",
      "\tspeed: 0.0278s/iter; left time: 359.0035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:06.88s\n",
      "Steps: 226 | Train Loss: 0.0054087 Vali Loss: 0.0082251 Test Loss: 0.0099428\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0065977\n",
      "\tspeed: 0.0757s/iter; left time: 967.1938s\n",
      "\titers: 200, epoch: 44 | loss: 0.0057450\n",
      "\tspeed: 0.0321s/iter; left time: 406.8943s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:07.64s\n",
      "Steps: 226 | Train Loss: 0.0053970 Vali Loss: 0.0082129 Test Loss: 0.0099601\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0049890\n",
      "\tspeed: 0.0751s/iter; left time: 942.9276s\n",
      "\titers: 200, epoch: 45 | loss: 0.0063225\n",
      "\tspeed: 0.0308s/iter; left time: 383.7337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:07.52s\n",
      "Steps: 226 | Train Loss: 0.0053993 Vali Loss: 0.0082445 Test Loss: 0.0099413\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0051483\n",
      "\tspeed: 0.0765s/iter; left time: 942.8198s\n",
      "\titers: 200, epoch: 46 | loss: 0.0049865\n",
      "\tspeed: 0.0303s/iter; left time: 370.8887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:07.26s\n",
      "Steps: 226 | Train Loss: 0.0053943 Vali Loss: 0.0082098 Test Loss: 0.0099457\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0052320\n",
      "\tspeed: 0.0796s/iter; left time: 964.0414s\n",
      "\titers: 200, epoch: 47 | loss: 0.0059659\n",
      "\tspeed: 0.0293s/iter; left time: 351.7331s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 226 | Train Loss: 0.0053895 Vali Loss: 0.0081924 Test Loss: 0.0099481\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0055492\n",
      "\tspeed: 0.0707s/iter; left time: 839.5113s\n",
      "\titers: 200, epoch: 48 | loss: 0.0043556\n",
      "\tspeed: 0.0293s/iter; left time: 345.1927s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:07.18s\n",
      "Steps: 226 | Train Loss: 0.0053922 Vali Loss: 0.0082154 Test Loss: 0.0099448\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0051179\n",
      "\tspeed: 0.0767s/iter; left time: 893.7971s\n",
      "\titers: 200, epoch: 49 | loss: 0.0056693\n",
      "\tspeed: 0.0318s/iter; left time: 366.8317s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 226 | Train Loss: 0.0053863 Vali Loss: 0.0081961 Test Loss: 0.0099444\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0054214\n",
      "\tspeed: 0.0731s/iter; left time: 835.1078s\n",
      "\titers: 200, epoch: 50 | loss: 0.0059862\n",
      "\tspeed: 0.0274s/iter; left time: 309.9842s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:06.86s\n",
      "Steps: 226 | Train Loss: 0.0053821 Vali Loss: 0.0082106 Test Loss: 0.0099440\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0053560\n",
      "\tspeed: 0.0720s/iter; left time: 806.7189s\n",
      "\titers: 200, epoch: 51 | loss: 0.0060457\n",
      "\tspeed: 0.0293s/iter; left time: 325.0421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:07.42s\n",
      "Steps: 226 | Train Loss: 0.0053953 Vali Loss: 0.0082016 Test Loss: 0.0099440\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0059672\n",
      "\tspeed: 0.0650s/iter; left time: 712.8809s\n",
      "\titers: 200, epoch: 52 | loss: 0.0054211\n",
      "\tspeed: 0.0311s/iter; left time: 338.3667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:07.28s\n",
      "Steps: 226 | Train Loss: 0.0053981 Vali Loss: 0.0081929 Test Loss: 0.0099474\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009941143915057182, rmse:0.0997052863240242, mae:0.056643787771463394, rse:0.3846602439880371\n",
      "Intermediate time for FR and pred_len 24: 00h:08m:31.16s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0242119\n",
      "\tspeed: 0.0675s/iter; left time: 1512.1852s\n",
      "\titers: 200, epoch: 1 | loss: 0.0189905\n",
      "\tspeed: 0.0330s/iter; left time: 736.1255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.49s\n",
      "Steps: 225 | Train Loss: 0.0249051 Vali Loss: 0.0217590 Test Loss: 0.0277699\n",
      "Validation loss decreased (inf --> 0.021759).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0133864\n",
      "\tspeed: 0.0747s/iter; left time: 1657.5674s\n",
      "\titers: 200, epoch: 2 | loss: 0.0129357\n",
      "\tspeed: 0.0305s/iter; left time: 674.0878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.10s\n",
      "Steps: 225 | Train Loss: 0.0133166 Vali Loss: 0.0155336 Test Loss: 0.0199100\n",
      "Validation loss decreased (0.021759 --> 0.015534).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0114889\n",
      "\tspeed: 0.0740s/iter; left time: 1624.7860s\n",
      "\titers: 200, epoch: 3 | loss: 0.0128945\n",
      "\tspeed: 0.0365s/iter; left time: 797.5956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 225 | Train Loss: 0.0115833 Vali Loss: 0.0146534 Test Loss: 0.0191689\n",
      "Validation loss decreased (0.015534 --> 0.014653).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0096506\n",
      "\tspeed: 0.0867s/iter; left time: 1883.9506s\n",
      "\titers: 200, epoch: 4 | loss: 0.0106959\n",
      "\tspeed: 0.0409s/iter; left time: 884.5815s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.76s\n",
      "Steps: 225 | Train Loss: 0.0110096 Vali Loss: 0.0143571 Test Loss: 0.0189645\n",
      "Validation loss decreased (0.014653 --> 0.014357).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0110854\n",
      "\tspeed: 0.0926s/iter; left time: 1990.0259s\n",
      "\titers: 200, epoch: 5 | loss: 0.0108944\n",
      "\tspeed: 0.0498s/iter; left time: 1066.5335s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 225 | Train Loss: 0.0107397 Vali Loss: 0.0141109 Test Loss: 0.0188817\n",
      "Validation loss decreased (0.014357 --> 0.014111).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0105365\n",
      "\tspeed: 0.0972s/iter; left time: 2067.5188s\n",
      "\titers: 200, epoch: 6 | loss: 0.0099358\n",
      "\tspeed: 0.0439s/iter; left time: 929.1238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.19s\n",
      "Steps: 225 | Train Loss: 0.0105499 Vali Loss: 0.0140572 Test Loss: 0.0187163\n",
      "Validation loss decreased (0.014111 --> 0.014057).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0089927\n",
      "\tspeed: 0.0978s/iter; left time: 2057.8290s\n",
      "\titers: 200, epoch: 7 | loss: 0.0099737\n",
      "\tspeed: 0.0451s/iter; left time: 945.7169s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.91s\n",
      "Steps: 225 | Train Loss: 0.0103798 Vali Loss: 0.0140918 Test Loss: 0.0186790\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0118735\n",
      "\tspeed: 0.0945s/iter; left time: 1969.0095s\n",
      "\titers: 200, epoch: 8 | loss: 0.0107587\n",
      "\tspeed: 0.0449s/iter; left time: 930.4285s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 225 | Train Loss: 0.0102556 Vali Loss: 0.0141602 Test Loss: 0.0188255\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0094852\n",
      "\tspeed: 0.1207s/iter; left time: 2487.4936s\n",
      "\titers: 200, epoch: 9 | loss: 0.0104117\n",
      "\tspeed: 0.0472s/iter; left time: 967.2519s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.35s\n",
      "Steps: 225 | Train Loss: 0.0101461 Vali Loss: 0.0141134 Test Loss: 0.0188376\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0100241\n",
      "\tspeed: 0.0948s/iter; left time: 1931.4085s\n",
      "\titers: 200, epoch: 10 | loss: 0.0114260\n",
      "\tspeed: 0.0498s/iter; left time: 1010.1916s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 225 | Train Loss: 0.0100464 Vali Loss: 0.0139718 Test Loss: 0.0186181\n",
      "Validation loss decreased (0.014057 --> 0.013972).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0092512\n",
      "\tspeed: 0.0860s/iter; left time: 1732.0959s\n",
      "\titers: 200, epoch: 11 | loss: 0.0105780\n",
      "\tspeed: 0.0528s/iter; left time: 1057.8352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 225 | Train Loss: 0.0099659 Vali Loss: 0.0140622 Test Loss: 0.0185865\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0095099\n",
      "\tspeed: 0.0871s/iter; left time: 1734.6307s\n",
      "\titers: 200, epoch: 12 | loss: 0.0113929\n",
      "\tspeed: 0.0470s/iter; left time: 932.1768s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.36s\n",
      "Steps: 225 | Train Loss: 0.0098971 Vali Loss: 0.0139690 Test Loss: 0.0185923\n",
      "Validation loss decreased (0.013972 --> 0.013969).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0089053\n",
      "\tspeed: 0.1034s/iter; left time: 2036.4028s\n",
      "\titers: 200, epoch: 13 | loss: 0.0110970\n",
      "\tspeed: 0.0519s/iter; left time: 1017.3723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 225 | Train Loss: 0.0098009 Vali Loss: 0.0139987 Test Loss: 0.0186758\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0104156\n",
      "\tspeed: 0.0910s/iter; left time: 1772.4151s\n",
      "\titers: 200, epoch: 14 | loss: 0.0107352\n",
      "\tspeed: 0.0482s/iter; left time: 933.1722s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:09.87s\n",
      "Steps: 225 | Train Loss: 0.0097639 Vali Loss: 0.0139314 Test Loss: 0.0185757\n",
      "Validation loss decreased (0.013969 --> 0.013931).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0100680\n",
      "\tspeed: 0.0957s/iter; left time: 1842.1949s\n",
      "\titers: 200, epoch: 15 | loss: 0.0093769\n",
      "\tspeed: 0.0449s/iter; left time: 860.6688s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.49s\n",
      "Steps: 225 | Train Loss: 0.0097022 Vali Loss: 0.0140342 Test Loss: 0.0185551\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0094552\n",
      "\tspeed: 0.1083s/iter; left time: 2060.6460s\n",
      "\titers: 200, epoch: 16 | loss: 0.0097943\n",
      "\tspeed: 0.0413s/iter; left time: 780.7715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.21s\n",
      "Steps: 225 | Train Loss: 0.0096613 Vali Loss: 0.0140575 Test Loss: 0.0186617\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0094170\n",
      "\tspeed: 0.0985s/iter; left time: 1851.9953s\n",
      "\titers: 200, epoch: 17 | loss: 0.0103829\n",
      "\tspeed: 0.0526s/iter; left time: 984.5678s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.97s\n",
      "Steps: 225 | Train Loss: 0.0096146 Vali Loss: 0.0140356 Test Loss: 0.0187498\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0102264\n",
      "\tspeed: 0.0952s/iter; left time: 1767.6326s\n",
      "\titers: 200, epoch: 18 | loss: 0.0095341\n",
      "\tspeed: 0.0508s/iter; left time: 939.2880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 225 | Train Loss: 0.0095707 Vali Loss: 0.0140555 Test Loss: 0.0185867\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0091605\n",
      "\tspeed: 0.0927s/iter; left time: 1700.7312s\n",
      "\titers: 200, epoch: 19 | loss: 0.0093767\n",
      "\tspeed: 0.0475s/iter; left time: 867.7873s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 225 | Train Loss: 0.0095344 Vali Loss: 0.0140573 Test Loss: 0.0187457\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0095285\n",
      "\tspeed: 0.0843s/iter; left time: 1528.6786s\n",
      "\titers: 200, epoch: 20 | loss: 0.0106554\n",
      "\tspeed: 0.0426s/iter; left time: 767.4184s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:09.57s\n",
      "Steps: 225 | Train Loss: 0.0095132 Vali Loss: 0.0140521 Test Loss: 0.0187558\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0086229\n",
      "\tspeed: 0.0999s/iter; left time: 1787.7186s\n",
      "\titers: 200, epoch: 21 | loss: 0.0081618\n",
      "\tspeed: 0.0500s/iter; left time: 890.9304s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 225 | Train Loss: 0.0094732 Vali Loss: 0.0140823 Test Loss: 0.0186404\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0094810\n",
      "\tspeed: 0.1088s/iter; left time: 1923.6633s\n",
      "\titers: 200, epoch: 22 | loss: 0.0104590\n",
      "\tspeed: 0.0426s/iter; left time: 748.5200s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.10s\n",
      "Steps: 225 | Train Loss: 0.0094494 Vali Loss: 0.0140780 Test Loss: 0.0187164\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0111367\n",
      "\tspeed: 0.0701s/iter; left time: 1222.5957s\n",
      "\titers: 200, epoch: 23 | loss: 0.0086470\n",
      "\tspeed: 0.0457s/iter; left time: 792.4132s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.88s\n",
      "Steps: 225 | Train Loss: 0.0094175 Vali Loss: 0.0141186 Test Loss: 0.0186971\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0093648\n",
      "\tspeed: 0.0528s/iter; left time: 909.8051s\n",
      "\titers: 200, epoch: 24 | loss: 0.0095880\n",
      "\tspeed: 0.0164s/iter; left time: 281.6998s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:04.18s\n",
      "Steps: 225 | Train Loss: 0.0093960 Vali Loss: 0.0141392 Test Loss: 0.0187757\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018575744703412056, rmse:0.13629285991191864, mae:0.08204299211502075, rse:0.5272170305252075\n",
      "Intermediate time for FR and pred_len 96: 00h:05m:32.63s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0265635\n",
      "\tspeed: 0.0575s/iter; left time: 1288.6036s\n",
      "\titers: 200, epoch: 1 | loss: 0.0194377\n",
      "\tspeed: 0.0637s/iter; left time: 1421.3725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:12.25s\n",
      "Steps: 225 | Train Loss: 0.0264126 Vali Loss: 0.0232208 Test Loss: 0.0288062\n",
      "Validation loss decreased (inf --> 0.023221).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0166239\n",
      "\tspeed: 0.1232s/iter; left time: 2731.1538s\n",
      "\titers: 200, epoch: 2 | loss: 0.0140248\n",
      "\tspeed: 0.0609s/iter; left time: 1344.3090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:12.62s\n",
      "Steps: 225 | Train Loss: 0.0148250 Vali Loss: 0.0169483 Test Loss: 0.0215553\n",
      "Validation loss decreased (0.023221 --> 0.016948).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0114372\n",
      "\tspeed: 0.0983s/iter; left time: 2157.7028s\n",
      "\titers: 200, epoch: 3 | loss: 0.0119995\n",
      "\tspeed: 0.0625s/iter; left time: 1366.1036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.62s\n",
      "Steps: 225 | Train Loss: 0.0129030 Vali Loss: 0.0159594 Test Loss: 0.0209190\n",
      "Validation loss decreased (0.016948 --> 0.015959).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0142003\n",
      "\tspeed: 0.0995s/iter; left time: 2161.5971s\n",
      "\titers: 200, epoch: 4 | loss: 0.0132558\n",
      "\tspeed: 0.0528s/iter; left time: 1141.5349s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.91s\n",
      "Steps: 225 | Train Loss: 0.0122763 Vali Loss: 0.0157880 Test Loss: 0.0208853\n",
      "Validation loss decreased (0.015959 --> 0.015788).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0120211\n",
      "\tspeed: 0.0914s/iter; left time: 1964.7984s\n",
      "\titers: 200, epoch: 5 | loss: 0.0105536\n",
      "\tspeed: 0.0267s/iter; left time: 570.9060s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.96s\n",
      "Steps: 225 | Train Loss: 0.0119905 Vali Loss: 0.0156816 Test Loss: 0.0204916\n",
      "Validation loss decreased (0.015788 --> 0.015682).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0112292\n",
      "\tspeed: 0.0656s/iter; left time: 1395.6634s\n",
      "\titers: 200, epoch: 6 | loss: 0.0134623\n",
      "\tspeed: 0.0334s/iter; left time: 706.2847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.35s\n",
      "Steps: 225 | Train Loss: 0.0117887 Vali Loss: 0.0155945 Test Loss: 0.0205650\n",
      "Validation loss decreased (0.015682 --> 0.015595).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0112592\n",
      "\tspeed: 0.1344s/iter; left time: 2828.9337s\n",
      "\titers: 200, epoch: 7 | loss: 0.0118189\n",
      "\tspeed: 0.0585s/iter; left time: 1224.9963s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 225 | Train Loss: 0.0116513 Vali Loss: 0.0155949 Test Loss: 0.0205162\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0098861\n",
      "\tspeed: 0.1293s/iter; left time: 2692.5948s\n",
      "\titers: 200, epoch: 8 | loss: 0.0110440\n",
      "\tspeed: 0.0488s/iter; left time: 1012.2551s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.72s\n",
      "Steps: 225 | Train Loss: 0.0115205 Vali Loss: 0.0155843 Test Loss: 0.0205340\n",
      "Validation loss decreased (0.015595 --> 0.015584).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0120436\n",
      "\tspeed: 0.1436s/iter; left time: 2958.4088s\n",
      "\titers: 200, epoch: 9 | loss: 0.0117982\n",
      "\tspeed: 0.0414s/iter; left time: 848.9145s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 225 | Train Loss: 0.0114065 Vali Loss: 0.0155008 Test Loss: 0.0203243\n",
      "Validation loss decreased (0.015584 --> 0.015501).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0118167\n",
      "\tspeed: 0.1751s/iter; left time: 3568.7616s\n",
      "\titers: 200, epoch: 10 | loss: 0.0113397\n",
      "\tspeed: 0.0390s/iter; left time: 789.7745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 225 | Train Loss: 0.0113024 Vali Loss: 0.0155792 Test Loss: 0.0203798\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0119269\n",
      "\tspeed: 0.1243s/iter; left time: 2505.1871s\n",
      "\titers: 200, epoch: 11 | loss: 0.0098879\n",
      "\tspeed: 0.0551s/iter; left time: 1104.6570s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.76s\n",
      "Steps: 225 | Train Loss: 0.0112100 Vali Loss: 0.0155863 Test Loss: 0.0206468\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0109906\n",
      "\tspeed: 0.1021s/iter; left time: 2034.0708s\n",
      "\titers: 200, epoch: 12 | loss: 0.0112945\n",
      "\tspeed: 0.0530s/iter; left time: 1050.7848s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 225 | Train Loss: 0.0111068 Vali Loss: 0.0156394 Test Loss: 0.0207387\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0111686\n",
      "\tspeed: 0.1442s/iter; left time: 2841.0370s\n",
      "\titers: 200, epoch: 13 | loss: 0.0104042\n",
      "\tspeed: 0.0527s/iter; left time: 1033.8337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.92s\n",
      "Steps: 225 | Train Loss: 0.0110203 Vali Loss: 0.0156567 Test Loss: 0.0205409\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0118282\n",
      "\tspeed: 0.1576s/iter; left time: 3070.3153s\n",
      "\titers: 200, epoch: 14 | loss: 0.0114619\n",
      "\tspeed: 0.0391s/iter; left time: 757.8374s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.10s\n",
      "Steps: 225 | Train Loss: 0.0109452 Vali Loss: 0.0157138 Test Loss: 0.0208073\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0103363\n",
      "\tspeed: 0.1424s/iter; left time: 2741.6020s\n",
      "\titers: 200, epoch: 15 | loss: 0.0099776\n",
      "\tspeed: 0.0421s/iter; left time: 805.9097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 225 | Train Loss: 0.0108966 Vali Loss: 0.0157213 Test Loss: 0.0208918\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0109692\n",
      "\tspeed: 0.1228s/iter; left time: 2336.9919s\n",
      "\titers: 200, epoch: 16 | loss: 0.0112045\n",
      "\tspeed: 0.0531s/iter; left time: 1004.0711s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.34s\n",
      "Steps: 225 | Train Loss: 0.0108314 Vali Loss: 0.0157334 Test Loss: 0.0208079\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0108547\n",
      "\tspeed: 0.1590s/iter; left time: 2989.3833s\n",
      "\titers: 200, epoch: 17 | loss: 0.0111214\n",
      "\tspeed: 0.0454s/iter; left time: 849.6576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.91s\n",
      "Steps: 225 | Train Loss: 0.0107619 Vali Loss: 0.0157618 Test Loss: 0.0209842\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0103792\n",
      "\tspeed: 0.1284s/iter; left time: 2385.9108s\n",
      "\titers: 200, epoch: 18 | loss: 0.0104057\n",
      "\tspeed: 0.0278s/iter; left time: 513.1756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.71s\n",
      "Steps: 225 | Train Loss: 0.0107229 Vali Loss: 0.0157928 Test Loss: 0.0209592\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0105714\n",
      "\tspeed: 0.1042s/iter; left time: 1911.7321s\n",
      "\titers: 200, epoch: 19 | loss: 0.0113793\n",
      "\tspeed: 0.0387s/iter; left time: 706.2586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.63s\n",
      "Steps: 225 | Train Loss: 0.0106709 Vali Loss: 0.0158113 Test Loss: 0.0211378\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.020324300974607468, rmse:0.1425633281469345, mae:0.08861789107322693, rse:0.5521612763404846\n",
      "Intermediate time for FR and pred_len 168: 00h:05m:31.76s\n",
      "Intermediate time for FR: 00h:19m:35.55s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0427644\n",
      "\tspeed: 0.0485s/iter; left time: 1090.3603s\n",
      "\titers: 200, epoch: 1 | loss: 0.0291665\n",
      "\tspeed: 0.0382s/iter; left time: 855.1709s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.16s\n",
      "Steps: 226 | Train Loss: 0.0426089 Vali Loss: 0.0234300 Test Loss: 0.0253591\n",
      "Validation loss decreased (inf --> 0.023430).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0135666\n",
      "\tspeed: 0.0784s/iter; left time: 1746.0179s\n",
      "\titers: 200, epoch: 2 | loss: 0.0129317\n",
      "\tspeed: 0.0320s/iter; left time: 709.6154s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.61s\n",
      "Steps: 226 | Train Loss: 0.0153345 Vali Loss: 0.0111285 Test Loss: 0.0120855\n",
      "Validation loss decreased (0.023430 --> 0.011128).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0125586\n",
      "\tspeed: 0.0749s/iter; left time: 1650.5465s\n",
      "\titers: 200, epoch: 3 | loss: 0.0112898\n",
      "\tspeed: 0.0305s/iter; left time: 669.8211s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.21s\n",
      "Steps: 226 | Train Loss: 0.0119622 Vali Loss: 0.0103058 Test Loss: 0.0113176\n",
      "Validation loss decreased (0.011128 --> 0.010306).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0107425\n",
      "\tspeed: 0.0881s/iter; left time: 1922.8519s\n",
      "\titers: 200, epoch: 4 | loss: 0.0094230\n",
      "\tspeed: 0.0280s/iter; left time: 607.7616s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 226 | Train Loss: 0.0111439 Vali Loss: 0.0099383 Test Loss: 0.0109307\n",
      "Validation loss decreased (0.010306 --> 0.009938).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0119819\n",
      "\tspeed: 0.0759s/iter; left time: 1640.1053s\n",
      "\titers: 200, epoch: 5 | loss: 0.0102829\n",
      "\tspeed: 0.0475s/iter; left time: 1020.6552s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.57s\n",
      "Steps: 226 | Train Loss: 0.0106644 Vali Loss: 0.0096654 Test Loss: 0.0106351\n",
      "Validation loss decreased (0.009938 --> 0.009665).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0100474\n",
      "\tspeed: 0.0543s/iter; left time: 1160.2392s\n",
      "\titers: 200, epoch: 6 | loss: 0.0094613\n",
      "\tspeed: 0.0169s/iter; left time: 360.1263s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.12s\n",
      "Steps: 226 | Train Loss: 0.0103493 Vali Loss: 0.0094481 Test Loss: 0.0103848\n",
      "Validation loss decreased (0.009665 --> 0.009448).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0118266\n",
      "\tspeed: 0.0504s/iter; left time: 1064.8666s\n",
      "\titers: 200, epoch: 7 | loss: 0.0097277\n",
      "\tspeed: 0.0271s/iter; left time: 571.2305s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 226 | Train Loss: 0.0101450 Vali Loss: 0.0094589 Test Loss: 0.0103997\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0095389\n",
      "\tspeed: 0.0705s/iter; left time: 1475.8192s\n",
      "\titers: 200, epoch: 8 | loss: 0.0106729\n",
      "\tspeed: 0.0472s/iter; left time: 982.7192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:09.42s\n",
      "Steps: 226 | Train Loss: 0.0100044 Vali Loss: 0.0093035 Test Loss: 0.0102250\n",
      "Validation loss decreased (0.009448 --> 0.009303).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0097942\n",
      "\tspeed: 0.0720s/iter; left time: 1490.8777s\n",
      "\titers: 200, epoch: 9 | loss: 0.0097993\n",
      "\tspeed: 0.0329s/iter; left time: 677.3811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 226 | Train Loss: 0.0098369 Vali Loss: 0.0092346 Test Loss: 0.0102146\n",
      "Validation loss decreased (0.009303 --> 0.009235).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0099564\n",
      "\tspeed: 0.0960s/iter; left time: 1964.7149s\n",
      "\titers: 200, epoch: 10 | loss: 0.0102991\n",
      "\tspeed: 0.0246s/iter; left time: 501.4904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.42s\n",
      "Steps: 226 | Train Loss: 0.0097580 Vali Loss: 0.0091586 Test Loss: 0.0100977\n",
      "Validation loss decreased (0.009235 --> 0.009159).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0086434\n",
      "\tspeed: 0.0542s/iter; left time: 1098.0237s\n",
      "\titers: 200, epoch: 11 | loss: 0.0096359\n",
      "\tspeed: 0.0195s/iter; left time: 392.5545s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 226 | Train Loss: 0.0096705 Vali Loss: 0.0092086 Test Loss: 0.0101226\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0111360\n",
      "\tspeed: 0.0692s/iter; left time: 1384.2870s\n",
      "\titers: 200, epoch: 12 | loss: 0.0106724\n",
      "\tspeed: 0.0313s/iter; left time: 623.2161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 226 | Train Loss: 0.0096070 Vali Loss: 0.0090734 Test Loss: 0.0100747\n",
      "Validation loss decreased (0.009159 --> 0.009073).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0089536\n",
      "\tspeed: 0.0839s/iter; left time: 1661.1133s\n",
      "\titers: 200, epoch: 13 | loss: 0.0075973\n",
      "\tspeed: 0.0343s/iter; left time: 675.8967s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:08.39s\n",
      "Steps: 226 | Train Loss: 0.0095315 Vali Loss: 0.0090904 Test Loss: 0.0100318\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0101849\n",
      "\tspeed: 0.0684s/iter; left time: 1337.6736s\n",
      "\titers: 200, epoch: 14 | loss: 0.0094804\n",
      "\tspeed: 0.0323s/iter; left time: 628.8837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.14s\n",
      "Steps: 226 | Train Loss: 0.0094820 Vali Loss: 0.0090765 Test Loss: 0.0100181\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0085579\n",
      "\tspeed: 0.0862s/iter; left time: 1666.6054s\n",
      "\titers: 200, epoch: 15 | loss: 0.0107631\n",
      "\tspeed: 0.0283s/iter; left time: 543.8782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0094281 Vali Loss: 0.0090492 Test Loss: 0.0100166\n",
      "Validation loss decreased (0.009073 --> 0.009049).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0101539\n",
      "\tspeed: 0.0720s/iter; left time: 1376.7790s\n",
      "\titers: 200, epoch: 16 | loss: 0.0084199\n",
      "\tspeed: 0.0500s/iter; left time: 950.5012s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:09.97s\n",
      "Steps: 226 | Train Loss: 0.0093823 Vali Loss: 0.0091001 Test Loss: 0.0100515\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0090759\n",
      "\tspeed: 0.0736s/iter; left time: 1389.6985s\n",
      "\titers: 200, epoch: 17 | loss: 0.0091737\n",
      "\tspeed: 0.0274s/iter; left time: 514.1774s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.03s\n",
      "Steps: 226 | Train Loss: 0.0093435 Vali Loss: 0.0090378 Test Loss: 0.0099843\n",
      "Validation loss decreased (0.009049 --> 0.009038).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0101232\n",
      "\tspeed: 0.0948s/iter; left time: 1768.0497s\n",
      "\titers: 200, epoch: 18 | loss: 0.0084545\n",
      "\tspeed: 0.0276s/iter; left time: 512.0467s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 226 | Train Loss: 0.0093282 Vali Loss: 0.0090161 Test Loss: 0.0099726\n",
      "Validation loss decreased (0.009038 --> 0.009016).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0101296\n",
      "\tspeed: 0.0672s/iter; left time: 1237.9290s\n",
      "\titers: 200, epoch: 19 | loss: 0.0081247\n",
      "\tspeed: 0.0481s/iter; left time: 881.0214s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:09.28s\n",
      "Steps: 226 | Train Loss: 0.0093000 Vali Loss: 0.0090169 Test Loss: 0.0099598\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0085207\n",
      "\tspeed: 0.0729s/iter; left time: 1327.4617s\n",
      "\titers: 200, epoch: 20 | loss: 0.0106575\n",
      "\tspeed: 0.0293s/iter; left time: 530.2584s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.99s\n",
      "Steps: 226 | Train Loss: 0.0092577 Vali Loss: 0.0090158 Test Loss: 0.0099629\n",
      "Validation loss decreased (0.009016 --> 0.009016).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0096524\n",
      "\tspeed: 0.0881s/iter; left time: 1583.2618s\n",
      "\titers: 200, epoch: 21 | loss: 0.0096965\n",
      "\tspeed: 0.0301s/iter; left time: 539.0862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.19s\n",
      "Steps: 226 | Train Loss: 0.0092286 Vali Loss: 0.0090278 Test Loss: 0.0099740\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0085044\n",
      "\tspeed: 0.0778s/iter; left time: 1381.4710s\n",
      "\titers: 200, epoch: 22 | loss: 0.0097745\n",
      "\tspeed: 0.0318s/iter; left time: 560.9670s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.40s\n",
      "Steps: 226 | Train Loss: 0.0092048 Vali Loss: 0.0090266 Test Loss: 0.0099292\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0081876\n",
      "\tspeed: 0.0867s/iter; left time: 1518.9184s\n",
      "\titers: 200, epoch: 23 | loss: 0.0080436\n",
      "\tspeed: 0.0394s/iter; left time: 686.3334s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.37s\n",
      "Steps: 226 | Train Loss: 0.0091790 Vali Loss: 0.0089924 Test Loss: 0.0099458\n",
      "Validation loss decreased (0.009016 --> 0.008992).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0080196\n",
      "\tspeed: 0.0745s/iter; left time: 1289.3025s\n",
      "\titers: 200, epoch: 24 | loss: 0.0102902\n",
      "\tspeed: 0.0409s/iter; left time: 703.0702s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:09.00s\n",
      "Steps: 226 | Train Loss: 0.0091637 Vali Loss: 0.0089613 Test Loss: 0.0099342\n",
      "Validation loss decreased (0.008992 --> 0.008961).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0088788\n",
      "\tspeed: 0.0683s/iter; left time: 1166.8872s\n",
      "\titers: 200, epoch: 25 | loss: 0.0095923\n",
      "\tspeed: 0.0273s/iter; left time: 463.9541s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0091597 Vali Loss: 0.0090058 Test Loss: 0.0099207\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0080711\n",
      "\tspeed: 0.0949s/iter; left time: 1599.0179s\n",
      "\titers: 200, epoch: 26 | loss: 0.0104032\n",
      "\tspeed: 0.0285s/iter; left time: 476.6298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 226 | Train Loss: 0.0091317 Vali Loss: 0.0089826 Test Loss: 0.0099222\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0088905\n",
      "\tspeed: 0.0767s/iter; left time: 1274.4187s\n",
      "\titers: 200, epoch: 27 | loss: 0.0105031\n",
      "\tspeed: 0.0455s/iter; left time: 751.4591s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:09.64s\n",
      "Steps: 226 | Train Loss: 0.0091199 Vali Loss: 0.0089627 Test Loss: 0.0099322\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0093362\n",
      "\tspeed: 0.0723s/iter; left time: 1186.0515s\n",
      "\titers: 200, epoch: 28 | loss: 0.0086763\n",
      "\tspeed: 0.0279s/iter; left time: 455.4829s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.96s\n",
      "Steps: 226 | Train Loss: 0.0091062 Vali Loss: 0.0089637 Test Loss: 0.0099520\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0090730\n",
      "\tspeed: 0.0927s/iter; left time: 1498.8831s\n",
      "\titers: 200, epoch: 29 | loss: 0.0088845\n",
      "\tspeed: 0.0286s/iter; left time: 458.9106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.94s\n",
      "Steps: 226 | Train Loss: 0.0090892 Vali Loss: 0.0089406 Test Loss: 0.0098978\n",
      "Validation loss decreased (0.008961 --> 0.008941).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0076724\n",
      "\tspeed: 0.0758s/iter; left time: 1208.3972s\n",
      "\titers: 200, epoch: 30 | loss: 0.0101498\n",
      "\tspeed: 0.0227s/iter; left time: 359.1034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0090841 Vali Loss: 0.0089625 Test Loss: 0.0099098\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0089099\n",
      "\tspeed: 0.0460s/iter; left time: 723.2144s\n",
      "\titers: 200, epoch: 31 | loss: 0.0098940\n",
      "\tspeed: 0.0187s/iter; left time: 292.4243s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0090745 Vali Loss: 0.0089565 Test Loss: 0.0099237\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0088118\n",
      "\tspeed: 0.0736s/iter; left time: 1140.6685s\n",
      "\titers: 200, epoch: 32 | loss: 0.0087904\n",
      "\tspeed: 0.0433s/iter; left time: 667.2638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.97s\n",
      "Steps: 226 | Train Loss: 0.0090637 Vali Loss: 0.0089329 Test Loss: 0.0099249\n",
      "Validation loss decreased (0.008941 --> 0.008933).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0092595\n",
      "\tspeed: 0.0701s/iter; left time: 1069.9731s\n",
      "\titers: 200, epoch: 33 | loss: 0.0082681\n",
      "\tspeed: 0.0255s/iter; left time: 386.6213s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.37s\n",
      "Steps: 226 | Train Loss: 0.0090772 Vali Loss: 0.0089731 Test Loss: 0.0099125\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0079741\n",
      "\tspeed: 0.0814s/iter; left time: 1224.5307s\n",
      "\titers: 200, epoch: 34 | loss: 0.0097403\n",
      "\tspeed: 0.0221s/iter; left time: 330.0705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:06.29s\n",
      "Steps: 226 | Train Loss: 0.0090595 Vali Loss: 0.0089322 Test Loss: 0.0098990\n",
      "Validation loss decreased (0.008933 --> 0.008932).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0085803\n",
      "\tspeed: 0.0440s/iter; left time: 651.3799s\n",
      "\titers: 200, epoch: 35 | loss: 0.0098124\n",
      "\tspeed: 0.0167s/iter; left time: 246.1038s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:04.28s\n",
      "Steps: 226 | Train Loss: 0.0090542 Vali Loss: 0.0089748 Test Loss: 0.0099321\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0090371\n",
      "\tspeed: 0.0579s/iter; left time: 844.3759s\n",
      "\titers: 200, epoch: 36 | loss: 0.0094171\n",
      "\tspeed: 0.0291s/iter; left time: 421.8049s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0090484 Vali Loss: 0.0089509 Test Loss: 0.0099013\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0090749\n",
      "\tspeed: 0.0663s/iter; left time: 952.4311s\n",
      "\titers: 200, epoch: 37 | loss: 0.0093885\n",
      "\tspeed: 0.0299s/iter; left time: 426.1332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:07.01s\n",
      "Steps: 226 | Train Loss: 0.0090552 Vali Loss: 0.0089714 Test Loss: 0.0099124\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0082394\n",
      "\tspeed: 0.0646s/iter; left time: 913.2864s\n",
      "\titers: 200, epoch: 38 | loss: 0.0093596\n",
      "\tspeed: 0.0284s/iter; left time: 398.0774s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 226 | Train Loss: 0.0090332 Vali Loss: 0.0089565 Test Loss: 0.0099143\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0078252\n",
      "\tspeed: 0.0658s/iter; left time: 915.5177s\n",
      "\titers: 200, epoch: 39 | loss: 0.0088208\n",
      "\tspeed: 0.0284s/iter; left time: 391.9578s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 226 | Train Loss: 0.0090471 Vali Loss: 0.0089371 Test Loss: 0.0099035\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0105462\n",
      "\tspeed: 0.0692s/iter; left time: 946.7984s\n",
      "\titers: 200, epoch: 40 | loss: 0.0090573\n",
      "\tspeed: 0.0267s/iter; left time: 362.6619s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0090234 Vali Loss: 0.0089457 Test Loss: 0.0099033\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0088055\n",
      "\tspeed: 0.0689s/iter; left time: 927.3560s\n",
      "\titers: 200, epoch: 41 | loss: 0.0104520\n",
      "\tspeed: 0.0307s/iter; left time: 409.8555s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0090427 Vali Loss: 0.0089316 Test Loss: 0.0098993\n",
      "Validation loss decreased (0.008932 --> 0.008932).  Saving model ...\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0084742\n",
      "\tspeed: 0.0626s/iter; left time: 828.2486s\n",
      "\titers: 200, epoch: 42 | loss: 0.0093432\n",
      "\tspeed: 0.0293s/iter; left time: 384.9107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.92s\n",
      "Steps: 226 | Train Loss: 0.0090335 Vali Loss: 0.0089267 Test Loss: 0.0098911\n",
      "Validation loss decreased (0.008932 --> 0.008927).  Saving model ...\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0097170\n",
      "\tspeed: 0.0640s/iter; left time: 832.3867s\n",
      "\titers: 200, epoch: 43 | loss: 0.0076540\n",
      "\tspeed: 0.0258s/iter; left time: 333.0699s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:06.24s\n",
      "Steps: 226 | Train Loss: 0.0090321 Vali Loss: 0.0089353 Test Loss: 0.0099073\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0092617\n",
      "\tspeed: 0.0661s/iter; left time: 844.9406s\n",
      "\titers: 200, epoch: 44 | loss: 0.0087024\n",
      "\tspeed: 0.0270s/iter; left time: 342.2599s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 226 | Train Loss: 0.0090337 Vali Loss: 0.0089408 Test Loss: 0.0098926\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0088285\n",
      "\tspeed: 0.0683s/iter; left time: 857.2198s\n",
      "\titers: 200, epoch: 45 | loss: 0.0092159\n",
      "\tspeed: 0.0281s/iter; left time: 350.5602s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:06.61s\n",
      "Steps: 226 | Train Loss: 0.0090323 Vali Loss: 0.0089285 Test Loss: 0.0098980\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0102098\n",
      "\tspeed: 0.0723s/iter; left time: 892.0211s\n",
      "\titers: 200, epoch: 46 | loss: 0.0079350\n",
      "\tspeed: 0.0298s/iter; left time: 364.2793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:07.33s\n",
      "Steps: 226 | Train Loss: 0.0090227 Vali Loss: 0.0089318 Test Loss: 0.0099059\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0093856\n",
      "\tspeed: 0.0618s/iter; left time: 747.8551s\n",
      "\titers: 200, epoch: 47 | loss: 0.0095248\n",
      "\tspeed: 0.0301s/iter; left time: 361.3959s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0090159 Vali Loss: 0.0089450 Test Loss: 0.0099015\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0090503\n",
      "\tspeed: 0.0618s/iter; left time: 733.6224s\n",
      "\titers: 200, epoch: 48 | loss: 0.0091753\n",
      "\tspeed: 0.0255s/iter; left time: 300.4921s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 226 | Train Loss: 0.0090108 Vali Loss: 0.0089341 Test Loss: 0.0098960\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0083525\n",
      "\tspeed: 0.0668s/iter; left time: 778.0072s\n",
      "\titers: 200, epoch: 49 | loss: 0.0098274\n",
      "\tspeed: 0.0252s/iter; left time: 290.9835s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:06.21s\n",
      "Steps: 226 | Train Loss: 0.0090235 Vali Loss: 0.0089498 Test Loss: 0.0099027\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0091457\n",
      "\tspeed: 0.0693s/iter; left time: 792.4603s\n",
      "\titers: 200, epoch: 50 | loss: 0.0105588\n",
      "\tspeed: 0.0300s/iter; left time: 339.4670s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 226 | Train Loss: 0.0090247 Vali Loss: 0.0089380 Test Loss: 0.0098937\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0099731\n",
      "\tspeed: 0.0644s/iter; left time: 721.2085s\n",
      "\titers: 200, epoch: 51 | loss: 0.0085722\n",
      "\tspeed: 0.0298s/iter; left time: 331.1560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 226 | Train Loss: 0.0090074 Vali Loss: 0.0089378 Test Loss: 0.0098993\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0085397\n",
      "\tspeed: 0.0627s/iter; left time: 688.5853s\n",
      "\titers: 200, epoch: 52 | loss: 0.0099546\n",
      "\tspeed: 0.0294s/iter; left time: 319.2593s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:06.70s\n",
      "Steps: 226 | Train Loss: 0.0090018 Vali Loss: 0.0089331 Test Loss: 0.0099019\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009891098365187645, rmse:0.09945400059223175, mae:0.05819772183895111, rse:0.3757877051830292\n",
      "Intermediate time for IT and pred_len 24: 00h:08m:58.20s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0462537\n",
      "\tspeed: 0.0689s/iter; left time: 1543.6157s\n",
      "\titers: 200, epoch: 1 | loss: 0.0364715\n",
      "\tspeed: 0.0280s/iter; left time: 625.0756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.98s\n",
      "Steps: 225 | Train Loss: 0.0460628 Vali Loss: 0.0273578 Test Loss: 0.0298426\n",
      "Validation loss decreased (inf --> 0.027358).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0235899\n",
      "\tspeed: 0.0676s/iter; left time: 1498.9230s\n",
      "\titers: 200, epoch: 2 | loss: 0.0210041\n",
      "\tspeed: 0.0362s/iter; left time: 798.1010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.66s\n",
      "Steps: 225 | Train Loss: 0.0232539 Vali Loss: 0.0176938 Test Loss: 0.0193522\n",
      "Validation loss decreased (0.027358 --> 0.017694).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0189749\n",
      "\tspeed: 0.0678s/iter; left time: 1488.1416s\n",
      "\titers: 200, epoch: 3 | loss: 0.0182328\n",
      "\tspeed: 0.0338s/iter; left time: 738.0705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.32s\n",
      "Steps: 225 | Train Loss: 0.0193855 Vali Loss: 0.0168231 Test Loss: 0.0184362\n",
      "Validation loss decreased (0.017694 --> 0.016823).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0177972\n",
      "\tspeed: 0.0729s/iter; left time: 1583.3243s\n",
      "\titers: 200, epoch: 4 | loss: 0.0183742\n",
      "\tspeed: 0.0332s/iter; left time: 718.3900s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.72s\n",
      "Steps: 225 | Train Loss: 0.0185190 Vali Loss: 0.0164864 Test Loss: 0.0181530\n",
      "Validation loss decreased (0.016823 --> 0.016486).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0168590\n",
      "\tspeed: 0.0798s/iter; left time: 1714.9373s\n",
      "\titers: 200, epoch: 5 | loss: 0.0164473\n",
      "\tspeed: 0.0296s/iter; left time: 633.0589s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 225 | Train Loss: 0.0180803 Vali Loss: 0.0164282 Test Loss: 0.0180659\n",
      "Validation loss decreased (0.016486 --> 0.016428).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0169709\n",
      "\tspeed: 0.0739s/iter; left time: 1572.3145s\n",
      "\titers: 200, epoch: 6 | loss: 0.0191333\n",
      "\tspeed: 0.0322s/iter; left time: 682.7761s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.44s\n",
      "Steps: 225 | Train Loss: 0.0177726 Vali Loss: 0.0161812 Test Loss: 0.0177180\n",
      "Validation loss decreased (0.016428 --> 0.016181).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0158284\n",
      "\tspeed: 0.0733s/iter; left time: 1542.2157s\n",
      "\titers: 200, epoch: 7 | loss: 0.0175322\n",
      "\tspeed: 0.0319s/iter; left time: 668.7348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 225 | Train Loss: 0.0175136 Vali Loss: 0.0161265 Test Loss: 0.0176915\n",
      "Validation loss decreased (0.016181 --> 0.016126).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0174552\n",
      "\tspeed: 0.0818s/iter; left time: 1702.6980s\n",
      "\titers: 200, epoch: 8 | loss: 0.0166485\n",
      "\tspeed: 0.0306s/iter; left time: 635.0506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.18s\n",
      "Steps: 225 | Train Loss: 0.0173040 Vali Loss: 0.0160004 Test Loss: 0.0174142\n",
      "Validation loss decreased (0.016126 --> 0.016000).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0163455\n",
      "\tspeed: 0.0799s/iter; left time: 1646.4733s\n",
      "\titers: 200, epoch: 9 | loss: 0.0154967\n",
      "\tspeed: 0.0335s/iter; left time: 686.6061s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 225 | Train Loss: 0.0171474 Vali Loss: 0.0159577 Test Loss: 0.0173985\n",
      "Validation loss decreased (0.016000 --> 0.015958).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0160120\n",
      "\tspeed: 0.0664s/iter; left time: 1353.1241s\n",
      "\titers: 200, epoch: 10 | loss: 0.0178833\n",
      "\tspeed: 0.0409s/iter; left time: 829.5337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.39s\n",
      "Steps: 225 | Train Loss: 0.0170059 Vali Loss: 0.0159840 Test Loss: 0.0174097\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0159951\n",
      "\tspeed: 0.0929s/iter; left time: 1871.6360s\n",
      "\titers: 200, epoch: 11 | loss: 0.0157917\n",
      "\tspeed: 0.0429s/iter; left time: 860.3901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:09.43s\n",
      "Steps: 225 | Train Loss: 0.0169099 Vali Loss: 0.0159336 Test Loss: 0.0172732\n",
      "Validation loss decreased (0.015958 --> 0.015934).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0165329\n",
      "\tspeed: 0.0664s/iter; left time: 1323.1145s\n",
      "\titers: 200, epoch: 12 | loss: 0.0170412\n",
      "\tspeed: 0.0182s/iter; left time: 360.5674s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 225 | Train Loss: 0.0167639 Vali Loss: 0.0159042 Test Loss: 0.0173147\n",
      "Validation loss decreased (0.015934 --> 0.015904).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0180271\n",
      "\tspeed: 0.0814s/iter; left time: 1603.0954s\n",
      "\titers: 200, epoch: 13 | loss: 0.0178022\n",
      "\tspeed: 0.0400s/iter; left time: 783.6891s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:08.96s\n",
      "Steps: 225 | Train Loss: 0.0166703 Vali Loss: 0.0158887 Test Loss: 0.0171973\n",
      "Validation loss decreased (0.015904 --> 0.015889).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0176169\n",
      "\tspeed: 0.1266s/iter; left time: 2465.5381s\n",
      "\titers: 200, epoch: 14 | loss: 0.0166390\n",
      "\tspeed: 0.0494s/iter; left time: 956.5359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 225 | Train Loss: 0.0165915 Vali Loss: 0.0158640 Test Loss: 0.0172642\n",
      "Validation loss decreased (0.015889 --> 0.015864).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0162922\n",
      "\tspeed: 0.1073s/iter; left time: 2065.5748s\n",
      "\titers: 200, epoch: 15 | loss: 0.0161882\n",
      "\tspeed: 0.0380s/iter; left time: 727.0989s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.32s\n",
      "Steps: 225 | Train Loss: 0.0165111 Vali Loss: 0.0159664 Test Loss: 0.0174457\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0164338\n",
      "\tspeed: 0.0877s/iter; left time: 1668.2082s\n",
      "\titers: 200, epoch: 16 | loss: 0.0168174\n",
      "\tspeed: 0.0504s/iter; left time: 954.1934s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.16s\n",
      "Steps: 225 | Train Loss: 0.0164226 Vali Loss: 0.0158663 Test Loss: 0.0172530\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0158943\n",
      "\tspeed: 0.1024s/iter; left time: 1924.4480s\n",
      "\titers: 200, epoch: 17 | loss: 0.0164825\n",
      "\tspeed: 0.0454s/iter; left time: 849.4878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.34s\n",
      "Steps: 225 | Train Loss: 0.0163565 Vali Loss: 0.0159070 Test Loss: 0.0174244\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0174788\n",
      "\tspeed: 0.0993s/iter; left time: 1844.1894s\n",
      "\titers: 200, epoch: 18 | loss: 0.0145421\n",
      "\tspeed: 0.0266s/iter; left time: 490.7336s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.04s\n",
      "Steps: 225 | Train Loss: 0.0162801 Vali Loss: 0.0158426 Test Loss: 0.0172457\n",
      "Validation loss decreased (0.015864 --> 0.015843).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0168303\n",
      "\tspeed: 0.0720s/iter; left time: 1320.7256s\n",
      "\titers: 200, epoch: 19 | loss: 0.0163630\n",
      "\tspeed: 0.0431s/iter; left time: 785.8042s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.40s\n",
      "Steps: 225 | Train Loss: 0.0162624 Vali Loss: 0.0157878 Test Loss: 0.0171725\n",
      "Validation loss decreased (0.015843 --> 0.015788).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0152932\n",
      "\tspeed: 0.1118s/iter; left time: 2026.0311s\n",
      "\titers: 200, epoch: 20 | loss: 0.0158245\n",
      "\tspeed: 0.0545s/iter; left time: 983.3168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:12.25s\n",
      "Steps: 225 | Train Loss: 0.0161951 Vali Loss: 0.0158540 Test Loss: 0.0172939\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0160206\n",
      "\tspeed: 0.0957s/iter; left time: 1713.6502s\n",
      "\titers: 200, epoch: 21 | loss: 0.0143377\n",
      "\tspeed: 0.0489s/iter; left time: 870.4033s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.50s\n",
      "Steps: 225 | Train Loss: 0.0161581 Vali Loss: 0.0158551 Test Loss: 0.0173828\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0173990\n",
      "\tspeed: 0.1255s/iter; left time: 2218.2540s\n",
      "\titers: 200, epoch: 22 | loss: 0.0164247\n",
      "\tspeed: 0.0570s/iter; left time: 1002.0298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:12.20s\n",
      "Steps: 225 | Train Loss: 0.0161385 Vali Loss: 0.0158426 Test Loss: 0.0173311\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0165023\n",
      "\tspeed: 0.1310s/iter; left time: 2286.8177s\n",
      "\titers: 200, epoch: 23 | loss: 0.0158310\n",
      "\tspeed: 0.0618s/iter; left time: 1071.5133s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:13.71s\n",
      "Steps: 225 | Train Loss: 0.0160991 Vali Loss: 0.0158318 Test Loss: 0.0172664\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0162533\n",
      "\tspeed: 0.1002s/iter; left time: 1726.0865s\n",
      "\titers: 200, epoch: 24 | loss: 0.0154683\n",
      "\tspeed: 0.0392s/iter; left time: 671.9594s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:09.55s\n",
      "Steps: 225 | Train Loss: 0.0160415 Vali Loss: 0.0158316 Test Loss: 0.0173423\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0155253\n",
      "\tspeed: 0.1592s/iter; left time: 2705.8815s\n",
      "\titers: 200, epoch: 25 | loss: 0.0173770\n",
      "\tspeed: 0.0411s/iter; left time: 694.2856s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:09.41s\n",
      "Steps: 225 | Train Loss: 0.0160281 Vali Loss: 0.0158332 Test Loss: 0.0173701\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0182681\n",
      "\tspeed: 0.1172s/iter; left time: 1966.5665s\n",
      "\titers: 200, epoch: 26 | loss: 0.0169437\n",
      "\tspeed: 0.0589s/iter; left time: 981.8063s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:13.76s\n",
      "Steps: 225 | Train Loss: 0.0159760 Vali Loss: 0.0158007 Test Loss: 0.0172860\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0168032\n",
      "\tspeed: 0.1253s/iter; left time: 2073.1897s\n",
      "\titers: 200, epoch: 27 | loss: 0.0156003\n",
      "\tspeed: 0.0590s/iter; left time: 970.2114s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:12.08s\n",
      "Steps: 225 | Train Loss: 0.0159695 Vali Loss: 0.0158024 Test Loss: 0.0172899\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0167254\n",
      "\tspeed: 0.1086s/iter; left time: 1772.7258s\n",
      "\titers: 200, epoch: 28 | loss: 0.0156412\n",
      "\tspeed: 0.0438s/iter; left time: 711.4205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:10.18s\n",
      "Steps: 225 | Train Loss: 0.0159519 Vali Loss: 0.0158223 Test Loss: 0.0174044\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0168503\n",
      "\tspeed: 0.1492s/iter; left time: 2402.5702s\n",
      "\titers: 200, epoch: 29 | loss: 0.0137340\n",
      "\tspeed: 0.0462s/iter; left time: 739.4509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:10.80s\n",
      "Steps: 225 | Train Loss: 0.0159480 Vali Loss: 0.0157722 Test Loss: 0.0172937\n",
      "Validation loss decreased (0.015788 --> 0.015772).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0163661\n",
      "\tspeed: 0.1377s/iter; left time: 2186.5077s\n",
      "\titers: 200, epoch: 30 | loss: 0.0171112\n",
      "\tspeed: 0.0534s/iter; left time: 843.0461s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:12.46s\n",
      "Steps: 225 | Train Loss: 0.0159193 Vali Loss: 0.0157847 Test Loss: 0.0173282\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0153719\n",
      "\tspeed: 0.1280s/iter; left time: 2004.0865s\n",
      "\titers: 200, epoch: 31 | loss: 0.0155335\n",
      "\tspeed: 0.0505s/iter; left time: 785.3466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:11.18s\n",
      "Steps: 225 | Train Loss: 0.0159121 Vali Loss: 0.0157999 Test Loss: 0.0173632\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0156373\n",
      "\tspeed: 0.1244s/iter; left time: 1918.2645s\n",
      "\titers: 200, epoch: 32 | loss: 0.0162509\n",
      "\tspeed: 0.0240s/iter; left time: 368.1725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 225 | Train Loss: 0.0158688 Vali Loss: 0.0157687 Test Loss: 0.0172948\n",
      "Validation loss decreased (0.015772 --> 0.015769).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0183278\n",
      "\tspeed: 0.0458s/iter; left time: 696.4836s\n",
      "\titers: 200, epoch: 33 | loss: 0.0169869\n",
      "\tspeed: 0.0160s/iter; left time: 242.0783s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.00s\n",
      "Steps: 225 | Train Loss: 0.0158634 Vali Loss: 0.0157925 Test Loss: 0.0173219\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0172865\n",
      "\tspeed: 0.0626s/iter; left time: 937.2979s\n",
      "\titers: 200, epoch: 34 | loss: 0.0153092\n",
      "\tspeed: 0.0428s/iter; left time: 636.7960s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:09.66s\n",
      "Steps: 225 | Train Loss: 0.0158540 Vali Loss: 0.0158062 Test Loss: 0.0173842\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0161870\n",
      "\tspeed: 0.1479s/iter; left time: 2182.1716s\n",
      "\titers: 200, epoch: 35 | loss: 0.0151124\n",
      "\tspeed: 0.0352s/iter; left time: 515.5297s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:09.73s\n",
      "Steps: 225 | Train Loss: 0.0158580 Vali Loss: 0.0157895 Test Loss: 0.0173378\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0166946\n",
      "\tspeed: 0.0992s/iter; left time: 1440.3194s\n",
      "\titers: 200, epoch: 36 | loss: 0.0162752\n",
      "\tspeed: 0.0550s/iter; left time: 793.6798s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:12.02s\n",
      "Steps: 225 | Train Loss: 0.0158467 Vali Loss: 0.0157970 Test Loss: 0.0173953\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0172352\n",
      "\tspeed: 0.1000s/iter; left time: 1429.5845s\n",
      "\titers: 200, epoch: 37 | loss: 0.0143222\n",
      "\tspeed: 0.0514s/iter; left time: 730.6271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:10.01s\n",
      "Steps: 225 | Train Loss: 0.0158265 Vali Loss: 0.0157891 Test Loss: 0.0173757\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0172777\n",
      "\tspeed: 0.1148s/iter; left time: 1615.3501s\n",
      "\titers: 200, epoch: 38 | loss: 0.0145041\n",
      "\tspeed: 0.0608s/iter; left time: 850.3474s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:13.65s\n",
      "Steps: 225 | Train Loss: 0.0158066 Vali Loss: 0.0157820 Test Loss: 0.0173516\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0138649\n",
      "\tspeed: 0.1076s/iter; left time: 1489.7403s\n",
      "\titers: 200, epoch: 39 | loss: 0.0156466\n",
      "\tspeed: 0.0451s/iter; left time: 619.9543s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:09.55s\n",
      "Steps: 225 | Train Loss: 0.0158153 Vali Loss: 0.0158029 Test Loss: 0.0173771\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0144005\n",
      "\tspeed: 0.1295s/iter; left time: 1765.1740s\n",
      "\titers: 200, epoch: 40 | loss: 0.0168604\n",
      "\tspeed: 0.0376s/iter; left time: 508.2581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:08.72s\n",
      "Steps: 225 | Train Loss: 0.0158220 Vali Loss: 0.0157745 Test Loss: 0.0173589\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0143205\n",
      "\tspeed: 0.1196s/iter; left time: 1602.9526s\n",
      "\titers: 200, epoch: 41 | loss: 0.0147199\n",
      "\tspeed: 0.0420s/iter; left time: 558.0596s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 225 | Train Loss: 0.0158101 Vali Loss: 0.0157864 Test Loss: 0.0173573\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0171100\n",
      "\tspeed: 0.1009s/iter; left time: 1328.8184s\n",
      "\titers: 200, epoch: 42 | loss: 0.0152776\n",
      "\tspeed: 0.0591s/iter; left time: 773.2677s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:12.83s\n",
      "Steps: 225 | Train Loss: 0.0158103 Vali Loss: 0.0158030 Test Loss: 0.0174064\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01729476824402809, rmse:0.13150957226753235, mae:0.07975035905838013, rse:0.49725186824798584\n",
      "Intermediate time for IT and pred_len 96: 00h:10m:15.34s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0470817\n",
      "\tspeed: 0.0776s/iter; left time: 1739.1330s\n",
      "\titers: 200, epoch: 1 | loss: 0.0334309\n",
      "\tspeed: 0.0264s/iter; left time: 589.4221s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.77s\n",
      "Steps: 225 | Train Loss: 0.0477718 Vali Loss: 0.0284479 Test Loss: 0.0308728\n",
      "Validation loss decreased (inf --> 0.028448).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0239116\n",
      "\tspeed: 0.0542s/iter; left time: 1201.2453s\n",
      "\titers: 200, epoch: 2 | loss: 0.0233685\n",
      "\tspeed: 0.0288s/iter; left time: 636.7362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.85s\n",
      "Steps: 225 | Train Loss: 0.0249409 Vali Loss: 0.0192272 Test Loss: 0.0207638\n",
      "Validation loss decreased (0.028448 --> 0.019227).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0201211\n",
      "\tspeed: 0.0849s/iter; left time: 1863.3549s\n",
      "\titers: 200, epoch: 3 | loss: 0.0224873\n",
      "\tspeed: 0.0449s/iter; left time: 981.2784s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.92s\n",
      "Steps: 225 | Train Loss: 0.0209690 Vali Loss: 0.0184162 Test Loss: 0.0194297\n",
      "Validation loss decreased (0.019227 --> 0.018416).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0236589\n",
      "\tspeed: 0.0881s/iter; left time: 1914.0939s\n",
      "\titers: 200, epoch: 4 | loss: 0.0205874\n",
      "\tspeed: 0.0464s/iter; left time: 1003.2668s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.03s\n",
      "Steps: 225 | Train Loss: 0.0201329 Vali Loss: 0.0180921 Test Loss: 0.0192037\n",
      "Validation loss decreased (0.018416 --> 0.018092).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0182775\n",
      "\tspeed: 0.0935s/iter; left time: 2010.1367s\n",
      "\titers: 200, epoch: 5 | loss: 0.0187295\n",
      "\tspeed: 0.0447s/iter; left time: 957.0799s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.19s\n",
      "Steps: 225 | Train Loss: 0.0197307 Vali Loss: 0.0178989 Test Loss: 0.0190696\n",
      "Validation loss decreased (0.018092 --> 0.017899).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0189552\n",
      "\tspeed: 0.0739s/iter; left time: 1571.8617s\n",
      "\titers: 200, epoch: 6 | loss: 0.0199359\n",
      "\tspeed: 0.0454s/iter; left time: 960.3956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.44s\n",
      "Steps: 225 | Train Loss: 0.0194634 Vali Loss: 0.0177294 Test Loss: 0.0189508\n",
      "Validation loss decreased (0.017899 --> 0.017729).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0199815\n",
      "\tspeed: 0.1056s/iter; left time: 2222.1892s\n",
      "\titers: 200, epoch: 7 | loss: 0.0189001\n",
      "\tspeed: 0.0324s/iter; left time: 679.7654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0192553 Vali Loss: 0.0177561 Test Loss: 0.0189389\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0165607\n",
      "\tspeed: 0.1290s/iter; left time: 2685.6895s\n",
      "\titers: 200, epoch: 8 | loss: 0.0194142\n",
      "\tspeed: 0.0294s/iter; left time: 608.4771s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.23s\n",
      "Steps: 225 | Train Loss: 0.0190906 Vali Loss: 0.0176011 Test Loss: 0.0187670\n",
      "Validation loss decreased (0.017729 --> 0.017601).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0192651\n",
      "\tspeed: 0.0936s/iter; left time: 1928.7353s\n",
      "\titers: 200, epoch: 9 | loss: 0.0187292\n",
      "\tspeed: 0.0274s/iter; left time: 560.8369s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.40s\n",
      "Steps: 225 | Train Loss: 0.0189444 Vali Loss: 0.0175180 Test Loss: 0.0188351\n",
      "Validation loss decreased (0.017601 --> 0.017518).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0185771\n",
      "\tspeed: 0.0422s/iter; left time: 859.4170s\n",
      "\titers: 200, epoch: 10 | loss: 0.0195553\n",
      "\tspeed: 0.0207s/iter; left time: 418.9505s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 225 | Train Loss: 0.0188059 Vali Loss: 0.0176197 Test Loss: 0.0188907\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0188781\n",
      "\tspeed: 0.0523s/iter; left time: 1054.8610s\n",
      "\titers: 200, epoch: 11 | loss: 0.0185935\n",
      "\tspeed: 0.0335s/iter; left time: 672.2974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 225 | Train Loss: 0.0187199 Vali Loss: 0.0175749 Test Loss: 0.0188651\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0184689\n",
      "\tspeed: 0.0752s/iter; left time: 1497.4903s\n",
      "\titers: 200, epoch: 12 | loss: 0.0183612\n",
      "\tspeed: 0.0443s/iter; left time: 878.7240s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:09.83s\n",
      "Steps: 225 | Train Loss: 0.0186157 Vali Loss: 0.0175330 Test Loss: 0.0188398\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0175366\n",
      "\tspeed: 0.0779s/iter; left time: 1533.9832s\n",
      "\titers: 200, epoch: 13 | loss: 0.0188624\n",
      "\tspeed: 0.0327s/iter; left time: 640.1132s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.95s\n",
      "Steps: 225 | Train Loss: 0.0185028 Vali Loss: 0.0174663 Test Loss: 0.0187327\n",
      "Validation loss decreased (0.017518 --> 0.017466).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0175015\n",
      "\tspeed: 0.1002s/iter; left time: 1951.3282s\n",
      "\titers: 200, epoch: 14 | loss: 0.0188210\n",
      "\tspeed: 0.0325s/iter; left time: 629.0999s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.01s\n",
      "Steps: 225 | Train Loss: 0.0184339 Vali Loss: 0.0175618 Test Loss: 0.0188765\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0180571\n",
      "\tspeed: 0.0714s/iter; left time: 1375.3754s\n",
      "\titers: 200, epoch: 15 | loss: 0.0174339\n",
      "\tspeed: 0.0338s/iter; left time: 648.0443s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:08.55s\n",
      "Steps: 225 | Train Loss: 0.0183563 Vali Loss: 0.0175691 Test Loss: 0.0188944\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0190036\n",
      "\tspeed: 0.1002s/iter; left time: 1905.9641s\n",
      "\titers: 200, epoch: 16 | loss: 0.0194198\n",
      "\tspeed: 0.0333s/iter; left time: 630.2952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:08.22s\n",
      "Steps: 225 | Train Loss: 0.0182919 Vali Loss: 0.0174588 Test Loss: 0.0187264\n",
      "Validation loss decreased (0.017466 --> 0.017459).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0162348\n",
      "\tspeed: 0.0945s/iter; left time: 1777.4391s\n",
      "\titers: 200, epoch: 17 | loss: 0.0178654\n",
      "\tspeed: 0.0395s/iter; left time: 738.7342s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.35s\n",
      "Steps: 225 | Train Loss: 0.0182279 Vali Loss: 0.0174852 Test Loss: 0.0188695\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0169097\n",
      "\tspeed: 0.0727s/iter; left time: 1349.8465s\n",
      "\titers: 200, epoch: 18 | loss: 0.0179845\n",
      "\tspeed: 0.0362s/iter; left time: 669.0736s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:08.89s\n",
      "Steps: 225 | Train Loss: 0.0181624 Vali Loss: 0.0174539 Test Loss: 0.0188264\n",
      "Validation loss decreased (0.017459 --> 0.017454).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0169253\n",
      "\tspeed: 0.0989s/iter; left time: 1815.3184s\n",
      "\titers: 200, epoch: 19 | loss: 0.0200264\n",
      "\tspeed: 0.0326s/iter; left time: 594.0972s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.26s\n",
      "Steps: 225 | Train Loss: 0.0181152 Vali Loss: 0.0174486 Test Loss: 0.0187742\n",
      "Validation loss decreased (0.017454 --> 0.017449).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0178235\n",
      "\tspeed: 0.0966s/iter; left time: 1751.5849s\n",
      "\titers: 200, epoch: 20 | loss: 0.0182496\n",
      "\tspeed: 0.0385s/iter; left time: 694.1309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.74s\n",
      "Steps: 225 | Train Loss: 0.0180658 Vali Loss: 0.0174563 Test Loss: 0.0187698\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0182403\n",
      "\tspeed: 0.0792s/iter; left time: 1416.8968s\n",
      "\titers: 200, epoch: 21 | loss: 0.0182276\n",
      "\tspeed: 0.0449s/iter; left time: 799.1169s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:09.93s\n",
      "Steps: 225 | Train Loss: 0.0180051 Vali Loss: 0.0174450 Test Loss: 0.0188208\n",
      "Validation loss decreased (0.017449 --> 0.017445).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0177208\n",
      "\tspeed: 0.0821s/iter; left time: 1451.4964s\n",
      "\titers: 200, epoch: 22 | loss: 0.0181698\n",
      "\tspeed: 0.0391s/iter; left time: 686.5119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.93s\n",
      "Steps: 225 | Train Loss: 0.0179754 Vali Loss: 0.0174402 Test Loss: 0.0188235\n",
      "Validation loss decreased (0.017445 --> 0.017440).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0174111\n",
      "\tspeed: 0.1194s/iter; left time: 2083.9236s\n",
      "\titers: 200, epoch: 23 | loss: 0.0177269\n",
      "\tspeed: 0.0354s/iter; left time: 614.0233s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:07.74s\n",
      "Steps: 225 | Train Loss: 0.0179590 Vali Loss: 0.0174663 Test Loss: 0.0188353\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0175101\n",
      "\tspeed: 0.0957s/iter; left time: 1647.6924s\n",
      "\titers: 200, epoch: 24 | loss: 0.0178409\n",
      "\tspeed: 0.0388s/iter; left time: 665.2474s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:10.20s\n",
      "Steps: 225 | Train Loss: 0.0179136 Vali Loss: 0.0174419 Test Loss: 0.0188890\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0191682\n",
      "\tspeed: 0.0868s/iter; left time: 1475.4563s\n",
      "\titers: 200, epoch: 25 | loss: 0.0188505\n",
      "\tspeed: 0.0479s/iter; left time: 808.8288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.00s\n",
      "Steps: 225 | Train Loss: 0.0178740 Vali Loss: 0.0174738 Test Loss: 0.0188258\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0170615\n",
      "\tspeed: 0.0955s/iter; left time: 1602.4787s\n",
      "\titers: 200, epoch: 26 | loss: 0.0181179\n",
      "\tspeed: 0.0327s/iter; left time: 544.6765s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:08.64s\n",
      "Steps: 225 | Train Loss: 0.0178429 Vali Loss: 0.0174697 Test Loss: 0.0188988\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0179335\n",
      "\tspeed: 0.1142s/iter; left time: 1890.3786s\n",
      "\titers: 200, epoch: 27 | loss: 0.0182124\n",
      "\tspeed: 0.0369s/iter; left time: 607.0715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.58s\n",
      "Steps: 225 | Train Loss: 0.0178122 Vali Loss: 0.0174661 Test Loss: 0.0188872\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0199118\n",
      "\tspeed: 0.0933s/iter; left time: 1523.8650s\n",
      "\titers: 200, epoch: 28 | loss: 0.0184817\n",
      "\tspeed: 0.0355s/iter; left time: 576.1225s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:09.28s\n",
      "Steps: 225 | Train Loss: 0.0177697 Vali Loss: 0.0174359 Test Loss: 0.0188458\n",
      "Validation loss decreased (0.017440 --> 0.017436).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0185850\n",
      "\tspeed: 0.0471s/iter; left time: 758.2170s\n",
      "\titers: 200, epoch: 29 | loss: 0.0159490\n",
      "\tspeed: 0.0194s/iter; left time: 309.7327s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0177890 Vali Loss: 0.0174572 Test Loss: 0.0188938\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0190641\n",
      "\tspeed: 0.0504s/iter; left time: 800.5204s\n",
      "\titers: 200, epoch: 30 | loss: 0.0184726\n",
      "\tspeed: 0.0402s/iter; left time: 633.5199s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.74s\n",
      "Steps: 225 | Train Loss: 0.0177593 Vali Loss: 0.0174437 Test Loss: 0.0188625\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0178892\n",
      "\tspeed: 0.0846s/iter; left time: 1324.2394s\n",
      "\titers: 200, epoch: 31 | loss: 0.0159046\n",
      "\tspeed: 0.0338s/iter; left time: 525.3135s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 225 | Train Loss: 0.0177573 Vali Loss: 0.0174455 Test Loss: 0.0188721\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0162854\n",
      "\tspeed: 0.0765s/iter; left time: 1180.2324s\n",
      "\titers: 200, epoch: 32 | loss: 0.0192957\n",
      "\tspeed: 0.0369s/iter; left time: 565.7178s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.38s\n",
      "Steps: 225 | Train Loss: 0.0177434 Vali Loss: 0.0174310 Test Loss: 0.0188878\n",
      "Validation loss decreased (0.017436 --> 0.017431).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0186600\n",
      "\tspeed: 0.0809s/iter; left time: 1230.3704s\n",
      "\titers: 200, epoch: 33 | loss: 0.0190157\n",
      "\tspeed: 0.0298s/iter; left time: 449.8309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:07.73s\n",
      "Steps: 225 | Train Loss: 0.0177179 Vali Loss: 0.0174451 Test Loss: 0.0188824\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0177958\n",
      "\tspeed: 0.0857s/iter; left time: 1284.1060s\n",
      "\titers: 200, epoch: 34 | loss: 0.0167717\n",
      "\tspeed: 0.0323s/iter; left time: 480.9991s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:08.15s\n",
      "Steps: 225 | Train Loss: 0.0177123 Vali Loss: 0.0174233 Test Loss: 0.0188737\n",
      "Validation loss decreased (0.017431 --> 0.017423).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0172185\n",
      "\tspeed: 0.0882s/iter; left time: 1301.2977s\n",
      "\titers: 200, epoch: 35 | loss: 0.0184308\n",
      "\tspeed: 0.0374s/iter; left time: 547.2242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:09.38s\n",
      "Steps: 225 | Train Loss: 0.0176920 Vali Loss: 0.0174605 Test Loss: 0.0188986\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0180091\n",
      "\tspeed: 0.0877s/iter; left time: 1273.2713s\n",
      "\titers: 200, epoch: 36 | loss: 0.0181451\n",
      "\tspeed: 0.0382s/iter; left time: 550.5720s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:08.85s\n",
      "Steps: 225 | Train Loss: 0.0176895 Vali Loss: 0.0174417 Test Loss: 0.0189240\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0197505\n",
      "\tspeed: 0.0734s/iter; left time: 1050.0201s\n",
      "\titers: 200, epoch: 37 | loss: 0.0171026\n",
      "\tspeed: 0.0372s/iter; left time: 528.1712s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:08.34s\n",
      "Steps: 225 | Train Loss: 0.0176750 Vali Loss: 0.0174587 Test Loss: 0.0189393\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0182414\n",
      "\tspeed: 0.0830s/iter; left time: 1168.1594s\n",
      "\titers: 200, epoch: 38 | loss: 0.0174553\n",
      "\tspeed: 0.0418s/iter; left time: 583.7188s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:09.44s\n",
      "Steps: 225 | Train Loss: 0.0176732 Vali Loss: 0.0174202 Test Loss: 0.0189041\n",
      "Validation loss decreased (0.017423 --> 0.017420).  Saving model ...\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0179154\n",
      "\tspeed: 0.0932s/iter; left time: 1291.4824s\n",
      "\titers: 200, epoch: 39 | loss: 0.0177326\n",
      "\tspeed: 0.0364s/iter; left time: 500.9927s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:09.01s\n",
      "Steps: 225 | Train Loss: 0.0176698 Vali Loss: 0.0174568 Test Loss: 0.0188915\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0167302\n",
      "\tspeed: 0.0873s/iter; left time: 1189.7488s\n",
      "\titers: 200, epoch: 40 | loss: 0.0172194\n",
      "\tspeed: 0.0368s/iter; left time: 497.1109s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:08.69s\n",
      "Steps: 225 | Train Loss: 0.0176391 Vali Loss: 0.0174383 Test Loss: 0.0188776\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0164773\n",
      "\tspeed: 0.0867s/iter; left time: 1161.2019s\n",
      "\titers: 200, epoch: 41 | loss: 0.0188107\n",
      "\tspeed: 0.0380s/iter; left time: 506.1030s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:09.04s\n",
      "Steps: 225 | Train Loss: 0.0176388 Vali Loss: 0.0174470 Test Loss: 0.0188990\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0169925\n",
      "\tspeed: 0.0812s/iter; left time: 1069.6559s\n",
      "\titers: 200, epoch: 42 | loss: 0.0179315\n",
      "\tspeed: 0.0341s/iter; left time: 445.3259s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 225 | Train Loss: 0.0176362 Vali Loss: 0.0174358 Test Loss: 0.0189112\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0172228\n",
      "\tspeed: 0.0771s/iter; left time: 998.7934s\n",
      "\titers: 200, epoch: 43 | loss: 0.0194211\n",
      "\tspeed: 0.0424s/iter; left time: 544.3037s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:09.66s\n",
      "Steps: 225 | Train Loss: 0.0176347 Vali Loss: 0.0174557 Test Loss: 0.0189262\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0177851\n",
      "\tspeed: 0.0768s/iter; left time: 977.8385s\n",
      "\titers: 200, epoch: 44 | loss: 0.0175024\n",
      "\tspeed: 0.0395s/iter; left time: 498.2766s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:09.14s\n",
      "Steps: 225 | Train Loss: 0.0176328 Vali Loss: 0.0174426 Test Loss: 0.0189024\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0169976\n",
      "\tspeed: 0.0783s/iter; left time: 979.4436s\n",
      "\titers: 200, epoch: 45 | loss: 0.0195613\n",
      "\tspeed: 0.0383s/iter; left time: 475.1258s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:08.68s\n",
      "Steps: 225 | Train Loss: 0.0176215 Vali Loss: 0.0174558 Test Loss: 0.0189192\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0171266\n",
      "\tspeed: 0.0895s/iter; left time: 1099.2866s\n",
      "\titers: 200, epoch: 46 | loss: 0.0168313\n",
      "\tspeed: 0.0383s/iter; left time: 466.0661s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:08.63s\n",
      "Steps: 225 | Train Loss: 0.0176224 Vali Loss: 0.0174636 Test Loss: 0.0189049\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0155394\n",
      "\tspeed: 0.0776s/iter; left time: 934.7370s\n",
      "\titers: 200, epoch: 47 | loss: 0.0177897\n",
      "\tspeed: 0.0368s/iter; left time: 439.8636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:08.84s\n",
      "Steps: 225 | Train Loss: 0.0176342 Vali Loss: 0.0174581 Test Loss: 0.0189175\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0182304\n",
      "\tspeed: 0.0784s/iter; left time: 926.6719s\n",
      "\titers: 200, epoch: 48 | loss: 0.0158067\n",
      "\tspeed: 0.0322s/iter; left time: 377.2727s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:08.45s\n",
      "Steps: 225 | Train Loss: 0.0176249 Vali Loss: 0.0174510 Test Loss: 0.0189265\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.018904151394963264, rmse:0.137492373585701, mae:0.08580073714256287, rse:0.5203565359115601\n",
      "Intermediate time for IT and pred_len 168: 00h:09m:48.80s\n",
      "Intermediate time for IT: 00h:29m:02.33s\n",
      "Total time: 01h:39m:31.65s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PatchTST/21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>0.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.1912</td>\n",
       "      <td>0.1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.2186</td>\n",
       "      <td>0.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1371</td>\n",
       "      <td>0.0883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.0951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>0.0566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.0798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.0858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            PatchTST/21                \n",
       "Metrics                  MSE    RMSE     MAE\n",
       "Country Pred_len                            \n",
       "DE      24            0.0210  0.1447  0.0914\n",
       "        96            0.0366  0.1912  0.1298\n",
       "        168           0.0406  0.2015  0.1382\n",
       "GB      24            0.0252  0.1588  0.1029\n",
       "        96            0.0446  0.2112  0.1462\n",
       "        168           0.0478  0.2186  0.1531\n",
       "ES      24            0.0098  0.0988  0.0608\n",
       "        96            0.0188  0.1371  0.0883\n",
       "        168           0.0211  0.1454  0.0951\n",
       "FR      24            0.0099  0.0997  0.0566\n",
       "        96            0.0186  0.1363  0.0820\n",
       "        168           0.0203  0.1426  0.0886\n",
       "IT      24            0.0099  0.0995  0.0582\n",
       "        96            0.0173  0.1315  0.0798\n",
       "        168           0.0189  0.1375  0.0858"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "#os.rename(\"results_transformers\", 'patchtst_npy_168')\n",
    "#os.rename(\"test_results\", \"patchtst_pics_168\")\n",
    "\n",
    "path = 'results/patchtst'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False, itr=1)\n",
    "patchtst_df.drop(columns=['Iteration'], inplace=True)\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['PatchTST/21'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_168_21_patch.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PatchTST 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 336\n",
    "model = \"PatchTST\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128 \n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_336_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0331299\n",
      "\tspeed: 0.0707s/iter; left time: 1575.6582s\n",
      "\titers: 200, epoch: 1 | loss: 0.0287433\n",
      "\tspeed: 0.0384s/iter; left time: 851.8963s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.53s\n",
      "Steps: 224 | Train Loss: 0.0354802 Vali Loss: 0.0319702 Test Loss: 0.0360805\n",
      "Validation loss decreased (inf --> 0.031970).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0159255\n",
      "\tspeed: 0.0674s/iter; left time: 1487.0161s\n",
      "\titers: 200, epoch: 2 | loss: 0.0169086\n",
      "\tspeed: 0.0336s/iter; left time: 738.9185s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 224 | Train Loss: 0.0172425 Vali Loss: 0.0207260 Test Loss: 0.0221175\n",
      "Validation loss decreased (0.031970 --> 0.020726).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0144151\n",
      "\tspeed: 0.0790s/iter; left time: 1726.1905s\n",
      "\titers: 200, epoch: 3 | loss: 0.0146772\n",
      "\tspeed: 0.0416s/iter; left time: 903.9022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.79s\n",
      "Steps: 224 | Train Loss: 0.0146521 Vali Loss: 0.0201139 Test Loss: 0.0216509\n",
      "Validation loss decreased (0.020726 --> 0.020114).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0158383\n",
      "\tspeed: 0.0821s/iter; left time: 1775.0880s\n",
      "\titers: 200, epoch: 4 | loss: 0.0160625\n",
      "\tspeed: 0.0384s/iter; left time: 826.4032s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.29s\n",
      "Steps: 224 | Train Loss: 0.0140296 Vali Loss: 0.0195571 Test Loss: 0.0211821\n",
      "Validation loss decreased (0.020114 --> 0.019557).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0142890\n",
      "\tspeed: 0.0822s/iter; left time: 1758.5232s\n",
      "\titers: 200, epoch: 5 | loss: 0.0131326\n",
      "\tspeed: 0.0412s/iter; left time: 877.3811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.32s\n",
      "Steps: 224 | Train Loss: 0.0136609 Vali Loss: 0.0193199 Test Loss: 0.0209985\n",
      "Validation loss decreased (0.019557 --> 0.019320).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0164826\n",
      "\tspeed: 0.0817s/iter; left time: 1730.5515s\n",
      "\titers: 200, epoch: 6 | loss: 0.0126204\n",
      "\tspeed: 0.0443s/iter; left time: 933.5930s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.48s\n",
      "Steps: 224 | Train Loss: 0.0134594 Vali Loss: 0.0192443 Test Loss: 0.0211259\n",
      "Validation loss decreased (0.019320 --> 0.019244).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0128278\n",
      "\tspeed: 0.0834s/iter; left time: 1747.2703s\n",
      "\titers: 200, epoch: 7 | loss: 0.0147099\n",
      "\tspeed: 0.0536s/iter; left time: 1116.9657s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.52s\n",
      "Steps: 224 | Train Loss: 0.0132826 Vali Loss: 0.0190823 Test Loss: 0.0211098\n",
      "Validation loss decreased (0.019244 --> 0.019082).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0118263\n",
      "\tspeed: 0.1093s/iter; left time: 2265.2138s\n",
      "\titers: 200, epoch: 8 | loss: 0.0127398\n",
      "\tspeed: 0.0483s/iter; left time: 996.3191s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.01s\n",
      "Steps: 224 | Train Loss: 0.0131595 Vali Loss: 0.0192576 Test Loss: 0.0208804\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0148438\n",
      "\tspeed: 0.1102s/iter; left time: 2259.8762s\n",
      "\titers: 200, epoch: 9 | loss: 0.0141029\n",
      "\tspeed: 0.0637s/iter; left time: 1300.4499s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.94s\n",
      "Steps: 224 | Train Loss: 0.0130791 Vali Loss: 0.0191878 Test Loss: 0.0210775\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0133966\n",
      "\tspeed: 0.1038s/iter; left time: 2106.1482s\n",
      "\titers: 200, epoch: 10 | loss: 0.0124890\n",
      "\tspeed: 0.0477s/iter; left time: 962.0660s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.90s\n",
      "Steps: 224 | Train Loss: 0.0129684 Vali Loss: 0.0189475 Test Loss: 0.0207936\n",
      "Validation loss decreased (0.019082 --> 0.018948).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0106154\n",
      "\tspeed: 0.1090s/iter; left time: 2186.6887s\n",
      "\titers: 200, epoch: 11 | loss: 0.0129889\n",
      "\tspeed: 0.0557s/iter; left time: 1112.4528s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 224 | Train Loss: 0.0129028 Vali Loss: 0.0188844 Test Loss: 0.0209089\n",
      "Validation loss decreased (0.018948 --> 0.018884).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0125292\n",
      "\tspeed: 0.1107s/iter; left time: 2196.0378s\n",
      "\titers: 200, epoch: 12 | loss: 0.0131749\n",
      "\tspeed: 0.0459s/iter; left time: 905.1567s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:12.07s\n",
      "Steps: 224 | Train Loss: 0.0128195 Vali Loss: 0.0188164 Test Loss: 0.0208695\n",
      "Validation loss decreased (0.018884 --> 0.018816).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0113560\n",
      "\tspeed: 0.1107s/iter; left time: 2171.3120s\n",
      "\titers: 200, epoch: 13 | loss: 0.0122889\n",
      "\tspeed: 0.0522s/iter; left time: 1018.6841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.88s\n",
      "Steps: 224 | Train Loss: 0.0127381 Vali Loss: 0.0188539 Test Loss: 0.0208254\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0127220\n",
      "\tspeed: 0.1141s/iter; left time: 2212.7356s\n",
      "\titers: 200, epoch: 14 | loss: 0.0135412\n",
      "\tspeed: 0.0512s/iter; left time: 987.5221s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:12.80s\n",
      "Steps: 224 | Train Loss: 0.0127018 Vali Loss: 0.0187021 Test Loss: 0.0206928\n",
      "Validation loss decreased (0.018816 --> 0.018702).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0127345\n",
      "\tspeed: 0.1138s/iter; left time: 2181.9086s\n",
      "\titers: 200, epoch: 15 | loss: 0.0107379\n",
      "\tspeed: 0.0581s/iter; left time: 1107.1712s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:12.32s\n",
      "Steps: 224 | Train Loss: 0.0126647 Vali Loss: 0.0187818 Test Loss: 0.0208291\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0131628\n",
      "\tspeed: 0.1331s/iter; left time: 2520.5197s\n",
      "\titers: 200, epoch: 16 | loss: 0.0119425\n",
      "\tspeed: 0.0613s/iter; left time: 1155.0066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:15.05s\n",
      "Steps: 224 | Train Loss: 0.0126147 Vali Loss: 0.0187834 Test Loss: 0.0210146\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0133438\n",
      "\tspeed: 0.1468s/iter; left time: 2747.9899s\n",
      "\titers: 200, epoch: 17 | loss: 0.0122820\n",
      "\tspeed: 0.0577s/iter; left time: 1074.1064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:14.21s\n",
      "Steps: 224 | Train Loss: 0.0125655 Vali Loss: 0.0185552 Test Loss: 0.0206705\n",
      "Validation loss decreased (0.018702 --> 0.018555).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0129390\n",
      "\tspeed: 0.1468s/iter; left time: 2715.0802s\n",
      "\titers: 200, epoch: 18 | loss: 0.0157475\n",
      "\tspeed: 0.0836s/iter; left time: 1538.2893s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:16.27s\n",
      "Steps: 224 | Train Loss: 0.0125308 Vali Loss: 0.0186281 Test Loss: 0.0207464\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0110804\n",
      "\tspeed: 0.1466s/iter; left time: 2677.9508s\n",
      "\titers: 200, epoch: 19 | loss: 0.0113313\n",
      "\tspeed: 0.0694s/iter; left time: 1260.8440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:14.99s\n",
      "Steps: 224 | Train Loss: 0.0125154 Vali Loss: 0.0187080 Test Loss: 0.0210039\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0120892\n",
      "\tspeed: 0.1453s/iter; left time: 2622.2886s\n",
      "\titers: 200, epoch: 20 | loss: 0.0129364\n",
      "\tspeed: 0.0577s/iter; left time: 1035.4401s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:15.27s\n",
      "Steps: 224 | Train Loss: 0.0124660 Vali Loss: 0.0186161 Test Loss: 0.0207673\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0110016\n",
      "\tspeed: 0.1372s/iter; left time: 2444.2884s\n",
      "\titers: 200, epoch: 21 | loss: 0.0132078\n",
      "\tspeed: 0.0635s/iter; left time: 1125.2830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:15.06s\n",
      "Steps: 224 | Train Loss: 0.0124346 Vali Loss: 0.0186258 Test Loss: 0.0207704\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0102475\n",
      "\tspeed: 0.1537s/iter; left time: 2704.5341s\n",
      "\titers: 200, epoch: 22 | loss: 0.0119495\n",
      "\tspeed: 0.0742s/iter; left time: 1298.1239s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:16.61s\n",
      "Steps: 224 | Train Loss: 0.0124349 Vali Loss: 0.0185870 Test Loss: 0.0208066\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0129670\n",
      "\tspeed: 0.1210s/iter; left time: 2102.5517s\n",
      "\titers: 200, epoch: 23 | loss: 0.0114171\n",
      "\tspeed: 0.0690s/iter; left time: 1191.4217s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:13.87s\n",
      "Steps: 224 | Train Loss: 0.0124044 Vali Loss: 0.0185987 Test Loss: 0.0207789\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0116020\n",
      "\tspeed: 0.0766s/iter; left time: 1312.9771s\n",
      "\titers: 200, epoch: 24 | loss: 0.0120706\n",
      "\tspeed: 0.0434s/iter; left time: 739.7107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:09.70s\n",
      "Steps: 224 | Train Loss: 0.0123961 Vali Loss: 0.0185948 Test Loss: 0.0207864\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0124222\n",
      "\tspeed: 0.1250s/iter; left time: 2115.7450s\n",
      "\titers: 200, epoch: 25 | loss: 0.0153793\n",
      "\tspeed: 0.0649s/iter; left time: 1091.5692s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:15.31s\n",
      "Steps: 224 | Train Loss: 0.0123830 Vali Loss: 0.0185996 Test Loss: 0.0207961\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0122711\n",
      "\tspeed: 0.1567s/iter; left time: 2616.6654s\n",
      "\titers: 200, epoch: 26 | loss: 0.0115288\n",
      "\tspeed: 0.0651s/iter; left time: 1080.2294s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:15.79s\n",
      "Steps: 224 | Train Loss: 0.0123559 Vali Loss: 0.0185369 Test Loss: 0.0206521\n",
      "Validation loss decreased (0.018555 --> 0.018537).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0131443\n",
      "\tspeed: 0.1325s/iter; left time: 2182.9369s\n",
      "\titers: 200, epoch: 27 | loss: 0.0136372\n",
      "\tspeed: 0.0636s/iter; left time: 1041.6113s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:13.97s\n",
      "Steps: 224 | Train Loss: 0.0123368 Vali Loss: 0.0186206 Test Loss: 0.0208083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0118613\n",
      "\tspeed: 0.1272s/iter; left time: 2067.6074s\n",
      "\titers: 200, epoch: 28 | loss: 0.0137763\n",
      "\tspeed: 0.0586s/iter; left time: 946.7049s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:13.45s\n",
      "Steps: 224 | Train Loss: 0.0123275 Vali Loss: 0.0185284 Test Loss: 0.0208035\n",
      "Validation loss decreased (0.018537 --> 0.018528).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0121510\n",
      "\tspeed: 0.1405s/iter; left time: 2252.2776s\n",
      "\titers: 200, epoch: 29 | loss: 0.0107840\n",
      "\tspeed: 0.0712s/iter; left time: 1134.5924s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:16.30s\n",
      "Steps: 224 | Train Loss: 0.0123369 Vali Loss: 0.0185281 Test Loss: 0.0207493\n",
      "Validation loss decreased (0.018528 --> 0.018528).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0138683\n",
      "\tspeed: 0.1391s/iter; left time: 2198.7351s\n",
      "\titers: 200, epoch: 30 | loss: 0.0133592\n",
      "\tspeed: 0.0617s/iter; left time: 969.2713s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:14.09s\n",
      "Steps: 224 | Train Loss: 0.0123014 Vali Loss: 0.0185362 Test Loss: 0.0208496\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0119280\n",
      "\tspeed: 0.1239s/iter; left time: 1930.4301s\n",
      "\titers: 200, epoch: 31 | loss: 0.0113524\n",
      "\tspeed: 0.0678s/iter; left time: 1050.2827s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:14.30s\n",
      "Steps: 224 | Train Loss: 0.0123056 Vali Loss: 0.0185103 Test Loss: 0.0208459\n",
      "Validation loss decreased (0.018528 --> 0.018510).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0114114\n",
      "\tspeed: 0.1271s/iter; left time: 1951.3709s\n",
      "\titers: 200, epoch: 32 | loss: 0.0130700\n",
      "\tspeed: 0.0614s/iter; left time: 937.3906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:14.60s\n",
      "Steps: 224 | Train Loss: 0.0123037 Vali Loss: 0.0185335 Test Loss: 0.0207588\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0132595\n",
      "\tspeed: 0.1552s/iter; left time: 2348.9561s\n",
      "\titers: 200, epoch: 33 | loss: 0.0107495\n",
      "\tspeed: 0.0586s/iter; left time: 880.2307s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:14.24s\n",
      "Steps: 224 | Train Loss: 0.0122890 Vali Loss: 0.0185438 Test Loss: 0.0208019\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0122293\n",
      "\tspeed: 0.1298s/iter; left time: 1935.9060s\n",
      "\titers: 200, epoch: 34 | loss: 0.0126933\n",
      "\tspeed: 0.0633s/iter; left time: 937.3475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:14.02s\n",
      "Steps: 224 | Train Loss: 0.0122830 Vali Loss: 0.0185324 Test Loss: 0.0207754\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0129222\n",
      "\tspeed: 0.1343s/iter; left time: 1972.1209s\n",
      "\titers: 200, epoch: 35 | loss: 0.0108182\n",
      "\tspeed: 0.0615s/iter; left time: 897.3826s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:14.57s\n",
      "Steps: 224 | Train Loss: 0.0122650 Vali Loss: 0.0185707 Test Loss: 0.0208121\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0125490\n",
      "\tspeed: 0.1408s/iter; left time: 2036.5078s\n",
      "\titers: 200, epoch: 36 | loss: 0.0140222\n",
      "\tspeed: 0.0683s/iter; left time: 980.9939s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:15.46s\n",
      "Steps: 224 | Train Loss: 0.0122610 Vali Loss: 0.0185492 Test Loss: 0.0208118\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0152776\n",
      "\tspeed: 0.1168s/iter; left time: 1663.3829s\n",
      "\titers: 200, epoch: 37 | loss: 0.0121370\n",
      "\tspeed: 0.0571s/iter; left time: 806.8245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:12.74s\n",
      "Steps: 224 | Train Loss: 0.0122684 Vali Loss: 0.0185470 Test Loss: 0.0208452\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0121063\n",
      "\tspeed: 0.1275s/iter; left time: 1786.5817s\n",
      "\titers: 200, epoch: 38 | loss: 0.0114475\n",
      "\tspeed: 0.0562s/iter; left time: 782.3565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:13.17s\n",
      "Steps: 224 | Train Loss: 0.0122699 Vali Loss: 0.0185565 Test Loss: 0.0208614\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0134420\n",
      "\tspeed: 0.1198s/iter; left time: 1651.4766s\n",
      "\titers: 200, epoch: 39 | loss: 0.0113555\n",
      "\tspeed: 0.0696s/iter; left time: 953.4177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:15.03s\n",
      "Steps: 224 | Train Loss: 0.0122534 Vali Loss: 0.0185123 Test Loss: 0.0208491\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0111702\n",
      "\tspeed: 0.1392s/iter; left time: 1887.5824s\n",
      "\titers: 200, epoch: 40 | loss: 0.0136204\n",
      "\tspeed: 0.0584s/iter; left time: 785.7224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:13.67s\n",
      "Steps: 224 | Train Loss: 0.0122599 Vali Loss: 0.0185720 Test Loss: 0.0208392\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0110373\n",
      "\tspeed: 0.1189s/iter; left time: 1585.5869s\n",
      "\titers: 200, epoch: 41 | loss: 0.0121039\n",
      "\tspeed: 0.0601s/iter; left time: 796.0753s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:12.82s\n",
      "Steps: 224 | Train Loss: 0.0122421 Vali Loss: 0.0184958 Test Loss: 0.0207846\n",
      "Validation loss decreased (0.018510 --> 0.018496).  Saving model ...\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0117363\n",
      "\tspeed: 0.1270s/iter; left time: 1665.2070s\n",
      "\titers: 200, epoch: 42 | loss: 0.0141523\n",
      "\tspeed: 0.0556s/iter; left time: 723.7693s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:13.06s\n",
      "Steps: 224 | Train Loss: 0.0122492 Vali Loss: 0.0185481 Test Loss: 0.0208161\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0135011\n",
      "\tspeed: 0.1422s/iter; left time: 1833.2903s\n",
      "\titers: 200, epoch: 43 | loss: 0.0126329\n",
      "\tspeed: 0.0631s/iter; left time: 807.5494s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:15.43s\n",
      "Steps: 224 | Train Loss: 0.0122592 Vali Loss: 0.0185243 Test Loss: 0.0208267\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0100381\n",
      "\tspeed: 0.1192s/iter; left time: 1510.3343s\n",
      "\titers: 200, epoch: 44 | loss: 0.0122319\n",
      "\tspeed: 0.0520s/iter; left time: 653.5911s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:11.84s\n",
      "Steps: 224 | Train Loss: 0.0122403 Vali Loss: 0.0185204 Test Loss: 0.0207974\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0127638\n",
      "\tspeed: 0.0910s/iter; left time: 1132.3478s\n",
      "\titers: 200, epoch: 45 | loss: 0.0129958\n",
      "\tspeed: 0.0476s/iter; left time: 587.6923s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:11.09s\n",
      "Steps: 224 | Train Loss: 0.0122323 Vali Loss: 0.0185284 Test Loss: 0.0208220\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0121234\n",
      "\tspeed: 0.0910s/iter; left time: 1112.1473s\n",
      "\titers: 200, epoch: 46 | loss: 0.0132324\n",
      "\tspeed: 0.0441s/iter; left time: 534.9395s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:10.01s\n",
      "Steps: 224 | Train Loss: 0.0122421 Vali Loss: 0.0185286 Test Loss: 0.0207993\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0124325\n",
      "\tspeed: 0.0878s/iter; left time: 1053.5875s\n",
      "\titers: 200, epoch: 47 | loss: 0.0115689\n",
      "\tspeed: 0.0497s/iter; left time: 591.7240s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 224 | Train Loss: 0.0122320 Vali Loss: 0.0185471 Test Loss: 0.0208486\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0131577\n",
      "\tspeed: 0.1046s/iter; left time: 1230.9453s\n",
      "\titers: 200, epoch: 48 | loss: 0.0117313\n",
      "\tspeed: 0.0462s/iter; left time: 538.7897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 224 | Train Loss: 0.0122374 Vali Loss: 0.0185213 Test Loss: 0.0208071\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0126173\n",
      "\tspeed: 0.1129s/iter; left time: 1304.1703s\n",
      "\titers: 200, epoch: 49 | loss: 0.0135240\n",
      "\tspeed: 0.0472s/iter; left time: 540.6533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 224 | Train Loss: 0.0122360 Vali Loss: 0.0184857 Test Loss: 0.0208153\n",
      "Validation loss decreased (0.018496 --> 0.018486).  Saving model ...\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0112510\n",
      "\tspeed: 0.0976s/iter; left time: 1105.7939s\n",
      "\titers: 200, epoch: 50 | loss: 0.0112302\n",
      "\tspeed: 0.0548s/iter; left time: 615.6263s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:12.03s\n",
      "Steps: 224 | Train Loss: 0.0122434 Vali Loss: 0.0185543 Test Loss: 0.0208179\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0113205\n",
      "\tspeed: 0.1005s/iter; left time: 1116.0836s\n",
      "\titers: 200, epoch: 51 | loss: 0.0120899\n",
      "\tspeed: 0.0567s/iter; left time: 623.6655s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:12.34s\n",
      "Steps: 224 | Train Loss: 0.0122339 Vali Loss: 0.0185324 Test Loss: 0.0208079\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0130218\n",
      "\tspeed: 0.1070s/iter; left time: 1163.7954s\n",
      "\titers: 200, epoch: 52 | loss: 0.0135182\n",
      "\tspeed: 0.0462s/iter; left time: 497.6466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:10.78s\n",
      "Steps: 224 | Train Loss: 0.0122330 Vali Loss: 0.0185057 Test Loss: 0.0208085\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0123628\n",
      "\tspeed: 0.1162s/iter; left time: 1237.6015s\n",
      "\titers: 200, epoch: 53 | loss: 0.0136013\n",
      "\tspeed: 0.0489s/iter; left time: 516.4281s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 224 | Train Loss: 0.0122287 Vali Loss: 0.0185395 Test Loss: 0.0208171\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0124737\n",
      "\tspeed: 0.1097s/iter; left time: 1144.2586s\n",
      "\titers: 200, epoch: 54 | loss: 0.0119168\n",
      "\tspeed: 0.0528s/iter; left time: 544.9805s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:12.56s\n",
      "Steps: 224 | Train Loss: 0.0122233 Vali Loss: 0.0185316 Test Loss: 0.0208232\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0115388\n",
      "\tspeed: 0.0969s/iter; left time: 988.4192s\n",
      "\titers: 200, epoch: 55 | loss: 0.0133567\n",
      "\tspeed: 0.0584s/iter; left time: 590.5362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:12.14s\n",
      "Steps: 224 | Train Loss: 0.0122370 Vali Loss: 0.0185180 Test Loss: 0.0208282\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0134816\n",
      "\tspeed: 0.1025s/iter; left time: 1022.7812s\n",
      "\titers: 200, epoch: 56 | loss: 0.0113304\n",
      "\tspeed: 0.0492s/iter; left time: 485.9324s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:11.49s\n",
      "Steps: 224 | Train Loss: 0.0122237 Vali Loss: 0.0184812 Test Loss: 0.0208191\n",
      "Validation loss decreased (0.018486 --> 0.018481).  Saving model ...\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0127599\n",
      "\tspeed: 0.1228s/iter; left time: 1197.7781s\n",
      "\titers: 200, epoch: 57 | loss: 0.0126832\n",
      "\tspeed: 0.0487s/iter; left time: 470.1340s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:11.19s\n",
      "Steps: 224 | Train Loss: 0.0122149 Vali Loss: 0.0185322 Test Loss: 0.0208272\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.0128764\n",
      "\tspeed: 0.1076s/iter; left time: 1025.8135s\n",
      "\titers: 200, epoch: 58 | loss: 0.0134127\n",
      "\tspeed: 0.0420s/iter; left time: 395.8140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 58\n",
      "Cost time: 00h:00m:11.67s\n",
      "Steps: 224 | Train Loss: 0.0122195 Vali Loss: 0.0185319 Test Loss: 0.0208226\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.0128281\n",
      "\tspeed: 0.0714s/iter; left time: 664.3876s\n",
      "\titers: 200, epoch: 59 | loss: 0.0132629\n",
      "\tspeed: 0.0470s/iter; left time: 432.9041s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 59\n",
      "Cost time: 00h:00m:09.65s\n",
      "Steps: 224 | Train Loss: 0.0122318 Vali Loss: 0.0185162 Test Loss: 0.0208412\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.0120071\n",
      "\tspeed: 0.0985s/iter; left time: 894.8921s\n",
      "\titers: 200, epoch: 60 | loss: 0.0123658\n",
      "\tspeed: 0.0424s/iter; left time: 380.5809s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 60\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 224 | Train Loss: 0.0122378 Vali Loss: 0.0185121 Test Loss: 0.0208320\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.0136733\n",
      "\tspeed: 0.1227s/iter; left time: 1087.6455s\n",
      "\titers: 200, epoch: 61 | loss: 0.0110619\n",
      "\tspeed: 0.0548s/iter; left time: 479.6697s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 61\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 224 | Train Loss: 0.0122343 Vali Loss: 0.0185155 Test Loss: 0.0208189\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.0118293\n",
      "\tspeed: 0.1127s/iter; left time: 973.2033s\n",
      "\titers: 200, epoch: 62 | loss: 0.0110706\n",
      "\tspeed: 0.0456s/iter; left time: 389.1568s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 62\n",
      "Cost time: 00h:00m:12.87s\n",
      "Steps: 224 | Train Loss: 0.0122327 Vali Loss: 0.0185753 Test Loss: 0.0208160\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.0124438\n",
      "\tspeed: 0.0982s/iter; left time: 825.7776s\n",
      "\titers: 200, epoch: 63 | loss: 0.0103165\n",
      "\tspeed: 0.0650s/iter; left time: 540.6758s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 63\n",
      "Cost time: 00h:00m:12.78s\n",
      "Steps: 224 | Train Loss: 0.0122317 Vali Loss: 0.0185079 Test Loss: 0.0208250\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.0115705\n",
      "\tspeed: 0.1035s/iter; left time: 847.7687s\n",
      "\titers: 200, epoch: 64 | loss: 0.0128942\n",
      "\tspeed: 0.0482s/iter; left time: 389.5647s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 64\n",
      "Cost time: 00h:00m:12.07s\n",
      "Steps: 224 | Train Loss: 0.0122176 Vali Loss: 0.0185142 Test Loss: 0.0208247\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.0127902\n",
      "\tspeed: 0.1255s/iter; left time: 999.6059s\n",
      "\titers: 200, epoch: 65 | loss: 0.0122773\n",
      "\tspeed: 0.0529s/iter; left time: 415.8707s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 65\n",
      "Cost time: 00h:00m:11.59s\n",
      "Steps: 224 | Train Loss: 0.0122278 Vali Loss: 0.0184961 Test Loss: 0.0208173\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.0115899\n",
      "\tspeed: 0.1097s/iter; left time: 849.0888s\n",
      "\titers: 200, epoch: 66 | loss: 0.0130514\n",
      "\tspeed: 0.0504s/iter; left time: 385.4667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 66\n",
      "Cost time: 00h:00m:12.76s\n",
      "Steps: 224 | Train Loss: 0.0122333 Vali Loss: 0.0185431 Test Loss: 0.0208185\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.020819105207920074, rmse:0.14428827166557312, mae:0.09108439832925797, rse:0.5092132687568665\n",
      "Intermediate time for DE and pred_len 24: 00h:18m:52.55s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_336_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_336_96_DE_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0377843\n",
      "\tspeed: 0.0664s/iter; left time: 1480.6712s\n",
      "\titers: 200, epoch: 1 | loss: 0.0344038\n",
      "\tspeed: 0.0434s/iter; left time: 963.5698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.99s\n",
      "Steps: 224 | Train Loss: 0.0388491 Vali Loss: 0.0371462 Test Loss: 0.0431195\n",
      "Validation loss decreased (inf --> 0.037146).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0295151\n",
      "\tspeed: 0.1346s/iter; left time: 2972.3932s\n",
      "\titers: 200, epoch: 2 | loss: 0.0236684\n",
      "\tspeed: 0.0402s/iter; left time: 883.2348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 224 | Train Loss: 0.0264033 Vali Loss: 0.0309098 Test Loss: 0.0364046\n",
      "Validation loss decreased (0.037146 --> 0.030910).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0219902\n",
      "\tspeed: 0.0939s/iter; left time: 2052.1336s\n",
      "\titers: 200, epoch: 3 | loss: 0.0236232\n",
      "\tspeed: 0.0450s/iter; left time: 978.4734s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.75s\n",
      "Steps: 224 | Train Loss: 0.0239233 Vali Loss: 0.0303387 Test Loss: 0.0362125\n",
      "Validation loss decreased (0.030910 --> 0.030339).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0230603\n",
      "\tspeed: 0.1200s/iter; left time: 2595.1718s\n",
      "\titers: 200, epoch: 4 | loss: 0.0216371\n",
      "\tspeed: 0.0476s/iter; left time: 1025.1456s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:12.42s\n",
      "Steps: 224 | Train Loss: 0.0233313 Vali Loss: 0.0303084 Test Loss: 0.0360353\n",
      "Validation loss decreased (0.030339 --> 0.030308).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0232691\n",
      "\tspeed: 0.1140s/iter; left time: 2440.5904s\n",
      "\titers: 200, epoch: 5 | loss: 0.0200354\n",
      "\tspeed: 0.0644s/iter; left time: 1372.9709s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.60s\n",
      "Steps: 224 | Train Loss: 0.0229674 Vali Loss: 0.0302206 Test Loss: 0.0355811\n",
      "Validation loss decreased (0.030308 --> 0.030221).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0257173\n",
      "\tspeed: 0.0781s/iter; left time: 1653.3021s\n",
      "\titers: 200, epoch: 6 | loss: 0.0252964\n",
      "\tspeed: 0.0441s/iter; left time: 929.2472s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.37s\n",
      "Steps: 224 | Train Loss: 0.0226332 Vali Loss: 0.0301113 Test Loss: 0.0356656\n",
      "Validation loss decreased (0.030221 --> 0.030111).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0228483\n",
      "\tspeed: 0.1411s/iter; left time: 2956.4237s\n",
      "\titers: 200, epoch: 7 | loss: 0.0201746\n",
      "\tspeed: 0.0433s/iter; left time: 903.4735s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 224 | Train Loss: 0.0223822 Vali Loss: 0.0301649 Test Loss: 0.0361735\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0217567\n",
      "\tspeed: 0.1252s/iter; left time: 2594.8327s\n",
      "\titers: 200, epoch: 8 | loss: 0.0212575\n",
      "\tspeed: 0.0566s/iter; left time: 1168.7912s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.55s\n",
      "Steps: 224 | Train Loss: 0.0221981 Vali Loss: 0.0303818 Test Loss: 0.0366595\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0209937\n",
      "\tspeed: 0.1061s/iter; left time: 2176.4208s\n",
      "\titers: 200, epoch: 9 | loss: 0.0226185\n",
      "\tspeed: 0.0535s/iter; left time: 1092.3557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.06s\n",
      "Steps: 224 | Train Loss: 0.0220167 Vali Loss: 0.0303453 Test Loss: 0.0365737\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0213579\n",
      "\tspeed: 0.1082s/iter; left time: 2194.6800s\n",
      "\titers: 200, epoch: 10 | loss: 0.0205298\n",
      "\tspeed: 0.0425s/iter; left time: 857.6942s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 224 | Train Loss: 0.0218319 Vali Loss: 0.0301455 Test Loss: 0.0364236\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0205914\n",
      "\tspeed: 0.1483s/iter; left time: 2975.4447s\n",
      "\titers: 200, epoch: 11 | loss: 0.0229521\n",
      "\tspeed: 0.0510s/iter; left time: 1018.9159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 224 | Train Loss: 0.0216898 Vali Loss: 0.0301673 Test Loss: 0.0366581\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0213532\n",
      "\tspeed: 0.1105s/iter; left time: 2192.2680s\n",
      "\titers: 200, epoch: 12 | loss: 0.0208601\n",
      "\tspeed: 0.0536s/iter; left time: 1057.8349s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:12.29s\n",
      "Steps: 224 | Train Loss: 0.0215192 Vali Loss: 0.0302230 Test Loss: 0.0368246\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0199397\n",
      "\tspeed: 0.1140s/iter; left time: 2236.7315s\n",
      "\titers: 200, epoch: 13 | loss: 0.0213505\n",
      "\tspeed: 0.0566s/iter; left time: 1105.2538s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:12.79s\n",
      "Steps: 224 | Train Loss: 0.0213826 Vali Loss: 0.0304040 Test Loss: 0.0372023\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0217223\n",
      "\tspeed: 0.1183s/iter; left time: 2293.1308s\n",
      "\titers: 200, epoch: 14 | loss: 0.0203427\n",
      "\tspeed: 0.0478s/iter; left time: 921.3369s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.27s\n",
      "Steps: 224 | Train Loss: 0.0212423 Vali Loss: 0.0303967 Test Loss: 0.0372141\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0200363\n",
      "\tspeed: 0.1387s/iter; left time: 2658.2310s\n",
      "\titers: 200, epoch: 15 | loss: 0.0214020\n",
      "\tspeed: 0.0548s/iter; left time: 1045.4599s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 224 | Train Loss: 0.0211565 Vali Loss: 0.0301792 Test Loss: 0.0372579\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0227665\n",
      "\tspeed: 0.1230s/iter; left time: 2330.6739s\n",
      "\titers: 200, epoch: 16 | loss: 0.0229693\n",
      "\tspeed: 0.0443s/iter; left time: 834.7340s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.36s\n",
      "Steps: 224 | Train Loss: 0.0210283 Vali Loss: 0.0303737 Test Loss: 0.0374546\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_336_96_DE_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.03566562011837959, rmse:0.1888534426689148, mae:0.12909182906150818, rse:0.6687682271003723\n",
      "Intermediate time for DE and pred_len 96: 00h:04m:38.76s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_336_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_336_168_DE_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0404512\n",
      "\tspeed: 0.0849s/iter; left time: 1884.2158s\n",
      "\titers: 200, epoch: 1 | loss: 0.0355846\n",
      "\tspeed: 0.0574s/iter; left time: 1268.2577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:13.17s\n",
      "Steps: 223 | Train Loss: 0.0402154 Vali Loss: 0.0383989 Test Loss: 0.0446866\n",
      "Validation loss decreased (inf --> 0.038399).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0304991\n",
      "\tspeed: 0.1510s/iter; left time: 3318.3208s\n",
      "\titers: 200, epoch: 2 | loss: 0.0254042\n",
      "\tspeed: 0.0690s/iter; left time: 1508.7007s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:15.10s\n",
      "Steps: 223 | Train Loss: 0.0286390 Vali Loss: 0.0326475 Test Loss: 0.0383843\n",
      "Validation loss decreased (0.038399 --> 0.032647).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0274383\n",
      "\tspeed: 0.1799s/iter; left time: 3913.8558s\n",
      "\titers: 200, epoch: 3 | loss: 0.0261717\n",
      "\tspeed: 0.0693s/iter; left time: 1499.8781s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:14.76s\n",
      "Steps: 223 | Train Loss: 0.0262391 Vali Loss: 0.0323455 Test Loss: 0.0382562\n",
      "Validation loss decreased (0.032647 --> 0.032346).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0245347\n",
      "\tspeed: 0.1978s/iter; left time: 4258.7947s\n",
      "\titers: 200, epoch: 4 | loss: 0.0272657\n",
      "\tspeed: 0.0649s/iter; left time: 1390.4488s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:14.48s\n",
      "Steps: 223 | Train Loss: 0.0256516 Vali Loss: 0.0320062 Test Loss: 0.0384216\n",
      "Validation loss decreased (0.032346 --> 0.032006).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0242917\n",
      "\tspeed: 0.1065s/iter; left time: 2269.4284s\n",
      "\titers: 200, epoch: 5 | loss: 0.0264873\n",
      "\tspeed: 0.0371s/iter; left time: 786.0712s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.62s\n",
      "Steps: 223 | Train Loss: 0.0252095 Vali Loss: 0.0319972 Test Loss: 0.0383190\n",
      "Validation loss decreased (0.032006 --> 0.031997).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0232972\n",
      "\tspeed: 0.1191s/iter; left time: 2512.3451s\n",
      "\titers: 200, epoch: 6 | loss: 0.0272545\n",
      "\tspeed: 0.0593s/iter; left time: 1245.5091s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:14.41s\n",
      "Steps: 223 | Train Loss: 0.0248804 Vali Loss: 0.0321177 Test Loss: 0.0387783\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0248502\n",
      "\tspeed: 0.1626s/iter; left time: 3391.9986s\n",
      "\titers: 200, epoch: 7 | loss: 0.0246590\n",
      "\tspeed: 0.0609s/iter; left time: 1264.0768s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:14.44s\n",
      "Steps: 223 | Train Loss: 0.0245486 Vali Loss: 0.0322575 Test Loss: 0.0385280\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0231809\n",
      "\tspeed: 0.1776s/iter; left time: 3664.7994s\n",
      "\titers: 200, epoch: 8 | loss: 0.0264457\n",
      "\tspeed: 0.0693s/iter; left time: 1422.6680s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:16.45s\n",
      "Steps: 223 | Train Loss: 0.0242934 Vali Loss: 0.0320401 Test Loss: 0.0383450\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0242314\n",
      "\tspeed: 0.1477s/iter; left time: 3015.4758s\n",
      "\titers: 200, epoch: 9 | loss: 0.0211270\n",
      "\tspeed: 0.0601s/iter; left time: 1220.9300s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:14.65s\n",
      "Steps: 223 | Train Loss: 0.0240388 Vali Loss: 0.0323531 Test Loss: 0.0390620\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0227715\n",
      "\tspeed: 0.1534s/iter; left time: 3098.5266s\n",
      "\titers: 200, epoch: 10 | loss: 0.0230594\n",
      "\tspeed: 0.0654s/iter; left time: 1314.7389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:14.52s\n",
      "Steps: 223 | Train Loss: 0.0237923 Vali Loss: 0.0326520 Test Loss: 0.0393335\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0234806\n",
      "\tspeed: 0.1480s/iter; left time: 2955.7649s\n",
      "\titers: 200, epoch: 11 | loss: 0.0218085\n",
      "\tspeed: 0.0751s/iter; left time: 1493.1195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:15.41s\n",
      "Steps: 223 | Train Loss: 0.0235347 Vali Loss: 0.0327545 Test Loss: 0.0397263\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0224282\n",
      "\tspeed: 0.1723s/iter; left time: 3403.4856s\n",
      "\titers: 200, epoch: 12 | loss: 0.0245247\n",
      "\tspeed: 0.0573s/iter; left time: 1126.1386s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:14.05s\n",
      "Steps: 223 | Train Loss: 0.0233189 Vali Loss: 0.0326866 Test Loss: 0.0398961\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0245513\n",
      "\tspeed: 0.1618s/iter; left time: 3160.0306s\n",
      "\titers: 200, epoch: 13 | loss: 0.0234690\n",
      "\tspeed: 0.0645s/iter; left time: 1253.2803s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:14.69s\n",
      "Steps: 223 | Train Loss: 0.0231075 Vali Loss: 0.0328178 Test Loss: 0.0398631\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0202346\n",
      "\tspeed: 0.1602s/iter; left time: 3091.2178s\n",
      "\titers: 200, epoch: 14 | loss: 0.0251649\n",
      "\tspeed: 0.0634s/iter; left time: 1217.6993s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:14.31s\n",
      "Steps: 223 | Train Loss: 0.0229071 Vali Loss: 0.0329766 Test Loss: 0.0403753\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0229254\n",
      "\tspeed: 0.1855s/iter; left time: 3538.8579s\n",
      "\titers: 200, epoch: 15 | loss: 0.0261855\n",
      "\tspeed: 0.0613s/iter; left time: 1163.3658s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:14.69s\n",
      "Steps: 223 | Train Loss: 0.0227360 Vali Loss: 0.0331558 Test Loss: 0.0409875\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_336_168_DE_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.03831901401281357, rmse:0.19575242698192596, mae:0.13659459352493286, rse:0.6933708786964417\n",
      "Intermediate time for DE and pred_len 168: 00h:05m:44.54s\n",
      "Intermediate time for DE: 00h:29m:15.85s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_336_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_336_24_GB_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0269724\n",
      "\tspeed: 0.0949s/iter; left time: 2117.0566s\n",
      "\titers: 200, epoch: 1 | loss: 0.0243362\n",
      "\tspeed: 0.0577s/iter; left time: 1280.6094s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:14.06s\n",
      "Steps: 224 | Train Loss: 0.0302129 Vali Loss: 0.0295490 Test Loss: 0.0403864\n",
      "Validation loss decreased (inf --> 0.029549).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0150994\n",
      "\tspeed: 0.1301s/iter; left time: 2873.2971s\n",
      "\titers: 200, epoch: 2 | loss: 0.0153803\n",
      "\tspeed: 0.0772s/iter; left time: 1696.6466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:16.26s\n",
      "Steps: 224 | Train Loss: 0.0160300 Vali Loss: 0.0200448 Test Loss: 0.0257564\n",
      "Validation loss decreased (0.029549 --> 0.020045).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0125600\n",
      "\tspeed: 0.1311s/iter; left time: 2865.6311s\n",
      "\titers: 200, epoch: 3 | loss: 0.0146680\n",
      "\tspeed: 0.0579s/iter; left time: 1259.8559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:13.43s\n",
      "Steps: 224 | Train Loss: 0.0141722 Vali Loss: 0.0194474 Test Loss: 0.0255129\n",
      "Validation loss decreased (0.020045 --> 0.019447).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0126545\n",
      "\tspeed: 0.1327s/iter; left time: 2869.7229s\n",
      "\titers: 200, epoch: 4 | loss: 0.0144309\n",
      "\tspeed: 0.0618s/iter; left time: 1330.4901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:13.65s\n",
      "Steps: 224 | Train Loss: 0.0138278 Vali Loss: 0.0195728 Test Loss: 0.0255898\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0136685\n",
      "\tspeed: 0.1314s/iter; left time: 2813.2953s\n",
      "\titers: 200, epoch: 5 | loss: 0.0130324\n",
      "\tspeed: 0.0590s/iter; left time: 1256.9042s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:14.64s\n",
      "Steps: 224 | Train Loss: 0.0136225 Vali Loss: 0.0192595 Test Loss: 0.0253750\n",
      "Validation loss decreased (0.019447 --> 0.019259).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0141169\n",
      "\tspeed: 0.1540s/iter; left time: 3262.9077s\n",
      "\titers: 200, epoch: 6 | loss: 0.0152996\n",
      "\tspeed: 0.0685s/iter; left time: 1443.1657s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.94s\n",
      "Steps: 224 | Train Loss: 0.0134702 Vali Loss: 0.0192517 Test Loss: 0.0253149\n",
      "Validation loss decreased (0.019259 --> 0.019252).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0136902\n",
      "\tspeed: 0.1234s/iter; left time: 2586.8724s\n",
      "\titers: 200, epoch: 7 | loss: 0.0145498\n",
      "\tspeed: 0.0653s/iter; left time: 1361.1872s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:14.39s\n",
      "Steps: 224 | Train Loss: 0.0133439 Vali Loss: 0.0191847 Test Loss: 0.0253538\n",
      "Validation loss decreased (0.019252 --> 0.019185).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0131529\n",
      "\tspeed: 0.1294s/iter; left time: 2682.9253s\n",
      "\titers: 200, epoch: 8 | loss: 0.0136596\n",
      "\tspeed: 0.0565s/iter; left time: 1165.3310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:13.61s\n",
      "Steps: 224 | Train Loss: 0.0132458 Vali Loss: 0.0190764 Test Loss: 0.0249816\n",
      "Validation loss decreased (0.019185 --> 0.019076).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0127635\n",
      "\tspeed: 0.1341s/iter; left time: 2749.5369s\n",
      "\titers: 200, epoch: 9 | loss: 0.0152654\n",
      "\tspeed: 0.0700s/iter; left time: 1428.7750s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:15.53s\n",
      "Steps: 224 | Train Loss: 0.0131349 Vali Loss: 0.0190671 Test Loss: 0.0250864\n",
      "Validation loss decreased (0.019076 --> 0.019067).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0148076\n",
      "\tspeed: 0.1325s/iter; left time: 2687.5516s\n",
      "\titers: 200, epoch: 10 | loss: 0.0125041\n",
      "\tspeed: 0.0629s/iter; left time: 1269.6300s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:13.74s\n",
      "Steps: 224 | Train Loss: 0.0130521 Vali Loss: 0.0188532 Test Loss: 0.0251634\n",
      "Validation loss decreased (0.019067 --> 0.018853).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0137696\n",
      "\tspeed: 0.1191s/iter; left time: 2389.7092s\n",
      "\titers: 200, epoch: 11 | loss: 0.0129565\n",
      "\tspeed: 0.0569s/iter; left time: 1136.0052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:13.57s\n",
      "Steps: 224 | Train Loss: 0.0129698 Vali Loss: 0.0189545 Test Loss: 0.0249062\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0138540\n",
      "\tspeed: 0.1267s/iter; left time: 2514.1986s\n",
      "\titers: 200, epoch: 12 | loss: 0.0117907\n",
      "\tspeed: 0.0545s/iter; left time: 1075.3879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:13.13s\n",
      "Steps: 224 | Train Loss: 0.0128797 Vali Loss: 0.0189448 Test Loss: 0.0250207\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0122733\n",
      "\tspeed: 0.1497s/iter; left time: 2935.8357s\n",
      "\titers: 200, epoch: 13 | loss: 0.0124052\n",
      "\tspeed: 0.0596s/iter; left time: 1162.8773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:14.95s\n",
      "Steps: 224 | Train Loss: 0.0128237 Vali Loss: 0.0189629 Test Loss: 0.0248485\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0136241\n",
      "\tspeed: 0.1196s/iter; left time: 2319.5275s\n",
      "\titers: 200, epoch: 14 | loss: 0.0113933\n",
      "\tspeed: 0.0644s/iter; left time: 1241.4276s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:13.78s\n",
      "Steps: 224 | Train Loss: 0.0127703 Vali Loss: 0.0189413 Test Loss: 0.0249125\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0105021\n",
      "\tspeed: 0.1229s/iter; left time: 2355.1425s\n",
      "\titers: 200, epoch: 15 | loss: 0.0112538\n",
      "\tspeed: 0.0582s/iter; left time: 1109.7729s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:12.84s\n",
      "Steps: 224 | Train Loss: 0.0127224 Vali Loss: 0.0189495 Test Loss: 0.0250717\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0118816\n",
      "\tspeed: 0.1369s/iter; left time: 2592.8825s\n",
      "\titers: 200, epoch: 16 | loss: 0.0133852\n",
      "\tspeed: 0.0708s/iter; left time: 1333.7023s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:15.54s\n",
      "Steps: 224 | Train Loss: 0.0126632 Vali Loss: 0.0187300 Test Loss: 0.0250755\n",
      "Validation loss decreased (0.018853 --> 0.018730).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0128408\n",
      "\tspeed: 0.1243s/iter; left time: 2326.6923s\n",
      "\titers: 200, epoch: 17 | loss: 0.0137415\n",
      "\tspeed: 0.0612s/iter; left time: 1139.6783s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:14.06s\n",
      "Steps: 224 | Train Loss: 0.0126192 Vali Loss: 0.0188007 Test Loss: 0.0249625\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0120534\n",
      "\tspeed: 0.1170s/iter; left time: 2164.2999s\n",
      "\titers: 200, epoch: 18 | loss: 0.0132773\n",
      "\tspeed: 0.0579s/iter; left time: 1064.2629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:13.56s\n",
      "Steps: 224 | Train Loss: 0.0126237 Vali Loss: 0.0187780 Test Loss: 0.0249065\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0123758\n",
      "\tspeed: 0.1237s/iter; left time: 2260.2447s\n",
      "\titers: 200, epoch: 19 | loss: 0.0131285\n",
      "\tspeed: 0.0468s/iter; left time: 849.6184s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.91s\n",
      "Steps: 224 | Train Loss: 0.0125494 Vali Loss: 0.0189852 Test Loss: 0.0251082\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0104999\n",
      "\tspeed: 0.1079s/iter; left time: 1946.6551s\n",
      "\titers: 200, epoch: 20 | loss: 0.0119633\n",
      "\tspeed: 0.0584s/iter; left time: 1048.6901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:13.76s\n",
      "Steps: 224 | Train Loss: 0.0125140 Vali Loss: 0.0189457 Test Loss: 0.0251339\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0133592\n",
      "\tspeed: 0.1363s/iter; left time: 2428.5024s\n",
      "\titers: 200, epoch: 21 | loss: 0.0120427\n",
      "\tspeed: 0.0692s/iter; left time: 1227.1416s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:14.69s\n",
      "Steps: 224 | Train Loss: 0.0124908 Vali Loss: 0.0188245 Test Loss: 0.0249186\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0118607\n",
      "\tspeed: 0.1304s/iter; left time: 2295.3449s\n",
      "\titers: 200, epoch: 22 | loss: 0.0109531\n",
      "\tspeed: 0.0607s/iter; left time: 1061.7371s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:14.56s\n",
      "Steps: 224 | Train Loss: 0.0124526 Vali Loss: 0.0187356 Test Loss: 0.0249374\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0119522\n",
      "\tspeed: 0.1434s/iter; left time: 2491.7680s\n",
      "\titers: 200, epoch: 23 | loss: 0.0112567\n",
      "\tspeed: 0.0722s/iter; left time: 1247.4973s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:16.07s\n",
      "Steps: 224 | Train Loss: 0.0124514 Vali Loss: 0.0188555 Test Loss: 0.0249920\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0119894\n",
      "\tspeed: 0.1292s/iter; left time: 2216.1683s\n",
      "\titers: 200, epoch: 24 | loss: 0.0123275\n",
      "\tspeed: 0.0445s/iter; left time: 758.9499s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.01s\n",
      "Steps: 224 | Train Loss: 0.0124463 Vali Loss: 0.0188186 Test Loss: 0.0249083\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0132149\n",
      "\tspeed: 0.1291s/iter; left time: 2184.7322s\n",
      "\titers: 200, epoch: 25 | loss: 0.0138823\n",
      "\tspeed: 0.0669s/iter; left time: 1125.0219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:14.41s\n",
      "Steps: 224 | Train Loss: 0.0124279 Vali Loss: 0.0187712 Test Loss: 0.0249708\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0122586\n",
      "\tspeed: 0.1518s/iter; left time: 2534.7762s\n",
      "\titers: 200, epoch: 26 | loss: 0.0116505\n",
      "\tspeed: 0.0620s/iter; left time: 1029.8992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:14.47s\n",
      "Steps: 224 | Train Loss: 0.0123840 Vali Loss: 0.0188168 Test Loss: 0.0250552\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_336_24_GB_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.025075480341911316, rmse:0.15835238993167877, mae:0.10364750027656555, rse:0.5462708473205566\n",
      "Intermediate time for GB and pred_len 24: 00h:08m:36.83s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_336_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_336_96_GB_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0330599\n",
      "\tspeed: 0.0721s/iter; left time: 1608.6119s\n",
      "\titers: 200, epoch: 1 | loss: 0.0320975\n",
      "\tspeed: 0.0488s/iter; left time: 1083.7458s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.77s\n",
      "Steps: 224 | Train Loss: 0.0326172 Vali Loss: 0.0341243 Test Loss: 0.0486324\n",
      "Validation loss decreased (inf --> 0.034124).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0248736\n",
      "\tspeed: 0.1356s/iter; left time: 2994.4345s\n",
      "\titers: 200, epoch: 2 | loss: 0.0204750\n",
      "\tspeed: 0.0480s/iter; left time: 1055.6716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 224 | Train Loss: 0.0233874 Vali Loss: 0.0292890 Test Loss: 0.0413262\n",
      "Validation loss decreased (0.034124 --> 0.029289).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0213269\n",
      "\tspeed: 0.1554s/iter; left time: 3395.9949s\n",
      "\titers: 200, epoch: 3 | loss: 0.0210294\n",
      "\tspeed: 0.0511s/iter; left time: 1111.1576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.65s\n",
      "Steps: 224 | Train Loss: 0.0218675 Vali Loss: 0.0292476 Test Loss: 0.0413741\n",
      "Validation loss decreased (0.029289 --> 0.029248).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0207670\n",
      "\tspeed: 0.1219s/iter; left time: 2636.4919s\n",
      "\titers: 200, epoch: 4 | loss: 0.0213264\n",
      "\tspeed: 0.0487s/iter; left time: 1048.5725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:13.18s\n",
      "Steps: 224 | Train Loss: 0.0215076 Vali Loss: 0.0292306 Test Loss: 0.0416261\n",
      "Validation loss decreased (0.029248 --> 0.029231).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0218179\n",
      "\tspeed: 0.1079s/iter; left time: 2309.6949s\n",
      "\titers: 200, epoch: 5 | loss: 0.0186270\n",
      "\tspeed: 0.0542s/iter; left time: 1154.9270s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.33s\n",
      "Steps: 224 | Train Loss: 0.0212008 Vali Loss: 0.0290131 Test Loss: 0.0415289\n",
      "Validation loss decreased (0.029231 --> 0.029013).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0219785\n",
      "\tspeed: 0.1077s/iter; left time: 2280.8785s\n",
      "\titers: 200, epoch: 6 | loss: 0.0216234\n",
      "\tspeed: 0.0547s/iter; left time: 1153.5454s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:12.45s\n",
      "Steps: 224 | Train Loss: 0.0209583 Vali Loss: 0.0289114 Test Loss: 0.0414900\n",
      "Validation loss decreased (0.029013 --> 0.028911).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0219029\n",
      "\tspeed: 0.1349s/iter; left time: 2827.5189s\n",
      "\titers: 200, epoch: 7 | loss: 0.0201141\n",
      "\tspeed: 0.0520s/iter; left time: 1084.6765s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.79s\n",
      "Steps: 224 | Train Loss: 0.0207239 Vali Loss: 0.0289981 Test Loss: 0.0413217\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0213522\n",
      "\tspeed: 0.1377s/iter; left time: 2855.3276s\n",
      "\titers: 200, epoch: 8 | loss: 0.0198995\n",
      "\tspeed: 0.0517s/iter; left time: 1066.8311s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.86s\n",
      "Steps: 224 | Train Loss: 0.0205090 Vali Loss: 0.0290755 Test Loss: 0.0419447\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0203818\n",
      "\tspeed: 0.1119s/iter; left time: 2294.0607s\n",
      "\titers: 200, epoch: 9 | loss: 0.0208378\n",
      "\tspeed: 0.0504s/iter; left time: 1029.2805s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:13.07s\n",
      "Steps: 224 | Train Loss: 0.0202890 Vali Loss: 0.0291642 Test Loss: 0.0433464\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0215469\n",
      "\tspeed: 0.0802s/iter; left time: 1627.7250s\n",
      "\titers: 200, epoch: 10 | loss: 0.0189829\n",
      "\tspeed: 0.0403s/iter; left time: 813.6639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 224 | Train Loss: 0.0201078 Vali Loss: 0.0291582 Test Loss: 0.0431275\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0196008\n",
      "\tspeed: 0.1197s/iter; left time: 2401.1102s\n",
      "\titers: 200, epoch: 11 | loss: 0.0204214\n",
      "\tspeed: 0.0534s/iter; left time: 1066.5709s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.55s\n",
      "Steps: 224 | Train Loss: 0.0199237 Vali Loss: 0.0290428 Test Loss: 0.0433948\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0214353\n",
      "\tspeed: 0.1452s/iter; left time: 2880.7847s\n",
      "\titers: 200, epoch: 12 | loss: 0.0202651\n",
      "\tspeed: 0.0460s/iter; left time: 908.0462s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 224 | Train Loss: 0.0197674 Vali Loss: 0.0287947 Test Loss: 0.0423261\n",
      "Validation loss decreased (0.028911 --> 0.028795).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0199370\n",
      "\tspeed: 0.1579s/iter; left time: 3097.2591s\n",
      "\titers: 200, epoch: 13 | loss: 0.0198088\n",
      "\tspeed: 0.0503s/iter; left time: 980.7151s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.20s\n",
      "Steps: 224 | Train Loss: 0.0195824 Vali Loss: 0.0293069 Test Loss: 0.0439301\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0197234\n",
      "\tspeed: 0.1392s/iter; left time: 2699.0323s\n",
      "\titers: 200, epoch: 14 | loss: 0.0186938\n",
      "\tspeed: 0.0530s/iter; left time: 1022.9801s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:12.91s\n",
      "Steps: 224 | Train Loss: 0.0194025 Vali Loss: 0.0294660 Test Loss: 0.0436790\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0176121\n",
      "\tspeed: 0.1200s/iter; left time: 2299.3362s\n",
      "\titers: 200, epoch: 15 | loss: 0.0187093\n",
      "\tspeed: 0.0555s/iter; left time: 1058.0709s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:13.26s\n",
      "Steps: 224 | Train Loss: 0.0192549 Vali Loss: 0.0296301 Test Loss: 0.0443228\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0197526\n",
      "\tspeed: 0.1209s/iter; left time: 2289.9882s\n",
      "\titers: 200, epoch: 16 | loss: 0.0209090\n",
      "\tspeed: 0.0619s/iter; left time: 1166.0985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:13.32s\n",
      "Steps: 224 | Train Loss: 0.0191137 Vali Loss: 0.0298030 Test Loss: 0.0447214\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0185755\n",
      "\tspeed: 0.1009s/iter; left time: 1888.2981s\n",
      "\titers: 200, epoch: 17 | loss: 0.0183995\n",
      "\tspeed: 0.0401s/iter; left time: 745.7428s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.86s\n",
      "Steps: 224 | Train Loss: 0.0189747 Vali Loss: 0.0297888 Test Loss: 0.0445576\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0186117\n",
      "\tspeed: 0.1168s/iter; left time: 2160.6665s\n",
      "\titers: 200, epoch: 18 | loss: 0.0184859\n",
      "\tspeed: 0.0502s/iter; left time: 922.6528s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.29s\n",
      "Steps: 224 | Train Loss: 0.0188568 Vali Loss: 0.0300322 Test Loss: 0.0449636\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0194068\n",
      "\tspeed: 0.1304s/iter; left time: 2382.0724s\n",
      "\titers: 200, epoch: 19 | loss: 0.0204162\n",
      "\tspeed: 0.0452s/iter; left time: 821.4111s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.19s\n",
      "Steps: 224 | Train Loss: 0.0187836 Vali Loss: 0.0299262 Test Loss: 0.0444477\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0171403\n",
      "\tspeed: 0.1235s/iter; left time: 2229.1043s\n",
      "\titers: 200, epoch: 20 | loss: 0.0184043\n",
      "\tspeed: 0.0503s/iter; left time: 902.0645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.99s\n",
      "Steps: 224 | Train Loss: 0.0186674 Vali Loss: 0.0300881 Test Loss: 0.0449346\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0191293\n",
      "\tspeed: 0.1142s/iter; left time: 2035.0315s\n",
      "\titers: 200, epoch: 21 | loss: 0.0157105\n",
      "\tspeed: 0.0595s/iter; left time: 1054.7874s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:13.43s\n",
      "Steps: 224 | Train Loss: 0.0186050 Vali Loss: 0.0299886 Test Loss: 0.0451011\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0181225\n",
      "\tspeed: 0.1128s/iter; left time: 1984.6494s\n",
      "\titers: 200, epoch: 22 | loss: 0.0193367\n",
      "\tspeed: 0.0533s/iter; left time: 931.7766s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:12.27s\n",
      "Steps: 224 | Train Loss: 0.0185242 Vali Loss: 0.0302375 Test Loss: 0.0453603\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_336_96_GB_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.042326126247644424, rmse:0.20573315024375916, mae:0.14367079734802246, rse:0.711453914642334\n",
      "Intermediate time for GB and pred_len 96: 00h:06m:35.78s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_336_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_336_168_GB_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0324556\n",
      "\tspeed: 0.0669s/iter; left time: 1484.2263s\n",
      "\titers: 200, epoch: 1 | loss: 0.0294266\n",
      "\tspeed: 0.0447s/iter; left time: 986.8875s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.19s\n",
      "Steps: 223 | Train Loss: 0.0334026 Vali Loss: 0.0353121 Test Loss: 0.0505240\n",
      "Validation loss decreased (inf --> 0.035312).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0253777\n",
      "\tspeed: 0.1231s/iter; left time: 2704.9071s\n",
      "\titers: 200, epoch: 2 | loss: 0.0229065\n",
      "\tspeed: 0.0490s/iter; left time: 1072.6404s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:12.65s\n",
      "Steps: 223 | Train Loss: 0.0249163 Vali Loss: 0.0309957 Test Loss: 0.0438974\n",
      "Validation loss decreased (0.035312 --> 0.030996).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0244064\n",
      "\tspeed: 0.1089s/iter; left time: 2368.8973s\n",
      "\titers: 200, epoch: 3 | loss: 0.0227683\n",
      "\tspeed: 0.0548s/iter; left time: 1186.7974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:12.06s\n",
      "Steps: 223 | Train Loss: 0.0233406 Vali Loss: 0.0306889 Test Loss: 0.0435962\n",
      "Validation loss decreased (0.030996 --> 0.030689).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0229893\n",
      "\tspeed: 0.1143s/iter; left time: 2461.5903s\n",
      "\titers: 200, epoch: 4 | loss: 0.0252356\n",
      "\tspeed: 0.0499s/iter; left time: 1069.7177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 223 | Train Loss: 0.0229192 Vali Loss: 0.0309550 Test Loss: 0.0438111\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0214661\n",
      "\tspeed: 0.1358s/iter; left time: 2894.1252s\n",
      "\titers: 200, epoch: 5 | loss: 0.0242698\n",
      "\tspeed: 0.0500s/iter; left time: 1060.6659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 223 | Train Loss: 0.0225604 Vali Loss: 0.0310903 Test Loss: 0.0442144\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0217058\n",
      "\tspeed: 0.1231s/iter; left time: 2595.5265s\n",
      "\titers: 200, epoch: 6 | loss: 0.0224381\n",
      "\tspeed: 0.0541s/iter; left time: 1136.1254s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:12.50s\n",
      "Steps: 223 | Train Loss: 0.0222235 Vali Loss: 0.0310295 Test Loss: 0.0448184\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0219715\n",
      "\tspeed: 0.1057s/iter; left time: 2205.3009s\n",
      "\titers: 200, epoch: 7 | loss: 0.0198097\n",
      "\tspeed: 0.0564s/iter; left time: 1170.4516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:12.31s\n",
      "Steps: 223 | Train Loss: 0.0219527 Vali Loss: 0.0310001 Test Loss: 0.0459959\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0215335\n",
      "\tspeed: 0.1103s/iter; left time: 2276.4955s\n",
      "\titers: 200, epoch: 8 | loss: 0.0237119\n",
      "\tspeed: 0.0499s/iter; left time: 1024.1760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.27s\n",
      "Steps: 223 | Train Loss: 0.0217264 Vali Loss: 0.0306654 Test Loss: 0.0454366\n",
      "Validation loss decreased (0.030689 --> 0.030665).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0217619\n",
      "\tspeed: 0.1486s/iter; left time: 3034.0733s\n",
      "\titers: 200, epoch: 9 | loss: 0.0194218\n",
      "\tspeed: 0.0505s/iter; left time: 1026.0671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 223 | Train Loss: 0.0215284 Vali Loss: 0.0308548 Test Loss: 0.0461860\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0203193\n",
      "\tspeed: 0.1241s/iter; left time: 2505.8855s\n",
      "\titers: 200, epoch: 10 | loss: 0.0218020\n",
      "\tspeed: 0.0512s/iter; left time: 1029.4043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.38s\n",
      "Steps: 223 | Train Loss: 0.0213034 Vali Loss: 0.0310266 Test Loss: 0.0474695\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209533\n",
      "\tspeed: 0.1086s/iter; left time: 2168.0828s\n",
      "\titers: 200, epoch: 11 | loss: 0.0206968\n",
      "\tspeed: 0.0610s/iter; left time: 1211.4503s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.48s\n",
      "Steps: 223 | Train Loss: 0.0211460 Vali Loss: 0.0307943 Test Loss: 0.0460785\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0194066\n",
      "\tspeed: 0.1100s/iter; left time: 2172.6223s\n",
      "\titers: 200, epoch: 12 | loss: 0.0205669\n",
      "\tspeed: 0.0468s/iter; left time: 919.2826s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 223 | Train Loss: 0.0209543 Vali Loss: 0.0309621 Test Loss: 0.0466941\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0209157\n",
      "\tspeed: 0.1305s/iter; left time: 2547.7339s\n",
      "\titers: 200, epoch: 13 | loss: 0.0217956\n",
      "\tspeed: 0.0357s/iter; left time: 693.6325s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:09.87s\n",
      "Steps: 223 | Train Loss: 0.0207500 Vali Loss: 0.0310574 Test Loss: 0.0467395\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0201685\n",
      "\tspeed: 0.0900s/iter; left time: 1737.1115s\n",
      "\titers: 200, epoch: 14 | loss: 0.0209625\n",
      "\tspeed: 0.0554s/iter; left time: 1063.2732s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:12.09s\n",
      "Steps: 223 | Train Loss: 0.0206069 Vali Loss: 0.0311464 Test Loss: 0.0473922\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0213707\n",
      "\tspeed: 0.1168s/iter; left time: 2228.1034s\n",
      "\titers: 200, epoch: 15 | loss: 0.0221779\n",
      "\tspeed: 0.0542s/iter; left time: 1028.2778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.75s\n",
      "Steps: 223 | Train Loss: 0.0204338 Vali Loss: 0.0315270 Test Loss: 0.0482186\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0207527\n",
      "\tspeed: 0.1145s/iter; left time: 2159.6316s\n",
      "\titers: 200, epoch: 16 | loss: 0.0220013\n",
      "\tspeed: 0.0549s/iter; left time: 1029.4716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:12.21s\n",
      "Steps: 223 | Train Loss: 0.0202920 Vali Loss: 0.0313826 Test Loss: 0.0471232\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0202375\n",
      "\tspeed: 0.1286s/iter; left time: 2396.3411s\n",
      "\titers: 200, epoch: 17 | loss: 0.0214163\n",
      "\tspeed: 0.0499s/iter; left time: 923.9435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.79s\n",
      "Steps: 223 | Train Loss: 0.0201838 Vali Loss: 0.0314722 Test Loss: 0.0471828\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0205100\n",
      "\tspeed: 0.1199s/iter; left time: 2207.4392s\n",
      "\titers: 200, epoch: 18 | loss: 0.0198990\n",
      "\tspeed: 0.0499s/iter; left time: 913.4533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 223 | Train Loss: 0.0200635 Vali Loss: 0.0314998 Test Loss: 0.0473011\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_336_168_GB_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.045436594635248184, rmse:0.21315860748291016, mae:0.1511395126581192, rse:0.7390515208244324\n",
      "Intermediate time for GB and pred_len 168: 00h:05m:17.05s\n",
      "Intermediate time for GB: 00h:20m:29.67s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0296890\n",
      "\tspeed: 0.0636s/iter; left time: 1418.9647s\n",
      "\titers: 200, epoch: 1 | loss: 0.0237544\n",
      "\tspeed: 0.0316s/iter; left time: 702.1954s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.95s\n",
      "Steps: 224 | Train Loss: 0.0329008 Vali Loss: 0.0190733 Test Loss: 0.0246103\n",
      "Validation loss decreased (inf --> 0.019073).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0116199\n",
      "\tspeed: 0.0752s/iter; left time: 1659.3678s\n",
      "\titers: 200, epoch: 2 | loss: 0.0107149\n",
      "\tspeed: 0.0344s/iter; left time: 756.0972s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.47s\n",
      "Steps: 224 | Train Loss: 0.0124929 Vali Loss: 0.0097106 Test Loss: 0.0120963\n",
      "Validation loss decreased (0.019073 --> 0.009711).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0093019\n",
      "\tspeed: 0.0783s/iter; left time: 1712.0487s\n",
      "\titers: 200, epoch: 3 | loss: 0.0094715\n",
      "\tspeed: 0.0379s/iter; left time: 824.7608s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.20s\n",
      "Steps: 224 | Train Loss: 0.0097760 Vali Loss: 0.0089975 Test Loss: 0.0112857\n",
      "Validation loss decreased (0.009711 --> 0.008997).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0086354\n",
      "\tspeed: 0.0812s/iter; left time: 1755.5133s\n",
      "\titers: 200, epoch: 4 | loss: 0.0091337\n",
      "\tspeed: 0.0316s/iter; left time: 679.5160s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.09s\n",
      "Steps: 224 | Train Loss: 0.0090991 Vali Loss: 0.0086354 Test Loss: 0.0108668\n",
      "Validation loss decreased (0.008997 --> 0.008635).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0083218\n",
      "\tspeed: 0.0765s/iter; left time: 1638.3038s\n",
      "\titers: 200, epoch: 5 | loss: 0.0086437\n",
      "\tspeed: 0.0295s/iter; left time: 628.8241s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.44s\n",
      "Steps: 224 | Train Loss: 0.0086944 Vali Loss: 0.0083791 Test Loss: 0.0106129\n",
      "Validation loss decreased (0.008635 --> 0.008379).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0074544\n",
      "\tspeed: 0.0730s/iter; left time: 1546.9106s\n",
      "\titers: 200, epoch: 6 | loss: 0.0082814\n",
      "\tspeed: 0.0334s/iter; left time: 704.7319s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 224 | Train Loss: 0.0084254 Vali Loss: 0.0082155 Test Loss: 0.0103570\n",
      "Validation loss decreased (0.008379 --> 0.008215).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0101399\n",
      "\tspeed: 0.0800s/iter; left time: 1677.0133s\n",
      "\titers: 200, epoch: 7 | loss: 0.0085558\n",
      "\tspeed: 0.0337s/iter; left time: 703.1976s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 224 | Train Loss: 0.0082341 Vali Loss: 0.0081511 Test Loss: 0.0103137\n",
      "Validation loss decreased (0.008215 --> 0.008151).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0085888\n",
      "\tspeed: 0.0762s/iter; left time: 1579.7588s\n",
      "\titers: 200, epoch: 8 | loss: 0.0073330\n",
      "\tspeed: 0.0305s/iter; left time: 629.3303s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.22s\n",
      "Steps: 224 | Train Loss: 0.0080856 Vali Loss: 0.0080615 Test Loss: 0.0102863\n",
      "Validation loss decreased (0.008151 --> 0.008062).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0083045\n",
      "\tspeed: 0.0765s/iter; left time: 1567.9369s\n",
      "\titers: 200, epoch: 9 | loss: 0.0074788\n",
      "\tspeed: 0.0336s/iter; left time: 686.4912s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.53s\n",
      "Steps: 224 | Train Loss: 0.0079546 Vali Loss: 0.0079857 Test Loss: 0.0101347\n",
      "Validation loss decreased (0.008062 --> 0.007986).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0087893\n",
      "\tspeed: 0.0782s/iter; left time: 1586.2101s\n",
      "\titers: 200, epoch: 10 | loss: 0.0075799\n",
      "\tspeed: 0.0296s/iter; left time: 598.4523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.32s\n",
      "Steps: 224 | Train Loss: 0.0078699 Vali Loss: 0.0079657 Test Loss: 0.0100636\n",
      "Validation loss decreased (0.007986 --> 0.007966).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0083630\n",
      "\tspeed: 0.0793s/iter; left time: 1590.1206s\n",
      "\titers: 200, epoch: 11 | loss: 0.0083637\n",
      "\tspeed: 0.0298s/iter; left time: 594.8909s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.07s\n",
      "Steps: 224 | Train Loss: 0.0078086 Vali Loss: 0.0079774 Test Loss: 0.0101389\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0080166\n",
      "\tspeed: 0.0788s/iter; left time: 1563.7166s\n",
      "\titers: 200, epoch: 12 | loss: 0.0074236\n",
      "\tspeed: 0.0315s/iter; left time: 622.4686s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 224 | Train Loss: 0.0077407 Vali Loss: 0.0078617 Test Loss: 0.0100172\n",
      "Validation loss decreased (0.007966 --> 0.007862).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0070259\n",
      "\tspeed: 0.0766s/iter; left time: 1501.5707s\n",
      "\titers: 200, epoch: 13 | loss: 0.0064969\n",
      "\tspeed: 0.0277s/iter; left time: 539.7558s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 224 | Train Loss: 0.0076735 Vali Loss: 0.0078890 Test Loss: 0.0099257\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0083835\n",
      "\tspeed: 0.0809s/iter; left time: 1568.3910s\n",
      "\titers: 200, epoch: 14 | loss: 0.0070978\n",
      "\tspeed: 0.0352s/iter; left time: 679.4157s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.91s\n",
      "Steps: 224 | Train Loss: 0.0076318 Vali Loss: 0.0078510 Test Loss: 0.0099501\n",
      "Validation loss decreased (0.007862 --> 0.007851).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0073405\n",
      "\tspeed: 0.0712s/iter; left time: 1364.7115s\n",
      "\titers: 200, epoch: 15 | loss: 0.0070576\n",
      "\tspeed: 0.0322s/iter; left time: 613.4928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.22s\n",
      "Steps: 224 | Train Loss: 0.0075842 Vali Loss: 0.0077827 Test Loss: 0.0098846\n",
      "Validation loss decreased (0.007851 --> 0.007783).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0066532\n",
      "\tspeed: 0.0744s/iter; left time: 1409.1273s\n",
      "\titers: 200, epoch: 16 | loss: 0.0075404\n",
      "\tspeed: 0.0338s/iter; left time: 636.8557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.56s\n",
      "Steps: 224 | Train Loss: 0.0075499 Vali Loss: 0.0078112 Test Loss: 0.0098918\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0072845\n",
      "\tspeed: 0.0728s/iter; left time: 1362.5500s\n",
      "\titers: 200, epoch: 17 | loss: 0.0075278\n",
      "\tspeed: 0.0364s/iter; left time: 678.4131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 224 | Train Loss: 0.0075029 Vali Loss: 0.0077626 Test Loss: 0.0098836\n",
      "Validation loss decreased (0.007783 --> 0.007763).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0070151\n",
      "\tspeed: 0.0572s/iter; left time: 1057.5788s\n",
      "\titers: 200, epoch: 18 | loss: 0.0067206\n",
      "\tspeed: 0.0233s/iter; left time: 428.7176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.39s\n",
      "Steps: 224 | Train Loss: 0.0074731 Vali Loss: 0.0077108 Test Loss: 0.0098255\n",
      "Validation loss decreased (0.007763 --> 0.007711).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0076519\n",
      "\tspeed: 0.0546s/iter; left time: 996.6237s\n",
      "\titers: 200, epoch: 19 | loss: 0.0072590\n",
      "\tspeed: 0.0332s/iter; left time: 604.0107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.17s\n",
      "Steps: 224 | Train Loss: 0.0074561 Vali Loss: 0.0077372 Test Loss: 0.0098622\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0074429\n",
      "\tspeed: 0.0819s/iter; left time: 1477.1287s\n",
      "\titers: 200, epoch: 20 | loss: 0.0081411\n",
      "\tspeed: 0.0297s/iter; left time: 532.3863s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.64s\n",
      "Steps: 224 | Train Loss: 0.0074083 Vali Loss: 0.0077439 Test Loss: 0.0098159\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0075138\n",
      "\tspeed: 0.0783s/iter; left time: 1394.8427s\n",
      "\titers: 200, epoch: 21 | loss: 0.0077157\n",
      "\tspeed: 0.0315s/iter; left time: 558.3217s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 224 | Train Loss: 0.0073987 Vali Loss: 0.0077213 Test Loss: 0.0098094\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0079712\n",
      "\tspeed: 0.0855s/iter; left time: 1504.2407s\n",
      "\titers: 200, epoch: 22 | loss: 0.0067386\n",
      "\tspeed: 0.0331s/iter; left time: 579.6989s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.27s\n",
      "Steps: 224 | Train Loss: 0.0073739 Vali Loss: 0.0076907 Test Loss: 0.0097665\n",
      "Validation loss decreased (0.007711 --> 0.007691).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0069302\n",
      "\tspeed: 0.0891s/iter; left time: 1547.8979s\n",
      "\titers: 200, epoch: 23 | loss: 0.0073624\n",
      "\tspeed: 0.0408s/iter; left time: 705.4595s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:09.67s\n",
      "Steps: 224 | Train Loss: 0.0073590 Vali Loss: 0.0077081 Test Loss: 0.0097860\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0074125\n",
      "\tspeed: 0.1093s/iter; left time: 1873.7194s\n",
      "\titers: 200, epoch: 24 | loss: 0.0077528\n",
      "\tspeed: 0.0396s/iter; left time: 675.9697s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:10.39s\n",
      "Steps: 224 | Train Loss: 0.0073373 Vali Loss: 0.0077147 Test Loss: 0.0097757\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0084801\n",
      "\tspeed: 0.0980s/iter; left time: 1659.2641s\n",
      "\titers: 200, epoch: 25 | loss: 0.0073303\n",
      "\tspeed: 0.0413s/iter; left time: 695.2205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.34s\n",
      "Steps: 224 | Train Loss: 0.0073261 Vali Loss: 0.0076876 Test Loss: 0.0097680\n",
      "Validation loss decreased (0.007691 --> 0.007688).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0075227\n",
      "\tspeed: 0.0986s/iter; left time: 1647.3918s\n",
      "\titers: 200, epoch: 26 | loss: 0.0073098\n",
      "\tspeed: 0.0392s/iter; left time: 650.0628s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:10.11s\n",
      "Steps: 224 | Train Loss: 0.0073206 Vali Loss: 0.0077104 Test Loss: 0.0097499\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0066695\n",
      "\tspeed: 0.1103s/iter; left time: 1816.6040s\n",
      "\titers: 200, epoch: 27 | loss: 0.0084324\n",
      "\tspeed: 0.0460s/iter; left time: 753.1385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:11.67s\n",
      "Steps: 224 | Train Loss: 0.0072957 Vali Loss: 0.0077138 Test Loss: 0.0097647\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0059264\n",
      "\tspeed: 0.0959s/iter; left time: 1557.9681s\n",
      "\titers: 200, epoch: 28 | loss: 0.0072996\n",
      "\tspeed: 0.0393s/iter; left time: 634.7760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:09.99s\n",
      "Steps: 224 | Train Loss: 0.0072941 Vali Loss: 0.0076749 Test Loss: 0.0097210\n",
      "Validation loss decreased (0.007688 --> 0.007675).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0069776\n",
      "\tspeed: 0.1001s/iter; left time: 1603.9535s\n",
      "\titers: 200, epoch: 29 | loss: 0.0073290\n",
      "\tspeed: 0.0380s/iter; left time: 604.7992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:10.05s\n",
      "Steps: 224 | Train Loss: 0.0072689 Vali Loss: 0.0076519 Test Loss: 0.0097318\n",
      "Validation loss decreased (0.007675 --> 0.007652).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0072907\n",
      "\tspeed: 0.0996s/iter; left time: 1573.9871s\n",
      "\titers: 200, epoch: 30 | loss: 0.0072649\n",
      "\tspeed: 0.0371s/iter; left time: 582.5170s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:09.51s\n",
      "Steps: 224 | Train Loss: 0.0072635 Vali Loss: 0.0076725 Test Loss: 0.0097222\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0075929\n",
      "\tspeed: 0.0966s/iter; left time: 1505.1194s\n",
      "\titers: 200, epoch: 31 | loss: 0.0085643\n",
      "\tspeed: 0.0465s/iter; left time: 720.4831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 224 | Train Loss: 0.0072595 Vali Loss: 0.0076283 Test Loss: 0.0097321\n",
      "Validation loss decreased (0.007652 --> 0.007628).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0068163\n",
      "\tspeed: 0.1045s/iter; left time: 1604.8593s\n",
      "\titers: 200, epoch: 32 | loss: 0.0068024\n",
      "\tspeed: 0.0427s/iter; left time: 650.7120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:10.22s\n",
      "Steps: 224 | Train Loss: 0.0072530 Vali Loss: 0.0076806 Test Loss: 0.0097346\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0077122\n",
      "\tspeed: 0.0897s/iter; left time: 1357.2723s\n",
      "\titers: 200, epoch: 33 | loss: 0.0074561\n",
      "\tspeed: 0.0445s/iter; left time: 668.2823s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:09.93s\n",
      "Steps: 224 | Train Loss: 0.0072397 Vali Loss: 0.0076467 Test Loss: 0.0097170\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0078902\n",
      "\tspeed: 0.0899s/iter; left time: 1339.7985s\n",
      "\titers: 200, epoch: 34 | loss: 0.0068785\n",
      "\tspeed: 0.0484s/iter; left time: 716.8394s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:10.16s\n",
      "Steps: 224 | Train Loss: 0.0072408 Vali Loss: 0.0076447 Test Loss: 0.0097050\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0074193\n",
      "\tspeed: 0.0870s/iter; left time: 1277.6801s\n",
      "\titers: 200, epoch: 35 | loss: 0.0073635\n",
      "\tspeed: 0.0471s/iter; left time: 686.4820s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:09.69s\n",
      "Steps: 224 | Train Loss: 0.0072479 Vali Loss: 0.0076825 Test Loss: 0.0097164\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0075246\n",
      "\tspeed: 0.1072s/iter; left time: 1549.8557s\n",
      "\titers: 200, epoch: 36 | loss: 0.0068101\n",
      "\tspeed: 0.0490s/iter; left time: 704.1861s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 224 | Train Loss: 0.0072440 Vali Loss: 0.0076385 Test Loss: 0.0097149\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0068397\n",
      "\tspeed: 0.0899s/iter; left time: 1279.9625s\n",
      "\titers: 200, epoch: 37 | loss: 0.0073553\n",
      "\tspeed: 0.0465s/iter; left time: 657.1650s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:10.06s\n",
      "Steps: 224 | Train Loss: 0.0072382 Vali Loss: 0.0076576 Test Loss: 0.0097067\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0065189\n",
      "\tspeed: 0.0923s/iter; left time: 1293.8674s\n",
      "\titers: 200, epoch: 38 | loss: 0.0077119\n",
      "\tspeed: 0.0504s/iter; left time: 701.5504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:10.46s\n",
      "Steps: 224 | Train Loss: 0.0072418 Vali Loss: 0.0076496 Test Loss: 0.0097053\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0072017\n",
      "\tspeed: 0.0923s/iter; left time: 1272.2152s\n",
      "\titers: 200, epoch: 39 | loss: 0.0068445\n",
      "\tspeed: 0.0508s/iter; left time: 694.8471s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:10.68s\n",
      "Steps: 224 | Train Loss: 0.0072241 Vali Loss: 0.0076411 Test Loss: 0.0097058\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0064202\n",
      "\tspeed: 0.1032s/iter; left time: 1400.2378s\n",
      "\titers: 200, epoch: 40 | loss: 0.0071409\n",
      "\tspeed: 0.0560s/iter; left time: 754.1648s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:11.91s\n",
      "Steps: 224 | Train Loss: 0.0072160 Vali Loss: 0.0076559 Test Loss: 0.0097114\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0072009\n",
      "\tspeed: 0.0931s/iter; left time: 1241.8648s\n",
      "\titers: 200, epoch: 41 | loss: 0.0074637\n",
      "\tspeed: 0.0449s/iter; left time: 594.5727s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:10.01s\n",
      "Steps: 224 | Train Loss: 0.0072244 Vali Loss: 0.0076395 Test Loss: 0.0096962\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.00973209086805582, rmse:0.09865135699510574, mae:0.060730285942554474, rse:0.29031902551651\n",
      "Intermediate time for ES and pred_len 24: 00h:08m:40.03s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0309228\n",
      "\tspeed: 0.0551s/iter; left time: 1229.7799s\n",
      "\titers: 200, epoch: 1 | loss: 0.0244755\n",
      "\tspeed: 0.0362s/iter; left time: 804.7248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 224 | Train Loss: 0.0342055 Vali Loss: 0.0226442 Test Loss: 0.0286723\n",
      "Validation loss decreased (inf --> 0.022644).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0189586\n",
      "\tspeed: 0.1378s/iter; left time: 3041.1119s\n",
      "\titers: 200, epoch: 2 | loss: 0.0158069\n",
      "\tspeed: 0.0503s/iter; left time: 1104.7825s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:12.51s\n",
      "Steps: 224 | Train Loss: 0.0182901 Vali Loss: 0.0165065 Test Loss: 0.0204272\n",
      "Validation loss decreased (0.022644 --> 0.016506).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0157190\n",
      "\tspeed: 0.1254s/iter; left time: 2740.7390s\n",
      "\titers: 200, epoch: 3 | loss: 0.0146123\n",
      "\tspeed: 0.0405s/iter; left time: 881.5793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.09s\n",
      "Steps: 224 | Train Loss: 0.0157535 Vali Loss: 0.0156040 Test Loss: 0.0193479\n",
      "Validation loss decreased (0.016506 --> 0.015604).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0148218\n",
      "\tspeed: 0.1281s/iter; left time: 2770.4105s\n",
      "\titers: 200, epoch: 4 | loss: 0.0146640\n",
      "\tspeed: 0.0447s/iter; left time: 962.2575s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 224 | Train Loss: 0.0150019 Vali Loss: 0.0154184 Test Loss: 0.0190234\n",
      "Validation loss decreased (0.015604 --> 0.015418).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0142901\n",
      "\tspeed: 0.1242s/iter; left time: 2657.7461s\n",
      "\titers: 200, epoch: 5 | loss: 0.0133076\n",
      "\tspeed: 0.0255s/iter; left time: 543.5870s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.53s\n",
      "Steps: 224 | Train Loss: 0.0145813 Vali Loss: 0.0150859 Test Loss: 0.0186584\n",
      "Validation loss decreased (0.015418 --> 0.015086).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0143639\n",
      "\tspeed: 0.0724s/iter; left time: 1533.7826s\n",
      "\titers: 200, epoch: 6 | loss: 0.0143160\n",
      "\tspeed: 0.0385s/iter; left time: 811.2298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.12s\n",
      "Steps: 224 | Train Loss: 0.0142854 Vali Loss: 0.0149837 Test Loss: 0.0185050\n",
      "Validation loss decreased (0.015086 --> 0.014984).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0144117\n",
      "\tspeed: 0.1519s/iter; left time: 3183.2084s\n",
      "\titers: 200, epoch: 7 | loss: 0.0131367\n",
      "\tspeed: 0.0630s/iter; left time: 1314.4052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:13.43s\n",
      "Steps: 224 | Train Loss: 0.0140586 Vali Loss: 0.0148926 Test Loss: 0.0182597\n",
      "Validation loss decreased (0.014984 --> 0.014893).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0140473\n",
      "\tspeed: 0.1388s/iter; left time: 2878.0148s\n",
      "\titers: 200, epoch: 8 | loss: 0.0135981\n",
      "\tspeed: 0.0693s/iter; left time: 1429.8117s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:13.84s\n",
      "Steps: 224 | Train Loss: 0.0139104 Vali Loss: 0.0148134 Test Loss: 0.0183719\n",
      "Validation loss decreased (0.014893 --> 0.014813).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0138677\n",
      "\tspeed: 0.1296s/iter; left time: 2658.1521s\n",
      "\titers: 200, epoch: 9 | loss: 0.0129254\n",
      "\tspeed: 0.0649s/iter; left time: 1324.7005s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:13.65s\n",
      "Steps: 224 | Train Loss: 0.0137508 Vali Loss: 0.0147350 Test Loss: 0.0181817\n",
      "Validation loss decreased (0.014813 --> 0.014735).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0146253\n",
      "\tspeed: 0.1554s/iter; left time: 3152.8680s\n",
      "\titers: 200, epoch: 10 | loss: 0.0145938\n",
      "\tspeed: 0.0609s/iter; left time: 1228.8994s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.62s\n",
      "Steps: 224 | Train Loss: 0.0136046 Vali Loss: 0.0147417 Test Loss: 0.0182025\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0133169\n",
      "\tspeed: 0.1644s/iter; left time: 3297.8493s\n",
      "\titers: 200, epoch: 11 | loss: 0.0142514\n",
      "\tspeed: 0.0533s/iter; left time: 1063.3108s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.82s\n",
      "Steps: 224 | Train Loss: 0.0134796 Vali Loss: 0.0147851 Test Loss: 0.0181727\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0135381\n",
      "\tspeed: 0.1606s/iter; left time: 3186.6268s\n",
      "\titers: 200, epoch: 12 | loss: 0.0134822\n",
      "\tspeed: 0.0533s/iter; left time: 1051.1577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:13.27s\n",
      "Steps: 224 | Train Loss: 0.0133539 Vali Loss: 0.0147297 Test Loss: 0.0181911\n",
      "Validation loss decreased (0.014735 --> 0.014730).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0121429\n",
      "\tspeed: 0.1758s/iter; left time: 3446.9937s\n",
      "\titers: 200, epoch: 13 | loss: 0.0133668\n",
      "\tspeed: 0.0495s/iter; left time: 965.4897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:12.84s\n",
      "Steps: 224 | Train Loss: 0.0132528 Vali Loss: 0.0148262 Test Loss: 0.0182404\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0116928\n",
      "\tspeed: 0.1594s/iter; left time: 3091.4192s\n",
      "\titers: 200, epoch: 14 | loss: 0.0132391\n",
      "\tspeed: 0.0477s/iter; left time: 920.4998s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:13.19s\n",
      "Steps: 224 | Train Loss: 0.0131951 Vali Loss: 0.0147897 Test Loss: 0.0182665\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0122485\n",
      "\tspeed: 0.1298s/iter; left time: 2487.5700s\n",
      "\titers: 200, epoch: 15 | loss: 0.0145092\n",
      "\tspeed: 0.0621s/iter; left time: 1184.8316s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:13.01s\n",
      "Steps: 224 | Train Loss: 0.0130912 Vali Loss: 0.0149205 Test Loss: 0.0182158\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0130883\n",
      "\tspeed: 0.1501s/iter; left time: 2843.5236s\n",
      "\titers: 200, epoch: 16 | loss: 0.0134652\n",
      "\tspeed: 0.0628s/iter; left time: 1183.2257s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:13.74s\n",
      "Steps: 224 | Train Loss: 0.0129957 Vali Loss: 0.0149039 Test Loss: 0.0183016\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0127407\n",
      "\tspeed: 0.1116s/iter; left time: 2089.2602s\n",
      "\titers: 200, epoch: 17 | loss: 0.0138984\n",
      "\tspeed: 0.0422s/iter; left time: 785.0125s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:09.63s\n",
      "Steps: 224 | Train Loss: 0.0129110 Vali Loss: 0.0149141 Test Loss: 0.0182459\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0134658\n",
      "\tspeed: 0.1244s/iter; left time: 2300.0877s\n",
      "\titers: 200, epoch: 18 | loss: 0.0132504\n",
      "\tspeed: 0.0451s/iter; left time: 829.0458s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:09.55s\n",
      "Steps: 224 | Train Loss: 0.0128538 Vali Loss: 0.0148301 Test Loss: 0.0181864\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0130002\n",
      "\tspeed: 0.0592s/iter; left time: 1082.3587s\n",
      "\titers: 200, epoch: 19 | loss: 0.0141721\n",
      "\tspeed: 0.0367s/iter; left time: 667.5381s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.16s\n",
      "Steps: 224 | Train Loss: 0.0127877 Vali Loss: 0.0150252 Test Loss: 0.0182931\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0118381\n",
      "\tspeed: 0.0907s/iter; left time: 1636.6244s\n",
      "\titers: 200, epoch: 20 | loss: 0.0125709\n",
      "\tspeed: 0.0567s/iter; left time: 1016.9838s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.42s\n",
      "Steps: 224 | Train Loss: 0.0127468 Vali Loss: 0.0150428 Test Loss: 0.0182738\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0119616\n",
      "\tspeed: 0.0973s/iter; left time: 1734.3317s\n",
      "\titers: 200, epoch: 21 | loss: 0.0113186\n",
      "\tspeed: 0.0446s/iter; left time: 790.8293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.10s\n",
      "Steps: 224 | Train Loss: 0.0126835 Vali Loss: 0.0149792 Test Loss: 0.0182552\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0137008\n",
      "\tspeed: 0.1025s/iter; left time: 1804.5472s\n",
      "\titers: 200, epoch: 22 | loss: 0.0129450\n",
      "\tspeed: 0.0455s/iter; left time: 796.5745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.26s\n",
      "Steps: 224 | Train Loss: 0.0126265 Vali Loss: 0.0150438 Test Loss: 0.0182974\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01819109171628952, rmse:0.1348743587732315, mae:0.08794556558132172, rse:0.39622026681900024\n",
      "Intermediate time for ES and pred_len 96: 00h:06m:40.79s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0327703\n",
      "\tspeed: 0.0641s/iter; left time: 1422.7227s\n",
      "\titers: 200, epoch: 1 | loss: 0.0277364\n",
      "\tspeed: 0.0435s/iter; left time: 961.3649s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.58s\n",
      "Steps: 223 | Train Loss: 0.0350410 Vali Loss: 0.0238507 Test Loss: 0.0296973\n",
      "Validation loss decreased (inf --> 0.023851).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0199245\n",
      "\tspeed: 0.1145s/iter; left time: 2517.2741s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166473\n",
      "\tspeed: 0.0448s/iter; left time: 979.4956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 223 | Train Loss: 0.0197155 Vali Loss: 0.0186296 Test Loss: 0.0224006\n",
      "Validation loss decreased (0.023851 --> 0.018630).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0183971\n",
      "\tspeed: 0.1097s/iter; left time: 2387.2392s\n",
      "\titers: 200, epoch: 3 | loss: 0.0177713\n",
      "\tspeed: 0.0326s/iter; left time: 706.2489s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.59s\n",
      "Steps: 223 | Train Loss: 0.0172167 Vali Loss: 0.0177712 Test Loss: 0.0211671\n",
      "Validation loss decreased (0.018630 --> 0.017771).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0174361\n",
      "\tspeed: 0.0605s/iter; left time: 1302.7380s\n",
      "\titers: 200, epoch: 4 | loss: 0.0157802\n",
      "\tspeed: 0.0393s/iter; left time: 842.2135s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.19s\n",
      "Steps: 223 | Train Loss: 0.0164956 Vali Loss: 0.0173551 Test Loss: 0.0206725\n",
      "Validation loss decreased (0.017771 --> 0.017355).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0168731\n",
      "\tspeed: 0.1029s/iter; left time: 2191.6926s\n",
      "\titers: 200, epoch: 5 | loss: 0.0152332\n",
      "\tspeed: 0.0509s/iter; left time: 1079.3175s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 223 | Train Loss: 0.0160609 Vali Loss: 0.0171954 Test Loss: 0.0203742\n",
      "Validation loss decreased (0.017355 --> 0.017195).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0153892\n",
      "\tspeed: 0.0991s/iter; left time: 2089.3214s\n",
      "\titers: 200, epoch: 6 | loss: 0.0156782\n",
      "\tspeed: 0.0510s/iter; left time: 1069.8379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.79s\n",
      "Steps: 223 | Train Loss: 0.0157492 Vali Loss: 0.0170438 Test Loss: 0.0202812\n",
      "Validation loss decreased (0.017195 --> 0.017044).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0157425\n",
      "\tspeed: 0.1027s/iter; left time: 2142.8339s\n",
      "\titers: 200, epoch: 7 | loss: 0.0153941\n",
      "\tspeed: 0.0518s/iter; left time: 1075.4837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.14s\n",
      "Steps: 223 | Train Loss: 0.0154863 Vali Loss: 0.0169180 Test Loss: 0.0201942\n",
      "Validation loss decreased (0.017044 --> 0.016918).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0142000\n",
      "\tspeed: 0.1121s/iter; left time: 2313.0750s\n",
      "\titers: 200, epoch: 8 | loss: 0.0152045\n",
      "\tspeed: 0.0420s/iter; left time: 861.6695s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.21s\n",
      "Steps: 223 | Train Loss: 0.0152446 Vali Loss: 0.0170044 Test Loss: 0.0203218\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0152434\n",
      "\tspeed: 0.1320s/iter; left time: 2695.3751s\n",
      "\titers: 200, epoch: 9 | loss: 0.0145945\n",
      "\tspeed: 0.0453s/iter; left time: 920.8837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.00s\n",
      "Steps: 223 | Train Loss: 0.0150549 Vali Loss: 0.0169346 Test Loss: 0.0202678\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0148309\n",
      "\tspeed: 0.1180s/iter; left time: 2382.4281s\n",
      "\titers: 200, epoch: 10 | loss: 0.0151564\n",
      "\tspeed: 0.0451s/iter; left time: 906.0058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0148671 Vali Loss: 0.0169728 Test Loss: 0.0202405\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0145157\n",
      "\tspeed: 0.1124s/iter; left time: 2244.3829s\n",
      "\titers: 200, epoch: 11 | loss: 0.0137834\n",
      "\tspeed: 0.0430s/iter; left time: 854.6198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.72s\n",
      "Steps: 223 | Train Loss: 0.0147031 Vali Loss: 0.0170753 Test Loss: 0.0201891\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0135146\n",
      "\tspeed: 0.0886s/iter; left time: 1750.1451s\n",
      "\titers: 200, epoch: 12 | loss: 0.0152712\n",
      "\tspeed: 0.0567s/iter; left time: 1114.4220s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 223 | Train Loss: 0.0145602 Vali Loss: 0.0170106 Test Loss: 0.0202370\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0140620\n",
      "\tspeed: 0.0938s/iter; left time: 1831.6130s\n",
      "\titers: 200, epoch: 13 | loss: 0.0136154\n",
      "\tspeed: 0.0531s/iter; left time: 1031.5584s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 223 | Train Loss: 0.0143957 Vali Loss: 0.0172204 Test Loss: 0.0203622\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0139850\n",
      "\tspeed: 0.0885s/iter; left time: 1707.6003s\n",
      "\titers: 200, epoch: 14 | loss: 0.0148465\n",
      "\tspeed: 0.0415s/iter; left time: 797.6370s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.10s\n",
      "Steps: 223 | Train Loss: 0.0142812 Vali Loss: 0.0173684 Test Loss: 0.0203415\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0141903\n",
      "\tspeed: 0.1223s/iter; left time: 2332.8992s\n",
      "\titers: 200, epoch: 15 | loss: 0.0156419\n",
      "\tspeed: 0.0487s/iter; left time: 924.5823s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:09.86s\n",
      "Steps: 223 | Train Loss: 0.0141361 Vali Loss: 0.0175488 Test Loss: 0.0205400\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0146647\n",
      "\tspeed: 0.1135s/iter; left time: 2140.0844s\n",
      "\titers: 200, epoch: 16 | loss: 0.0141779\n",
      "\tspeed: 0.0412s/iter; left time: 772.9944s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.22s\n",
      "Steps: 223 | Train Loss: 0.0140371 Vali Loss: 0.0176834 Test Loss: 0.0205687\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0134907\n",
      "\tspeed: 0.1038s/iter; left time: 1933.7283s\n",
      "\titers: 200, epoch: 17 | loss: 0.0134168\n",
      "\tspeed: 0.0383s/iter; left time: 708.9371s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.83s\n",
      "Steps: 223 | Train Loss: 0.0139591 Vali Loss: 0.0176696 Test Loss: 0.0204780\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.0201941579580307, rmse:0.14210614562034607, mae:0.09476298838853836, rse:0.41749510169029236\n",
      "Intermediate time for ES and pred_len 168: 00h:04m:25.17s\n",
      "Intermediate time for ES: 00h:19m:45.99s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_336_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_336_24_FR_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0163190\n",
      "\tspeed: 0.0544s/iter; left time: 1214.2560s\n",
      "\titers: 200, epoch: 1 | loss: 0.0146600\n",
      "\tspeed: 0.0416s/iter; left time: 924.5395s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.62s\n",
      "Steps: 224 | Train Loss: 0.0202520 Vali Loss: 0.0160400 Test Loss: 0.0191508\n",
      "Validation loss decreased (inf --> 0.016040).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0066351\n",
      "\tspeed: 0.0951s/iter; left time: 2100.3451s\n",
      "\titers: 200, epoch: 2 | loss: 0.0067104\n",
      "\tspeed: 0.0354s/iter; left time: 778.4421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.55s\n",
      "Steps: 224 | Train Loss: 0.0081940 Vali Loss: 0.0096639 Test Loss: 0.0110337\n",
      "Validation loss decreased (0.016040 --> 0.009664).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0066599\n",
      "\tspeed: 0.0950s/iter; left time: 2075.5629s\n",
      "\titers: 200, epoch: 3 | loss: 0.0061675\n",
      "\tspeed: 0.0255s/iter; left time: 553.9086s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.38s\n",
      "Steps: 224 | Train Loss: 0.0066374 Vali Loss: 0.0091936 Test Loss: 0.0107437\n",
      "Validation loss decreased (0.009664 --> 0.009194).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0064794\n",
      "\tspeed: 0.0533s/iter; left time: 1153.0197s\n",
      "\titers: 200, epoch: 4 | loss: 0.0069979\n",
      "\tspeed: 0.0311s/iter; left time: 668.7697s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 224 | Train Loss: 0.0062819 Vali Loss: 0.0089752 Test Loss: 0.0104317\n",
      "Validation loss decreased (0.009194 --> 0.008975).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059530\n",
      "\tspeed: 0.0819s/iter; left time: 1753.4147s\n",
      "\titers: 200, epoch: 5 | loss: 0.0070641\n",
      "\tspeed: 0.0386s/iter; left time: 822.9779s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.83s\n",
      "Steps: 224 | Train Loss: 0.0060674 Vali Loss: 0.0087049 Test Loss: 0.0102102\n",
      "Validation loss decreased (0.008975 --> 0.008705).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0063576\n",
      "\tspeed: 0.0819s/iter; left time: 1735.2260s\n",
      "\titers: 200, epoch: 6 | loss: 0.0049777\n",
      "\tspeed: 0.0347s/iter; left time: 731.9350s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.22s\n",
      "Steps: 224 | Train Loss: 0.0059176 Vali Loss: 0.0086892 Test Loss: 0.0103117\n",
      "Validation loss decreased (0.008705 --> 0.008689).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0069176\n",
      "\tspeed: 0.0806s/iter; left time: 1689.9213s\n",
      "\titers: 200, epoch: 7 | loss: 0.0063449\n",
      "\tspeed: 0.0346s/iter; left time: 721.5283s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 224 | Train Loss: 0.0057757 Vali Loss: 0.0085956 Test Loss: 0.0101283\n",
      "Validation loss decreased (0.008689 --> 0.008596).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0059064\n",
      "\tspeed: 0.0844s/iter; left time: 1750.4521s\n",
      "\titers: 200, epoch: 8 | loss: 0.0056319\n",
      "\tspeed: 0.0388s/iter; left time: 800.0644s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.64s\n",
      "Steps: 224 | Train Loss: 0.0057092 Vali Loss: 0.0085681 Test Loss: 0.0100957\n",
      "Validation loss decreased (0.008596 --> 0.008568).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0048203\n",
      "\tspeed: 0.0826s/iter; left time: 1694.0421s\n",
      "\titers: 200, epoch: 9 | loss: 0.0059386\n",
      "\tspeed: 0.0349s/iter; left time: 713.1985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.13s\n",
      "Steps: 224 | Train Loss: 0.0056026 Vali Loss: 0.0084239 Test Loss: 0.0101090\n",
      "Validation loss decreased (0.008568 --> 0.008424).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0058560\n",
      "\tspeed: 0.0812s/iter; left time: 1647.4570s\n",
      "\titers: 200, epoch: 10 | loss: 0.0055602\n",
      "\tspeed: 0.0341s/iter; left time: 688.8113s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.49s\n",
      "Steps: 224 | Train Loss: 0.0055465 Vali Loss: 0.0083629 Test Loss: 0.0101492\n",
      "Validation loss decreased (0.008424 --> 0.008363).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0054779\n",
      "\tspeed: 0.0782s/iter; left time: 1568.5426s\n",
      "\titers: 200, epoch: 11 | loss: 0.0054597\n",
      "\tspeed: 0.0343s/iter; left time: 685.1674s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 224 | Train Loss: 0.0054899 Vali Loss: 0.0084630 Test Loss: 0.0102342\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0050498\n",
      "\tspeed: 0.0802s/iter; left time: 1590.8331s\n",
      "\titers: 200, epoch: 12 | loss: 0.0049490\n",
      "\tspeed: 0.0385s/iter; left time: 759.0275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.63s\n",
      "Steps: 224 | Train Loss: 0.0054395 Vali Loss: 0.0083584 Test Loss: 0.0100740\n",
      "Validation loss decreased (0.008363 --> 0.008358).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0054406\n",
      "\tspeed: 0.0856s/iter; left time: 1678.9852s\n",
      "\titers: 200, epoch: 13 | loss: 0.0047616\n",
      "\tspeed: 0.0369s/iter; left time: 720.7773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:08.42s\n",
      "Steps: 224 | Train Loss: 0.0054074 Vali Loss: 0.0084354 Test Loss: 0.0101401\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0050504\n",
      "\tspeed: 0.0825s/iter; left time: 1598.9079s\n",
      "\titers: 200, epoch: 14 | loss: 0.0054030\n",
      "\tspeed: 0.0330s/iter; left time: 636.0145s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:08.18s\n",
      "Steps: 224 | Train Loss: 0.0053647 Vali Loss: 0.0083279 Test Loss: 0.0101184\n",
      "Validation loss decreased (0.008358 --> 0.008328).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0053471\n",
      "\tspeed: 0.0844s/iter; left time: 1616.8035s\n",
      "\titers: 200, epoch: 15 | loss: 0.0043377\n",
      "\tspeed: 0.0384s/iter; left time: 732.3532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:08.70s\n",
      "Steps: 224 | Train Loss: 0.0053162 Vali Loss: 0.0083566 Test Loss: 0.0100685\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0049260\n",
      "\tspeed: 0.0829s/iter; left time: 1570.2516s\n",
      "\titers: 200, epoch: 16 | loss: 0.0048694\n",
      "\tspeed: 0.0360s/iter; left time: 679.1593s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:08.64s\n",
      "Steps: 224 | Train Loss: 0.0052947 Vali Loss: 0.0083633 Test Loss: 0.0101274\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0060960\n",
      "\tspeed: 0.0845s/iter; left time: 1580.8268s\n",
      "\titers: 200, epoch: 17 | loss: 0.0050376\n",
      "\tspeed: 0.0379s/iter; left time: 705.5866s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.77s\n",
      "Steps: 224 | Train Loss: 0.0052817 Vali Loss: 0.0083467 Test Loss: 0.0100814\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0048779\n",
      "\tspeed: 0.0791s/iter; left time: 1463.4473s\n",
      "\titers: 200, epoch: 18 | loss: 0.0047544\n",
      "\tspeed: 0.0250s/iter; left time: 460.1482s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.71s\n",
      "Steps: 224 | Train Loss: 0.0052451 Vali Loss: 0.0082610 Test Loss: 0.0100459\n",
      "Validation loss decreased (0.008328 --> 0.008261).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0045703\n",
      "\tspeed: 0.0535s/iter; left time: 977.4847s\n",
      "\titers: 200, epoch: 19 | loss: 0.0048963\n",
      "\tspeed: 0.0239s/iter; left time: 433.8940s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 224 | Train Loss: 0.0052210 Vali Loss: 0.0083092 Test Loss: 0.0100771\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0050457\n",
      "\tspeed: 0.0817s/iter; left time: 1473.6391s\n",
      "\titers: 200, epoch: 20 | loss: 0.0050213\n",
      "\tspeed: 0.0384s/iter; left time: 688.5427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 224 | Train Loss: 0.0051960 Vali Loss: 0.0082870 Test Loss: 0.0100884\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0052502\n",
      "\tspeed: 0.0800s/iter; left time: 1426.4232s\n",
      "\titers: 200, epoch: 21 | loss: 0.0049596\n",
      "\tspeed: 0.0381s/iter; left time: 674.7705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.81s\n",
      "Steps: 224 | Train Loss: 0.0051845 Vali Loss: 0.0082839 Test Loss: 0.0100520\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0050836\n",
      "\tspeed: 0.0856s/iter; left time: 1507.1230s\n",
      "\titers: 200, epoch: 22 | loss: 0.0041651\n",
      "\tspeed: 0.0347s/iter; left time: 607.1565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.86s\n",
      "Steps: 224 | Train Loss: 0.0051848 Vali Loss: 0.0083158 Test Loss: 0.0100871\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0051514\n",
      "\tspeed: 0.0892s/iter; left time: 1549.4638s\n",
      "\titers: 200, epoch: 23 | loss: 0.0056497\n",
      "\tspeed: 0.0379s/iter; left time: 655.1541s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:09.05s\n",
      "Steps: 224 | Train Loss: 0.0051422 Vali Loss: 0.0083156 Test Loss: 0.0100756\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0049192\n",
      "\tspeed: 0.0876s/iter; left time: 1502.2883s\n",
      "\titers: 200, epoch: 24 | loss: 0.0047177\n",
      "\tspeed: 0.0404s/iter; left time: 689.4526s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:09.24s\n",
      "Steps: 224 | Train Loss: 0.0051442 Vali Loss: 0.0083113 Test Loss: 0.0101006\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0050956\n",
      "\tspeed: 0.0833s/iter; left time: 1409.0926s\n",
      "\titers: 200, epoch: 25 | loss: 0.0054086\n",
      "\tspeed: 0.0394s/iter; left time: 663.3180s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:08.79s\n",
      "Steps: 224 | Train Loss: 0.0051337 Vali Loss: 0.0082635 Test Loss: 0.0101114\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0049215\n",
      "\tspeed: 0.0808s/iter; left time: 1350.0991s\n",
      "\titers: 200, epoch: 26 | loss: 0.0042607\n",
      "\tspeed: 0.0419s/iter; left time: 695.2608s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:08.69s\n",
      "Steps: 224 | Train Loss: 0.0051240 Vali Loss: 0.0082871 Test Loss: 0.0101289\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0053276\n",
      "\tspeed: 0.0892s/iter; left time: 1469.0890s\n",
      "\titers: 200, epoch: 27 | loss: 0.0058882\n",
      "\tspeed: 0.0383s/iter; left time: 626.7117s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 224 | Train Loss: 0.0051055 Vali Loss: 0.0082980 Test Loss: 0.0100792\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0057416\n",
      "\tspeed: 0.0973s/iter; left time: 1581.1841s\n",
      "\titers: 200, epoch: 28 | loss: 0.0043777\n",
      "\tspeed: 0.0423s/iter; left time: 683.0535s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:10.45s\n",
      "Steps: 224 | Train Loss: 0.0050962 Vali Loss: 0.0083100 Test Loss: 0.0101125\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_336_24_FR_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010045873932540417, rmse:0.10022910684347153, mae:0.058125320822000504, rse:0.38668110966682434\n",
      "Intermediate time for FR and pred_len 24: 00h:05m:41.44s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_336_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_336_96_FR_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0192468\n",
      "\tspeed: 0.0735s/iter; left time: 1638.7207s\n",
      "\titers: 200, epoch: 1 | loss: 0.0168081\n",
      "\tspeed: 0.0560s/iter; left time: 1243.0840s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:13.35s\n",
      "Steps: 224 | Train Loss: 0.0214459 Vali Loss: 0.0186825 Test Loss: 0.0227955\n",
      "Validation loss decreased (inf --> 0.018682).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0140202\n",
      "\tspeed: 0.1517s/iter; left time: 3349.0851s\n",
      "\titers: 200, epoch: 2 | loss: 0.0098686\n",
      "\tspeed: 0.0633s/iter; left time: 1390.2830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:14.33s\n",
      "Steps: 224 | Train Loss: 0.0122427 Vali Loss: 0.0151879 Test Loss: 0.0189364\n",
      "Validation loss decreased (0.018682 --> 0.015188).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0105417\n",
      "\tspeed: 0.1200s/iter; left time: 2622.8552s\n",
      "\titers: 200, epoch: 3 | loss: 0.0104323\n",
      "\tspeed: 0.0569s/iter; left time: 1238.6915s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:12.65s\n",
      "Steps: 224 | Train Loss: 0.0107077 Vali Loss: 0.0143052 Test Loss: 0.0187372\n",
      "Validation loss decreased (0.015188 --> 0.014305).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0098304\n",
      "\tspeed: 0.1205s/iter; left time: 2607.0778s\n",
      "\titers: 200, epoch: 4 | loss: 0.0106838\n",
      "\tspeed: 0.0509s/iter; left time: 1096.5264s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:12.80s\n",
      "Steps: 224 | Train Loss: 0.0102742 Vali Loss: 0.0140303 Test Loss: 0.0184560\n",
      "Validation loss decreased (0.014305 --> 0.014030).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0104062\n",
      "\tspeed: 0.1245s/iter; left time: 2665.6016s\n",
      "\titers: 200, epoch: 5 | loss: 0.0088304\n",
      "\tspeed: 0.0395s/iter; left time: 841.8861s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.19s\n",
      "Steps: 224 | Train Loss: 0.0099902 Vali Loss: 0.0139883 Test Loss: 0.0185109\n",
      "Validation loss decreased (0.014030 --> 0.013988).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0112083\n",
      "\tspeed: 0.0879s/iter; left time: 1862.5477s\n",
      "\titers: 200, epoch: 6 | loss: 0.0089282\n",
      "\tspeed: 0.0497s/iter; left time: 1046.9285s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.43s\n",
      "Steps: 224 | Train Loss: 0.0097728 Vali Loss: 0.0139388 Test Loss: 0.0186560\n",
      "Validation loss decreased (0.013988 --> 0.013939).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0108884\n",
      "\tspeed: 0.1485s/iter; left time: 3112.5526s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092326\n",
      "\tspeed: 0.0659s/iter; left time: 1373.4391s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:14.58s\n",
      "Steps: 224 | Train Loss: 0.0096038 Vali Loss: 0.0141724 Test Loss: 0.0186877\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0100623\n",
      "\tspeed: 0.1268s/iter; left time: 2628.0025s\n",
      "\titers: 200, epoch: 8 | loss: 0.0087844\n",
      "\tspeed: 0.0665s/iter; left time: 1372.4295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:14.08s\n",
      "Steps: 224 | Train Loss: 0.0094615 Vali Loss: 0.0140696 Test Loss: 0.0183964\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0090472\n",
      "\tspeed: 0.1424s/iter; left time: 2921.2276s\n",
      "\titers: 200, epoch: 9 | loss: 0.0091278\n",
      "\tspeed: 0.0658s/iter; left time: 1342.5450s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:15.56s\n",
      "Steps: 224 | Train Loss: 0.0093288 Vali Loss: 0.0139739 Test Loss: 0.0187636\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0093390\n",
      "\tspeed: 0.1413s/iter; left time: 2866.3405s\n",
      "\titers: 200, epoch: 10 | loss: 0.0084434\n",
      "\tspeed: 0.0487s/iter; left time: 983.0949s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.18s\n",
      "Steps: 224 | Train Loss: 0.0091933 Vali Loss: 0.0140088 Test Loss: 0.0184228\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0094005\n",
      "\tspeed: 0.1756s/iter; left time: 3522.1266s\n",
      "\titers: 200, epoch: 11 | loss: 0.0085185\n",
      "\tspeed: 0.0541s/iter; left time: 1079.1116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.06s\n",
      "Steps: 224 | Train Loss: 0.0090669 Vali Loss: 0.0143414 Test Loss: 0.0190096\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0090827\n",
      "\tspeed: 0.1775s/iter; left time: 3520.4966s\n",
      "\titers: 200, epoch: 12 | loss: 0.0082314\n",
      "\tspeed: 0.0644s/iter; left time: 1270.9345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:13.97s\n",
      "Steps: 224 | Train Loss: 0.0089487 Vali Loss: 0.0141372 Test Loss: 0.0194590\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0093521\n",
      "\tspeed: 0.1630s/iter; left time: 3197.0117s\n",
      "\titers: 200, epoch: 13 | loss: 0.0087221\n",
      "\tspeed: 0.0472s/iter; left time: 921.5198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:12.90s\n",
      "Steps: 224 | Train Loss: 0.0088474 Vali Loss: 0.0141513 Test Loss: 0.0195888\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0090534\n",
      "\tspeed: 0.1495s/iter; left time: 2899.1199s\n",
      "\titers: 200, epoch: 14 | loss: 0.0085698\n",
      "\tspeed: 0.0510s/iter; left time: 983.7746s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:13.84s\n",
      "Steps: 224 | Train Loss: 0.0087251 Vali Loss: 0.0144129 Test Loss: 0.0194788\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0078854\n",
      "\tspeed: 0.1578s/iter; left time: 3024.7998s\n",
      "\titers: 200, epoch: 15 | loss: 0.0094088\n",
      "\tspeed: 0.0638s/iter; left time: 1215.7399s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:15.26s\n",
      "Steps: 224 | Train Loss: 0.0086403 Vali Loss: 0.0144602 Test Loss: 0.0199399\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0091333\n",
      "\tspeed: 0.1343s/iter; left time: 2544.7052s\n",
      "\titers: 200, epoch: 16 | loss: 0.0093283\n",
      "\tspeed: 0.0640s/iter; left time: 1206.0336s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:12.92s\n",
      "Steps: 224 | Train Loss: 0.0085441 Vali Loss: 0.0146382 Test Loss: 0.0203037\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_336_96_FR_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01865600049495697, rmse:0.1365869641304016, mae:0.0842278003692627, rse:0.5283547043800354\n",
      "Intermediate time for FR and pred_len 96: 00h:05m:25.57s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_336_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_336_168_FR_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0217671\n",
      "\tspeed: 0.0760s/iter; left time: 1687.2711s\n",
      "\titers: 200, epoch: 1 | loss: 0.0174734\n",
      "\tspeed: 0.0483s/iter; left time: 1066.8366s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.88s\n",
      "Steps: 223 | Train Loss: 0.0221774 Vali Loss: 0.0196754 Test Loss: 0.0230590\n",
      "Validation loss decreased (inf --> 0.019675).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0151478\n",
      "\tspeed: 0.1619s/iter; left time: 3558.6095s\n",
      "\titers: 200, epoch: 2 | loss: 0.0122771\n",
      "\tspeed: 0.0515s/iter; left time: 1126.7788s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0134631 Vali Loss: 0.0164465 Test Loss: 0.0201141\n",
      "Validation loss decreased (0.019675 --> 0.016447).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0134099\n",
      "\tspeed: 0.1432s/iter; left time: 3114.7897s\n",
      "\titers: 200, epoch: 3 | loss: 0.0106679\n",
      "\tspeed: 0.0515s/iter; left time: 1114.2820s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:12.70s\n",
      "Steps: 223 | Train Loss: 0.0118243 Vali Loss: 0.0154997 Test Loss: 0.0201013\n",
      "Validation loss decreased (0.016447 --> 0.015500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110507\n",
      "\tspeed: 0.1313s/iter; left time: 2826.7481s\n",
      "\titers: 200, epoch: 4 | loss: 0.0124329\n",
      "\tspeed: 0.0604s/iter; left time: 1294.3673s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:13.61s\n",
      "Steps: 223 | Train Loss: 0.0113176 Vali Loss: 0.0154131 Test Loss: 0.0202327\n",
      "Validation loss decreased (0.015500 --> 0.015413).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0114767\n",
      "\tspeed: 0.1510s/iter; left time: 3217.0880s\n",
      "\titers: 200, epoch: 5 | loss: 0.0113688\n",
      "\tspeed: 0.0515s/iter; left time: 1092.2614s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.68s\n",
      "Steps: 223 | Train Loss: 0.0110176 Vali Loss: 0.0152929 Test Loss: 0.0205191\n",
      "Validation loss decreased (0.015413 --> 0.015293).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0099502\n",
      "\tspeed: 0.1506s/iter; left time: 3174.9071s\n",
      "\titers: 200, epoch: 6 | loss: 0.0114028\n",
      "\tspeed: 0.0469s/iter; left time: 984.0613s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.07s\n",
      "Steps: 223 | Train Loss: 0.0108221 Vali Loss: 0.0151385 Test Loss: 0.0205659\n",
      "Validation loss decreased (0.015293 --> 0.015139).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0116266\n",
      "\tspeed: 0.1398s/iter; left time: 2915.7130s\n",
      "\titers: 200, epoch: 7 | loss: 0.0108079\n",
      "\tspeed: 0.0568s/iter; left time: 1178.8676s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.92s\n",
      "Steps: 223 | Train Loss: 0.0106779 Vali Loss: 0.0152505 Test Loss: 0.0202467\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0092505\n",
      "\tspeed: 0.1495s/iter; left time: 3085.2713s\n",
      "\titers: 200, epoch: 8 | loss: 0.0118105\n",
      "\tspeed: 0.0542s/iter; left time: 1114.0854s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.92s\n",
      "Steps: 223 | Train Loss: 0.0105538 Vali Loss: 0.0151836 Test Loss: 0.0212055\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0112282\n",
      "\tspeed: 0.0479s/iter; left time: 977.9044s\n",
      "\titers: 200, epoch: 9 | loss: 0.0087360\n",
      "\tspeed: 0.0239s/iter; left time: 485.5004s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.69s\n",
      "Steps: 223 | Train Loss: 0.0104307 Vali Loss: 0.0151765 Test Loss: 0.0207481\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0098023\n",
      "\tspeed: 0.0868s/iter; left time: 1752.7006s\n",
      "\titers: 200, epoch: 10 | loss: 0.0097993\n",
      "\tspeed: 0.0284s/iter; left time: 570.1199s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 223 | Train Loss: 0.0103014 Vali Loss: 0.0152652 Test Loss: 0.0218221\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0115547\n",
      "\tspeed: 0.0619s/iter; left time: 1235.7968s\n",
      "\titers: 200, epoch: 11 | loss: 0.0088405\n",
      "\tspeed: 0.0471s/iter; left time: 936.0981s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:09.06s\n",
      "Steps: 223 | Train Loss: 0.0101679 Vali Loss: 0.0155769 Test Loss: 0.0223729\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0094300\n",
      "\tspeed: 0.1061s/iter; left time: 2095.8478s\n",
      "\titers: 200, epoch: 12 | loss: 0.0104352\n",
      "\tspeed: 0.0447s/iter; left time: 878.0499s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.18s\n",
      "Steps: 223 | Train Loss: 0.0100577 Vali Loss: 0.0154377 Test Loss: 0.0223074\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0102223\n",
      "\tspeed: 0.1029s/iter; left time: 2008.5873s\n",
      "\titers: 200, epoch: 13 | loss: 0.0098616\n",
      "\tspeed: 0.0493s/iter; left time: 958.5676s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.79s\n",
      "Steps: 223 | Train Loss: 0.0098909 Vali Loss: 0.0153592 Test Loss: 0.0219094\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0099257\n",
      "\tspeed: 0.0966s/iter; left time: 1864.1323s\n",
      "\titers: 200, epoch: 14 | loss: 0.0105992\n",
      "\tspeed: 0.0382s/iter; left time: 734.4563s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:09.96s\n",
      "Steps: 223 | Train Loss: 0.0097699 Vali Loss: 0.0154899 Test Loss: 0.0228119\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0090252\n",
      "\tspeed: 0.1321s/iter; left time: 2520.5384s\n",
      "\titers: 200, epoch: 15 | loss: 0.0107843\n",
      "\tspeed: 0.0378s/iter; left time: 717.9941s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:09.50s\n",
      "Steps: 223 | Train Loss: 0.0096513 Vali Loss: 0.0157378 Test Loss: 0.0234815\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0110678\n",
      "\tspeed: 0.1335s/iter; left time: 2516.6332s\n",
      "\titers: 200, epoch: 16 | loss: 0.0095977\n",
      "\tspeed: 0.0327s/iter; left time: 612.7315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:08.54s\n",
      "Steps: 223 | Train Loss: 0.0095612 Vali Loss: 0.0156056 Test Loss: 0.0231728\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_336_168_FR_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.020565837621688843, rmse:0.143407940864563, mae:0.0905294120311737, rse:0.5554324984550476\n",
      "Intermediate time for FR and pred_len 168: 00h:04m:36.80s\n",
      "Intermediate time for FR: 00h:15m:43.82s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_24_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0337719\n",
      "\tspeed: 0.0748s/iter; left time: 1667.5996s\n",
      "\titers: 200, epoch: 1 | loss: 0.0257990\n",
      "\tspeed: 0.0368s/iter; left time: 816.2001s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.75s\n",
      "Steps: 224 | Train Loss: 0.0372998 Vali Loss: 0.0199048 Test Loss: 0.0213673\n",
      "Validation loss decreased (inf --> 0.019905).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0146027\n",
      "\tspeed: 0.0806s/iter; left time: 1778.8062s\n",
      "\titers: 200, epoch: 2 | loss: 0.0115269\n",
      "\tspeed: 0.0412s/iter; left time: 906.2576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:09.11s\n",
      "Steps: 224 | Train Loss: 0.0142875 Vali Loss: 0.0105368 Test Loss: 0.0117939\n",
      "Validation loss decreased (0.019905 --> 0.010537).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0094670\n",
      "\tspeed: 0.0772s/iter; left time: 1686.9764s\n",
      "\titers: 200, epoch: 3 | loss: 0.0109080\n",
      "\tspeed: 0.0407s/iter; left time: 884.3749s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 224 | Train Loss: 0.0111489 Vali Loss: 0.0099324 Test Loss: 0.0110654\n",
      "Validation loss decreased (0.010537 --> 0.009932).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0104475\n",
      "\tspeed: 0.0932s/iter; left time: 2015.7451s\n",
      "\titers: 200, epoch: 4 | loss: 0.0092308\n",
      "\tspeed: 0.0391s/iter; left time: 840.8819s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.14s\n",
      "Steps: 224 | Train Loss: 0.0104576 Vali Loss: 0.0096689 Test Loss: 0.0106889\n",
      "Validation loss decreased (0.009932 --> 0.009669).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0101719\n",
      "\tspeed: 0.0785s/iter; left time: 1679.2972s\n",
      "\titers: 200, epoch: 5 | loss: 0.0100693\n",
      "\tspeed: 0.0479s/iter; left time: 1020.8139s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.59s\n",
      "Steps: 224 | Train Loss: 0.0100438 Vali Loss: 0.0094782 Test Loss: 0.0104772\n",
      "Validation loss decreased (0.009669 --> 0.009478).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0088217\n",
      "\tspeed: 0.0747s/iter; left time: 1581.6100s\n",
      "\titers: 200, epoch: 6 | loss: 0.0084346\n",
      "\tspeed: 0.0410s/iter; left time: 864.8501s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.46s\n",
      "Steps: 224 | Train Loss: 0.0098076 Vali Loss: 0.0093499 Test Loss: 0.0103677\n",
      "Validation loss decreased (0.009478 --> 0.009350).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0102884\n",
      "\tspeed: 0.0907s/iter; left time: 1900.0656s\n",
      "\titers: 200, epoch: 7 | loss: 0.0085527\n",
      "\tspeed: 0.0388s/iter; left time: 809.9692s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.39s\n",
      "Steps: 224 | Train Loss: 0.0096329 Vali Loss: 0.0092716 Test Loss: 0.0102219\n",
      "Validation loss decreased (0.009350 --> 0.009272).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0093430\n",
      "\tspeed: 0.0819s/iter; left time: 1698.3422s\n",
      "\titers: 200, epoch: 8 | loss: 0.0086638\n",
      "\tspeed: 0.0461s/iter; left time: 950.3896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:09.65s\n",
      "Steps: 224 | Train Loss: 0.0094812 Vali Loss: 0.0092745 Test Loss: 0.0102459\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0080648\n",
      "\tspeed: 0.0765s/iter; left time: 1569.9152s\n",
      "\titers: 200, epoch: 9 | loss: 0.0098493\n",
      "\tspeed: 0.0400s/iter; left time: 816.5297s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.70s\n",
      "Steps: 224 | Train Loss: 0.0093521 Vali Loss: 0.0092073 Test Loss: 0.0102064\n",
      "Validation loss decreased (0.009272 --> 0.009207).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0094067\n",
      "\tspeed: 0.0931s/iter; left time: 1888.5345s\n",
      "\titers: 200, epoch: 10 | loss: 0.0087228\n",
      "\tspeed: 0.0409s/iter; left time: 825.9592s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.55s\n",
      "Steps: 224 | Train Loss: 0.0092702 Vali Loss: 0.0091450 Test Loss: 0.0101353\n",
      "Validation loss decreased (0.009207 --> 0.009145).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0100116\n",
      "\tspeed: 0.0836s/iter; left time: 1677.2674s\n",
      "\titers: 200, epoch: 11 | loss: 0.0076847\n",
      "\tspeed: 0.0422s/iter; left time: 843.2451s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:09.60s\n",
      "Steps: 224 | Train Loss: 0.0092088 Vali Loss: 0.0092084 Test Loss: 0.0101961\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0085566\n",
      "\tspeed: 0.0746s/iter; left time: 1479.5862s\n",
      "\titers: 200, epoch: 12 | loss: 0.0099394\n",
      "\tspeed: 0.0425s/iter; left time: 839.3870s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:09.07s\n",
      "Steps: 224 | Train Loss: 0.0091057 Vali Loss: 0.0090851 Test Loss: 0.0100833\n",
      "Validation loss decreased (0.009145 --> 0.009085).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0082878\n",
      "\tspeed: 0.0950s/iter; left time: 1862.9662s\n",
      "\titers: 200, epoch: 13 | loss: 0.0094125\n",
      "\tspeed: 0.0318s/iter; left time: 619.6207s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 224 | Train Loss: 0.0090499 Vali Loss: 0.0091029 Test Loss: 0.0100914\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0104545\n",
      "\tspeed: 0.0967s/iter; left time: 1875.6651s\n",
      "\titers: 200, epoch: 14 | loss: 0.0091111\n",
      "\tspeed: 0.0315s/iter; left time: 607.8311s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:08.34s\n",
      "Steps: 224 | Train Loss: 0.0089776 Vali Loss: 0.0090998 Test Loss: 0.0100945\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0089378\n",
      "\tspeed: 0.0834s/iter; left time: 1598.7685s\n",
      "\titers: 200, epoch: 15 | loss: 0.0082186\n",
      "\tspeed: 0.0445s/iter; left time: 849.0974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:09.11s\n",
      "Steps: 224 | Train Loss: 0.0089379 Vali Loss: 0.0090437 Test Loss: 0.0100269\n",
      "Validation loss decreased (0.009085 --> 0.009044).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0095094\n",
      "\tspeed: 0.0925s/iter; left time: 1752.9039s\n",
      "\titers: 200, epoch: 16 | loss: 0.0091797\n",
      "\tspeed: 0.0321s/iter; left time: 604.0603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 224 | Train Loss: 0.0088961 Vali Loss: 0.0090957 Test Loss: 0.0100385\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0103442\n",
      "\tspeed: 0.1009s/iter; left time: 1887.7796s\n",
      "\titers: 200, epoch: 17 | loss: 0.0105167\n",
      "\tspeed: 0.0336s/iter; left time: 625.6839s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.78s\n",
      "Steps: 224 | Train Loss: 0.0088648 Vali Loss: 0.0090046 Test Loss: 0.0099948\n",
      "Validation loss decreased (0.009044 --> 0.009005).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0080705\n",
      "\tspeed: 0.0915s/iter; left time: 1692.4111s\n",
      "\titers: 200, epoch: 18 | loss: 0.0091674\n",
      "\tspeed: 0.0391s/iter; left time: 719.6138s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:09.69s\n",
      "Steps: 224 | Train Loss: 0.0088095 Vali Loss: 0.0089891 Test Loss: 0.0100064\n",
      "Validation loss decreased (0.009005 --> 0.008989).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0084524\n",
      "\tspeed: 0.0606s/iter; left time: 1107.5950s\n",
      "\titers: 200, epoch: 19 | loss: 0.0092970\n",
      "\tspeed: 0.0235s/iter; left time: 426.1623s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 224 | Train Loss: 0.0087701 Vali Loss: 0.0090202 Test Loss: 0.0099850\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0084774\n",
      "\tspeed: 0.0609s/iter; left time: 1099.1839s\n",
      "\titers: 200, epoch: 20 | loss: 0.0082826\n",
      "\tspeed: 0.0336s/iter; left time: 602.8586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.25s\n",
      "Steps: 224 | Train Loss: 0.0087392 Vali Loss: 0.0089616 Test Loss: 0.0099696\n",
      "Validation loss decreased (0.008989 --> 0.008962).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0093423\n",
      "\tspeed: 0.0820s/iter; left time: 1461.7483s\n",
      "\titers: 200, epoch: 21 | loss: 0.0084285\n",
      "\tspeed: 0.0325s/iter; left time: 576.5965s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0087088 Vali Loss: 0.0089967 Test Loss: 0.0099756\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0086534\n",
      "\tspeed: 0.0827s/iter; left time: 1455.2713s\n",
      "\titers: 200, epoch: 22 | loss: 0.0072769\n",
      "\tspeed: 0.0370s/iter; left time: 647.6512s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 224 | Train Loss: 0.0086855 Vali Loss: 0.0090156 Test Loss: 0.0100246\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0082621\n",
      "\tspeed: 0.0840s/iter; left time: 1460.0945s\n",
      "\titers: 200, epoch: 23 | loss: 0.0077030\n",
      "\tspeed: 0.0341s/iter; left time: 589.3892s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 224 | Train Loss: 0.0086702 Vali Loss: 0.0090501 Test Loss: 0.0100442\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0095401\n",
      "\tspeed: 0.0838s/iter; left time: 1436.8825s\n",
      "\titers: 200, epoch: 24 | loss: 0.0083708\n",
      "\tspeed: 0.0341s/iter; left time: 581.6863s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:08.11s\n",
      "Steps: 224 | Train Loss: 0.0086297 Vali Loss: 0.0089953 Test Loss: 0.0100381\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0075658\n",
      "\tspeed: 0.0879s/iter; left time: 1488.5287s\n",
      "\titers: 200, epoch: 25 | loss: 0.0090909\n",
      "\tspeed: 0.0342s/iter; left time: 574.8488s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:08.13s\n",
      "Steps: 224 | Train Loss: 0.0086283 Vali Loss: 0.0089546 Test Loss: 0.0100080\n",
      "Validation loss decreased (0.008962 --> 0.008955).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0093857\n",
      "\tspeed: 0.0850s/iter; left time: 1420.1487s\n",
      "\titers: 200, epoch: 26 | loss: 0.0090390\n",
      "\tspeed: 0.0339s/iter; left time: 563.3276s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 224 | Train Loss: 0.0086089 Vali Loss: 0.0089541 Test Loss: 0.0099845\n",
      "Validation loss decreased (0.008955 --> 0.008954).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0090468\n",
      "\tspeed: 0.0846s/iter; left time: 1394.1454s\n",
      "\titers: 200, epoch: 27 | loss: 0.0093831\n",
      "\tspeed: 0.0336s/iter; left time: 550.6140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.24s\n",
      "Steps: 224 | Train Loss: 0.0085981 Vali Loss: 0.0089757 Test Loss: 0.0099962\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0089399\n",
      "\tspeed: 0.0845s/iter; left time: 1373.9456s\n",
      "\titers: 200, epoch: 28 | loss: 0.0079743\n",
      "\tspeed: 0.0332s/iter; left time: 537.0184s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:07.99s\n",
      "Steps: 224 | Train Loss: 0.0085586 Vali Loss: 0.0089295 Test Loss: 0.0099634\n",
      "Validation loss decreased (0.008954 --> 0.008930).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0084361\n",
      "\tspeed: 0.0865s/iter; left time: 1386.4688s\n",
      "\titers: 200, epoch: 29 | loss: 0.0077757\n",
      "\tspeed: 0.0327s/iter; left time: 520.3649s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.56s\n",
      "Steps: 224 | Train Loss: 0.0085565 Vali Loss: 0.0089361 Test Loss: 0.0100059\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0087085\n",
      "\tspeed: 0.0845s/iter; left time: 1335.0557s\n",
      "\titers: 200, epoch: 30 | loss: 0.0095842\n",
      "\tspeed: 0.0333s/iter; left time: 522.8796s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.94s\n",
      "Steps: 224 | Train Loss: 0.0085303 Vali Loss: 0.0089562 Test Loss: 0.0099901\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0080835\n",
      "\tspeed: 0.0831s/iter; left time: 1295.4918s\n",
      "\titers: 200, epoch: 31 | loss: 0.0094568\n",
      "\tspeed: 0.0363s/iter; left time: 561.2539s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.55s\n",
      "Steps: 224 | Train Loss: 0.0085279 Vali Loss: 0.0089598 Test Loss: 0.0100058\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0082462\n",
      "\tspeed: 0.0837s/iter; left time: 1284.7822s\n",
      "\titers: 200, epoch: 32 | loss: 0.0096425\n",
      "\tspeed: 0.0333s/iter; left time: 508.7048s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 224 | Train Loss: 0.0085532 Vali Loss: 0.0089628 Test Loss: 0.0099933\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0080213\n",
      "\tspeed: 0.0783s/iter; left time: 1184.3373s\n",
      "\titers: 200, epoch: 33 | loss: 0.0084753\n",
      "\tspeed: 0.0342s/iter; left time: 514.7524s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:08.12s\n",
      "Steps: 224 | Train Loss: 0.0085203 Vali Loss: 0.0089441 Test Loss: 0.0099886\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0096172\n",
      "\tspeed: 0.0823s/iter; left time: 1226.4733s\n",
      "\titers: 200, epoch: 34 | loss: 0.0091738\n",
      "\tspeed: 0.0327s/iter; left time: 484.3545s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:08.23s\n",
      "Steps: 224 | Train Loss: 0.0085164 Vali Loss: 0.0089387 Test Loss: 0.0099906\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0081549\n",
      "\tspeed: 0.0832s/iter; left time: 1221.5979s\n",
      "\titers: 200, epoch: 35 | loss: 0.0083837\n",
      "\tspeed: 0.0349s/iter; left time: 509.2585s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:08.52s\n",
      "Steps: 224 | Train Loss: 0.0085096 Vali Loss: 0.0089591 Test Loss: 0.0099931\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0085839\n",
      "\tspeed: 0.0843s/iter; left time: 1219.0877s\n",
      "\titers: 200, epoch: 36 | loss: 0.0085593\n",
      "\tspeed: 0.0347s/iter; left time: 498.7204s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:08.78s\n",
      "Steps: 224 | Train Loss: 0.0085119 Vali Loss: 0.0089610 Test Loss: 0.0099651\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0082230\n",
      "\tspeed: 0.0865s/iter; left time: 1231.4378s\n",
      "\titers: 200, epoch: 37 | loss: 0.0077552\n",
      "\tspeed: 0.0340s/iter; left time: 480.2208s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:08.93s\n",
      "Steps: 224 | Train Loss: 0.0084972 Vali Loss: 0.0089316 Test Loss: 0.0099725\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0084324\n",
      "\tspeed: 0.0802s/iter; left time: 1123.7365s\n",
      "\titers: 200, epoch: 38 | loss: 0.0086269\n",
      "\tspeed: 0.0355s/iter; left time: 493.6280s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:08.49s\n",
      "Steps: 224 | Train Loss: 0.0084884 Vali Loss: 0.0089388 Test Loss: 0.0099787\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_24_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009963399730622768, rmse:0.09981682896614075, mae:0.058140531182289124, rse:0.37715864181518555\n",
      "Intermediate time for IT and pred_len 24: 00h:07m:49.35s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_96_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0371732\n",
      "\tspeed: 0.0425s/iter; left time: 948.1996s\n",
      "\titers: 200, epoch: 1 | loss: 0.0287763\n",
      "\tspeed: 0.0208s/iter; left time: 462.7846s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 224 | Train Loss: 0.0390735 Vali Loss: 0.0230477 Test Loss: 0.0248180\n",
      "Validation loss decreased (inf --> 0.023048).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0210800\n",
      "\tspeed: 0.0513s/iter; left time: 1132.7785s\n",
      "\titers: 200, epoch: 2 | loss: 0.0182079\n",
      "\tspeed: 0.0208s/iter; left time: 456.3942s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.35s\n",
      "Steps: 224 | Train Loss: 0.0211989 Vali Loss: 0.0170225 Test Loss: 0.0186545\n",
      "Validation loss decreased (0.023048 --> 0.017022).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0189358\n",
      "\tspeed: 0.0490s/iter; left time: 1070.6179s\n",
      "\titers: 200, epoch: 3 | loss: 0.0172008\n",
      "\tspeed: 0.0224s/iter; left time: 486.7154s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 224 | Train Loss: 0.0180473 Vali Loss: 0.0164983 Test Loss: 0.0179922\n",
      "Validation loss decreased (0.017022 --> 0.016498).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0170671\n",
      "\tspeed: 0.0470s/iter; left time: 1016.7012s\n",
      "\titers: 200, epoch: 4 | loss: 0.0164635\n",
      "\tspeed: 0.0209s/iter; left time: 450.8112s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.04s\n",
      "Steps: 224 | Train Loss: 0.0173269 Vali Loss: 0.0162273 Test Loss: 0.0178494\n",
      "Validation loss decreased (0.016498 --> 0.016227).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0185882\n",
      "\tspeed: 0.0486s/iter; left time: 1041.1875s\n",
      "\titers: 200, epoch: 5 | loss: 0.0157344\n",
      "\tspeed: 0.0214s/iter; left time: 456.8292s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.16s\n",
      "Steps: 224 | Train Loss: 0.0169245 Vali Loss: 0.0159996 Test Loss: 0.0178206\n",
      "Validation loss decreased (0.016227 --> 0.016000).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0168839\n",
      "\tspeed: 0.0491s/iter; left time: 1039.8157s\n",
      "\titers: 200, epoch: 6 | loss: 0.0167227\n",
      "\tspeed: 0.0238s/iter; left time: 501.6362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.41s\n",
      "Steps: 224 | Train Loss: 0.0166422 Vali Loss: 0.0158670 Test Loss: 0.0176874\n",
      "Validation loss decreased (0.016000 --> 0.015867).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0173547\n",
      "\tspeed: 0.0503s/iter; left time: 1054.3527s\n",
      "\titers: 200, epoch: 7 | loss: 0.0157185\n",
      "\tspeed: 0.0225s/iter; left time: 470.3101s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 224 | Train Loss: 0.0164024 Vali Loss: 0.0159056 Test Loss: 0.0178127\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0160923\n",
      "\tspeed: 0.0510s/iter; left time: 1057.1580s\n",
      "\titers: 200, epoch: 8 | loss: 0.0145047\n",
      "\tspeed: 0.0210s/iter; left time: 432.8760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 224 | Train Loss: 0.0161933 Vali Loss: 0.0157249 Test Loss: 0.0177374\n",
      "Validation loss decreased (0.015867 --> 0.015725).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0155673\n",
      "\tspeed: 0.0488s/iter; left time: 1000.0195s\n",
      "\titers: 200, epoch: 9 | loss: 0.0142138\n",
      "\tspeed: 0.0216s/iter; left time: 440.9310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.16s\n",
      "Steps: 224 | Train Loss: 0.0159982 Vali Loss: 0.0157612 Test Loss: 0.0176047\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0145641\n",
      "\tspeed: 0.0471s/iter; left time: 954.8946s\n",
      "\titers: 200, epoch: 10 | loss: 0.0170093\n",
      "\tspeed: 0.0212s/iter; left time: 428.3900s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 224 | Train Loss: 0.0158333 Vali Loss: 0.0156579 Test Loss: 0.0177839\n",
      "Validation loss decreased (0.015725 --> 0.015658).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0164054\n",
      "\tspeed: 0.0537s/iter; left time: 1078.2749s\n",
      "\titers: 200, epoch: 11 | loss: 0.0158935\n",
      "\tspeed: 0.0214s/iter; left time: 426.6837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.20s\n",
      "Steps: 224 | Train Loss: 0.0157068 Vali Loss: 0.0157430 Test Loss: 0.0176609\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0162831\n",
      "\tspeed: 0.0501s/iter; left time: 993.9899s\n",
      "\titers: 200, epoch: 12 | loss: 0.0157943\n",
      "\tspeed: 0.0214s/iter; left time: 421.9097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.39s\n",
      "Steps: 224 | Train Loss: 0.0155495 Vali Loss: 0.0157374 Test Loss: 0.0176193\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0154624\n",
      "\tspeed: 0.0490s/iter; left time: 960.3477s\n",
      "\titers: 200, epoch: 13 | loss: 0.0158125\n",
      "\tspeed: 0.0212s/iter; left time: 414.2353s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.10s\n",
      "Steps: 224 | Train Loss: 0.0154042 Vali Loss: 0.0157997 Test Loss: 0.0179022\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0144169\n",
      "\tspeed: 0.0494s/iter; left time: 958.7619s\n",
      "\titers: 200, epoch: 14 | loss: 0.0150392\n",
      "\tspeed: 0.0209s/iter; left time: 403.6563s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.09s\n",
      "Steps: 224 | Train Loss: 0.0152836 Vali Loss: 0.0156491 Test Loss: 0.0178490\n",
      "Validation loss decreased (0.015658 --> 0.015649).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0151338\n",
      "\tspeed: 0.0492s/iter; left time: 942.6435s\n",
      "\titers: 200, epoch: 15 | loss: 0.0155342\n",
      "\tspeed: 0.0226s/iter; left time: 430.5565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.35s\n",
      "Steps: 224 | Train Loss: 0.0152057 Vali Loss: 0.0156826 Test Loss: 0.0177711\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0172772\n",
      "\tspeed: 0.0478s/iter; left time: 906.2072s\n",
      "\titers: 200, epoch: 16 | loss: 0.0152490\n",
      "\tspeed: 0.0219s/iter; left time: 413.2578s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.18s\n",
      "Steps: 224 | Train Loss: 0.0150662 Vali Loss: 0.0157058 Test Loss: 0.0177606\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0153979\n",
      "\tspeed: 0.0515s/iter; left time: 964.5141s\n",
      "\titers: 200, epoch: 17 | loss: 0.0151868\n",
      "\tspeed: 0.0223s/iter; left time: 415.8438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:05.34s\n",
      "Steps: 224 | Train Loss: 0.0149973 Vali Loss: 0.0157220 Test Loss: 0.0178153\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0147232\n",
      "\tspeed: 0.0501s/iter; left time: 926.7080s\n",
      "\titers: 200, epoch: 18 | loss: 0.0165930\n",
      "\tspeed: 0.0214s/iter; left time: 393.0901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 224 | Train Loss: 0.0148880 Vali Loss: 0.0157485 Test Loss: 0.0178049\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0133762\n",
      "\tspeed: 0.0474s/iter; left time: 866.3181s\n",
      "\titers: 200, epoch: 19 | loss: 0.0149687\n",
      "\tspeed: 0.0217s/iter; left time: 394.4318s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.18s\n",
      "Steps: 224 | Train Loss: 0.0148254 Vali Loss: 0.0157007 Test Loss: 0.0178293\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0147325\n",
      "\tspeed: 0.0480s/iter; left time: 866.2290s\n",
      "\titers: 200, epoch: 20 | loss: 0.0156527\n",
      "\tspeed: 0.0211s/iter; left time: 379.4896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 224 | Train Loss: 0.0147292 Vali Loss: 0.0156710 Test Loss: 0.0177907\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0145474\n",
      "\tspeed: 0.0470s/iter; left time: 837.6363s\n",
      "\titers: 200, epoch: 21 | loss: 0.0122341\n",
      "\tspeed: 0.0216s/iter; left time: 383.3445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.12s\n",
      "Steps: 224 | Train Loss: 0.0146501 Vali Loss: 0.0157059 Test Loss: 0.0178243\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0147194\n",
      "\tspeed: 0.0479s/iter; left time: 843.2110s\n",
      "\titers: 200, epoch: 22 | loss: 0.0152239\n",
      "\tspeed: 0.0237s/iter; left time: 414.0171s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:05.43s\n",
      "Steps: 224 | Train Loss: 0.0146113 Vali Loss: 0.0157876 Test Loss: 0.0178172\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0141952\n",
      "\tspeed: 0.0502s/iter; left time: 871.9619s\n",
      "\titers: 200, epoch: 23 | loss: 0.0136017\n",
      "\tspeed: 0.0213s/iter; left time: 367.6173s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.18s\n",
      "Steps: 224 | Train Loss: 0.0145538 Vali Loss: 0.0157679 Test Loss: 0.0178681\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0145188\n",
      "\tspeed: 0.0495s/iter; left time: 848.8626s\n",
      "\titers: 200, epoch: 24 | loss: 0.0136532\n",
      "\tspeed: 0.0211s/iter; left time: 359.7698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 224 | Train Loss: 0.0144843 Vali Loss: 0.0157702 Test Loss: 0.0179332\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_96_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01784895919263363, rmse:0.13359999656677246, mae:0.08161764591932297, rse:0.5051559805870056\n",
      "Intermediate time for IT and pred_len 96: 00h:02m:59.38s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_168_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0369255\n",
      "\tspeed: 0.0507s/iter; left time: 1126.5050s\n",
      "\titers: 200, epoch: 1 | loss: 0.0297755\n",
      "\tspeed: 0.0213s/iter; left time: 470.9769s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.47s\n",
      "Steps: 223 | Train Loss: 0.0397611 Vali Loss: 0.0237945 Test Loss: 0.0251291\n",
      "Validation loss decreased (inf --> 0.023794).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0209399\n",
      "\tspeed: 0.0473s/iter; left time: 1038.6170s\n",
      "\titers: 200, epoch: 2 | loss: 0.0188481\n",
      "\tspeed: 0.0235s/iter; left time: 514.5731s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 223 | Train Loss: 0.0225348 Vali Loss: 0.0185233 Test Loss: 0.0195790\n",
      "Validation loss decreased (0.023794 --> 0.018523).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0206072\n",
      "\tspeed: 0.0531s/iter; left time: 1156.0818s\n",
      "\titers: 200, epoch: 3 | loss: 0.0186305\n",
      "\tspeed: 0.0253s/iter; left time: 548.5064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 223 | Train Loss: 0.0193464 Vali Loss: 0.0180038 Test Loss: 0.0191504\n",
      "Validation loss decreased (0.018523 --> 0.018004).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0191462\n",
      "\tspeed: 0.0498s/iter; left time: 1071.2509s\n",
      "\titers: 200, epoch: 4 | loss: 0.0204389\n",
      "\tspeed: 0.0212s/iter; left time: 453.2919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 223 | Train Loss: 0.0186470 Vali Loss: 0.0178889 Test Loss: 0.0193618\n",
      "Validation loss decreased (0.018004 --> 0.017889).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0174371\n",
      "\tspeed: 0.0489s/iter; left time: 1042.8023s\n",
      "\titers: 200, epoch: 5 | loss: 0.0175992\n",
      "\tspeed: 0.0217s/iter; left time: 459.9728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.20s\n",
      "Steps: 223 | Train Loss: 0.0182212 Vali Loss: 0.0175770 Test Loss: 0.0193352\n",
      "Validation loss decreased (0.017889 --> 0.017577).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0169263\n",
      "\tspeed: 0.0493s/iter; left time: 1040.2883s\n",
      "\titers: 200, epoch: 6 | loss: 0.0189777\n",
      "\tspeed: 0.0212s/iter; left time: 445.3449s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.18s\n",
      "Steps: 223 | Train Loss: 0.0178814 Vali Loss: 0.0174025 Test Loss: 0.0192189\n",
      "Validation loss decreased (0.017577 --> 0.017402).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0178873\n",
      "\tspeed: 0.0535s/iter; left time: 1116.6465s\n",
      "\titers: 200, epoch: 7 | loss: 0.0182649\n",
      "\tspeed: 0.0214s/iter; left time: 444.1022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 223 | Train Loss: 0.0176283 Vali Loss: 0.0174294 Test Loss: 0.0193924\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0176257\n",
      "\tspeed: 0.0517s/iter; left time: 1067.7889s\n",
      "\titers: 200, epoch: 8 | loss: 0.0169084\n",
      "\tspeed: 0.0236s/iter; left time: 484.9721s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.68s\n",
      "Steps: 223 | Train Loss: 0.0173846 Vali Loss: 0.0173592 Test Loss: 0.0194279\n",
      "Validation loss decreased (0.017402 --> 0.017359).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0171479\n",
      "\tspeed: 0.0499s/iter; left time: 1018.8378s\n",
      "\titers: 200, epoch: 9 | loss: 0.0163122\n",
      "\tspeed: 0.0213s/iter; left time: 433.0551s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.20s\n",
      "Steps: 223 | Train Loss: 0.0171772 Vali Loss: 0.0172226 Test Loss: 0.0193564\n",
      "Validation loss decreased (0.017359 --> 0.017223).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0160433\n",
      "\tspeed: 0.0534s/iter; left time: 1078.1602s\n",
      "\titers: 200, epoch: 10 | loss: 0.0175120\n",
      "\tspeed: 0.0233s/iter; left time: 469.1493s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 223 | Train Loss: 0.0169679 Vali Loss: 0.0173008 Test Loss: 0.0195370\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0176645\n",
      "\tspeed: 0.0538s/iter; left time: 1074.1828s\n",
      "\titers: 200, epoch: 11 | loss: 0.0154894\n",
      "\tspeed: 0.0289s/iter; left time: 573.4385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.12s\n",
      "Steps: 223 | Train Loss: 0.0167844 Vali Loss: 0.0172710 Test Loss: 0.0197374\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0156374\n",
      "\tspeed: 0.0547s/iter; left time: 1079.7601s\n",
      "\titers: 200, epoch: 12 | loss: 0.0162350\n",
      "\tspeed: 0.0235s/iter; left time: 462.2320s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.61s\n",
      "Steps: 223 | Train Loss: 0.0165962 Vali Loss: 0.0172021 Test Loss: 0.0195468\n",
      "Validation loss decreased (0.017223 --> 0.017202).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0160061\n",
      "\tspeed: 0.0610s/iter; left time: 1190.6081s\n",
      "\titers: 200, epoch: 13 | loss: 0.0165230\n",
      "\tspeed: 0.0243s/iter; left time: 471.3065s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.47s\n",
      "Steps: 223 | Train Loss: 0.0164414 Vali Loss: 0.0171890 Test Loss: 0.0197550\n",
      "Validation loss decreased (0.017202 --> 0.017189).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0159156\n",
      "\tspeed: 0.0548s/iter; left time: 1057.5153s\n",
      "\titers: 200, epoch: 14 | loss: 0.0163138\n",
      "\tspeed: 0.0287s/iter; left time: 550.2985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.47s\n",
      "Steps: 223 | Train Loss: 0.0162846 Vali Loss: 0.0172921 Test Loss: 0.0197784\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0160599\n",
      "\tspeed: 0.0602s/iter; left time: 1148.8767s\n",
      "\titers: 200, epoch: 15 | loss: 0.0180776\n",
      "\tspeed: 0.0250s/iter; left time: 474.6625s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 223 | Train Loss: 0.0161662 Vali Loss: 0.0172911 Test Loss: 0.0196204\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0166016\n",
      "\tspeed: 0.0722s/iter; left time: 1361.6496s\n",
      "\titers: 200, epoch: 16 | loss: 0.0168872\n",
      "\tspeed: 0.0244s/iter; left time: 458.4343s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.00s\n",
      "Steps: 223 | Train Loss: 0.0160063 Vali Loss: 0.0173133 Test Loss: 0.0197688\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0163692\n",
      "\tspeed: 0.0502s/iter; left time: 935.2129s\n",
      "\titers: 200, epoch: 17 | loss: 0.0158019\n",
      "\tspeed: 0.0263s/iter; left time: 488.0951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 223 | Train Loss: 0.0159090 Vali Loss: 0.0172856 Test Loss: 0.0198253\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0144021\n",
      "\tspeed: 0.0576s/iter; left time: 1060.1217s\n",
      "\titers: 200, epoch: 18 | loss: 0.0162366\n",
      "\tspeed: 0.0235s/iter; left time: 429.8685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.57s\n",
      "Steps: 223 | Train Loss: 0.0158164 Vali Loss: 0.0173659 Test Loss: 0.0198755\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0145723\n",
      "\tspeed: 0.0532s/iter; left time: 968.1630s\n",
      "\titers: 200, epoch: 19 | loss: 0.0150361\n",
      "\tspeed: 0.0277s/iter; left time: 501.7882s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 223 | Train Loss: 0.0157038 Vali Loss: 0.0173463 Test Loss: 0.0198465\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0164767\n",
      "\tspeed: 0.0564s/iter; left time: 1013.8309s\n",
      "\titers: 200, epoch: 20 | loss: 0.0145590\n",
      "\tspeed: 0.0258s/iter; left time: 460.2691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.02s\n",
      "Steps: 223 | Train Loss: 0.0156102 Vali Loss: 0.0173839 Test Loss: 0.0200144\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0158374\n",
      "\tspeed: 0.0604s/iter; left time: 1072.4078s\n",
      "\titers: 200, epoch: 21 | loss: 0.0149751\n",
      "\tspeed: 0.0270s/iter; left time: 475.7056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.07s\n",
      "Steps: 223 | Train Loss: 0.0155649 Vali Loss: 0.0173720 Test Loss: 0.0198600\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0163231\n",
      "\tspeed: 0.0707s/iter; left time: 1239.2860s\n",
      "\titers: 200, epoch: 22 | loss: 0.0162265\n",
      "\tspeed: 0.0241s/iter; left time: 420.4306s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.25s\n",
      "Steps: 223 | Train Loss: 0.0154726 Vali Loss: 0.0174812 Test Loss: 0.0200674\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0139256\n",
      "\tspeed: 0.0547s/iter; left time: 945.5040s\n",
      "\titers: 200, epoch: 23 | loss: 0.0155654\n",
      "\tspeed: 0.0296s/iter; left time: 508.8566s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 223 | Train Loss: 0.0154203 Vali Loss: 0.0175127 Test Loss: 0.0199061\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_168_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019755041226744652, rmse:0.14055262506008148, mae:0.08749067783355713, rse:0.5319384932518005\n",
      "Intermediate time for IT and pred_len 168: 00h:03m:14.98s\n",
      "Intermediate time for IT: 00h:14m:03.71s\n",
      "Total time: 01h:39m:19.06s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PatchTST/42</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.0911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.1889</td>\n",
       "      <td>0.1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>0.1511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.0879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.0948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1002</td>\n",
       "      <td>0.0581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>0.0905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.0581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.0816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            PatchTST/42                \n",
       "Metrics                  MSE    RMSE     MAE\n",
       "Country Pred_len                            \n",
       "DE      24            0.0208  0.1443  0.0911\n",
       "        96            0.0357  0.1889  0.1291\n",
       "        168           0.0383  0.1958  0.1366\n",
       "GB      24            0.0251  0.1584  0.1036\n",
       "        96            0.0423  0.2057  0.1437\n",
       "        168           0.0454  0.2132  0.1511\n",
       "ES      24            0.0097  0.0987  0.0607\n",
       "        96            0.0182  0.1349  0.0879\n",
       "        168           0.0202  0.1421  0.0948\n",
       "FR      24            0.0100  0.1002  0.0581\n",
       "        96            0.0187  0.1366  0.0842\n",
       "        168           0.0206  0.1434  0.0905\n",
       "IT      24            0.0100  0.0998  0.0581\n",
       "        96            0.0178  0.1336  0.0816\n",
       "        168           0.0198  0.1406  0.0875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False, itr=1)\n",
    "patchtst_df.drop(columns=['Iteration'], inplace=True)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['PatchTST/42'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_128.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PatchTST 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 512\n",
    "model = \"PatchTST\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_512.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128 \n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_512_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_512_24_DE_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 1 | loss: 0.0320419\n",
      "\tspeed: 0.0844s/iter; left time: 1874.0395s\n",
      "\titers: 200, epoch: 1 | loss: 0.0287837\n",
      "\tspeed: 0.0607s/iter; left time: 1341.2552s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:14.64s\n",
      "Steps: 223 | Train Loss: 0.0330901 Vali Loss: 0.0307639 Test Loss: 0.0346829\n",
      "Validation loss decreased (inf --> 0.030764).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0162435\n",
      "\tspeed: 0.1313s/iter; left time: 2886.3432s\n",
      "\titers: 200, epoch: 2 | loss: 0.0146976\n",
      "\tspeed: 0.0655s/iter; left time: 1432.3063s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:14.81s\n",
      "Steps: 223 | Train Loss: 0.0167506 Vali Loss: 0.0205728 Test Loss: 0.0221135\n",
      "Validation loss decreased (0.030764 --> 0.020573).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0122273\n",
      "\tspeed: 0.1317s/iter; left time: 2864.9778s\n",
      "\titers: 200, epoch: 3 | loss: 0.0134441\n",
      "\tspeed: 0.0658s/iter; left time: 1425.3053s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:14.11s\n",
      "Steps: 223 | Train Loss: 0.0142496 Vali Loss: 0.0195842 Test Loss: 0.0213711\n",
      "Validation loss decreased (0.020573 --> 0.019584).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0147640\n",
      "\tspeed: 0.1271s/iter; left time: 2736.1219s\n",
      "\titers: 200, epoch: 4 | loss: 0.0156996\n",
      "\tspeed: 0.0677s/iter; left time: 1450.6764s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:14.54s\n",
      "Steps: 223 | Train Loss: 0.0137508 Vali Loss: 0.0193296 Test Loss: 0.0211547\n",
      "Validation loss decreased (0.019584 --> 0.019330).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0132802\n",
      "\tspeed: 0.1247s/iter; left time: 2657.8179s\n",
      "\titers: 200, epoch: 5 | loss: 0.0119247\n",
      "\tspeed: 0.0692s/iter; left time: 1467.8194s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:14.75s\n",
      "Steps: 223 | Train Loss: 0.0134396 Vali Loss: 0.0194362 Test Loss: 0.0211613\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0149655\n",
      "\tspeed: 0.1171s/iter; left time: 2468.2815s\n",
      "\titers: 200, epoch: 6 | loss: 0.0150701\n",
      "\tspeed: 0.0710s/iter; left time: 1489.1346s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.74s\n",
      "Steps: 223 | Train Loss: 0.0132024 Vali Loss: 0.0192760 Test Loss: 0.0208219\n",
      "Validation loss decreased (0.019330 --> 0.019276).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0135355\n",
      "\tspeed: 0.1327s/iter; left time: 2767.6392s\n",
      "\titers: 200, epoch: 7 | loss: 0.0125636\n",
      "\tspeed: 0.0636s/iter; left time: 1320.2912s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:15.46s\n",
      "Steps: 223 | Train Loss: 0.0130524 Vali Loss: 0.0191781 Test Loss: 0.0209003\n",
      "Validation loss decreased (0.019276 --> 0.019178).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0122181\n",
      "\tspeed: 0.1489s/iter; left time: 3073.9915s\n",
      "\titers: 200, epoch: 8 | loss: 0.0130723\n",
      "\tspeed: 0.0791s/iter; left time: 1624.0692s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:18.06s\n",
      "Steps: 223 | Train Loss: 0.0128939 Vali Loss: 0.0190000 Test Loss: 0.0207389\n",
      "Validation loss decreased (0.019178 --> 0.019000).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0128966\n",
      "\tspeed: 0.1614s/iter; left time: 3295.7245s\n",
      "\titers: 200, epoch: 9 | loss: 0.0127980\n",
      "\tspeed: 0.0757s/iter; left time: 1537.5451s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:17.95s\n",
      "Steps: 223 | Train Loss: 0.0127719 Vali Loss: 0.0190203 Test Loss: 0.0208991\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0131356\n",
      "\tspeed: 0.1685s/iter; left time: 3401.8797s\n",
      "\titers: 200, epoch: 10 | loss: 0.0110794\n",
      "\tspeed: 0.0904s/iter; left time: 1817.2962s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:18.78s\n",
      "Steps: 223 | Train Loss: 0.0126830 Vali Loss: 0.0190369 Test Loss: 0.0209479\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0131971\n",
      "\tspeed: 0.1704s/iter; left time: 3402.4684s\n",
      "\titers: 200, epoch: 11 | loss: 0.0134337\n",
      "\tspeed: 0.0770s/iter; left time: 1529.1478s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:17.80s\n",
      "Steps: 223 | Train Loss: 0.0125695 Vali Loss: 0.0188517 Test Loss: 0.0208134\n",
      "Validation loss decreased (0.019000 --> 0.018852).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0118745\n",
      "\tspeed: 0.1625s/iter; left time: 3208.5745s\n",
      "\titers: 200, epoch: 12 | loss: 0.0137513\n",
      "\tspeed: 0.0819s/iter; left time: 1608.4980s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:17.78s\n",
      "Steps: 223 | Train Loss: 0.0125269 Vali Loss: 0.0187919 Test Loss: 0.0207906\n",
      "Validation loss decreased (0.018852 --> 0.018792).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0110322\n",
      "\tspeed: 0.1533s/iter; left time: 2993.1366s\n",
      "\titers: 200, epoch: 13 | loss: 0.0113198\n",
      "\tspeed: 0.0903s/iter; left time: 1753.8848s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:19.46s\n",
      "Steps: 223 | Train Loss: 0.0124106 Vali Loss: 0.0188365 Test Loss: 0.0206856\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0121888\n",
      "\tspeed: 0.1721s/iter; left time: 3322.2613s\n",
      "\titers: 200, epoch: 14 | loss: 0.0106991\n",
      "\tspeed: 0.0809s/iter; left time: 1553.6162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:18.14s\n",
      "Steps: 223 | Train Loss: 0.0123698 Vali Loss: 0.0187128 Test Loss: 0.0206816\n",
      "Validation loss decreased (0.018792 --> 0.018713).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0123657\n",
      "\tspeed: 0.1200s/iter; left time: 2289.9006s\n",
      "\titers: 200, epoch: 15 | loss: 0.0110482\n",
      "\tspeed: 0.0769s/iter; left time: 1460.1288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:15.48s\n",
      "Steps: 223 | Train Loss: 0.0123002 Vali Loss: 0.0188360 Test Loss: 0.0209916\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0123741\n",
      "\tspeed: 0.1710s/iter; left time: 3223.8392s\n",
      "\titers: 200, epoch: 16 | loss: 0.0140685\n",
      "\tspeed: 0.0939s/iter; left time: 1761.6076s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:20.08s\n",
      "Steps: 223 | Train Loss: 0.0122463 Vali Loss: 0.0187456 Test Loss: 0.0208574\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0118192\n",
      "\tspeed: 0.1630s/iter; left time: 3036.8727s\n",
      "\titers: 200, epoch: 17 | loss: 0.0139025\n",
      "\tspeed: 0.0755s/iter; left time: 1399.9155s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:17.84s\n",
      "Steps: 223 | Train Loss: 0.0122077 Vali Loss: 0.0188547 Test Loss: 0.0209965\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0114362\n",
      "\tspeed: 0.1627s/iter; left time: 2994.6279s\n",
      "\titers: 200, epoch: 18 | loss: 0.0104297\n",
      "\tspeed: 0.0806s/iter; left time: 1476.5260s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:18.27s\n",
      "Steps: 223 | Train Loss: 0.0121547 Vali Loss: 0.0187872 Test Loss: 0.0209580\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0124758\n",
      "\tspeed: 0.1792s/iter; left time: 3258.4104s\n",
      "\titers: 200, epoch: 19 | loss: 0.0147131\n",
      "\tspeed: 0.0899s/iter; left time: 1626.8919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:20.27s\n",
      "Steps: 223 | Train Loss: 0.0121514 Vali Loss: 0.0187204 Test Loss: 0.0209600\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0129257\n",
      "\tspeed: 0.1618s/iter; left time: 2906.0369s\n",
      "\titers: 200, epoch: 20 | loss: 0.0120061\n",
      "\tspeed: 0.0801s/iter; left time: 1430.6822s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:18.13s\n",
      "Steps: 223 | Train Loss: 0.0121002 Vali Loss: 0.0187933 Test Loss: 0.0209985\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0123232\n",
      "\tspeed: 0.1634s/iter; left time: 2898.3346s\n",
      "\titers: 200, epoch: 21 | loss: 0.0129772\n",
      "\tspeed: 0.0786s/iter; left time: 1387.1175s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:17.98s\n",
      "Steps: 223 | Train Loss: 0.0120827 Vali Loss: 0.0186429 Test Loss: 0.0209248\n",
      "Validation loss decreased (0.018713 --> 0.018643).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0126727\n",
      "\tspeed: 0.1856s/iter; left time: 3251.9179s\n",
      "\titers: 200, epoch: 22 | loss: 0.0121711\n",
      "\tspeed: 0.0781s/iter; left time: 1360.0215s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:18.82s\n",
      "Steps: 223 | Train Loss: 0.0120302 Vali Loss: 0.0186829 Test Loss: 0.0209834\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0123254\n",
      "\tspeed: 0.1596s/iter; left time: 2760.6722s\n",
      "\titers: 200, epoch: 23 | loss: 0.0118463\n",
      "\tspeed: 0.0799s/iter; left time: 1374.4799s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:17.59s\n",
      "Steps: 223 | Train Loss: 0.0120161 Vali Loss: 0.0187301 Test Loss: 0.0209487\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0130349\n",
      "\tspeed: 0.1611s/iter; left time: 2749.6830s\n",
      "\titers: 200, epoch: 24 | loss: 0.0112776\n",
      "\tspeed: 0.0945s/iter; left time: 1604.4927s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:19.55s\n",
      "Steps: 223 | Train Loss: 0.0119957 Vali Loss: 0.0187657 Test Loss: 0.0211017\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0118786\n",
      "\tspeed: 0.1801s/iter; left time: 3035.0522s\n",
      "\titers: 200, epoch: 25 | loss: 0.0106549\n",
      "\tspeed: 0.0817s/iter; left time: 1368.3043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:18.54s\n",
      "Steps: 223 | Train Loss: 0.0119781 Vali Loss: 0.0187408 Test Loss: 0.0210000\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0114627\n",
      "\tspeed: 0.1705s/iter; left time: 2835.3620s\n",
      "\titers: 200, epoch: 26 | loss: 0.0092416\n",
      "\tspeed: 0.0853s/iter; left time: 1409.9270s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:19.10s\n",
      "Steps: 223 | Train Loss: 0.0119641 Vali Loss: 0.0187895 Test Loss: 0.0210359\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0119440\n",
      "\tspeed: 0.1807s/iter; left time: 2964.1547s\n",
      "\titers: 200, epoch: 27 | loss: 0.0123574\n",
      "\tspeed: 0.0803s/iter; left time: 1309.7394s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:19.12s\n",
      "Steps: 223 | Train Loss: 0.0119533 Vali Loss: 0.0187410 Test Loss: 0.0210168\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0125613\n",
      "\tspeed: 0.1528s/iter; left time: 2471.9507s\n",
      "\titers: 200, epoch: 28 | loss: 0.0118585\n",
      "\tspeed: 0.0791s/iter; left time: 1271.4770s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:17.84s\n",
      "Steps: 223 | Train Loss: 0.0119295 Vali Loss: 0.0186849 Test Loss: 0.0209920\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0127008\n",
      "\tspeed: 0.1569s/iter; left time: 2503.0412s\n",
      "\titers: 200, epoch: 29 | loss: 0.0122770\n",
      "\tspeed: 0.0789s/iter; left time: 1250.6627s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:17.94s\n",
      "Steps: 223 | Train Loss: 0.0119278 Vali Loss: 0.0187045 Test Loss: 0.0210623\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0130427\n",
      "\tspeed: 0.1836s/iter; left time: 2889.5366s\n",
      "\titers: 200, epoch: 30 | loss: 0.0106693\n",
      "\tspeed: 0.0747s/iter; left time: 1168.0823s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:17.51s\n",
      "Steps: 223 | Train Loss: 0.0118964 Vali Loss: 0.0187110 Test Loss: 0.0210196\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0131365\n",
      "\tspeed: 0.1146s/iter; left time: 1778.2675s\n",
      "\titers: 200, epoch: 31 | loss: 0.0113876\n",
      "\tspeed: 0.0649s/iter; left time: 1000.0116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:14.26s\n",
      "Steps: 223 | Train Loss: 0.0118988 Vali Loss: 0.0187085 Test Loss: 0.0209946\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_512_24_DE_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.020924773067235947, rmse:0.14465397596359253, mae:0.09188058972358704, rse:0.510503888130188\n",
      "Intermediate time for DE and pred_len 24: 00h:12m:10.17s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_512_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0340526\n",
      "\tspeed: 0.0845s/iter; left time: 1868.2784s\n",
      "\titers: 200, epoch: 1 | loss: 0.0299916\n",
      "\tspeed: 0.0634s/iter; left time: 1394.4162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:14.93s\n",
      "Steps: 222 | Train Loss: 0.0366279 Vali Loss: 0.0357173 Test Loss: 0.0408750\n",
      "Validation loss decreased (inf --> 0.035717).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0257265\n",
      "\tspeed: 0.1339s/iter; left time: 2930.0571s\n",
      "\titers: 200, epoch: 2 | loss: 0.0250982\n",
      "\tspeed: 0.0619s/iter; left time: 1349.0700s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:14.38s\n",
      "Steps: 222 | Train Loss: 0.0252882 Vali Loss: 0.0306209 Test Loss: 0.0352044\n",
      "Validation loss decreased (0.035717 --> 0.030621).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0219128\n",
      "\tspeed: 0.1411s/iter; left time: 3055.0210s\n",
      "\titers: 200, epoch: 3 | loss: 0.0236439\n",
      "\tspeed: 0.0624s/iter; left time: 1344.9635s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:14.52s\n",
      "Steps: 222 | Train Loss: 0.0231006 Vali Loss: 0.0298894 Test Loss: 0.0345113\n",
      "Validation loss decreased (0.030621 --> 0.029889).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0207217\n",
      "\tspeed: 0.1351s/iter; left time: 2896.1018s\n",
      "\titers: 200, epoch: 4 | loss: 0.0237567\n",
      "\tspeed: 0.0606s/iter; left time: 1293.8464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:14.01s\n",
      "Steps: 222 | Train Loss: 0.0224882 Vali Loss: 0.0297643 Test Loss: 0.0349329\n",
      "Validation loss decreased (0.029889 --> 0.029764).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0241547\n",
      "\tspeed: 0.1557s/iter; left time: 3302.3420s\n",
      "\titers: 200, epoch: 5 | loss: 0.0216407\n",
      "\tspeed: 0.0591s/iter; left time: 1248.0740s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:14.13s\n",
      "Steps: 222 | Train Loss: 0.0221023 Vali Loss: 0.0293910 Test Loss: 0.0346567\n",
      "Validation loss decreased (0.029764 --> 0.029391).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0211580\n",
      "\tspeed: 0.1455s/iter; left time: 3053.5127s\n",
      "\titers: 200, epoch: 6 | loss: 0.0210696\n",
      "\tspeed: 0.0647s/iter; left time: 1351.9379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:14.41s\n",
      "Steps: 222 | Train Loss: 0.0217631 Vali Loss: 0.0295415 Test Loss: 0.0346689\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0209948\n",
      "\tspeed: 0.1415s/iter; left time: 2937.9443s\n",
      "\titers: 200, epoch: 7 | loss: 0.0228223\n",
      "\tspeed: 0.0606s/iter; left time: 1251.7211s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:13.79s\n",
      "Steps: 222 | Train Loss: 0.0215085 Vali Loss: 0.0293108 Test Loss: 0.0346146\n",
      "Validation loss decreased (0.029391 --> 0.029311).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0205567\n",
      "\tspeed: 0.1390s/iter; left time: 2856.5468s\n",
      "\titers: 200, epoch: 8 | loss: 0.0209374\n",
      "\tspeed: 0.0656s/iter; left time: 1341.8847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:14.19s\n",
      "Steps: 222 | Train Loss: 0.0213028 Vali Loss: 0.0295053 Test Loss: 0.0349664\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0204747\n",
      "\tspeed: 0.1412s/iter; left time: 2870.7318s\n",
      "\titers: 200, epoch: 9 | loss: 0.0186287\n",
      "\tspeed: 0.0635s/iter; left time: 1283.9209s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:14.03s\n",
      "Steps: 222 | Train Loss: 0.0210937 Vali Loss: 0.0296453 Test Loss: 0.0349096\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0212807\n",
      "\tspeed: 0.1346s/iter; left time: 2705.8379s\n",
      "\titers: 200, epoch: 10 | loss: 0.0228144\n",
      "\tspeed: 0.0529s/iter; left time: 1058.7982s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.60s\n",
      "Steps: 222 | Train Loss: 0.0209168 Vali Loss: 0.0297365 Test Loss: 0.0352872\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209331\n",
      "\tspeed: 0.1301s/iter; left time: 2587.3642s\n",
      "\titers: 200, epoch: 11 | loss: 0.0203615\n",
      "\tspeed: 0.0579s/iter; left time: 1146.0010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:14.66s\n",
      "Steps: 222 | Train Loss: 0.0207532 Vali Loss: 0.0296046 Test Loss: 0.0348628\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0202113\n",
      "\tspeed: 0.1521s/iter; left time: 2990.1375s\n",
      "\titers: 200, epoch: 12 | loss: 0.0181712\n",
      "\tspeed: 0.0606s/iter; left time: 1185.6618s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:14.59s\n",
      "Steps: 222 | Train Loss: 0.0206245 Vali Loss: 0.0300450 Test Loss: 0.0357422\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0188153\n",
      "\tspeed: 0.1541s/iter; left time: 2994.4092s\n",
      "\titers: 200, epoch: 13 | loss: 0.0192243\n",
      "\tspeed: 0.0594s/iter; left time: 1148.9639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:14.73s\n",
      "Steps: 222 | Train Loss: 0.0205002 Vali Loss: 0.0300774 Test Loss: 0.0357034\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0191601\n",
      "\tspeed: 0.1598s/iter; left time: 3071.2082s\n",
      "\titers: 200, epoch: 14 | loss: 0.0200292\n",
      "\tspeed: 0.0650s/iter; left time: 1243.0839s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:15.95s\n",
      "Steps: 222 | Train Loss: 0.0203286 Vali Loss: 0.0300384 Test Loss: 0.0358304\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0213947\n",
      "\tspeed: 0.1568s/iter; left time: 2978.4136s\n",
      "\titers: 200, epoch: 15 | loss: 0.0216762\n",
      "\tspeed: 0.0634s/iter; left time: 1198.2627s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:14.95s\n",
      "Steps: 222 | Train Loss: 0.0202605 Vali Loss: 0.0299154 Test Loss: 0.0358709\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0218316\n",
      "\tspeed: 0.1578s/iter; left time: 2962.0167s\n",
      "\titers: 200, epoch: 16 | loss: 0.0165395\n",
      "\tspeed: 0.0609s/iter; left time: 1136.9393s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:14.71s\n",
      "Steps: 222 | Train Loss: 0.0201360 Vali Loss: 0.0302061 Test Loss: 0.0358872\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0203842\n",
      "\tspeed: 0.1142s/iter; left time: 2118.8123s\n",
      "\titers: 200, epoch: 17 | loss: 0.0196210\n",
      "\tspeed: 0.0599s/iter; left time: 1104.3143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:14.45s\n",
      "Steps: 222 | Train Loss: 0.0200276 Vali Loss: 0.0301952 Test Loss: 0.0357881\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.034614626318216324, rmse:0.18605005741119385, mae:0.12817755341529846, rse:0.6588408946990967\n",
      "Intermediate time for DE and pred_len 96: 00h:05m:59.21s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_512_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0347508\n",
      "\tspeed: 0.0912s/iter; left time: 2016.0800s\n",
      "\titers: 200, epoch: 1 | loss: 0.0319016\n",
      "\tspeed: 0.0732s/iter; left time: 1611.5407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:16.42s\n",
      "Steps: 222 | Train Loss: 0.0376023 Vali Loss: 0.0364655 Test Loss: 0.0421210\n",
      "Validation loss decreased (inf --> 0.036465).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0248929\n",
      "\tspeed: 0.1447s/iter; left time: 3166.6810s\n",
      "\titers: 200, epoch: 2 | loss: 0.0261527\n",
      "\tspeed: 0.0642s/iter; left time: 1397.7261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:15.58s\n",
      "Steps: 222 | Train Loss: 0.0271947 Vali Loss: 0.0318771 Test Loss: 0.0369582\n",
      "Validation loss decreased (0.036465 --> 0.031877).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0264144\n",
      "\tspeed: 0.1428s/iter; left time: 3092.9601s\n",
      "\titers: 200, epoch: 3 | loss: 0.0258447\n",
      "\tspeed: 0.0593s/iter; left time: 1277.4666s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:13.20s\n",
      "Steps: 222 | Train Loss: 0.0250981 Vali Loss: 0.0316436 Test Loss: 0.0370929\n",
      "Validation loss decreased (0.031877 --> 0.031644).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0259978\n",
      "\tspeed: 0.1687s/iter; left time: 3615.8621s\n",
      "\titers: 200, epoch: 4 | loss: 0.0234923\n",
      "\tspeed: 0.0746s/iter; left time: 1592.3521s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:15.18s\n",
      "Steps: 222 | Train Loss: 0.0245029 Vali Loss: 0.0311583 Test Loss: 0.0370108\n",
      "Validation loss decreased (0.031644 --> 0.031158).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0226559\n",
      "\tspeed: 0.1475s/iter; left time: 3128.6500s\n",
      "\titers: 200, epoch: 5 | loss: 0.0258947\n",
      "\tspeed: 0.0686s/iter; left time: 1448.6639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:15.88s\n",
      "Steps: 222 | Train Loss: 0.0240243 Vali Loss: 0.0310695 Test Loss: 0.0371345\n",
      "Validation loss decreased (0.031158 --> 0.031070).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0230538\n",
      "\tspeed: 0.1484s/iter; left time: 3115.9290s\n",
      "\titers: 200, epoch: 6 | loss: 0.0229797\n",
      "\tspeed: 0.0618s/iter; left time: 1290.6435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.47s\n",
      "Steps: 222 | Train Loss: 0.0236170 Vali Loss: 0.0310748 Test Loss: 0.0370330\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0232741\n",
      "\tspeed: 0.1747s/iter; left time: 3628.1846s\n",
      "\titers: 200, epoch: 7 | loss: 0.0228543\n",
      "\tspeed: 0.0704s/iter; left time: 1456.1096s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:14.90s\n",
      "Steps: 222 | Train Loss: 0.0232866 Vali Loss: 0.0312337 Test Loss: 0.0371265\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0223212\n",
      "\tspeed: 0.1452s/iter; left time: 2983.4430s\n",
      "\titers: 200, epoch: 8 | loss: 0.0230172\n",
      "\tspeed: 0.0733s/iter; left time: 1499.1965s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:15.65s\n",
      "Steps: 222 | Train Loss: 0.0229774 Vali Loss: 0.0313451 Test Loss: 0.0380813\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0226680\n",
      "\tspeed: 0.1494s/iter; left time: 3036.0806s\n",
      "\titers: 200, epoch: 9 | loss: 0.0234345\n",
      "\tspeed: 0.0619s/iter; left time: 1251.2035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:15.34s\n",
      "Steps: 222 | Train Loss: 0.0226998 Vali Loss: 0.0314177 Test Loss: 0.0380009\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0210144\n",
      "\tspeed: 0.1659s/iter; left time: 3335.6507s\n",
      "\titers: 200, epoch: 10 | loss: 0.0225535\n",
      "\tspeed: 0.0689s/iter; left time: 1378.9097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:15.00s\n",
      "Steps: 222 | Train Loss: 0.0224297 Vali Loss: 0.0314702 Test Loss: 0.0386583\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0236188\n",
      "\tspeed: 0.1463s/iter; left time: 2908.4139s\n",
      "\titers: 200, epoch: 11 | loss: 0.0231610\n",
      "\tspeed: 0.0733s/iter; left time: 1450.6511s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:15.52s\n",
      "Steps: 222 | Train Loss: 0.0221310 Vali Loss: 0.0315919 Test Loss: 0.0389392\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0233894\n",
      "\tspeed: 0.1077s/iter; left time: 2118.0256s\n",
      "\titers: 200, epoch: 12 | loss: 0.0225971\n",
      "\tspeed: 0.0656s/iter; left time: 1283.2730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:13.99s\n",
      "Steps: 222 | Train Loss: 0.0218652 Vali Loss: 0.0316946 Test Loss: 0.0397190\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0197009\n",
      "\tspeed: 0.1603s/iter; left time: 3115.8205s\n",
      "\titers: 200, epoch: 13 | loss: 0.0216768\n",
      "\tspeed: 0.0800s/iter; left time: 1546.5028s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:18.60s\n",
      "Steps: 222 | Train Loss: 0.0216063 Vali Loss: 0.0318175 Test Loss: 0.0400800\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0204925\n",
      "\tspeed: 0.2133s/iter; left time: 4098.9372s\n",
      "\titers: 200, epoch: 14 | loss: 0.0193690\n",
      "\tspeed: 0.0806s/iter; left time: 1541.4040s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:18.76s\n",
      "Steps: 222 | Train Loss: 0.0213160 Vali Loss: 0.0321348 Test Loss: 0.0406129\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0197787\n",
      "\tspeed: 0.2205s/iter; left time: 4187.4902s\n",
      "\titers: 200, epoch: 15 | loss: 0.0213235\n",
      "\tspeed: 0.0755s/iter; left time: 1426.1903s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:17.92s\n",
      "Steps: 222 | Train Loss: 0.0210409 Vali Loss: 0.0324050 Test Loss: 0.0409346\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.03713453561067581, rmse:0.1927032321691513, mae:0.13542155921459198, rse:0.6825703382492065\n",
      "Intermediate time for DE and pred_len 168: 00h:05m:54.47s\n",
      "Intermediate time for DE: 00h:24m:03.85s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0259924\n",
      "\tspeed: 0.0988s/iter; left time: 2193.7110s\n",
      "\titers: 200, epoch: 1 | loss: 0.0246011\n",
      "\tspeed: 0.0785s/iter; left time: 1735.5560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:18.69s\n",
      "Steps: 223 | Train Loss: 0.0283259 Vali Loss: 0.0284845 Test Loss: 0.0390218\n",
      "Validation loss decreased (inf --> 0.028484).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0159642\n",
      "\tspeed: 0.1724s/iter; left time: 3789.1832s\n",
      "\titers: 200, epoch: 2 | loss: 0.0148842\n",
      "\tspeed: 0.0793s/iter; left time: 1735.2142s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:18.55s\n",
      "Steps: 223 | Train Loss: 0.0157466 Vali Loss: 0.0198990 Test Loss: 0.0255800\n",
      "Validation loss decreased (0.028484 --> 0.019899).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0146333\n",
      "\tspeed: 0.1882s/iter; left time: 4094.4508s\n",
      "\titers: 200, epoch: 3 | loss: 0.0138821\n",
      "\tspeed: 0.0842s/iter; left time: 1823.6436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:19.22s\n",
      "Steps: 223 | Train Loss: 0.0139770 Vali Loss: 0.0191788 Test Loss: 0.0251260\n",
      "Validation loss decreased (0.019899 --> 0.019179).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0149807\n",
      "\tspeed: 0.1660s/iter; left time: 3573.3253s\n",
      "\titers: 200, epoch: 4 | loss: 0.0159410\n",
      "\tspeed: 0.0795s/iter; left time: 1703.3821s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:18.15s\n",
      "Steps: 223 | Train Loss: 0.0136111 Vali Loss: 0.0193769 Test Loss: 0.0251419\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0124624\n",
      "\tspeed: 0.1633s/iter; left time: 3480.7743s\n",
      "\titers: 200, epoch: 5 | loss: 0.0124338\n",
      "\tspeed: 0.0783s/iter; left time: 1660.0159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:18.30s\n",
      "Steps: 223 | Train Loss: 0.0133677 Vali Loss: 0.0192061 Test Loss: 0.0252979\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0119990\n",
      "\tspeed: 0.1881s/iter; left time: 3965.5905s\n",
      "\titers: 200, epoch: 6 | loss: 0.0148567\n",
      "\tspeed: 0.0772s/iter; left time: 1619.7284s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:19.15s\n",
      "Steps: 223 | Train Loss: 0.0131876 Vali Loss: 0.0190067 Test Loss: 0.0248291\n",
      "Validation loss decreased (0.019179 --> 0.019007).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0132022\n",
      "\tspeed: 0.1716s/iter; left time: 3580.6283s\n",
      "\titers: 200, epoch: 7 | loss: 0.0121673\n",
      "\tspeed: 0.0799s/iter; left time: 1659.1799s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:18.07s\n",
      "Steps: 223 | Train Loss: 0.0130462 Vali Loss: 0.0191168 Test Loss: 0.0251083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0140444\n",
      "\tspeed: 0.1712s/iter; left time: 3533.0567s\n",
      "\titers: 200, epoch: 8 | loss: 0.0126242\n",
      "\tspeed: 0.0803s/iter; left time: 1649.9162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:18.58s\n",
      "Steps: 223 | Train Loss: 0.0129650 Vali Loss: 0.0188185 Test Loss: 0.0248416\n",
      "Validation loss decreased (0.019007 --> 0.018819).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0131284\n",
      "\tspeed: 0.1914s/iter; left time: 3908.6583s\n",
      "\titers: 200, epoch: 9 | loss: 0.0119249\n",
      "\tspeed: 0.0793s/iter; left time: 1612.1073s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:18.55s\n",
      "Steps: 223 | Train Loss: 0.0128425 Vali Loss: 0.0188309 Test Loss: 0.0245018\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0121159\n",
      "\tspeed: 0.1450s/iter; left time: 2928.3424s\n",
      "\titers: 200, epoch: 10 | loss: 0.0139951\n",
      "\tspeed: 0.0608s/iter; left time: 1220.9214s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:14.16s\n",
      "Steps: 223 | Train Loss: 0.0127500 Vali Loss: 0.0188925 Test Loss: 0.0247047\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0142653\n",
      "\tspeed: 0.1587s/iter; left time: 3169.3033s\n",
      "\titers: 200, epoch: 11 | loss: 0.0131171\n",
      "\tspeed: 0.0813s/iter; left time: 1615.5914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:18.08s\n",
      "Steps: 223 | Train Loss: 0.0126456 Vali Loss: 0.0187767 Test Loss: 0.0250738\n",
      "Validation loss decreased (0.018819 --> 0.018777).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0104647\n",
      "\tspeed: 0.1893s/iter; left time: 3737.9022s\n",
      "\titers: 200, epoch: 12 | loss: 0.0143763\n",
      "\tspeed: 0.0777s/iter; left time: 1527.4732s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:18.24s\n",
      "Steps: 223 | Train Loss: 0.0125827 Vali Loss: 0.0188292 Test Loss: 0.0245414\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0120354\n",
      "\tspeed: 0.1622s/iter; left time: 3167.6330s\n",
      "\titers: 200, epoch: 13 | loss: 0.0132992\n",
      "\tspeed: 0.0804s/iter; left time: 1561.4339s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:17.82s\n",
      "Steps: 223 | Train Loss: 0.0125262 Vali Loss: 0.0188340 Test Loss: 0.0245886\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0122780\n",
      "\tspeed: 0.1668s/iter; left time: 3220.3180s\n",
      "\titers: 200, epoch: 14 | loss: 0.0123523\n",
      "\tspeed: 0.0929s/iter; left time: 1784.2667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:19.75s\n",
      "Steps: 223 | Train Loss: 0.0124646 Vali Loss: 0.0188329 Test Loss: 0.0246720\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0123033\n",
      "\tspeed: 0.1816s/iter; left time: 3463.9585s\n",
      "\titers: 200, epoch: 15 | loss: 0.0120831\n",
      "\tspeed: 0.0794s/iter; left time: 1507.7057s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:18.53s\n",
      "Steps: 223 | Train Loss: 0.0124087 Vali Loss: 0.0187291 Test Loss: 0.0243788\n",
      "Validation loss decreased (0.018777 --> 0.018729).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0116331\n",
      "\tspeed: 0.1747s/iter; left time: 3294.3282s\n",
      "\titers: 200, epoch: 16 | loss: 0.0112471\n",
      "\tspeed: 0.0812s/iter; left time: 1522.7122s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:18.54s\n",
      "Steps: 223 | Train Loss: 0.0123589 Vali Loss: 0.0187595 Test Loss: 0.0244636\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0136333\n",
      "\tspeed: 0.1782s/iter; left time: 3320.8359s\n",
      "\titers: 200, epoch: 17 | loss: 0.0123088\n",
      "\tspeed: 0.0883s/iter; left time: 1635.6554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:20.34s\n",
      "Steps: 223 | Train Loss: 0.0123212 Vali Loss: 0.0186955 Test Loss: 0.0244878\n",
      "Validation loss decreased (0.018729 --> 0.018696).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0125058\n",
      "\tspeed: 0.1729s/iter; left time: 3182.4138s\n",
      "\titers: 200, epoch: 18 | loss: 0.0120818\n",
      "\tspeed: 0.0806s/iter; left time: 1475.4541s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:18.53s\n",
      "Steps: 223 | Train Loss: 0.0122789 Vali Loss: 0.0187429 Test Loss: 0.0246242\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0123783\n",
      "\tspeed: 0.1712s/iter; left time: 3113.3033s\n",
      "\titers: 200, epoch: 19 | loss: 0.0137483\n",
      "\tspeed: 0.0872s/iter; left time: 1577.8738s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:19.43s\n",
      "Steps: 223 | Train Loss: 0.0122455 Vali Loss: 0.0188140 Test Loss: 0.0245240\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0122385\n",
      "\tspeed: 0.1852s/iter; left time: 3327.3790s\n",
      "\titers: 200, epoch: 20 | loss: 0.0100243\n",
      "\tspeed: 0.0801s/iter; left time: 1430.3401s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:18.62s\n",
      "Steps: 223 | Train Loss: 0.0121935 Vali Loss: 0.0188129 Test Loss: 0.0245216\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0109536\n",
      "\tspeed: 0.1721s/iter; left time: 3054.0150s\n",
      "\titers: 200, epoch: 21 | loss: 0.0136657\n",
      "\tspeed: 0.0784s/iter; left time: 1383.6895s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:18.32s\n",
      "Steps: 223 | Train Loss: 0.0121563 Vali Loss: 0.0187994 Test Loss: 0.0245719\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0120630\n",
      "\tspeed: 0.1815s/iter; left time: 3179.9780s\n",
      "\titers: 200, epoch: 22 | loss: 0.0122324\n",
      "\tspeed: 0.0883s/iter; left time: 1538.6036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:20.21s\n",
      "Steps: 223 | Train Loss: 0.0121276 Vali Loss: 0.0187267 Test Loss: 0.0245971\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0132323\n",
      "\tspeed: 0.1700s/iter; left time: 2939.8612s\n",
      "\titers: 200, epoch: 23 | loss: 0.0106442\n",
      "\tspeed: 0.0836s/iter; left time: 1438.0590s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:18.56s\n",
      "Steps: 223 | Train Loss: 0.0121155 Vali Loss: 0.0187053 Test Loss: 0.0245364\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0125100\n",
      "\tspeed: 0.1744s/iter; left time: 2976.7754s\n",
      "\titers: 200, epoch: 24 | loss: 0.0125616\n",
      "\tspeed: 0.0677s/iter; left time: 1148.3035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:17.06s\n",
      "Steps: 223 | Train Loss: 0.0120919 Vali Loss: 0.0187602 Test Loss: 0.0246057\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0128259\n",
      "\tspeed: 0.1508s/iter; left time: 2540.6103s\n",
      "\titers: 200, epoch: 25 | loss: 0.0117420\n",
      "\tspeed: 0.0842s/iter; left time: 1409.5929s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:18.27s\n",
      "Steps: 223 | Train Loss: 0.0120816 Vali Loss: 0.0188265 Test Loss: 0.0245693\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0112484\n",
      "\tspeed: 0.1624s/iter; left time: 2700.6351s\n",
      "\titers: 200, epoch: 26 | loss: 0.0115009\n",
      "\tspeed: 0.0804s/iter; left time: 1328.4235s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:17.99s\n",
      "Steps: 223 | Train Loss: 0.0120537 Vali Loss: 0.0188555 Test Loss: 0.0245836\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0128660\n",
      "\tspeed: 0.1643s/iter; left time: 2694.2207s\n",
      "\titers: 200, epoch: 27 | loss: 0.0128459\n",
      "\tspeed: 0.0886s/iter; left time: 1444.5227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:19.44s\n",
      "Steps: 223 | Train Loss: 0.0120271 Vali Loss: 0.0187563 Test Loss: 0.0245329\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.024487832561135292, rmse:0.1564858853816986, mae:0.1026926264166832, rse:0.5398319363594055\n",
      "Intermediate time for GB and pred_len 24: 00h:11m:33.05s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0310066\n",
      "\tspeed: 0.1045s/iter; left time: 2309.0630s\n",
      "\titers: 200, epoch: 1 | loss: 0.0256376\n",
      "\tspeed: 0.0780s/iter; left time: 1715.1919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:18.66s\n",
      "Steps: 222 | Train Loss: 0.0307500 Vali Loss: 0.0325461 Test Loss: 0.0466486\n",
      "Validation loss decreased (inf --> 0.032546).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0226989\n",
      "\tspeed: 0.1905s/iter; left time: 4167.4717s\n",
      "\titers: 200, epoch: 2 | loss: 0.0240538\n",
      "\tspeed: 0.0861s/iter; left time: 1874.7176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:18.96s\n",
      "Steps: 222 | Train Loss: 0.0228539 Vali Loss: 0.0286221 Test Loss: 0.0401569\n",
      "Validation loss decreased (0.032546 --> 0.028622).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0210763\n",
      "\tspeed: 0.1760s/iter; left time: 3812.5232s\n",
      "\titers: 200, epoch: 3 | loss: 0.0222665\n",
      "\tspeed: 0.0678s/iter; left time: 1462.1288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:14.92s\n",
      "Steps: 222 | Train Loss: 0.0213867 Vali Loss: 0.0285050 Test Loss: 0.0408070\n",
      "Validation loss decreased (0.028622 --> 0.028505).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0192114\n",
      "\tspeed: 0.1438s/iter; left time: 3081.9326s\n",
      "\titers: 200, epoch: 4 | loss: 0.0215106\n",
      "\tspeed: 0.0647s/iter; left time: 1381.1581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:15.01s\n",
      "Steps: 222 | Train Loss: 0.0209945 Vali Loss: 0.0283547 Test Loss: 0.0408883\n",
      "Validation loss decreased (0.028505 --> 0.028355).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0211985\n",
      "\tspeed: 0.1711s/iter; left time: 3630.1994s\n",
      "\titers: 200, epoch: 5 | loss: 0.0218957\n",
      "\tspeed: 0.0679s/iter; left time: 1432.8549s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:15.37s\n",
      "Steps: 222 | Train Loss: 0.0206841 Vali Loss: 0.0281620 Test Loss: 0.0400784\n",
      "Validation loss decreased (0.028355 --> 0.028162).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0197395\n",
      "\tspeed: 0.1563s/iter; left time: 3280.5730s\n",
      "\titers: 200, epoch: 6 | loss: 0.0203199\n",
      "\tspeed: 0.0693s/iter; left time: 1447.4153s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.68s\n",
      "Steps: 222 | Train Loss: 0.0204184 Vali Loss: 0.0282535 Test Loss: 0.0406600\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0193540\n",
      "\tspeed: 0.1361s/iter; left time: 2827.6342s\n",
      "\titers: 200, epoch: 7 | loss: 0.0196574\n",
      "\tspeed: 0.0653s/iter; left time: 1350.3721s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:15.25s\n",
      "Steps: 222 | Train Loss: 0.0201654 Vali Loss: 0.0283629 Test Loss: 0.0403851\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0203656\n",
      "\tspeed: 0.1620s/iter; left time: 3329.5953s\n",
      "\titers: 200, epoch: 8 | loss: 0.0195816\n",
      "\tspeed: 0.0661s/iter; left time: 1352.0379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:15.36s\n",
      "Steps: 222 | Train Loss: 0.0199174 Vali Loss: 0.0284603 Test Loss: 0.0404209\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0188830\n",
      "\tspeed: 0.1501s/iter; left time: 3050.7680s\n",
      "\titers: 200, epoch: 9 | loss: 0.0194778\n",
      "\tspeed: 0.0693s/iter; left time: 1401.5903s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:15.22s\n",
      "Steps: 222 | Train Loss: 0.0196882 Vali Loss: 0.0284817 Test Loss: 0.0405771\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0192994\n",
      "\tspeed: 0.1495s/iter; left time: 3005.1048s\n",
      "\titers: 200, epoch: 10 | loss: 0.0190058\n",
      "\tspeed: 0.0559s/iter; left time: 1119.0738s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:13.64s\n",
      "Steps: 222 | Train Loss: 0.0194750 Vali Loss: 0.0283545 Test Loss: 0.0410616\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0189386\n",
      "\tspeed: 0.1137s/iter; left time: 2260.6298s\n",
      "\titers: 200, epoch: 11 | loss: 0.0185040\n",
      "\tspeed: 0.0662s/iter; left time: 1309.4033s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:15.22s\n",
      "Steps: 222 | Train Loss: 0.0192353 Vali Loss: 0.0286885 Test Loss: 0.0414736\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0179484\n",
      "\tspeed: 0.1641s/iter; left time: 3225.2765s\n",
      "\titers: 200, epoch: 12 | loss: 0.0180923\n",
      "\tspeed: 0.0650s/iter; left time: 1270.9406s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:15.08s\n",
      "Steps: 222 | Train Loss: 0.0190230 Vali Loss: 0.0285809 Test Loss: 0.0414058\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0176465\n",
      "\tspeed: 0.1743s/iter; left time: 3388.0317s\n",
      "\titers: 200, epoch: 13 | loss: 0.0193337\n",
      "\tspeed: 0.0674s/iter; left time: 1303.2268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:15.23s\n",
      "Steps: 222 | Train Loss: 0.0188276 Vali Loss: 0.0286796 Test Loss: 0.0423112\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0188346\n",
      "\tspeed: 0.1599s/iter; left time: 3071.9992s\n",
      "\titers: 200, epoch: 14 | loss: 0.0186280\n",
      "\tspeed: 0.0714s/iter; left time: 1365.3254s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:15.67s\n",
      "Steps: 222 | Train Loss: 0.0186396 Vali Loss: 0.0289282 Test Loss: 0.0422525\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0203469\n",
      "\tspeed: 0.1427s/iter; left time: 2710.9665s\n",
      "\titers: 200, epoch: 15 | loss: 0.0194815\n",
      "\tspeed: 0.0724s/iter; left time: 1368.2281s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:16.09s\n",
      "Steps: 222 | Train Loss: 0.0184927 Vali Loss: 0.0289733 Test Loss: 0.0426640\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04007839038968086, rmse:0.20019587874412537, mae:0.14094528555870056, rse:0.6923052072525024\n",
      "Intermediate time for GB and pred_len 96: 00h:05m:50.65s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0290644\n",
      "\tspeed: 0.1024s/iter; left time: 2263.8320s\n",
      "\titers: 200, epoch: 1 | loss: 0.0283159\n",
      "\tspeed: 0.0652s/iter; left time: 1434.6165s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:15.83s\n",
      "Steps: 222 | Train Loss: 0.0315331 Vali Loss: 0.0335719 Test Loss: 0.0481259\n",
      "Validation loss decreased (inf --> 0.033572).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0229541\n",
      "\tspeed: 0.1550s/iter; left time: 3390.9974s\n",
      "\titers: 200, epoch: 2 | loss: 0.0229998\n",
      "\tspeed: 0.0692s/iter; left time: 1506.2522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:15.27s\n",
      "Steps: 222 | Train Loss: 0.0243021 Vali Loss: 0.0301776 Test Loss: 0.0425740\n",
      "Validation loss decreased (0.033572 --> 0.030178).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0232384\n",
      "\tspeed: 0.1478s/iter; left time: 3201.6213s\n",
      "\titers: 200, epoch: 3 | loss: 0.0242960\n",
      "\tspeed: 0.0732s/iter; left time: 1577.6031s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:15.55s\n",
      "Steps: 222 | Train Loss: 0.0228905 Vali Loss: 0.0299546 Test Loss: 0.0426070\n",
      "Validation loss decreased (0.030178 --> 0.029955).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0240797\n",
      "\tspeed: 0.1528s/iter; left time: 3275.3638s\n",
      "\titers: 200, epoch: 4 | loss: 0.0210708\n",
      "\tspeed: 0.0660s/iter; left time: 1408.6938s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:15.67s\n",
      "Steps: 222 | Train Loss: 0.0224280 Vali Loss: 0.0298746 Test Loss: 0.0425166\n",
      "Validation loss decreased (0.029955 --> 0.029875).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0218961\n",
      "\tspeed: 0.1543s/iter; left time: 3273.1970s\n",
      "\titers: 200, epoch: 5 | loss: 0.0230778\n",
      "\tspeed: 0.0493s/iter; left time: 1040.0162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:13.08s\n",
      "Steps: 222 | Train Loss: 0.0220308 Vali Loss: 0.0301946 Test Loss: 0.0431376\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0205828\n",
      "\tspeed: 0.1066s/iter; left time: 2237.7128s\n",
      "\titers: 200, epoch: 6 | loss: 0.0217106\n",
      "\tspeed: 0.0683s/iter; left time: 1427.8156s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.04s\n",
      "Steps: 222 | Train Loss: 0.0216309 Vali Loss: 0.0304557 Test Loss: 0.0433500\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0208696\n",
      "\tspeed: 0.1547s/iter; left time: 3212.0485s\n",
      "\titers: 200, epoch: 7 | loss: 0.0206528\n",
      "\tspeed: 0.0719s/iter; left time: 1485.8624s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:15.96s\n",
      "Steps: 222 | Train Loss: 0.0212453 Vali Loss: 0.0307189 Test Loss: 0.0444726\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0202371\n",
      "\tspeed: 0.1469s/iter; left time: 3018.3356s\n",
      "\titers: 200, epoch: 8 | loss: 0.0194056\n",
      "\tspeed: 0.0708s/iter; left time: 1448.3545s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:16.36s\n",
      "Steps: 222 | Train Loss: 0.0208566 Vali Loss: 0.0309050 Test Loss: 0.0446516\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0206947\n",
      "\tspeed: 0.1744s/iter; left time: 3544.7049s\n",
      "\titers: 200, epoch: 9 | loss: 0.0204771\n",
      "\tspeed: 0.0728s/iter; left time: 1471.4076s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:16.00s\n",
      "Steps: 222 | Train Loss: 0.0205095 Vali Loss: 0.0311708 Test Loss: 0.0440427\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0200099\n",
      "\tspeed: 0.1603s/iter; left time: 3222.2536s\n",
      "\titers: 200, epoch: 10 | loss: 0.0207653\n",
      "\tspeed: 0.0691s/iter; left time: 1381.8390s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:16.02s\n",
      "Steps: 222 | Train Loss: 0.0201604 Vali Loss: 0.0313169 Test Loss: 0.0443286\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209964\n",
      "\tspeed: 0.1507s/iter; left time: 2996.9087s\n",
      "\titers: 200, epoch: 11 | loss: 0.0193575\n",
      "\tspeed: 0.0677s/iter; left time: 1339.3369s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:15.81s\n",
      "Steps: 222 | Train Loss: 0.0197860 Vali Loss: 0.0315444 Test Loss: 0.0445627\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0209620\n",
      "\tspeed: 0.1798s/iter; left time: 3534.4776s\n",
      "\titers: 200, epoch: 12 | loss: 0.0191670\n",
      "\tspeed: 0.0753s/iter; left time: 1473.4650s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:16.31s\n",
      "Steps: 222 | Train Loss: 0.0194906 Vali Loss: 0.0318858 Test Loss: 0.0450109\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0183557\n",
      "\tspeed: 0.1473s/iter; left time: 2863.7026s\n",
      "\titers: 200, epoch: 13 | loss: 0.0196181\n",
      "\tspeed: 0.0711s/iter; left time: 1374.8609s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:15.85s\n",
      "Steps: 222 | Train Loss: 0.0192031 Vali Loss: 0.0319863 Test Loss: 0.0463496\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0176900\n",
      "\tspeed: 0.1575s/iter; left time: 3026.2756s\n",
      "\titers: 200, epoch: 14 | loss: 0.0185688\n",
      "\tspeed: 0.0642s/iter; left time: 1227.4576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:15.66s\n",
      "Steps: 222 | Train Loss: 0.0189770 Vali Loss: 0.0320392 Test Loss: 0.0460689\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04251662269234657, rmse:0.20619559288024902, mae:0.14658014476299286, rse:0.7149097323417664\n",
      "Intermediate time for GB and pred_len 168: 00h:05m:25.44s\n",
      "Intermediate time for GB: 00h:22m:49.14s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_512_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_512_24_ES_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0248113\n",
      "\tspeed: 0.0658s/iter; left time: 1460.8716s\n",
      "\titers: 200, epoch: 1 | loss: 0.0209474\n",
      "\tspeed: 0.0471s/iter; left time: 1040.3756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.75s\n",
      "Steps: 223 | Train Loss: 0.0288529 Vali Loss: 0.0172380 Test Loss: 0.0227001\n",
      "Validation loss decreased (inf --> 0.017238).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0109474\n",
      "\tspeed: 0.1030s/iter; left time: 2264.6684s\n",
      "\titers: 200, epoch: 2 | loss: 0.0104648\n",
      "\tspeed: 0.0419s/iter; left time: 917.4543s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.31s\n",
      "Steps: 223 | Train Loss: 0.0118415 Vali Loss: 0.0092437 Test Loss: 0.0117329\n",
      "Validation loss decreased (0.017238 --> 0.009244).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0093500\n",
      "\tspeed: 0.0996s/iter; left time: 2167.7765s\n",
      "\titers: 200, epoch: 3 | loss: 0.0088967\n",
      "\tspeed: 0.0428s/iter; left time: 927.7131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.36s\n",
      "Steps: 223 | Train Loss: 0.0093735 Vali Loss: 0.0089112 Test Loss: 0.0112657\n",
      "Validation loss decreased (0.009244 --> 0.008911).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0097252\n",
      "\tspeed: 0.0940s/iter; left time: 2024.8809s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084611\n",
      "\tspeed: 0.0487s/iter; left time: 1044.0580s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0088100 Vali Loss: 0.0083192 Test Loss: 0.0106727\n",
      "Validation loss decreased (0.008911 --> 0.008319).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0086839\n",
      "\tspeed: 0.0916s/iter; left time: 1951.9656s\n",
      "\titers: 200, epoch: 5 | loss: 0.0082906\n",
      "\tspeed: 0.0466s/iter; left time: 988.6740s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.40s\n",
      "Steps: 223 | Train Loss: 0.0084299 Vali Loss: 0.0082377 Test Loss: 0.0105463\n",
      "Validation loss decreased (0.008319 --> 0.008238).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0073137\n",
      "\tspeed: 0.0909s/iter; left time: 1916.6573s\n",
      "\titers: 200, epoch: 6 | loss: 0.0077601\n",
      "\tspeed: 0.0412s/iter; left time: 863.9959s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.48s\n",
      "Steps: 223 | Train Loss: 0.0082041 Vali Loss: 0.0080951 Test Loss: 0.0103022\n",
      "Validation loss decreased (0.008238 --> 0.008095).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0072065\n",
      "\tspeed: 0.1055s/iter; left time: 2200.9428s\n",
      "\titers: 200, epoch: 7 | loss: 0.0076435\n",
      "\tspeed: 0.0329s/iter; left time: 682.5077s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.01s\n",
      "Steps: 223 | Train Loss: 0.0080231 Vali Loss: 0.0079687 Test Loss: 0.0102067\n",
      "Validation loss decreased (0.008095 --> 0.007969).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0069740\n",
      "\tspeed: 0.0679s/iter; left time: 1401.4325s\n",
      "\titers: 200, epoch: 8 | loss: 0.0070042\n",
      "\tspeed: 0.0415s/iter; left time: 852.2381s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.84s\n",
      "Steps: 223 | Train Loss: 0.0078809 Vali Loss: 0.0079813 Test Loss: 0.0101716\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0074675\n",
      "\tspeed: 0.0969s/iter; left time: 1977.9815s\n",
      "\titers: 200, epoch: 9 | loss: 0.0075371\n",
      "\tspeed: 0.0421s/iter; left time: 854.9347s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:09.94s\n",
      "Steps: 223 | Train Loss: 0.0077949 Vali Loss: 0.0078723 Test Loss: 0.0101530\n",
      "Validation loss decreased (0.007969 --> 0.007872).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0079730\n",
      "\tspeed: 0.0948s/iter; left time: 1913.6263s\n",
      "\titers: 200, epoch: 10 | loss: 0.0074285\n",
      "\tspeed: 0.0459s/iter; left time: 921.7508s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.47s\n",
      "Steps: 223 | Train Loss: 0.0076949 Vali Loss: 0.0078730 Test Loss: 0.0100437\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0077881\n",
      "\tspeed: 0.0899s/iter; left time: 1795.9156s\n",
      "\titers: 200, epoch: 11 | loss: 0.0080286\n",
      "\tspeed: 0.0429s/iter; left time: 852.8511s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.01s\n",
      "Steps: 223 | Train Loss: 0.0076302 Vali Loss: 0.0078841 Test Loss: 0.0100958\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0083016\n",
      "\tspeed: 0.0933s/iter; left time: 1842.8859s\n",
      "\titers: 200, epoch: 12 | loss: 0.0076919\n",
      "\tspeed: 0.0440s/iter; left time: 863.7425s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.02s\n",
      "Steps: 223 | Train Loss: 0.0075464 Vali Loss: 0.0078212 Test Loss: 0.0100952\n",
      "Validation loss decreased (0.007872 --> 0.007821).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0071964\n",
      "\tspeed: 0.0983s/iter; left time: 1919.6039s\n",
      "\titers: 200, epoch: 13 | loss: 0.0074306\n",
      "\tspeed: 0.0455s/iter; left time: 883.5155s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.56s\n",
      "Steps: 223 | Train Loss: 0.0075034 Vali Loss: 0.0078291 Test Loss: 0.0100455\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0072603\n",
      "\tspeed: 0.0967s/iter; left time: 1865.7520s\n",
      "\titers: 200, epoch: 14 | loss: 0.0069571\n",
      "\tspeed: 0.0492s/iter; left time: 944.4103s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.48s\n",
      "Steps: 223 | Train Loss: 0.0074366 Vali Loss: 0.0077338 Test Loss: 0.0099672\n",
      "Validation loss decreased (0.007821 --> 0.007734).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0063686\n",
      "\tspeed: 0.0941s/iter; left time: 1794.9965s\n",
      "\titers: 200, epoch: 15 | loss: 0.0089034\n",
      "\tspeed: 0.0449s/iter; left time: 852.0855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.33s\n",
      "Steps: 223 | Train Loss: 0.0074022 Vali Loss: 0.0077275 Test Loss: 0.0099593\n",
      "Validation loss decreased (0.007734 --> 0.007727).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0083698\n",
      "\tspeed: 0.0973s/iter; left time: 1835.4794s\n",
      "\titers: 200, epoch: 16 | loss: 0.0062602\n",
      "\tspeed: 0.0425s/iter; left time: 796.2110s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.30s\n",
      "Steps: 223 | Train Loss: 0.0073645 Vali Loss: 0.0077116 Test Loss: 0.0099065\n",
      "Validation loss decreased (0.007727 --> 0.007712).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0078689\n",
      "\tspeed: 0.0997s/iter; left time: 1858.3101s\n",
      "\titers: 200, epoch: 17 | loss: 0.0062379\n",
      "\tspeed: 0.0480s/iter; left time: 889.2552s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.52s\n",
      "Steps: 223 | Train Loss: 0.0073175 Vali Loss: 0.0077440 Test Loss: 0.0099426\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0077797\n",
      "\tspeed: 0.0958s/iter; left time: 1763.0436s\n",
      "\titers: 200, epoch: 18 | loss: 0.0077466\n",
      "\tspeed: 0.0469s/iter; left time: 858.3971s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.72s\n",
      "Steps: 223 | Train Loss: 0.0072808 Vali Loss: 0.0077294 Test Loss: 0.0099378\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0076851\n",
      "\tspeed: 0.1001s/iter; left time: 1820.5598s\n",
      "\titers: 200, epoch: 19 | loss: 0.0068682\n",
      "\tspeed: 0.0442s/iter; left time: 798.6017s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.43s\n",
      "Steps: 223 | Train Loss: 0.0072656 Vali Loss: 0.0077241 Test Loss: 0.0098723\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0075548\n",
      "\tspeed: 0.1008s/iter; left time: 1810.8780s\n",
      "\titers: 200, epoch: 20 | loss: 0.0078679\n",
      "\tspeed: 0.0472s/iter; left time: 843.1915s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.71s\n",
      "Steps: 223 | Train Loss: 0.0072433 Vali Loss: 0.0076860 Test Loss: 0.0099355\n",
      "Validation loss decreased (0.007712 --> 0.007686).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0072600\n",
      "\tspeed: 0.0969s/iter; left time: 1719.1844s\n",
      "\titers: 200, epoch: 21 | loss: 0.0071551\n",
      "\tspeed: 0.0348s/iter; left time: 614.3573s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:09.26s\n",
      "Steps: 223 | Train Loss: 0.0072157 Vali Loss: 0.0076330 Test Loss: 0.0098434\n",
      "Validation loss decreased (0.007686 --> 0.007633).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0068069\n",
      "\tspeed: 0.0754s/iter; left time: 1321.4233s\n",
      "\titers: 200, epoch: 22 | loss: 0.0079472\n",
      "\tspeed: 0.0476s/iter; left time: 828.6654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.04s\n",
      "Steps: 223 | Train Loss: 0.0071874 Vali Loss: 0.0076306 Test Loss: 0.0098311\n",
      "Validation loss decreased (0.007633 --> 0.007631).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0070788\n",
      "\tspeed: 0.1048s/iter; left time: 1812.4953s\n",
      "\titers: 200, epoch: 23 | loss: 0.0076406\n",
      "\tspeed: 0.0517s/iter; left time: 889.2725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:10.80s\n",
      "Steps: 223 | Train Loss: 0.0071563 Vali Loss: 0.0076458 Test Loss: 0.0098406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0087415\n",
      "\tspeed: 0.1034s/iter; left time: 1764.5742s\n",
      "\titers: 200, epoch: 24 | loss: 0.0065944\n",
      "\tspeed: 0.0445s/iter; left time: 754.9969s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 223 | Train Loss: 0.0071404 Vali Loss: 0.0076543 Test Loss: 0.0098225\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0075733\n",
      "\tspeed: 0.1019s/iter; left time: 1717.6650s\n",
      "\titers: 200, epoch: 25 | loss: 0.0067943\n",
      "\tspeed: 0.0477s/iter; left time: 798.6054s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:11.52s\n",
      "Steps: 223 | Train Loss: 0.0071305 Vali Loss: 0.0076521 Test Loss: 0.0098495\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0069825\n",
      "\tspeed: 0.1186s/iter; left time: 1971.6990s\n",
      "\titers: 200, epoch: 26 | loss: 0.0063963\n",
      "\tspeed: 0.0498s/iter; left time: 823.6999s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:12.42s\n",
      "Steps: 223 | Train Loss: 0.0071161 Vali Loss: 0.0076059 Test Loss: 0.0098246\n",
      "Validation loss decreased (0.007631 --> 0.007606).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0067918\n",
      "\tspeed: 0.1148s/iter; left time: 1882.3186s\n",
      "\titers: 200, epoch: 27 | loss: 0.0070344\n",
      "\tspeed: 0.0502s/iter; left time: 819.1113s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 223 | Train Loss: 0.0070990 Vali Loss: 0.0076348 Test Loss: 0.0098149\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0073184\n",
      "\tspeed: 0.1132s/iter; left time: 1831.9159s\n",
      "\titers: 200, epoch: 28 | loss: 0.0075491\n",
      "\tspeed: 0.0588s/iter; left time: 945.9827s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:12.53s\n",
      "Steps: 223 | Train Loss: 0.0070902 Vali Loss: 0.0076031 Test Loss: 0.0097912\n",
      "Validation loss decreased (0.007606 --> 0.007603).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0066793\n",
      "\tspeed: 0.1231s/iter; left time: 1964.0935s\n",
      "\titers: 200, epoch: 29 | loss: 0.0068923\n",
      "\tspeed: 0.0557s/iter; left time: 883.6404s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:12.41s\n",
      "Steps: 223 | Train Loss: 0.0070797 Vali Loss: 0.0076001 Test Loss: 0.0097856\n",
      "Validation loss decreased (0.007603 --> 0.007600).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0076665\n",
      "\tspeed: 0.1072s/iter; left time: 1685.9778s\n",
      "\titers: 200, epoch: 30 | loss: 0.0071035\n",
      "\tspeed: 0.0542s/iter; left time: 847.7206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:12.05s\n",
      "Steps: 223 | Train Loss: 0.0070798 Vali Loss: 0.0075942 Test Loss: 0.0097989\n",
      "Validation loss decreased (0.007600 --> 0.007594).  Saving model ...\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0067166\n",
      "\tspeed: 0.1138s/iter; left time: 1764.4951s\n",
      "\titers: 200, epoch: 31 | loss: 0.0067948\n",
      "\tspeed: 0.0469s/iter; left time: 723.2141s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 223 | Train Loss: 0.0070523 Vali Loss: 0.0075962 Test Loss: 0.0098016\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0069795\n",
      "\tspeed: 0.1171s/iter; left time: 1789.8310s\n",
      "\titers: 200, epoch: 32 | loss: 0.0077543\n",
      "\tspeed: 0.0551s/iter; left time: 837.2553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:13.30s\n",
      "Steps: 223 | Train Loss: 0.0070385 Vali Loss: 0.0076146 Test Loss: 0.0098008\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0079984\n",
      "\tspeed: 0.1150s/iter; left time: 1731.8079s\n",
      "\titers: 200, epoch: 33 | loss: 0.0080055\n",
      "\tspeed: 0.0511s/iter; left time: 765.2407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:11.71s\n",
      "Steps: 223 | Train Loss: 0.0070371 Vali Loss: 0.0075771 Test Loss: 0.0097709\n",
      "Validation loss decreased (0.007594 --> 0.007577).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0072682\n",
      "\tspeed: 0.1176s/iter; left time: 1745.3180s\n",
      "\titers: 200, epoch: 34 | loss: 0.0069474\n",
      "\tspeed: 0.0517s/iter; left time: 762.8939s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:12.08s\n",
      "Steps: 223 | Train Loss: 0.0070416 Vali Loss: 0.0075643 Test Loss: 0.0097814\n",
      "Validation loss decreased (0.007577 --> 0.007564).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0073579\n",
      "\tspeed: 0.1126s/iter; left time: 1645.3949s\n",
      "\titers: 200, epoch: 35 | loss: 0.0074479\n",
      "\tspeed: 0.0537s/iter; left time: 780.2116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:11.90s\n",
      "Steps: 223 | Train Loss: 0.0070233 Vali Loss: 0.0075564 Test Loss: 0.0097594\n",
      "Validation loss decreased (0.007564 --> 0.007556).  Saving model ...\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0076245\n",
      "\tspeed: 0.1246s/iter; left time: 1793.2666s\n",
      "\titers: 200, epoch: 36 | loss: 0.0071643\n",
      "\tspeed: 0.0584s/iter; left time: 834.9080s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:13.27s\n",
      "Steps: 223 | Train Loss: 0.0070201 Vali Loss: 0.0075789 Test Loss: 0.0097631\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0064672\n",
      "\tspeed: 0.1136s/iter; left time: 1609.8704s\n",
      "\titers: 200, epoch: 37 | loss: 0.0065914\n",
      "\tspeed: 0.0522s/iter; left time: 734.4686s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:12.13s\n",
      "Steps: 223 | Train Loss: 0.0070142 Vali Loss: 0.0075650 Test Loss: 0.0097781\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0071096\n",
      "\tspeed: 0.1138s/iter; left time: 1587.8199s\n",
      "\titers: 200, epoch: 38 | loss: 0.0075174\n",
      "\tspeed: 0.0532s/iter; left time: 737.0725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:12.37s\n",
      "Steps: 223 | Train Loss: 0.0070173 Vali Loss: 0.0076022 Test Loss: 0.0097703\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0068246\n",
      "\tspeed: 0.1183s/iter; left time: 1624.3566s\n",
      "\titers: 200, epoch: 39 | loss: 0.0067389\n",
      "\tspeed: 0.0598s/iter; left time: 815.2716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:13.19s\n",
      "Steps: 223 | Train Loss: 0.0070161 Vali Loss: 0.0075868 Test Loss: 0.0097710\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0068628\n",
      "\tspeed: 0.1196s/iter; left time: 1614.5589s\n",
      "\titers: 200, epoch: 40 | loss: 0.0068842\n",
      "\tspeed: 0.0535s/iter; left time: 717.6616s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:12.19s\n",
      "Steps: 223 | Train Loss: 0.0069897 Vali Loss: 0.0075877 Test Loss: 0.0097754\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0073349\n",
      "\tspeed: 0.1093s/iter; left time: 1451.6413s\n",
      "\titers: 200, epoch: 41 | loss: 0.0074677\n",
      "\tspeed: 0.0401s/iter; left time: 529.1667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:10.45s\n",
      "Steps: 223 | Train Loss: 0.0069966 Vali Loss: 0.0075716 Test Loss: 0.0097876\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0063157\n",
      "\tspeed: 0.0694s/iter; left time: 906.7179s\n",
      "\titers: 200, epoch: 42 | loss: 0.0066401\n",
      "\tspeed: 0.0446s/iter; left time: 577.8965s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:09.61s\n",
      "Steps: 223 | Train Loss: 0.0069928 Vali Loss: 0.0075713 Test Loss: 0.0097738\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0074802\n",
      "\tspeed: 0.1337s/iter; left time: 1715.8404s\n",
      "\titers: 200, epoch: 43 | loss: 0.0070209\n",
      "\tspeed: 0.0545s/iter; left time: 693.7373s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:13.26s\n",
      "Steps: 223 | Train Loss: 0.0069863 Vali Loss: 0.0075624 Test Loss: 0.0097604\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0070245\n",
      "\tspeed: 0.1232s/iter; left time: 1554.0433s\n",
      "\titers: 200, epoch: 44 | loss: 0.0066268\n",
      "\tspeed: 0.0464s/iter; left time: 580.5176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:12.00s\n",
      "Steps: 223 | Train Loss: 0.0069961 Vali Loss: 0.0075788 Test Loss: 0.0097643\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0079945\n",
      "\tspeed: 0.1280s/iter; left time: 1585.7939s\n",
      "\titers: 200, epoch: 45 | loss: 0.0068240\n",
      "\tspeed: 0.0464s/iter; left time: 570.6531s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:12.04s\n",
      "Steps: 223 | Train Loss: 0.0069994 Vali Loss: 0.0075962 Test Loss: 0.0097628\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_512_24_ES_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009759382344782352, rmse:0.09878958761692047, mae:0.06111053004860878, rse:0.29072579741477966\n",
      "Intermediate time for ES and pred_len 24: 00h:11m:38.23s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_512_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_512_96_ES_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0279417\n",
      "\tspeed: 0.1053s/iter; left time: 2327.6884s\n",
      "\titers: 200, epoch: 1 | loss: 0.0222138\n",
      "\tspeed: 0.0649s/iter; left time: 1427.1562s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:15.43s\n",
      "Steps: 222 | Train Loss: 0.0308711 Vali Loss: 0.0206606 Test Loss: 0.0273505\n",
      "Validation loss decreased (inf --> 0.020661).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0174282\n",
      "\tspeed: 0.1625s/iter; left time: 3556.3265s\n",
      "\titers: 200, epoch: 2 | loss: 0.0162973\n",
      "\tspeed: 0.0592s/iter; left time: 1289.3513s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:14.94s\n",
      "Steps: 222 | Train Loss: 0.0173404 Vali Loss: 0.0157199 Test Loss: 0.0201383\n",
      "Validation loss decreased (0.020661 --> 0.015720).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0150819\n",
      "\tspeed: 0.1711s/iter; left time: 3704.6788s\n",
      "\titers: 200, epoch: 3 | loss: 0.0137775\n",
      "\tspeed: 0.0607s/iter; left time: 1309.4556s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:15.29s\n",
      "Steps: 222 | Train Loss: 0.0151375 Vali Loss: 0.0152211 Test Loss: 0.0193364\n",
      "Validation loss decreased (0.015720 --> 0.015221).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0148460\n",
      "\tspeed: 0.1917s/iter; left time: 4108.4429s\n",
      "\titers: 200, epoch: 4 | loss: 0.0141339\n",
      "\tspeed: 0.0638s/iter; left time: 1361.2896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:13.93s\n",
      "Steps: 222 | Train Loss: 0.0145275 Vali Loss: 0.0147649 Test Loss: 0.0192249\n",
      "Validation loss decreased (0.015221 --> 0.014765).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0140156\n",
      "\tspeed: 0.1710s/iter; left time: 3626.6491s\n",
      "\titers: 200, epoch: 5 | loss: 0.0136321\n",
      "\tspeed: 0.0599s/iter; left time: 1264.7989s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:14.27s\n",
      "Steps: 222 | Train Loss: 0.0141358 Vali Loss: 0.0146385 Test Loss: 0.0185881\n",
      "Validation loss decreased (0.014765 --> 0.014638).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0140504\n",
      "\tspeed: 0.1821s/iter; left time: 3822.9595s\n",
      "\titers: 200, epoch: 6 | loss: 0.0138590\n",
      "\tspeed: 0.0689s/iter; left time: 1439.5175s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.57s\n",
      "Steps: 222 | Train Loss: 0.0138020 Vali Loss: 0.0146266 Test Loss: 0.0186628\n",
      "Validation loss decreased (0.014638 --> 0.014627).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0126132\n",
      "\tspeed: 0.1807s/iter; left time: 3753.0156s\n",
      "\titers: 200, epoch: 7 | loss: 0.0140159\n",
      "\tspeed: 0.0754s/iter; left time: 1558.7345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:15.26s\n",
      "Steps: 222 | Train Loss: 0.0135275 Vali Loss: 0.0145455 Test Loss: 0.0187065\n",
      "Validation loss decreased (0.014627 --> 0.014545).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0142885\n",
      "\tspeed: 0.1526s/iter; left time: 3135.4043s\n",
      "\titers: 200, epoch: 8 | loss: 0.0126322\n",
      "\tspeed: 0.0762s/iter; left time: 1558.5812s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:15.41s\n",
      "Steps: 222 | Train Loss: 0.0132833 Vali Loss: 0.0147672 Test Loss: 0.0186208\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0124638\n",
      "\tspeed: 0.1659s/iter; left time: 3372.6248s\n",
      "\titers: 200, epoch: 9 | loss: 0.0129195\n",
      "\tspeed: 0.0691s/iter; left time: 1397.5864s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:17.14s\n",
      "Steps: 222 | Train Loss: 0.0130904 Vali Loss: 0.0149191 Test Loss: 0.0186339\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0129059\n",
      "\tspeed: 0.1660s/iter; left time: 3336.3038s\n",
      "\titers: 200, epoch: 10 | loss: 0.0119503\n",
      "\tspeed: 0.0513s/iter; left time: 1026.3643s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:13.08s\n",
      "Steps: 222 | Train Loss: 0.0128996 Vali Loss: 0.0149892 Test Loss: 0.0186765\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0126177\n",
      "\tspeed: 0.1532s/iter; left time: 3045.4900s\n",
      "\titers: 200, epoch: 11 | loss: 0.0123047\n",
      "\tspeed: 0.0497s/iter; left time: 983.1155s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.99s\n",
      "Steps: 222 | Train Loss: 0.0127639 Vali Loss: 0.0151753 Test Loss: 0.0188451\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0131522\n",
      "\tspeed: 0.0869s/iter; left time: 1707.6281s\n",
      "\titers: 200, epoch: 12 | loss: 0.0129661\n",
      "\tspeed: 0.0567s/iter; left time: 1109.1159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.86s\n",
      "Steps: 222 | Train Loss: 0.0126158 Vali Loss: 0.0152723 Test Loss: 0.0189434\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0131074\n",
      "\tspeed: 0.1232s/iter; left time: 2395.4352s\n",
      "\titers: 200, epoch: 13 | loss: 0.0122381\n",
      "\tspeed: 0.0502s/iter; left time: 971.1137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:12.25s\n",
      "Steps: 222 | Train Loss: 0.0124981 Vali Loss: 0.0152723 Test Loss: 0.0189436\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0135793\n",
      "\tspeed: 0.1264s/iter; left time: 2428.3222s\n",
      "\titers: 200, epoch: 14 | loss: 0.0116055\n",
      "\tspeed: 0.0483s/iter; left time: 924.1186s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.66s\n",
      "Steps: 222 | Train Loss: 0.0123934 Vali Loss: 0.0155361 Test Loss: 0.0189617\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0115371\n",
      "\tspeed: 0.1276s/iter; left time: 2422.6855s\n",
      "\titers: 200, epoch: 15 | loss: 0.0138493\n",
      "\tspeed: 0.0331s/iter; left time: 624.8980s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:09.60s\n",
      "Steps: 222 | Train Loss: 0.0123032 Vali Loss: 0.0155056 Test Loss: 0.0190958\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0130373\n",
      "\tspeed: 0.0839s/iter; left time: 1575.5438s\n",
      "\titers: 200, epoch: 16 | loss: 0.0109443\n",
      "\tspeed: 0.0497s/iter; left time: 928.5545s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0121891 Vali Loss: 0.0157696 Test Loss: 0.0192633\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0115034\n",
      "\tspeed: 0.1272s/iter; left time: 2359.3282s\n",
      "\titers: 200, epoch: 17 | loss: 0.0117102\n",
      "\tspeed: 0.0581s/iter; left time: 1071.6693s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:12.63s\n",
      "Steps: 222 | Train Loss: 0.0120808 Vali Loss: 0.0154365 Test Loss: 0.0191169\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_512_96_ES_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018706467002630234, rmse:0.13677158951759338, mae:0.09075107425451279, rse:0.40179377794265747\n",
      "Intermediate time for ES and pred_len 96: 00h:06m:06.30s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_512_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_512_168_ES_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0279795\n",
      "\tspeed: 0.0677s/iter; left time: 1496.1061s\n",
      "\titers: 200, epoch: 1 | loss: 0.0230254\n",
      "\tspeed: 0.0555s/iter; left time: 1220.6097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:12.29s\n",
      "Steps: 222 | Train Loss: 0.0317384 Vali Loss: 0.0218558 Test Loss: 0.0284130\n",
      "Validation loss decreased (inf --> 0.021856).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0170325\n",
      "\tspeed: 0.1306s/iter; left time: 2856.6974s\n",
      "\titers: 200, epoch: 2 | loss: 0.0168607\n",
      "\tspeed: 0.0504s/iter; left time: 1096.6985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.99s\n",
      "Steps: 222 | Train Loss: 0.0186805 Vali Loss: 0.0176024 Test Loss: 0.0219750\n",
      "Validation loss decreased (0.021856 --> 0.017602).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0179503\n",
      "\tspeed: 0.1268s/iter; left time: 2747.1640s\n",
      "\titers: 200, epoch: 3 | loss: 0.0173319\n",
      "\tspeed: 0.0497s/iter; left time: 1070.5501s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:12.44s\n",
      "Steps: 222 | Train Loss: 0.0166084 Vali Loss: 0.0171047 Test Loss: 0.0214268\n",
      "Validation loss decreased (0.017602 --> 0.017105).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0171301\n",
      "\tspeed: 0.1339s/iter; left time: 2869.9073s\n",
      "\titers: 200, epoch: 4 | loss: 0.0153299\n",
      "\tspeed: 0.0494s/iter; left time: 1054.0750s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:12.44s\n",
      "Steps: 222 | Train Loss: 0.0159729 Vali Loss: 0.0167760 Test Loss: 0.0210731\n",
      "Validation loss decreased (0.017105 --> 0.016776).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0146820\n",
      "\tspeed: 0.1397s/iter; left time: 2962.6602s\n",
      "\titers: 200, epoch: 5 | loss: 0.0164592\n",
      "\tspeed: 0.0502s/iter; left time: 1059.9549s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.48s\n",
      "Steps: 222 | Train Loss: 0.0155208 Vali Loss: 0.0167824 Test Loss: 0.0206581\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0141154\n",
      "\tspeed: 0.1348s/iter; left time: 2829.7399s\n",
      "\titers: 200, epoch: 6 | loss: 0.0150462\n",
      "\tspeed: 0.0556s/iter; left time: 1162.4498s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:13.14s\n",
      "Steps: 222 | Train Loss: 0.0151302 Vali Loss: 0.0168366 Test Loss: 0.0207311\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0144720\n",
      "\tspeed: 0.1184s/iter; left time: 2458.3806s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150315\n",
      "\tspeed: 0.0559s/iter; left time: 1154.5225s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:12.08s\n",
      "Steps: 222 | Train Loss: 0.0148077 Vali Loss: 0.0174056 Test Loss: 0.0211542\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0165043\n",
      "\tspeed: 0.1275s/iter; left time: 2619.6452s\n",
      "\titers: 200, epoch: 8 | loss: 0.0153197\n",
      "\tspeed: 0.0544s/iter; left time: 1113.1346s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.68s\n",
      "Steps: 222 | Train Loss: 0.0145121 Vali Loss: 0.0173775 Test Loss: 0.0212291\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0138190\n",
      "\tspeed: 0.1234s/iter; left time: 2507.1282s\n",
      "\titers: 200, epoch: 9 | loss: 0.0155562\n",
      "\tspeed: 0.0483s/iter; left time: 976.3352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.06s\n",
      "Steps: 222 | Train Loss: 0.0142709 Vali Loss: 0.0170275 Test Loss: 0.0213013\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0142725\n",
      "\tspeed: 0.1369s/iter; left time: 2752.4958s\n",
      "\titers: 200, epoch: 10 | loss: 0.0142292\n",
      "\tspeed: 0.0558s/iter; left time: 1116.9348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.44s\n",
      "Steps: 222 | Train Loss: 0.0140482 Vali Loss: 0.0172849 Test Loss: 0.0211749\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0140199\n",
      "\tspeed: 0.1283s/iter; left time: 2551.2052s\n",
      "\titers: 200, epoch: 11 | loss: 0.0122982\n",
      "\tspeed: 0.0528s/iter; left time: 1044.6994s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.14s\n",
      "Steps: 222 | Train Loss: 0.0137952 Vali Loss: 0.0174233 Test Loss: 0.0212701\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0130280\n",
      "\tspeed: 0.1230s/iter; left time: 2417.1334s\n",
      "\titers: 200, epoch: 12 | loss: 0.0141811\n",
      "\tspeed: 0.0580s/iter; left time: 1134.2501s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:12.05s\n",
      "Steps: 222 | Train Loss: 0.0136176 Vali Loss: 0.0176614 Test Loss: 0.0213800\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0132042\n",
      "\tspeed: 0.0748s/iter; left time: 1454.7713s\n",
      "\titers: 200, epoch: 13 | loss: 0.0135097\n",
      "\tspeed: 0.0483s/iter; left time: 933.4039s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:09.72s\n",
      "Steps: 222 | Train Loss: 0.0134619 Vali Loss: 0.0177288 Test Loss: 0.0213930\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0134892\n",
      "\tspeed: 0.1276s/iter; left time: 2451.4706s\n",
      "\titers: 200, epoch: 14 | loss: 0.0120994\n",
      "\tspeed: 0.0462s/iter; left time: 883.2691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.77s\n",
      "Steps: 222 | Train Loss: 0.0132997 Vali Loss: 0.0177495 Test Loss: 0.0215108\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_512_168_ES_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02107308991253376, rmse:0.14516572654247284, mae:0.09893151372671127, rse:0.4264838695526123\n",
      "Intermediate time for ES and pred_len 168: 00h:04m:17.32s\n",
      "Intermediate time for ES: 00h:22m:01.85s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_512_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_512_24_FR_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0139474\n",
      "\tspeed: 0.0698s/iter; left time: 1548.8369s\n",
      "\titers: 200, epoch: 1 | loss: 0.0152327\n",
      "\tspeed: 0.0457s/iter; left time: 1010.8107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 223 | Train Loss: 0.0179653 Vali Loss: 0.0155975 Test Loss: 0.0186143\n",
      "Validation loss decreased (inf --> 0.015597).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0074836\n",
      "\tspeed: 0.0943s/iter; left time: 2072.9639s\n",
      "\titers: 200, epoch: 2 | loss: 0.0075474\n",
      "\tspeed: 0.0427s/iter; left time: 934.1118s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.16s\n",
      "Steps: 223 | Train Loss: 0.0079101 Vali Loss: 0.0097869 Test Loss: 0.0108252\n",
      "Validation loss decreased (0.015597 --> 0.009787).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0065753\n",
      "\tspeed: 0.0914s/iter; left time: 1989.4158s\n",
      "\titers: 200, epoch: 3 | loss: 0.0060550\n",
      "\tspeed: 0.0423s/iter; left time: 914.9831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.08s\n",
      "Steps: 223 | Train Loss: 0.0065123 Vali Loss: 0.0090169 Test Loss: 0.0104277\n",
      "Validation loss decreased (0.009787 --> 0.009017).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0064111\n",
      "\tspeed: 0.1011s/iter; left time: 2176.1686s\n",
      "\titers: 200, epoch: 4 | loss: 0.0066404\n",
      "\tspeed: 0.0474s/iter; left time: 1016.7465s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.60s\n",
      "Steps: 223 | Train Loss: 0.0061663 Vali Loss: 0.0089018 Test Loss: 0.0104195\n",
      "Validation loss decreased (0.009017 --> 0.008902).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0060630\n",
      "\tspeed: 0.1042s/iter; left time: 2219.3652s\n",
      "\titers: 200, epoch: 5 | loss: 0.0051825\n",
      "\tspeed: 0.0472s/iter; left time: 1001.7669s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0059630 Vali Loss: 0.0088152 Test Loss: 0.0102131\n",
      "Validation loss decreased (0.008902 --> 0.008815).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0049511\n",
      "\tspeed: 0.0912s/iter; left time: 1924.0069s\n",
      "\titers: 200, epoch: 6 | loss: 0.0058906\n",
      "\tspeed: 0.0448s/iter; left time: 940.2852s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.14s\n",
      "Steps: 223 | Train Loss: 0.0058262 Vali Loss: 0.0087123 Test Loss: 0.0102048\n",
      "Validation loss decreased (0.008815 --> 0.008712).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0052990\n",
      "\tspeed: 0.0937s/iter; left time: 1954.5444s\n",
      "\titers: 200, epoch: 7 | loss: 0.0053498\n",
      "\tspeed: 0.0440s/iter; left time: 914.4339s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.43s\n",
      "Steps: 223 | Train Loss: 0.0056651 Vali Loss: 0.0088441 Test Loss: 0.0102376\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0061529\n",
      "\tspeed: 0.0935s/iter; left time: 1929.3438s\n",
      "\titers: 200, epoch: 8 | loss: 0.0055662\n",
      "\tspeed: 0.0439s/iter; left time: 901.8342s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.12s\n",
      "Steps: 223 | Train Loss: 0.0055966 Vali Loss: 0.0086306 Test Loss: 0.0101378\n",
      "Validation loss decreased (0.008712 --> 0.008631).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0048350\n",
      "\tspeed: 0.0940s/iter; left time: 1919.4302s\n",
      "\titers: 200, epoch: 9 | loss: 0.0053921\n",
      "\tspeed: 0.0422s/iter; left time: 857.3832s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.37s\n",
      "Steps: 223 | Train Loss: 0.0054963 Vali Loss: 0.0086155 Test Loss: 0.0100501\n",
      "Validation loss decreased (0.008631 --> 0.008615).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0061372\n",
      "\tspeed: 0.0986s/iter; left time: 1991.5293s\n",
      "\titers: 200, epoch: 10 | loss: 0.0053672\n",
      "\tspeed: 0.0479s/iter; left time: 962.1932s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.66s\n",
      "Steps: 223 | Train Loss: 0.0054372 Vali Loss: 0.0087573 Test Loss: 0.0101747\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0064464\n",
      "\tspeed: 0.0939s/iter; left time: 1874.5958s\n",
      "\titers: 200, epoch: 11 | loss: 0.0057285\n",
      "\tspeed: 0.0454s/iter; left time: 902.2271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.46s\n",
      "Steps: 223 | Train Loss: 0.0053894 Vali Loss: 0.0086173 Test Loss: 0.0101487\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0065094\n",
      "\tspeed: 0.0944s/iter; left time: 1863.5860s\n",
      "\titers: 200, epoch: 12 | loss: 0.0056725\n",
      "\tspeed: 0.0446s/iter; left time: 876.6904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.20s\n",
      "Steps: 223 | Train Loss: 0.0053382 Vali Loss: 0.0085511 Test Loss: 0.0100537\n",
      "Validation loss decreased (0.008615 --> 0.008551).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0054896\n",
      "\tspeed: 0.0729s/iter; left time: 1424.0626s\n",
      "\titers: 200, epoch: 13 | loss: 0.0048544\n",
      "\tspeed: 0.0345s/iter; left time: 671.0629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:08.41s\n",
      "Steps: 223 | Train Loss: 0.0052761 Vali Loss: 0.0084969 Test Loss: 0.0101831\n",
      "Validation loss decreased (0.008551 --> 0.008497).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0044530\n",
      "\tspeed: 0.0951s/iter; left time: 1835.0276s\n",
      "\titers: 200, epoch: 14 | loss: 0.0053987\n",
      "\tspeed: 0.0444s/iter; left time: 853.3304s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.18s\n",
      "Steps: 223 | Train Loss: 0.0052514 Vali Loss: 0.0086520 Test Loss: 0.0102173\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0056224\n",
      "\tspeed: 0.0973s/iter; left time: 1856.5425s\n",
      "\titers: 200, epoch: 15 | loss: 0.0049725\n",
      "\tspeed: 0.0451s/iter; left time: 856.1393s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.37s\n",
      "Steps: 223 | Train Loss: 0.0052181 Vali Loss: 0.0085524 Test Loss: 0.0100507\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0048607\n",
      "\tspeed: 0.0983s/iter; left time: 1852.6386s\n",
      "\titers: 200, epoch: 16 | loss: 0.0057439\n",
      "\tspeed: 0.0495s/iter; left time: 929.0883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 223 | Train Loss: 0.0051846 Vali Loss: 0.0084813 Test Loss: 0.0100796\n",
      "Validation loss decreased (0.008497 --> 0.008481).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0058847\n",
      "\tspeed: 0.0952s/iter; left time: 1773.3453s\n",
      "\titers: 200, epoch: 17 | loss: 0.0051471\n",
      "\tspeed: 0.0429s/iter; left time: 794.7672s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.20s\n",
      "Steps: 223 | Train Loss: 0.0051597 Vali Loss: 0.0085043 Test Loss: 0.0101067\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0059730\n",
      "\tspeed: 0.0969s/iter; left time: 1784.4328s\n",
      "\titers: 200, epoch: 18 | loss: 0.0053032\n",
      "\tspeed: 0.0421s/iter; left time: 771.2538s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.39s\n",
      "Steps: 223 | Train Loss: 0.0051208 Vali Loss: 0.0085612 Test Loss: 0.0100721\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0046893\n",
      "\tspeed: 0.1018s/iter; left time: 1852.2072s\n",
      "\titers: 200, epoch: 19 | loss: 0.0055776\n",
      "\tspeed: 0.0507s/iter; left time: 917.4079s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.03s\n",
      "Steps: 223 | Train Loss: 0.0051015 Vali Loss: 0.0084870 Test Loss: 0.0101647\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0056610\n",
      "\tspeed: 0.0988s/iter; left time: 1775.0091s\n",
      "\titers: 200, epoch: 20 | loss: 0.0054683\n",
      "\tspeed: 0.0447s/iter; left time: 798.6681s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0050872 Vali Loss: 0.0085992 Test Loss: 0.0101133\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0050498\n",
      "\tspeed: 0.1118s/iter; left time: 1983.7229s\n",
      "\titers: 200, epoch: 21 | loss: 0.0050504\n",
      "\tspeed: 0.0488s/iter; left time: 861.5163s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 223 | Train Loss: 0.0050496 Vali Loss: 0.0084831 Test Loss: 0.0100783\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0051538\n",
      "\tspeed: 0.0973s/iter; left time: 1705.0191s\n",
      "\titers: 200, epoch: 22 | loss: 0.0054207\n",
      "\tspeed: 0.0539s/iter; left time: 938.6815s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.70s\n",
      "Steps: 223 | Train Loss: 0.0050444 Vali Loss: 0.0085164 Test Loss: 0.0101479\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0059541\n",
      "\tspeed: 0.1111s/iter; left time: 1922.1578s\n",
      "\titers: 200, epoch: 23 | loss: 0.0049745\n",
      "\tspeed: 0.0591s/iter; left time: 1016.6119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:13.36s\n",
      "Steps: 223 | Train Loss: 0.0050301 Vali Loss: 0.0085169 Test Loss: 0.0101683\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0051557\n",
      "\tspeed: 0.1213s/iter; left time: 2069.9953s\n",
      "\titers: 200, epoch: 24 | loss: 0.0052173\n",
      "\tspeed: 0.0565s/iter; left time: 959.6804s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:13.18s\n",
      "Steps: 223 | Train Loss: 0.0050013 Vali Loss: 0.0085068 Test Loss: 0.0101364\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0047271\n",
      "\tspeed: 0.1072s/iter; left time: 1806.8867s\n",
      "\titers: 200, epoch: 25 | loss: 0.0046135\n",
      "\tspeed: 0.0566s/iter; left time: 948.2323s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:12.30s\n",
      "Steps: 223 | Train Loss: 0.0050056 Vali Loss: 0.0085114 Test Loss: 0.0101567\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0051160\n",
      "\tspeed: 0.1116s/iter; left time: 1855.3557s\n",
      "\titers: 200, epoch: 26 | loss: 0.0045550\n",
      "\tspeed: 0.0562s/iter; left time: 928.6119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:12.27s\n",
      "Steps: 223 | Train Loss: 0.0049765 Vali Loss: 0.0084878 Test Loss: 0.0102236\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_512_24_FR_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010079635307192802, rmse:0.10039738565683365, mae:0.05860532075166702, rse:0.3873303234577179\n",
      "Intermediate time for FR and pred_len 24: 00h:06m:30.55s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_512_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_512_96_FR_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0182759\n",
      "\tspeed: 0.0886s/iter; left time: 1957.4676s\n",
      "\titers: 200, epoch: 1 | loss: 0.0136307\n",
      "\tspeed: 0.0754s/iter; left time: 1659.4243s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:15.99s\n",
      "Steps: 222 | Train Loss: 0.0194946 Vali Loss: 0.0182442 Test Loss: 0.0224482\n",
      "Validation loss decreased (inf --> 0.018244).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0132942\n",
      "\tspeed: 0.1587s/iter; left time: 3472.2644s\n",
      "\titers: 200, epoch: 2 | loss: 0.0107613\n",
      "\tspeed: 0.0679s/iter; left time: 1478.6079s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:15.58s\n",
      "Steps: 222 | Train Loss: 0.0117216 Vali Loss: 0.0150614 Test Loss: 0.0182255\n",
      "Validation loss decreased (0.018244 --> 0.015061).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0106593\n",
      "\tspeed: 0.1729s/iter; left time: 3744.5047s\n",
      "\titers: 200, epoch: 3 | loss: 0.0101106\n",
      "\tspeed: 0.0761s/iter; left time: 1640.5235s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:16.44s\n",
      "Steps: 222 | Train Loss: 0.0103542 Vali Loss: 0.0146145 Test Loss: 0.0182081\n",
      "Validation loss decreased (0.015061 --> 0.014615).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0093225\n",
      "\tspeed: 0.1812s/iter; left time: 3883.9121s\n",
      "\titers: 200, epoch: 4 | loss: 0.0102096\n",
      "\tspeed: 0.0567s/iter; left time: 1208.9379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:14.53s\n",
      "Steps: 222 | Train Loss: 0.0099825 Vali Loss: 0.0144292 Test Loss: 0.0187317\n",
      "Validation loss decreased (0.014615 --> 0.014429).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0102980\n",
      "\tspeed: 0.1906s/iter; left time: 4042.7359s\n",
      "\titers: 200, epoch: 5 | loss: 0.0092813\n",
      "\tspeed: 0.0623s/iter; left time: 1316.0374s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:14.99s\n",
      "Steps: 222 | Train Loss: 0.0097175 Vali Loss: 0.0139331 Test Loss: 0.0185891\n",
      "Validation loss decreased (0.014429 --> 0.013933).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0106928\n",
      "\tspeed: 0.1913s/iter; left time: 4015.9166s\n",
      "\titers: 200, epoch: 6 | loss: 0.0093496\n",
      "\tspeed: 0.0666s/iter; left time: 1390.7691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:15.70s\n",
      "Steps: 222 | Train Loss: 0.0095001 Vali Loss: 0.0141520 Test Loss: 0.0186459\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0088644\n",
      "\tspeed: 0.1718s/iter; left time: 3567.1714s\n",
      "\titers: 200, epoch: 7 | loss: 0.0091242\n",
      "\tspeed: 0.0658s/iter; left time: 1360.6947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:15.01s\n",
      "Steps: 222 | Train Loss: 0.0093526 Vali Loss: 0.0137556 Test Loss: 0.0188724\n",
      "Validation loss decreased (0.013933 --> 0.013756).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0093158\n",
      "\tspeed: 0.1643s/iter; left time: 3376.7363s\n",
      "\titers: 200, epoch: 8 | loss: 0.0089462\n",
      "\tspeed: 0.0803s/iter; left time: 1641.9772s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:15.77s\n",
      "Steps: 222 | Train Loss: 0.0092001 Vali Loss: 0.0137557 Test Loss: 0.0188953\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0090415\n",
      "\tspeed: 0.1802s/iter; left time: 3662.1848s\n",
      "\titers: 200, epoch: 9 | loss: 0.0085889\n",
      "\tspeed: 0.0673s/iter; left time: 1362.0295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:15.45s\n",
      "Steps: 222 | Train Loss: 0.0090813 Vali Loss: 0.0137663 Test Loss: 0.0190483\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0102218\n",
      "\tspeed: 0.1626s/iter; left time: 3268.3624s\n",
      "\titers: 200, epoch: 10 | loss: 0.0094605\n",
      "\tspeed: 0.0500s/iter; left time: 999.4367s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:13.88s\n",
      "Steps: 222 | Train Loss: 0.0089345 Vali Loss: 0.0137887 Test Loss: 0.0192490\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0087403\n",
      "\tspeed: 0.0874s/iter; left time: 1736.7222s\n",
      "\titers: 200, epoch: 11 | loss: 0.0088007\n",
      "\tspeed: 0.0685s/iter; left time: 1354.2796s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:13.70s\n",
      "Steps: 222 | Train Loss: 0.0088300 Vali Loss: 0.0139091 Test Loss: 0.0193859\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0079034\n",
      "\tspeed: 0.1981s/iter; left time: 3894.9521s\n",
      "\titers: 200, epoch: 12 | loss: 0.0079249\n",
      "\tspeed: 0.0738s/iter; left time: 1442.8433s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:15.31s\n",
      "Steps: 222 | Train Loss: 0.0087076 Vali Loss: 0.0138342 Test Loss: 0.0193991\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0080209\n",
      "\tspeed: 0.1671s/iter; left time: 3248.0543s\n",
      "\titers: 200, epoch: 13 | loss: 0.0088310\n",
      "\tspeed: 0.0723s/iter; left time: 1397.9389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:15.71s\n",
      "Steps: 222 | Train Loss: 0.0085859 Vali Loss: 0.0137632 Test Loss: 0.0198410\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0075823\n",
      "\tspeed: 0.1854s/iter; left time: 3561.9047s\n",
      "\titers: 200, epoch: 14 | loss: 0.0087876\n",
      "\tspeed: 0.0703s/iter; left time: 1343.9565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:16.70s\n",
      "Steps: 222 | Train Loss: 0.0084653 Vali Loss: 0.0138948 Test Loss: 0.0201002\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0094210\n",
      "\tspeed: 0.1748s/iter; left time: 3320.7223s\n",
      "\titers: 200, epoch: 15 | loss: 0.0087737\n",
      "\tspeed: 0.0641s/iter; left time: 1210.2716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:15.69s\n",
      "Steps: 222 | Train Loss: 0.0083676 Vali Loss: 0.0138664 Test Loss: 0.0202982\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0092054\n",
      "\tspeed: 0.1861s/iter; left time: 3492.7968s\n",
      "\titers: 200, epoch: 16 | loss: 0.0074394\n",
      "\tspeed: 0.0757s/iter; left time: 1414.2664s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:16.40s\n",
      "Steps: 222 | Train Loss: 0.0082579 Vali Loss: 0.0139795 Test Loss: 0.0206525\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0076474\n",
      "\tspeed: 0.1700s/iter; left time: 3152.5739s\n",
      "\titers: 200, epoch: 17 | loss: 0.0090378\n",
      "\tspeed: 0.0356s/iter; left time: 656.0772s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:09.65s\n",
      "Steps: 222 | Train Loss: 0.0081546 Vali Loss: 0.0140648 Test Loss: 0.0209167\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_512_96_FR_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01887238211929798, rmse:0.1373767852783203, mae:0.08460848778486252, rse:0.5314099788665771\n",
      "Intermediate time for FR and pred_len 96: 00h:06m:53.63s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='FR_512_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : FR_512_168_FR_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0181693\n",
      "\tspeed: 0.0700s/iter; left time: 1548.1167s\n",
      "\titers: 200, epoch: 1 | loss: 0.0165501\n",
      "\tspeed: 0.0577s/iter; left time: 1269.8642s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:12.97s\n",
      "Steps: 222 | Train Loss: 0.0202903 Vali Loss: 0.0191721 Test Loss: 0.0224243\n",
      "Validation loss decreased (inf --> 0.019172).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0110667\n",
      "\tspeed: 0.1219s/iter; left time: 2666.3063s\n",
      "\titers: 200, epoch: 2 | loss: 0.0115115\n",
      "\tspeed: 0.0607s/iter; left time: 1321.4528s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:13.50s\n",
      "Steps: 222 | Train Loss: 0.0128515 Vali Loss: 0.0161907 Test Loss: 0.0197017\n",
      "Validation loss decreased (0.019172 --> 0.016191).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0111191\n",
      "\tspeed: 0.1349s/iter; left time: 2921.2169s\n",
      "\titers: 200, epoch: 3 | loss: 0.0118327\n",
      "\tspeed: 0.0555s/iter; left time: 1196.4446s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:12.68s\n",
      "Steps: 222 | Train Loss: 0.0114029 Vali Loss: 0.0157406 Test Loss: 0.0194935\n",
      "Validation loss decreased (0.016191 --> 0.015741).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120790\n",
      "\tspeed: 0.1436s/iter; left time: 3077.1059s\n",
      "\titers: 200, epoch: 4 | loss: 0.0108056\n",
      "\tspeed: 0.0545s/iter; left time: 1163.6321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:13.41s\n",
      "Steps: 222 | Train Loss: 0.0110045 Vali Loss: 0.0153501 Test Loss: 0.0197299\n",
      "Validation loss decreased (0.015741 --> 0.015350).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0099571\n",
      "\tspeed: 0.1542s/iter; left time: 3271.7624s\n",
      "\titers: 200, epoch: 5 | loss: 0.0119233\n",
      "\tspeed: 0.0643s/iter; left time: 1357.2504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.82s\n",
      "Steps: 222 | Train Loss: 0.0106981 Vali Loss: 0.0152353 Test Loss: 0.0200679\n",
      "Validation loss decreased (0.015350 --> 0.015235).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0098806\n",
      "\tspeed: 0.0959s/iter; left time: 2012.2605s\n",
      "\titers: 200, epoch: 6 | loss: 0.0112916\n",
      "\tspeed: 0.0373s/iter; left time: 779.0922s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.85s\n",
      "Steps: 222 | Train Loss: 0.0104270 Vali Loss: 0.0152871 Test Loss: 0.0212898\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0099506\n",
      "\tspeed: 0.0921s/iter; left time: 1913.2112s\n",
      "\titers: 200, epoch: 7 | loss: 0.0113875\n",
      "\tspeed: 0.0523s/iter; left time: 1081.8780s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0102166 Vali Loss: 0.0151320 Test Loss: 0.0212015\n",
      "Validation loss decreased (0.015235 --> 0.015132).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0101450\n",
      "\tspeed: 0.1284s/iter; left time: 2638.7069s\n",
      "\titers: 200, epoch: 8 | loss: 0.0091039\n",
      "\tspeed: 0.0537s/iter; left time: 1097.1226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:13.09s\n",
      "Steps: 222 | Train Loss: 0.0099794 Vali Loss: 0.0149879 Test Loss: 0.0219970\n",
      "Validation loss decreased (0.015132 --> 0.014988).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0089927\n",
      "\tspeed: 0.1359s/iter; left time: 2762.2257s\n",
      "\titers: 200, epoch: 9 | loss: 0.0100710\n",
      "\tspeed: 0.0554s/iter; left time: 1121.2593s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.42s\n",
      "Steps: 222 | Train Loss: 0.0097713 Vali Loss: 0.0151657 Test Loss: 0.0218595\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0093267\n",
      "\tspeed: 0.1316s/iter; left time: 2644.6808s\n",
      "\titers: 200, epoch: 10 | loss: 0.0093912\n",
      "\tspeed: 0.0564s/iter; left time: 1127.4690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.59s\n",
      "Steps: 222 | Train Loss: 0.0095677 Vali Loss: 0.0151607 Test Loss: 0.0219700\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0101388\n",
      "\tspeed: 0.1359s/iter; left time: 2701.7309s\n",
      "\titers: 200, epoch: 11 | loss: 0.0090199\n",
      "\tspeed: 0.0511s/iter; left time: 1011.3051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:12.22s\n",
      "Steps: 222 | Train Loss: 0.0093663 Vali Loss: 0.0152078 Test Loss: 0.0221877\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0098213\n",
      "\tspeed: 0.1303s/iter; left time: 2561.0525s\n",
      "\titers: 200, epoch: 12 | loss: 0.0089773\n",
      "\tspeed: 0.0510s/iter; left time: 997.0589s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:12.24s\n",
      "Steps: 222 | Train Loss: 0.0091947 Vali Loss: 0.0153834 Test Loss: 0.0226582\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0084364\n",
      "\tspeed: 0.1254s/iter; left time: 2437.5442s\n",
      "\titers: 200, epoch: 13 | loss: 0.0094313\n",
      "\tspeed: 0.0565s/iter; left time: 1093.2007s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.64s\n",
      "Steps: 222 | Train Loss: 0.0090600 Vali Loss: 0.0153617 Test Loss: 0.0220547\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0080867\n",
      "\tspeed: 0.1172s/iter; left time: 2252.3610s\n",
      "\titers: 200, epoch: 14 | loss: 0.0095892\n",
      "\tspeed: 0.0495s/iter; left time: 946.1586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.87s\n",
      "Steps: 222 | Train Loss: 0.0089167 Vali Loss: 0.0156004 Test Loss: 0.0225311\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0081574\n",
      "\tspeed: 0.1398s/iter; left time: 2655.6132s\n",
      "\titers: 200, epoch: 15 | loss: 0.0082599\n",
      "\tspeed: 0.0510s/iter; left time: 964.4388s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.59s\n",
      "Steps: 222 | Train Loss: 0.0087950 Vali Loss: 0.0157697 Test Loss: 0.0223386\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0090888\n",
      "\tspeed: 0.1194s/iter; left time: 2241.2991s\n",
      "\titers: 200, epoch: 16 | loss: 0.0088899\n",
      "\tspeed: 0.0600s/iter; left time: 1120.9604s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:12.25s\n",
      "Steps: 222 | Train Loss: 0.0086991 Vali Loss: 0.0157934 Test Loss: 0.0226600\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0088229\n",
      "\tspeed: 0.1293s/iter; left time: 2398.0381s\n",
      "\titers: 200, epoch: 17 | loss: 0.0090584\n",
      "\tspeed: 0.0467s/iter; left time: 860.7947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:12.33s\n",
      "Steps: 222 | Train Loss: 0.0085768 Vali Loss: 0.0158606 Test Loss: 0.0228027\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0093302\n",
      "\tspeed: 0.1314s/iter; left time: 2408.5524s\n",
      "\titers: 200, epoch: 18 | loss: 0.0087869\n",
      "\tspeed: 0.0503s/iter; left time: 916.4770s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.52s\n",
      "Steps: 222 | Train Loss: 0.0084856 Vali Loss: 0.0158655 Test Loss: 0.0223761\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : FR_512_168_FR_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02199699729681015, rmse:0.14831385016441345, mae:0.09235343337059021, rse:0.5744336247444153\n",
      "Intermediate time for FR and pred_len 168: 00h:05m:35.86s\n",
      "Intermediate time for FR: 00h:19m:00.04s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_512_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_512_24_IT_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0273320\n",
      "\tspeed: 0.0840s/iter; left time: 1864.9467s\n",
      "\titers: 200, epoch: 1 | loss: 0.0246757\n",
      "\tspeed: 0.0495s/iter; left time: 1094.1268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 223 | Train Loss: 0.0328999 Vali Loss: 0.0188083 Test Loss: 0.0194664\n",
      "Validation loss decreased (inf --> 0.018808).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0111637\n",
      "\tspeed: 0.0959s/iter; left time: 2107.1710s\n",
      "\titers: 200, epoch: 2 | loss: 0.0116641\n",
      "\tspeed: 0.0487s/iter; left time: 1064.8915s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.07s\n",
      "Steps: 223 | Train Loss: 0.0135100 Vali Loss: 0.0103937 Test Loss: 0.0115777\n",
      "Validation loss decreased (0.018808 --> 0.010394).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0112492\n",
      "\tspeed: 0.1018s/iter; left time: 2214.8833s\n",
      "\titers: 200, epoch: 3 | loss: 0.0116855\n",
      "\tspeed: 0.0437s/iter; left time: 945.7141s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.80s\n",
      "Steps: 223 | Train Loss: 0.0107398 Vali Loss: 0.0096690 Test Loss: 0.0107840\n",
      "Validation loss decreased (0.010394 --> 0.009669).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0098871\n",
      "\tspeed: 0.0999s/iter; left time: 2150.9712s\n",
      "\titers: 200, epoch: 4 | loss: 0.0104576\n",
      "\tspeed: 0.0451s/iter; left time: 966.8255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.32s\n",
      "Steps: 223 | Train Loss: 0.0101198 Vali Loss: 0.0095668 Test Loss: 0.0105108\n",
      "Validation loss decreased (0.009669 --> 0.009567).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0087842\n",
      "\tspeed: 0.0993s/iter; left time: 2116.4694s\n",
      "\titers: 200, epoch: 5 | loss: 0.0094138\n",
      "\tspeed: 0.0519s/iter; left time: 1100.0677s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.18s\n",
      "Steps: 223 | Train Loss: 0.0097978 Vali Loss: 0.0095359 Test Loss: 0.0105608\n",
      "Validation loss decreased (0.009567 --> 0.009536).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0100069\n",
      "\tspeed: 0.0921s/iter; left time: 1942.0215s\n",
      "\titers: 200, epoch: 6 | loss: 0.0091311\n",
      "\tspeed: 0.0500s/iter; left time: 1049.8534s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.68s\n",
      "Steps: 223 | Train Loss: 0.0095830 Vali Loss: 0.0092272 Test Loss: 0.0103869\n",
      "Validation loss decreased (0.009536 --> 0.009227).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0100961\n",
      "\tspeed: 0.0984s/iter; left time: 2052.0670s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092195\n",
      "\tspeed: 0.0447s/iter; left time: 928.0385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.63s\n",
      "Steps: 223 | Train Loss: 0.0093842 Vali Loss: 0.0093546 Test Loss: 0.0103526\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0091741\n",
      "\tspeed: 0.1006s/iter; left time: 2076.8922s\n",
      "\titers: 200, epoch: 8 | loss: 0.0104509\n",
      "\tspeed: 0.0433s/iter; left time: 888.9460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.45s\n",
      "Steps: 223 | Train Loss: 0.0092764 Vali Loss: 0.0092418 Test Loss: 0.0102622\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0085590\n",
      "\tspeed: 0.1022s/iter; left time: 2086.7352s\n",
      "\titers: 200, epoch: 9 | loss: 0.0079136\n",
      "\tspeed: 0.0438s/iter; left time: 889.7078s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.37s\n",
      "Steps: 223 | Train Loss: 0.0091449 Vali Loss: 0.0091599 Test Loss: 0.0101425\n",
      "Validation loss decreased (0.009227 --> 0.009160).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0088278\n",
      "\tspeed: 0.1002s/iter; left time: 2024.0394s\n",
      "\titers: 200, epoch: 10 | loss: 0.0081916\n",
      "\tspeed: 0.0480s/iter; left time: 964.7495s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.64s\n",
      "Steps: 223 | Train Loss: 0.0090224 Vali Loss: 0.0090863 Test Loss: 0.0101605\n",
      "Validation loss decreased (0.009160 --> 0.009086).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0086446\n",
      "\tspeed: 0.0987s/iter; left time: 1970.7398s\n",
      "\titers: 200, epoch: 11 | loss: 0.0080936\n",
      "\tspeed: 0.0469s/iter; left time: 931.5691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.58s\n",
      "Steps: 223 | Train Loss: 0.0089498 Vali Loss: 0.0090250 Test Loss: 0.0101294\n",
      "Validation loss decreased (0.009086 --> 0.009025).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0095482\n",
      "\tspeed: 0.0989s/iter; left time: 1953.9930s\n",
      "\titers: 200, epoch: 12 | loss: 0.0089409\n",
      "\tspeed: 0.0427s/iter; left time: 838.3775s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.31s\n",
      "Steps: 223 | Train Loss: 0.0088589 Vali Loss: 0.0090217 Test Loss: 0.0100984\n",
      "Validation loss decreased (0.009025 --> 0.009022).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0081603\n",
      "\tspeed: 0.0979s/iter; left time: 1911.3676s\n",
      "\titers: 200, epoch: 13 | loss: 0.0104349\n",
      "\tspeed: 0.0433s/iter; left time: 841.9194s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.53s\n",
      "Steps: 223 | Train Loss: 0.0087785 Vali Loss: 0.0090420 Test Loss: 0.0101082\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0097593\n",
      "\tspeed: 0.1011s/iter; left time: 1951.0345s\n",
      "\titers: 200, epoch: 14 | loss: 0.0083543\n",
      "\tspeed: 0.0439s/iter; left time: 843.1115s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.44s\n",
      "Steps: 223 | Train Loss: 0.0087266 Vali Loss: 0.0089880 Test Loss: 0.0101084\n",
      "Validation loss decreased (0.009022 --> 0.008988).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0088825\n",
      "\tspeed: 0.0988s/iter; left time: 1885.7763s\n",
      "\titers: 200, epoch: 15 | loss: 0.0091031\n",
      "\tspeed: 0.0463s/iter; left time: 879.1058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.33s\n",
      "Steps: 223 | Train Loss: 0.0086749 Vali Loss: 0.0089221 Test Loss: 0.0101701\n",
      "Validation loss decreased (0.008988 --> 0.008922).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0091498\n",
      "\tspeed: 0.0965s/iter; left time: 1819.1325s\n",
      "\titers: 200, epoch: 16 | loss: 0.0100686\n",
      "\tspeed: 0.0378s/iter; left time: 709.8075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:09.42s\n",
      "Steps: 223 | Train Loss: 0.0086129 Vali Loss: 0.0088873 Test Loss: 0.0101329\n",
      "Validation loss decreased (0.008922 --> 0.008887).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0078947\n",
      "\tspeed: 0.0607s/iter; left time: 1131.2047s\n",
      "\titers: 200, epoch: 17 | loss: 0.0079249\n",
      "\tspeed: 0.0330s/iter; left time: 612.4789s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 223 | Train Loss: 0.0085479 Vali Loss: 0.0089116 Test Loss: 0.0100765\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0090177\n",
      "\tspeed: 0.0725s/iter; left time: 1334.8657s\n",
      "\titers: 200, epoch: 18 | loss: 0.0087904\n",
      "\tspeed: 0.0370s/iter; left time: 677.4285s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 223 | Train Loss: 0.0085240 Vali Loss: 0.0089549 Test Loss: 0.0102258\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0076368\n",
      "\tspeed: 0.0705s/iter; left time: 1281.3778s\n",
      "\titers: 200, epoch: 19 | loss: 0.0090357\n",
      "\tspeed: 0.0319s/iter; left time: 577.0389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.84s\n",
      "Steps: 223 | Train Loss: 0.0084777 Vali Loss: 0.0088796 Test Loss: 0.0100593\n",
      "Validation loss decreased (0.008887 --> 0.008880).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0081102\n",
      "\tspeed: 0.0715s/iter; left time: 1285.0070s\n",
      "\titers: 200, epoch: 20 | loss: 0.0094632\n",
      "\tspeed: 0.0359s/iter; left time: 641.7815s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 223 | Train Loss: 0.0084305 Vali Loss: 0.0088695 Test Loss: 0.0101159\n",
      "Validation loss decreased (0.008880 --> 0.008870).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0082413\n",
      "\tspeed: 0.0729s/iter; left time: 1294.0413s\n",
      "\titers: 200, epoch: 21 | loss: 0.0081291\n",
      "\tspeed: 0.0335s/iter; left time: 590.5968s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.09s\n",
      "Steps: 223 | Train Loss: 0.0083819 Vali Loss: 0.0088569 Test Loss: 0.0101416\n",
      "Validation loss decreased (0.008870 --> 0.008857).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0077153\n",
      "\tspeed: 0.0710s/iter; left time: 1243.7715s\n",
      "\titers: 200, epoch: 22 | loss: 0.0090150\n",
      "\tspeed: 0.0355s/iter; left time: 618.4595s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 223 | Train Loss: 0.0083680 Vali Loss: 0.0088447 Test Loss: 0.0100754\n",
      "Validation loss decreased (0.008857 --> 0.008845).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0075170\n",
      "\tspeed: 0.0694s/iter; left time: 1200.7623s\n",
      "\titers: 200, epoch: 23 | loss: 0.0087678\n",
      "\tspeed: 0.0317s/iter; left time: 545.6906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:07.79s\n",
      "Steps: 223 | Train Loss: 0.0083344 Vali Loss: 0.0088640 Test Loss: 0.0100877\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0087821\n",
      "\tspeed: 0.0706s/iter; left time: 1204.4766s\n",
      "\titers: 200, epoch: 24 | loss: 0.0078058\n",
      "\tspeed: 0.0365s/iter; left time: 619.7385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 223 | Train Loss: 0.0083262 Vali Loss: 0.0088149 Test Loss: 0.0101287\n",
      "Validation loss decreased (0.008845 --> 0.008815).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0083511\n",
      "\tspeed: 0.0716s/iter; left time: 1207.2103s\n",
      "\titers: 200, epoch: 25 | loss: 0.0083889\n",
      "\tspeed: 0.0331s/iter; left time: 555.1440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:07.92s\n",
      "Steps: 223 | Train Loss: 0.0082961 Vali Loss: 0.0087726 Test Loss: 0.0101147\n",
      "Validation loss decreased (0.008815 --> 0.008773).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0082952\n",
      "\tspeed: 0.0731s/iter; left time: 1215.7095s\n",
      "\titers: 200, epoch: 26 | loss: 0.0085091\n",
      "\tspeed: 0.0357s/iter; left time: 589.3165s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 223 | Train Loss: 0.0082870 Vali Loss: 0.0088303 Test Loss: 0.0101144\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0072543\n",
      "\tspeed: 0.0686s/iter; left time: 1124.9133s\n",
      "\titers: 200, epoch: 27 | loss: 0.0077673\n",
      "\tspeed: 0.0321s/iter; left time: 523.5074s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:07.80s\n",
      "Steps: 223 | Train Loss: 0.0082666 Vali Loss: 0.0088144 Test Loss: 0.0101097\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0081979\n",
      "\tspeed: 0.0724s/iter; left time: 1171.4920s\n",
      "\titers: 200, epoch: 28 | loss: 0.0084787\n",
      "\tspeed: 0.0355s/iter; left time: 571.5293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:08.25s\n",
      "Steps: 223 | Train Loss: 0.0082338 Vali Loss: 0.0088477 Test Loss: 0.0101129\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0076726\n",
      "\tspeed: 0.0705s/iter; left time: 1124.7060s\n",
      "\titers: 200, epoch: 29 | loss: 0.0080891\n",
      "\tspeed: 0.0335s/iter; left time: 531.4736s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.80s\n",
      "Steps: 223 | Train Loss: 0.0082228 Vali Loss: 0.0087939 Test Loss: 0.0101104\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0073676\n",
      "\tspeed: 0.0696s/iter; left time: 1094.5636s\n",
      "\titers: 200, epoch: 30 | loss: 0.0066830\n",
      "\tspeed: 0.0345s/iter; left time: 538.6328s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.98s\n",
      "Steps: 223 | Train Loss: 0.0082001 Vali Loss: 0.0088340 Test Loss: 0.0100905\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0083259\n",
      "\tspeed: 0.0695s/iter; left time: 1077.8398s\n",
      "\titers: 200, epoch: 31 | loss: 0.0083007\n",
      "\tspeed: 0.0344s/iter; left time: 530.7373s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0081792 Vali Loss: 0.0088265 Test Loss: 0.0101018\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0089704\n",
      "\tspeed: 0.0734s/iter; left time: 1122.4049s\n",
      "\titers: 200, epoch: 32 | loss: 0.0086215\n",
      "\tspeed: 0.0344s/iter; left time: 522.2726s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.24s\n",
      "Steps: 223 | Train Loss: 0.0081837 Vali Loss: 0.0088093 Test Loss: 0.0101177\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0068758\n",
      "\tspeed: 0.0691s/iter; left time: 1040.8913s\n",
      "\titers: 200, epoch: 33 | loss: 0.0082407\n",
      "\tspeed: 0.0337s/iter; left time: 504.7904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:07.70s\n",
      "Steps: 223 | Train Loss: 0.0081747 Vali Loss: 0.0088218 Test Loss: 0.0101026\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0074084\n",
      "\tspeed: 0.0586s/iter; left time: 870.3408s\n",
      "\titers: 200, epoch: 34 | loss: 0.0087486\n",
      "\tspeed: 0.0299s/iter; left time: 440.4218s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.00s\n",
      "Steps: 223 | Train Loss: 0.0081637 Vali Loss: 0.0088152 Test Loss: 0.0101121\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0068415\n",
      "\tspeed: 0.0700s/iter; left time: 1023.4825s\n",
      "\titers: 200, epoch: 35 | loss: 0.0088102\n",
      "\tspeed: 0.0329s/iter; left time: 477.3415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:08.10s\n",
      "Steps: 223 | Train Loss: 0.0081431 Vali Loss: 0.0087956 Test Loss: 0.0101007\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_512_24_IT_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01011473499238491, rmse:0.10057204216718674, mae:0.05913744866847992, rse:0.3800122141838074\n",
      "Intermediate time for IT and pred_len 24: 00h:07m:20.19s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_512_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_512_96_IT_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0328526\n",
      "\tspeed: 0.0505s/iter; left time: 1116.1392s\n",
      "\titers: 200, epoch: 1 | loss: 0.0254075\n",
      "\tspeed: 0.0377s/iter; left time: 828.9873s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.53s\n",
      "Steps: 222 | Train Loss: 0.0353576 Vali Loss: 0.0222836 Test Loss: 0.0232563\n",
      "Validation loss decreased (inf --> 0.022284).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0167053\n",
      "\tspeed: 0.0761s/iter; left time: 1665.2625s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166181\n",
      "\tspeed: 0.0334s/iter; left time: 727.5759s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.29s\n",
      "Steps: 222 | Train Loss: 0.0199959 Vali Loss: 0.0168431 Test Loss: 0.0179398\n",
      "Validation loss decreased (0.022284 --> 0.016843).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0173453\n",
      "\tspeed: 0.0786s/iter; left time: 1702.2580s\n",
      "\titers: 200, epoch: 3 | loss: 0.0149159\n",
      "\tspeed: 0.0390s/iter; left time: 840.1045s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.43s\n",
      "Steps: 222 | Train Loss: 0.0172793 Vali Loss: 0.0164697 Test Loss: 0.0176154\n",
      "Validation loss decreased (0.016843 --> 0.016470).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0155057\n",
      "\tspeed: 0.0809s/iter; left time: 1734.8332s\n",
      "\titers: 200, epoch: 4 | loss: 0.0164714\n",
      "\tspeed: 0.0339s/iter; left time: 723.5237s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.71s\n",
      "Steps: 222 | Train Loss: 0.0166619 Vali Loss: 0.0163459 Test Loss: 0.0176644\n",
      "Validation loss decreased (0.016470 --> 0.016346).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0169420\n",
      "\tspeed: 0.0799s/iter; left time: 1694.2728s\n",
      "\titers: 200, epoch: 5 | loss: 0.0158778\n",
      "\tspeed: 0.0425s/iter; left time: 898.0723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.04s\n",
      "Steps: 222 | Train Loss: 0.0162536 Vali Loss: 0.0161029 Test Loss: 0.0174955\n",
      "Validation loss decreased (0.016346 --> 0.016103).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0159567\n",
      "\tspeed: 0.0846s/iter; left time: 1775.2791s\n",
      "\titers: 200, epoch: 6 | loss: 0.0160744\n",
      "\tspeed: 0.0414s/iter; left time: 865.4338s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.98s\n",
      "Steps: 222 | Train Loss: 0.0159415 Vali Loss: 0.0160482 Test Loss: 0.0174880\n",
      "Validation loss decreased (0.016103 --> 0.016048).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0153478\n",
      "\tspeed: 0.0865s/iter; left time: 1795.8849s\n",
      "\titers: 200, epoch: 7 | loss: 0.0162646\n",
      "\tspeed: 0.0413s/iter; left time: 853.5581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.50s\n",
      "Steps: 222 | Train Loss: 0.0156396 Vali Loss: 0.0161996 Test Loss: 0.0176509\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0159476\n",
      "\tspeed: 0.0898s/iter; left time: 1845.1040s\n",
      "\titers: 200, epoch: 8 | loss: 0.0155621\n",
      "\tspeed: 0.0433s/iter; left time: 885.4117s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.12s\n",
      "Steps: 222 | Train Loss: 0.0153464 Vali Loss: 0.0160835 Test Loss: 0.0175796\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0140143\n",
      "\tspeed: 0.0932s/iter; left time: 1894.4672s\n",
      "\titers: 200, epoch: 9 | loss: 0.0149326\n",
      "\tspeed: 0.0391s/iter; left time: 790.8574s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:09.51s\n",
      "Steps: 222 | Train Loss: 0.0151106 Vali Loss: 0.0157193 Test Loss: 0.0176988\n",
      "Validation loss decreased (0.016048 --> 0.015719).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0167112\n",
      "\tspeed: 0.0859s/iter; left time: 1727.7516s\n",
      "\titers: 200, epoch: 10 | loss: 0.0142206\n",
      "\tspeed: 0.0416s/iter; left time: 832.0292s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.91s\n",
      "Steps: 222 | Train Loss: 0.0148664 Vali Loss: 0.0158889 Test Loss: 0.0179928\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0145628\n",
      "\tspeed: 0.0775s/iter; left time: 1540.5344s\n",
      "\titers: 200, epoch: 11 | loss: 0.0142519\n",
      "\tspeed: 0.0376s/iter; left time: 743.5674s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:08.90s\n",
      "Steps: 222 | Train Loss: 0.0146757 Vali Loss: 0.0158334 Test Loss: 0.0179947\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0137772\n",
      "\tspeed: 0.0861s/iter; left time: 1692.0137s\n",
      "\titers: 200, epoch: 12 | loss: 0.0146857\n",
      "\tspeed: 0.0431s/iter; left time: 842.7824s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:09.64s\n",
      "Steps: 222 | Train Loss: 0.0144614 Vali Loss: 0.0160475 Test Loss: 0.0179842\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0151137\n",
      "\tspeed: 0.0876s/iter; left time: 1701.8371s\n",
      "\titers: 200, epoch: 13 | loss: 0.0155950\n",
      "\tspeed: 0.0420s/iter; left time: 812.4192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:09.49s\n",
      "Steps: 222 | Train Loss: 0.0142666 Vali Loss: 0.0160337 Test Loss: 0.0182746\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0131219\n",
      "\tspeed: 0.0811s/iter; left time: 1557.6165s\n",
      "\titers: 200, epoch: 14 | loss: 0.0132504\n",
      "\tspeed: 0.0306s/iter; left time: 584.2368s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:08.16s\n",
      "Steps: 222 | Train Loss: 0.0140905 Vali Loss: 0.0158869 Test Loss: 0.0183412\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0138993\n",
      "\tspeed: 0.0622s/iter; left time: 1181.2519s\n",
      "\titers: 200, epoch: 15 | loss: 0.0121357\n",
      "\tspeed: 0.0307s/iter; left time: 579.5455s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.18s\n",
      "Steps: 222 | Train Loss: 0.0139332 Vali Loss: 0.0162174 Test Loss: 0.0185106\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0142256\n",
      "\tspeed: 0.0605s/iter; left time: 1136.2888s\n",
      "\titers: 200, epoch: 16 | loss: 0.0147980\n",
      "\tspeed: 0.0309s/iter; left time: 576.0503s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.15s\n",
      "Steps: 222 | Train Loss: 0.0137546 Vali Loss: 0.0161273 Test Loss: 0.0187841\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0136915\n",
      "\tspeed: 0.0596s/iter; left time: 1105.9175s\n",
      "\titers: 200, epoch: 17 | loss: 0.0133169\n",
      "\tspeed: 0.0328s/iter; left time: 605.8494s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.39s\n",
      "Steps: 222 | Train Loss: 0.0136617 Vali Loss: 0.0161170 Test Loss: 0.0186378\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0142514\n",
      "\tspeed: 0.0621s/iter; left time: 1137.2267s\n",
      "\titers: 200, epoch: 18 | loss: 0.0144767\n",
      "\tspeed: 0.0309s/iter; left time: 562.4523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.08s\n",
      "Steps: 222 | Train Loss: 0.0135320 Vali Loss: 0.0160363 Test Loss: 0.0187146\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0135079\n",
      "\tspeed: 0.0619s/iter; left time: 1121.1296s\n",
      "\titers: 200, epoch: 19 | loss: 0.0145177\n",
      "\tspeed: 0.0305s/iter; left time: 548.6320s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 222 | Train Loss: 0.0134313 Vali Loss: 0.0162054 Test Loss: 0.0187167\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_512_96_IT_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.017698798328638077, rmse:0.13303683698177338, mae:0.0829048603773117, rse:0.5030266046524048\n",
      "Intermediate time for IT and pred_len 96: 00h:03m:42.73s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_512_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_512_168_IT_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0316257\n",
      "\tspeed: 0.0577s/iter; left time: 1275.2934s\n",
      "\titers: 200, epoch: 1 | loss: 0.0270276\n",
      "\tspeed: 0.0302s/iter; left time: 663.5121s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 222 | Train Loss: 0.0361037 Vali Loss: 0.0229907 Test Loss: 0.0232988\n",
      "Validation loss decreased (inf --> 0.022991).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0195326\n",
      "\tspeed: 0.0665s/iter; left time: 1455.6541s\n",
      "\titers: 200, epoch: 2 | loss: 0.0187041\n",
      "\tspeed: 0.0330s/iter; left time: 718.7757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.64s\n",
      "Steps: 222 | Train Loss: 0.0212769 Vali Loss: 0.0184699 Test Loss: 0.0190285\n",
      "Validation loss decreased (0.022991 --> 0.018470).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0198192\n",
      "\tspeed: 0.0662s/iter; left time: 1432.6211s\n",
      "\titers: 200, epoch: 3 | loss: 0.0184512\n",
      "\tspeed: 0.0307s/iter; left time: 661.7295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 222 | Train Loss: 0.0186053 Vali Loss: 0.0178709 Test Loss: 0.0188089\n",
      "Validation loss decreased (0.018470 --> 0.017871).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0185364\n",
      "\tspeed: 0.0624s/iter; left time: 1337.0252s\n",
      "\titers: 200, epoch: 4 | loss: 0.0168949\n",
      "\tspeed: 0.0305s/iter; left time: 649.7111s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.08s\n",
      "Steps: 222 | Train Loss: 0.0179349 Vali Loss: 0.0177370 Test Loss: 0.0188451\n",
      "Validation loss decreased (0.017871 --> 0.017737).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0179947\n",
      "\tspeed: 0.0632s/iter; left time: 1341.2623s\n",
      "\titers: 200, epoch: 5 | loss: 0.0180004\n",
      "\tspeed: 0.0308s/iter; left time: 650.0205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.29s\n",
      "Steps: 222 | Train Loss: 0.0175016 Vali Loss: 0.0175720 Test Loss: 0.0188430\n",
      "Validation loss decreased (0.017737 --> 0.017572).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0155786\n",
      "\tspeed: 0.0610s/iter; left time: 1280.6893s\n",
      "\titers: 200, epoch: 6 | loss: 0.0176575\n",
      "\tspeed: 0.0308s/iter; left time: 643.3341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.12s\n",
      "Steps: 222 | Train Loss: 0.0170924 Vali Loss: 0.0177003 Test Loss: 0.0191284\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0175384\n",
      "\tspeed: 0.0618s/iter; left time: 1284.4456s\n",
      "\titers: 200, epoch: 7 | loss: 0.0170367\n",
      "\tspeed: 0.0321s/iter; left time: 663.0421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.30s\n",
      "Steps: 222 | Train Loss: 0.0167460 Vali Loss: 0.0177352 Test Loss: 0.0193516\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0177184\n",
      "\tspeed: 0.0644s/iter; left time: 1323.5961s\n",
      "\titers: 200, epoch: 8 | loss: 0.0161199\n",
      "\tspeed: 0.0308s/iter; left time: 629.0158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 222 | Train Loss: 0.0164274 Vali Loss: 0.0176792 Test Loss: 0.0197251\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0156842\n",
      "\tspeed: 0.0640s/iter; left time: 1299.9182s\n",
      "\titers: 200, epoch: 9 | loss: 0.0162864\n",
      "\tspeed: 0.0309s/iter; left time: 624.2516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.26s\n",
      "Steps: 222 | Train Loss: 0.0161275 Vali Loss: 0.0176713 Test Loss: 0.0199261\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0156236\n",
      "\tspeed: 0.0611s/iter; left time: 1228.1874s\n",
      "\titers: 200, epoch: 10 | loss: 0.0146530\n",
      "\tspeed: 0.0306s/iter; left time: 612.5636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 222 | Train Loss: 0.0158304 Vali Loss: 0.0177736 Test Loss: 0.0203212\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0154587\n",
      "\tspeed: 0.0583s/iter; left time: 1159.4810s\n",
      "\titers: 200, epoch: 11 | loss: 0.0154020\n",
      "\tspeed: 0.0307s/iter; left time: 607.1988s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.06s\n",
      "Steps: 222 | Train Loss: 0.0155766 Vali Loss: 0.0178505 Test Loss: 0.0206340\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0155636\n",
      "\tspeed: 0.0629s/iter; left time: 1237.0274s\n",
      "\titers: 200, epoch: 12 | loss: 0.0160708\n",
      "\tspeed: 0.0315s/iter; left time: 616.0999s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.26s\n",
      "Steps: 222 | Train Loss: 0.0153310 Vali Loss: 0.0177846 Test Loss: 0.0206701\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0138345\n",
      "\tspeed: 0.0640s/iter; left time: 1244.2533s\n",
      "\titers: 200, epoch: 13 | loss: 0.0144225\n",
      "\tspeed: 0.0312s/iter; left time: 602.4349s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.33s\n",
      "Steps: 222 | Train Loss: 0.0150875 Vali Loss: 0.0178638 Test Loss: 0.0210341\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0140639\n",
      "\tspeed: 0.0612s/iter; left time: 1175.0993s\n",
      "\titers: 200, epoch: 14 | loss: 0.0132242\n",
      "\tspeed: 0.0308s/iter; left time: 588.3716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.14s\n",
      "Steps: 222 | Train Loss: 0.0149037 Vali Loss: 0.0178357 Test Loss: 0.0212268\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0138437\n",
      "\tspeed: 0.0618s/iter; left time: 1172.9608s\n",
      "\titers: 200, epoch: 15 | loss: 0.0135696\n",
      "\tspeed: 0.0305s/iter; left time: 576.7350s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 222 | Train Loss: 0.0146914 Vali Loss: 0.0179464 Test Loss: 0.0210735\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_512_168_IT_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.01884302869439125, rmse:0.13726991415023804, mae:0.0885576382279396, rse:0.5195146203041077\n",
      "Intermediate time for IT and pred_len 168: 00h:02m:31.65s\n",
      "Intermediate time for IT: 00h:13m:34.57s\n",
      "Total time: 01h:41m:29.46s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PatchTST/64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>0.0919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>0.1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.0908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.0846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.0924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.0591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            PatchTST/64                \n",
       "Metrics                  MSE    RMSE     MAE\n",
       "Country Pred_len                            \n",
       "DE      24            0.0209  0.1447  0.0919\n",
       "        96            0.0346  0.1861  0.1282\n",
       "        168           0.0371  0.1927  0.1354\n",
       "GB      24            0.0245  0.1565  0.1027\n",
       "        96            0.0401  0.2002  0.1409\n",
       "        168           0.0425  0.2062  0.1466\n",
       "ES      24            0.0098  0.0988  0.0611\n",
       "        96            0.0187  0.1368  0.0908\n",
       "        168           0.0211  0.1452  0.0989\n",
       "FR      24            0.0101  0.1004  0.0586\n",
       "        96            0.0189  0.1374  0.0846\n",
       "        168           0.0220  0.1483  0.0924\n",
       "IT      24            0.0101  0.1006  0.0591\n",
       "        96            0.0177  0.1330  0.0829\n",
       "        168           0.0188  0.1373  0.0886"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False, itr=1)\n",
    "patchtst_df.drop(columns=['Iteration'], inplace=True)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['PatchTST/64'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_bs128_pl512.csv'))\n",
    "patchtst_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
