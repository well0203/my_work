{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. TimeLLM](#1-timellm)\n",
    "\n",
    "Results for TimeLLM with input lengths from PatchTST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "seq_lens = [512, 512, 336, 168, 168]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "patience = 5\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-13 09:49:23,155] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 09:49:23,160] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 09:49:23,165] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 09:49:23,168] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 09:49:23,872] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 09:49:23,872] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 09:49:23,872] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 09:49:23,872] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 09:49:23,872] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "train 143885\n",
      "train 143885\n",
      "train 143885\n",
      "val 31085\n",
      "val 31085\n",
      "val 31085\n",
      "test 31085\n",
      "test 31085\n",
      "test 31085\n",
      "[2024-11-13 09:49:25,859] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 09:49:26,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 09:49:26,817] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 09:49:26,817] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 09:49:26,818] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 09:49:26,818] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 09:49:26,818] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 09:49:26,818] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 09:49:26,818] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 09:49:26,818] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 09:49:26,819] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 09:49:27,187] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 09:49:27,188] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 09:49:27,188] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.14 GB, percent = 2.5%\n",
      "[2024-11-13 09:49:27,536] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 09:49:27,537] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 09:49:27,537] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.63 GB, percent = 2.5%\n",
      "[2024-11-13 09:49:27,537] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 09:49:27,722] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 09:49:27,723] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 09:49:27,723] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.99 GB, percent = 2.6%\n",
      "[2024-11-13 09:49:27,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 09:49:27,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 09:49:27,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 09:49:27,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 09:49:27,724] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 09:49:27,724] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 09:49:27,724] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 09:49:27,724] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 09:49:27,724] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f10336950>\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 09:49:27,725] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 09:49:27,726] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0632429\n",
      "\tspeed: 0.1596s/iter; left time: 14332.7096s\n",
      "\titers: 200, epoch: 1 | loss: 0.0507106\n",
      "\tspeed: 0.1277s/iter; left time: 11457.6386s\n",
      "\titers: 300, epoch: 1 | loss: 0.0394340\n",
      "\tspeed: 0.1267s/iter; left time: 11352.7212s\n",
      "\titers: 400, epoch: 1 | loss: 0.0388287\n",
      "\tspeed: 0.1272s/iter; left time: 11391.2018s\n",
      "\titers: 500, epoch: 1 | loss: 0.0251836\n",
      "\tspeed: 0.1280s/iter; left time: 11442.0396s\n",
      "\titers: 600, epoch: 1 | loss: 0.0225685\n",
      "\tspeed: 0.1286s/iter; left time: 11483.1533s\n",
      "\titers: 700, epoch: 1 | loss: 0.0203688\n",
      "\tspeed: 0.1274s/iter; left time: 11363.5946s\n",
      "\titers: 800, epoch: 1 | loss: 0.0230455\n",
      "\tspeed: 0.1284s/iter; left time: 11444.6231s\n",
      "\titers: 900, epoch: 1 | loss: 0.0221143\n",
      "\tspeed: 0.1272s/iter; left time: 11327.1772s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0220149\n",
      "\tspeed: 0.1269s/iter; left time: 11280.3250s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0171128\n",
      "\tspeed: 0.1269s/iter; left time: 11272.6784s\n",
      "Epoch: 1 cost time: 00h:02m:24.78s\n",
      "Epoch: 1 | Train Loss: 0.0333040 Vali Loss: 0.0229411 Test Loss: 0.0251673\n",
      "Validation loss decreased (inf --> 0.022941).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0113808\n",
      "\tspeed: 0.5043s/iter; left time: 43028.4042s\n",
      "\titers: 200, epoch: 2 | loss: 0.0146446\n",
      "\tspeed: 0.1264s/iter; left time: 10771.1634s\n",
      "\titers: 300, epoch: 2 | loss: 0.0160543\n",
      "\tspeed: 0.1260s/iter; left time: 10726.6096s\n",
      "\titers: 400, epoch: 2 | loss: 0.0154046\n",
      "\tspeed: 0.1261s/iter; left time: 10724.5610s\n",
      "\titers: 500, epoch: 2 | loss: 0.0173704\n",
      "\tspeed: 0.1267s/iter; left time: 10760.8354s\n",
      "\titers: 600, epoch: 2 | loss: 0.0362225\n",
      "\tspeed: 0.1261s/iter; left time: 10694.0763s\n",
      "\titers: 700, epoch: 2 | loss: 0.0231417\n",
      "\tspeed: 0.1260s/iter; left time: 10676.6195s\n",
      "\titers: 800, epoch: 2 | loss: 0.0158735\n",
      "\tspeed: 0.1262s/iter; left time: 10677.6936s\n",
      "\titers: 900, epoch: 2 | loss: 0.0282127\n",
      "\tspeed: 0.1261s/iter; left time: 10655.7184s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0127809\n",
      "\tspeed: 0.1263s/iter; left time: 10664.6570s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0152286\n",
      "\tspeed: 0.1265s/iter; left time: 10666.5750s\n",
      "Epoch: 2 cost time: 00h:02m:22.42s\n",
      "Epoch: 2 | Train Loss: 0.0189258 Vali Loss: 0.0213288 Test Loss: 0.0228869\n",
      "Validation loss decreased (0.022941 --> 0.021329).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0147866\n",
      "\tspeed: 0.5266s/iter; left time: 42567.8800s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251924\n",
      "\tspeed: 0.1256s/iter; left time: 10138.8693s\n",
      "\titers: 300, epoch: 3 | loss: 0.0179662\n",
      "\tspeed: 0.1278s/iter; left time: 10302.4417s\n",
      "\titers: 400, epoch: 3 | loss: 0.0170750\n",
      "\tspeed: 0.1271s/iter; left time: 10235.9319s\n",
      "\titers: 500, epoch: 3 | loss: 0.0222653\n",
      "\tspeed: 0.1271s/iter; left time: 10223.5881s\n",
      "\titers: 600, epoch: 3 | loss: 0.0223925\n",
      "\tspeed: 0.1254s/iter; left time: 10073.6427s\n",
      "\titers: 700, epoch: 3 | loss: 0.0192496\n",
      "\tspeed: 0.1290s/iter; left time: 10348.4963s\n",
      "\titers: 800, epoch: 3 | loss: 0.0108626\n",
      "\tspeed: 0.1269s/iter; left time: 10167.9189s\n",
      "\titers: 900, epoch: 3 | loss: 0.0230040\n",
      "\tspeed: 0.1281s/iter; left time: 10255.1222s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0219360\n",
      "\tspeed: 0.1270s/iter; left time: 10154.9390s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0159164\n",
      "\tspeed: 0.1259s/iter; left time: 10047.7412s\n",
      "Epoch: 3 cost time: 00h:02m:23.05s\n",
      "Epoch: 3 | Train Loss: 0.0180037 Vali Loss: 0.0207199 Test Loss: 0.0221680\n",
      "Validation loss decreased (0.021329 --> 0.020720).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0195420\n",
      "\tspeed: 0.4540s/iter; left time: 34655.4110s\n",
      "\titers: 200, epoch: 4 | loss: 0.0174730\n",
      "\tspeed: 0.1271s/iter; left time: 9692.9025s\n",
      "\titers: 300, epoch: 4 | loss: 0.0160964\n",
      "\tspeed: 0.1259s/iter; left time: 9587.5938s\n",
      "\titers: 400, epoch: 4 | loss: 0.0249365\n",
      "\tspeed: 0.1269s/iter; left time: 9648.4402s\n",
      "\titers: 500, epoch: 4 | loss: 0.0150605\n",
      "\tspeed: 0.1256s/iter; left time: 9535.0199s\n",
      "\titers: 600, epoch: 4 | loss: 0.0123712\n",
      "\tspeed: 0.1257s/iter; left time: 9534.9213s\n",
      "\titers: 700, epoch: 4 | loss: 0.0285275\n",
      "\tspeed: 0.1263s/iter; left time: 9565.1332s\n",
      "\titers: 800, epoch: 4 | loss: 0.0258730\n",
      "\tspeed: 0.1248s/iter; left time: 9435.6316s\n",
      "\titers: 900, epoch: 4 | loss: 0.0145841\n",
      "\tspeed: 0.1263s/iter; left time: 9538.6528s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0179337\n",
      "\tspeed: 0.1269s/iter; left time: 9574.9499s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0190074\n",
      "\tspeed: 0.1263s/iter; left time: 9516.3868s\n",
      "Epoch: 4 cost time: 00h:02m:22.73s\n",
      "Epoch: 4 | Train Loss: 0.0173761 Vali Loss: 0.0201705 Test Loss: 0.0218201\n",
      "Validation loss decreased (0.020720 --> 0.020170).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0248572\n",
      "\tspeed: 0.4683s/iter; left time: 33641.8401s\n",
      "\titers: 200, epoch: 5 | loss: 0.0181087\n",
      "\tspeed: 0.1239s/iter; left time: 8890.3790s\n",
      "\titers: 300, epoch: 5 | loss: 0.0130525\n",
      "\tspeed: 0.1247s/iter; left time: 8936.5995s\n",
      "\titers: 400, epoch: 5 | loss: 0.0138662\n",
      "\tspeed: 0.1241s/iter; left time: 8876.8397s\n",
      "\titers: 500, epoch: 5 | loss: 0.0155967\n",
      "\tspeed: 0.1237s/iter; left time: 8835.7610s\n",
      "\titers: 600, epoch: 5 | loss: 0.0198725\n",
      "\tspeed: 0.1245s/iter; left time: 8879.3120s\n",
      "\titers: 700, epoch: 5 | loss: 0.0202901\n",
      "\tspeed: 0.1243s/iter; left time: 8852.7851s\n",
      "\titers: 800, epoch: 5 | loss: 0.0220147\n",
      "\tspeed: 0.1247s/iter; left time: 8872.5105s\n",
      "\titers: 900, epoch: 5 | loss: 0.0173096\n",
      "\tspeed: 0.1246s/iter; left time: 8851.0300s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0179298\n",
      "\tspeed: 0.1243s/iter; left time: 8817.3521s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0144313\n",
      "\tspeed: 0.1243s/iter; left time: 8801.8770s\n",
      "Epoch: 5 cost time: 00h:02m:20.48s\n",
      "Epoch: 5 | Train Loss: 0.0169353 Vali Loss: 0.0197934 Test Loss: 0.0216913\n",
      "Validation loss decreased (0.020170 --> 0.019793).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0178052\n",
      "\tspeed: 0.4679s/iter; left time: 31510.8500s\n",
      "\titers: 200, epoch: 6 | loss: 0.0166879\n",
      "\tspeed: 0.1231s/iter; left time: 8280.7269s\n",
      "\titers: 300, epoch: 6 | loss: 0.0215266\n",
      "\tspeed: 0.1232s/iter; left time: 8269.1017s\n",
      "\titers: 400, epoch: 6 | loss: 0.0158959\n",
      "\tspeed: 0.1244s/iter; left time: 8340.1850s\n",
      "\titers: 500, epoch: 6 | loss: 0.0225539\n",
      "\tspeed: 0.1244s/iter; left time: 8328.6383s\n",
      "\titers: 600, epoch: 6 | loss: 0.0142451\n",
      "\tspeed: 0.1250s/iter; left time: 8353.4203s\n",
      "\titers: 700, epoch: 6 | loss: 0.0146011\n",
      "\tspeed: 0.1257s/iter; left time: 8389.7921s\n",
      "\titers: 800, epoch: 6 | loss: 0.0156253\n",
      "\tspeed: 0.1247s/iter; left time: 8308.7191s\n",
      "\titers: 900, epoch: 6 | loss: 0.0157494\n",
      "\tspeed: 0.1234s/iter; left time: 8208.0748s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0117435\n",
      "\tspeed: 0.1241s/iter; left time: 8248.1347s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0189988\n",
      "\tspeed: 0.1238s/iter; left time: 8212.3800s\n",
      "Epoch: 6 cost time: 00h:02m:20.36s\n",
      "Epoch: 6 | Train Loss: 0.0165692 Vali Loss: 0.0196932 Test Loss: 0.0218816\n",
      "Validation loss decreased (0.019793 --> 0.019693).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0172309\n",
      "\tspeed: 0.4446s/iter; left time: 27940.4976s\n",
      "\titers: 200, epoch: 7 | loss: 0.0123486\n",
      "\tspeed: 0.1234s/iter; left time: 7745.4177s\n",
      "\titers: 300, epoch: 7 | loss: 0.0212684\n",
      "\tspeed: 0.1236s/iter; left time: 7744.9487s\n",
      "\titers: 400, epoch: 7 | loss: 0.0195458\n",
      "\tspeed: 0.1233s/iter; left time: 7712.0591s\n",
      "\titers: 500, epoch: 7 | loss: 0.0102791\n",
      "\tspeed: 0.1230s/iter; left time: 7683.4528s\n",
      "\titers: 600, epoch: 7 | loss: 0.0223481\n",
      "\tspeed: 0.1245s/iter; left time: 7761.0378s\n",
      "\titers: 700, epoch: 7 | loss: 0.0160351\n",
      "\tspeed: 0.1236s/iter; left time: 7692.8235s\n",
      "\titers: 800, epoch: 7 | loss: 0.0161180\n",
      "\tspeed: 0.1230s/iter; left time: 7645.4037s\n",
      "\titers: 900, epoch: 7 | loss: 0.0166668\n",
      "\tspeed: 0.1229s/iter; left time: 7623.1419s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0117904\n",
      "\tspeed: 0.1232s/iter; left time: 7629.4794s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0165729\n",
      "\tspeed: 0.1241s/iter; left time: 7674.5664s\n",
      "Epoch: 7 cost time: 00h:02m:19.40s\n",
      "Epoch: 7 | Train Loss: 0.0163646 Vali Loss: 0.0194863 Test Loss: 0.0215178\n",
      "Validation loss decreased (0.019693 --> 0.019486).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0151283\n",
      "\tspeed: 0.4668s/iter; left time: 27238.7598s\n",
      "\titers: 200, epoch: 8 | loss: 0.0133968\n",
      "\tspeed: 0.1234s/iter; left time: 7186.7830s\n",
      "\titers: 300, epoch: 8 | loss: 0.0154707\n",
      "\tspeed: 0.1259s/iter; left time: 7318.7536s\n",
      "\titers: 400, epoch: 8 | loss: 0.0145500\n",
      "\tspeed: 0.1226s/iter; left time: 7118.8812s\n",
      "\titers: 500, epoch: 8 | loss: 0.0161144\n",
      "\tspeed: 0.1227s/iter; left time: 7108.9564s\n",
      "\titers: 600, epoch: 8 | loss: 0.0167301\n",
      "\tspeed: 0.1232s/iter; left time: 7125.2980s\n",
      "\titers: 700, epoch: 8 | loss: 0.0157079\n",
      "\tspeed: 0.1246s/iter; left time: 7193.0843s\n",
      "\titers: 800, epoch: 8 | loss: 0.0133750\n",
      "\tspeed: 0.1227s/iter; left time: 7072.5854s\n",
      "\titers: 900, epoch: 8 | loss: 0.0157046\n",
      "\tspeed: 0.1230s/iter; left time: 7080.2701s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0103165\n",
      "\tspeed: 0.1233s/iter; left time: 7081.5530s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0139662\n",
      "\tspeed: 0.1227s/iter; left time: 7037.3690s\n",
      "Epoch: 8 cost time: 00h:02m:19.39s\n",
      "Epoch: 8 | Train Loss: 0.0162527 Vali Loss: 0.0196475 Test Loss: 0.0218817\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0137342\n",
      "\tspeed: 0.4041s/iter; left time: 21763.5678s\n",
      "\titers: 200, epoch: 9 | loss: 0.0193019\n",
      "\tspeed: 0.1232s/iter; left time: 6619.9120s\n",
      "\titers: 300, epoch: 9 | loss: 0.0143334\n",
      "\tspeed: 0.1228s/iter; left time: 6590.2658s\n",
      "\titers: 400, epoch: 9 | loss: 0.0120264\n",
      "\tspeed: 0.1225s/iter; left time: 6559.3278s\n",
      "\titers: 500, epoch: 9 | loss: 0.0147567\n",
      "\tspeed: 0.1227s/iter; left time: 6559.1497s\n",
      "\titers: 600, epoch: 9 | loss: 0.0199759\n",
      "\tspeed: 0.1228s/iter; left time: 6552.0193s\n",
      "\titers: 700, epoch: 9 | loss: 0.0136194\n",
      "\tspeed: 0.1224s/iter; left time: 6518.3537s\n",
      "\titers: 800, epoch: 9 | loss: 0.0103802\n",
      "\tspeed: 0.1228s/iter; left time: 6529.0174s\n",
      "\titers: 900, epoch: 9 | loss: 0.0166926\n",
      "\tspeed: 0.1228s/iter; left time: 6516.5806s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0198049\n",
      "\tspeed: 0.1232s/iter; left time: 6521.8349s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0279420\n",
      "\tspeed: 0.1230s/iter; left time: 6502.3118s\n",
      "Epoch: 9 cost time: 00h:02m:18.59s\n",
      "Epoch: 9 | Train Loss: 0.0158304 Vali Loss: 0.0192229 Test Loss: 0.0215448\n",
      "Validation loss decreased (0.019486 --> 0.019223).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0203107\n",
      "\tspeed: 0.4512s/iter; left time: 22270.5010s\n",
      "\titers: 200, epoch: 10 | loss: 0.0133896\n",
      "\tspeed: 0.1244s/iter; left time: 6126.4958s\n",
      "\titers: 300, epoch: 10 | loss: 0.0116833\n",
      "\tspeed: 0.1231s/iter; left time: 6048.9521s\n",
      "\titers: 400, epoch: 10 | loss: 0.0166726\n",
      "\tspeed: 0.1225s/iter; left time: 6008.8623s\n",
      "\titers: 500, epoch: 10 | loss: 0.0108126\n",
      "\tspeed: 0.1230s/iter; left time: 6021.6406s\n",
      "\titers: 600, epoch: 10 | loss: 0.0193221\n",
      "\tspeed: 0.1224s/iter; left time: 5979.1583s\n",
      "\titers: 700, epoch: 10 | loss: 0.0144544\n",
      "\tspeed: 0.1224s/iter; left time: 5966.2592s\n",
      "\titers: 800, epoch: 10 | loss: 0.0147057\n",
      "\tspeed: 0.1223s/iter; left time: 5948.7394s\n",
      "\titers: 900, epoch: 10 | loss: 0.0103039\n",
      "\tspeed: 0.1224s/iter; left time: 5944.6946s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0158291\n",
      "\tspeed: 0.1229s/iter; left time: 5954.6180s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0099726\n",
      "\tspeed: 0.1225s/iter; left time: 5925.6257s\n",
      "Epoch: 10 cost time: 00h:02m:18.50s\n",
      "Epoch: 10 | Train Loss: 0.0154770 Vali Loss: 0.0195692 Test Loss: 0.0220183\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0165156\n",
      "\tspeed: 0.4088s/iter; left time: 18340.2348s\n",
      "\titers: 200, epoch: 11 | loss: 0.0128128\n",
      "\tspeed: 0.1220s/iter; left time: 5460.8415s\n",
      "\titers: 300, epoch: 11 | loss: 0.0199038\n",
      "\tspeed: 0.1231s/iter; left time: 5496.3082s\n",
      "\titers: 400, epoch: 11 | loss: 0.0113736\n",
      "\tspeed: 0.1230s/iter; left time: 5480.0786s\n",
      "\titers: 500, epoch: 11 | loss: 0.0177854\n",
      "\tspeed: 0.1220s/iter; left time: 5425.2071s\n",
      "\titers: 600, epoch: 11 | loss: 0.0116605\n",
      "\tspeed: 0.1222s/iter; left time: 5419.5216s\n",
      "\titers: 700, epoch: 11 | loss: 0.0130346\n",
      "\tspeed: 0.1223s/iter; left time: 5411.2270s\n",
      "\titers: 800, epoch: 11 | loss: 0.0176324\n",
      "\tspeed: 0.1221s/iter; left time: 5390.1957s\n",
      "\titers: 900, epoch: 11 | loss: 0.0118758\n",
      "\tspeed: 0.1221s/iter; left time: 5381.7021s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0107230\n",
      "\tspeed: 0.1220s/iter; left time: 5363.8690s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0178455\n",
      "\tspeed: 0.1223s/iter; left time: 5365.3155s\n",
      "Epoch: 11 cost time: 00h:02m:18.05s\n",
      "Epoch: 11 | Train Loss: 0.0154121 Vali Loss: 0.0196424 Test Loss: 0.0221752\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0175937\n",
      "\tspeed: 0.4084s/iter; left time: 16484.8962s\n",
      "\titers: 200, epoch: 12 | loss: 0.0126353\n",
      "\tspeed: 0.1227s/iter; left time: 4941.4697s\n",
      "\titers: 300, epoch: 12 | loss: 0.0206016\n",
      "\tspeed: 0.1220s/iter; left time: 4900.8378s\n",
      "\titers: 400, epoch: 12 | loss: 0.0102996\n",
      "\tspeed: 0.1225s/iter; left time: 4909.5327s\n",
      "\titers: 500, epoch: 12 | loss: 0.0157677\n",
      "\tspeed: 0.1220s/iter; left time: 4877.5704s\n",
      "\titers: 600, epoch: 12 | loss: 0.0146827\n",
      "\tspeed: 0.1222s/iter; left time: 4871.7130s\n",
      "\titers: 700, epoch: 12 | loss: 0.0145983\n",
      "\tspeed: 0.1221s/iter; left time: 4855.5477s\n",
      "\titers: 800, epoch: 12 | loss: 0.0172842\n",
      "\tspeed: 0.1221s/iter; left time: 4843.0983s\n",
      "\titers: 900, epoch: 12 | loss: 0.0198558\n",
      "\tspeed: 0.1219s/iter; left time: 4821.6897s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0173477\n",
      "\tspeed: 0.1228s/iter; left time: 4847.6978s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0088205\n",
      "\tspeed: 0.1224s/iter; left time: 4817.4609s\n",
      "Epoch: 12 cost time: 00h:02m:17.95s\n",
      "Epoch: 12 | Train Loss: 0.0152097 Vali Loss: 0.0197840 Test Loss: 0.0225309\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0192575\n",
      "\tspeed: 0.3992s/iter; left time: 14317.2902s\n",
      "\titers: 200, epoch: 13 | loss: 0.0178487\n",
      "\tspeed: 0.1212s/iter; left time: 4333.9485s\n",
      "\titers: 300, epoch: 13 | loss: 0.0284491\n",
      "\tspeed: 0.1214s/iter; left time: 4331.0614s\n",
      "\titers: 400, epoch: 13 | loss: 0.0159005\n",
      "\tspeed: 0.1224s/iter; left time: 4355.3968s\n",
      "\titers: 500, epoch: 13 | loss: 0.0128495\n",
      "\tspeed: 0.1223s/iter; left time: 4338.6898s\n",
      "\titers: 600, epoch: 13 | loss: 0.0117158\n",
      "\tspeed: 0.1220s/iter; left time: 4315.0544s\n",
      "\titers: 700, epoch: 13 | loss: 0.0135272\n",
      "\tspeed: 0.1219s/iter; left time: 4298.7148s\n",
      "\titers: 800, epoch: 13 | loss: 0.0158590\n",
      "\tspeed: 0.1219s/iter; left time: 4287.7217s\n",
      "\titers: 900, epoch: 13 | loss: 0.0106493\n",
      "\tspeed: 0.1219s/iter; left time: 4274.6949s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0102547\n",
      "\tspeed: 0.1221s/iter; left time: 4268.4944s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0108302\n",
      "\tspeed: 0.1224s/iter; left time: 4269.1645s\n",
      "Epoch: 13 cost time: 00h:02m:17.53s\n",
      "Epoch: 13 | Train Loss: 0.0150879 Vali Loss: 0.0196984 Test Loss: 0.0227838\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0201575\n",
      "\tspeed: 0.4000s/iter; left time: 12549.4930s\n",
      "\titers: 200, epoch: 14 | loss: 0.0167995\n",
      "\tspeed: 0.1231s/iter; left time: 3851.1071s\n",
      "\titers: 300, epoch: 14 | loss: 0.0204233\n",
      "\tspeed: 0.1221s/iter; left time: 3805.5627s\n",
      "\titers: 400, epoch: 14 | loss: 0.0163709\n",
      "\tspeed: 0.1220s/iter; left time: 3791.4749s\n",
      "\titers: 500, epoch: 14 | loss: 0.0143940\n",
      "\tspeed: 0.1223s/iter; left time: 3787.0049s\n",
      "\titers: 600, epoch: 14 | loss: 0.0164356\n",
      "\tspeed: 0.1221s/iter; left time: 3769.5670s\n",
      "\titers: 700, epoch: 14 | loss: 0.0193533\n",
      "\tspeed: 0.1220s/iter; left time: 3753.9533s\n",
      "\titers: 800, epoch: 14 | loss: 0.0155888\n",
      "\tspeed: 0.1222s/iter; left time: 3749.6343s\n",
      "\titers: 900, epoch: 14 | loss: 0.0123037\n",
      "\tspeed: 0.1219s/iter; left time: 3726.6297s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0190936\n",
      "\tspeed: 0.1227s/iter; left time: 3737.5671s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0183230\n",
      "\tspeed: 0.1222s/iter; left time: 3710.0902s\n",
      "Epoch: 14 cost time: 00h:02m:17.97s\n",
      "Epoch: 14 | Train Loss: 0.0148230 Vali Loss: 0.0200994 Test Loss: 0.0234704\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/./Time-LLM/run_main.py\", line 302, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/./Time-LLM/run_main.py\", line 302, in <module>\n",
      "    mae = test(args, accelerator, model, test_data, test_loader, criterion, setting)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/Time-LLM/utils/tools.py\", line 320, in test\n",
      "    mae = test(args, accelerator, model, test_data, test_loader, criterion, setting)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/Time-LLM/utils/tools.py\", line 320, in test\n",
      "    os.makedirs(p)\n",
      "    os.makedirs(p)\n",
      "  File \"<frozen os>\", line 225, in makedirs\n",
      "  File \"<frozen os>\", line 225, in makedirs\n",
      "FileExistsError: [Errno 17] File exists: './results_transformers'\n",
      "FileExistsError: [Errno 17] File exists: './results_transformers'\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "[2024-11-13 10:29:03,939] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3489 closing signal SIGTERM\n",
      "[2024-11-13 10:29:03,939] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3490 closing signal SIGTERM\n",
      "[2024-11-13 10:29:04,204] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3488) of binary: /vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./Time-LLM/run_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-11-13_10:29:03\n",
      "  host      : gruenau7.informatik.hu-berlin.de\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 3491)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-11-13_10:29:03\n",
      "  host      : gruenau7.informatik.hu-berlin.de\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3488)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "Intermediate time for DE and pred_len 24: 00h:39m:49.79s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-13 10:29:16,024] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 10:29:16,029] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 10:29:16,062] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 10:29:16,062] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 10:29:17,107] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "train 142645\n",
      "train 142645\n",
      "train 142645\n",
      "val 30725\n",
      "val 30725\n",
      "val 30725\n",
      "test 30725\n",
      "test 30725\n",
      "test 30725\n",
      "[2024-11-13 10:29:18,892] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 10:29:19,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 10:29:19,817] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 10:29:19,817] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 10:29:19,818] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 10:29:19,818] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 10:29:19,818] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 10:29:20,237] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 10:29:20,238] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 10:29:20,238] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.62 GB, percent = 2.5%\n",
      "[2024-11-13 10:29:20,447] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 10:29:20,448] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 10:29:20,448] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.08 GB, percent = 2.6%\n",
      "[2024-11-13 10:29:20,448] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 10:29:20,643] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 10:29:20,644] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 10:29:20,644] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.39 GB, percent = 2.6%\n",
      "[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 10:29:20,645] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6db7bd38d0>\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 10:29:20,647] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0446114\n",
      "\tspeed: 0.1660s/iter; left time: 14783.4114s\n",
      "\titers: 200, epoch: 1 | loss: 0.0507687\n",
      "\tspeed: 0.1377s/iter; left time: 12247.6350s\n",
      "\titers: 300, epoch: 1 | loss: 0.0448839\n",
      "\tspeed: 0.1369s/iter; left time: 12158.9270s\n",
      "\titers: 400, epoch: 1 | loss: 0.0454857\n",
      "\tspeed: 0.1368s/iter; left time: 12142.5777s\n",
      "\titers: 500, epoch: 1 | loss: 0.0477529\n",
      "\tspeed: 0.1369s/iter; left time: 12135.3471s\n",
      "\titers: 600, epoch: 1 | loss: 0.0380543\n",
      "\tspeed: 0.1394s/iter; left time: 12341.3727s\n",
      "\titers: 700, epoch: 1 | loss: 0.0391503\n",
      "\tspeed: 0.1378s/iter; left time: 12191.2799s\n",
      "\titers: 800, epoch: 1 | loss: 0.0337921\n",
      "\tspeed: 0.1376s/iter; left time: 12160.0810s\n",
      "\titers: 900, epoch: 1 | loss: 0.0199492\n",
      "\tspeed: 0.1385s/iter; left time: 12218.6009s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0314866\n",
      "\tspeed: 0.1376s/iter; left time: 12127.1685s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0306014\n",
      "\tspeed: 0.1371s/iter; left time: 12067.6042s\n",
      "Epoch: 1 cost time: 00h:02m:34.53s\n",
      "Epoch: 1 | Train Loss: 0.0429309 Vali Loss: 0.0322865 Test Loss: 0.0373079\n",
      "Validation loss decreased (inf --> 0.032287).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0302605\n",
      "\tspeed: 0.5005s/iter; left time: 42335.2406s\n",
      "\titers: 200, epoch: 2 | loss: 0.0323583\n",
      "\tspeed: 0.1425s/iter; left time: 12038.4969s\n",
      "\titers: 300, epoch: 2 | loss: 0.0289049\n",
      "\tspeed: 0.1392s/iter; left time: 11745.1487s\n",
      "\titers: 400, epoch: 2 | loss: 0.0225016\n",
      "\tspeed: 0.1412s/iter; left time: 11901.7324s\n",
      "\titers: 500, epoch: 2 | loss: 0.0280473\n",
      "\tspeed: 0.1380s/iter; left time: 11620.8339s\n",
      "\titers: 600, epoch: 2 | loss: 0.0231738\n",
      "\tspeed: 0.1359s/iter; left time: 11427.4173s\n",
      "\titers: 700, epoch: 2 | loss: 0.0285885\n",
      "\tspeed: 0.1376s/iter; left time: 11553.4280s\n",
      "\titers: 800, epoch: 2 | loss: 0.0226014\n",
      "\tspeed: 0.1382s/iter; left time: 11588.6630s\n",
      "\titers: 900, epoch: 2 | loss: 0.0252432\n",
      "\tspeed: 0.1367s/iter; left time: 11453.7880s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0303803\n",
      "\tspeed: 0.1359s/iter; left time: 11374.1162s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0346701\n",
      "\tspeed: 0.1369s/iter; left time: 11444.2170s\n",
      "Epoch: 2 cost time: 00h:02m:34.58s\n",
      "Epoch: 2 | Train Loss: 0.0267929 Vali Loss: 0.0310637 Test Loss: 0.0355101\n",
      "Validation loss decreased (0.032287 --> 0.031064).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0272676\n",
      "\tspeed: 0.5265s/iter; left time: 42183.8046s\n",
      "\titers: 200, epoch: 3 | loss: 0.0320771\n",
      "\tspeed: 0.1363s/iter; left time: 10906.2099s\n",
      "\titers: 300, epoch: 3 | loss: 0.0189163\n",
      "\tspeed: 0.1355s/iter; left time: 10832.6138s\n",
      "\titers: 400, epoch: 3 | loss: 0.0244222\n",
      "\tspeed: 0.1371s/iter; left time: 10945.0419s\n",
      "\titers: 500, epoch: 3 | loss: 0.0268812\n",
      "\tspeed: 0.1347s/iter; left time: 10736.1242s\n",
      "\titers: 600, epoch: 3 | loss: 0.0218725\n",
      "\tspeed: 0.1348s/iter; left time: 10732.4763s\n",
      "\titers: 700, epoch: 3 | loss: 0.0201757\n",
      "\tspeed: 0.1373s/iter; left time: 10915.5663s\n",
      "\titers: 800, epoch: 3 | loss: 0.0237758\n",
      "\tspeed: 0.1380s/iter; left time: 10962.3137s\n",
      "\titers: 900, epoch: 3 | loss: 0.0272236\n",
      "\tspeed: 0.1360s/iter; left time: 10789.2366s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0253749\n",
      "\tspeed: 0.1365s/iter; left time: 10816.0596s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0313581\n",
      "\tspeed: 0.1383s/iter; left time: 10942.4425s\n",
      "Epoch: 3 cost time: 00h:02m:33.15s\n",
      "Epoch: 3 | Train Loss: 0.0256442 Vali Loss: 0.0304424 Test Loss: 0.0349693\n",
      "Validation loss decreased (0.031064 --> 0.030442).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0234055\n",
      "\tspeed: 0.5104s/iter; left time: 38623.6626s\n",
      "\titers: 200, epoch: 4 | loss: 0.0379499\n",
      "\tspeed: 0.1372s/iter; left time: 10365.9854s\n",
      "\titers: 300, epoch: 4 | loss: 0.0260023\n",
      "\tspeed: 0.1367s/iter; left time: 10313.3352s\n",
      "\titers: 400, epoch: 4 | loss: 0.0309342\n",
      "\tspeed: 0.1371s/iter; left time: 10334.6951s\n",
      "\titers: 500, epoch: 4 | loss: 0.0230162\n",
      "\tspeed: 0.1363s/iter; left time: 10257.9880s\n",
      "\titers: 600, epoch: 4 | loss: 0.0329382\n",
      "\tspeed: 0.1361s/iter; left time: 10232.0065s\n",
      "\titers: 700, epoch: 4 | loss: 0.0270466\n",
      "\tspeed: 0.1351s/iter; left time: 10139.0892s\n",
      "\titers: 800, epoch: 4 | loss: 0.0279899\n",
      "\tspeed: 0.1389s/iter; left time: 10415.5284s\n",
      "\titers: 900, epoch: 4 | loss: 0.0265847\n",
      "\tspeed: 0.1338s/iter; left time: 10018.1953s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0298538\n",
      "\tspeed: 0.1343s/iter; left time: 10041.2317s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0180375\n",
      "\tspeed: 0.1348s/iter; left time: 10064.6278s\n",
      "Epoch: 4 cost time: 00h:02m:32.42s\n",
      "Epoch: 4 | Train Loss: 0.0250050 Vali Loss: 0.0307257 Test Loss: 0.0347905\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0198935\n",
      "\tspeed: 0.4449s/iter; left time: 31682.8080s\n",
      "\titers: 200, epoch: 5 | loss: 0.0202480\n",
      "\tspeed: 0.1360s/iter; left time: 9669.3895s\n",
      "\titers: 300, epoch: 5 | loss: 0.0214222\n",
      "\tspeed: 0.1334s/iter; left time: 9471.0439s\n",
      "\titers: 400, epoch: 5 | loss: 0.0283736\n",
      "\tspeed: 0.1345s/iter; left time: 9539.0708s\n",
      "\titers: 500, epoch: 5 | loss: 0.0473016\n",
      "\tspeed: 0.1343s/iter; left time: 9508.0877s\n",
      "\titers: 600, epoch: 5 | loss: 0.0202376\n",
      "\tspeed: 0.1332s/iter; left time: 9421.8602s\n",
      "\titers: 700, epoch: 5 | loss: 0.0211350\n",
      "\tspeed: 0.1337s/iter; left time: 9439.6364s\n",
      "\titers: 800, epoch: 5 | loss: 0.0247196\n",
      "\tspeed: 0.1334s/iter; left time: 9405.3486s\n",
      "\titers: 900, epoch: 5 | loss: 0.0205404\n",
      "\tspeed: 0.1339s/iter; left time: 9427.7018s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0239363\n",
      "\tspeed: 0.1360s/iter; left time: 9563.8871s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0213725\n",
      "\tspeed: 0.1334s/iter; left time: 9366.0544s\n",
      "Epoch: 5 cost time: 00h:02m:30.44s\n",
      "Epoch: 5 | Train Loss: 0.0242875 Vali Loss: 0.0304751 Test Loss: 0.0348097\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0309512\n",
      "\tspeed: 0.4661s/iter; left time: 31115.2195s\n",
      "\titers: 200, epoch: 6 | loss: 0.0205533\n",
      "\tspeed: 0.1372s/iter; left time: 9145.8859s\n",
      "\titers: 300, epoch: 6 | loss: 0.0224231\n",
      "\tspeed: 0.1357s/iter; left time: 9029.5893s\n",
      "\titers: 400, epoch: 6 | loss: 0.0260528\n",
      "\tspeed: 0.1338s/iter; left time: 8894.1612s\n",
      "\titers: 500, epoch: 6 | loss: 0.0180285\n",
      "\tspeed: 0.1339s/iter; left time: 8884.8938s\n",
      "\titers: 600, epoch: 6 | loss: 0.0195526\n",
      "\tspeed: 0.1332s/iter; left time: 8823.3560s\n",
      "\titers: 700, epoch: 6 | loss: 0.0172730\n",
      "\tspeed: 0.1328s/iter; left time: 8785.3991s\n",
      "\titers: 800, epoch: 6 | loss: 0.0267439\n",
      "\tspeed: 0.1335s/iter; left time: 8818.1725s\n",
      "\titers: 900, epoch: 6 | loss: 0.0241353\n",
      "\tspeed: 0.1352s/iter; left time: 8916.8592s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0157862\n",
      "\tspeed: 0.1336s/iter; left time: 8797.1455s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0216071\n",
      "\tspeed: 0.1334s/iter; left time: 8773.5081s\n",
      "Epoch: 6 cost time: 00h:02m:30.73s\n",
      "Epoch: 6 | Train Loss: 0.0236184 Vali Loss: 0.0298692 Test Loss: 0.0346515\n",
      "Validation loss decreased (0.030442 --> 0.029869).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0280138\n",
      "\tspeed: 0.5151s/iter; left time: 32089.1125s\n",
      "\titers: 200, epoch: 7 | loss: 0.0166881\n",
      "\tspeed: 0.1330s/iter; left time: 8274.2779s\n",
      "\titers: 300, epoch: 7 | loss: 0.0210482\n",
      "\tspeed: 0.1334s/iter; left time: 8282.1030s\n",
      "\titers: 400, epoch: 7 | loss: 0.0262818\n",
      "\tspeed: 0.1351s/iter; left time: 8379.1070s\n",
      "\titers: 500, epoch: 7 | loss: 0.0175326\n",
      "\tspeed: 0.1336s/iter; left time: 8269.3489s\n",
      "\titers: 600, epoch: 7 | loss: 0.0228772\n",
      "\tspeed: 0.1363s/iter; left time: 8424.8114s\n",
      "\titers: 700, epoch: 7 | loss: 0.0226232\n",
      "\tspeed: 0.1348s/iter; left time: 8318.8391s\n",
      "\titers: 800, epoch: 7 | loss: 0.0138826\n",
      "\tspeed: 0.1341s/iter; left time: 8259.0888s\n",
      "\titers: 900, epoch: 7 | loss: 0.0142537\n",
      "\tspeed: 0.1340s/iter; left time: 8240.6064s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0185196\n",
      "\tspeed: 0.1343s/iter; left time: 8247.4938s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0187493\n",
      "\tspeed: 0.1354s/iter; left time: 8297.4674s\n",
      "Epoch: 7 cost time: 00h:02m:30.26s\n",
      "Epoch: 7 | Train Loss: 0.0231316 Vali Loss: 0.0302424 Test Loss: 0.0350993\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0193801\n",
      "\tspeed: 0.4514s/iter; left time: 26110.7881s\n",
      "\titers: 200, epoch: 8 | loss: 0.0185916\n",
      "\tspeed: 0.1344s/iter; left time: 7759.8646s\n",
      "\titers: 300, epoch: 8 | loss: 0.0213563\n",
      "\tspeed: 0.1354s/iter; left time: 7803.5777s\n",
      "\titers: 400, epoch: 8 | loss: 0.0218396\n",
      "\tspeed: 0.1318s/iter; left time: 7584.4618s\n",
      "\titers: 500, epoch: 8 | loss: 0.0290579\n",
      "\tspeed: 0.1355s/iter; left time: 7782.1843s\n",
      "\titers: 600, epoch: 8 | loss: 0.0331087\n",
      "\tspeed: 0.1510s/iter; left time: 8655.8468s\n",
      "\titers: 700, epoch: 8 | loss: 0.0227459\n",
      "\tspeed: 0.1348s/iter; left time: 7713.7551s\n",
      "\titers: 800, epoch: 8 | loss: 0.0252364\n",
      "\tspeed: 0.1323s/iter; left time: 7560.6869s\n",
      "\titers: 900, epoch: 8 | loss: 0.0219207\n",
      "\tspeed: 0.1318s/iter; left time: 7517.8268s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0333456\n",
      "\tspeed: 0.1335s/iter; left time: 7604.4913s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0219904\n",
      "\tspeed: 0.1341s/iter; left time: 7623.0934s\n",
      "Epoch: 8 cost time: 00h:02m:31.45s\n",
      "Epoch: 8 | Train Loss: 0.0225641 Vali Loss: 0.0309571 Test Loss: 0.0370625\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0179517\n",
      "\tspeed: 0.4581s/iter; left time: 24458.1708s\n",
      "\titers: 200, epoch: 9 | loss: 0.0256749\n",
      "\tspeed: 0.1337s/iter; left time: 7125.7395s\n",
      "\titers: 300, epoch: 9 | loss: 0.0194726\n",
      "\tspeed: 0.1347s/iter; left time: 7164.2771s\n",
      "\titers: 400, epoch: 9 | loss: 0.0239984\n",
      "\tspeed: 0.1338s/iter; left time: 7100.6239s\n",
      "\titers: 500, epoch: 9 | loss: 0.0159293\n",
      "\tspeed: 0.1345s/iter; left time: 7125.6781s\n",
      "\titers: 600, epoch: 9 | loss: 0.0192116\n",
      "\tspeed: 0.1330s/iter; left time: 7034.6980s\n",
      "\titers: 700, epoch: 9 | loss: 0.0197610\n",
      "\tspeed: 0.1326s/iter; left time: 7001.6594s\n",
      "\titers: 800, epoch: 9 | loss: 0.0186225\n",
      "\tspeed: 0.1327s/iter; left time: 6989.3650s\n",
      "\titers: 900, epoch: 9 | loss: 0.0202066\n",
      "\tspeed: 0.1332s/iter; left time: 7002.1728s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0174803\n",
      "\tspeed: 0.1329s/iter; left time: 6974.2119s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0254039\n",
      "\tspeed: 0.1322s/iter; left time: 6924.9990s\n",
      "Epoch: 9 cost time: 00h:02m:29.02s\n",
      "Epoch: 9 | Train Loss: 0.0215361 Vali Loss: 0.0309942 Test Loss: 0.0375069\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0238908\n",
      "\tspeed: 0.4433s/iter; left time: 21687.4134s\n",
      "\titers: 200, epoch: 10 | loss: 0.0189626\n",
      "\tspeed: 0.1328s/iter; left time: 6482.5742s\n",
      "\titers: 300, epoch: 10 | loss: 0.0185715\n",
      "\tspeed: 0.1334s/iter; left time: 6499.3279s\n",
      "\titers: 400, epoch: 10 | loss: 0.0208660\n",
      "\tspeed: 0.1517s/iter; left time: 7376.7239s\n",
      "\titers: 500, epoch: 10 | loss: 0.0276698\n",
      "\tspeed: 0.1340s/iter; left time: 6500.9074s\n",
      "\titers: 600, epoch: 10 | loss: 0.0254691\n",
      "\tspeed: 0.1332s/iter; left time: 6452.5390s\n",
      "\titers: 700, epoch: 10 | loss: 0.0252488\n",
      "\tspeed: 0.1334s/iter; left time: 6448.5702s\n",
      "\titers: 800, epoch: 10 | loss: 0.0194837\n",
      "\tspeed: 0.1330s/iter; left time: 6412.6507s\n",
      "\titers: 900, epoch: 10 | loss: 0.0210351\n",
      "\tspeed: 0.1319s/iter; left time: 6347.9444s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0163288\n",
      "\tspeed: 0.1336s/iter; left time: 6414.8274s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0178875\n",
      "\tspeed: 0.1337s/iter; left time: 6407.5426s\n",
      "Epoch: 10 cost time: 00h:02m:30.98s\n",
      "Epoch: 10 | Train Loss: 0.0208566 Vali Loss: 0.0317293 Test Loss: 0.0390418\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0179506\n",
      "\tspeed: 0.4443s/iter; left time: 19756.4008s\n",
      "\titers: 200, epoch: 11 | loss: 0.0186568\n",
      "\tspeed: 0.1374s/iter; left time: 6095.1041s\n",
      "\titers: 300, epoch: 11 | loss: 0.0261067\n",
      "\tspeed: 0.1360s/iter; left time: 6022.0215s\n",
      "\titers: 400, epoch: 11 | loss: 0.0201972\n",
      "\tspeed: 0.1411s/iter; left time: 6233.8208s\n",
      "\titers: 500, epoch: 11 | loss: 0.0227132\n",
      "\tspeed: 0.1445s/iter; left time: 6368.5941s\n",
      "\titers: 600, epoch: 11 | loss: 0.0168747\n",
      "\tspeed: 0.1320s/iter; left time: 5806.0565s\n",
      "\titers: 700, epoch: 11 | loss: 0.0244474\n",
      "\tspeed: 0.1326s/iter; left time: 5819.1693s\n",
      "\titers: 800, epoch: 11 | loss: 0.0158708\n",
      "\tspeed: 0.1348s/iter; left time: 5898.1910s\n",
      "\titers: 900, epoch: 11 | loss: 0.0233177\n",
      "\tspeed: 0.1324s/iter; left time: 5779.9168s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0231835\n",
      "\tspeed: 0.1352s/iter; left time: 5889.8572s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0227261\n",
      "\tspeed: 0.1359s/iter; left time: 5909.5680s\n",
      "Epoch: 11 cost time: 00h:02m:32.88s\n",
      "Epoch: 11 | Train Loss: 0.0203122 Vali Loss: 0.0328566 Test Loss: 0.0383958\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Intermediate time for DE and pred_len 96: 00h:34m:11.77s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-13 11:03:27,894] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:03:27,932] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:03:27,932] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:03:27,937] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:03:29,020] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "train 142285\n",
      "train 142285\n",
      "val 30365\n",
      "val 30365\n",
      "train 142285\n",
      "test 30365\n",
      "test 30365\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-13 11:03:30,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 11:03:31,995] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 11:03:31,996] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 11:03:31,997] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 11:03:31,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 11:03:31,998] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 11:03:31,998] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 11:03:32,394] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 11:03:32,395] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:03:32,395] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.76 GB, percent = 2.9%\n",
      "[2024-11-13 11:03:32,558] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 11:03:32,559] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:03:32,559] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.09 GB, percent = 2.9%\n",
      "[2024-11-13 11:03:32,559] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 11:03:32,705] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 11:03:32,705] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:03:32,705] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.52 GB, percent = 2.9%\n",
      "[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 11:03:32,706] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2acedd5b90>\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 11:03:32,708] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0359368\n",
      "\tspeed: 0.1722s/iter; left time: 15298.7937s\n",
      "\titers: 200, epoch: 1 | loss: 0.0448534\n",
      "\tspeed: 0.1391s/iter; left time: 12339.8766s\n",
      "\titers: 300, epoch: 1 | loss: 0.0433941\n",
      "\tspeed: 0.1450s/iter; left time: 12845.9561s\n",
      "\titers: 400, epoch: 1 | loss: 0.0418473\n",
      "\tspeed: 0.1493s/iter; left time: 13217.9844s\n",
      "\titers: 500, epoch: 1 | loss: 0.0473000\n",
      "\tspeed: 0.1431s/iter; left time: 12653.4435s\n",
      "\titers: 600, epoch: 1 | loss: 0.0449542\n",
      "\tspeed: 0.1420s/iter; left time: 12543.6537s\n",
      "\titers: 700, epoch: 1 | loss: 0.0429755\n",
      "\tspeed: 0.1420s/iter; left time: 12530.1423s\n",
      "\titers: 800, epoch: 1 | loss: 0.0494640\n",
      "\tspeed: 0.1402s/iter; left time: 12352.5080s\n",
      "\titers: 900, epoch: 1 | loss: 0.0413226\n",
      "\tspeed: 0.1414s/iter; left time: 12443.0984s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0430699\n",
      "\tspeed: 0.1437s/iter; left time: 12634.2753s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0416704\n",
      "\tspeed: 0.1491s/iter; left time: 13095.6824s\n",
      "Epoch: 1 cost time: 00h:02m:40.67s\n",
      "Epoch: 1 | Train Loss: 0.0463432 Vali Loss: 0.0367494 Test Loss: 0.0425076\n",
      "Validation loss decreased (inf --> 0.036749).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0301424\n",
      "\tspeed: 0.5470s/iter; left time: 46156.3133s\n",
      "\titers: 200, epoch: 2 | loss: 0.0237679\n",
      "\tspeed: 0.1380s/iter; left time: 11629.3456s\n",
      "\titers: 300, epoch: 2 | loss: 0.0273275\n",
      "\tspeed: 0.1396s/iter; left time: 11747.6284s\n",
      "\titers: 400, epoch: 2 | loss: 0.0363315\n",
      "\tspeed: 0.1532s/iter; left time: 12877.1144s\n",
      "\titers: 500, epoch: 2 | loss: 0.0213703\n",
      "\tspeed: 0.1408s/iter; left time: 11825.7740s\n",
      "\titers: 600, epoch: 2 | loss: 0.0212502\n",
      "\tspeed: 0.1375s/iter; left time: 11532.5011s\n",
      "\titers: 700, epoch: 2 | loss: 0.0261255\n",
      "\tspeed: 0.1380s/iter; left time: 11560.4512s\n",
      "\titers: 800, epoch: 2 | loss: 0.0245793\n",
      "\tspeed: 0.1376s/iter; left time: 11510.0097s\n",
      "\titers: 900, epoch: 2 | loss: 0.0246260\n",
      "\tspeed: 0.1379s/iter; left time: 11528.3340s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0303873\n",
      "\tspeed: 0.1396s/iter; left time: 11652.7868s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0278004\n",
      "\tspeed: 0.1860s/iter; left time: 15507.0476s\n",
      "Epoch: 2 cost time: 00h:02m:40.92s\n",
      "Epoch: 2 | Train Loss: 0.0293619 Vali Loss: 0.0321608 Test Loss: 0.0374062\n",
      "Validation loss decreased (0.036749 --> 0.032161).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0327742\n",
      "\tspeed: 0.7670s/iter; left time: 61302.4705s\n",
      "\titers: 200, epoch: 3 | loss: 0.0426615\n",
      "\tspeed: 0.1419s/iter; left time: 11325.9891s\n",
      "\titers: 300, epoch: 3 | loss: 0.0251191\n",
      "\tspeed: 0.1383s/iter; left time: 11030.4376s\n",
      "\titers: 400, epoch: 3 | loss: 0.0265200\n",
      "\tspeed: 0.1394s/iter; left time: 11102.7374s\n",
      "\titers: 500, epoch: 3 | loss: 0.0287092\n",
      "\tspeed: 0.1381s/iter; left time: 10984.0975s\n",
      "\titers: 600, epoch: 3 | loss: 0.0326744\n",
      "\tspeed: 0.1406s/iter; left time: 11164.0744s\n",
      "\titers: 700, epoch: 3 | loss: 0.0272740\n",
      "\tspeed: 0.1370s/iter; left time: 10870.6939s\n",
      "\titers: 800, epoch: 3 | loss: 0.0219435\n",
      "\tspeed: 0.1365s/iter; left time: 10810.8142s\n",
      "\titers: 900, epoch: 3 | loss: 0.0302642\n",
      "\tspeed: 0.1479s/iter; left time: 11706.2359s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0288475\n",
      "\tspeed: 0.1450s/iter; left time: 11456.6996s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0385880\n",
      "\tspeed: 0.1409s/iter; left time: 11124.5729s\n",
      "Epoch: 3 cost time: 00h:02m:37.21s\n",
      "Epoch: 3 | Train Loss: 0.0275353 Vali Loss: 0.0315600 Test Loss: 0.0373444\n",
      "Validation loss decreased (0.032161 --> 0.031560).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0265749\n",
      "\tspeed: 0.5563s/iter; left time: 41990.0144s\n",
      "\titers: 200, epoch: 4 | loss: 0.0307901\n",
      "\tspeed: 0.1382s/iter; left time: 10415.4185s\n",
      "\titers: 300, epoch: 4 | loss: 0.0240945\n",
      "\tspeed: 0.1376s/iter; left time: 10359.3495s\n",
      "\titers: 400, epoch: 4 | loss: 0.0293844\n",
      "\tspeed: 0.1484s/iter; left time: 11155.4229s\n",
      "\titers: 500, epoch: 4 | loss: 0.0243979\n",
      "\tspeed: 0.1698s/iter; left time: 12746.2554s\n",
      "\titers: 600, epoch: 4 | loss: 0.0342317\n",
      "\tspeed: 0.1387s/iter; left time: 10401.2931s\n",
      "\titers: 700, epoch: 4 | loss: 0.0235715\n",
      "\tspeed: 0.1576s/iter; left time: 11803.1621s\n",
      "\titers: 800, epoch: 4 | loss: 0.0298171\n",
      "\tspeed: 0.1488s/iter; left time: 11124.8708s\n",
      "\titers: 900, epoch: 4 | loss: 0.0243449\n",
      "\tspeed: 0.1443s/iter; left time: 10777.9346s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0236856\n",
      "\tspeed: 0.1365s/iter; left time: 10178.4176s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0263085\n",
      "\tspeed: 0.1393s/iter; left time: 10372.4091s\n",
      "Epoch: 4 cost time: 00h:02m:42.83s\n",
      "Epoch: 4 | Train Loss: 0.0266994 Vali Loss: 0.0314476 Test Loss: 0.0374681\n",
      "Validation loss decreased (0.031560 --> 0.031448).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0282643\n",
      "\tspeed: 0.5089s/iter; left time: 36154.2422s\n",
      "\titers: 200, epoch: 5 | loss: 0.0272768\n",
      "\tspeed: 0.1390s/iter; left time: 9859.2993s\n",
      "\titers: 300, epoch: 5 | loss: 0.0217751\n",
      "\tspeed: 0.1361s/iter; left time: 9639.1548s\n",
      "\titers: 400, epoch: 5 | loss: 0.0190624\n",
      "\tspeed: 0.1354s/iter; left time: 9574.5745s\n",
      "\titers: 500, epoch: 5 | loss: 0.0376097\n",
      "\tspeed: 0.1360s/iter; left time: 9604.8419s\n",
      "\titers: 600, epoch: 5 | loss: 0.0226067\n",
      "\tspeed: 0.1356s/iter; left time: 9566.3206s\n",
      "\titers: 700, epoch: 5 | loss: 0.0193413\n",
      "\tspeed: 0.1353s/iter; left time: 9528.9752s\n",
      "\titers: 800, epoch: 5 | loss: 0.0192956\n",
      "\tspeed: 0.1354s/iter; left time: 9525.3985s\n",
      "\titers: 900, epoch: 5 | loss: 0.0285773\n",
      "\tspeed: 0.1371s/iter; left time: 9626.8953s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0276020\n",
      "\tspeed: 0.1358s/iter; left time: 9522.4454s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0244862\n",
      "\tspeed: 0.1366s/iter; left time: 9568.9091s\n",
      "Epoch: 5 cost time: 00h:02m:31.96s\n",
      "Epoch: 5 | Train Loss: 0.0259062 Vali Loss: 0.0320041 Test Loss: 0.0369165\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0214580\n",
      "\tspeed: 0.4321s/iter; left time: 28773.2660s\n",
      "\titers: 200, epoch: 6 | loss: 0.0234301\n",
      "\tspeed: 0.1380s/iter; left time: 9177.8334s\n",
      "\titers: 300, epoch: 6 | loss: 0.0200381\n",
      "\tspeed: 0.1366s/iter; left time: 9070.3838s\n",
      "\titers: 400, epoch: 6 | loss: 0.0311940\n",
      "\tspeed: 0.1360s/iter; left time: 9013.8870s\n",
      "\titers: 500, epoch: 6 | loss: 0.0301950\n",
      "\tspeed: 0.1356s/iter; left time: 8977.4810s\n",
      "\titers: 600, epoch: 6 | loss: 0.0203328\n",
      "\tspeed: 0.1355s/iter; left time: 8953.2152s\n",
      "\titers: 700, epoch: 6 | loss: 0.0225864\n",
      "\tspeed: 0.1355s/iter; left time: 8939.1808s\n",
      "\titers: 800, epoch: 6 | loss: 0.0181251\n",
      "\tspeed: 0.1355s/iter; left time: 8927.4561s\n",
      "\titers: 900, epoch: 6 | loss: 0.0198194\n",
      "\tspeed: 0.1363s/iter; left time: 8967.0378s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0240496\n",
      "\tspeed: 0.1372s/iter; left time: 9014.9313s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0215907\n",
      "\tspeed: 0.1351s/iter; left time: 8863.7746s\n",
      "Epoch: 6 cost time: 00h:02m:32.10s\n",
      "Epoch: 6 | Train Loss: 0.0249147 Vali Loss: 0.0328208 Test Loss: 0.0379578\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0212492\n",
      "\tspeed: 0.4438s/iter; left time: 27578.1033s\n",
      "\titers: 200, epoch: 7 | loss: 0.0209108\n",
      "\tspeed: 0.1347s/iter; left time: 8360.3285s\n",
      "\titers: 300, epoch: 7 | loss: 0.0251728\n",
      "\tspeed: 0.1354s/iter; left time: 8384.9002s\n",
      "\titers: 400, epoch: 7 | loss: 0.0205974\n",
      "\tspeed: 0.1346s/iter; left time: 8325.8712s\n",
      "\titers: 500, epoch: 7 | loss: 0.0166121\n",
      "\tspeed: 0.1346s/iter; left time: 8309.2218s\n",
      "\titers: 600, epoch: 7 | loss: 0.0294926\n",
      "\tspeed: 0.1347s/iter; left time: 8302.5436s\n",
      "\titers: 700, epoch: 7 | loss: 0.0198359\n",
      "\tspeed: 0.1350s/iter; left time: 8310.0735s\n",
      "\titers: 800, epoch: 7 | loss: 0.0242572\n",
      "\tspeed: 0.1352s/iter; left time: 8304.4934s\n",
      "\titers: 900, epoch: 7 | loss: 0.0213706\n",
      "\tspeed: 0.1349s/iter; left time: 8277.2322s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0236055\n",
      "\tspeed: 0.1352s/iter; left time: 8280.2130s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0265674\n",
      "\tspeed: 0.1349s/iter; left time: 8250.0903s\n",
      "Epoch: 7 cost time: 00h:02m:30.45s\n",
      "Epoch: 7 | Train Loss: 0.0237292 Vali Loss: 0.0337015 Test Loss: 0.0404406\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0256783\n",
      "\tspeed: 0.4228s/iter; left time: 24393.0934s\n",
      "\titers: 200, epoch: 8 | loss: 0.0266715\n",
      "\tspeed: 0.1351s/iter; left time: 7779.5734s\n",
      "\titers: 300, epoch: 8 | loss: 0.0207095\n",
      "\tspeed: 0.1362s/iter; left time: 7832.3826s\n",
      "\titers: 400, epoch: 8 | loss: 0.0248391\n",
      "\tspeed: 0.1352s/iter; left time: 7761.6284s\n",
      "\titers: 500, epoch: 8 | loss: 0.0196725\n",
      "\tspeed: 0.1351s/iter; left time: 7740.5252s\n",
      "\titers: 600, epoch: 8 | loss: 0.0184297\n",
      "\tspeed: 0.1356s/iter; left time: 7753.9399s\n",
      "\titers: 700, epoch: 8 | loss: 0.0277658\n",
      "\tspeed: 0.1359s/iter; left time: 7761.8067s\n",
      "\titers: 800, epoch: 8 | loss: 0.0265737\n",
      "\tspeed: 0.1367s/iter; left time: 7793.6799s\n",
      "\titers: 900, epoch: 8 | loss: 0.0243952\n",
      "\tspeed: 0.1349s/iter; left time: 7675.3084s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0238605\n",
      "\tspeed: 0.1349s/iter; left time: 7659.9294s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0290543\n",
      "\tspeed: 0.1349s/iter; left time: 7649.6008s\n",
      "Epoch: 8 cost time: 00h:02m:31.05s\n",
      "Epoch: 8 | Train Loss: 0.0225662 Vali Loss: 0.0353154 Test Loss: 0.0423327\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0227777\n",
      "\tspeed: 0.4248s/iter; left time: 22623.1170s\n",
      "\titers: 200, epoch: 9 | loss: 0.0262419\n",
      "\tspeed: 0.1364s/iter; left time: 7249.8248s\n",
      "\titers: 300, epoch: 9 | loss: 0.0207251\n",
      "\tspeed: 0.1370s/iter; left time: 7270.5946s\n",
      "\titers: 400, epoch: 9 | loss: 0.0197221\n",
      "\tspeed: 0.1358s/iter; left time: 7192.1330s\n",
      "\titers: 500, epoch: 9 | loss: 0.0167211\n",
      "\tspeed: 0.1353s/iter; left time: 7151.8524s\n",
      "\titers: 600, epoch: 9 | loss: 0.0236003\n",
      "\tspeed: 0.1357s/iter; left time: 7161.1481s\n",
      "\titers: 700, epoch: 9 | loss: 0.0170809\n",
      "\tspeed: 0.1347s/iter; left time: 7092.8895s\n",
      "\titers: 800, epoch: 9 | loss: 0.0209369\n",
      "\tspeed: 0.1344s/iter; left time: 7062.4339s\n",
      "\titers: 900, epoch: 9 | loss: 0.0263193\n",
      "\tspeed: 0.1481s/iter; left time: 7767.3616s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0252886\n",
      "\tspeed: 0.1777s/iter; left time: 9305.0562s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0198990\n",
      "\tspeed: 0.1418s/iter; left time: 7411.8532s\n",
      "Epoch: 9 cost time: 00h:02m:37.36s\n",
      "Epoch: 9 | Train Loss: 0.0214407 Vali Loss: 0.0361602 Test Loss: 0.0435897\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Intermediate time for DE and pred_len 168: 00h:29m:20.95s\n",
      "\n",
      "Intermediate time for DE: 01h:43m:22.52s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-13 11:32:47,356] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:32:47,364] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:32:47,366] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:32:47,373] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 11:32:48,369] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:32:48,369] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:32:48,370] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:32:48,370] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 11:32:48,370] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "train 143005\n",
      "train 143005\n",
      "train 143005\n",
      "val 31085\n",
      "val 31085\n",
      "val 31085\n",
      "test 31085\n",
      "test 31085\n",
      "test 31085\n",
      "[2024-11-13 11:32:50,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 11:32:51,137] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 11:32:51,138] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 11:32:51,138] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 11:32:51,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 11:32:51,139] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 11:32:51,140] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 11:32:51,461] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 11:32:51,462] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:32:51,466] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.52 GB, percent = 3.0%\n",
      "[2024-11-13 11:32:51,628] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 11:32:51,628] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:32:51,629] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 3.1%\n",
      "[2024-11-13 11:32:51,629] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 11:32:51,778] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 11:32:51,779] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 11:32:51,779] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.09 GB, percent = 3.1%\n",
      "[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5fb44d2ed0>\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 11:32:51,781] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0445508\n",
      "\tspeed: 0.1689s/iter; left time: 15079.9524s\n",
      "\titers: 200, epoch: 1 | loss: 0.0364530\n",
      "\tspeed: 0.1673s/iter; left time: 14916.7352s\n",
      "\titers: 300, epoch: 1 | loss: 0.0445299\n",
      "\tspeed: 0.1678s/iter; left time: 14942.4247s\n",
      "\titers: 400, epoch: 1 | loss: 0.0314661\n",
      "\tspeed: 0.1670s/iter; left time: 14852.4109s\n",
      "\titers: 500, epoch: 1 | loss: 0.0345089\n",
      "\tspeed: 0.1688s/iter; left time: 14996.9240s\n",
      "\titers: 600, epoch: 1 | loss: 0.0283519\n",
      "\tspeed: 0.1704s/iter; left time: 15126.5485s\n",
      "\titers: 700, epoch: 1 | loss: 0.0160935\n",
      "\tspeed: 0.1562s/iter; left time: 13847.6493s\n",
      "\titers: 800, epoch: 1 | loss: 0.0235337\n",
      "\tspeed: 0.1582s/iter; left time: 14010.2982s\n",
      "\titers: 900, epoch: 1 | loss: 0.0168510\n",
      "\tspeed: 0.1689s/iter; left time: 14936.8908s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0143824\n",
      "\tspeed: 0.1665s/iter; left time: 14713.4831s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0171138\n",
      "\tspeed: 0.1690s/iter; left time: 14916.8498s\n",
      "Epoch: 1 cost time: 00h:03m:04.57s\n",
      "Epoch: 1 | Train Loss: 0.0295342 Vali Loss: 0.0213105 Test Loss: 0.0279612\n",
      "Validation loss decreased (inf --> 0.021310).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0167756\n",
      "\tspeed: 0.6396s/iter; left time: 54235.3679s\n",
      "\titers: 200, epoch: 2 | loss: 0.0156655\n",
      "\tspeed: 0.1383s/iter; left time: 11713.1563s\n",
      "\titers: 300, epoch: 2 | loss: 0.0240314\n",
      "\tspeed: 0.1398s/iter; left time: 11828.6467s\n",
      "\titers: 400, epoch: 2 | loss: 0.0155516\n",
      "\tspeed: 0.1391s/iter; left time: 11749.0185s\n",
      "\titers: 500, epoch: 2 | loss: 0.0197155\n",
      "\tspeed: 0.1631s/iter; left time: 13763.8673s\n",
      "\titers: 600, epoch: 2 | loss: 0.0126239\n",
      "\tspeed: 0.1368s/iter; left time: 11533.9759s\n",
      "\titers: 700, epoch: 2 | loss: 0.0161524\n",
      "\tspeed: 0.1373s/iter; left time: 11562.3026s\n",
      "\titers: 800, epoch: 2 | loss: 0.0159542\n",
      "\tspeed: 0.1377s/iter; left time: 11578.6068s\n",
      "\titers: 900, epoch: 2 | loss: 0.0152068\n",
      "\tspeed: 0.1375s/iter; left time: 11547.2452s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0121104\n",
      "\tspeed: 0.1370s/iter; left time: 11494.6049s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0178026\n",
      "\tspeed: 0.1372s/iter; left time: 11496.7225s\n",
      "Epoch: 2 cost time: 00h:02m:37.20s\n",
      "Epoch: 2 | Train Loss: 0.0173583 Vali Loss: 0.0201293 Test Loss: 0.0262493\n",
      "Validation loss decreased (0.021310 --> 0.020129).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0216605\n",
      "\tspeed: 0.6332s/iter; left time: 50859.1483s\n",
      "\titers: 200, epoch: 3 | loss: 0.0127704\n",
      "\tspeed: 0.1516s/iter; left time: 12159.6553s\n",
      "\titers: 300, epoch: 3 | loss: 0.0188288\n",
      "\tspeed: 0.1481s/iter; left time: 11867.3880s\n",
      "\titers: 400, epoch: 3 | loss: 0.0127714\n",
      "\tspeed: 0.1499s/iter; left time: 11999.0089s\n",
      "\titers: 500, epoch: 3 | loss: 0.0111119\n",
      "\tspeed: 0.1500s/iter; left time: 11986.5586s\n",
      "\titers: 600, epoch: 3 | loss: 0.0135245\n",
      "\tspeed: 0.1521s/iter; left time: 12140.7001s\n",
      "\titers: 700, epoch: 3 | loss: 0.0192609\n",
      "\tspeed: 0.1467s/iter; left time: 11697.7410s\n",
      "\titers: 800, epoch: 3 | loss: 0.0223024\n",
      "\tspeed: 0.1472s/iter; left time: 11720.5749s\n",
      "\titers: 900, epoch: 3 | loss: 0.0113499\n",
      "\tspeed: 0.1518s/iter; left time: 12068.2996s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0146333\n",
      "\tspeed: 0.1515s/iter; left time: 12036.7716s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0131600\n",
      "\tspeed: 0.1501s/iter; left time: 11906.7998s\n",
      "Epoch: 3 cost time: 00h:02m:48.42s\n",
      "Epoch: 3 | Train Loss: 0.0163123 Vali Loss: 0.0200448 Test Loss: 0.0264302\n",
      "Validation loss decreased (0.020129 --> 0.020045).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0187275\n",
      "\tspeed: 0.5255s/iter; left time: 39864.9755s\n",
      "\titers: 200, epoch: 4 | loss: 0.0202246\n",
      "\tspeed: 0.1358s/iter; left time: 10284.4427s\n",
      "\titers: 300, epoch: 4 | loss: 0.0182328\n",
      "\tspeed: 0.1394s/iter; left time: 10548.3262s\n",
      "\titers: 400, epoch: 4 | loss: 0.0135744\n",
      "\tspeed: 0.1797s/iter; left time: 13581.2004s\n",
      "\titers: 500, epoch: 4 | loss: 0.0195635\n",
      "\tspeed: 0.1685s/iter; left time: 12716.0054s\n",
      "\titers: 600, epoch: 4 | loss: 0.0121597\n",
      "\tspeed: 0.1414s/iter; left time: 10655.6937s\n",
      "\titers: 700, epoch: 4 | loss: 0.0168482\n",
      "\tspeed: 0.1511s/iter; left time: 11372.1743s\n",
      "\titers: 800, epoch: 4 | loss: 0.0135412\n",
      "\tspeed: 0.1427s/iter; left time: 10727.5482s\n",
      "\titers: 900, epoch: 4 | loss: 0.0165432\n",
      "\tspeed: 0.1479s/iter; left time: 11099.6962s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0164242\n",
      "\tspeed: 0.1476s/iter; left time: 11060.4945s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0161591\n",
      "\tspeed: 0.1453s/iter; left time: 10875.8727s\n",
      "Epoch: 4 cost time: 00h:02m:46.79s\n",
      "Epoch: 4 | Train Loss: 0.0159980 Vali Loss: 0.0201996 Test Loss: 0.0260893\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0141485\n",
      "\tspeed: 0.4704s/iter; left time: 33581.9524s\n",
      "\titers: 200, epoch: 5 | loss: 0.0138484\n",
      "\tspeed: 0.1479s/iter; left time: 10541.3037s\n",
      "\titers: 300, epoch: 5 | loss: 0.0135508\n",
      "\tspeed: 0.1406s/iter; left time: 10007.6190s\n",
      "\titers: 400, epoch: 5 | loss: 0.0143307\n",
      "\tspeed: 0.1481s/iter; left time: 10524.7770s\n",
      "\titers: 500, epoch: 5 | loss: 0.0136095\n",
      "\tspeed: 0.1464s/iter; left time: 10391.2816s\n",
      "\titers: 600, epoch: 5 | loss: 0.0170796\n",
      "\tspeed: 0.1447s/iter; left time: 10254.4810s\n",
      "\titers: 700, epoch: 5 | loss: 0.0158733\n",
      "\tspeed: 0.1441s/iter; left time: 10199.7434s\n",
      "\titers: 800, epoch: 5 | loss: 0.0140419\n",
      "\tspeed: 0.1465s/iter; left time: 10355.5123s\n",
      "\titers: 900, epoch: 5 | loss: 0.0146981\n",
      "\tspeed: 0.1459s/iter; left time: 10297.4784s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0159149\n",
      "\tspeed: 0.1657s/iter; left time: 11679.7014s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0173821\n",
      "\tspeed: 0.1356s/iter; left time: 9543.6569s\n",
      "Epoch: 5 cost time: 00h:02m:44.14s\n",
      "Epoch: 5 | Train Loss: 0.0158847 Vali Loss: 0.0195334 Test Loss: 0.0254453\n",
      "Validation loss decreased (0.020045 --> 0.019533).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0143947\n",
      "\tspeed: 0.5572s/iter; left time: 37285.3580s\n",
      "\titers: 200, epoch: 6 | loss: 0.0158171\n",
      "\tspeed: 0.1436s/iter; left time: 9598.3176s\n",
      "\titers: 300, epoch: 6 | loss: 0.0155350\n",
      "\tspeed: 0.1425s/iter; left time: 9505.8236s\n",
      "\titers: 400, epoch: 6 | loss: 0.0205618\n",
      "\tspeed: 0.1475s/iter; left time: 9825.6910s\n",
      "\titers: 500, epoch: 6 | loss: 0.0150704\n",
      "\tspeed: 0.1468s/iter; left time: 9765.8414s\n",
      "\titers: 600, epoch: 6 | loss: 0.0142039\n",
      "\tspeed: 0.1409s/iter; left time: 9357.1907s\n",
      "\titers: 700, epoch: 6 | loss: 0.0139274\n",
      "\tspeed: 0.1466s/iter; left time: 9719.8030s\n",
      "\titers: 800, epoch: 6 | loss: 0.0114122\n",
      "\tspeed: 0.1391s/iter; left time: 9212.6672s\n",
      "\titers: 900, epoch: 6 | loss: 0.0197675\n",
      "\tspeed: 0.1466s/iter; left time: 9696.4502s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0182349\n",
      "\tspeed: 0.1440s/iter; left time: 9506.8018s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0153248\n",
      "\tspeed: 0.1451s/iter; left time: 9561.9837s\n",
      "Epoch: 6 cost time: 00h:02m:42.22s\n",
      "Epoch: 6 | Train Loss: 0.0155889 Vali Loss: 0.0196016 Test Loss: 0.0255106\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0122040\n",
      "\tspeed: 0.4647s/iter; left time: 29020.8853s\n",
      "\titers: 200, epoch: 7 | loss: 0.0146544\n",
      "\tspeed: 0.1464s/iter; left time: 9127.8423s\n",
      "\titers: 300, epoch: 7 | loss: 0.0146292\n",
      "\tspeed: 0.1468s/iter; left time: 9140.4399s\n",
      "\titers: 400, epoch: 7 | loss: 0.0161332\n",
      "\tspeed: 0.1527s/iter; left time: 9487.8659s\n",
      "\titers: 500, epoch: 7 | loss: 0.0123958\n",
      "\tspeed: 0.1769s/iter; left time: 10979.2061s\n",
      "\titers: 600, epoch: 7 | loss: 0.0120419\n",
      "\tspeed: 0.1528s/iter; left time: 9468.6101s\n",
      "\titers: 700, epoch: 7 | loss: 0.0091025\n",
      "\tspeed: 0.1497s/iter; left time: 9256.7827s\n",
      "\titers: 800, epoch: 7 | loss: 0.0131330\n",
      "\tspeed: 0.1454s/iter; left time: 8976.2784s\n",
      "\titers: 900, epoch: 7 | loss: 0.0155955\n",
      "\tspeed: 0.1426s/iter; left time: 8790.2632s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0199920\n",
      "\tspeed: 0.1477s/iter; left time: 9090.2497s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0171997\n",
      "\tspeed: 0.1468s/iter; left time: 9019.8215s\n",
      "Epoch: 7 cost time: 00h:02m:48.28s\n",
      "Epoch: 7 | Train Loss: 0.0154535 Vali Loss: 0.0196389 Test Loss: 0.0258326\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0115717\n",
      "\tspeed: 0.4594s/iter; left time: 26635.7757s\n",
      "\titers: 200, epoch: 8 | loss: 0.0151119\n",
      "\tspeed: 0.1439s/iter; left time: 8331.4829s\n",
      "\titers: 300, epoch: 8 | loss: 0.0155807\n",
      "\tspeed: 0.1419s/iter; left time: 8202.2757s\n",
      "\titers: 400, epoch: 8 | loss: 0.0152087\n",
      "\tspeed: 0.1442s/iter; left time: 8316.6771s\n",
      "\titers: 500, epoch: 8 | loss: 0.0133925\n",
      "\tspeed: 0.1481s/iter; left time: 8528.4769s\n",
      "\titers: 600, epoch: 8 | loss: 0.0131203\n",
      "\tspeed: 0.1452s/iter; left time: 8349.3047s\n",
      "\titers: 700, epoch: 8 | loss: 0.0103167\n",
      "\tspeed: 0.1489s/iter; left time: 8547.3725s\n",
      "\titers: 800, epoch: 8 | loss: 0.0162530\n",
      "\tspeed: 0.1362s/iter; left time: 7802.3516s\n",
      "\titers: 900, epoch: 8 | loss: 0.0113713\n",
      "\tspeed: 0.1336s/iter; left time: 7641.3838s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0115047\n",
      "\tspeed: 0.1334s/iter; left time: 7613.9256s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0225006\n",
      "\tspeed: 0.1340s/iter; left time: 7638.0302s\n",
      "Epoch: 8 cost time: 00h:02m:38.24s\n",
      "Epoch: 8 | Train Loss: 0.0151374 Vali Loss: 0.0196318 Test Loss: 0.0257646\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0119662\n",
      "\tspeed: 0.4323s/iter; left time: 23136.4566s\n",
      "\titers: 200, epoch: 9 | loss: 0.0165778\n",
      "\tspeed: 0.1415s/iter; left time: 7558.6949s\n",
      "\titers: 300, epoch: 9 | loss: 0.0131666\n",
      "\tspeed: 0.1729s/iter; left time: 9220.9088s\n",
      "\titers: 400, epoch: 9 | loss: 0.0152484\n",
      "\tspeed: 0.1743s/iter; left time: 9273.2876s\n",
      "\titers: 500, epoch: 9 | loss: 0.0153882\n",
      "\tspeed: 0.1357s/iter; left time: 7210.5692s\n",
      "\titers: 600, epoch: 9 | loss: 0.0150025\n",
      "\tspeed: 0.1538s/iter; left time: 8152.9610s\n",
      "\titers: 700, epoch: 9 | loss: 0.0164857\n",
      "\tspeed: 0.1407s/iter; left time: 7443.6514s\n",
      "\titers: 800, epoch: 9 | loss: 0.0132324\n",
      "\tspeed: 0.1478s/iter; left time: 7803.9538s\n",
      "\titers: 900, epoch: 9 | loss: 0.0190716\n",
      "\tspeed: 0.1481s/iter; left time: 7805.7896s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0120974\n",
      "\tspeed: 0.1443s/iter; left time: 7592.4004s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0095748\n",
      "\tspeed: 0.1460s/iter; left time: 7669.7458s\n",
      "Epoch: 9 cost time: 00h:02m:47.24s\n",
      "Epoch: 9 | Train Loss: 0.0148546 Vali Loss: 0.0195053 Test Loss: 0.0259596\n",
      "Validation loss decreased (0.019533 --> 0.019505).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0140567\n",
      "\tspeed: 0.6005s/iter; left time: 29452.7297s\n",
      "\titers: 200, epoch: 10 | loss: 0.0134446\n",
      "\tspeed: 0.1445s/iter; left time: 7074.5708s\n",
      "\titers: 300, epoch: 10 | loss: 0.0100605\n",
      "\tspeed: 0.1473s/iter; left time: 7194.5783s\n",
      "\titers: 400, epoch: 10 | loss: 0.0239360\n",
      "\tspeed: 0.1429s/iter; left time: 6964.2641s\n",
      "\titers: 500, epoch: 10 | loss: 0.0136836\n",
      "\tspeed: 0.1463s/iter; left time: 7115.9583s\n",
      "\titers: 600, epoch: 10 | loss: 0.0134052\n",
      "\tspeed: 0.1414s/iter; left time: 6863.0326s\n",
      "\titers: 700, epoch: 10 | loss: 0.0111096\n",
      "\tspeed: 0.1770s/iter; left time: 8576.8994s\n",
      "\titers: 800, epoch: 10 | loss: 0.0095586\n",
      "\tspeed: 0.1656s/iter; left time: 8006.5191s\n",
      "\titers: 900, epoch: 10 | loss: 0.0141145\n",
      "\tspeed: 0.1395s/iter; left time: 6732.0777s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0113207\n",
      "\tspeed: 0.1505s/iter; left time: 7248.4576s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0197075\n",
      "\tspeed: 0.1438s/iter; left time: 6910.6822s\n",
      "Epoch: 10 cost time: 00h:02m:47.56s\n",
      "Epoch: 10 | Train Loss: 0.0145774 Vali Loss: 0.0199446 Test Loss: 0.0268420\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0142818\n",
      "\tspeed: 0.4627s/iter; left time: 20626.4121s\n",
      "\titers: 200, epoch: 11 | loss: 0.0155543\n",
      "\tspeed: 0.1447s/iter; left time: 6437.4328s\n",
      "\titers: 300, epoch: 11 | loss: 0.0120371\n",
      "\tspeed: 0.1429s/iter; left time: 6343.1596s\n",
      "\titers: 400, epoch: 11 | loss: 0.0102055\n",
      "\tspeed: 0.1406s/iter; left time: 6225.9999s\n",
      "\titers: 500, epoch: 11 | loss: 0.0138252\n",
      "\tspeed: 0.1434s/iter; left time: 6337.3687s\n",
      "\titers: 600, epoch: 11 | loss: 0.0128423\n",
      "\tspeed: 0.1398s/iter; left time: 6160.6946s\n",
      "\titers: 700, epoch: 11 | loss: 0.0203481\n",
      "\tspeed: 0.1427s/iter; left time: 6273.9398s\n",
      "\titers: 800, epoch: 11 | loss: 0.0165933\n",
      "\tspeed: 0.1444s/iter; left time: 6336.4041s\n",
      "\titers: 900, epoch: 11 | loss: 0.0200436\n",
      "\tspeed: 0.1412s/iter; left time: 6181.4347s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0186101\n",
      "\tspeed: 0.1473s/iter; left time: 6435.7428s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0099120\n",
      "\tspeed: 0.1396s/iter; left time: 6084.1253s\n",
      "Epoch: 11 cost time: 00h:02m:39.88s\n",
      "Epoch: 11 | Train Loss: 0.0143655 Vali Loss: 0.0198954 Test Loss: 0.0276270\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0114732\n",
      "\tspeed: 0.4985s/iter; left time: 19997.8949s\n",
      "\titers: 200, epoch: 12 | loss: 0.0146292\n",
      "\tspeed: 0.1533s/iter; left time: 6132.4669s\n",
      "\titers: 300, epoch: 12 | loss: 0.0205463\n",
      "\tspeed: 0.1450s/iter; left time: 5786.8678s\n",
      "\titers: 400, epoch: 12 | loss: 0.0147206\n",
      "\tspeed: 0.1491s/iter; left time: 5934.5702s\n",
      "\titers: 500, epoch: 12 | loss: 0.0103157\n",
      "\tspeed: 0.1519s/iter; left time: 6030.5000s\n",
      "\titers: 600, epoch: 12 | loss: 0.0151519\n",
      "\tspeed: 0.1441s/iter; left time: 5709.7368s\n",
      "\titers: 700, epoch: 12 | loss: 0.0115998\n",
      "\tspeed: 0.1447s/iter; left time: 5718.7836s\n",
      "\titers: 800, epoch: 12 | loss: 0.0137087\n",
      "\tspeed: 0.1386s/iter; left time: 5462.2397s\n",
      "\titers: 900, epoch: 12 | loss: 0.0197190\n",
      "\tspeed: 0.1449s/iter; left time: 5696.6512s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0094789\n",
      "\tspeed: 0.1448s/iter; left time: 5677.7864s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0175112\n",
      "\tspeed: 0.1419s/iter; left time: 5549.5587s\n",
      "Epoch: 12 cost time: 00h:02m:42.25s\n",
      "Epoch: 12 | Train Loss: 0.0140272 Vali Loss: 0.0196622 Test Loss: 0.0274446\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0165435\n",
      "\tspeed: 0.4616s/iter; left time: 16453.3358s\n",
      "\titers: 200, epoch: 13 | loss: 0.0082239\n",
      "\tspeed: 0.1422s/iter; left time: 5055.3934s\n",
      "\titers: 300, epoch: 13 | loss: 0.0136536\n",
      "\tspeed: 0.1434s/iter; left time: 5082.6446s\n",
      "\titers: 400, epoch: 13 | loss: 0.0121044\n",
      "\tspeed: 0.1565s/iter; left time: 5533.1552s\n",
      "\titers: 500, epoch: 13 | loss: 0.0146031\n",
      "\tspeed: 0.1759s/iter; left time: 6198.7555s\n",
      "\titers: 600, epoch: 13 | loss: 0.0114206\n",
      "\tspeed: 0.1473s/iter; left time: 5175.3620s\n",
      "\titers: 700, epoch: 13 | loss: 0.0122001\n",
      "\tspeed: 0.1502s/iter; left time: 5264.4591s\n",
      "\titers: 800, epoch: 13 | loss: 0.0092386\n",
      "\tspeed: 0.1477s/iter; left time: 5163.0182s\n",
      "\titers: 900, epoch: 13 | loss: 0.0119760\n",
      "\tspeed: 0.1398s/iter; left time: 4870.8015s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0124882\n",
      "\tspeed: 0.1474s/iter; left time: 5122.1298s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0165476\n",
      "\tspeed: 0.1435s/iter; left time: 4971.0180s\n",
      "Epoch: 13 cost time: 00h:02m:47.22s\n",
      "Epoch: 13 | Train Loss: 0.0137605 Vali Loss: 0.0195286 Test Loss: 0.0274269\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0123071\n",
      "\tspeed: 0.4582s/iter; left time: 14286.8493s\n",
      "\titers: 200, epoch: 14 | loss: 0.0137600\n",
      "\tspeed: 0.1445s/iter; left time: 4490.1253s\n",
      "\titers: 300, epoch: 14 | loss: 0.0163909\n",
      "\tspeed: 0.1445s/iter; left time: 4477.7194s\n",
      "\titers: 400, epoch: 14 | loss: 0.0137735\n",
      "\tspeed: 0.1435s/iter; left time: 4431.6945s\n",
      "\titers: 500, epoch: 14 | loss: 0.0141575\n",
      "\tspeed: 0.1471s/iter; left time: 4526.3406s\n",
      "\titers: 600, epoch: 14 | loss: 0.0134898\n",
      "\tspeed: 0.1418s/iter; left time: 4349.2324s\n",
      "\titers: 700, epoch: 14 | loss: 0.0115834\n",
      "\tspeed: 0.1480s/iter; left time: 4526.1229s\n",
      "\titers: 800, epoch: 14 | loss: 0.0097802\n",
      "\tspeed: 0.1404s/iter; left time: 4279.3233s\n",
      "\titers: 900, epoch: 14 | loss: 0.0110015\n",
      "\tspeed: 0.1468s/iter; left time: 4457.9061s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0159150\n",
      "\tspeed: 0.1759s/iter; left time: 5325.3338s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0127477\n",
      "\tspeed: 0.1632s/iter; left time: 4923.7249s\n",
      "Epoch: 14 cost time: 00h:02m:46.93s\n",
      "Epoch: 14 | Train Loss: 0.0135975 Vali Loss: 0.0199316 Test Loss: 0.0280450\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036\n",
      "Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036\n",
      "Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036\n",
      "Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036\n",
      "Intermediate time for GB and pred_len 24: 00h:47m:05.58s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-13 12:19:54,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:19:54,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:19:54,320] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:19:54,328] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:19:55,346] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:19:55,346] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "train 142645\n",
      "train 142645\n",
      "val 30725\n",
      "val 30725\n",
      "test 30725\n",
      "test 30725\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-13 12:19:57,695] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 12:19:58,674] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 12:19:58,675] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 12:19:59,007] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 12:19:59,008] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:19:59,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.27 GB, percent = 2.9%\n",
      "[2024-11-13 12:19:59,175] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 12:19:59,176] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:19:59,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.63 GB, percent = 2.9%\n",
      "[2024-11-13 12:19:59,176] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 12:19:59,317] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 12:19:59,318] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:19:59,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.9 GB, percent = 3.0%\n",
      "[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 12:19:59,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fee46d773d0>\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 12:19:59,321] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0431685\n",
      "\tspeed: 0.1758s/iter; left time: 15654.4433s\n",
      "\titers: 200, epoch: 1 | loss: 0.0412425\n",
      "\tspeed: 0.1474s/iter; left time: 13112.2237s\n",
      "\titers: 300, epoch: 1 | loss: 0.0381312\n",
      "\tspeed: 0.1495s/iter; left time: 13278.8728s\n",
      "\titers: 400, epoch: 1 | loss: 0.0340931\n",
      "\tspeed: 0.1471s/iter; left time: 13055.5807s\n",
      "\titers: 500, epoch: 1 | loss: 0.0292018\n",
      "\tspeed: 0.1484s/iter; left time: 13154.3788s\n",
      "\titers: 600, epoch: 1 | loss: 0.0356489\n",
      "\tspeed: 0.1485s/iter; left time: 13145.4687s\n",
      "\titers: 700, epoch: 1 | loss: 0.0240586\n",
      "\tspeed: 0.1500s/iter; left time: 13265.3475s\n",
      "\titers: 800, epoch: 1 | loss: 0.0366761\n",
      "\tspeed: 0.1513s/iter; left time: 13365.8592s\n",
      "\titers: 900, epoch: 1 | loss: 0.0273430\n",
      "\tspeed: 0.1496s/iter; left time: 13205.1400s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0331169\n",
      "\tspeed: 0.1460s/iter; left time: 12869.0423s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0259525\n",
      "\tspeed: 0.1393s/iter; left time: 12263.0066s\n",
      "Epoch: 1 cost time: 00h:02m:45.73s\n",
      "Epoch: 1 | Train Loss: 0.0362132 Vali Loss: 0.0313898 Test Loss: 0.0448382\n",
      "Validation loss decreased (inf --> 0.031390).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0258306\n",
      "\tspeed: 0.5717s/iter; left time: 48360.6021s\n",
      "\titers: 200, epoch: 2 | loss: 0.0334581\n",
      "\tspeed: 0.1566s/iter; left time: 13230.5393s\n",
      "\titers: 300, epoch: 2 | loss: 0.0252429\n",
      "\tspeed: 0.1482s/iter; left time: 12507.3714s\n",
      "\titers: 400, epoch: 2 | loss: 0.0185966\n",
      "\tspeed: 0.1514s/iter; left time: 12760.8535s\n",
      "\titers: 500, epoch: 2 | loss: 0.0246332\n",
      "\tspeed: 0.1430s/iter; left time: 12040.1216s\n",
      "\titers: 600, epoch: 2 | loss: 0.0231118\n",
      "\tspeed: 0.1479s/iter; left time: 12431.8229s\n",
      "\titers: 700, epoch: 2 | loss: 0.0219791\n",
      "\tspeed: 0.1509s/iter; left time: 12671.5795s\n",
      "\titers: 800, epoch: 2 | loss: 0.0198914\n",
      "\tspeed: 0.1448s/iter; left time: 12148.5088s\n",
      "\titers: 900, epoch: 2 | loss: 0.0247300\n",
      "\tspeed: 0.1471s/iter; left time: 12327.7538s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0231949\n",
      "\tspeed: 0.1481s/iter; left time: 12395.4467s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0246845\n",
      "\tspeed: 0.1459s/iter; left time: 12197.9972s\n",
      "Epoch: 2 cost time: 00h:02m:48.81s\n",
      "Epoch: 2 | Train Loss: 0.0239260 Vali Loss: 0.0285503 Test Loss: 0.0405891\n",
      "Validation loss decreased (0.031390 --> 0.028550).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0197042\n",
      "\tspeed: 0.5781s/iter; left time: 46320.3913s\n",
      "\titers: 200, epoch: 3 | loss: 0.0239601\n",
      "\tspeed: 0.1454s/iter; left time: 11636.4902s\n",
      "\titers: 300, epoch: 3 | loss: 0.0183007\n",
      "\tspeed: 0.1429s/iter; left time: 11419.4287s\n",
      "\titers: 400, epoch: 3 | loss: 0.0246140\n",
      "\tspeed: 0.1638s/iter; left time: 13074.1593s\n",
      "\titers: 500, epoch: 3 | loss: 0.0191527\n",
      "\tspeed: 0.1760s/iter; left time: 14028.3477s\n",
      "\titers: 600, epoch: 3 | loss: 0.0159092\n",
      "\tspeed: 0.1406s/iter; left time: 11197.5995s\n",
      "\titers: 700, epoch: 3 | loss: 0.0165353\n",
      "\tspeed: 0.1554s/iter; left time: 12356.1520s\n",
      "\titers: 800, epoch: 3 | loss: 0.0242688\n",
      "\tspeed: 0.1414s/iter; left time: 11227.4441s\n",
      "\titers: 900, epoch: 3 | loss: 0.0225599\n",
      "\tspeed: 0.1484s/iter; left time: 11769.1555s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0251363\n",
      "\tspeed: 0.1501s/iter; left time: 11894.0352s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0191519\n",
      "\tspeed: 0.1436s/iter; left time: 11363.0548s\n",
      "Epoch: 3 cost time: 00h:02m:47.74s\n",
      "Epoch: 3 | Train Loss: 0.0226770 Vali Loss: 0.0285465 Test Loss: 0.0406802\n",
      "Validation loss decreased (0.028550 --> 0.028547).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0234475\n",
      "\tspeed: 0.5714s/iter; left time: 43234.9674s\n",
      "\titers: 200, epoch: 4 | loss: 0.0300897\n",
      "\tspeed: 0.1446s/iter; left time: 10923.7429s\n",
      "\titers: 300, epoch: 4 | loss: 0.0228381\n",
      "\tspeed: 0.1427s/iter; left time: 10767.7970s\n",
      "\titers: 400, epoch: 4 | loss: 0.0283763\n",
      "\tspeed: 0.1485s/iter; left time: 11189.0675s\n",
      "\titers: 500, epoch: 4 | loss: 0.0187848\n",
      "\tspeed: 0.1460s/iter; left time: 10989.6127s\n",
      "\titers: 600, epoch: 4 | loss: 0.0241174\n",
      "\tspeed: 0.1433s/iter; left time: 10775.4521s\n",
      "\titers: 700, epoch: 4 | loss: 0.0249683\n",
      "\tspeed: 0.1419s/iter; left time: 10653.7677s\n",
      "\titers: 800, epoch: 4 | loss: 0.0259485\n",
      "\tspeed: 0.1730s/iter; left time: 12971.5867s\n",
      "\titers: 900, epoch: 4 | loss: 0.0209155\n",
      "\tspeed: 0.1697s/iter; left time: 12701.7659s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0263899\n",
      "\tspeed: 0.1388s/iter; left time: 10379.1844s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0173570\n",
      "\tspeed: 0.1495s/iter; left time: 11164.1992s\n",
      "Epoch: 4 cost time: 00h:02m:47.23s\n",
      "Epoch: 4 | Train Loss: 0.0224530 Vali Loss: 0.0287918 Test Loss: 0.0410931\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0178938\n",
      "\tspeed: 0.4623s/iter; left time: 32919.4405s\n",
      "\titers: 200, epoch: 5 | loss: 0.0188043\n",
      "\tspeed: 0.1400s/iter; left time: 9956.5457s\n",
      "\titers: 300, epoch: 5 | loss: 0.0216615\n",
      "\tspeed: 0.1446s/iter; left time: 10268.4174s\n",
      "\titers: 400, epoch: 5 | loss: 0.0233857\n",
      "\tspeed: 0.1430s/iter; left time: 10141.0748s\n",
      "\titers: 500, epoch: 5 | loss: 0.0290505\n",
      "\tspeed: 0.1413s/iter; left time: 10005.8098s\n",
      "\titers: 600, epoch: 5 | loss: 0.0166351\n",
      "\tspeed: 0.1410s/iter; left time: 9967.6144s\n",
      "\titers: 700, epoch: 5 | loss: 0.0162357\n",
      "\tspeed: 0.1436s/iter; left time: 10137.5615s\n",
      "\titers: 800, epoch: 5 | loss: 0.0181134\n",
      "\tspeed: 0.1482s/iter; left time: 10448.4301s\n",
      "\titers: 900, epoch: 5 | loss: 0.0203686\n",
      "\tspeed: 0.1451s/iter; left time: 10219.6400s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0181668\n",
      "\tspeed: 0.1450s/iter; left time: 10197.7090s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0223588\n",
      "\tspeed: 0.1440s/iter; left time: 10112.0823s\n",
      "Epoch: 5 cost time: 00h:02m:40.72s\n",
      "Epoch: 5 | Train Loss: 0.0219209 Vali Loss: 0.0294841 Test Loss: 0.0420929\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0236786\n",
      "\tspeed: 0.4999s/iter; left time: 33373.5461s\n",
      "\titers: 200, epoch: 6 | loss: 0.0201914\n",
      "\tspeed: 0.1455s/iter; left time: 9697.6561s\n",
      "\titers: 300, epoch: 6 | loss: 0.0241761\n",
      "\tspeed: 0.1462s/iter; left time: 9730.7753s\n",
      "\titers: 400, epoch: 6 | loss: 0.0242343\n",
      "\tspeed: 0.1419s/iter; left time: 9429.5514s\n",
      "\titers: 500, epoch: 6 | loss: 0.0198615\n",
      "\tspeed: 0.1505s/iter; left time: 9985.8626s\n",
      "\titers: 600, epoch: 6 | loss: 0.0203659\n",
      "\tspeed: 0.1428s/iter; left time: 9462.1292s\n",
      "\titers: 700, epoch: 6 | loss: 0.0144106\n",
      "\tspeed: 0.1396s/iter; left time: 9236.1745s\n",
      "\titers: 800, epoch: 6 | loss: 0.0290639\n",
      "\tspeed: 0.1423s/iter; left time: 9398.0438s\n",
      "\titers: 900, epoch: 6 | loss: 0.0208715\n",
      "\tspeed: 0.1430s/iter; left time: 9432.2366s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0160980\n",
      "\tspeed: 0.1420s/iter; left time: 9351.1661s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0191238\n",
      "\tspeed: 0.1429s/iter; left time: 9397.4567s\n",
      "Epoch: 6 cost time: 00h:02m:41.67s\n",
      "Epoch: 6 | Train Loss: 0.0211830 Vali Loss: 0.0289895 Test Loss: 0.0423584\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0238875\n",
      "\tspeed: 0.4596s/iter; left time: 28635.5410s\n",
      "\titers: 200, epoch: 7 | loss: 0.0230123\n",
      "\tspeed: 0.1411s/iter; left time: 8775.3995s\n",
      "\titers: 300, epoch: 7 | loss: 0.0194589\n",
      "\tspeed: 0.1392s/iter; left time: 8644.8621s\n",
      "\titers: 400, epoch: 7 | loss: 0.0207183\n",
      "\tspeed: 0.1760s/iter; left time: 10910.7722s\n",
      "\titers: 500, epoch: 7 | loss: 0.0176361\n",
      "\tspeed: 0.1695s/iter; left time: 10490.1977s\n",
      "\titers: 600, epoch: 7 | loss: 0.0229180\n",
      "\tspeed: 0.1340s/iter; left time: 8279.7777s\n",
      "\titers: 700, epoch: 7 | loss: 0.0217962\n",
      "\tspeed: 0.1533s/iter; left time: 9456.2479s\n",
      "\titers: 800, epoch: 7 | loss: 0.0188232\n",
      "\tspeed: 0.1396s/iter; left time: 8596.6262s\n",
      "\titers: 900, epoch: 7 | loss: 0.0143067\n",
      "\tspeed: 0.1454s/iter; left time: 8940.9797s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0162435\n",
      "\tspeed: 0.1444s/iter; left time: 8863.3899s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0184402\n",
      "\tspeed: 0.1458s/iter; left time: 8937.3963s\n",
      "Epoch: 7 cost time: 00h:02m:45.57s\n",
      "Epoch: 7 | Train Loss: 0.0204943 Vali Loss: 0.0291370 Test Loss: 0.0436702\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0156560\n",
      "\tspeed: 0.4491s/iter; left time: 25975.3082s\n",
      "\titers: 200, epoch: 8 | loss: 0.0162347\n",
      "\tspeed: 0.1441s/iter; left time: 8321.3504s\n",
      "\titers: 300, epoch: 8 | loss: 0.0205112\n",
      "\tspeed: 0.1436s/iter; left time: 8278.8374s\n",
      "\titers: 400, epoch: 8 | loss: 0.0194970\n",
      "\tspeed: 0.1491s/iter; left time: 8581.8332s\n",
      "\titers: 500, epoch: 8 | loss: 0.0208870\n",
      "\tspeed: 0.1405s/iter; left time: 8072.3808s\n",
      "\titers: 600, epoch: 8 | loss: 0.0229489\n",
      "\tspeed: 0.1467s/iter; left time: 8412.1015s\n",
      "\titers: 700, epoch: 8 | loss: 0.0246555\n",
      "\tspeed: 0.1415s/iter; left time: 8101.8741s\n",
      "\titers: 800, epoch: 8 | loss: 0.0203001\n",
      "\tspeed: 0.1408s/iter; left time: 8044.6193s\n",
      "\titers: 900, epoch: 8 | loss: 0.0155339\n",
      "\tspeed: 0.1752s/iter; left time: 9991.2409s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0228974\n",
      "\tspeed: 0.1624s/iter; left time: 9246.3945s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0213769\n",
      "\tspeed: 0.1399s/iter; left time: 7950.4401s\n",
      "Epoch: 8 cost time: 00h:02m:45.05s\n",
      "Epoch: 8 | Train Loss: 0.0197084 Vali Loss: 0.0296450 Test Loss: 0.0439350\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515\n",
      "Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515\n",
      "Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515\n",
      "Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515\n",
      "Intermediate time for GB and pred_len 96: 00h:27m:06.62s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-13 12:47:00,827] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:47:00,827] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:47:00,828] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:47:00,829] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 12:47:01,852] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "train 142285\n",
      "train 142285\n",
      "train 142285\n",
      "val 30365\n",
      "val 30365\n",
      "val 30365\n",
      "test 30365\n",
      "test 30365\n",
      "test 30365\n",
      "[2024-11-13 12:47:03,961] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 12:47:04,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 12:47:04,463] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 12:47:04,463] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 12:47:04,463] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 12:47:04,464] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 12:47:04,464] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 12:47:04,866] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 12:47:04,867] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:47:04,867] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.11 GB, percent = 3.0%\n",
      "[2024-11-13 12:47:05,320] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 12:47:05,320] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:47:05,320] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.81 GB, percent = 3.1%\n",
      "[2024-11-13 12:47:05,321] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 12:47:05,531] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 12:47:05,531] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 12:47:05,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.32 GB, percent = 3.1%\n",
      "[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 12:47:05,532] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f60468b89d0>\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 12:47:05,534] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0378977\n",
      "\tspeed: 0.1759s/iter; left time: 15622.6922s\n",
      "\titers: 200, epoch: 1 | loss: 0.0419873\n",
      "\tspeed: 0.1489s/iter; left time: 13210.5809s\n",
      "\titers: 300, epoch: 1 | loss: 0.0356091\n",
      "\tspeed: 0.1491s/iter; left time: 13215.5034s\n",
      "\titers: 400, epoch: 1 | loss: 0.0364937\n",
      "\tspeed: 0.1476s/iter; left time: 13063.3147s\n",
      "\titers: 500, epoch: 1 | loss: 0.0337698\n",
      "\tspeed: 0.1496s/iter; left time: 13231.5394s\n",
      "\titers: 600, epoch: 1 | loss: 0.0345404\n",
      "\tspeed: 0.1550s/iter; left time: 13692.6434s\n",
      "\titers: 700, epoch: 1 | loss: 0.0367745\n",
      "\tspeed: 0.1469s/iter; left time: 12960.7442s\n",
      "\titers: 800, epoch: 1 | loss: 0.0408153\n",
      "\tspeed: 0.1521s/iter; left time: 13403.7619s\n",
      "\titers: 900, epoch: 1 | loss: 0.0269480\n",
      "\tspeed: 0.1458s/iter; left time: 12830.5565s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0305300\n",
      "\tspeed: 0.1492s/iter; left time: 13113.5783s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0259015\n",
      "\tspeed: 0.1835s/iter; left time: 16113.5196s\n",
      "Epoch: 1 cost time: 00h:02m:50.95s\n",
      "Epoch: 1 | Train Loss: 0.0362847 Vali Loss: 0.0319874 Test Loss: 0.0458686\n",
      "Validation loss decreased (inf --> 0.031987).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0231011\n",
      "\tspeed: 0.6749s/iter; left time: 56943.6581s\n",
      "\titers: 200, epoch: 2 | loss: 0.0233680\n",
      "\tspeed: 0.1475s/iter; left time: 12431.8754s\n",
      "\titers: 300, epoch: 2 | loss: 0.0231609\n",
      "\tspeed: 0.1489s/iter; left time: 12532.0878s\n",
      "\titers: 400, epoch: 2 | loss: 0.0254489\n",
      "\tspeed: 0.1460s/iter; left time: 12279.0582s\n",
      "\titers: 500, epoch: 2 | loss: 0.0178552\n",
      "\tspeed: 0.1473s/iter; left time: 12365.7776s\n",
      "\titers: 600, epoch: 2 | loss: 0.0188664\n",
      "\tspeed: 0.1472s/iter; left time: 12344.5553s\n",
      "\titers: 700, epoch: 2 | loss: 0.0252296\n",
      "\tspeed: 0.1491s/iter; left time: 12493.4521s\n",
      "\titers: 800, epoch: 2 | loss: 0.0281855\n",
      "\tspeed: 0.1493s/iter; left time: 12495.0919s\n",
      "\titers: 900, epoch: 2 | loss: 0.0223051\n",
      "\tspeed: 0.1481s/iter; left time: 12379.3665s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0262830\n",
      "\tspeed: 0.1503s/iter; left time: 12549.6521s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0261961\n",
      "\tspeed: 0.1507s/iter; left time: 12567.3697s\n",
      "Epoch: 2 cost time: 00h:02m:46.41s\n",
      "Epoch: 2 | Train Loss: 0.0253058 Vali Loss: 0.0303148 Test Loss: 0.0436852\n",
      "Validation loss decreased (0.031987 --> 0.030315).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0261015\n",
      "\tspeed: 0.6465s/iter; left time: 51674.9232s\n",
      "\titers: 200, epoch: 3 | loss: 0.0404731\n",
      "\tspeed: 0.1518s/iter; left time: 12115.0665s\n",
      "\titers: 300, epoch: 3 | loss: 0.0243075\n",
      "\tspeed: 0.1485s/iter; left time: 11837.8628s\n",
      "\titers: 400, epoch: 3 | loss: 0.0239190\n",
      "\tspeed: 0.1505s/iter; left time: 11986.0432s\n",
      "\titers: 500, epoch: 3 | loss: 0.0216389\n",
      "\tspeed: 0.1492s/iter; left time: 11862.7518s\n",
      "\titers: 600, epoch: 3 | loss: 0.0287541\n",
      "\tspeed: 0.1470s/iter; left time: 11674.5047s\n",
      "\titers: 700, epoch: 3 | loss: 0.0206204\n",
      "\tspeed: 0.1460s/iter; left time: 11584.7796s\n",
      "\titers: 800, epoch: 3 | loss: 0.0211433\n",
      "\tspeed: 0.1478s/iter; left time: 11709.8314s\n",
      "\titers: 900, epoch: 3 | loss: 0.0259170\n",
      "\tspeed: 0.1490s/iter; left time: 11787.0177s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0205409\n",
      "\tspeed: 0.1468s/iter; left time: 11604.5434s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0319823\n",
      "\tspeed: 0.1464s/iter; left time: 11557.4452s\n",
      "Epoch: 3 cost time: 00h:02m:45.28s\n",
      "Epoch: 3 | Train Loss: 0.0244032 Vali Loss: 0.0301797 Test Loss: 0.0434866\n",
      "Validation loss decreased (0.030315 --> 0.030180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0256963\n",
      "\tspeed: 0.5675s/iter; left time: 42833.9604s\n",
      "\titers: 200, epoch: 4 | loss: 0.0290772\n",
      "\tspeed: 0.1536s/iter; left time: 11581.1860s\n",
      "\titers: 300, epoch: 4 | loss: 0.0239815\n",
      "\tspeed: 0.1784s/iter; left time: 13432.1521s\n",
      "\titers: 400, epoch: 4 | loss: 0.0218931\n",
      "\tspeed: 0.1553s/iter; left time: 11676.0520s\n",
      "\titers: 500, epoch: 4 | loss: 0.0247705\n",
      "\tspeed: 0.1521s/iter; left time: 11416.5307s\n",
      "\titers: 600, epoch: 4 | loss: 0.0266350\n",
      "\tspeed: 0.1482s/iter; left time: 11116.1300s\n",
      "\titers: 700, epoch: 4 | loss: 0.0252275\n",
      "\tspeed: 0.1489s/iter; left time: 11153.3651s\n",
      "\titers: 800, epoch: 4 | loss: 0.0297814\n",
      "\tspeed: 0.1506s/iter; left time: 11259.0229s\n",
      "\titers: 900, epoch: 4 | loss: 0.0179410\n",
      "\tspeed: 0.1444s/iter; left time: 10785.9688s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0197732\n",
      "\tspeed: 0.1458s/iter; left time: 10874.2652s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0223960\n",
      "\tspeed: 0.1485s/iter; left time: 11061.1264s\n",
      "Epoch: 4 cost time: 00h:02m:49.57s\n",
      "Epoch: 4 | Train Loss: 0.0238908 Vali Loss: 0.0309684 Test Loss: 0.0434581\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0283829\n",
      "\tspeed: 0.4510s/iter; left time: 32034.2971s\n",
      "\titers: 200, epoch: 5 | loss: 0.0219900\n",
      "\tspeed: 0.1470s/iter; left time: 10424.5901s\n",
      "\titers: 300, epoch: 5 | loss: 0.0191130\n",
      "\tspeed: 0.1471s/iter; left time: 10423.3004s\n",
      "\titers: 400, epoch: 5 | loss: 0.0148625\n",
      "\tspeed: 0.1521s/iter; left time: 10756.7540s\n",
      "\titers: 500, epoch: 5 | loss: 0.0246373\n",
      "\tspeed: 0.1452s/iter; left time: 10254.4069s\n",
      "\titers: 600, epoch: 5 | loss: 0.0179953\n",
      "\tspeed: 0.1469s/iter; left time: 10361.4160s\n",
      "\titers: 700, epoch: 5 | loss: 0.0196560\n",
      "\tspeed: 0.1573s/iter; left time: 11078.0661s\n",
      "\titers: 800, epoch: 5 | loss: 0.0215981\n",
      "\tspeed: 0.1793s/iter; left time: 12609.0967s\n",
      "\titers: 900, epoch: 5 | loss: 0.0253618\n",
      "\tspeed: 0.1521s/iter; left time: 10684.7874s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0208310\n",
      "\tspeed: 0.1538s/iter; left time: 10790.4714s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0218497\n",
      "\tspeed: 0.1474s/iter; left time: 10326.7091s\n",
      "Epoch: 5 cost time: 00h:02m:49.61s\n",
      "Epoch: 5 | Train Loss: 0.0230881 Vali Loss: 0.0315672 Test Loss: 0.0440530\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0195508\n",
      "\tspeed: 0.4564s/iter; left time: 30393.5712s\n",
      "\titers: 200, epoch: 6 | loss: 0.0208991\n",
      "\tspeed: 0.1454s/iter; left time: 9665.8534s\n",
      "\titers: 300, epoch: 6 | loss: 0.0208607\n",
      "\tspeed: 0.1391s/iter; left time: 9233.0083s\n",
      "\titers: 400, epoch: 6 | loss: 0.0259936\n",
      "\tspeed: 0.1463s/iter; left time: 9701.5206s\n",
      "\titers: 500, epoch: 6 | loss: 0.0239710\n",
      "\tspeed: 0.1449s/iter; left time: 9594.2853s\n",
      "\titers: 600, epoch: 6 | loss: 0.0177536\n",
      "\tspeed: 0.1448s/iter; left time: 9572.2872s\n",
      "\titers: 700, epoch: 6 | loss: 0.0203063\n",
      "\tspeed: 0.1469s/iter; left time: 9696.4900s\n",
      "\titers: 800, epoch: 6 | loss: 0.0190913\n",
      "\tspeed: 0.1462s/iter; left time: 9634.5712s\n",
      "\titers: 900, epoch: 6 | loss: 0.0180605\n",
      "\tspeed: 0.1487s/iter; left time: 9782.7205s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0214202\n",
      "\tspeed: 0.1496s/iter; left time: 9827.1739s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0192209\n",
      "\tspeed: 0.1452s/iter; left time: 9522.7678s\n",
      "Epoch: 6 cost time: 00h:02m:42.59s\n",
      "Epoch: 6 | Train Loss: 0.0222028 Vali Loss: 0.0321920 Test Loss: 0.0460837\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0182597\n",
      "\tspeed: 0.4976s/iter; left time: 30926.3723s\n",
      "\titers: 200, epoch: 7 | loss: 0.0198322\n",
      "\tspeed: 0.1501s/iter; left time: 9315.1383s\n",
      "\titers: 300, epoch: 7 | loss: 0.0240528\n",
      "\tspeed: 0.1445s/iter; left time: 8953.1140s\n",
      "\titers: 400, epoch: 7 | loss: 0.0182669\n",
      "\tspeed: 0.1529s/iter; left time: 9453.0261s\n",
      "\titers: 500, epoch: 7 | loss: 0.0143159\n",
      "\tspeed: 0.1428s/iter; left time: 8814.4324s\n",
      "\titers: 600, epoch: 7 | loss: 0.0193958\n",
      "\tspeed: 0.1451s/iter; left time: 8943.3694s\n",
      "\titers: 700, epoch: 7 | loss: 0.0183734\n",
      "\tspeed: 0.1472s/iter; left time: 9057.0261s\n",
      "\titers: 800, epoch: 7 | loss: 0.0228030\n",
      "\tspeed: 0.1458s/iter; left time: 8955.9752s\n",
      "\titers: 900, epoch: 7 | loss: 0.0241459\n",
      "\tspeed: 0.1442s/iter; left time: 8844.6763s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0196830\n",
      "\tspeed: 0.1438s/iter; left time: 8807.0666s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0201751\n",
      "\tspeed: 0.1454s/iter; left time: 8888.6415s\n",
      "Epoch: 7 cost time: 00h:02m:43.27s\n",
      "Epoch: 7 | Train Loss: 0.0211709 Vali Loss: 0.0328852 Test Loss: 0.0476506\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0257308\n",
      "\tspeed: 0.4564s/iter; left time: 26335.0466s\n",
      "\titers: 200, epoch: 8 | loss: 0.0251673\n",
      "\tspeed: 0.1442s/iter; left time: 8308.5205s\n",
      "\titers: 300, epoch: 8 | loss: 0.0193176\n",
      "\tspeed: 0.1653s/iter; left time: 9502.6545s\n",
      "\titers: 400, epoch: 8 | loss: 0.0221599\n",
      "\tspeed: 0.1773s/iter; left time: 10176.6146s\n",
      "\titers: 500, epoch: 8 | loss: 0.0184828\n",
      "\tspeed: 0.1404s/iter; left time: 8044.8569s\n",
      "\titers: 600, epoch: 8 | loss: 0.0199777\n",
      "\tspeed: 0.1533s/iter; left time: 8770.9536s\n",
      "\titers: 700, epoch: 8 | loss: 0.0262421\n",
      "\tspeed: 0.1444s/iter; left time: 8247.0761s\n",
      "\titers: 800, epoch: 8 | loss: 0.0216570\n",
      "\tspeed: 0.1474s/iter; left time: 8401.0695s\n",
      "\titers: 900, epoch: 8 | loss: 0.0189521\n",
      "\tspeed: 0.1461s/iter; left time: 8313.5330s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0170493\n",
      "\tspeed: 0.1487s/iter; left time: 8444.5690s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0247314\n",
      "\tspeed: 0.1461s/iter; left time: 8284.4914s\n",
      "Epoch: 8 cost time: 00h:02m:47.85s\n",
      "Epoch: 8 | Train Loss: 0.0200113 Vali Loss: 0.0340090 Test Loss: 0.0489460\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512\n",
      "Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512\n",
      "Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512\n",
      "Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512\n",
      "Intermediate time for GB and pred_len 168: 00h:27m:36.47s\n",
      "\n",
      "Intermediate time for GB: 01h:41m:48.68s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-13 13:14:37,168] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:14:37,200] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:14:37,201] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:14:37,204] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:14:38,221] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "train 86331\n",
      "train 86331\n",
      "train 86331\n",
      "val 18651\n",
      "val 18651\n",
      "val 18651\n",
      "test 18651\n",
      "test 18651\n",
      "test 18651\n",
      "[2024-11-13 13:14:40,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 13:14:41,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 13:14:41,002] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 13:14:41,002] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 13:14:41,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 13:14:41,003] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 13:14:41,003] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 13:14:41,373] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 13:14:41,374] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:14:41,374] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.6 GB, percent = 3.0%\n",
      "[2024-11-13 13:14:41,518] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 13:14:41,519] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:14:41,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.9 GB, percent = 3.1%\n",
      "[2024-11-13 13:14:41,519] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 13:14:41,642] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 13:14:41,643] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:14:41,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.16 GB, percent = 3.1%\n",
      "[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 13:14:41,644] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f05605bf990>\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 13:14:41,646] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0585907\n",
      "\tspeed: 0.1656s/iter; left time: 8918.4506s\n",
      "\titers: 200, epoch: 1 | loss: 0.0585956\n",
      "\tspeed: 0.1400s/iter; left time: 7521.9832s\n",
      "\titers: 300, epoch: 1 | loss: 0.0362778\n",
      "\tspeed: 0.1364s/iter; left time: 7317.1248s\n",
      "\titers: 400, epoch: 1 | loss: 0.0259126\n",
      "\tspeed: 0.1335s/iter; left time: 7146.9061s\n",
      "\titers: 500, epoch: 1 | loss: 0.0213988\n",
      "\tspeed: 0.1639s/iter; left time: 8758.2332s\n",
      "\titers: 600, epoch: 1 | loss: 0.0159848\n",
      "\tspeed: 0.1621s/iter; left time: 8648.6353s\n",
      "Epoch: 1 cost time: 00h:01m:38.55s\n",
      "Epoch: 1 | Train Loss: 0.0414548 Vali Loss: 0.0136225 Test Loss: 0.0177220\n",
      "Validation loss decreased (inf --> 0.013622).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0234336\n",
      "\tspeed: 0.5833s/iter; left time: 29830.8631s\n",
      "\titers: 200, epoch: 2 | loss: 0.0188118\n",
      "\tspeed: 0.1344s/iter; left time: 6857.9279s\n",
      "\titers: 300, epoch: 2 | loss: 0.0143802\n",
      "\tspeed: 0.1353s/iter; left time: 6893.1007s\n",
      "\titers: 400, epoch: 2 | loss: 0.0143195\n",
      "\tspeed: 0.1390s/iter; left time: 7066.3500s\n",
      "\titers: 500, epoch: 2 | loss: 0.0167430\n",
      "\tspeed: 0.1370s/iter; left time: 6951.9470s\n",
      "\titers: 600, epoch: 2 | loss: 0.0185735\n",
      "\tspeed: 0.1372s/iter; left time: 6947.7519s\n",
      "Epoch: 2 cost time: 00h:01m:32.80s\n",
      "Epoch: 2 | Train Loss: 0.0162483 Vali Loss: 0.0110432 Test Loss: 0.0145780\n",
      "Validation loss decreased (0.013622 --> 0.011043).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0135493\n",
      "\tspeed: 0.4978s/iter; left time: 24117.7371s\n",
      "\titers: 200, epoch: 3 | loss: 0.0160991\n",
      "\tspeed: 0.1346s/iter; left time: 6505.6790s\n",
      "\titers: 300, epoch: 3 | loss: 0.0124542\n",
      "\tspeed: 0.1422s/iter; left time: 6862.6207s\n",
      "\titers: 400, epoch: 3 | loss: 0.0136194\n",
      "\tspeed: 0.1538s/iter; left time: 7403.0051s\n",
      "\titers: 500, epoch: 3 | loss: 0.0154468\n",
      "\tspeed: 0.1635s/iter; left time: 7856.1242s\n",
      "\titers: 600, epoch: 3 | loss: 0.0114750\n",
      "\tspeed: 0.1435s/iter; left time: 6878.8406s\n",
      "Epoch: 3 cost time: 00h:01m:38.26s\n",
      "Epoch: 3 | Train Loss: 0.0148898 Vali Loss: 0.0102945 Test Loss: 0.0132678\n",
      "Validation loss decreased (0.011043 --> 0.010294).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0159320\n",
      "\tspeed: 0.5488s/iter; left time: 25106.9163s\n",
      "\titers: 200, epoch: 4 | loss: 0.0105587\n",
      "\tspeed: 0.1502s/iter; left time: 6854.8949s\n",
      "\titers: 300, epoch: 4 | loss: 0.0124891\n",
      "\tspeed: 0.1534s/iter; left time: 6985.9092s\n",
      "\titers: 400, epoch: 4 | loss: 0.0100903\n",
      "\tspeed: 0.1475s/iter; left time: 6701.9853s\n",
      "\titers: 500, epoch: 4 | loss: 0.0174520\n",
      "\tspeed: 0.1625s/iter; left time: 7369.0156s\n",
      "\titers: 600, epoch: 4 | loss: 0.0138244\n",
      "\tspeed: 0.1497s/iter; left time: 6775.8155s\n",
      "Epoch: 4 cost time: 00h:01m:40.94s\n",
      "Epoch: 4 | Train Loss: 0.0141273 Vali Loss: 0.0100167 Test Loss: 0.0129411\n",
      "Validation loss decreased (0.010294 --> 0.010017).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0148488\n",
      "\tspeed: 0.4785s/iter; left time: 20601.5128s\n",
      "\titers: 200, epoch: 5 | loss: 0.0141623\n",
      "\tspeed: 0.1244s/iter; left time: 5342.6730s\n",
      "\titers: 300, epoch: 5 | loss: 0.0118647\n",
      "\tspeed: 0.1236s/iter; left time: 5296.3334s\n",
      "\titers: 400, epoch: 5 | loss: 0.0145806\n",
      "\tspeed: 0.1236s/iter; left time: 5284.5546s\n",
      "\titers: 500, epoch: 5 | loss: 0.0123270\n",
      "\tspeed: 0.1258s/iter; left time: 5366.4768s\n",
      "\titers: 600, epoch: 5 | loss: 0.0115003\n",
      "\tspeed: 0.1266s/iter; left time: 5387.6036s\n",
      "Epoch: 5 cost time: 00h:01m:25.14s\n",
      "Epoch: 5 | Train Loss: 0.0135997 Vali Loss: 0.0102100 Test Loss: 0.0134052\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0105686\n",
      "\tspeed: 0.3814s/iter; left time: 15390.1491s\n",
      "\titers: 200, epoch: 6 | loss: 0.0137064\n",
      "\tspeed: 0.1242s/iter; left time: 4999.5486s\n",
      "\titers: 300, epoch: 6 | loss: 0.0133618\n",
      "\tspeed: 0.1238s/iter; left time: 4972.7704s\n",
      "\titers: 400, epoch: 6 | loss: 0.0140145\n",
      "\tspeed: 0.1240s/iter; left time: 4965.0977s\n",
      "\titers: 500, epoch: 6 | loss: 0.0130147\n",
      "\tspeed: 0.1229s/iter; left time: 4912.5422s\n",
      "\titers: 600, epoch: 6 | loss: 0.0116438\n",
      "\tspeed: 0.1231s/iter; left time: 4906.5062s\n",
      "Epoch: 6 cost time: 00h:01m:23.95s\n",
      "Epoch: 6 | Train Loss: 0.0134458 Vali Loss: 0.0094027 Test Loss: 0.0120869\n",
      "Validation loss decreased (0.010017 --> 0.009403).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0137705\n",
      "\tspeed: 0.4651s/iter; left time: 17513.8505s\n",
      "\titers: 200, epoch: 7 | loss: 0.0118949\n",
      "\tspeed: 0.1246s/iter; left time: 4677.9814s\n",
      "\titers: 300, epoch: 7 | loss: 0.0193670\n",
      "\tspeed: 0.1248s/iter; left time: 4673.2386s\n",
      "\titers: 400, epoch: 7 | loss: 0.0122732\n",
      "\tspeed: 0.1270s/iter; left time: 4744.8331s\n",
      "\titers: 500, epoch: 7 | loss: 0.0111094\n",
      "\tspeed: 0.1238s/iter; left time: 4611.5533s\n",
      "\titers: 600, epoch: 7 | loss: 0.0107920\n",
      "\tspeed: 0.1242s/iter; left time: 4615.9904s\n",
      "Epoch: 7 cost time: 00h:01m:24.75s\n",
      "Epoch: 7 | Train Loss: 0.0130861 Vali Loss: 0.0091752 Test Loss: 0.0117255\n",
      "Validation loss decreased (0.009403 --> 0.009175).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0113322\n",
      "\tspeed: 0.4539s/iter; left time: 15870.3355s\n",
      "\titers: 200, epoch: 8 | loss: 0.0130333\n",
      "\tspeed: 0.1229s/iter; left time: 4284.8534s\n",
      "\titers: 300, epoch: 8 | loss: 0.0108141\n",
      "\tspeed: 0.1244s/iter; left time: 4324.9072s\n",
      "\titers: 400, epoch: 8 | loss: 0.0103628\n",
      "\tspeed: 0.1236s/iter; left time: 4283.4558s\n",
      "\titers: 500, epoch: 8 | loss: 0.0118539\n",
      "\tspeed: 0.1225s/iter; left time: 4232.7593s\n",
      "\titers: 600, epoch: 8 | loss: 0.0142968\n",
      "\tspeed: 0.1232s/iter; left time: 4244.6338s\n",
      "Epoch: 8 cost time: 00h:01m:23.68s\n",
      "Epoch: 8 | Train Loss: 0.0127573 Vali Loss: 0.0092726 Test Loss: 0.0122493\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0123131\n",
      "\tspeed: 0.3818s/iter; left time: 12319.2802s\n",
      "\titers: 200, epoch: 9 | loss: 0.0127209\n",
      "\tspeed: 0.1222s/iter; left time: 3931.3831s\n",
      "\titers: 300, epoch: 9 | loss: 0.0072432\n",
      "\tspeed: 0.1232s/iter; left time: 3951.0173s\n",
      "\titers: 400, epoch: 9 | loss: 0.0095887\n",
      "\tspeed: 0.1253s/iter; left time: 4004.0000s\n",
      "\titers: 500, epoch: 9 | loss: 0.0158286\n",
      "\tspeed: 0.1229s/iter; left time: 3914.8901s\n",
      "\titers: 600, epoch: 9 | loss: 0.0081707\n",
      "\tspeed: 0.1216s/iter; left time: 3863.5983s\n",
      "Epoch: 9 cost time: 00h:01m:24.02s\n",
      "Epoch: 9 | Train Loss: 0.0124855 Vali Loss: 0.0090592 Test Loss: 0.0120632\n",
      "Validation loss decreased (0.009175 --> 0.009059).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0132127\n",
      "\tspeed: 0.4664s/iter; left time: 13790.5333s\n",
      "\titers: 200, epoch: 10 | loss: 0.0110980\n",
      "\tspeed: 0.1346s/iter; left time: 3966.3410s\n",
      "\titers: 300, epoch: 10 | loss: 0.0123905\n",
      "\tspeed: 0.1217s/iter; left time: 3573.3920s\n",
      "\titers: 400, epoch: 10 | loss: 0.0147953\n",
      "\tspeed: 0.1245s/iter; left time: 3644.1321s\n",
      "\titers: 500, epoch: 10 | loss: 0.0115478\n",
      "\tspeed: 0.1238s/iter; left time: 3609.6607s\n",
      "\titers: 600, epoch: 10 | loss: 0.0128286\n",
      "\tspeed: 0.1228s/iter; left time: 3570.9932s\n",
      "Epoch: 10 cost time: 00h:01m:25.35s\n",
      "Epoch: 10 | Train Loss: 0.0122391 Vali Loss: 0.0091130 Test Loss: 0.0120960\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0174535\n",
      "\tspeed: 0.3766s/iter; left time: 10120.7733s\n",
      "\titers: 200, epoch: 11 | loss: 0.0126983\n",
      "\tspeed: 0.1236s/iter; left time: 3308.6341s\n",
      "\titers: 300, epoch: 11 | loss: 0.0130372\n",
      "\tspeed: 0.1277s/iter; left time: 3406.5130s\n",
      "\titers: 400, epoch: 11 | loss: 0.0176958\n",
      "\tspeed: 0.1239s/iter; left time: 3291.3225s\n",
      "\titers: 500, epoch: 11 | loss: 0.0106839\n",
      "\tspeed: 0.1219s/iter; left time: 3227.3351s\n",
      "\titers: 600, epoch: 11 | loss: 0.0114549\n",
      "\tspeed: 0.1232s/iter; left time: 3248.1451s\n",
      "Epoch: 11 cost time: 00h:01m:25.34s\n",
      "Epoch: 11 | Train Loss: 0.0121536 Vali Loss: 0.0088604 Test Loss: 0.0117209\n",
      "Validation loss decreased (0.009059 --> 0.008860).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0116588\n",
      "\tspeed: 0.4860s/iter; left time: 11749.6513s\n",
      "\titers: 200, epoch: 12 | loss: 0.0145723\n",
      "\tspeed: 0.1232s/iter; left time: 2966.3678s\n",
      "\titers: 300, epoch: 12 | loss: 0.0116074\n",
      "\tspeed: 0.1251s/iter; left time: 2999.1238s\n",
      "\titers: 400, epoch: 12 | loss: 0.0114902\n",
      "\tspeed: 0.1279s/iter; left time: 3052.3157s\n",
      "\titers: 500, epoch: 12 | loss: 0.0144032\n",
      "\tspeed: 0.1235s/iter; left time: 2937.0960s\n",
      "\titers: 600, epoch: 12 | loss: 0.0147044\n",
      "\tspeed: 0.1237s/iter; left time: 2928.9708s\n",
      "Epoch: 12 cost time: 00h:01m:24.76s\n",
      "Epoch: 12 | Train Loss: 0.0119597 Vali Loss: 0.0086890 Test Loss: 0.0113151\n",
      "Validation loss decreased (0.008860 --> 0.008689).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0143123\n",
      "\tspeed: 0.4718s/iter; left time: 10132.6443s\n",
      "\titers: 200, epoch: 13 | loss: 0.0108766\n",
      "\tspeed: 0.1222s/iter; left time: 2612.6563s\n",
      "\titers: 300, epoch: 13 | loss: 0.0097793\n",
      "\tspeed: 0.1241s/iter; left time: 2639.4163s\n",
      "\titers: 400, epoch: 13 | loss: 0.0138142\n",
      "\tspeed: 0.1240s/iter; left time: 2625.9096s\n",
      "\titers: 500, epoch: 13 | loss: 0.0085804\n",
      "\tspeed: 0.1227s/iter; left time: 2585.1142s\n",
      "\titers: 600, epoch: 13 | loss: 0.0130738\n",
      "\tspeed: 0.1230s/iter; left time: 2579.7988s\n",
      "Epoch: 13 cost time: 00h:01m:23.75s\n",
      "Epoch: 13 | Train Loss: 0.0120229 Vali Loss: 0.0088181 Test Loss: 0.0115324\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0109734\n",
      "\tspeed: 0.3768s/iter; left time: 7076.8174s\n",
      "\titers: 200, epoch: 14 | loss: 0.0097976\n",
      "\tspeed: 0.1241s/iter; left time: 2318.7731s\n",
      "\titers: 300, epoch: 14 | loss: 0.0141105\n",
      "\tspeed: 0.1230s/iter; left time: 2285.3795s\n",
      "\titers: 400, epoch: 14 | loss: 0.0139851\n",
      "\tspeed: 0.1231s/iter; left time: 2274.5216s\n",
      "\titers: 500, epoch: 14 | loss: 0.0111326\n",
      "\tspeed: 0.1383s/iter; left time: 2542.5687s\n",
      "\titers: 600, epoch: 14 | loss: 0.0157568\n",
      "\tspeed: 0.1342s/iter; left time: 2453.0623s\n",
      "Epoch: 14 cost time: 00h:01m:26.16s\n",
      "Epoch: 14 | Train Loss: 0.0117440 Vali Loss: 0.0087840 Test Loss: 0.0116497\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0096404\n",
      "\tspeed: 0.3657s/iter; left time: 5881.2191s\n",
      "\titers: 200, epoch: 15 | loss: 0.0107696\n",
      "\tspeed: 0.1215s/iter; left time: 1942.6471s\n",
      "\titers: 300, epoch: 15 | loss: 0.0091478\n",
      "\tspeed: 0.1226s/iter; left time: 1947.2254s\n",
      "\titers: 400, epoch: 15 | loss: 0.0130447\n",
      "\tspeed: 0.1236s/iter; left time: 1951.4498s\n",
      "\titers: 500, epoch: 15 | loss: 0.0125198\n",
      "\tspeed: 0.1230s/iter; left time: 1929.2286s\n",
      "\titers: 600, epoch: 15 | loss: 0.0120653\n",
      "\tspeed: 0.1224s/iter; left time: 1907.0463s\n",
      "Epoch: 15 cost time: 00h:01m:23.14s\n",
      "Epoch: 15 | Train Loss: 0.0116359 Vali Loss: 0.0085023 Test Loss: 0.0108946\n",
      "Validation loss decreased (0.008689 --> 0.008502).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0109141\n",
      "\tspeed: 0.4585s/iter; left time: 6137.9785s\n",
      "\titers: 200, epoch: 16 | loss: 0.0124269\n",
      "\tspeed: 0.1223s/iter; left time: 1624.6643s\n",
      "\titers: 300, epoch: 16 | loss: 0.0105661\n",
      "\tspeed: 0.1223s/iter; left time: 1612.9687s\n",
      "\titers: 400, epoch: 16 | loss: 0.0130263\n",
      "\tspeed: 0.1249s/iter; left time: 1633.9415s\n",
      "\titers: 500, epoch: 16 | loss: 0.0099179\n",
      "\tspeed: 0.1221s/iter; left time: 1585.0623s\n",
      "\titers: 600, epoch: 16 | loss: 0.0106084\n",
      "\tspeed: 0.1233s/iter; left time: 1588.5283s\n",
      "Epoch: 16 cost time: 00h:01m:23.43s\n",
      "Epoch: 16 | Train Loss: 0.0116072 Vali Loss: 0.0090153 Test Loss: 0.0119307\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0144753\n",
      "\tspeed: 0.3820s/iter; left time: 4083.6862s\n",
      "\titers: 200, epoch: 17 | loss: 0.0077437\n",
      "\tspeed: 0.1230s/iter; left time: 1302.7615s\n",
      "\titers: 300, epoch: 17 | loss: 0.0111782\n",
      "\tspeed: 0.1232s/iter; left time: 1291.9337s\n",
      "\titers: 400, epoch: 17 | loss: 0.0086899\n",
      "\tspeed: 0.1242s/iter; left time: 1290.5495s\n",
      "\titers: 500, epoch: 17 | loss: 0.0117684\n",
      "\tspeed: 0.1221s/iter; left time: 1256.5225s\n",
      "\titers: 600, epoch: 17 | loss: 0.0110266\n",
      "\tspeed: 0.1256s/iter; left time: 1280.1298s\n",
      "Epoch: 17 cost time: 00h:01m:23.91s\n",
      "Epoch: 17 | Train Loss: 0.0115094 Vali Loss: 0.0088644 Test Loss: 0.0113639\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0082685\n",
      "\tspeed: 0.3730s/iter; left time: 2980.6251s\n",
      "\titers: 200, epoch: 18 | loss: 0.0090307\n",
      "\tspeed: 0.1224s/iter; left time: 966.0165s\n",
      "\titers: 300, epoch: 18 | loss: 0.0130933\n",
      "\tspeed: 0.1243s/iter; left time: 968.7339s\n",
      "\titers: 400, epoch: 18 | loss: 0.0109216\n",
      "\tspeed: 0.1236s/iter; left time: 950.4307s\n",
      "\titers: 500, epoch: 18 | loss: 0.0152812\n",
      "\tspeed: 0.1232s/iter; left time: 935.0758s\n",
      "\titers: 600, epoch: 18 | loss: 0.0119109\n",
      "\tspeed: 0.1225s/iter; left time: 917.7594s\n",
      "Epoch: 18 cost time: 00h:01m:23.39s\n",
      "Epoch: 18 | Train Loss: 0.0116320 Vali Loss: 0.0088605 Test Loss: 0.0113738\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0093845\n",
      "\tspeed: 0.3765s/iter; left time: 1993.6031s\n",
      "\titers: 200, epoch: 19 | loss: 0.0103307\n",
      "\tspeed: 0.1378s/iter; left time: 716.1294s\n",
      "\titers: 300, epoch: 19 | loss: 0.0093786\n",
      "\tspeed: 0.1349s/iter; left time: 687.4181s\n",
      "\titers: 400, epoch: 19 | loss: 0.0089233\n",
      "\tspeed: 0.1224s/iter; left time: 611.2726s\n",
      "\titers: 500, epoch: 19 | loss: 0.0120973\n",
      "\tspeed: 0.1218s/iter; left time: 596.2324s\n",
      "\titers: 600, epoch: 19 | loss: 0.0120883\n",
      "\tspeed: 0.1226s/iter; left time: 587.7565s\n",
      "Epoch: 19 cost time: 00h:01m:26.00s\n",
      "Epoch: 19 | Train Loss: 0.0112558 Vali Loss: 0.0087352 Test Loss: 0.0110916\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 20 | loss: 0.0130322\n",
      "\tspeed: 0.3735s/iter; left time: 970.4306s\n",
      "\titers: 200, epoch: 20 | loss: 0.0099038\n",
      "\tspeed: 0.1248s/iter; left time: 311.8140s\n",
      "\titers: 300, epoch: 20 | loss: 0.0115065\n",
      "\tspeed: 0.1231s/iter; left time: 295.2821s\n",
      "\titers: 400, epoch: 20 | loss: 0.0149006\n",
      "\tspeed: 0.1253s/iter; left time: 287.9809s\n",
      "\titers: 500, epoch: 20 | loss: 0.0128831\n",
      "\tspeed: 0.1233s/iter; left time: 271.0752s\n",
      "\titers: 600, epoch: 20 | loss: 0.0122716\n",
      "\tspeed: 0.1240s/iter; left time: 260.0797s\n",
      "Epoch: 20 cost time: 00h:01m:24.43s\n",
      "Epoch: 20 | Train Loss: 0.0112232 Vali Loss: 0.0089175 Test Loss: 0.0112712\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506\n",
      "Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506\n",
      "Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506\n",
      "Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506\n",
      "Intermediate time for ES and pred_len 24: 00h:36m:26.31s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-13 13:51:04,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:51:04,321] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:51:04,322] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:51:04,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:51:05,637] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "train 86115\n",
      "train 86115\n",
      "val 18435\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "val 18435\n",
      "test 18435\n",
      "test 18435\n",
      "[2024-11-13 13:51:07,713] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 13:51:08,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 13:51:08,677] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 13:51:08,677] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 13:51:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 13:51:08,678] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 13:51:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 13:51:08,679] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 13:51:09,018] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 13:51:09,019] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:51:09,019] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.37 GB, percent = 3.7%\n",
      "[2024-11-13 13:51:09,191] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 13:51:09,191] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:51:09,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.75 GB, percent = 3.7%\n",
      "[2024-11-13 13:51:09,192] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 13:51:09,376] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 13:51:09,377] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 13:51:09,377] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 38.18 GB, percent = 3.8%\n",
      "[2024-11-13 13:51:09,377] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 13:51:09,377] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 13:51:09,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 13:51:09,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 13:51:09,378] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd05348e750>\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 13:51:09,381] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0596709\n",
      "\tspeed: 0.1541s/iter; left time: 8280.2504s\n",
      "\titers: 200, epoch: 1 | loss: 0.0532119\n",
      "\tspeed: 0.1262s/iter; left time: 6768.7826s\n",
      "\titers: 300, epoch: 1 | loss: 0.0370430\n",
      "\tspeed: 0.1273s/iter; left time: 6813.1354s\n",
      "\titers: 400, epoch: 1 | loss: 0.0336645\n",
      "\tspeed: 0.1272s/iter; left time: 6794.8625s\n",
      "\titers: 500, epoch: 1 | loss: 0.0248052\n",
      "\tspeed: 0.1275s/iter; left time: 6799.8293s\n",
      "\titers: 600, epoch: 1 | loss: 0.0247847\n",
      "\tspeed: 0.1287s/iter; left time: 6848.0630s\n",
      "Epoch: 1 cost time: 00h:01m:26.73s\n",
      "Epoch: 1 | Train Loss: 0.0483457 Vali Loss: 0.0212816 Test Loss: 0.0269613\n",
      "Validation loss decreased (inf --> 0.021282).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0258570\n",
      "\tspeed: 0.5548s/iter; left time: 28312.8267s\n",
      "\titers: 200, epoch: 2 | loss: 0.0201012\n",
      "\tspeed: 0.1282s/iter; left time: 6529.3688s\n",
      "\titers: 300, epoch: 2 | loss: 0.0219315\n",
      "\tspeed: 0.1265s/iter; left time: 6432.2708s\n",
      "\titers: 400, epoch: 2 | loss: 0.0248280\n",
      "\tspeed: 0.1256s/iter; left time: 6374.2098s\n",
      "\titers: 500, epoch: 2 | loss: 0.0258012\n",
      "\tspeed: 0.1278s/iter; left time: 6468.3951s\n",
      "\titers: 600, epoch: 2 | loss: 0.0192499\n",
      "\tspeed: 0.1472s/iter; left time: 7436.4860s\n",
      "Epoch: 2 cost time: 00h:01m:27.72s\n",
      "Epoch: 2 | Train Loss: 0.0219829 Vali Loss: 0.0185177 Test Loss: 0.0236853\n",
      "Validation loss decreased (0.021282 --> 0.018518).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0183666\n",
      "\tspeed: 0.4213s/iter; left time: 20366.3054s\n",
      "\titers: 200, epoch: 3 | loss: 0.0207339\n",
      "\tspeed: 0.1249s/iter; left time: 6026.3829s\n",
      "\titers: 300, epoch: 3 | loss: 0.0203469\n",
      "\tspeed: 0.1244s/iter; left time: 5987.8589s\n",
      "\titers: 400, epoch: 3 | loss: 0.0147172\n",
      "\tspeed: 0.1248s/iter; left time: 5995.2718s\n",
      "\titers: 500, epoch: 3 | loss: 0.0166696\n",
      "\tspeed: 0.1237s/iter; left time: 5927.8786s\n",
      "\titers: 600, epoch: 3 | loss: 0.0183942\n",
      "\tspeed: 0.1234s/iter; left time: 5904.6205s\n",
      "Epoch: 3 cost time: 00h:01m:24.15s\n",
      "Epoch: 3 | Train Loss: 0.0206732 Vali Loss: 0.0176023 Test Loss: 0.0222772\n",
      "Validation loss decreased (0.018518 --> 0.017602).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0219100\n",
      "\tspeed: 0.4192s/iter; left time: 19137.3536s\n",
      "\titers: 200, epoch: 4 | loss: 0.0203517\n",
      "\tspeed: 0.1238s/iter; left time: 5636.6998s\n",
      "\titers: 300, epoch: 4 | loss: 0.0225772\n",
      "\tspeed: 0.1226s/iter; left time: 5570.7821s\n",
      "\titers: 400, epoch: 4 | loss: 0.0159180\n",
      "\tspeed: 0.1231s/iter; left time: 5582.2138s\n",
      "\titers: 500, epoch: 4 | loss: 0.0181224\n",
      "\tspeed: 0.1256s/iter; left time: 5683.1568s\n",
      "\titers: 600, epoch: 4 | loss: 0.0191104\n",
      "\tspeed: 0.1297s/iter; left time: 5857.7328s\n",
      "Epoch: 4 cost time: 00h:01m:24.77s\n",
      "Epoch: 4 | Train Loss: 0.0200406 Vali Loss: 0.0169479 Test Loss: 0.0212023\n",
      "Validation loss decreased (0.017602 --> 0.016948).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0224293\n",
      "\tspeed: 0.4625s/iter; left time: 19867.5847s\n",
      "\titers: 200, epoch: 5 | loss: 0.0173440\n",
      "\tspeed: 0.1222s/iter; left time: 5238.2678s\n",
      "\titers: 300, epoch: 5 | loss: 0.0235391\n",
      "\tspeed: 0.1227s/iter; left time: 5246.0820s\n",
      "\titers: 400, epoch: 5 | loss: 0.0183559\n",
      "\tspeed: 0.1228s/iter; left time: 5237.8337s\n",
      "\titers: 500, epoch: 5 | loss: 0.0137428\n",
      "\tspeed: 0.1229s/iter; left time: 5230.3099s\n",
      "\titers: 600, epoch: 5 | loss: 0.0179286\n",
      "\tspeed: 0.1219s/iter; left time: 5177.0057s\n",
      "Epoch: 5 cost time: 00h:01m:22.92s\n",
      "Epoch: 5 | Train Loss: 0.0192499 Vali Loss: 0.0166892 Test Loss: 0.0214334\n",
      "Validation loss decreased (0.016948 --> 0.016689).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0143708\n",
      "\tspeed: 0.4215s/iter; left time: 16972.8677s\n",
      "\titers: 200, epoch: 6 | loss: 0.0234077\n",
      "\tspeed: 0.1219s/iter; left time: 4896.6734s\n",
      "\titers: 300, epoch: 6 | loss: 0.0196592\n",
      "\tspeed: 0.1218s/iter; left time: 4880.0618s\n",
      "\titers: 400, epoch: 6 | loss: 0.0198589\n",
      "\tspeed: 0.1225s/iter; left time: 4895.4348s\n",
      "\titers: 500, epoch: 6 | loss: 0.0156133\n",
      "\tspeed: 0.1243s/iter; left time: 4956.4937s\n",
      "\titers: 600, epoch: 6 | loss: 0.0178505\n",
      "\tspeed: 0.1229s/iter; left time: 4886.8690s\n",
      "Epoch: 6 cost time: 00h:01m:22.93s\n",
      "Epoch: 6 | Train Loss: 0.0190729 Vali Loss: 0.0161652 Test Loss: 0.0208077\n",
      "Validation loss decreased (0.016689 --> 0.016165).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0223626\n",
      "\tspeed: 0.4239s/iter; left time: 15927.1970s\n",
      "\titers: 200, epoch: 7 | loss: 0.0202940\n",
      "\tspeed: 0.1220s/iter; left time: 4570.4599s\n",
      "\titers: 300, epoch: 7 | loss: 0.0214871\n",
      "\tspeed: 0.1224s/iter; left time: 4575.3221s\n",
      "\titers: 400, epoch: 7 | loss: 0.0202324\n",
      "\tspeed: 0.1214s/iter; left time: 4523.7880s\n",
      "\titers: 500, epoch: 7 | loss: 0.0148672\n",
      "\tspeed: 0.1216s/iter; left time: 4521.0482s\n",
      "\titers: 600, epoch: 7 | loss: 0.0179687\n",
      "\tspeed: 0.1231s/iter; left time: 4565.2045s\n",
      "Epoch: 7 cost time: 00h:01m:22.81s\n",
      "Epoch: 7 | Train Loss: 0.0186758 Vali Loss: 0.0160311 Test Loss: 0.0205422\n",
      "Validation loss decreased (0.016165 --> 0.016031).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0157311\n",
      "\tspeed: 0.4318s/iter; left time: 15064.4128s\n",
      "\titers: 200, epoch: 8 | loss: 0.0234816\n",
      "\tspeed: 0.1213s/iter; left time: 4219.9651s\n",
      "\titers: 300, epoch: 8 | loss: 0.0252436\n",
      "\tspeed: 0.1216s/iter; left time: 4218.3211s\n",
      "\titers: 400, epoch: 8 | loss: 0.0179009\n",
      "\tspeed: 0.1224s/iter; left time: 4234.7011s\n",
      "\titers: 500, epoch: 8 | loss: 0.0176396\n",
      "\tspeed: 0.1212s/iter; left time: 4179.5099s\n",
      "\titers: 600, epoch: 8 | loss: 0.0130606\n",
      "\tspeed: 0.1211s/iter; left time: 4165.0731s\n",
      "Epoch: 8 cost time: 00h:01m:22.51s\n",
      "Epoch: 8 | Train Loss: 0.0181763 Vali Loss: 0.0157263 Test Loss: 0.0200718\n",
      "Validation loss decreased (0.016031 --> 0.015726).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0179632\n",
      "\tspeed: 0.4888s/iter; left time: 15737.2908s\n",
      "\titers: 200, epoch: 9 | loss: 0.0205819\n",
      "\tspeed: 0.1211s/iter; left time: 3885.2754s\n",
      "\titers: 300, epoch: 9 | loss: 0.0164489\n",
      "\tspeed: 0.1227s/iter; left time: 3924.7723s\n",
      "\titers: 400, epoch: 9 | loss: 0.0201637\n",
      "\tspeed: 0.1210s/iter; left time: 3857.4777s\n",
      "\titers: 500, epoch: 9 | loss: 0.0176181\n",
      "\tspeed: 0.1211s/iter; left time: 3850.4494s\n",
      "\titers: 600, epoch: 9 | loss: 0.0184517\n",
      "\tspeed: 0.1220s/iter; left time: 3867.1648s\n",
      "Epoch: 9 cost time: 00h:01m:22.48s\n",
      "Epoch: 9 | Train Loss: 0.0178964 Vali Loss: 0.0159344 Test Loss: 0.0200433\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0136520\n",
      "\tspeed: 0.3705s/iter; left time: 10930.7898s\n",
      "\titers: 200, epoch: 10 | loss: 0.0210689\n",
      "\tspeed: 0.1205s/iter; left time: 3541.9777s\n",
      "\titers: 300, epoch: 10 | loss: 0.0225531\n",
      "\tspeed: 0.1217s/iter; left time: 3565.3884s\n",
      "\titers: 400, epoch: 10 | loss: 0.0213916\n",
      "\tspeed: 0.1214s/iter; left time: 3545.6285s\n",
      "\titers: 500, epoch: 10 | loss: 0.0140378\n",
      "\tspeed: 0.1213s/iter; left time: 3531.0026s\n",
      "\titers: 600, epoch: 10 | loss: 0.0144618\n",
      "\tspeed: 0.1215s/iter; left time: 3524.6228s\n",
      "Epoch: 10 cost time: 00h:01m:22.03s\n",
      "Epoch: 10 | Train Loss: 0.0174420 Vali Loss: 0.0157547 Test Loss: 0.0199326\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0239945\n",
      "\tspeed: 0.3676s/iter; left time: 9855.5267s\n",
      "\titers: 200, epoch: 11 | loss: 0.0163100\n",
      "\tspeed: 0.1208s/iter; left time: 3227.4399s\n",
      "\titers: 300, epoch: 11 | loss: 0.0167650\n",
      "\tspeed: 0.1219s/iter; left time: 3243.5680s\n",
      "\titers: 400, epoch: 11 | loss: 0.0160354\n",
      "\tspeed: 0.1214s/iter; left time: 3218.6524s\n",
      "\titers: 500, epoch: 11 | loss: 0.0200000\n",
      "\tspeed: 0.1205s/iter; left time: 3182.0253s\n",
      "\titers: 600, epoch: 11 | loss: 0.0175237\n",
      "\tspeed: 0.1203s/iter; left time: 3163.9138s\n",
      "Epoch: 11 cost time: 00h:01m:21.82s\n",
      "Epoch: 11 | Train Loss: 0.0173280 Vali Loss: 0.0163085 Test Loss: 0.0206065\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0190451\n",
      "\tspeed: 0.3650s/iter; left time: 8802.7618s\n",
      "\titers: 200, epoch: 12 | loss: 0.0176930\n",
      "\tspeed: 0.1219s/iter; left time: 2928.7405s\n",
      "\titers: 300, epoch: 12 | loss: 0.0149796\n",
      "\tspeed: 0.1228s/iter; left time: 2937.8192s\n",
      "\titers: 400, epoch: 12 | loss: 0.0171055\n",
      "\tspeed: 0.1229s/iter; left time: 2927.3107s\n",
      "\titers: 500, epoch: 12 | loss: 0.0161820\n",
      "\tspeed: 0.1205s/iter; left time: 2857.3275s\n",
      "\titers: 600, epoch: 12 | loss: 0.0151442\n",
      "\tspeed: 0.1205s/iter; left time: 2847.2264s\n",
      "Epoch: 12 cost time: 00h:01m:22.16s\n",
      "Epoch: 12 | Train Loss: 0.0169954 Vali Loss: 0.0162357 Test Loss: 0.0206839\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0163781\n",
      "\tspeed: 0.3688s/iter; left time: 7903.8693s\n",
      "\titers: 200, epoch: 13 | loss: 0.0216128\n",
      "\tspeed: 0.1207s/iter; left time: 2573.4739s\n",
      "\titers: 300, epoch: 13 | loss: 0.0111176\n",
      "\tspeed: 0.1201s/iter; left time: 2550.2163s\n",
      "\titers: 400, epoch: 13 | loss: 0.0153951\n",
      "\tspeed: 0.1202s/iter; left time: 2539.4739s\n",
      "\titers: 500, epoch: 13 | loss: 0.0192523\n",
      "\tspeed: 0.1212s/iter; left time: 2547.8362s\n",
      "\titers: 600, epoch: 13 | loss: 0.0164807\n",
      "\tspeed: 0.1207s/iter; left time: 2526.6495s\n",
      "Epoch: 13 cost time: 00h:01m:21.59s\n",
      "Epoch: 13 | Train Loss: 0.0166521 Vali Loss: 0.0167950 Test Loss: 0.0205306\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086\n",
      "Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086\n",
      "Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086\n",
      "Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086\n",
      "Intermediate time for ES and pred_len 96: 00h:23m:00.60s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-13 14:14:04,733] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:14:04,736] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:14:04,736] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:14:04,745] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:14:05,810] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:14:05,810] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:14:05,811] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:14:05,811] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:14:05,811] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "train 85899\n",
      "train 85899\n",
      "train 85899\n",
      "val 18219\n",
      "val 18219\n",
      "val 18219\n",
      "test 18219\n",
      "test 18219\n",
      "test 18219\n",
      "[2024-11-13 14:14:08,233] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 14:14:09,266] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 14:14:09,267] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 14:14:09,267] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 14:14:09,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 14:14:09,268] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 14:14:09,268] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 14:14:09,584] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 14:14:09,584] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:14:09,585] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.54 GB, percent = 3.3%\n",
      "[2024-11-13 14:14:09,749] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 14:14:09,750] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:14:09,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.77 GB, percent = 3.4%\n",
      "[2024-11-13 14:14:09,750] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 14:14:09,907] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 14:14:09,907] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:14:09,908] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 34.14 GB, percent = 3.4%\n",
      "[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe038c7e810>\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 14:14:09,910] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0565824\n",
      "\tspeed: 0.1600s/iter; left time: 8572.6883s\n",
      "\titers: 200, epoch: 1 | loss: 0.0569034\n",
      "\tspeed: 0.1269s/iter; left time: 6786.0773s\n",
      "\titers: 300, epoch: 1 | loss: 0.0518587\n",
      "\tspeed: 0.1271s/iter; left time: 6783.3071s\n",
      "\titers: 400, epoch: 1 | loss: 0.0307872\n",
      "\tspeed: 0.1271s/iter; left time: 6769.3660s\n",
      "\titers: 500, epoch: 1 | loss: 0.0314419\n",
      "\tspeed: 0.1273s/iter; left time: 6768.2172s\n",
      "\titers: 600, epoch: 1 | loss: 0.0260363\n",
      "\tspeed: 0.1280s/iter; left time: 6793.3712s\n",
      "Epoch: 1 cost time: 00h:01m:27.22s\n",
      "Epoch: 1 | Train Loss: 0.0476036 Vali Loss: 0.0233658 Test Loss: 0.0288830\n",
      "Validation loss decreased (inf --> 0.023366).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0213778\n",
      "\tspeed: 0.5946s/iter; left time: 30265.1573s\n",
      "\titers: 200, epoch: 2 | loss: 0.0244658\n",
      "\tspeed: 0.1267s/iter; left time: 6433.4810s\n",
      "\titers: 300, epoch: 2 | loss: 0.0254603\n",
      "\tspeed: 0.1270s/iter; left time: 6440.6542s\n",
      "\titers: 400, epoch: 2 | loss: 0.0223233\n",
      "\tspeed: 0.1289s/iter; left time: 6521.2317s\n",
      "\titers: 500, epoch: 2 | loss: 0.0193527\n",
      "\tspeed: 0.1262s/iter; left time: 6374.5446s\n",
      "\titers: 600, epoch: 2 | loss: 0.0209664\n",
      "\tspeed: 0.1261s/iter; left time: 6357.2339s\n",
      "Epoch: 2 cost time: 00h:01m:25.67s\n",
      "Epoch: 2 | Train Loss: 0.0233597 Vali Loss: 0.0200944 Test Loss: 0.0255333\n",
      "Validation loss decreased (0.023366 --> 0.020094).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0208655\n",
      "\tspeed: 0.4640s/iter; left time: 22371.6611s\n",
      "\titers: 200, epoch: 3 | loss: 0.0178275\n",
      "\tspeed: 0.1246s/iter; left time: 5993.6641s\n",
      "\titers: 300, epoch: 3 | loss: 0.0186255\n",
      "\tspeed: 0.1271s/iter; left time: 6101.4731s\n",
      "\titers: 400, epoch: 3 | loss: 0.0170459\n",
      "\tspeed: 0.1262s/iter; left time: 6044.6347s\n",
      "\titers: 500, epoch: 3 | loss: 0.0153410\n",
      "\tspeed: 0.1263s/iter; left time: 6037.7861s\n",
      "\titers: 600, epoch: 3 | loss: 0.0206425\n",
      "\tspeed: 0.1267s/iter; left time: 6043.2833s\n",
      "Epoch: 3 cost time: 00h:01m:25.22s\n",
      "Epoch: 3 | Train Loss: 0.0217546 Vali Loss: 0.0193492 Test Loss: 0.0245727\n",
      "Validation loss decreased (0.020094 --> 0.019349).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0185548\n",
      "\tspeed: 0.4429s/iter; left time: 20164.6581s\n",
      "\titers: 200, epoch: 4 | loss: 0.0180719\n",
      "\tspeed: 0.1259s/iter; left time: 5720.7953s\n",
      "\titers: 300, epoch: 4 | loss: 0.0205472\n",
      "\tspeed: 0.1270s/iter; left time: 5757.3097s\n",
      "\titers: 400, epoch: 4 | loss: 0.0181765\n",
      "\tspeed: 0.1276s/iter; left time: 5772.6649s\n",
      "\titers: 500, epoch: 4 | loss: 0.0236331\n",
      "\tspeed: 0.1252s/iter; left time: 5649.7645s\n",
      "\titers: 600, epoch: 4 | loss: 0.0187238\n",
      "\tspeed: 0.1287s/iter; left time: 5794.1486s\n",
      "Epoch: 4 cost time: 00h:01m:26.87s\n",
      "Epoch: 4 | Train Loss: 0.0211093 Vali Loss: 0.0182891 Test Loss: 0.0227510\n",
      "Validation loss decreased (0.019349 --> 0.018289).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0187627\n",
      "\tspeed: 0.4851s/iter; left time: 20783.1460s\n",
      "\titers: 200, epoch: 5 | loss: 0.0220212\n",
      "\tspeed: 0.1276s/iter; left time: 5454.4736s\n",
      "\titers: 300, epoch: 5 | loss: 0.0210257\n",
      "\tspeed: 0.1266s/iter; left time: 5399.9272s\n",
      "\titers: 400, epoch: 5 | loss: 0.0186015\n",
      "\tspeed: 0.1258s/iter; left time: 5352.2610s\n",
      "\titers: 500, epoch: 5 | loss: 0.0257642\n",
      "\tspeed: 0.1249s/iter; left time: 5301.4490s\n",
      "\titers: 600, epoch: 5 | loss: 0.0190592\n",
      "\tspeed: 0.1299s/iter; left time: 5501.5710s\n",
      "Epoch: 5 cost time: 00h:01m:27.13s\n",
      "Epoch: 5 | Train Loss: 0.0204508 Vali Loss: 0.0182497 Test Loss: 0.0227887\n",
      "Validation loss decreased (0.018289 --> 0.018250).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0195714\n",
      "\tspeed: 0.4645s/iter; left time: 18656.2162s\n",
      "\titers: 200, epoch: 6 | loss: 0.0196226\n",
      "\tspeed: 0.1262s/iter; left time: 5056.3637s\n",
      "\titers: 300, epoch: 6 | loss: 0.0216896\n",
      "\tspeed: 0.1310s/iter; left time: 5234.3878s\n",
      "\titers: 400, epoch: 6 | loss: 0.0158579\n",
      "\tspeed: 0.1255s/iter; left time: 5001.6455s\n",
      "\titers: 500, epoch: 6 | loss: 0.0186769\n",
      "\tspeed: 0.1407s/iter; left time: 5595.3024s\n",
      "\titers: 600, epoch: 6 | loss: 0.0187091\n",
      "\tspeed: 0.1530s/iter; left time: 6067.1431s\n",
      "Epoch: 6 cost time: 00h:01m:29.94s\n",
      "Epoch: 6 | Train Loss: 0.0200203 Vali Loss: 0.0177962 Test Loss: 0.0221645\n",
      "Validation loss decreased (0.018250 --> 0.017796).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0158030\n",
      "\tspeed: 0.4879s/iter; left time: 18285.1390s\n",
      "\titers: 200, epoch: 7 | loss: 0.0222547\n",
      "\tspeed: 0.1297s/iter; left time: 4846.0219s\n",
      "\titers: 300, epoch: 7 | loss: 0.0215090\n",
      "\tspeed: 0.1568s/iter; left time: 5845.7176s\n",
      "\titers: 400, epoch: 7 | loss: 0.0224705\n",
      "\tspeed: 0.1688s/iter; left time: 6276.3726s\n",
      "\titers: 500, epoch: 7 | loss: 0.0232703\n",
      "\tspeed: 0.1624s/iter; left time: 6021.7367s\n",
      "\titers: 600, epoch: 7 | loss: 0.0177696\n",
      "\tspeed: 0.1647s/iter; left time: 6088.3996s\n",
      "Epoch: 7 cost time: 00h:01m:44.05s\n",
      "Epoch: 7 | Train Loss: 0.0197670 Vali Loss: 0.0177549 Test Loss: 0.0221883\n",
      "Validation loss decreased (0.017796 --> 0.017755).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0203262\n",
      "\tspeed: 0.6675s/iter; left time: 23222.8095s\n",
      "\titers: 200, epoch: 8 | loss: 0.0188753\n",
      "\tspeed: 0.1815s/iter; left time: 6295.9654s\n",
      "\titers: 300, epoch: 8 | loss: 0.0204109\n",
      "\tspeed: 0.1672s/iter; left time: 5785.3996s\n",
      "\titers: 400, epoch: 8 | loss: 0.0213176\n",
      "\tspeed: 0.1696s/iter; left time: 5850.0454s\n",
      "\titers: 500, epoch: 8 | loss: 0.0165740\n",
      "\tspeed: 0.1688s/iter; left time: 5804.8748s\n",
      "\titers: 600, epoch: 8 | loss: 0.0212293\n",
      "\tspeed: 0.1639s/iter; left time: 5620.0228s\n",
      "Epoch: 8 cost time: 00h:01m:56.30s\n",
      "Epoch: 8 | Train Loss: 0.0193864 Vali Loss: 0.0175740 Test Loss: 0.0218846\n",
      "Validation loss decreased (0.017755 --> 0.017574).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0163664\n",
      "\tspeed: 0.5237s/iter; left time: 16813.9164s\n",
      "\titers: 200, epoch: 9 | loss: 0.0184284\n",
      "\tspeed: 0.1447s/iter; left time: 4630.3970s\n",
      "\titers: 300, epoch: 9 | loss: 0.0206715\n",
      "\tspeed: 0.1305s/iter; left time: 4163.1917s\n",
      "\titers: 400, epoch: 9 | loss: 0.0168874\n",
      "\tspeed: 0.1275s/iter; left time: 4054.7804s\n",
      "\titers: 500, epoch: 9 | loss: 0.0197874\n",
      "\tspeed: 0.1272s/iter; left time: 4032.6580s\n",
      "\titers: 600, epoch: 9 | loss: 0.0207158\n",
      "\tspeed: 0.1253s/iter; left time: 3959.2846s\n",
      "Epoch: 9 cost time: 00h:01m:27.87s\n",
      "Epoch: 9 | Train Loss: 0.0189410 Vali Loss: 0.0176340 Test Loss: 0.0218443\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0205027\n",
      "\tspeed: 0.3829s/iter; left time: 11266.9392s\n",
      "\titers: 200, epoch: 10 | loss: 0.0160529\n",
      "\tspeed: 0.1255s/iter; left time: 3681.1416s\n",
      "\titers: 300, epoch: 10 | loss: 0.0160090\n",
      "\tspeed: 0.1230s/iter; left time: 3594.8396s\n",
      "\titers: 400, epoch: 10 | loss: 0.0175601\n",
      "\tspeed: 0.1260s/iter; left time: 3668.7326s\n",
      "\titers: 500, epoch: 10 | loss: 0.0185336\n",
      "\tspeed: 0.1253s/iter; left time: 3635.8107s\n",
      "\titers: 600, epoch: 10 | loss: 0.0158149\n",
      "\tspeed: 0.1265s/iter; left time: 3659.0470s\n",
      "Epoch: 10 cost time: 00h:01m:24.55s\n",
      "Epoch: 10 | Train Loss: 0.0186634 Vali Loss: 0.0173984 Test Loss: 0.0219072\n",
      "Validation loss decreased (0.017574 --> 0.017398).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0155978\n",
      "\tspeed: 0.5511s/iter; left time: 14736.5266s\n",
      "\titers: 200, epoch: 11 | loss: 0.0173789\n",
      "\tspeed: 0.1247s/iter; left time: 3321.7816s\n",
      "\titers: 300, epoch: 11 | loss: 0.0213597\n",
      "\tspeed: 0.1263s/iter; left time: 3351.7796s\n",
      "\titers: 400, epoch: 11 | loss: 0.0158266\n",
      "\tspeed: 0.1277s/iter; left time: 3376.2096s\n",
      "\titers: 500, epoch: 11 | loss: 0.0126218\n",
      "\tspeed: 0.1295s/iter; left time: 3410.4362s\n",
      "\titers: 600, epoch: 11 | loss: 0.0165448\n",
      "\tspeed: 0.1395s/iter; left time: 3659.7786s\n",
      "Epoch: 11 cost time: 00h:01m:29.19s\n",
      "Epoch: 11 | Train Loss: 0.0183404 Vali Loss: 0.0178120 Test Loss: 0.0219151\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0241187\n",
      "\tspeed: 0.4348s/iter; left time: 10459.6825s\n",
      "\titers: 200, epoch: 12 | loss: 0.0165954\n",
      "\tspeed: 0.1371s/iter; left time: 3284.9798s\n",
      "\titers: 300, epoch: 12 | loss: 0.0191694\n",
      "\tspeed: 0.1234s/iter; left time: 2944.6971s\n",
      "\titers: 400, epoch: 12 | loss: 0.0125043\n",
      "\tspeed: 0.1230s/iter; left time: 2923.2223s\n",
      "\titers: 500, epoch: 12 | loss: 0.0127750\n",
      "\tspeed: 0.1223s/iter; left time: 2892.7088s\n",
      "\titers: 600, epoch: 12 | loss: 0.0202185\n",
      "\tspeed: 0.1221s/iter; left time: 2875.4930s\n",
      "Epoch: 12 cost time: 00h:01m:26.24s\n",
      "Epoch: 12 | Train Loss: 0.0179019 Vali Loss: 0.0179225 Test Loss: 0.0220747\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0203301\n",
      "\tspeed: 0.3734s/iter; left time: 7981.7241s\n",
      "\titers: 200, epoch: 13 | loss: 0.0183116\n",
      "\tspeed: 0.1223s/iter; left time: 2602.3052s\n",
      "\titers: 300, epoch: 13 | loss: 0.0150865\n",
      "\tspeed: 0.1215s/iter; left time: 2571.5067s\n",
      "\titers: 400, epoch: 13 | loss: 0.0208507\n",
      "\tspeed: 0.1221s/iter; left time: 2573.5021s\n",
      "\titers: 500, epoch: 13 | loss: 0.0178677\n",
      "\tspeed: 0.1220s/iter; left time: 2558.1077s\n",
      "\titers: 600, epoch: 13 | loss: 0.0164449\n",
      "\tspeed: 0.1219s/iter; left time: 2544.2793s\n",
      "Epoch: 13 cost time: 00h:01m:22.57s\n",
      "Epoch: 13 | Train Loss: 0.0176932 Vali Loss: 0.0184704 Test Loss: 0.0224477\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0207964\n",
      "\tspeed: 0.3702s/iter; left time: 6919.2874s\n",
      "\titers: 200, epoch: 14 | loss: 0.0177327\n",
      "\tspeed: 0.1217s/iter; left time: 2261.6883s\n",
      "\titers: 300, epoch: 14 | loss: 0.0148072\n",
      "\tspeed: 0.1212s/iter; left time: 2241.0804s\n",
      "\titers: 400, epoch: 14 | loss: 0.0167184\n",
      "\tspeed: 0.1302s/iter; left time: 2394.1710s\n",
      "\titers: 500, epoch: 14 | loss: 0.0167975\n",
      "\tspeed: 0.1513s/iter; left time: 2767.8354s\n",
      "\titers: 600, epoch: 14 | loss: 0.0154496\n",
      "\tspeed: 0.1469s/iter; left time: 2672.7491s\n",
      "Epoch: 14 cost time: 00h:01m:28.90s\n",
      "Epoch: 14 | Train Loss: 0.0171912 Vali Loss: 0.0198765 Test Loss: 0.0227924\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0150788\n",
      "\tspeed: 0.3746s/iter; left time: 5996.2363s\n",
      "\titers: 200, epoch: 15 | loss: 0.0178858\n",
      "\tspeed: 0.1244s/iter; left time: 1978.2785s\n",
      "\titers: 300, epoch: 15 | loss: 0.0212907\n",
      "\tspeed: 0.1231s/iter; left time: 1945.0361s\n",
      "\titers: 400, epoch: 15 | loss: 0.0173432\n",
      "\tspeed: 0.1245s/iter; left time: 1955.4916s\n",
      "\titers: 500, epoch: 15 | loss: 0.0160355\n",
      "\tspeed: 0.1244s/iter; left time: 1940.7078s\n",
      "\titers: 600, epoch: 15 | loss: 0.0175801\n",
      "\tspeed: 0.1255s/iter; left time: 1945.6707s\n",
      "Epoch: 15 cost time: 00h:01m:23.88s\n",
      "Epoch: 15 | Train Loss: 0.0169270 Vali Loss: 0.0197246 Test Loss: 0.0226869\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958\n",
      "Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958\n",
      "Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958\n",
      "Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958\n",
      "Intermediate time for ES and pred_len 168: 00h:28m:46.51s\n",
      "\n",
      "Intermediate time for ES: 01h:28m:13.42s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-13 14:42:55,640] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:42:55,641] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:42:55,642] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:42:55,733] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 14:42:56,678] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:42:56,679] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:42:56,679] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 14:42:56,679] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 14:42:56,679] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86835\n",
      "val 18651\n",
      "test 18651\n",
      "train 86835\n",
      "val 18651\n",
      "test 18651\n",
      "train 86835\n",
      "train 86835\n",
      "val 18651\n",
      "test 18651\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-13 14:42:59,760] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 14:43:00,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 14:43:00,758] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 14:43:00,758] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 14:43:00,759] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 14:43:00,759] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 14:43:00,760] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 14:43:00,760] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 14:43:00,760] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 14:43:00,760] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 14:43:00,760] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 14:43:01,138] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 14:43:01,139] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:43:01,139] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.08 GB, percent = 20.1%\n",
      "[2024-11-13 14:43:01,298] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 14:43:01,299] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:43:01,299] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.34 GB, percent = 20.1%\n",
      "[2024-11-13 14:43:01,299] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 14:43:01,436] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 14:43:01,436] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 14:43:01,436] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.5 GB, percent = 20.1%\n",
      "[2024-11-13 14:43:01,437] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 14:43:01,437] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 14:43:01,437] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 14:43:01,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 14:43:01,437] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f320b314cd0>\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 14:43:01,438] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 14:43:01,439] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0239559\n",
      "\tspeed: 0.1535s/iter; left time: 8314.6429s\n",
      "\titers: 200, epoch: 1 | loss: 0.0201448\n",
      "\tspeed: 0.1207s/iter; left time: 6526.6814s\n",
      "\titers: 300, epoch: 1 | loss: 0.0162691\n",
      "\tspeed: 0.1228s/iter; left time: 6626.3690s\n",
      "\titers: 400, epoch: 1 | loss: 0.0126257\n",
      "\tspeed: 0.1208s/iter; left time: 6505.4305s\n",
      "\titers: 500, epoch: 1 | loss: 0.0144120\n",
      "\tspeed: 0.1197s/iter; left time: 6432.6045s\n",
      "\titers: 600, epoch: 1 | loss: 0.0185334\n",
      "\tspeed: 0.1181s/iter; left time: 6335.7325s\n",
      "Epoch: 1 cost time: 00h:01m:23.21s\n",
      "Epoch: 1 | Train Loss: 0.0225287 Vali Loss: 0.0118601 Test Loss: 0.0136884\n",
      "Validation loss decreased (inf --> 0.011860).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0131000\n",
      "\tspeed: 0.5963s/iter; left time: 30678.4429s\n",
      "\titers: 200, epoch: 2 | loss: 0.0220157\n",
      "\tspeed: 0.1151s/iter; left time: 5910.4524s\n",
      "\titers: 300, epoch: 2 | loss: 0.0084849\n",
      "\tspeed: 0.1178s/iter; left time: 6034.6866s\n",
      "\titers: 400, epoch: 2 | loss: 0.0084919\n",
      "\tspeed: 0.1163s/iter; left time: 5947.0902s\n",
      "\titers: 500, epoch: 2 | loss: 0.0147144\n",
      "\tspeed: 0.1174s/iter; left time: 5991.6140s\n",
      "\titers: 600, epoch: 2 | loss: 0.0093752\n",
      "\tspeed: 0.1197s/iter; left time: 6099.6163s\n",
      "Epoch: 2 cost time: 00h:01m:21.82s\n",
      "Epoch: 2 | Train Loss: 0.0104779 Vali Loss: 0.0106168 Test Loss: 0.0121037\n",
      "Validation loss decreased (0.011860 --> 0.010617).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0076656\n",
      "\tspeed: 0.4328s/iter; left time: 21092.2924s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123325\n",
      "\tspeed: 0.1402s/iter; left time: 6816.7212s\n",
      "\titers: 300, epoch: 3 | loss: 0.0073657\n",
      "\tspeed: 0.1401s/iter; left time: 6801.9330s\n",
      "\titers: 400, epoch: 3 | loss: 0.0088692\n",
      "\tspeed: 0.1356s/iter; left time: 6570.0101s\n",
      "\titers: 500, epoch: 3 | loss: 0.0086650\n",
      "\tspeed: 0.1379s/iter; left time: 6665.3065s\n",
      "\titers: 600, epoch: 3 | loss: 0.0082646\n",
      "\tspeed: 0.1381s/iter; left time: 6659.8929s\n",
      "Epoch: 3 cost time: 00h:01m:34.19s\n",
      "Epoch: 3 | Train Loss: 0.0094833 Vali Loss: 0.0101822 Test Loss: 0.0114591\n",
      "Validation loss decreased (0.010617 --> 0.010182).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0109312\n",
      "\tspeed: 0.4545s/iter; left time: 20918.7704s\n",
      "\titers: 200, epoch: 4 | loss: 0.0118849\n",
      "\tspeed: 0.1396s/iter; left time: 6409.6032s\n",
      "\titers: 300, epoch: 4 | loss: 0.0099334\n",
      "\tspeed: 0.1370s/iter; left time: 6279.5640s\n",
      "\titers: 400, epoch: 4 | loss: 0.0101027\n",
      "\tspeed: 0.1373s/iter; left time: 6277.3975s\n",
      "\titers: 500, epoch: 4 | loss: 0.0118740\n",
      "\tspeed: 0.1343s/iter; left time: 6127.4003s\n",
      "\titers: 600, epoch: 4 | loss: 0.0064906\n",
      "\tspeed: 0.1345s/iter; left time: 6124.8659s\n",
      "Epoch: 4 cost time: 00h:01m:34.49s\n",
      "Epoch: 4 | Train Loss: 0.0094642 Vali Loss: 0.0098645 Test Loss: 0.0111941\n",
      "Validation loss decreased (0.010182 --> 0.009865).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0145120\n",
      "\tspeed: 0.4298s/iter; left time: 18614.0588s\n",
      "\titers: 200, epoch: 5 | loss: 0.0107904\n",
      "\tspeed: 0.1247s/iter; left time: 5389.6077s\n",
      "\titers: 300, epoch: 5 | loss: 0.0115122\n",
      "\tspeed: 0.1140s/iter; left time: 4913.9500s\n",
      "\titers: 400, epoch: 5 | loss: 0.0064738\n",
      "\tspeed: 0.1139s/iter; left time: 4897.5264s\n",
      "\titers: 500, epoch: 5 | loss: 0.0065169\n",
      "\tspeed: 0.1137s/iter; left time: 4878.3686s\n",
      "\titers: 600, epoch: 5 | loss: 0.0121063\n",
      "\tspeed: 0.1139s/iter; left time: 4875.0252s\n",
      "Epoch: 5 cost time: 00h:01m:20.96s\n",
      "Epoch: 5 | Train Loss: 0.0089185 Vali Loss: 0.0098854 Test Loss: 0.0113841\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0107378\n",
      "\tspeed: 0.3697s/iter; left time: 15008.8728s\n",
      "\titers: 200, epoch: 6 | loss: 0.0071157\n",
      "\tspeed: 0.1367s/iter; left time: 5537.7969s\n",
      "\titers: 300, epoch: 6 | loss: 0.0085659\n",
      "\tspeed: 0.1129s/iter; left time: 4561.8675s\n",
      "\titers: 400, epoch: 6 | loss: 0.0074021\n",
      "\tspeed: 0.1132s/iter; left time: 4561.9657s\n",
      "\titers: 500, epoch: 6 | loss: 0.0067580\n",
      "\tspeed: 0.1134s/iter; left time: 4559.1630s\n",
      "\titers: 600, epoch: 6 | loss: 0.0085170\n",
      "\tspeed: 0.1150s/iter; left time: 4611.0675s\n",
      "Epoch: 6 cost time: 00h:01m:21.26s\n",
      "Epoch: 6 | Train Loss: 0.0088672 Vali Loss: 0.0098212 Test Loss: 0.0112326\n",
      "Validation loss decreased (0.009865 --> 0.009821).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0133668\n",
      "\tspeed: 0.3919s/iter; left time: 14845.5601s\n",
      "\titers: 200, epoch: 7 | loss: 0.0055773\n",
      "\tspeed: 0.1172s/iter; left time: 4429.8556s\n",
      "\titers: 300, epoch: 7 | loss: 0.0110484\n",
      "\tspeed: 0.1280s/iter; left time: 4824.3672s\n",
      "\titers: 400, epoch: 7 | loss: 0.0098669\n",
      "\tspeed: 0.1344s/iter; left time: 5050.5473s\n",
      "\titers: 500, epoch: 7 | loss: 0.0105809\n",
      "\tspeed: 0.1354s/iter; left time: 5075.7303s\n",
      "\titers: 600, epoch: 7 | loss: 0.0083511\n",
      "\tspeed: 0.1339s/iter; left time: 5006.1533s\n",
      "Epoch: 7 cost time: 00h:01m:27.69s\n",
      "Epoch: 7 | Train Loss: 0.0086675 Vali Loss: 0.0096691 Test Loss: 0.0111444\n",
      "Validation loss decreased (0.009821 --> 0.009669).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0081424\n",
      "\tspeed: 0.4543s/iter; left time: 15976.7866s\n",
      "\titers: 200, epoch: 8 | loss: 0.0119270\n",
      "\tspeed: 0.1370s/iter; left time: 4804.7230s\n",
      "\titers: 300, epoch: 8 | loss: 0.0090215\n",
      "\tspeed: 0.1376s/iter; left time: 4810.4875s\n",
      "\titers: 400, epoch: 8 | loss: 0.0059014\n",
      "\tspeed: 0.1351s/iter; left time: 4709.6023s\n",
      "\titers: 500, epoch: 8 | loss: 0.0100736\n",
      "\tspeed: 0.1359s/iter; left time: 4726.3167s\n",
      "\titers: 600, epoch: 8 | loss: 0.0117893\n",
      "\tspeed: 0.1332s/iter; left time: 4616.8839s\n",
      "Epoch: 8 cost time: 00h:01m:32.36s\n",
      "Epoch: 8 | Train Loss: 0.0086480 Vali Loss: 0.0095132 Test Loss: 0.0111062\n",
      "Validation loss decreased (0.009669 --> 0.009513).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0090044\n",
      "\tspeed: 0.4220s/iter; left time: 13695.6154s\n",
      "\titers: 200, epoch: 9 | loss: 0.0094946\n",
      "\tspeed: 0.1130s/iter; left time: 3657.6006s\n",
      "\titers: 300, epoch: 9 | loss: 0.0079705\n",
      "\tspeed: 0.1146s/iter; left time: 3696.0086s\n",
      "\titers: 400, epoch: 9 | loss: 0.0099703\n",
      "\tspeed: 0.1144s/iter; left time: 3677.6264s\n",
      "\titers: 500, epoch: 9 | loss: 0.0080995\n",
      "\tspeed: 0.1120s/iter; left time: 3589.5967s\n",
      "\titers: 600, epoch: 9 | loss: 0.0063729\n",
      "\tspeed: 0.1128s/iter; left time: 3606.3261s\n",
      "Epoch: 9 cost time: 00h:01m:17.40s\n",
      "Epoch: 9 | Train Loss: 0.0085195 Vali Loss: 0.0092639 Test Loss: 0.0106769\n",
      "Validation loss decreased (0.009513 --> 0.009264).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0053214\n",
      "\tspeed: 0.3968s/iter; left time: 11803.6134s\n",
      "\titers: 200, epoch: 10 | loss: 0.0139130\n",
      "\tspeed: 0.1224s/iter; left time: 3629.3693s\n",
      "\titers: 300, epoch: 10 | loss: 0.0081261\n",
      "\tspeed: 0.1421s/iter; left time: 4197.5543s\n",
      "\titers: 400, epoch: 10 | loss: 0.0048974\n",
      "\tspeed: 0.1316s/iter; left time: 3875.8314s\n",
      "\titers: 500, epoch: 10 | loss: 0.0076337\n",
      "\tspeed: 0.1234s/iter; left time: 3621.5456s\n",
      "\titers: 600, epoch: 10 | loss: 0.0100944\n",
      "\tspeed: 0.1181s/iter; left time: 3452.9911s\n",
      "Epoch: 10 cost time: 00h:01m:24.50s\n",
      "Epoch: 10 | Train Loss: 0.0086789 Vali Loss: 0.0094028 Test Loss: 0.0108369\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0053404\n",
      "\tspeed: 0.3583s/iter; left time: 9684.7092s\n",
      "\titers: 200, epoch: 11 | loss: 0.0082792\n",
      "\tspeed: 0.1176s/iter; left time: 3168.3203s\n",
      "\titers: 300, epoch: 11 | loss: 0.0070582\n",
      "\tspeed: 0.1144s/iter; left time: 3069.2251s\n",
      "\titers: 400, epoch: 11 | loss: 0.0067524\n",
      "\tspeed: 0.1145s/iter; left time: 3061.2162s\n",
      "\titers: 500, epoch: 11 | loss: 0.0078302\n",
      "\tspeed: 0.1166s/iter; left time: 3105.0071s\n",
      "\titers: 600, epoch: 11 | loss: 0.0079525\n",
      "\tspeed: 0.1123s/iter; left time: 2978.8869s\n",
      "Epoch: 11 cost time: 00h:01m:19.56s\n",
      "Epoch: 11 | Train Loss: 0.0083401 Vali Loss: 0.0091377 Test Loss: 0.0106622\n",
      "Validation loss decreased (0.009264 --> 0.009138).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0096092\n",
      "\tspeed: 0.5652s/iter; left time: 13745.0025s\n",
      "\titers: 200, epoch: 12 | loss: 0.0080460\n",
      "\tspeed: 0.1198s/iter; left time: 2902.3752s\n",
      "\titers: 300, epoch: 12 | loss: 0.0127255\n",
      "\tspeed: 0.1402s/iter; left time: 3381.4459s\n",
      "\titers: 400, epoch: 12 | loss: 0.0106730\n",
      "\tspeed: 0.1315s/iter; left time: 3157.4855s\n",
      "\titers: 500, epoch: 12 | loss: 0.0083658\n",
      "\tspeed: 0.1368s/iter; left time: 3272.7590s\n",
      "\titers: 600, epoch: 12 | loss: 0.0132466\n",
      "\tspeed: 0.1388s/iter; left time: 3306.0601s\n",
      "Epoch: 12 cost time: 00h:01m:28.28s\n",
      "Epoch: 12 | Train Loss: 0.0082366 Vali Loss: 0.0090408 Test Loss: 0.0104260\n",
      "Validation loss decreased (0.009138 --> 0.009041).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0112591\n",
      "\tspeed: 0.5561s/iter; left time: 12013.6904s\n",
      "\titers: 200, epoch: 13 | loss: 0.0129779\n",
      "\tspeed: 0.1274s/iter; left time: 2740.5263s\n",
      "\titers: 300, epoch: 13 | loss: 0.0067636\n",
      "\tspeed: 0.1321s/iter; left time: 2828.4366s\n",
      "\titers: 400, epoch: 13 | loss: 0.0061778\n",
      "\tspeed: 0.1343s/iter; left time: 2861.2577s\n",
      "\titers: 500, epoch: 13 | loss: 0.0104312\n",
      "\tspeed: 0.1257s/iter; left time: 2664.9778s\n",
      "\titers: 600, epoch: 13 | loss: 0.0091502\n",
      "\tspeed: 0.1458s/iter; left time: 3076.6329s\n",
      "Epoch: 13 cost time: 00h:01m:29.87s\n",
      "Epoch: 13 | Train Loss: 0.0080974 Vali Loss: 0.0090926 Test Loss: 0.0107355\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0067687\n",
      "\tspeed: 0.3882s/iter; left time: 7333.6089s\n",
      "\titers: 200, epoch: 14 | loss: 0.0091137\n",
      "\tspeed: 0.1293s/iter; left time: 2429.6993s\n",
      "\titers: 300, epoch: 14 | loss: 0.0098937\n",
      "\tspeed: 0.1283s/iter; left time: 2397.5826s\n",
      "\titers: 400, epoch: 14 | loss: 0.0086841\n",
      "\tspeed: 0.1293s/iter; left time: 2404.1979s\n",
      "\titers: 500, epoch: 14 | loss: 0.0082142\n",
      "\tspeed: 0.1294s/iter; left time: 2392.4609s\n",
      "\titers: 600, epoch: 14 | loss: 0.0087004\n",
      "\tspeed: 0.1337s/iter; left time: 2458.7431s\n",
      "Epoch: 14 cost time: 00h:01m:28.82s\n",
      "Epoch: 14 | Train Loss: 0.0081368 Vali Loss: 0.0089570 Test Loss: 0.0104242\n",
      "Validation loss decreased (0.009041 --> 0.008957).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0167940\n",
      "\tspeed: 0.4305s/iter; left time: 6965.3747s\n",
      "\titers: 200, epoch: 15 | loss: 0.0050574\n",
      "\tspeed: 0.1322s/iter; left time: 2125.4118s\n",
      "\titers: 300, epoch: 15 | loss: 0.0104902\n",
      "\tspeed: 0.1139s/iter; left time: 1820.6538s\n",
      "\titers: 400, epoch: 15 | loss: 0.0089468\n",
      "\tspeed: 0.1115s/iter; left time: 1770.2470s\n",
      "\titers: 500, epoch: 15 | loss: 0.0126251\n",
      "\tspeed: 0.1146s/iter; left time: 1807.4899s\n",
      "\titers: 600, epoch: 15 | loss: 0.0059486\n",
      "\tspeed: 0.1334s/iter; left time: 2091.3762s\n",
      "Epoch: 15 cost time: 00h:01m:22.72s\n",
      "Epoch: 15 | Train Loss: 0.0081152 Vali Loss: 0.0090319 Test Loss: 0.0105617\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0057051\n",
      "\tspeed: 0.3942s/iter; left time: 5308.0479s\n",
      "\titers: 200, epoch: 16 | loss: 0.0066385\n",
      "\tspeed: 0.1331s/iter; left time: 1779.0068s\n",
      "\titers: 300, epoch: 16 | loss: 0.0073851\n",
      "\tspeed: 0.1337s/iter; left time: 1774.0160s\n",
      "\titers: 400, epoch: 16 | loss: 0.0081082\n",
      "\tspeed: 0.1343s/iter; left time: 1768.2760s\n",
      "\titers: 500, epoch: 16 | loss: 0.0123262\n",
      "\tspeed: 0.1459s/iter; left time: 1906.1656s\n",
      "\titers: 600, epoch: 16 | loss: 0.0072221\n",
      "\tspeed: 0.1130s/iter; left time: 1465.3041s\n",
      "Epoch: 16 cost time: 00h:01m:29.70s\n",
      "Epoch: 16 | Train Loss: 0.0078252 Vali Loss: 0.0091145 Test Loss: 0.0108264\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0097102\n",
      "\tspeed: 0.3997s/iter; left time: 4298.2763s\n",
      "\titers: 200, epoch: 17 | loss: 0.0064988\n",
      "\tspeed: 0.1276s/iter; left time: 1359.5931s\n",
      "\titers: 300, epoch: 17 | loss: 0.0057568\n",
      "\tspeed: 0.1315s/iter; left time: 1387.6995s\n",
      "\titers: 400, epoch: 17 | loss: 0.0065034\n",
      "\tspeed: 0.1291s/iter; left time: 1349.1259s\n",
      "\titers: 500, epoch: 17 | loss: 0.0095674\n",
      "\tspeed: 0.1360s/iter; left time: 1408.2471s\n",
      "\titers: 600, epoch: 17 | loss: 0.0062516\n",
      "\tspeed: 0.1289s/iter; left time: 1321.8942s\n",
      "Epoch: 17 cost time: 00h:01m:29.83s\n",
      "Epoch: 17 | Train Loss: 0.0078117 Vali Loss: 0.0089328 Test Loss: 0.0105026\n",
      "Validation loss decreased (0.008957 --> 0.008933).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0078043\n",
      "\tspeed: 0.4592s/iter; left time: 3692.0060s\n",
      "\titers: 200, epoch: 18 | loss: 0.0063590\n",
      "\tspeed: 0.1344s/iter; left time: 1067.5159s\n",
      "\titers: 300, epoch: 18 | loss: 0.0094975\n",
      "\tspeed: 0.1145s/iter; left time: 898.0118s\n",
      "\titers: 400, epoch: 18 | loss: 0.0071088\n",
      "\tspeed: 0.1119s/iter; left time: 866.0930s\n",
      "\titers: 500, epoch: 18 | loss: 0.0055368\n",
      "\tspeed: 0.1115s/iter; left time: 851.6252s\n",
      "\titers: 600, epoch: 18 | loss: 0.0132828\n",
      "\tspeed: 0.1120s/iter; left time: 844.7505s\n",
      "Epoch: 18 cost time: 00h:01m:20.12s\n",
      "Epoch: 18 | Train Loss: 0.0078160 Vali Loss: 0.0089081 Test Loss: 0.0103755\n",
      "Validation loss decreased (0.008933 --> 0.008908).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0115771\n",
      "\tspeed: 0.4422s/iter; left time: 2355.7336s\n",
      "\titers: 200, epoch: 19 | loss: 0.0104875\n",
      "\tspeed: 0.1131s/iter; left time: 591.1323s\n",
      "\titers: 300, epoch: 19 | loss: 0.0096097\n",
      "\tspeed: 0.1136s/iter; left time: 582.6704s\n",
      "\titers: 400, epoch: 19 | loss: 0.0031140\n",
      "\tspeed: 0.1133s/iter; left time: 569.7313s\n",
      "\titers: 500, epoch: 19 | loss: 0.0102776\n",
      "\tspeed: 0.1135s/iter; left time: 559.4106s\n",
      "\titers: 600, epoch: 19 | loss: 0.0063839\n",
      "\tspeed: 0.1129s/iter; left time: 544.9096s\n",
      "Epoch: 19 cost time: 00h:01m:17.48s\n",
      "Epoch: 19 | Train Loss: 0.0077374 Vali Loss: 0.0088936 Test Loss: 0.0104487\n",
      "Validation loss decreased (0.008908 --> 0.008894).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 20 | loss: 0.0030808\n",
      "\tspeed: 0.4090s/iter; left time: 1069.2144s\n",
      "\titers: 200, epoch: 20 | loss: 0.0082016\n",
      "\tspeed: 0.1130s/iter; left time: 284.0617s\n",
      "\titers: 300, epoch: 20 | loss: 0.0043443\n",
      "\tspeed: 0.1122s/iter; left time: 270.9532s\n",
      "\titers: 400, epoch: 20 | loss: 0.0143065\n",
      "\tspeed: 0.1130s/iter; left time: 261.3999s\n",
      "\titers: 500, epoch: 20 | loss: 0.0083869\n",
      "\tspeed: 0.1122s/iter; left time: 248.4275s\n",
      "\titers: 600, epoch: 20 | loss: 0.0069334\n",
      "\tspeed: 0.1124s/iter; left time: 237.6108s\n",
      "Epoch: 20 cost time: 00h:01m:17.04s\n",
      "Epoch: 20 | Train Loss: 0.0076499 Vali Loss: 0.0088768 Test Loss: 0.0103445\n",
      "Validation loss decreased (0.008894 --> 0.008877).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "Scaled mse:0.010344491340219975, rmse:0.10170787572860718, mae:0.0594850592315197, rse:0.39389368891716003\n",
      "Scaled mse:0.010344491340219975, rmse:0.10170787572860718, mae:0.0594850592315197, rse:0.39389368891716003\n",
      "Scaled mse:0.010344491340219975, rmse:0.10170787572860718, mae:0.0594850592315197, rse:0.39389368891716003\n",
      "Scaled mse:0.010344491340219975, rmse:0.10170787572860718, mae:0.0594850592315197, rse:0.39389368891716003\n",
      "Intermediate time for FR and pred_len 24: 00h:35m:56.45s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-13 15:18:46,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:18:46,865] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:18:46,872] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:18:46,876] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:18:47,868] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:18:47,869] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 86619\n",
      "val 18435\n",
      "test 18435\n",
      "train 86619\n",
      "val 18435\n",
      "train 86619\n",
      "train 86619\n",
      "test 18435\n",
      "valval  1843518435\n",
      "\n",
      "test 18435\n",
      "test 18435\n",
      "[2024-11-13 15:18:50,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 15:18:50,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 15:18:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 15:18:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 15:18:50,874] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 15:18:50,875] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 15:18:50,875] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 15:18:51,208] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 15:18:51,209] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:18:51,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.61 GB, percent = 20.3%\n",
      "[2024-11-13 15:18:51,392] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 15:18:51,392] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:18:51,393] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.79 GB, percent = 20.3%\n",
      "[2024-11-13 15:18:51,393] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 15:18:51,553] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 15:18:51,553] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:18:51,553] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 205.03 GB, percent = 20.4%\n",
      "[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 15:18:51,554] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4554b6ebd0>\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 15:18:51,556] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0365720\n",
      "\tspeed: 0.1419s/iter; left time: 7664.2947s\n",
      "\titers: 200, epoch: 1 | loss: 0.0280516\n",
      "\tspeed: 0.1165s/iter; left time: 6280.5259s\n",
      "\titers: 300, epoch: 1 | loss: 0.0237334\n",
      "\tspeed: 0.1152s/iter; left time: 6199.3223s\n",
      "\titers: 400, epoch: 1 | loss: 0.0220072\n",
      "\tspeed: 0.1150s/iter; left time: 6176.2841s\n",
      "\titers: 500, epoch: 1 | loss: 0.0152992\n",
      "\tspeed: 0.1148s/iter; left time: 6153.5580s\n",
      "\titers: 600, epoch: 1 | loss: 0.0149078\n",
      "\tspeed: 0.1153s/iter; left time: 6173.1032s\n",
      "Epoch: 1 cost time: 00h:01m:19.30s\n",
      "Epoch: 1 | Train Loss: 0.0296274 Vali Loss: 0.0188544 Test Loss: 0.0241413\n",
      "Validation loss decreased (inf --> 0.018854).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0257344\n",
      "\tspeed: 0.4362s/iter; left time: 22381.5116s\n",
      "\titers: 200, epoch: 2 | loss: 0.0131987\n",
      "\tspeed: 0.1145s/iter; left time: 5861.7240s\n",
      "\titers: 300, epoch: 2 | loss: 0.0125734\n",
      "\tspeed: 0.1177s/iter; left time: 6017.1740s\n",
      "\titers: 400, epoch: 2 | loss: 0.0112410\n",
      "\tspeed: 0.1131s/iter; left time: 5768.9233s\n",
      "\titers: 500, epoch: 2 | loss: 0.0113286\n",
      "\tspeed: 0.1142s/iter; left time: 5814.1264s\n",
      "\titers: 600, epoch: 2 | loss: 0.0112728\n",
      "\tspeed: 0.1139s/iter; left time: 5789.5294s\n",
      "Epoch: 2 cost time: 00h:01m:17.98s\n",
      "Epoch: 2 | Train Loss: 0.0150897 Vali Loss: 0.0162923 Test Loss: 0.0207778\n",
      "Validation loss decreased (0.018854 --> 0.016292).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0193557\n",
      "\tspeed: 0.3832s/iter; left time: 18627.1257s\n",
      "\titers: 200, epoch: 3 | loss: 0.0122473\n",
      "\tspeed: 0.1138s/iter; left time: 5522.5575s\n",
      "\titers: 300, epoch: 3 | loss: 0.0140332\n",
      "\tspeed: 0.1137s/iter; left time: 5502.2013s\n",
      "\titers: 400, epoch: 3 | loss: 0.0137614\n",
      "\tspeed: 0.1128s/iter; left time: 5446.8939s\n",
      "\titers: 500, epoch: 3 | loss: 0.0180832\n",
      "\tspeed: 0.1153s/iter; left time: 5556.6085s\n",
      "\titers: 600, epoch: 3 | loss: 0.0061270\n",
      "\tspeed: 0.1132s/iter; left time: 5448.0168s\n",
      "Epoch: 3 cost time: 00h:01m:17.49s\n",
      "Epoch: 3 | Train Loss: 0.0144389 Vali Loss: 0.0159258 Test Loss: 0.0202510\n",
      "Validation loss decreased (0.016292 --> 0.015926).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0191393\n",
      "\tspeed: 0.3971s/iter; left time: 18229.9854s\n",
      "\titers: 200, epoch: 4 | loss: 0.0209338\n",
      "\tspeed: 0.1128s/iter; left time: 5165.3610s\n",
      "\titers: 300, epoch: 4 | loss: 0.0178645\n",
      "\tspeed: 0.1127s/iter; left time: 5151.2119s\n",
      "\titers: 400, epoch: 4 | loss: 0.0173927\n",
      "\tspeed: 0.1135s/iter; left time: 5176.2412s\n",
      "\titers: 500, epoch: 4 | loss: 0.0158787\n",
      "\tspeed: 0.1125s/iter; left time: 5121.1046s\n",
      "\titers: 600, epoch: 4 | loss: 0.0139707\n",
      "\tspeed: 0.1122s/iter; left time: 5094.4243s\n",
      "Epoch: 4 cost time: 00h:01m:16.74s\n",
      "Epoch: 4 | Train Loss: 0.0139130 Vali Loss: 0.0156960 Test Loss: 0.0202585\n",
      "Validation loss decreased (0.015926 --> 0.015696).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0164406\n",
      "\tspeed: 0.4045s/iter; left time: 17474.8781s\n",
      "\titers: 200, epoch: 5 | loss: 0.0133676\n",
      "\tspeed: 0.1116s/iter; left time: 4808.6903s\n",
      "\titers: 300, epoch: 5 | loss: 0.0133471\n",
      "\tspeed: 0.1123s/iter; left time: 4828.8202s\n",
      "\titers: 400, epoch: 5 | loss: 0.0130934\n",
      "\tspeed: 0.1122s/iter; left time: 4811.6602s\n",
      "\titers: 500, epoch: 5 | loss: 0.0143422\n",
      "\tspeed: 0.1123s/iter; left time: 4806.3726s\n",
      "\titers: 600, epoch: 5 | loss: 0.0112816\n",
      "\tspeed: 0.1114s/iter; left time: 4758.3289s\n",
      "Epoch: 5 cost time: 00h:01m:16.45s\n",
      "Epoch: 5 | Train Loss: 0.0133617 Vali Loss: 0.0154382 Test Loss: 0.0200492\n",
      "Validation loss decreased (0.015696 --> 0.015438).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0113203\n",
      "\tspeed: 0.4274s/iter; left time: 17304.8190s\n",
      "\titers: 200, epoch: 6 | loss: 0.0115995\n",
      "\tspeed: 0.1112s/iter; left time: 4491.5802s\n",
      "\titers: 300, epoch: 6 | loss: 0.0073734\n",
      "\tspeed: 0.1113s/iter; left time: 4485.9441s\n",
      "\titers: 400, epoch: 6 | loss: 0.0149999\n",
      "\tspeed: 0.1113s/iter; left time: 4472.1406s\n",
      "\titers: 500, epoch: 6 | loss: 0.0097439\n",
      "\tspeed: 0.1119s/iter; left time: 4486.1774s\n",
      "\titers: 600, epoch: 6 | loss: 0.0117388\n",
      "\tspeed: 0.1116s/iter; left time: 4462.6731s\n",
      "Epoch: 6 cost time: 00h:01m:15.96s\n",
      "Epoch: 6 | Train Loss: 0.0132906 Vali Loss: 0.0152655 Test Loss: 0.0199258\n",
      "Validation loss decreased (0.015438 --> 0.015265).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0131203\n",
      "\tspeed: 0.4346s/iter; left time: 16422.5427s\n",
      "\titers: 200, epoch: 7 | loss: 0.0149721\n",
      "\tspeed: 0.1110s/iter; left time: 4184.6685s\n",
      "\titers: 300, epoch: 7 | loss: 0.0097917\n",
      "\tspeed: 0.1107s/iter; left time: 4159.5746s\n",
      "\titers: 400, epoch: 7 | loss: 0.0154822\n",
      "\tspeed: 0.1111s/iter; left time: 4165.3162s\n",
      "\titers: 500, epoch: 7 | loss: 0.0144257\n",
      "\tspeed: 0.1110s/iter; left time: 4151.0080s\n",
      "\titers: 600, epoch: 7 | loss: 0.0235436\n",
      "\tspeed: 0.1113s/iter; left time: 4150.6749s\n",
      "Epoch: 7 cost time: 00h:01m:15.79s\n",
      "Epoch: 7 | Train Loss: 0.0129925 Vali Loss: 0.0151174 Test Loss: 0.0199530\n",
      "Validation loss decreased (0.015265 --> 0.015117).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0148050\n",
      "\tspeed: 0.4010s/iter; left time: 14067.4559s\n",
      "\titers: 200, epoch: 8 | loss: 0.0146219\n",
      "\tspeed: 0.1114s/iter; left time: 3898.3741s\n",
      "\titers: 300, epoch: 8 | loss: 0.0177321\n",
      "\tspeed: 0.1105s/iter; left time: 3855.7388s\n",
      "\titers: 400, epoch: 8 | loss: 0.0141665\n",
      "\tspeed: 0.1105s/iter; left time: 3843.9874s\n",
      "\titers: 500, epoch: 8 | loss: 0.0116750\n",
      "\tspeed: 0.1106s/iter; left time: 3833.8648s\n",
      "\titers: 600, epoch: 8 | loss: 0.0111578\n",
      "\tspeed: 0.1112s/iter; left time: 3845.3055s\n",
      "Epoch: 8 cost time: 00h:01m:15.63s\n",
      "Epoch: 8 | Train Loss: 0.0128574 Vali Loss: 0.0150844 Test Loss: 0.0198784\n",
      "Validation loss decreased (0.015117 --> 0.015084).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0125546\n",
      "\tspeed: 0.3980s/iter; left time: 12885.7508s\n",
      "\titers: 200, epoch: 9 | loss: 0.0176416\n",
      "\tspeed: 0.1111s/iter; left time: 3586.3694s\n",
      "\titers: 300, epoch: 9 | loss: 0.0117473\n",
      "\tspeed: 0.1105s/iter; left time: 3555.9730s\n",
      "\titers: 400, epoch: 9 | loss: 0.0085340\n",
      "\tspeed: 0.1106s/iter; left time: 3547.2687s\n",
      "\titers: 500, epoch: 9 | loss: 0.0147927\n",
      "\tspeed: 0.1102s/iter; left time: 3524.2646s\n",
      "\titers: 600, epoch: 9 | loss: 0.0098768\n",
      "\tspeed: 0.1103s/iter; left time: 3516.0856s\n",
      "Epoch: 9 cost time: 00h:01m:15.41s\n",
      "Epoch: 9 | Train Loss: 0.0128993 Vali Loss: 0.0149726 Test Loss: 0.0199329\n",
      "Validation loss decreased (0.015084 --> 0.014973).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0176372\n",
      "\tspeed: 0.4246s/iter; left time: 12596.2343s\n",
      "\titers: 200, epoch: 10 | loss: 0.0126642\n",
      "\tspeed: 0.1105s/iter; left time: 3266.2789s\n",
      "\titers: 300, epoch: 10 | loss: 0.0110052\n",
      "\tspeed: 0.1106s/iter; left time: 3258.6639s\n",
      "\titers: 400, epoch: 10 | loss: 0.0197459\n",
      "\tspeed: 0.1106s/iter; left time: 3246.9784s\n",
      "\titers: 500, epoch: 10 | loss: 0.0204530\n",
      "\tspeed: 0.1105s/iter; left time: 3233.4938s\n",
      "\titers: 600, epoch: 10 | loss: 0.0130680\n",
      "\tspeed: 0.1099s/iter; left time: 3205.7374s\n",
      "Epoch: 10 cost time: 00h:01m:15.28s\n",
      "Epoch: 10 | Train Loss: 0.0128206 Vali Loss: 0.0151203 Test Loss: 0.0199767\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0164479\n",
      "\tspeed: 0.3447s/iter; left time: 9293.0614s\n",
      "\titers: 200, epoch: 11 | loss: 0.0108230\n",
      "\tspeed: 0.1108s/iter; left time: 2977.2863s\n",
      "\titers: 300, epoch: 11 | loss: 0.0085999\n",
      "\tspeed: 0.1118s/iter; left time: 2993.1143s\n",
      "\titers: 400, epoch: 11 | loss: 0.0096984\n",
      "\tspeed: 0.1106s/iter; left time: 2947.9467s\n",
      "\titers: 500, epoch: 11 | loss: 0.0173206\n",
      "\tspeed: 0.1112s/iter; left time: 2954.8101s\n",
      "\titers: 600, epoch: 11 | loss: 0.0124708\n",
      "\tspeed: 0.1106s/iter; left time: 2927.1686s\n",
      "Epoch: 11 cost time: 00h:01m:15.78s\n",
      "Epoch: 11 | Train Loss: 0.0124435 Vali Loss: 0.0150630 Test Loss: 0.0202035\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0131188\n",
      "\tspeed: 0.3425s/iter; left time: 8307.4692s\n",
      "\titers: 200, epoch: 12 | loss: 0.0120664\n",
      "\tspeed: 0.1111s/iter; left time: 2683.0771s\n",
      "\titers: 300, epoch: 12 | loss: 0.0137616\n",
      "\tspeed: 0.1113s/iter; left time: 2677.7270s\n",
      "\titers: 400, epoch: 12 | loss: 0.0140155\n",
      "\tspeed: 0.1115s/iter; left time: 2669.8734s\n",
      "\titers: 500, epoch: 12 | loss: 0.0105363\n",
      "\tspeed: 0.1110s/iter; left time: 2648.3874s\n",
      "\titers: 600, epoch: 12 | loss: 0.0159831\n",
      "\tspeed: 0.1106s/iter; left time: 2626.8158s\n",
      "Epoch: 12 cost time: 00h:01m:15.51s\n",
      "Epoch: 12 | Train Loss: 0.0123154 Vali Loss: 0.0148068 Test Loss: 0.0198685\n",
      "Validation loss decreased (0.014973 --> 0.014807).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0107450\n",
      "\tspeed: 0.3894s/iter; left time: 8392.2184s\n",
      "\titers: 200, epoch: 13 | loss: 0.0130737\n",
      "\tspeed: 0.1120s/iter; left time: 2402.1775s\n",
      "\titers: 300, epoch: 13 | loss: 0.0093372\n",
      "\tspeed: 0.1115s/iter; left time: 2379.8026s\n",
      "\titers: 400, epoch: 13 | loss: 0.0096826\n",
      "\tspeed: 0.1099s/iter; left time: 2335.0702s\n",
      "\titers: 500, epoch: 13 | loss: 0.0082334\n",
      "\tspeed: 0.1100s/iter; left time: 2325.8892s\n",
      "\titers: 600, epoch: 13 | loss: 0.0136555\n",
      "\tspeed: 0.1105s/iter; left time: 2325.4441s\n",
      "Epoch: 13 cost time: 00h:01m:15.36s\n",
      "Epoch: 13 | Train Loss: 0.0123478 Vali Loss: 0.0147222 Test Loss: 0.0195867\n",
      "Validation loss decreased (0.014807 --> 0.014722).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0138494\n",
      "\tspeed: 0.3884s/iter; left time: 7318.9826s\n",
      "\titers: 200, epoch: 14 | loss: 0.0141361\n",
      "\tspeed: 0.1105s/iter; left time: 2071.1033s\n",
      "\titers: 300, epoch: 14 | loss: 0.0092835\n",
      "\tspeed: 0.1116s/iter; left time: 2079.8785s\n",
      "\titers: 400, epoch: 14 | loss: 0.0081837\n",
      "\tspeed: 0.1170s/iter; left time: 2169.1301s\n",
      "\titers: 500, epoch: 14 | loss: 0.0109799\n",
      "\tspeed: 0.1158s/iter; left time: 2136.2294s\n",
      "\titers: 600, epoch: 14 | loss: 0.0090381\n",
      "\tspeed: 0.1116s/iter; left time: 2047.5919s\n",
      "Epoch: 14 cost time: 00h:01m:17.05s\n",
      "Epoch: 14 | Train Loss: 0.0121958 Vali Loss: 0.0146016 Test Loss: 0.0196531\n",
      "Validation loss decreased (0.014722 --> 0.014602).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0129044\n",
      "\tspeed: 0.3849s/iter; left time: 6211.5884s\n",
      "\titers: 200, epoch: 15 | loss: 0.0082274\n",
      "\tspeed: 0.1107s/iter; left time: 1774.5290s\n",
      "\titers: 300, epoch: 15 | loss: 0.0082835\n",
      "\tspeed: 0.1101s/iter; left time: 1754.2830s\n",
      "\titers: 400, epoch: 15 | loss: 0.0094719\n",
      "\tspeed: 0.1150s/iter; left time: 1820.6878s\n",
      "\titers: 500, epoch: 15 | loss: 0.0099078\n",
      "\tspeed: 0.1112s/iter; left time: 1750.1903s\n",
      "\titers: 600, epoch: 15 | loss: 0.0100165\n",
      "\tspeed: 0.1101s/iter; left time: 1721.8566s\n",
      "Epoch: 15 cost time: 00h:01m:15.74s\n",
      "Epoch: 15 | Train Loss: 0.0118769 Vali Loss: 0.0149706 Test Loss: 0.0198224\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0098050\n",
      "\tspeed: 0.3467s/iter; left time: 4656.7881s\n",
      "\titers: 200, epoch: 16 | loss: 0.0165337\n",
      "\tspeed: 0.1117s/iter; left time: 1488.5019s\n",
      "\titers: 300, epoch: 16 | loss: 0.0093741\n",
      "\tspeed: 0.1112s/iter; left time: 1470.6937s\n",
      "\titers: 400, epoch: 16 | loss: 0.0085768\n",
      "\tspeed: 0.1132s/iter; left time: 1485.8487s\n",
      "\titers: 500, epoch: 16 | loss: 0.0100504\n",
      "\tspeed: 0.1122s/iter; left time: 1462.5196s\n",
      "\titers: 600, epoch: 16 | loss: 0.0101264\n",
      "\tspeed: 0.1111s/iter; left time: 1436.1961s\n",
      "Epoch: 16 cost time: 00h:01m:15.93s\n",
      "Epoch: 16 | Train Loss: 0.0117034 Vali Loss: 0.0150434 Test Loss: 0.0202259\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0177830\n",
      "\tspeed: 0.3434s/iter; left time: 3682.5295s\n",
      "\titers: 200, epoch: 17 | loss: 0.0133350\n",
      "\tspeed: 0.1122s/iter; left time: 1192.1194s\n",
      "\titers: 300, epoch: 17 | loss: 0.0065481\n",
      "\tspeed: 0.1107s/iter; left time: 1164.8550s\n",
      "\titers: 400, epoch: 17 | loss: 0.0142507\n",
      "\tspeed: 0.1113s/iter; left time: 1160.4833s\n",
      "\titers: 500, epoch: 17 | loss: 0.0093826\n",
      "\tspeed: 0.1112s/iter; left time: 1147.9576s\n",
      "\titers: 600, epoch: 17 | loss: 0.0085390\n",
      "\tspeed: 0.1114s/iter; left time: 1138.6955s\n",
      "Epoch: 17 cost time: 00h:01m:15.84s\n",
      "Epoch: 17 | Train Loss: 0.0115625 Vali Loss: 0.0149463 Test Loss: 0.0203532\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0068310\n",
      "\tspeed: 0.3423s/iter; left time: 2744.6538s\n",
      "\titers: 200, epoch: 18 | loss: 0.0092751\n",
      "\tspeed: 0.1104s/iter; left time: 873.9001s\n",
      "\titers: 300, epoch: 18 | loss: 0.0090604\n",
      "\tspeed: 0.1106s/iter; left time: 864.9826s\n",
      "\titers: 400, epoch: 18 | loss: 0.0113523\n",
      "\tspeed: 0.1116s/iter; left time: 861.5789s\n",
      "\titers: 500, epoch: 18 | loss: 0.0100290\n",
      "\tspeed: 0.1106s/iter; left time: 842.9753s\n",
      "\titers: 600, epoch: 18 | loss: 0.0088446\n",
      "\tspeed: 0.1101s/iter; left time: 827.7318s\n",
      "Epoch: 18 cost time: 00h:01m:15.23s\n",
      "Epoch: 18 | Train Loss: 0.0114536 Vali Loss: 0.0154895 Test Loss: 0.0203540\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0120453\n",
      "\tspeed: 0.3382s/iter; left time: 1797.1140s\n",
      "\titers: 200, epoch: 19 | loss: 0.0099022\n",
      "\tspeed: 0.1101s/iter; left time: 573.9307s\n",
      "\titers: 300, epoch: 19 | loss: 0.0156211\n",
      "\tspeed: 0.1124s/iter; left time: 574.5642s\n",
      "\titers: 400, epoch: 19 | loss: 0.0092177\n",
      "\tspeed: 0.1123s/iter; left time: 562.8975s\n",
      "\titers: 500, epoch: 19 | loss: 0.0075598\n",
      "\tspeed: 0.1114s/iter; left time: 547.0750s\n",
      "\titers: 600, epoch: 19 | loss: 0.0066154\n",
      "\tspeed: 0.1104s/iter; left time: 531.3639s\n",
      "Epoch: 19 cost time: 00h:01m:15.56s\n",
      "Epoch: 19 | Train Loss: 0.0113176 Vali Loss: 0.0156360 Test Loss: 0.0205360\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391\n",
      "Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391\n",
      "Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391\n",
      "Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391\n",
      "Intermediate time for FR and pred_len 96: 00h:30m:21.47s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-13 15:49:08,824] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:49:08,836] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:49:08,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:49:08,864] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:49:09,868] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 15:49:09,869] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86403\n",
      "val 18219\n",
      "test 18219\n",
      "train 86403\n",
      "train 86403\n",
      "val 18219\n",
      "train 86403\n",
      "val 18219\n",
      "test 18219\n",
      "val 18219\n",
      "test 18219\n",
      "test 18219\n",
      "[2024-11-13 15:49:12,125] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 15:49:13,171] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 15:49:13,172] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 15:49:13,172] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 15:49:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 15:49:13,173] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 15:49:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 15:49:13,562] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 15:49:13,562] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:49:13,563] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 201.53 GB, percent = 20.0%\n",
      "[2024-11-13 15:49:13,761] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 15:49:13,762] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:49:13,762] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 201.91 GB, percent = 20.0%\n",
      "[2024-11-13 15:49:13,762] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 15:49:13,949] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 15:49:13,949] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 15:49:13,949] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.39 GB, percent = 20.1%\n",
      "[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4d03a04910>\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 15:49:13,952] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0417766\n",
      "\tspeed: 0.1477s/iter; left time: 7959.7124s\n",
      "\titers: 200, epoch: 1 | loss: 0.0256541\n",
      "\tspeed: 0.1169s/iter; left time: 6289.3909s\n",
      "\titers: 300, epoch: 1 | loss: 0.0370757\n",
      "\tspeed: 0.1172s/iter; left time: 6296.3554s\n",
      "\titers: 400, epoch: 1 | loss: 0.0266412\n",
      "\tspeed: 0.1174s/iter; left time: 6291.7026s\n",
      "\titers: 500, epoch: 1 | loss: 0.0169627\n",
      "\tspeed: 0.1172s/iter; left time: 6271.3629s\n",
      "\titers: 600, epoch: 1 | loss: 0.0200696\n",
      "\tspeed: 0.1175s/iter; left time: 6274.2301s\n",
      "Epoch: 1 cost time: 00h:01m:20.98s\n",
      "Epoch: 1 | Train Loss: 0.0314196 Vali Loss: 0.0201541 Test Loss: 0.0246362\n",
      "Validation loss decreased (inf --> 0.020154).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0140743\n",
      "\tspeed: 0.4194s/iter; left time: 21474.6910s\n",
      "\titers: 200, epoch: 2 | loss: 0.0158384\n",
      "\tspeed: 0.1160s/iter; left time: 5930.2192s\n",
      "\titers: 300, epoch: 2 | loss: 0.0162745\n",
      "\tspeed: 0.1157s/iter; left time: 5900.6336s\n",
      "\titers: 400, epoch: 2 | loss: 0.0154659\n",
      "\tspeed: 0.1155s/iter; left time: 5878.6740s\n",
      "\titers: 500, epoch: 2 | loss: 0.0126034\n",
      "\tspeed: 0.1156s/iter; left time: 5872.2664s\n",
      "\titers: 600, epoch: 2 | loss: 0.0152980\n",
      "\tspeed: 0.1165s/iter; left time: 5904.5133s\n",
      "Epoch: 2 cost time: 00h:01m:19.24s\n",
      "Epoch: 2 | Train Loss: 0.0164696 Vali Loss: 0.0180134 Test Loss: 0.0222745\n",
      "Validation loss decreased (0.020154 --> 0.018013).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0205479\n",
      "\tspeed: 0.3963s/iter; left time: 19220.1896s\n",
      "\titers: 200, epoch: 3 | loss: 0.0187195\n",
      "\tspeed: 0.1169s/iter; left time: 5659.5534s\n",
      "\titers: 300, epoch: 3 | loss: 0.0161316\n",
      "\tspeed: 0.1163s/iter; left time: 5616.5458s\n",
      "\titers: 400, epoch: 3 | loss: 0.0095569\n",
      "\tspeed: 0.1162s/iter; left time: 5598.8585s\n",
      "\titers: 500, epoch: 3 | loss: 0.0130455\n",
      "\tspeed: 0.1171s/iter; left time: 5632.8513s\n",
      "\titers: 600, epoch: 3 | loss: 0.0106364\n",
      "\tspeed: 0.1152s/iter; left time: 5530.9617s\n",
      "Epoch: 3 cost time: 00h:01m:19.02s\n",
      "Epoch: 3 | Train Loss: 0.0156743 Vali Loss: 0.0172826 Test Loss: 0.0217130\n",
      "Validation loss decreased (0.018013 --> 0.017283).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0156313\n",
      "\tspeed: 0.4117s/iter; left time: 18857.1162s\n",
      "\titers: 200, epoch: 4 | loss: 0.0166580\n",
      "\tspeed: 0.1146s/iter; left time: 5238.3839s\n",
      "\titers: 300, epoch: 4 | loss: 0.0135303\n",
      "\tspeed: 0.1141s/iter; left time: 5201.0737s\n",
      "\titers: 400, epoch: 4 | loss: 0.0096387\n",
      "\tspeed: 0.1160s/iter; left time: 5277.0169s\n",
      "\titers: 500, epoch: 4 | loss: 0.0092750\n",
      "\tspeed: 0.1157s/iter; left time: 5251.9154s\n",
      "\titers: 600, epoch: 4 | loss: 0.0137595\n",
      "\tspeed: 0.1148s/iter; left time: 5199.4185s\n",
      "Epoch: 4 cost time: 00h:01m:18.57s\n",
      "Epoch: 4 | Train Loss: 0.0152146 Vali Loss: 0.0170755 Test Loss: 0.0217249\n",
      "Validation loss decreased (0.017283 --> 0.017076).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0197449\n",
      "\tspeed: 0.3953s/iter; left time: 17039.2556s\n",
      "\titers: 200, epoch: 5 | loss: 0.0156107\n",
      "\tspeed: 0.1151s/iter; left time: 4951.2011s\n",
      "\titers: 300, epoch: 5 | loss: 0.0155113\n",
      "\tspeed: 0.1142s/iter; left time: 4898.3954s\n",
      "\titers: 400, epoch: 5 | loss: 0.0121896\n",
      "\tspeed: 0.1157s/iter; left time: 4953.3144s\n",
      "\titers: 500, epoch: 5 | loss: 0.0110335\n",
      "\tspeed: 0.1164s/iter; left time: 4970.5123s\n",
      "\titers: 600, epoch: 5 | loss: 0.0154606\n",
      "\tspeed: 0.1152s/iter; left time: 4906.7805s\n",
      "Epoch: 5 cost time: 00h:01m:18.37s\n",
      "Epoch: 5 | Train Loss: 0.0150166 Vali Loss: 0.0169449 Test Loss: 0.0217564\n",
      "Validation loss decreased (0.017076 --> 0.016945).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0126255\n",
      "\tspeed: 0.3971s/iter; left time: 16042.8409s\n",
      "\titers: 200, epoch: 6 | loss: 0.0132552\n",
      "\tspeed: 0.1149s/iter; left time: 4630.1364s\n",
      "\titers: 300, epoch: 6 | loss: 0.0162678\n",
      "\tspeed: 0.1147s/iter; left time: 4610.6448s\n",
      "\titers: 400, epoch: 6 | loss: 0.0198398\n",
      "\tspeed: 0.1162s/iter; left time: 4660.8443s\n",
      "\titers: 500, epoch: 6 | loss: 0.0115809\n",
      "\tspeed: 0.1138s/iter; left time: 4551.9183s\n",
      "\titers: 600, epoch: 6 | loss: 0.0190240\n",
      "\tspeed: 0.1137s/iter; left time: 4537.5913s\n",
      "Epoch: 6 cost time: 00h:01m:18.17s\n",
      "Epoch: 6 | Train Loss: 0.0145469 Vali Loss: 0.0170547 Test Loss: 0.0219276\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0134221\n",
      "\tspeed: 0.3479s/iter; left time: 13114.4315s\n",
      "\titers: 200, epoch: 7 | loss: 0.0113875\n",
      "\tspeed: 0.1129s/iter; left time: 4245.2983s\n",
      "\titers: 300, epoch: 7 | loss: 0.0183335\n",
      "\tspeed: 0.1137s/iter; left time: 4262.7963s\n",
      "\titers: 400, epoch: 7 | loss: 0.0140515\n",
      "\tspeed: 0.1222s/iter; left time: 4571.8369s\n",
      "\titers: 500, epoch: 7 | loss: 0.0185097\n",
      "\tspeed: 0.1970s/iter; left time: 7348.2177s\n",
      "\titers: 600, epoch: 7 | loss: 0.0128493\n",
      "\tspeed: 0.1129s/iter; left time: 4200.4297s\n",
      "Epoch: 7 cost time: 00h:01m:26.35s\n",
      "Epoch: 7 | Train Loss: 0.0144284 Vali Loss: 0.0167750 Test Loss: 0.0216554\n",
      "Validation loss decreased (0.016945 --> 0.016775).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0152631\n",
      "\tspeed: 0.4829s/iter; left time: 16901.5566s\n",
      "\titers: 200, epoch: 8 | loss: 0.0107777\n",
      "\tspeed: 0.1519s/iter; left time: 5300.6262s\n",
      "\titers: 300, epoch: 8 | loss: 0.0169159\n",
      "\tspeed: 0.1147s/iter; left time: 3992.6808s\n",
      "\titers: 400, epoch: 8 | loss: 0.0165618\n",
      "\tspeed: 0.1147s/iter; left time: 3981.8822s\n",
      "\titers: 500, epoch: 8 | loss: 0.0104190\n",
      "\tspeed: 0.1140s/iter; left time: 3945.8500s\n",
      "\titers: 600, epoch: 8 | loss: 0.0168079\n",
      "\tspeed: 0.1134s/iter; left time: 3913.1608s\n",
      "Epoch: 8 cost time: 00h:01m:26.72s\n",
      "Epoch: 8 | Train Loss: 0.0142601 Vali Loss: 0.0163743 Test Loss: 0.0212844\n",
      "Validation loss decreased (0.016775 --> 0.016374).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0102134\n",
      "\tspeed: 0.4321s/iter; left time: 13957.6054s\n",
      "\titers: 200, epoch: 9 | loss: 0.0134811\n",
      "\tspeed: 0.1125s/iter; left time: 3624.2015s\n",
      "\titers: 300, epoch: 9 | loss: 0.0126132\n",
      "\tspeed: 0.1130s/iter; left time: 3626.0318s\n",
      "\titers: 400, epoch: 9 | loss: 0.0204920\n",
      "\tspeed: 0.1134s/iter; left time: 3627.6755s\n",
      "\titers: 500, epoch: 9 | loss: 0.0125607\n",
      "\tspeed: 0.1125s/iter; left time: 3588.3175s\n",
      "\titers: 600, epoch: 9 | loss: 0.0123675\n",
      "\tspeed: 0.1128s/iter; left time: 3586.3467s\n",
      "Epoch: 9 cost time: 00h:01m:17.57s\n",
      "Epoch: 9 | Train Loss: 0.0142351 Vali Loss: 0.0164664 Test Loss: 0.0217097\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0138933\n",
      "\tspeed: 0.3439s/iter; left time: 10180.2120s\n",
      "\titers: 200, epoch: 10 | loss: 0.0144352\n",
      "\tspeed: 0.1128s/iter; left time: 3326.5281s\n",
      "\titers: 300, epoch: 10 | loss: 0.0188366\n",
      "\tspeed: 0.1124s/iter; left time: 3303.6556s\n",
      "\titers: 400, epoch: 10 | loss: 0.0131616\n",
      "\tspeed: 0.1130s/iter; left time: 3309.6236s\n",
      "\titers: 500, epoch: 10 | loss: 0.0119591\n",
      "\tspeed: 0.1137s/iter; left time: 3320.8010s\n",
      "\titers: 600, epoch: 10 | loss: 0.0107009\n",
      "\tspeed: 0.1134s/iter; left time: 3298.8945s\n",
      "Epoch: 10 cost time: 00h:01m:16.65s\n",
      "Epoch: 10 | Train Loss: 0.0140122 Vali Loss: 0.0163399 Test Loss: 0.0212699\n",
      "Validation loss decreased (0.016374 --> 0.016340).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0127098\n",
      "\tspeed: 0.4626s/iter; left time: 12445.2411s\n",
      "\titers: 200, epoch: 11 | loss: 0.0168296\n",
      "\tspeed: 0.1129s/iter; left time: 3027.0891s\n",
      "\titers: 300, epoch: 11 | loss: 0.0170023\n",
      "\tspeed: 0.1144s/iter; left time: 3055.4112s\n",
      "\titers: 400, epoch: 11 | loss: 0.0155827\n",
      "\tspeed: 0.1139s/iter; left time: 3031.0551s\n",
      "\titers: 500, epoch: 11 | loss: 0.0107336\n",
      "\tspeed: 0.1140s/iter; left time: 3020.7316s\n",
      "\titers: 600, epoch: 11 | loss: 0.0104780\n",
      "\tspeed: 0.1165s/iter; left time: 3076.7567s\n",
      "Epoch: 11 cost time: 00h:01m:17.59s\n",
      "Epoch: 11 | Train Loss: 0.0137326 Vali Loss: 0.0162093 Test Loss: 0.0212543\n",
      "Validation loss decreased (0.016340 --> 0.016209).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0166218\n",
      "\tspeed: 0.4469s/iter; left time: 10815.4111s\n",
      "\titers: 200, epoch: 12 | loss: 0.0157183\n",
      "\tspeed: 0.1151s/iter; left time: 2774.3302s\n",
      "\titers: 300, epoch: 12 | loss: 0.0122932\n",
      "\tspeed: 0.1124s/iter; left time: 2697.0745s\n",
      "\titers: 400, epoch: 12 | loss: 0.0200626\n",
      "\tspeed: 0.1126s/iter; left time: 2690.6896s\n",
      "\titers: 500, epoch: 12 | loss: 0.0111632\n",
      "\tspeed: 0.1124s/iter; left time: 2674.5361s\n",
      "\titers: 600, epoch: 12 | loss: 0.0163757\n",
      "\tspeed: 0.1125s/iter; left time: 2666.8778s\n",
      "Epoch: 12 cost time: 00h:01m:17.04s\n",
      "Epoch: 12 | Train Loss: 0.0135342 Vali Loss: 0.0164481 Test Loss: 0.0215607\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0125801\n",
      "\tspeed: 0.3429s/iter; left time: 7372.9554s\n",
      "\titers: 200, epoch: 13 | loss: 0.0092519\n",
      "\tspeed: 0.1122s/iter; left time: 2402.1986s\n",
      "\titers: 300, epoch: 13 | loss: 0.0114003\n",
      "\tspeed: 0.1126s/iter; left time: 2399.3992s\n",
      "\titers: 400, epoch: 13 | loss: 0.0143267\n",
      "\tspeed: 0.1133s/iter; left time: 2402.5757s\n",
      "\titers: 500, epoch: 13 | loss: 0.0193831\n",
      "\tspeed: 0.1134s/iter; left time: 2393.4618s\n",
      "\titers: 600, epoch: 13 | loss: 0.0142914\n",
      "\tspeed: 0.1137s/iter; left time: 2387.4374s\n",
      "Epoch: 13 cost time: 00h:01m:16.74s\n",
      "Epoch: 13 | Train Loss: 0.0134906 Vali Loss: 0.0167998 Test Loss: 0.0217321\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0120610\n",
      "\tspeed: 0.3434s/iter; left time: 6456.8045s\n",
      "\titers: 200, epoch: 14 | loss: 0.0161036\n",
      "\tspeed: 0.1124s/iter; left time: 2101.4046s\n",
      "\titers: 300, epoch: 14 | loss: 0.0149346\n",
      "\tspeed: 0.1121s/iter; left time: 2085.1297s\n",
      "\titers: 400, epoch: 14 | loss: 0.0147595\n",
      "\tspeed: 0.1137s/iter; left time: 2104.3682s\n",
      "\titers: 500, epoch: 14 | loss: 0.0182757\n",
      "\tspeed: 0.1136s/iter; left time: 2089.5769s\n",
      "\titers: 600, epoch: 14 | loss: 0.0136856\n",
      "\tspeed: 0.1143s/iter; left time: 2091.2705s\n",
      "Epoch: 14 cost time: 00h:01m:16.92s\n",
      "Epoch: 14 | Train Loss: 0.0134139 Vali Loss: 0.0169548 Test Loss: 0.0227987\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0185989\n",
      "\tspeed: 0.3409s/iter; left time: 5489.4018s\n",
      "\titers: 200, epoch: 15 | loss: 0.0171201\n",
      "\tspeed: 0.1121s/iter; left time: 1794.4336s\n",
      "\titers: 300, epoch: 15 | loss: 0.0171193\n",
      "\tspeed: 0.1126s/iter; left time: 1789.9002s\n",
      "\titers: 400, epoch: 15 | loss: 0.0135243\n",
      "\tspeed: 0.1120s/iter; left time: 1769.7305s\n",
      "\titers: 500, epoch: 15 | loss: 0.0115249\n",
      "\tspeed: 0.1119s/iter; left time: 1756.7221s\n",
      "\titers: 600, epoch: 15 | loss: 0.0136147\n",
      "\tspeed: 0.1120s/iter; left time: 1747.7410s\n",
      "Epoch: 15 cost time: 00h:01m:16.30s\n",
      "Epoch: 15 | Train Loss: 0.0131047 Vali Loss: 0.0173370 Test Loss: 0.0227311\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0187483\n",
      "\tspeed: 0.3391s/iter; left time: 4543.9042s\n",
      "\titers: 200, epoch: 16 | loss: 0.0144263\n",
      "\tspeed: 0.1120s/iter; left time: 1489.2937s\n",
      "\titers: 300, epoch: 16 | loss: 0.0091274\n",
      "\tspeed: 0.1119s/iter; left time: 1477.5226s\n",
      "\titers: 400, epoch: 16 | loss: 0.0168024\n",
      "\tspeed: 0.1122s/iter; left time: 1469.4416s\n",
      "\titers: 500, epoch: 16 | loss: 0.0127802\n",
      "\tspeed: 0.1129s/iter; left time: 1468.3672s\n",
      "\titers: 600, epoch: 16 | loss: 0.0151057\n",
      "\tspeed: 0.1126s/iter; left time: 1452.8706s\n",
      "Epoch: 16 cost time: 00h:01m:16.33s\n",
      "Epoch: 16 | Train Loss: 0.0130004 Vali Loss: 0.0173143 Test Loss: 0.0228985\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354\n",
      "Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354\n",
      "Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354\n",
      "Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354\n",
      "Intermediate time for FR and pred_len 168: 00h:26m:21.56s\n",
      "\n",
      "Intermediate time for FR: 01h:32m:39.48s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-13 16:15:30,654] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:15:30,664] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:15:30,665] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:15:30,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:15:31,728] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 86835\n",
      "val 18651\n",
      "test 18651\n",
      "train 86835\n",
      "train 86835\n",
      "train 86835\n",
      "val 18651\n",
      "val 18651\n",
      "val 18651\n",
      "test 18651\n",
      "test 18651\n",
      "test 18651\n",
      "[2024-11-13 16:15:34,079] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 16:15:35,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 16:15:35,004] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 16:15:35,004] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 16:15:35,335] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 16:15:35,336] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:15:35,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.05 GB, percent = 20.1%\n",
      "[2024-11-13 16:15:35,508] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 16:15:35,508] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:15:35,508] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.23 GB, percent = 20.1%\n",
      "[2024-11-13 16:15:35,509] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 16:15:35,637] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 16:15:35,638] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:15:35,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.53 GB, percent = 20.1%\n",
      "[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabecaf0850>\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 16:15:35,640] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0669913\n",
      "\tspeed: 0.1452s/iter; left time: 7865.3973s\n",
      "\titers: 200, epoch: 1 | loss: 0.0426923\n",
      "\tspeed: 0.1172s/iter; left time: 6337.3856s\n",
      "\titers: 300, epoch: 1 | loss: 0.0284488\n",
      "\tspeed: 0.1179s/iter; left time: 6360.4377s\n",
      "\titers: 400, epoch: 1 | loss: 0.0195312\n",
      "\tspeed: 0.1174s/iter; left time: 6322.7274s\n",
      "\titers: 500, epoch: 1 | loss: 0.0354719\n",
      "\tspeed: 0.1174s/iter; left time: 6312.8136s\n",
      "\titers: 600, epoch: 1 | loss: 0.0156711\n",
      "\tspeed: 0.1167s/iter; left time: 6263.0584s\n",
      "Epoch: 1 cost time: 00h:01m:20.86s\n",
      "Epoch: 1 | Train Loss: 0.0390933 Vali Loss: 0.0132475 Test Loss: 0.0142686\n",
      "Validation loss decreased (inf --> 0.013247).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0214472\n",
      "\tspeed: 0.4357s/iter; left time: 22414.6633s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166737\n",
      "\tspeed: 0.1891s/iter; left time: 9711.8633s\n",
      "\titers: 300, epoch: 2 | loss: 0.0151975\n",
      "\tspeed: 0.1149s/iter; left time: 5886.9193s\n",
      "\titers: 400, epoch: 2 | loss: 0.0165503\n",
      "\tspeed: 0.1166s/iter; left time: 5966.1397s\n",
      "\titers: 500, epoch: 2 | loss: 0.0280452\n",
      "\tspeed: 0.1155s/iter; left time: 5896.9445s\n",
      "\titers: 600, epoch: 2 | loss: 0.0204850\n",
      "\tspeed: 0.1196s/iter; left time: 6094.7441s\n",
      "Epoch: 2 cost time: 00h:01m:28.50s\n",
      "Epoch: 2 | Train Loss: 0.0186876 Vali Loss: 0.0117981 Test Loss: 0.0127852\n",
      "Validation loss decreased (0.013247 --> 0.011798).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0146356\n",
      "\tspeed: 0.4916s/iter; left time: 23957.2551s\n",
      "\titers: 200, epoch: 3 | loss: 0.0132557\n",
      "\tspeed: 0.1145s/iter; left time: 5568.5583s\n",
      "\titers: 300, epoch: 3 | loss: 0.0179834\n",
      "\tspeed: 0.1144s/iter; left time: 5554.6485s\n",
      "\titers: 400, epoch: 3 | loss: 0.0231912\n",
      "\tspeed: 0.1147s/iter; left time: 5554.1902s\n",
      "\titers: 500, epoch: 3 | loss: 0.0207518\n",
      "\tspeed: 0.1145s/iter; left time: 5531.9955s\n",
      "\titers: 600, epoch: 3 | loss: 0.0146786\n",
      "\tspeed: 0.1146s/iter; left time: 5529.7541s\n",
      "Epoch: 3 cost time: 00h:01m:23.27s\n",
      "Epoch: 3 | Train Loss: 0.0170714 Vali Loss: 0.0112131 Test Loss: 0.0121951\n",
      "Validation loss decreased (0.011798 --> 0.011213).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0206654\n",
      "\tspeed: 0.4239s/iter; left time: 19509.9904s\n",
      "\titers: 200, epoch: 4 | loss: 0.0149954\n",
      "\tspeed: 0.1157s/iter; left time: 5311.4744s\n",
      "\titers: 300, epoch: 4 | loss: 0.0181798\n",
      "\tspeed: 0.1144s/iter; left time: 5243.0492s\n",
      "\titers: 400, epoch: 4 | loss: 0.0221356\n",
      "\tspeed: 0.1371s/iter; left time: 6267.5608s\n",
      "\titers: 500, epoch: 4 | loss: 0.0188067\n",
      "\tspeed: 0.1227s/iter; left time: 5596.6184s\n",
      "\titers: 600, epoch: 4 | loss: 0.0086526\n",
      "\tspeed: 0.1164s/iter; left time: 5300.9456s\n",
      "Epoch: 4 cost time: 00h:01m:21.90s\n",
      "Epoch: 4 | Train Loss: 0.0162319 Vali Loss: 0.0109968 Test Loss: 0.0120540\n",
      "Validation loss decreased (0.011213 --> 0.010997).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0238934\n",
      "\tspeed: 0.4692s/iter; left time: 20321.0175s\n",
      "\titers: 200, epoch: 5 | loss: 0.0171008\n",
      "\tspeed: 0.1147s/iter; left time: 4956.1446s\n",
      "\titers: 300, epoch: 5 | loss: 0.0146524\n",
      "\tspeed: 0.1138s/iter; left time: 4905.4229s\n",
      "\titers: 400, epoch: 5 | loss: 0.0147643\n",
      "\tspeed: 0.1150s/iter; left time: 4946.2218s\n",
      "\titers: 500, epoch: 5 | loss: 0.0178727\n",
      "\tspeed: 0.1144s/iter; left time: 4909.4947s\n",
      "\titers: 600, epoch: 5 | loss: 0.0203826\n",
      "\tspeed: 0.1141s/iter; left time: 4884.5804s\n",
      "Epoch: 5 cost time: 00h:01m:18.20s\n",
      "Epoch: 5 | Train Loss: 0.0159699 Vali Loss: 0.0107941 Test Loss: 0.0118797\n",
      "Validation loss decreased (0.010997 --> 0.010794).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0171005\n",
      "\tspeed: 0.4256s/iter; left time: 17275.7073s\n",
      "\titers: 200, epoch: 6 | loss: 0.0183887\n",
      "\tspeed: 0.1152s/iter; left time: 4663.3577s\n",
      "\titers: 300, epoch: 6 | loss: 0.0152069\n",
      "\tspeed: 0.1142s/iter; left time: 4614.1110s\n",
      "\titers: 400, epoch: 6 | loss: 0.0157148\n",
      "\tspeed: 0.1147s/iter; left time: 4621.5525s\n",
      "\titers: 500, epoch: 6 | loss: 0.0130175\n",
      "\tspeed: 0.1247s/iter; left time: 5010.4445s\n",
      "\titers: 600, epoch: 6 | loss: 0.0195304\n",
      "\tspeed: 0.2539s/iter; left time: 10180.1290s\n",
      "Epoch: 6 cost time: 00h:01m:43.44s\n",
      "Epoch: 6 | Train Loss: 0.0155751 Vali Loss: 0.0103284 Test Loss: 0.0113590\n",
      "Validation loss decreased (0.010794 --> 0.010328).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0149800\n",
      "\tspeed: 0.8237s/iter; left time: 31205.0641s\n",
      "\titers: 200, epoch: 7 | loss: 0.0142167\n",
      "\tspeed: 0.2453s/iter; left time: 9269.0863s\n",
      "\titers: 300, epoch: 7 | loss: 0.0094101\n",
      "\tspeed: 0.1755s/iter; left time: 6614.5832s\n",
      "\titers: 400, epoch: 7 | loss: 0.0143765\n",
      "\tspeed: 0.2438s/iter; left time: 9162.8789s\n",
      "\titers: 500, epoch: 7 | loss: 0.0181995\n",
      "\tspeed: 0.2442s/iter; left time: 9151.9915s\n",
      "\titers: 600, epoch: 7 | loss: 0.0154168\n",
      "\tspeed: 0.2447s/iter; left time: 9145.8870s\n",
      "Epoch: 7 cost time: 00h:02m:38.83s\n",
      "Epoch: 7 | Train Loss: 0.0153713 Vali Loss: 0.0102515 Test Loss: 0.0112287\n",
      "Validation loss decreased (0.010328 --> 0.010252).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0148691\n",
      "\tspeed: 0.7465s/iter; left time: 26255.7157s\n",
      "\titers: 200, epoch: 8 | loss: 0.0168062\n",
      "\tspeed: 0.1944s/iter; left time: 6819.2589s\n",
      "\titers: 300, epoch: 8 | loss: 0.0113323\n",
      "\tspeed: 0.2416s/iter; left time: 8447.5544s\n",
      "\titers: 400, epoch: 8 | loss: 0.0202741\n",
      "\tspeed: 0.2437s/iter; left time: 8496.8439s\n",
      "\titers: 500, epoch: 8 | loss: 0.0235890\n",
      "\tspeed: 0.2553s/iter; left time: 8875.2362s\n",
      "\titers: 600, epoch: 8 | loss: 0.0162556\n",
      "\tspeed: 0.2579s/iter; left time: 8941.4790s\n",
      "Epoch: 8 cost time: 00h:02m:36.47s\n",
      "Epoch: 8 | Train Loss: 0.0149047 Vali Loss: 0.0104321 Test Loss: 0.0115182\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0166072\n",
      "\tspeed: 0.5086s/iter; left time: 16508.3473s\n",
      "\titers: 200, epoch: 9 | loss: 0.0144150\n",
      "\tspeed: 0.1152s/iter; left time: 3727.8166s\n",
      "\titers: 300, epoch: 9 | loss: 0.0153691\n",
      "\tspeed: 0.1625s/iter; left time: 5240.1736s\n",
      "\titers: 400, epoch: 9 | loss: 0.0152353\n",
      "\tspeed: 0.1550s/iter; left time: 4983.7200s\n",
      "\titers: 500, epoch: 9 | loss: 0.0203724\n",
      "\tspeed: 0.1142s/iter; left time: 3660.8631s\n",
      "\titers: 600, epoch: 9 | loss: 0.0157388\n",
      "\tspeed: 0.1136s/iter; left time: 3630.6724s\n",
      "Epoch: 9 cost time: 00h:01m:29.43s\n",
      "Epoch: 9 | Train Loss: 0.0150001 Vali Loss: 0.0099908 Test Loss: 0.0111266\n",
      "Validation loss decreased (0.010252 --> 0.009991).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0183513\n",
      "\tspeed: 0.4843s/iter; left time: 14405.3458s\n",
      "\titers: 200, epoch: 10 | loss: 0.0142193\n",
      "\tspeed: 0.1125s/iter; left time: 3335.5192s\n",
      "\titers: 300, epoch: 10 | loss: 0.0162596\n",
      "\tspeed: 0.1133s/iter; left time: 3347.6672s\n",
      "\titers: 400, epoch: 10 | loss: 0.0119137\n",
      "\tspeed: 0.1137s/iter; left time: 3348.7877s\n",
      "\titers: 500, epoch: 10 | loss: 0.0134835\n",
      "\tspeed: 0.1135s/iter; left time: 3330.9299s\n",
      "\titers: 600, epoch: 10 | loss: 0.0192499\n",
      "\tspeed: 0.1136s/iter; left time: 3320.9394s\n",
      "Epoch: 10 cost time: 00h:01m:17.41s\n",
      "Epoch: 10 | Train Loss: 0.0148069 Vali Loss: 0.0100791 Test Loss: 0.0109957\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0120941\n",
      "\tspeed: 0.3553s/iter; left time: 9602.7900s\n",
      "\titers: 200, epoch: 11 | loss: 0.0227418\n",
      "\tspeed: 0.1119s/iter; left time: 3013.6185s\n",
      "\titers: 300, epoch: 11 | loss: 0.0172830\n",
      "\tspeed: 0.1118s/iter; left time: 3000.9905s\n",
      "\titers: 400, epoch: 11 | loss: 0.0157759\n",
      "\tspeed: 0.1138s/iter; left time: 3042.4452s\n",
      "\titers: 500, epoch: 11 | loss: 0.0149913\n",
      "\tspeed: 0.1139s/iter; left time: 3032.2947s\n",
      "\titers: 600, epoch: 11 | loss: 0.0125543\n",
      "\tspeed: 0.1130s/iter; left time: 2999.2075s\n",
      "Epoch: 11 cost time: 00h:01m:17.41s\n",
      "Epoch: 11 | Train Loss: 0.0144006 Vali Loss: 0.0102262 Test Loss: 0.0111913\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0114660\n",
      "\tspeed: 0.3513s/iter; left time: 8542.0242s\n",
      "\titers: 200, epoch: 12 | loss: 0.0127194\n",
      "\tspeed: 0.1131s/iter; left time: 2738.2732s\n",
      "\titers: 300, epoch: 12 | loss: 0.0106502\n",
      "\tspeed: 0.1142s/iter; left time: 2753.1607s\n",
      "\titers: 400, epoch: 12 | loss: 0.0113894\n",
      "\tspeed: 0.1128s/iter; left time: 2710.3290s\n",
      "\titers: 500, epoch: 12 | loss: 0.0169212\n",
      "\tspeed: 0.1142s/iter; left time: 2731.6365s\n",
      "\titers: 600, epoch: 12 | loss: 0.0151293\n",
      "\tspeed: 0.1126s/iter; left time: 2682.9997s\n",
      "Epoch: 12 cost time: 00h:01m:17.47s\n",
      "Epoch: 12 | Train Loss: 0.0144180 Vali Loss: 0.0095405 Test Loss: 0.0104502\n",
      "Validation loss decreased (0.009991 --> 0.009541).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0119666\n",
      "\tspeed: 0.4201s/iter; left time: 9076.3618s\n",
      "\titers: 200, epoch: 13 | loss: 0.0167867\n",
      "\tspeed: 0.1122s/iter; left time: 2411.9673s\n",
      "\titers: 300, epoch: 13 | loss: 0.0118353\n",
      "\tspeed: 0.1141s/iter; left time: 2442.2437s\n",
      "\titers: 400, epoch: 13 | loss: 0.0124032\n",
      "\tspeed: 0.1157s/iter; left time: 2466.0145s\n",
      "\titers: 500, epoch: 13 | loss: 0.0202020\n",
      "\tspeed: 0.1440s/iter; left time: 3053.7562s\n",
      "\titers: 600, epoch: 13 | loss: 0.0089123\n",
      "\tspeed: 0.1130s/iter; left time: 2384.9065s\n",
      "Epoch: 13 cost time: 00h:01m:21.63s\n",
      "Epoch: 13 | Train Loss: 0.0141540 Vali Loss: 0.0096531 Test Loss: 0.0106672\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0114304\n",
      "\tspeed: 0.3491s/iter; left time: 6594.9007s\n",
      "\titers: 200, epoch: 14 | loss: 0.0137170\n",
      "\tspeed: 0.1124s/iter; left time: 2112.7172s\n",
      "\titers: 300, epoch: 14 | loss: 0.0187478\n",
      "\tspeed: 0.1121s/iter; left time: 2094.7089s\n",
      "\titers: 400, epoch: 14 | loss: 0.0149776\n",
      "\tspeed: 0.1129s/iter; left time: 2098.6591s\n",
      "\titers: 500, epoch: 14 | loss: 0.0118570\n",
      "\tspeed: 0.1129s/iter; left time: 2087.9124s\n",
      "\titers: 600, epoch: 14 | loss: 0.0176578\n",
      "\tspeed: 0.1202s/iter; left time: 2210.9154s\n",
      "Epoch: 14 cost time: 00h:01m:26.06s\n",
      "Epoch: 14 | Train Loss: 0.0141004 Vali Loss: 0.0095795 Test Loss: 0.0105532\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0244140\n",
      "\tspeed: 0.7338s/iter; left time: 11872.0227s\n",
      "\titers: 200, epoch: 15 | loss: 0.0115425\n",
      "\tspeed: 0.2378s/iter; left time: 3822.9631s\n",
      "\titers: 300, epoch: 15 | loss: 0.0092068\n",
      "\tspeed: 0.2277s/iter; left time: 3638.7324s\n",
      "\titers: 400, epoch: 15 | loss: 0.0166868\n",
      "\tspeed: 0.1126s/iter; left time: 1788.1649s\n",
      "\titers: 500, epoch: 15 | loss: 0.0117777\n",
      "\tspeed: 0.1133s/iter; left time: 1787.3895s\n",
      "\titers: 600, epoch: 15 | loss: 0.0119113\n",
      "\tspeed: 0.1391s/iter; left time: 2180.5823s\n",
      "Epoch: 15 cost time: 00h:02m:05.31s\n",
      "Epoch: 15 | Train Loss: 0.0141697 Vali Loss: 0.0096087 Test Loss: 0.0106601\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0134954\n",
      "\tspeed: 0.7404s/iter; left time: 9970.3791s\n",
      "\titers: 200, epoch: 16 | loss: 0.0110016\n",
      "\tspeed: 0.2370s/iter; left time: 3167.8364s\n",
      "\titers: 300, epoch: 16 | loss: 0.0136263\n",
      "\tspeed: 0.2361s/iter; left time: 3132.1143s\n",
      "\titers: 400, epoch: 16 | loss: 0.0144925\n",
      "\tspeed: 0.2337s/iter; left time: 3076.5303s\n",
      "\titers: 500, epoch: 16 | loss: 0.0162292\n",
      "\tspeed: 0.2369s/iter; left time: 3095.5136s\n",
      "\titers: 600, epoch: 16 | loss: 0.0148276\n",
      "\tspeed: 0.2366s/iter; left time: 3067.8407s\n",
      "Epoch: 16 cost time: 00h:02m:40.38s\n",
      "Epoch: 16 | Train Loss: 0.0135790 Vali Loss: 0.0096248 Test Loss: 0.0105937\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0132713\n",
      "\tspeed: 0.7458s/iter; left time: 8020.0443s\n",
      "\titers: 200, epoch: 17 | loss: 0.0123942\n",
      "\tspeed: 0.2335s/iter; left time: 2487.1403s\n",
      "\titers: 300, epoch: 17 | loss: 0.0115208\n",
      "\tspeed: 0.2319s/iter; left time: 2447.3100s\n",
      "\titers: 400, epoch: 17 | loss: 0.0120183\n",
      "\tspeed: 0.2297s/iter; left time: 2401.3973s\n",
      "\titers: 500, epoch: 17 | loss: 0.0133054\n",
      "\tspeed: 0.2337s/iter; left time: 2419.7483s\n",
      "\titers: 600, epoch: 17 | loss: 0.0104209\n",
      "\tspeed: 0.2316s/iter; left time: 2374.2674s\n",
      "Epoch: 17 cost time: 00h:02m:37.98s\n",
      "Epoch: 17 | Train Loss: 0.0135725 Vali Loss: 0.0096924 Test Loss: 0.0105969\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871\n",
      "Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871\n",
      "Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871\n",
      "Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871\n",
      "Intermediate time for IT and pred_len 24: 00h:37m:06.66s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-13 16:52:55,838] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:52:55,845] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:52:55,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:52:55,864] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 16:52:56,642] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:52:56,643] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:52:56,645] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 16:52:56,645] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 16:52:56,645] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86619\n",
      "val 18435\n",
      "test 18435\n",
      "train 86619\n",
      "train 86619\n",
      "val 18435\n",
      "val 18435\n",
      "test 18435\n",
      "test 18435\n",
      "train 86619\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-13 16:52:58,918] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 16:53:00,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 16:53:00,360] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 16:53:00,360] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 16:53:00,361] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 16:53:00,361] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 16:53:00,361] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 16:53:00,685] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 16:53:00,686] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:53:00,686] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.54 GB, percent = 3.7%\n",
      "[2024-11-13 16:53:00,927] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 16:53:00,927] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:53:00,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.73 GB, percent = 3.7%\n",
      "[2024-11-13 16:53:00,928] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 16:53:01,031] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 16:53:01,031] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 16:53:01,031] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 38.05 GB, percent = 3.8%\n",
      "[2024-11-13 16:53:01,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6b92dfdb50>\n",
      "[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 16:53:01,034] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0709942\n",
      "\tspeed: 0.2675s/iter; left time: 14451.6049s\n",
      "\titers: 200, epoch: 1 | loss: 0.0518521\n",
      "\tspeed: 0.2367s/iter; left time: 12762.4700s\n",
      "\titers: 300, epoch: 1 | loss: 0.0491614\n",
      "\tspeed: 0.1374s/iter; left time: 7395.6174s\n",
      "\titers: 400, epoch: 1 | loss: 0.0385732\n",
      "\tspeed: 0.1238s/iter; left time: 6651.0406s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327860\n",
      "\tspeed: 0.1286s/iter; left time: 6894.0485s\n",
      "\titers: 600, epoch: 1 | loss: 0.0372788\n",
      "\tspeed: 0.1703s/iter; left time: 9116.7871s\n",
      "Epoch: 1 cost time: 00h:01m:57.54s\n",
      "Epoch: 1 | Train Loss: 0.0561526 Vali Loss: 0.0237931 Test Loss: 0.0257605\n",
      "Validation loss decreased (inf --> 0.023793).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0234595\n",
      "\tspeed: 0.7463s/iter; left time: 38298.2729s\n",
      "\titers: 200, epoch: 2 | loss: 0.0231870\n",
      "\tspeed: 0.1229s/iter; left time: 6294.9837s\n",
      "\titers: 300, epoch: 2 | loss: 0.0352537\n",
      "\tspeed: 0.1219s/iter; left time: 6229.9994s\n",
      "\titers: 400, epoch: 2 | loss: 0.0295735\n",
      "\tspeed: 0.1259s/iter; left time: 6423.6052s\n",
      "\titers: 500, epoch: 2 | loss: 0.0283380\n",
      "\tspeed: 0.1235s/iter; left time: 6290.3216s\n",
      "\titers: 600, epoch: 2 | loss: 0.0225025\n",
      "\tspeed: 0.1210s/iter; left time: 6149.3929s\n",
      "Epoch: 2 cost time: 00h:01m:24.28s\n",
      "Epoch: 2 | Train Loss: 0.0272431 Vali Loss: 0.0194962 Test Loss: 0.0212060\n",
      "Validation loss decreased (0.023793 --> 0.019496).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0334998\n",
      "\tspeed: 0.4813s/iter; left time: 23396.3363s\n",
      "\titers: 200, epoch: 3 | loss: 0.0210327\n",
      "\tspeed: 0.1165s/iter; left time: 5652.7593s\n",
      "\titers: 300, epoch: 3 | loss: 0.0221955\n",
      "\tspeed: 0.1197s/iter; left time: 5792.7736s\n",
      "\titers: 400, epoch: 3 | loss: 0.0250691\n",
      "\tspeed: 0.1204s/iter; left time: 5818.3800s\n",
      "\titers: 500, epoch: 3 | loss: 0.0253366\n",
      "\tspeed: 0.1229s/iter; left time: 5923.6119s\n",
      "\titers: 600, epoch: 3 | loss: 0.0158106\n",
      "\tspeed: 0.1185s/iter; left time: 5699.8039s\n",
      "Epoch: 3 cost time: 00h:01m:22.30s\n",
      "Epoch: 3 | Train Loss: 0.0251084 Vali Loss: 0.0182750 Test Loss: 0.0199267\n",
      "Validation loss decreased (0.019496 --> 0.018275).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0260743\n",
      "\tspeed: 0.4879s/iter; left time: 22398.2073s\n",
      "\titers: 200, epoch: 4 | loss: 0.0281503\n",
      "\tspeed: 0.1219s/iter; left time: 5584.0512s\n",
      "\titers: 300, epoch: 4 | loss: 0.0267283\n",
      "\tspeed: 0.1181s/iter; left time: 5395.4331s\n",
      "\titers: 400, epoch: 4 | loss: 0.0361682\n",
      "\tspeed: 0.1251s/iter; left time: 5706.8554s\n",
      "\titers: 500, epoch: 4 | loss: 0.0227277\n",
      "\tspeed: 0.1187s/iter; left time: 5402.3643s\n",
      "\titers: 600, epoch: 4 | loss: 0.0195750\n",
      "\tspeed: 0.1200s/iter; left time: 5448.5430s\n",
      "Epoch: 4 cost time: 00h:01m:22.58s\n",
      "Epoch: 4 | Train Loss: 0.0242923 Vali Loss: 0.0177414 Test Loss: 0.0192689\n",
      "Validation loss decreased (0.018275 --> 0.017741).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0244805\n",
      "\tspeed: 0.4993s/iter; left time: 21567.4749s\n",
      "\titers: 200, epoch: 5 | loss: 0.0192584\n",
      "\tspeed: 0.1186s/iter; left time: 5109.9401s\n",
      "\titers: 300, epoch: 5 | loss: 0.0236494\n",
      "\tspeed: 0.1199s/iter; left time: 5157.2703s\n",
      "\titers: 400, epoch: 5 | loss: 0.0253007\n",
      "\tspeed: 0.1219s/iter; left time: 5230.7889s\n",
      "\titers: 500, epoch: 5 | loss: 0.0333484\n",
      "\tspeed: 0.1171s/iter; left time: 5012.2101s\n",
      "\titers: 600, epoch: 5 | loss: 0.0208841\n",
      "\tspeed: 0.1211s/iter; left time: 5171.9480s\n",
      "Epoch: 5 cost time: 00h:01m:21.83s\n",
      "Epoch: 5 | Train Loss: 0.0233276 Vali Loss: 0.0176504 Test Loss: 0.0191216\n",
      "Validation loss decreased (0.017741 --> 0.017650).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0227093\n",
      "\tspeed: 0.4187s/iter; left time: 16954.2441s\n",
      "\titers: 200, epoch: 6 | loss: 0.0254766\n",
      "\tspeed: 0.1250s/iter; left time: 5048.4486s\n",
      "\titers: 300, epoch: 6 | loss: 0.0200936\n",
      "\tspeed: 0.1291s/iter; left time: 5200.1711s\n",
      "\titers: 400, epoch: 6 | loss: 0.0175824\n",
      "\tspeed: 0.1286s/iter; left time: 5170.3003s\n",
      "\titers: 500, epoch: 6 | loss: 0.0142362\n",
      "\tspeed: 0.1200s/iter; left time: 4810.0125s\n",
      "\titers: 600, epoch: 6 | loss: 0.0185892\n",
      "\tspeed: 0.1205s/iter; left time: 4818.6293s\n",
      "Epoch: 6 cost time: 00h:01m:24.31s\n",
      "Epoch: 6 | Train Loss: 0.0230509 Vali Loss: 0.0173448 Test Loss: 0.0188160\n",
      "Validation loss decreased (0.017650 --> 0.017345).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0263479\n",
      "\tspeed: 0.4576s/iter; left time: 17290.5216s\n",
      "\titers: 200, epoch: 7 | loss: 0.0225599\n",
      "\tspeed: 0.1180s/iter; left time: 4446.4617s\n",
      "\titers: 300, epoch: 7 | loss: 0.0173663\n",
      "\tspeed: 0.1153s/iter; left time: 4331.9419s\n",
      "\titers: 400, epoch: 7 | loss: 0.0211402\n",
      "\tspeed: 0.1182s/iter; left time: 4429.8356s\n",
      "\titers: 500, epoch: 7 | loss: 0.0260078\n",
      "\tspeed: 0.1189s/iter; left time: 4446.4647s\n",
      "\titers: 600, epoch: 7 | loss: 0.0225444\n",
      "\tspeed: 0.1210s/iter; left time: 4512.1696s\n",
      "Epoch: 7 cost time: 00h:01m:21.13s\n",
      "Epoch: 7 | Train Loss: 0.0225729 Vali Loss: 0.0170060 Test Loss: 0.0185562\n",
      "Validation loss decreased (0.017345 --> 0.017006).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0153552\n",
      "\tspeed: 0.4092s/iter; left time: 14354.4146s\n",
      "\titers: 200, epoch: 8 | loss: 0.0211704\n",
      "\tspeed: 0.1724s/iter; left time: 6029.4184s\n",
      "\titers: 300, epoch: 8 | loss: 0.0244371\n",
      "\tspeed: 0.2222s/iter; left time: 7748.8142s\n",
      "\titers: 400, epoch: 8 | loss: 0.0261077\n",
      "\tspeed: 0.2157s/iter; left time: 7501.7075s\n",
      "\titers: 500, epoch: 8 | loss: 0.0245105\n",
      "\tspeed: 0.2178s/iter; left time: 7553.2370s\n",
      "\titers: 600, epoch: 8 | loss: 0.0276707\n",
      "\tspeed: 0.2154s/iter; left time: 7448.6495s\n",
      "Epoch: 8 cost time: 00h:02m:14.06s\n",
      "Epoch: 8 | Train Loss: 0.0222899 Vali Loss: 0.0168095 Test Loss: 0.0182955\n",
      "Validation loss decreased (0.017006 --> 0.016809).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0243195\n",
      "\tspeed: 0.7627s/iter; left time: 24691.0131s\n",
      "\titers: 200, epoch: 9 | loss: 0.0280231\n",
      "\tspeed: 0.2126s/iter; left time: 6862.3340s\n",
      "\titers: 300, epoch: 9 | loss: 0.0226743\n",
      "\tspeed: 0.2131s/iter; left time: 6857.4477s\n",
      "\titers: 400, epoch: 9 | loss: 0.0140769\n",
      "\tspeed: 0.2189s/iter; left time: 7022.1264s\n",
      "\titers: 500, epoch: 9 | loss: 0.0240373\n",
      "\tspeed: 0.2192s/iter; left time: 7009.5796s\n",
      "\titers: 600, epoch: 9 | loss: 0.0273113\n",
      "\tspeed: 0.2219s/iter; left time: 7073.5384s\n",
      "Epoch: 9 cost time: 00h:02m:27.67s\n",
      "Epoch: 9 | Train Loss: 0.0220465 Vali Loss: 0.0168548 Test Loss: 0.0183864\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0290507\n",
      "\tspeed: 0.6796s/iter; left time: 20160.5646s\n",
      "\titers: 200, epoch: 10 | loss: 0.0204478\n",
      "\tspeed: 0.2147s/iter; left time: 6347.7927s\n",
      "\titers: 300, epoch: 10 | loss: 0.0142628\n",
      "\tspeed: 0.2125s/iter; left time: 6262.7020s\n",
      "\titers: 400, epoch: 10 | loss: 0.0201488\n",
      "\tspeed: 0.2188s/iter; left time: 6426.1579s\n",
      "\titers: 500, epoch: 10 | loss: 0.0202513\n",
      "\tspeed: 0.2186s/iter; left time: 6398.3859s\n",
      "\titers: 600, epoch: 10 | loss: 0.0239269\n",
      "\tspeed: 0.2192s/iter; left time: 6394.0182s\n",
      "Epoch: 10 cost time: 00h:02m:26.21s\n",
      "Epoch: 10 | Train Loss: 0.0217414 Vali Loss: 0.0165702 Test Loss: 0.0181968\n",
      "Validation loss decreased (0.016809 --> 0.016570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0212407\n",
      "\tspeed: 0.6567s/iter; left time: 17706.0212s\n",
      "\titers: 200, epoch: 11 | loss: 0.0194598\n",
      "\tspeed: 0.2097s/iter; left time: 5633.4177s\n",
      "\titers: 300, epoch: 11 | loss: 0.0158257\n",
      "\tspeed: 0.2173s/iter; left time: 5815.0829s\n",
      "\titers: 400, epoch: 11 | loss: 0.0238603\n",
      "\tspeed: 0.2041s/iter; left time: 5440.3537s\n",
      "\titers: 500, epoch: 11 | loss: 0.0193602\n",
      "\tspeed: 0.2202s/iter; left time: 5847.5737s\n",
      "\titers: 600, epoch: 11 | loss: 0.0151740\n",
      "\tspeed: 0.2187s/iter; left time: 5786.3479s\n",
      "Epoch: 11 cost time: 00h:02m:26.18s\n",
      "Epoch: 11 | Train Loss: 0.0212928 Vali Loss: 0.0166454 Test Loss: 0.0182453\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0265997\n",
      "\tspeed: 0.6909s/iter; left time: 16757.9608s\n",
      "\titers: 200, epoch: 12 | loss: 0.0186985\n",
      "\tspeed: 0.2091s/iter; left time: 5051.7166s\n",
      "\titers: 300, epoch: 12 | loss: 0.0248060\n",
      "\tspeed: 0.2113s/iter; left time: 5082.9155s\n",
      "\titers: 400, epoch: 12 | loss: 0.0258594\n",
      "\tspeed: 0.2168s/iter; left time: 5192.7651s\n",
      "\titers: 500, epoch: 12 | loss: 0.0200538\n",
      "\tspeed: 0.2043s/iter; left time: 4874.1620s\n",
      "\titers: 600, epoch: 12 | loss: 0.0228923\n",
      "\tspeed: 0.2180s/iter; left time: 5179.5704s\n",
      "Epoch: 12 cost time: 00h:02m:25.85s\n",
      "Epoch: 12 | Train Loss: 0.0211033 Vali Loss: 0.0167095 Test Loss: 0.0181806\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0230677\n",
      "\tspeed: 0.6708s/iter; left time: 14454.3561s\n",
      "\titers: 200, epoch: 13 | loss: 0.0197223\n",
      "\tspeed: 0.2213s/iter; left time: 4747.2691s\n",
      "\titers: 300, epoch: 13 | loss: 0.0230520\n",
      "\tspeed: 0.1514s/iter; left time: 3231.8059s\n",
      "\titers: 400, epoch: 13 | loss: 0.0240007\n",
      "\tspeed: 0.2040s/iter; left time: 4335.0122s\n",
      "\titers: 500, epoch: 13 | loss: 0.0168106\n",
      "\tspeed: 0.2147s/iter; left time: 4540.0864s\n",
      "\titers: 600, epoch: 13 | loss: 0.0218856\n",
      "\tspeed: 0.2101s/iter; left time: 4421.9463s\n",
      "Epoch: 13 cost time: 00h:02m:16.74s\n",
      "Epoch: 13 | Train Loss: 0.0207625 Vali Loss: 0.0165105 Test Loss: 0.0183249\n",
      "Validation loss decreased (0.016570 --> 0.016510).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0177562\n",
      "\tspeed: 0.7061s/iter; left time: 13305.5459s\n",
      "\titers: 200, epoch: 14 | loss: 0.0215960\n",
      "\tspeed: 0.2118s/iter; left time: 3969.7001s\n",
      "\titers: 300, epoch: 14 | loss: 0.0218200\n",
      "\tspeed: 0.2135s/iter; left time: 3979.7274s\n",
      "\titers: 400, epoch: 14 | loss: 0.0114039\n",
      "\tspeed: 0.2121s/iter; left time: 3933.2258s\n",
      "\titers: 500, epoch: 14 | loss: 0.0204489\n",
      "\tspeed: 0.2079s/iter; left time: 3833.5743s\n",
      "\titers: 600, epoch: 14 | loss: 0.0184212\n",
      "\tspeed: 0.2134s/iter; left time: 3913.6305s\n",
      "Epoch: 14 cost time: 00h:02m:23.16s\n",
      "Epoch: 14 | Train Loss: 0.0207159 Vali Loss: 0.0182826 Test Loss: 0.0195461\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0221164\n",
      "\tspeed: 0.6640s/iter; left time: 10714.8964s\n",
      "\titers: 200, epoch: 15 | loss: 0.0162195\n",
      "\tspeed: 0.2127s/iter; left time: 3410.8746s\n",
      "\titers: 300, epoch: 15 | loss: 0.0149570\n",
      "\tspeed: 0.2117s/iter; left time: 3373.5503s\n",
      "\titers: 400, epoch: 15 | loss: 0.0151238\n",
      "\tspeed: 0.2130s/iter; left time: 3373.4287s\n",
      "\titers: 500, epoch: 15 | loss: 0.0271093\n",
      "\tspeed: 0.1996s/iter; left time: 3140.7084s\n",
      "\titers: 600, epoch: 15 | loss: 0.0163410\n",
      "\tspeed: 0.2117s/iter; left time: 3310.1664s\n",
      "Epoch: 15 cost time: 00h:02m:24.32s\n",
      "Epoch: 15 | Train Loss: 0.0203983 Vali Loss: 0.0172118 Test Loss: 0.0190688\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0171180\n",
      "\tspeed: 0.6457s/iter; left time: 8672.5662s\n",
      "\titers: 200, epoch: 16 | loss: 0.0223030\n",
      "\tspeed: 0.2108s/iter; left time: 2809.8969s\n",
      "\titers: 300, epoch: 16 | loss: 0.0295624\n",
      "\tspeed: 0.2022s/iter; left time: 2675.2776s\n",
      "\titers: 400, epoch: 16 | loss: 0.0213049\n",
      "\tspeed: 0.2116s/iter; left time: 2778.1906s\n",
      "\titers: 500, epoch: 16 | loss: 0.0199282\n",
      "\tspeed: 0.2079s/iter; left time: 2709.1071s\n",
      "\titers: 600, epoch: 16 | loss: 0.0195881\n",
      "\tspeed: 0.2087s/iter; left time: 2699.0198s\n",
      "Epoch: 16 cost time: 00h:02m:20.05s\n",
      "Epoch: 16 | Train Loss: 0.0203052 Vali Loss: 0.0174881 Test Loss: 0.0194449\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0297713\n",
      "\tspeed: 0.7365s/iter; left time: 7898.8086s\n",
      "\titers: 200, epoch: 17 | loss: 0.0205002\n",
      "\tspeed: 0.2717s/iter; left time: 2886.5750s\n",
      "\titers: 300, epoch: 17 | loss: 0.0108258\n",
      "\tspeed: 0.2295s/iter; left time: 2415.2405s\n",
      "\titers: 400, epoch: 17 | loss: 0.0248199\n",
      "\tspeed: 0.2075s/iter; left time: 2163.3793s\n",
      "\titers: 500, epoch: 17 | loss: 0.0154806\n",
      "\tspeed: 0.2051s/iter; left time: 2117.7817s\n",
      "\titers: 600, epoch: 17 | loss: 0.0166328\n",
      "\tspeed: 0.1398s/iter; left time: 1429.2783s\n",
      "Epoch: 17 cost time: 00h:02m:28.40s\n",
      "Epoch: 17 | Train Loss: 0.0199837 Vali Loss: 0.0173948 Test Loss: 0.0190550\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0193233\n",
      "\tspeed: 0.7304s/iter; left time: 5856.9636s\n",
      "\titers: 200, epoch: 18 | loss: 0.0147264\n",
      "\tspeed: 0.2415s/iter; left time: 1912.2206s\n",
      "\titers: 300, epoch: 18 | loss: 0.0189813\n",
      "\tspeed: 0.2400s/iter; left time: 1876.4391s\n",
      "\titers: 400, epoch: 18 | loss: 0.0168320\n",
      "\tspeed: 0.2412s/iter; left time: 1861.7095s\n",
      "\titers: 500, epoch: 18 | loss: 0.0130358\n",
      "\tspeed: 0.2393s/iter; left time: 1823.5378s\n",
      "\titers: 600, epoch: 18 | loss: 0.0204194\n",
      "\tspeed: 0.2425s/iter; left time: 1823.0238s\n",
      "Epoch: 18 cost time: 00h:02m:42.58s\n",
      "Epoch: 18 | Train Loss: 0.0197306 Vali Loss: 0.0180781 Test Loss: 0.0192325\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355\n",
      "Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355\n",
      "Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355\n",
      "Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355\n",
      "Intermediate time for IT and pred_len 96: 00h:46m:40.98s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-13 17:39:17,127] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 17:39:17,127] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 17:39:17,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 17:39:17,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-13 17:39:18,128] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 17:39:18,128] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-13 17:39:18,129] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 17:39:18,129] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-13 17:39:18,130] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86403\n",
      "val 18219\n",
      "test 18219\n",
      "train 86403\n",
      "train 86403\n",
      "train 86403\n",
      "val 18219\n",
      "val 18219\n",
      "val 18219\n",
      "test 18219\n",
      "test 18219\n",
      "test 18219\n",
      "[2024-11-13 17:39:20,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-13 17:39:21,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-13 17:39:21,203] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-13 17:39:21,203] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-13 17:39:21,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-13 17:39:21,204] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-13 17:39:21,204] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-13 17:39:21,658] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-13 17:39:21,659] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-11-13 17:39:21,660] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.58 GB, percent = 3.7%\n",
      "[2024-11-13 17:39:21,850] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-13 17:39:21,850] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 17:39:21,851] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.75 GB, percent = 3.7%\n",
      "[2024-11-13 17:39:21,851] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-13 17:39:21,983] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-13 17:39:21,984] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-13 17:39:21,984] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.94 GB, percent = 3.8%\n",
      "[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-13 17:39:21,985] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7eff603e6490>\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-13 17:39:21,988] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0616591\n",
      "\tspeed: 0.2700s/iter; left time: 14553.1081s\n",
      "\titers: 200, epoch: 1 | loss: 0.0595522\n",
      "\tspeed: 0.2382s/iter; left time: 12813.9867s\n",
      "\titers: 300, epoch: 1 | loss: 0.0564701\n",
      "\tspeed: 0.2408s/iter; left time: 12931.2314s\n",
      "\titers: 400, epoch: 1 | loss: 0.0572593\n",
      "\tspeed: 0.1235s/iter; left time: 6617.4790s\n",
      "\titers: 500, epoch: 1 | loss: 0.0395582\n",
      "\tspeed: 0.1277s/iter; left time: 6830.5201s\n",
      "\titers: 600, epoch: 1 | loss: 0.0293994\n",
      "\tspeed: 0.1255s/iter; left time: 6699.8988s\n",
      "Epoch: 1 cost time: 00h:02m:00.18s\n",
      "Epoch: 1 | Train Loss: 0.0605154 Vali Loss: 0.0258984 Test Loss: 0.0277663\n",
      "Validation loss decreased (inf --> 0.025898).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0331507\n",
      "\tspeed: 0.5792s/iter; left time: 29655.7831s\n",
      "\titers: 200, epoch: 2 | loss: 0.0272233\n",
      "\tspeed: 0.1267s/iter; left time: 6476.2187s\n",
      "\titers: 300, epoch: 2 | loss: 0.0330929\n",
      "\tspeed: 0.1267s/iter; left time: 6459.7128s\n",
      "\titers: 400, epoch: 2 | loss: 0.0377437\n",
      "\tspeed: 0.1238s/iter; left time: 6300.5302s\n",
      "\titers: 500, epoch: 2 | loss: 0.0229581\n",
      "\tspeed: 0.1251s/iter; left time: 6353.2610s\n",
      "\titers: 600, epoch: 2 | loss: 0.0337943\n",
      "\tspeed: 0.1267s/iter; left time: 6423.8079s\n",
      "Epoch: 2 cost time: 00h:01m:25.70s\n",
      "Epoch: 2 | Train Loss: 0.0292056 Vali Loss: 0.0210110 Test Loss: 0.0223801\n",
      "Validation loss decreased (0.025898 --> 0.021011).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0387124\n",
      "\tspeed: 0.4440s/iter; left time: 21536.7097s\n",
      "\titers: 200, epoch: 3 | loss: 0.0326666\n",
      "\tspeed: 0.1246s/iter; left time: 6032.1983s\n",
      "\titers: 300, epoch: 3 | loss: 0.0301178\n",
      "\tspeed: 0.1256s/iter; left time: 6065.3144s\n",
      "\titers: 400, epoch: 3 | loss: 0.0209178\n",
      "\tspeed: 0.1285s/iter; left time: 6194.7146s\n",
      "\titers: 500, epoch: 3 | loss: 0.0242972\n",
      "\tspeed: 0.2385s/iter; left time: 11474.1226s\n",
      "\titers: 600, epoch: 3 | loss: 0.0246501\n",
      "\tspeed: 0.2455s/iter; left time: 11784.9761s\n",
      "Epoch: 3 cost time: 00h:01m:55.53s\n",
      "Epoch: 3 | Train Loss: 0.0270320 Vali Loss: 0.0202293 Test Loss: 0.0216505\n",
      "Validation loss decreased (0.021011 --> 0.020229).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0288879\n",
      "\tspeed: 0.5225s/iter; left time: 23931.2974s\n",
      "\titers: 200, epoch: 4 | loss: 0.0373396\n",
      "\tspeed: 0.1209s/iter; left time: 5523.5512s\n",
      "\titers: 300, epoch: 4 | loss: 0.0252346\n",
      "\tspeed: 0.1254s/iter; left time: 5720.4639s\n",
      "\titers: 400, epoch: 4 | loss: 0.0202597\n",
      "\tspeed: 0.1216s/iter; left time: 5532.5119s\n",
      "\titers: 500, epoch: 4 | loss: 0.0241824\n",
      "\tspeed: 0.1230s/iter; left time: 5584.1586s\n",
      "\titers: 600, epoch: 4 | loss: 0.0204294\n",
      "\tspeed: 0.1244s/iter; left time: 5635.1281s\n",
      "Epoch: 4 cost time: 00h:01m:23.79s\n",
      "Epoch: 4 | Train Loss: 0.0258587 Vali Loss: 0.0196277 Test Loss: 0.0208590\n",
      "Validation loss decreased (0.020229 --> 0.019628).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0271386\n",
      "\tspeed: 0.4502s/iter; left time: 19404.8022s\n",
      "\titers: 200, epoch: 5 | loss: 0.0153049\n",
      "\tspeed: 0.1199s/iter; left time: 5155.5849s\n",
      "\titers: 300, epoch: 5 | loss: 0.0206532\n",
      "\tspeed: 0.1258s/iter; left time: 5398.8703s\n",
      "\titers: 400, epoch: 5 | loss: 0.0219328\n",
      "\tspeed: 0.1224s/iter; left time: 5237.7481s\n",
      "\titers: 500, epoch: 5 | loss: 0.0251162\n",
      "\tspeed: 0.1284s/iter; left time: 5482.3793s\n",
      "\titers: 600, epoch: 5 | loss: 0.0258013\n",
      "\tspeed: 0.1208s/iter; left time: 5144.9730s\n",
      "Epoch: 5 cost time: 00h:01m:24.07s\n",
      "Epoch: 5 | Train Loss: 0.0249864 Vali Loss: 0.0190997 Test Loss: 0.0204004\n",
      "Validation loss decreased (0.019628 --> 0.019100).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0198722\n",
      "\tspeed: 0.4405s/iter; left time: 17797.1756s\n",
      "\titers: 200, epoch: 6 | loss: 0.0364575\n",
      "\tspeed: 0.1207s/iter; left time: 4865.7212s\n",
      "\titers: 300, epoch: 6 | loss: 0.0198864\n",
      "\tspeed: 0.1240s/iter; left time: 4985.6761s\n",
      "\titers: 400, epoch: 6 | loss: 0.0321605\n",
      "\tspeed: 0.1205s/iter; left time: 4831.8809s\n",
      "\titers: 500, epoch: 6 | loss: 0.0232292\n",
      "\tspeed: 0.1178s/iter; left time: 4711.0001s\n",
      "\titers: 600, epoch: 6 | loss: 0.0303167\n",
      "\tspeed: 0.1210s/iter; left time: 4827.4800s\n",
      "Epoch: 6 cost time: 00h:01m:22.71s\n",
      "Epoch: 6 | Train Loss: 0.0245004 Vali Loss: 0.0188114 Test Loss: 0.0201435\n",
      "Validation loss decreased (0.019100 --> 0.018811).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0226749\n",
      "\tspeed: 0.4422s/iter; left time: 16670.1691s\n",
      "\titers: 200, epoch: 7 | loss: 0.0304455\n",
      "\tspeed: 0.1325s/iter; left time: 4980.8824s\n",
      "\titers: 300, epoch: 7 | loss: 0.0270933\n",
      "\tspeed: 0.2501s/iter; left time: 9379.7193s\n",
      "\titers: 400, epoch: 7 | loss: 0.0280105\n",
      "\tspeed: 0.2466s/iter; left time: 9223.4111s\n",
      "\titers: 500, epoch: 7 | loss: 0.0284047\n",
      "\tspeed: 0.2431s/iter; left time: 9066.0577s\n",
      "\titers: 600, epoch: 7 | loss: 0.0226332\n",
      "\tspeed: 0.2450s/iter; left time: 9113.1425s\n",
      "Epoch: 7 cost time: 00h:02m:23.13s\n",
      "Epoch: 7 | Train Loss: 0.0242849 Vali Loss: 0.0186533 Test Loss: 0.0199403\n",
      "Validation loss decreased (0.018811 --> 0.018653).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0290103\n",
      "\tspeed: 0.8248s/iter; left time: 28869.1663s\n",
      "\titers: 200, epoch: 8 | loss: 0.0232801\n",
      "\tspeed: 0.2372s/iter; left time: 8279.5518s\n",
      "\titers: 300, epoch: 8 | loss: 0.0265249\n",
      "\tspeed: 0.2419s/iter; left time: 8417.1788s\n",
      "\titers: 400, epoch: 8 | loss: 0.0232811\n",
      "\tspeed: 0.2404s/iter; left time: 8343.2988s\n",
      "\titers: 500, epoch: 8 | loss: 0.0170798\n",
      "\tspeed: 0.2440s/iter; left time: 8443.1287s\n",
      "\titers: 600, epoch: 8 | loss: 0.0341334\n",
      "\tspeed: 0.2494s/iter; left time: 8605.0228s\n",
      "Epoch: 8 cost time: 00h:02m:45.39s\n",
      "Epoch: 8 | Train Loss: 0.0239119 Vali Loss: 0.0185142 Test Loss: 0.0197774\n",
      "Validation loss decreased (0.018653 --> 0.018514).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0226050\n",
      "\tspeed: 0.6948s/iter; left time: 22443.8715s\n",
      "\titers: 200, epoch: 9 | loss: 0.0330355\n",
      "\tspeed: 0.1218s/iter; left time: 3922.8665s\n",
      "\titers: 300, epoch: 9 | loss: 0.0210071\n",
      "\tspeed: 0.1257s/iter; left time: 4036.4820s\n",
      "\titers: 400, epoch: 9 | loss: 0.0231865\n",
      "\tspeed: 0.1210s/iter; left time: 3873.0008s\n",
      "\titers: 500, epoch: 9 | loss: 0.0199373\n",
      "\tspeed: 0.1208s/iter; left time: 3854.8738s\n",
      "\titers: 600, epoch: 9 | loss: 0.0160295\n",
      "\tspeed: 0.1241s/iter; left time: 3947.7961s\n",
      "Epoch: 9 cost time: 00h:01m:23.65s\n",
      "Epoch: 9 | Train Loss: 0.0235146 Vali Loss: 0.0181856 Test Loss: 0.0194546\n",
      "Validation loss decreased (0.018514 --> 0.018186).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0233166\n",
      "\tspeed: 0.4351s/iter; left time: 12879.2191s\n",
      "\titers: 200, epoch: 10 | loss: 0.0208000\n",
      "\tspeed: 0.1219s/iter; left time: 3597.2194s\n",
      "\titers: 300, epoch: 10 | loss: 0.0231443\n",
      "\tspeed: 0.1233s/iter; left time: 3625.8678s\n",
      "\titers: 400, epoch: 10 | loss: 0.0267396\n",
      "\tspeed: 0.1830s/iter; left time: 5361.3541s\n",
      "\titers: 500, epoch: 10 | loss: 0.0223235\n",
      "\tspeed: 0.1707s/iter; left time: 4984.7580s\n",
      "\titers: 600, epoch: 10 | loss: 0.0260408\n",
      "\tspeed: 0.1262s/iter; left time: 3673.8941s\n",
      "Epoch: 10 cost time: 00h:01m:34.58s\n",
      "Epoch: 10 | Train Loss: 0.0231291 Vali Loss: 0.0183109 Test Loss: 0.0195542\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0200581\n",
      "\tspeed: 0.3753s/iter; left time: 10094.7683s\n",
      "\titers: 200, epoch: 11 | loss: 0.0248631\n",
      "\tspeed: 0.1192s/iter; left time: 3194.5035s\n",
      "\titers: 300, epoch: 11 | loss: 0.0240047\n",
      "\tspeed: 0.1242s/iter; left time: 3317.1796s\n",
      "\titers: 400, epoch: 11 | loss: 0.0290795\n",
      "\tspeed: 0.1195s/iter; left time: 3178.4241s\n",
      "\titers: 500, epoch: 11 | loss: 0.0240012\n",
      "\tspeed: 0.1180s/iter; left time: 3128.3767s\n",
      "\titers: 600, epoch: 11 | loss: 0.0178534\n",
      "\tspeed: 0.1214s/iter; left time: 3204.8893s\n",
      "Epoch: 11 cost time: 00h:01m:21.85s\n",
      "Epoch: 11 | Train Loss: 0.0227248 Vali Loss: 0.0180482 Test Loss: 0.0191756\n",
      "Validation loss decreased (0.018186 --> 0.018048).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0224432\n",
      "\tspeed: 0.4454s/iter; left time: 10780.3132s\n",
      "\titers: 200, epoch: 12 | loss: 0.0273953\n",
      "\tspeed: 0.1234s/iter; left time: 2975.1092s\n",
      "\titers: 300, epoch: 12 | loss: 0.0173645\n",
      "\tspeed: 0.1205s/iter; left time: 2892.8861s\n",
      "\titers: 400, epoch: 12 | loss: 0.0313485\n",
      "\tspeed: 0.1225s/iter; left time: 2929.0562s\n",
      "\titers: 500, epoch: 12 | loss: 0.0260381\n",
      "\tspeed: 0.1227s/iter; left time: 2920.5502s\n",
      "\titers: 600, epoch: 12 | loss: 0.0249483\n",
      "\tspeed: 0.1218s/iter; left time: 2886.9367s\n",
      "Epoch: 12 cost time: 00h:01m:23.00s\n",
      "Epoch: 12 | Train Loss: 0.0226154 Vali Loss: 0.0181090 Test Loss: 0.0194397\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0240247\n",
      "\tspeed: 0.3829s/iter; left time: 8232.4275s\n",
      "\titers: 200, epoch: 13 | loss: 0.0206983\n",
      "\tspeed: 0.1205s/iter; left time: 2578.1292s\n",
      "\titers: 300, epoch: 13 | loss: 0.0213481\n",
      "\tspeed: 0.1215s/iter; left time: 2587.3213s\n",
      "\titers: 400, epoch: 13 | loss: 0.0253885\n",
      "\tspeed: 0.1225s/iter; left time: 2596.3894s\n",
      "\titers: 500, epoch: 13 | loss: 0.0272860\n",
      "\tspeed: 0.1210s/iter; left time: 2552.6524s\n",
      "\titers: 600, epoch: 13 | loss: 0.0219219\n",
      "\tspeed: 0.1207s/iter; left time: 2535.0586s\n",
      "Epoch: 13 cost time: 00h:01m:23.19s\n",
      "Epoch: 13 | Train Loss: 0.0222798 Vali Loss: 0.0179139 Test Loss: 0.0195298\n",
      "Validation loss decreased (0.018048 --> 0.017914).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0212308\n",
      "\tspeed: 0.5432s/iter; left time: 10212.0143s\n",
      "\titers: 200, epoch: 14 | loss: 0.0247572\n",
      "\tspeed: 0.1216s/iter; left time: 2273.5017s\n",
      "\titers: 300, epoch: 14 | loss: 0.0236004\n",
      "\tspeed: 0.1241s/iter; left time: 2309.2169s\n",
      "\titers: 400, epoch: 14 | loss: 0.0279050\n",
      "\tspeed: 0.1209s/iter; left time: 2236.0902s\n",
      "\titers: 500, epoch: 14 | loss: 0.0250699\n",
      "\tspeed: 0.1224s/iter; left time: 2252.2742s\n",
      "\titers: 600, epoch: 14 | loss: 0.0282776\n",
      "\tspeed: 0.1198s/iter; left time: 2192.7360s\n",
      "Epoch: 14 cost time: 00h:01m:22.78s\n",
      "Epoch: 14 | Train Loss: 0.0223627 Vali Loss: 0.0181161 Test Loss: 0.0196190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0230317\n",
      "\tspeed: 0.4702s/iter; left time: 7570.3239s\n",
      "\titers: 200, epoch: 15 | loss: 0.0248076\n",
      "\tspeed: 0.1533s/iter; left time: 2452.9333s\n",
      "\titers: 300, epoch: 15 | loss: 0.0263772\n",
      "\tspeed: 0.1542s/iter; left time: 2452.1800s\n",
      "\titers: 400, epoch: 15 | loss: 0.0261949\n",
      "\tspeed: 0.1520s/iter; left time: 2401.2823s\n",
      "\titers: 500, epoch: 15 | loss: 0.0259247\n",
      "\tspeed: 0.1575s/iter; left time: 2473.2455s\n",
      "\titers: 600, epoch: 15 | loss: 0.0233346\n",
      "\tspeed: 0.1540s/iter; left time: 2402.2484s\n",
      "Epoch: 15 cost time: 00h:01m:45.36s\n",
      "Epoch: 15 | Train Loss: 0.0219749 Vali Loss: 0.0182892 Test Loss: 0.0197237\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0221001\n",
      "\tspeed: 0.4765s/iter; left time: 6385.1486s\n",
      "\titers: 200, epoch: 16 | loss: 0.0218049\n",
      "\tspeed: 0.1535s/iter; left time: 2042.3210s\n",
      "\titers: 300, epoch: 16 | loss: 0.0204753\n",
      "\tspeed: 0.1509s/iter; left time: 1992.1280s\n",
      "\titers: 400, epoch: 16 | loss: 0.0198632\n",
      "\tspeed: 0.1507s/iter; left time: 1974.8548s\n",
      "\titers: 500, epoch: 16 | loss: 0.0186507\n",
      "\tspeed: 0.1335s/iter; left time: 1735.6644s\n",
      "\titers: 600, epoch: 16 | loss: 0.0270961\n",
      "\tspeed: 0.1318s/iter; left time: 1699.9723s\n",
      "Epoch: 16 cost time: 00h:01m:37.97s\n",
      "Epoch: 16 | Train Loss: 0.0218581 Vali Loss: 0.0179782 Test Loss: 0.0199283\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0198847\n",
      "\tspeed: 0.3962s/iter; left time: 4239.3411s\n",
      "\titers: 200, epoch: 17 | loss: 0.0139290\n",
      "\tspeed: 0.1193s/iter; left time: 1264.2036s\n",
      "\titers: 300, epoch: 17 | loss: 0.0195014\n",
      "\tspeed: 0.1244s/iter; left time: 1306.5843s\n",
      "\titers: 400, epoch: 17 | loss: 0.0257014\n",
      "\tspeed: 0.1209s/iter; left time: 1257.9345s\n",
      "\titers: 500, epoch: 17 | loss: 0.0204623\n",
      "\tspeed: 0.1240s/iter; left time: 1277.8212s\n",
      "\titers: 600, epoch: 17 | loss: 0.0272740\n",
      "\tspeed: 0.1222s/iter; left time: 1246.8299s\n",
      "Epoch: 17 cost time: 00h:01m:23.40s\n",
      "Epoch: 17 | Train Loss: 0.0217688 Vali Loss: 0.0184210 Test Loss: 0.0201249\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0292499\n",
      "\tspeed: 0.4253s/iter; left time: 3402.6861s\n",
      "\titers: 200, epoch: 18 | loss: 0.0208086\n",
      "\tspeed: 0.1403s/iter; left time: 1108.1858s\n",
      "\titers: 300, epoch: 18 | loss: 0.0191685\n",
      "\tspeed: 0.1488s/iter; left time: 1160.8442s\n",
      "\titers: 400, epoch: 18 | loss: 0.0194677\n",
      "\tspeed: 0.1688s/iter; left time: 1300.1497s\n",
      "\titers: 500, epoch: 18 | loss: 0.0234823\n",
      "\tspeed: 0.1891s/iter; left time: 1437.6629s\n",
      "\titers: 600, epoch: 18 | loss: 0.0240304\n",
      "\tspeed: 0.1357s/iter; left time: 1018.2579s\n",
      "Epoch: 18 cost time: 00h:01m:43.71s\n",
      "Epoch: 18 | Train Loss: 0.0211778 Vali Loss: 0.0183267 Test Loss: 0.0202651\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603\n",
      "Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603\n",
      "Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603\n",
      "Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603\n",
      "Intermediate time for IT and pred_len 168: 00h:37m:21.16s\n",
      "\n",
      "Intermediate time for IT: 02h:01m:08.80s\n",
      "\n",
      "Total time: 08h:27m:12.89s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For single GPU use following command:\n",
    "# python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "            \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "            \n",
    "            if country == \"FR\" and pred_len == 24:\n",
    "                train_epochs = 30\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --multi_gpu --num_processes=4 --num_machines 1 --dynamo_backend \"no\"  --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience {patience} \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>0.0962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>0.1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>0.1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.0959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.0857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.0847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0215  0.1468  0.0962\n",
       "        96        0.0347  0.1861  0.1281\n",
       "        168       0.0375  0.1936  0.1386\n",
       "GB      24        0.0260  0.1611  0.1074\n",
       "        96        0.0407  0.2017  0.1428\n",
       "        168       0.0435  0.2085  0.1494\n",
       "ES      24        0.0109  0.1044  0.0666\n",
       "        96        0.0201  0.1417  0.0959\n",
       "        168       0.0219  0.1480  0.1005\n",
       "FR      24        0.0104  0.1019  0.0594\n",
       "        96        0.0197  0.1402  0.0857\n",
       "        168       0.0213  0.1458  0.0912\n",
       "IT      24        0.0105  0.1022  0.0618\n",
       "        96        0.0183  0.1354  0.0847\n",
       "        168       0.0195  0.1397  0.0898"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm.csv'))\n",
    "timellm_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
