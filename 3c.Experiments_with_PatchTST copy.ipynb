{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. No RevIN](#1-no-revin-instanse-normalization)\n",
    "- [2. No channel-independence (Channel-Mixing)](#2-no-channel-independence-channel-mixing)\n",
    "- [3. No Patching](#3-no-patching)\n",
    "- [4. Time series decomposition](#4-ts-decomposition)\n",
    "\n",
    "Ablation study on PatchTST components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['ES']\n",
    "num_cols = [3]\n",
    "seq_lens = [336]\n",
    "\n",
    "model = \"PatchTST\"\n",
    "loss = \"MSE\"\n",
    "itr=1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_revin.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0222541\n",
      "\tspeed: 0.0692s/iter; left time: 1543.7454s\n",
      "\titers: 200, epoch: 1 | loss: 0.0173791\n",
      "\tspeed: 0.0405s/iter; left time: 899.8214s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.83s\n",
      "Steps: 224 | Train Loss: 0.0242278 Vali Loss: 0.0146892 Test Loss: 0.0199106\n",
      "Validation loss decreased (inf --> 0.014689).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0093632\n",
      "\tspeed: 0.0764s/iter; left time: 1686.8610s\n",
      "\titers: 200, epoch: 2 | loss: 0.0095968\n",
      "\tspeed: 0.0405s/iter; left time: 890.3930s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:09.41s\n",
      "Steps: 224 | Train Loss: 0.0104013 Vali Loss: 0.0090444 Test Loss: 0.0117678\n",
      "Validation loss decreased (0.014689 --> 0.009044).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0082353\n",
      "\tspeed: 0.0753s/iter; left time: 1646.4720s\n",
      "\titers: 200, epoch: 3 | loss: 0.0082141\n",
      "\tspeed: 0.0409s/iter; left time: 890.6764s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.35s\n",
      "Steps: 224 | Train Loss: 0.0086646 Vali Loss: 0.0087289 Test Loss: 0.0114058\n",
      "Validation loss decreased (0.009044 --> 0.008729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0081308\n",
      "\tspeed: 0.0753s/iter; left time: 1629.0387s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084052\n",
      "\tspeed: 0.0407s/iter; left time: 876.2774s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.30s\n",
      "Steps: 224 | Train Loss: 0.0081697 Vali Loss: 0.0094098 Test Loss: 0.0121627\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0081944\n",
      "\tspeed: 0.0710s/iter; left time: 1520.4119s\n",
      "\titers: 200, epoch: 5 | loss: 0.0074900\n",
      "\tspeed: 0.0347s/iter; left time: 739.5252s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.15s\n",
      "Steps: 224 | Train Loss: 0.0075967 Vali Loss: 0.0089697 Test Loss: 0.0119432\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0067727\n",
      "\tspeed: 0.0651s/iter; left time: 1378.6244s\n",
      "\titers: 200, epoch: 6 | loss: 0.0068486\n",
      "\tspeed: 0.0347s/iter; left time: 731.6540s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.32s\n",
      "Steps: 224 | Train Loss: 0.0070614 Vali Loss: 0.0092476 Test Loss: 0.0119057\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0061740\n",
      "\tspeed: 0.0697s/iter; left time: 1460.6921s\n",
      "\titers: 200, epoch: 7 | loss: 0.0062932\n",
      "\tspeed: 0.0353s/iter; left time: 735.3777s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.24s\n",
      "Steps: 224 | Train Loss: 0.0064601 Vali Loss: 0.0094871 Test Loss: 0.0122043\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0064154\n",
      "\tspeed: 0.0633s/iter; left time: 1312.7713s\n",
      "\titers: 200, epoch: 8 | loss: 0.0066724\n",
      "\tspeed: 0.0258s/iter; left time: 531.8149s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 224 | Train Loss: 0.0058832 Vali Loss: 0.0096027 Test Loss: 0.0125173\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0051365\n",
      "\tspeed: 0.0723s/iter; left time: 1482.0197s\n",
      "\titers: 200, epoch: 9 | loss: 0.0048415\n",
      "\tspeed: 0.0412s/iter; left time: 840.9039s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:09.40s\n",
      "Steps: 224 | Train Loss: 0.0053815 Vali Loss: 0.0100015 Test Loss: 0.0128093\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049133\n",
      "\tspeed: 0.0741s/iter; left time: 1503.8411s\n",
      "\titers: 200, epoch: 10 | loss: 0.0047802\n",
      "\tspeed: 0.0411s/iter; left time: 829.0622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:09.43s\n",
      "Steps: 224 | Train Loss: 0.0049502 Vali Loss: 0.0102042 Test Loss: 0.0132874\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0047629\n",
      "\tspeed: 0.0743s/iter; left time: 1489.7320s\n",
      "\titers: 200, epoch: 11 | loss: 0.0041915\n",
      "\tspeed: 0.0412s/iter; left time: 822.1021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:09.45s\n",
      "Steps: 224 | Train Loss: 0.0046080 Vali Loss: 0.0105283 Test Loss: 0.0133615\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0044058\n",
      "\tspeed: 0.0754s/iter; left time: 1496.3528s\n",
      "\titers: 200, epoch: 12 | loss: 0.0043703\n",
      "\tspeed: 0.0411s/iter; left time: 811.8491s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:09.45s\n",
      "Steps: 224 | Train Loss: 0.0042848 Vali Loss: 0.0104694 Test Loss: 0.0136671\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0038280\n",
      "\tspeed: 0.0756s/iter; left time: 1483.5228s\n",
      "\titers: 200, epoch: 13 | loss: 0.0037476\n",
      "\tspeed: 0.0409s/iter; left time: 798.3350s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:09.46s\n",
      "Steps: 224 | Train Loss: 0.0040548 Vali Loss: 0.0109012 Test Loss: 0.0137471\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011405767872929573, rmse:0.10679779201745987, mae:0.06919430196285248, rse:0.3142929971218109\n",
      "Intermediate time for ES and pred_len 24: 00h:02m:34.08s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249662\n",
      "\tspeed: 0.0705s/iter; left time: 1572.1023s\n",
      "\titers: 200, epoch: 1 | loss: 0.0213045\n",
      "\tspeed: 0.0416s/iter; left time: 922.8767s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.92s\n",
      "Steps: 224 | Train Loss: 0.0275395 Vali Loss: 0.0199210 Test Loss: 0.0259587\n",
      "Validation loss decreased (inf --> 0.019921).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0154199\n",
      "\tspeed: 0.0788s/iter; left time: 1738.5796s\n",
      "\titers: 200, epoch: 2 | loss: 0.0147711\n",
      "\tspeed: 0.0418s/iter; left time: 918.1243s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:09.62s\n",
      "Steps: 224 | Train Loss: 0.0159194 Vali Loss: 0.0164074 Test Loss: 0.0206553\n",
      "Validation loss decreased (0.019921 --> 0.016407).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0128123\n",
      "\tspeed: 0.0781s/iter; left time: 1706.5496s\n",
      "\titers: 200, epoch: 3 | loss: 0.0120069\n",
      "\tspeed: 0.0418s/iter; left time: 908.2248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.58s\n",
      "Steps: 224 | Train Loss: 0.0133727 Vali Loss: 0.0177141 Test Loss: 0.0220083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110963\n",
      "\tspeed: 0.0765s/iter; left time: 1655.2324s\n",
      "\titers: 200, epoch: 4 | loss: 0.0101504\n",
      "\tspeed: 0.0421s/iter; left time: 905.4984s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.71s\n",
      "Steps: 224 | Train Loss: 0.0110040 Vali Loss: 0.0177026 Test Loss: 0.0225919\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0089270\n",
      "\tspeed: 0.0769s/iter; left time: 1646.7984s\n",
      "\titers: 200, epoch: 5 | loss: 0.0081671\n",
      "\tspeed: 0.0424s/iter; left time: 902.7354s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.70s\n",
      "Steps: 224 | Train Loss: 0.0087992 Vali Loss: 0.0184645 Test Loss: 0.0240935\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0072723\n",
      "\tspeed: 0.0777s/iter; left time: 1646.1968s\n",
      "\titers: 200, epoch: 6 | loss: 0.0064863\n",
      "\tspeed: 0.0418s/iter; left time: 881.8835s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.72s\n",
      "Steps: 224 | Train Loss: 0.0072933 Vali Loss: 0.0184985 Test Loss: 0.0245565\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064227\n",
      "\tspeed: 0.0771s/iter; left time: 1615.2366s\n",
      "\titers: 200, epoch: 7 | loss: 0.0061511\n",
      "\tspeed: 0.0422s/iter; left time: 879.3431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.70s\n",
      "Steps: 224 | Train Loss: 0.0063212 Vali Loss: 0.0185263 Test Loss: 0.0238103\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0058017\n",
      "\tspeed: 0.0774s/iter; left time: 1603.7068s\n",
      "\titers: 200, epoch: 8 | loss: 0.0057851\n",
      "\tspeed: 0.0383s/iter; left time: 790.0378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:09.18s\n",
      "Steps: 224 | Train Loss: 0.0056385 Vali Loss: 0.0192424 Test Loss: 0.0244339\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0049183\n",
      "\tspeed: 0.0733s/iter; left time: 1502.9099s\n",
      "\titers: 200, epoch: 9 | loss: 0.0051946\n",
      "\tspeed: 0.0357s/iter; left time: 727.6786s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.33s\n",
      "Steps: 224 | Train Loss: 0.0051166 Vali Loss: 0.0190300 Test Loss: 0.0246217\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049855\n",
      "\tspeed: 0.0695s/iter; left time: 1409.4918s\n",
      "\titers: 200, epoch: 10 | loss: 0.0045004\n",
      "\tspeed: 0.0354s/iter; left time: 714.1857s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.11s\n",
      "Steps: 224 | Train Loss: 0.0047357 Vali Loss: 0.0190064 Test Loss: 0.0247210\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0043923\n",
      "\tspeed: 0.0682s/iter; left time: 1367.9024s\n",
      "\titers: 200, epoch: 11 | loss: 0.0043126\n",
      "\tspeed: 0.0313s/iter; left time: 625.2021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.62s\n",
      "Steps: 224 | Train Loss: 0.0044053 Vali Loss: 0.0192821 Test Loss: 0.0248229\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0039878\n",
      "\tspeed: 0.0670s/iter; left time: 1328.4522s\n",
      "\titers: 200, epoch: 12 | loss: 0.0041882\n",
      "\tspeed: 0.0419s/iter; left time: 827.2692s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:09.84s\n",
      "Steps: 224 | Train Loss: 0.0041530 Vali Loss: 0.0190465 Test Loss: 0.0247059\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020655285567045212, rmse:0.14371946454048157, mae:0.0965302586555481, rse:0.42220455408096313\n",
      "Intermediate time for ES and pred_len 96: 00h:02m:28.51s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0244475\n",
      "\tspeed: 0.0692s/iter; left time: 1535.8019s\n",
      "\titers: 200, epoch: 1 | loss: 0.0229005\n",
      "\tspeed: 0.0415s/iter; left time: 917.4026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.86s\n",
      "Steps: 223 | Train Loss: 0.0284261 Vali Loss: 0.0214357 Test Loss: 0.0273612\n",
      "Validation loss decreased (inf --> 0.021436).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0168547\n",
      "\tspeed: 0.0789s/iter; left time: 1734.4176s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166410\n",
      "\tspeed: 0.0429s/iter; left time: 939.2233s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:09.68s\n",
      "Steps: 223 | Train Loss: 0.0171577 Vali Loss: 0.0180411 Test Loss: 0.0229071\n",
      "Validation loss decreased (0.021436 --> 0.018041).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0136818\n",
      "\tspeed: 0.0811s/iter; left time: 1764.4034s\n",
      "\titers: 200, epoch: 3 | loss: 0.0134842\n",
      "\tspeed: 0.0432s/iter; left time: 936.5068s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:09.75s\n",
      "Steps: 223 | Train Loss: 0.0141816 Vali Loss: 0.0183799 Test Loss: 0.0234680\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0115397\n",
      "\tspeed: 0.0771s/iter; left time: 1660.0036s\n",
      "\titers: 200, epoch: 4 | loss: 0.0105017\n",
      "\tspeed: 0.0430s/iter; left time: 921.4794s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:09.72s\n",
      "Steps: 223 | Train Loss: 0.0117780 Vali Loss: 0.0189678 Test Loss: 0.0249497\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0095834\n",
      "\tspeed: 0.0778s/iter; left time: 1656.7796s\n",
      "\titers: 200, epoch: 5 | loss: 0.0090101\n",
      "\tspeed: 0.0424s/iter; left time: 899.9767s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:09.75s\n",
      "Steps: 223 | Train Loss: 0.0094315 Vali Loss: 0.0192081 Test Loss: 0.0266327\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0077858\n",
      "\tspeed: 0.0779s/iter; left time: 1641.7413s\n",
      "\titers: 200, epoch: 6 | loss: 0.0073812\n",
      "\tspeed: 0.0433s/iter; left time: 908.7539s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:09.80s\n",
      "Steps: 223 | Train Loss: 0.0079274 Vali Loss: 0.0197801 Test Loss: 0.0270143\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0065276\n",
      "\tspeed: 0.0773s/iter; left time: 1611.8397s\n",
      "\titers: 200, epoch: 7 | loss: 0.0063776\n",
      "\tspeed: 0.0434s/iter; left time: 901.0493s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.74s\n",
      "Steps: 223 | Train Loss: 0.0069369 Vali Loss: 0.0198990 Test Loss: 0.0276234\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0062215\n",
      "\tspeed: 0.0771s/iter; left time: 1590.4080s\n",
      "\titers: 200, epoch: 8 | loss: 0.0059965\n",
      "\tspeed: 0.0426s/iter; left time: 875.0903s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:09.78s\n",
      "Steps: 223 | Train Loss: 0.0062075 Vali Loss: 0.0199308 Test Loss: 0.0275127\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0058798\n",
      "\tspeed: 0.0765s/iter; left time: 1562.7335s\n",
      "\titers: 200, epoch: 9 | loss: 0.0054274\n",
      "\tspeed: 0.0425s/iter; left time: 864.3827s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:09.64s\n",
      "Steps: 223 | Train Loss: 0.0056564 Vali Loss: 0.0199600 Test Loss: 0.0280890\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0053195\n",
      "\tspeed: 0.0775s/iter; left time: 1565.5544s\n",
      "\titers: 200, epoch: 10 | loss: 0.0049607\n",
      "\tspeed: 0.0432s/iter; left time: 867.8779s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:09.83s\n",
      "Steps: 223 | Train Loss: 0.0052265 Vali Loss: 0.0201242 Test Loss: 0.0278490\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0049777\n",
      "\tspeed: 0.0781s/iter; left time: 1560.6724s\n",
      "\titers: 200, epoch: 11 | loss: 0.0049066\n",
      "\tspeed: 0.0430s/iter; left time: 853.4661s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:09.72s\n",
      "Steps: 223 | Train Loss: 0.0048739 Vali Loss: 0.0201201 Test Loss: 0.0282332\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0045771\n",
      "\tspeed: 0.0735s/iter; left time: 1452.1810s\n",
      "\titers: 200, epoch: 12 | loss: 0.0044787\n",
      "\tspeed: 0.0364s/iter; left time: 715.1342s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.51s\n",
      "Steps: 223 | Train Loss: 0.0045925 Vali Loss: 0.0199313 Test Loss: 0.0279702\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02290712110698223, rmse:0.1513509899377823, mae:0.10324922204017639, rse:0.4446555972099304\n",
      "Intermediate time for ES and pred_len 168: 00h:02m:34.38s\n",
      "Intermediate time for ES: 00h:07m:36.97s\n",
      "Total time: 00h:07m:36.97s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_ES.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            model_id = f\"channel_mixing_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.0692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "ES      24        0.0114  0.1068  0.0692\n",
       "        96        0.0207  0.1437  0.0965\n",
       "        168       0.0229  0.1514  0.1032"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_es.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1235685\n",
      "\tspeed: 0.0584s/iter; left time: 1302.9294s\n",
      "\titers: 200, epoch: 1 | loss: 0.1166312\n",
      "\tspeed: 0.0319s/iter; left time: 708.7286s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.86s\n",
      "Steps: 224 | Train Loss: 0.1306442 Vali Loss: 0.0743393 Test Loss: 0.0910288\n",
      "Validation loss decreased (inf --> 0.074339).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0414139\n",
      "\tspeed: 0.0626s/iter; left time: 1382.7111s\n",
      "\titers: 200, epoch: 2 | loss: 0.0266411\n",
      "\tspeed: 0.0322s/iter; left time: 707.9793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.62s\n",
      "Steps: 224 | Train Loss: 0.0490632 Vali Loss: 0.0170418 Test Loss: 0.0199101\n",
      "Validation loss decreased (0.074339 --> 0.017042).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0221347\n",
      "\tspeed: 0.0719s/iter; left time: 1572.0768s\n",
      "\titers: 200, epoch: 3 | loss: 0.0209304\n",
      "\tspeed: 0.0346s/iter; left time: 753.1321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.15s\n",
      "Steps: 224 | Train Loss: 0.0225532 Vali Loss: 0.0141340 Test Loss: 0.0165208\n",
      "Validation loss decreased (0.017042 --> 0.014134).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0175069\n",
      "\tspeed: 0.0636s/iter; left time: 1375.9035s\n",
      "\titers: 200, epoch: 4 | loss: 0.0180933\n",
      "\tspeed: 0.0337s/iter; left time: 724.5114s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.58s\n",
      "Steps: 224 | Train Loss: 0.0189549 Vali Loss: 0.0126682 Test Loss: 0.0149519\n",
      "Validation loss decreased (0.014134 --> 0.012668).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0158055\n",
      "\tspeed: 0.0694s/iter; left time: 1484.5818s\n",
      "\titers: 200, epoch: 5 | loss: 0.0164286\n",
      "\tspeed: 0.0373s/iter; left time: 794.2028s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.52s\n",
      "Steps: 224 | Train Loss: 0.0169486 Vali Loss: 0.0124042 Test Loss: 0.0145784\n",
      "Validation loss decreased (0.012668 --> 0.012404).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0160598\n",
      "\tspeed: 0.0675s/iter; left time: 1428.8947s\n",
      "\titers: 200, epoch: 6 | loss: 0.0156461\n",
      "\tspeed: 0.0376s/iter; left time: 792.6010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.27s\n",
      "Steps: 224 | Train Loss: 0.0154126 Vali Loss: 0.0123237 Test Loss: 0.0142485\n",
      "Validation loss decreased (0.012404 --> 0.012324).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0163689\n",
      "\tspeed: 0.0701s/iter; left time: 1469.7544s\n",
      "\titers: 200, epoch: 7 | loss: 0.0149154\n",
      "\tspeed: 0.0338s/iter; left time: 704.0336s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.06s\n",
      "Steps: 224 | Train Loss: 0.0142636 Vali Loss: 0.0114231 Test Loss: 0.0135450\n",
      "Validation loss decreased (0.012324 --> 0.011423).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0132940\n",
      "\tspeed: 0.0650s/iter; left time: 1348.1211s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116993\n",
      "\tspeed: 0.0328s/iter; left time: 677.3795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 224 | Train Loss: 0.0130814 Vali Loss: 0.0109222 Test Loss: 0.0140578\n",
      "Validation loss decreased (0.011423 --> 0.010922).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0113352\n",
      "\tspeed: 0.0674s/iter; left time: 1382.6011s\n",
      "\titers: 200, epoch: 9 | loss: 0.0105395\n",
      "\tspeed: 0.0345s/iter; left time: 703.4444s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.96s\n",
      "Steps: 224 | Train Loss: 0.0120762 Vali Loss: 0.0108338 Test Loss: 0.0172134\n",
      "Validation loss decreased (0.010922 --> 0.010834).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0116649\n",
      "\tspeed: 0.0723s/iter; left time: 1466.7255s\n",
      "\titers: 200, epoch: 10 | loss: 0.0099158\n",
      "\tspeed: 0.0350s/iter; left time: 705.7269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 224 | Train Loss: 0.0114562 Vali Loss: 0.0104998 Test Loss: 0.0165373\n",
      "Validation loss decreased (0.010834 --> 0.010500).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0113771\n",
      "\tspeed: 0.0633s/iter; left time: 1268.8919s\n",
      "\titers: 200, epoch: 11 | loss: 0.0114219\n",
      "\tspeed: 0.0352s/iter; left time: 701.8794s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.86s\n",
      "Steps: 224 | Train Loss: 0.0110821 Vali Loss: 0.0103820 Test Loss: 0.0219415\n",
      "Validation loss decreased (0.010500 --> 0.010382).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0108638\n",
      "\tspeed: 0.0689s/iter; left time: 1366.1695s\n",
      "\titers: 200, epoch: 12 | loss: 0.0102516\n",
      "\tspeed: 0.0319s/iter; left time: 628.7570s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 224 | Train Loss: 0.0106935 Vali Loss: 0.0100026 Test Loss: 0.0214718\n",
      "Validation loss decreased (0.010382 --> 0.010003).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0098957\n",
      "\tspeed: 0.0612s/iter; left time: 1199.4222s\n",
      "\titers: 200, epoch: 13 | loss: 0.0092505\n",
      "\tspeed: 0.0290s/iter; left time: 565.3401s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.74s\n",
      "Steps: 224 | Train Loss: 0.0104717 Vali Loss: 0.0099261 Test Loss: 0.0182114\n",
      "Validation loss decreased (0.010003 --> 0.009926).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0108826\n",
      "\tspeed: 0.0637s/iter; left time: 1235.8885s\n",
      "\titers: 200, epoch: 14 | loss: 0.0088690\n",
      "\tspeed: 0.0344s/iter; left time: 663.1042s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 224 | Train Loss: 0.0102703 Vali Loss: 0.0098426 Test Loss: 0.0177606\n",
      "Validation loss decreased (0.009926 --> 0.009843).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0105166\n",
      "\tspeed: 0.0559s/iter; left time: 1071.3337s\n",
      "\titers: 200, epoch: 15 | loss: 0.0097342\n",
      "\tspeed: 0.0227s/iter; left time: 432.4013s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 224 | Train Loss: 0.0101483 Vali Loss: 0.0098085 Test Loss: 0.0205569\n",
      "Validation loss decreased (0.009843 --> 0.009809).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0087296\n",
      "\tspeed: 0.0424s/iter; left time: 802.4277s\n",
      "\titers: 200, epoch: 16 | loss: 0.0096730\n",
      "\tspeed: 0.0295s/iter; left time: 555.0353s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 224 | Train Loss: 0.0101333 Vali Loss: 0.0097114 Test Loss: 0.0196735\n",
      "Validation loss decreased (0.009809 --> 0.009711).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0108441\n",
      "\tspeed: 0.0656s/iter; left time: 1227.4040s\n",
      "\titers: 200, epoch: 17 | loss: 0.0097255\n",
      "\tspeed: 0.0332s/iter; left time: 617.2192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.76s\n",
      "Steps: 224 | Train Loss: 0.0099599 Vali Loss: 0.0095411 Test Loss: 0.0179444\n",
      "Validation loss decreased (0.009711 --> 0.009541).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0088029\n",
      "\tspeed: 0.0672s/iter; left time: 1242.0426s\n",
      "\titers: 200, epoch: 18 | loss: 0.0095050\n",
      "\tspeed: 0.0319s/iter; left time: 586.4735s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 224 | Train Loss: 0.0098107 Vali Loss: 0.0095047 Test Loss: 0.0188733\n",
      "Validation loss decreased (0.009541 --> 0.009505).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0096069\n",
      "\tspeed: 0.0691s/iter; left time: 1261.7855s\n",
      "\titers: 200, epoch: 19 | loss: 0.0100707\n",
      "\tspeed: 0.0318s/iter; left time: 577.2990s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 224 | Train Loss: 0.0097356 Vali Loss: 0.0095299 Test Loss: 0.0189880\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0096012\n",
      "\tspeed: 0.0662s/iter; left time: 1195.0160s\n",
      "\titers: 200, epoch: 20 | loss: 0.0103081\n",
      "\tspeed: 0.0336s/iter; left time: 603.6716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.79s\n",
      "Steps: 224 | Train Loss: 0.0096700 Vali Loss: 0.0094713 Test Loss: 0.0193751\n",
      "Validation loss decreased (0.009505 --> 0.009471).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0099798\n",
      "\tspeed: 0.0752s/iter; left time: 1339.4890s\n",
      "\titers: 200, epoch: 21 | loss: 0.0106904\n",
      "\tspeed: 0.0350s/iter; left time: 619.6264s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.34s\n",
      "Steps: 224 | Train Loss: 0.0096355 Vali Loss: 0.0094038 Test Loss: 0.0190703\n",
      "Validation loss decreased (0.009471 --> 0.009404).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0098239\n",
      "\tspeed: 0.0695s/iter; left time: 1223.3233s\n",
      "\titers: 200, epoch: 22 | loss: 0.0093015\n",
      "\tspeed: 0.0336s/iter; left time: 587.0746s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:08.23s\n",
      "Steps: 224 | Train Loss: 0.0095963 Vali Loss: 0.0094222 Test Loss: 0.0179196\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0091338\n",
      "\tspeed: 0.0636s/iter; left time: 1104.8615s\n",
      "\titers: 200, epoch: 23 | loss: 0.0098065\n",
      "\tspeed: 0.0339s/iter; left time: 586.0158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 224 | Train Loss: 0.0095129 Vali Loss: 0.0092669 Test Loss: 0.0183864\n",
      "Validation loss decreased (0.009404 --> 0.009267).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0093905\n",
      "\tspeed: 0.0680s/iter; left time: 1165.5587s\n",
      "\titers: 200, epoch: 24 | loss: 0.0102782\n",
      "\tspeed: 0.0327s/iter; left time: 557.6188s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 224 | Train Loss: 0.0095316 Vali Loss: 0.0093195 Test Loss: 0.0209839\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0104343\n",
      "\tspeed: 0.0698s/iter; left time: 1181.4679s\n",
      "\titers: 200, epoch: 25 | loss: 0.0088957\n",
      "\tspeed: 0.0328s/iter; left time: 551.1008s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 224 | Train Loss: 0.0094462 Vali Loss: 0.0092798 Test Loss: 0.0181019\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0092811\n",
      "\tspeed: 0.0680s/iter; left time: 1135.6562s\n",
      "\titers: 200, epoch: 26 | loss: 0.0094501\n",
      "\tspeed: 0.0323s/iter; left time: 535.7183s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.65s\n",
      "Steps: 224 | Train Loss: 0.0096182 Vali Loss: 0.0094344 Test Loss: 0.0172716\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0091324\n",
      "\tspeed: 0.0743s/iter; left time: 1225.0588s\n",
      "\titers: 200, epoch: 27 | loss: 0.0106261\n",
      "\tspeed: 0.0327s/iter; left time: 535.7578s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.35s\n",
      "Steps: 224 | Train Loss: 0.0094139 Vali Loss: 0.0092859 Test Loss: 0.0192018\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0077775\n",
      "\tspeed: 0.0668s/iter; left time: 1086.3652s\n",
      "\titers: 200, epoch: 28 | loss: 0.0089043\n",
      "\tspeed: 0.0331s/iter; left time: 535.0280s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0093605 Vali Loss: 0.0092553 Test Loss: 0.0188021\n",
      "Validation loss decreased (0.009267 --> 0.009255).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0087630\n",
      "\tspeed: 0.0673s/iter; left time: 1078.4406s\n",
      "\titers: 200, epoch: 29 | loss: 0.0096931\n",
      "\tspeed: 0.0372s/iter; left time: 591.9534s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:08.31s\n",
      "Steps: 224 | Train Loss: 0.0093333 Vali Loss: 0.0091649 Test Loss: 0.0186937\n",
      "Validation loss decreased (0.009255 --> 0.009165).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0092893\n",
      "\tspeed: 0.0757s/iter; left time: 1197.1580s\n",
      "\titers: 200, epoch: 30 | loss: 0.0085216\n",
      "\tspeed: 0.0339s/iter; left time: 531.9662s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 224 | Train Loss: 0.0094238 Vali Loss: 0.0093011 Test Loss: 0.0215952\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0095737\n",
      "\tspeed: 0.0633s/iter; left time: 986.7050s\n",
      "\titers: 200, epoch: 31 | loss: 0.0114639\n",
      "\tspeed: 0.0306s/iter; left time: 473.8219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 224 | Train Loss: 0.0093481 Vali Loss: 0.0091937 Test Loss: 0.0199297\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0099808\n",
      "\tspeed: 0.0578s/iter; left time: 887.4318s\n",
      "\titers: 200, epoch: 32 | loss: 0.0079586\n",
      "\tspeed: 0.0379s/iter; left time: 578.5559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:07.66s\n",
      "Steps: 224 | Train Loss: 0.0092504 Vali Loss: 0.0091131 Test Loss: 0.0179925\n",
      "Validation loss decreased (0.009165 --> 0.009113).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0097558\n",
      "\tspeed: 0.0618s/iter; left time: 935.4649s\n",
      "\titers: 200, epoch: 33 | loss: 0.0094637\n",
      "\tspeed: 0.0325s/iter; left time: 488.2426s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:07.20s\n",
      "Steps: 224 | Train Loss: 0.0092967 Vali Loss: 0.0091046 Test Loss: 0.0196414\n",
      "Validation loss decreased (0.009113 --> 0.009105).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0093406\n",
      "\tspeed: 0.0670s/iter; left time: 999.1567s\n",
      "\titers: 200, epoch: 34 | loss: 0.0088018\n",
      "\tspeed: 0.0323s/iter; left time: 478.3532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:08.05s\n",
      "Steps: 224 | Train Loss: 0.0092845 Vali Loss: 0.0091666 Test Loss: 0.0193905\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0092743\n",
      "\tspeed: 0.0635s/iter; left time: 932.3974s\n",
      "\titers: 200, epoch: 35 | loss: 0.0083632\n",
      "\tspeed: 0.0345s/iter; left time: 503.7828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 224 | Train Loss: 0.0092069 Vali Loss: 0.0090927 Test Loss: 0.0179031\n",
      "Validation loss decreased (0.009105 --> 0.009093).  Saving model ...\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0098525\n",
      "\tspeed: 0.0695s/iter; left time: 1005.0829s\n",
      "\titers: 200, epoch: 36 | loss: 0.0083036\n",
      "\tspeed: 0.0339s/iter; left time: 487.2637s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 224 | Train Loss: 0.0092492 Vali Loss: 0.0090937 Test Loss: 0.0208244\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0083163\n",
      "\tspeed: 0.0720s/iter; left time: 1024.9486s\n",
      "\titers: 200, epoch: 37 | loss: 0.0091936\n",
      "\tspeed: 0.0335s/iter; left time: 473.0187s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 224 | Train Loss: 0.0092016 Vali Loss: 0.0090485 Test Loss: 0.0183503\n",
      "Validation loss decreased (0.009093 --> 0.009049).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0080988\n",
      "\tspeed: 0.0705s/iter; left time: 987.3785s\n",
      "\titers: 200, epoch: 38 | loss: 0.0095375\n",
      "\tspeed: 0.0332s/iter; left time: 461.6985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:08.27s\n",
      "Steps: 224 | Train Loss: 0.0092376 Vali Loss: 0.0091220 Test Loss: 0.0189542\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0091136\n",
      "\tspeed: 0.0646s/iter; left time: 890.5798s\n",
      "\titers: 200, epoch: 39 | loss: 0.0088850\n",
      "\tspeed: 0.0349s/iter; left time: 477.3490s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 224 | Train Loss: 0.0092021 Vali Loss: 0.0090822 Test Loss: 0.0201605\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0085782\n",
      "\tspeed: 0.0775s/iter; left time: 1051.5229s\n",
      "\titers: 200, epoch: 40 | loss: 0.0086339\n",
      "\tspeed: 0.0446s/iter; left time: 600.1926s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:09.61s\n",
      "Steps: 224 | Train Loss: 0.0091749 Vali Loss: 0.0090968 Test Loss: 0.0189611\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0097929\n",
      "\tspeed: 0.0632s/iter; left time: 843.7170s\n",
      "\titers: 200, epoch: 41 | loss: 0.0088069\n",
      "\tspeed: 0.0323s/iter; left time: 427.0714s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:07.66s\n",
      "Steps: 224 | Train Loss: 0.0091887 Vali Loss: 0.0091413 Test Loss: 0.0185922\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0104537\n",
      "\tspeed: 0.0650s/iter; left time: 853.0560s\n",
      "\titers: 200, epoch: 42 | loss: 0.0085751\n",
      "\tspeed: 0.0342s/iter; left time: 445.7137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 224 | Train Loss: 0.0092292 Vali Loss: 0.0091321 Test Loss: 0.0179263\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0089744\n",
      "\tspeed: 0.0676s/iter; left time: 871.6863s\n",
      "\titers: 200, epoch: 43 | loss: 0.0091920\n",
      "\tspeed: 0.0321s/iter; left time: 410.3368s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:07.61s\n",
      "Steps: 224 | Train Loss: 0.0092599 Vali Loss: 0.0091090 Test Loss: 0.0174770\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0096402\n",
      "\tspeed: 0.0692s/iter; left time: 876.0870s\n",
      "\titers: 200, epoch: 44 | loss: 0.0095632\n",
      "\tspeed: 0.0363s/iter; left time: 456.3408s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:08.57s\n",
      "Steps: 224 | Train Loss: 0.0092053 Vali Loss: 0.0091317 Test Loss: 0.0196726\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0099961\n",
      "\tspeed: 0.0657s/iter; left time: 817.2273s\n",
      "\titers: 200, epoch: 45 | loss: 0.0104771\n",
      "\tspeed: 0.0326s/iter; left time: 402.8310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 224 | Train Loss: 0.0092551 Vali Loss: 0.0091076 Test Loss: 0.0179130\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0084438\n",
      "\tspeed: 0.0664s/iter; left time: 811.0736s\n",
      "\titers: 200, epoch: 46 | loss: 0.0094436\n",
      "\tspeed: 0.0341s/iter; left time: 413.1245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:08.04s\n",
      "Steps: 224 | Train Loss: 0.0091619 Vali Loss: 0.0090871 Test Loss: 0.0172821\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0097950\n",
      "\tspeed: 0.0704s/iter; left time: 844.3442s\n",
      "\titers: 200, epoch: 47 | loss: 0.0084180\n",
      "\tspeed: 0.0341s/iter; left time: 405.7638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:07.95s\n",
      "Steps: 224 | Train Loss: 0.0091385 Vali Loss: 0.0091280 Test Loss: 0.0175781\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01835033856332302, rmse:0.1354634165763855, mae:0.08412059396505356, rse:0.39865246415138245\n",
      "Intermediate time for ES and pred_len 24: 00h:08m:02.77s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1243099\n",
      "\tspeed: 0.0617s/iter; left time: 1376.4784s\n",
      "\titers: 200, epoch: 1 | loss: 0.1112777\n",
      "\tspeed: 0.0306s/iter; left time: 678.3505s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 224 | Train Loss: 0.1304680 Vali Loss: 0.0749759 Test Loss: 0.0934199\n",
      "Validation loss decreased (inf --> 0.074976).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0422750\n",
      "\tspeed: 0.0576s/iter; left time: 1272.0179s\n",
      "\titers: 200, epoch: 2 | loss: 0.0295403\n",
      "\tspeed: 0.0248s/iter; left time: 544.0051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.99s\n",
      "Steps: 224 | Train Loss: 0.0484345 Vali Loss: 0.0234643 Test Loss: 0.0291021\n",
      "Validation loss decreased (0.074976 --> 0.023464).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0261098\n",
      "\tspeed: 0.0704s/iter; left time: 1539.3126s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251757\n",
      "\tspeed: 0.0340s/iter; left time: 739.0874s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.38s\n",
      "Steps: 224 | Train Loss: 0.0267445 Vali Loss: 0.0204402 Test Loss: 0.0251841\n",
      "Validation loss decreased (0.023464 --> 0.020440).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0220553\n",
      "\tspeed: 0.0651s/iter; left time: 1408.6487s\n",
      "\titers: 200, epoch: 4 | loss: 0.0216708\n",
      "\tspeed: 0.0346s/iter; left time: 744.4531s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.94s\n",
      "Steps: 224 | Train Loss: 0.0230021 Vali Loss: 0.0190462 Test Loss: 0.0245052\n",
      "Validation loss decreased (0.020440 --> 0.019046).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0207762\n",
      "\tspeed: 0.0680s/iter; left time: 1454.7322s\n",
      "\titers: 200, epoch: 5 | loss: 0.0181857\n",
      "\tspeed: 0.0349s/iter; left time: 743.8005s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.08s\n",
      "Steps: 224 | Train Loss: 0.0199932 Vali Loss: 0.0178145 Test Loss: 0.0349214\n",
      "Validation loss decreased (0.019046 --> 0.017814).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0178383\n",
      "\tspeed: 0.0641s/iter; left time: 1357.7454s\n",
      "\titers: 200, epoch: 6 | loss: 0.0184396\n",
      "\tspeed: 0.0329s/iter; left time: 693.7278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.65s\n",
      "Steps: 224 | Train Loss: 0.0179342 Vali Loss: 0.0170949 Test Loss: 0.0270423\n",
      "Validation loss decreased (0.017814 --> 0.017095).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0178106\n",
      "\tspeed: 0.0806s/iter; left time: 1688.4535s\n",
      "\titers: 200, epoch: 7 | loss: 0.0158232\n",
      "\tspeed: 0.0333s/iter; left time: 694.3862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.43s\n",
      "Steps: 224 | Train Loss: 0.0169621 Vali Loss: 0.0164534 Test Loss: 0.0254716\n",
      "Validation loss decreased (0.017095 --> 0.016453).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0172422\n",
      "\tspeed: 0.0673s/iter; left time: 1394.3534s\n",
      "\titers: 200, epoch: 8 | loss: 0.0163265\n",
      "\tspeed: 0.0345s/iter; left time: 710.9013s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.96s\n",
      "Steps: 224 | Train Loss: 0.0164334 Vali Loss: 0.0159479 Test Loss: 0.0251510\n",
      "Validation loss decreased (0.016453 --> 0.015948).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0159623\n",
      "\tspeed: 0.0715s/iter; left time: 1465.8270s\n",
      "\titers: 200, epoch: 9 | loss: 0.0149368\n",
      "\tspeed: 0.0379s/iter; left time: 773.4056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.73s\n",
      "Steps: 224 | Train Loss: 0.0161261 Vali Loss: 0.0157977 Test Loss: 0.0278607\n",
      "Validation loss decreased (0.015948 --> 0.015798).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0162833\n",
      "\tspeed: 0.0751s/iter; left time: 1523.3336s\n",
      "\titers: 200, epoch: 10 | loss: 0.0167888\n",
      "\tspeed: 0.0335s/iter; left time: 676.5176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:08.72s\n",
      "Steps: 224 | Train Loss: 0.0157125 Vali Loss: 0.0157265 Test Loss: 0.0291120\n",
      "Validation loss decreased (0.015798 --> 0.015727).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0157385\n",
      "\tspeed: 0.0772s/iter; left time: 1548.8039s\n",
      "\titers: 200, epoch: 11 | loss: 0.0161697\n",
      "\tspeed: 0.0338s/iter; left time: 674.0981s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:08.38s\n",
      "Steps: 224 | Train Loss: 0.0155772 Vali Loss: 0.0154291 Test Loss: 0.0283270\n",
      "Validation loss decreased (0.015727 --> 0.015429).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0157377\n",
      "\tspeed: 0.0716s/iter; left time: 1420.7511s\n",
      "\titers: 200, epoch: 12 | loss: 0.0161220\n",
      "\tspeed: 0.0327s/iter; left time: 645.4541s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.92s\n",
      "Steps: 224 | Train Loss: 0.0153947 Vali Loss: 0.0155128 Test Loss: 0.0267294\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0145680\n",
      "\tspeed: 0.0649s/iter; left time: 1273.7174s\n",
      "\titers: 200, epoch: 13 | loss: 0.0151161\n",
      "\tspeed: 0.0331s/iter; left time: 645.8575s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.77s\n",
      "Steps: 224 | Train Loss: 0.0152482 Vali Loss: 0.0152456 Test Loss: 0.0313342\n",
      "Validation loss decreased (0.015429 --> 0.015246).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0144295\n",
      "\tspeed: 0.0710s/iter; left time: 1376.2148s\n",
      "\titers: 200, epoch: 14 | loss: 0.0149595\n",
      "\tspeed: 0.0381s/iter; left time: 733.9659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:08.44s\n",
      "Steps: 224 | Train Loss: 0.0156610 Vali Loss: 0.0153661 Test Loss: 0.0282838\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0138063\n",
      "\tspeed: 0.0618s/iter; left time: 1184.8536s\n",
      "\titers: 200, epoch: 15 | loss: 0.0160840\n",
      "\tspeed: 0.0373s/iter; left time: 710.2193s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 224 | Train Loss: 0.0153227 Vali Loss: 0.0163661 Test Loss: 0.0256972\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0147027\n",
      "\tspeed: 0.0673s/iter; left time: 1275.5846s\n",
      "\titers: 200, epoch: 16 | loss: 0.0159377\n",
      "\tspeed: 0.0329s/iter; left time: 620.3771s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 224 | Train Loss: 0.0152654 Vali Loss: 0.0152266 Test Loss: 0.0313957\n",
      "Validation loss decreased (0.015246 --> 0.015227).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0147857\n",
      "\tspeed: 0.0651s/iter; left time: 1218.6164s\n",
      "\titers: 200, epoch: 17 | loss: 0.0158567\n",
      "\tspeed: 0.0314s/iter; left time: 584.5487s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 224 | Train Loss: 0.0149680 Vali Loss: 0.0152546 Test Loss: 0.0288141\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0150313\n",
      "\tspeed: 0.0594s/iter; left time: 1098.0424s\n",
      "\titers: 200, epoch: 18 | loss: 0.0153670\n",
      "\tspeed: 0.0377s/iter; left time: 693.5811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.81s\n",
      "Steps: 224 | Train Loss: 0.0148922 Vali Loss: 0.0151359 Test Loss: 0.0352062\n",
      "Validation loss decreased (0.015227 --> 0.015136).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0152953\n",
      "\tspeed: 0.0583s/iter; left time: 1064.8301s\n",
      "\titers: 200, epoch: 19 | loss: 0.0164243\n",
      "\tspeed: 0.0279s/iter; left time: 506.4266s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:06.53s\n",
      "Steps: 224 | Train Loss: 0.0154202 Vali Loss: 0.0154077 Test Loss: 0.0260406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0139980\n",
      "\tspeed: 0.0666s/iter; left time: 1201.3364s\n",
      "\titers: 200, epoch: 20 | loss: 0.0153673\n",
      "\tspeed: 0.0342s/iter; left time: 613.0738s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:08.55s\n",
      "Steps: 224 | Train Loss: 0.0148775 Vali Loss: 0.0150819 Test Loss: 0.0291436\n",
      "Validation loss decreased (0.015136 --> 0.015082).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0152630\n",
      "\tspeed: 0.0658s/iter; left time: 1171.8346s\n",
      "\titers: 200, epoch: 21 | loss: 0.0139147\n",
      "\tspeed: 0.0346s/iter; left time: 613.3407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 224 | Train Loss: 0.0147524 Vali Loss: 0.0150175 Test Loss: 0.0302568\n",
      "Validation loss decreased (0.015082 --> 0.015018).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0152948\n",
      "\tspeed: 0.0649s/iter; left time: 1141.3071s\n",
      "\titers: 200, epoch: 22 | loss: 0.0151246\n",
      "\tspeed: 0.0345s/iter; left time: 604.4906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 224 | Train Loss: 0.0147291 Vali Loss: 0.0150637 Test Loss: 0.0294112\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0138211\n",
      "\tspeed: 0.0703s/iter; left time: 1221.1014s\n",
      "\titers: 200, epoch: 23 | loss: 0.0160007\n",
      "\tspeed: 0.0366s/iter; left time: 633.0278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.35s\n",
      "Steps: 224 | Train Loss: 0.0146824 Vali Loss: 0.0149438 Test Loss: 0.0299825\n",
      "Validation loss decreased (0.015018 --> 0.014944).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0137403\n",
      "\tspeed: 0.0699s/iter; left time: 1198.5750s\n",
      "\titers: 200, epoch: 24 | loss: 0.0137934\n",
      "\tspeed: 0.0359s/iter; left time: 612.2421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:08.29s\n",
      "Steps: 224 | Train Loss: 0.0146960 Vali Loss: 0.0149939 Test Loss: 0.0284647\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0147044\n",
      "\tspeed: 0.0729s/iter; left time: 1233.2951s\n",
      "\titers: 200, epoch: 25 | loss: 0.0162568\n",
      "\tspeed: 0.0346s/iter; left time: 582.9099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:08.64s\n",
      "Steps: 224 | Train Loss: 0.0146439 Vali Loss: 0.0149701 Test Loss: 0.0297256\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0135267\n",
      "\tspeed: 0.0725s/iter; left time: 1209.9924s\n",
      "\titers: 200, epoch: 26 | loss: 0.0140600\n",
      "\tspeed: 0.0356s/iter; left time: 591.5465s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:08.57s\n",
      "Steps: 224 | Train Loss: 0.0146344 Vali Loss: 0.0149647 Test Loss: 0.0292373\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0148893\n",
      "\tspeed: 0.0694s/iter; left time: 1144.0827s\n",
      "\titers: 200, epoch: 27 | loss: 0.0141754\n",
      "\tspeed: 0.0340s/iter; left time: 557.4435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:08.20s\n",
      "Steps: 224 | Train Loss: 0.0145488 Vali Loss: 0.0149350 Test Loss: 0.0312378\n",
      "Validation loss decreased (0.014944 --> 0.014935).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0155144\n",
      "\tspeed: 0.0688s/iter; left time: 1117.6884s\n",
      "\titers: 200, epoch: 28 | loss: 0.0170834\n",
      "\tspeed: 0.0334s/iter; left time: 539.8284s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:08.07s\n",
      "Steps: 224 | Train Loss: 0.0145691 Vali Loss: 0.0149834 Test Loss: 0.0311552\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0156587\n",
      "\tspeed: 0.0659s/iter; left time: 1056.4232s\n",
      "\titers: 200, epoch: 29 | loss: 0.0152508\n",
      "\tspeed: 0.0438s/iter; left time: 696.9015s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:09.28s\n",
      "Steps: 224 | Train Loss: 0.0145267 Vali Loss: 0.0149104 Test Loss: 0.0324814\n",
      "Validation loss decreased (0.014935 --> 0.014910).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0140890\n",
      "\tspeed: 0.0734s/iter; left time: 1159.7660s\n",
      "\titers: 200, epoch: 30 | loss: 0.0135267\n",
      "\tspeed: 0.0397s/iter; left time: 623.1687s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:08.72s\n",
      "Steps: 224 | Train Loss: 0.0145276 Vali Loss: 0.0149391 Test Loss: 0.0319644\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0140089\n",
      "\tspeed: 0.0611s/iter; left time: 951.2772s\n",
      "\titers: 200, epoch: 31 | loss: 0.0148899\n",
      "\tspeed: 0.0384s/iter; left time: 595.0901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.24s\n",
      "Steps: 224 | Train Loss: 0.0144888 Vali Loss: 0.0148844 Test Loss: 0.0310164\n",
      "Validation loss decreased (0.014910 --> 0.014884).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0149111\n",
      "\tspeed: 0.0664s/iter; left time: 1020.1068s\n",
      "\titers: 200, epoch: 32 | loss: 0.0149692\n",
      "\tspeed: 0.0335s/iter; left time: 511.1251s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:07.77s\n",
      "Steps: 224 | Train Loss: 0.0145123 Vali Loss: 0.0149362 Test Loss: 0.0324887\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0149417\n",
      "\tspeed: 0.0648s/iter; left time: 981.0322s\n",
      "\titers: 200, epoch: 33 | loss: 0.0151715\n",
      "\tspeed: 0.0322s/iter; left time: 483.9088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:07.72s\n",
      "Steps: 224 | Train Loss: 0.0145124 Vali Loss: 0.0149080 Test Loss: 0.0316317\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0130034\n",
      "\tspeed: 0.0741s/iter; left time: 1105.2367s\n",
      "\titers: 200, epoch: 34 | loss: 0.0153687\n",
      "\tspeed: 0.0297s/iter; left time: 439.3788s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.88s\n",
      "Steps: 224 | Train Loss: 0.0144306 Vali Loss: 0.0149091 Test Loss: 0.0325113\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0147780\n",
      "\tspeed: 0.0594s/iter; left time: 872.5483s\n",
      "\titers: 200, epoch: 35 | loss: 0.0155870\n",
      "\tspeed: 0.0294s/iter; left time: 428.2292s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 224 | Train Loss: 0.0145847 Vali Loss: 0.0149064 Test Loss: 0.0315548\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0142555\n",
      "\tspeed: 0.0678s/iter; left time: 980.2007s\n",
      "\titers: 200, epoch: 36 | loss: 0.0138206\n",
      "\tspeed: 0.0335s/iter; left time: 481.3036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:07.46s\n",
      "Steps: 224 | Train Loss: 0.0145906 Vali Loss: 0.0149479 Test Loss: 0.0295329\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0151577\n",
      "\tspeed: 0.0731s/iter; left time: 1040.8403s\n",
      "\titers: 200, epoch: 37 | loss: 0.0137488\n",
      "\tspeed: 0.0372s/iter; left time: 526.5308s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:09.14s\n",
      "Steps: 224 | Train Loss: 0.0144642 Vali Loss: 0.0148912 Test Loss: 0.0300454\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0134963\n",
      "\tspeed: 0.0706s/iter; left time: 989.7093s\n",
      "\titers: 200, epoch: 38 | loss: 0.0137275\n",
      "\tspeed: 0.0372s/iter; left time: 518.0299s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:08.61s\n",
      "Steps: 224 | Train Loss: 0.0147023 Vali Loss: 0.0149361 Test Loss: 0.0297693\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0135200\n",
      "\tspeed: 0.0643s/iter; left time: 886.5110s\n",
      "\titers: 200, epoch: 39 | loss: 0.0149992\n",
      "\tspeed: 0.0327s/iter; left time: 446.9630s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:07.60s\n",
      "Steps: 224 | Train Loss: 0.0145202 Vali Loss: 0.0149646 Test Loss: 0.0300370\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0150121\n",
      "\tspeed: 0.0616s/iter; left time: 835.7713s\n",
      "\titers: 200, epoch: 40 | loss: 0.0140267\n",
      "\tspeed: 0.0396s/iter; left time: 532.9583s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 224 | Train Loss: 0.0145302 Vali Loss: 0.0149335 Test Loss: 0.0295037\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0145172\n",
      "\tspeed: 0.0695s/iter; left time: 927.5189s\n",
      "\titers: 200, epoch: 41 | loss: 0.0145294\n",
      "\tspeed: 0.0360s/iter; left time: 476.2811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:08.33s\n",
      "Steps: 224 | Train Loss: 0.0144130 Vali Loss: 0.0149308 Test Loss: 0.0298137\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.031016401946544647, rmse:0.1761147379875183, mae:0.11428863555192947, rse:0.5173721313476562\n",
      "Intermediate time for ES and pred_len 96: 00h:07m:10.46s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1284169\n",
      "\tspeed: 0.0630s/iter; left time: 1399.5427s\n",
      "\titers: 200, epoch: 1 | loss: 0.1172287\n",
      "\tspeed: 0.0333s/iter; left time: 735.3522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.04s\n",
      "Steps: 223 | Train Loss: 0.1315195 Vali Loss: 0.0755308 Test Loss: 0.0935711\n",
      "Validation loss decreased (inf --> 0.075531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0394520\n",
      "\tspeed: 0.0683s/iter; left time: 1501.9118s\n",
      "\titers: 200, epoch: 2 | loss: 0.0300034\n",
      "\tspeed: 0.0342s/iter; left time: 749.0825s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.18s\n",
      "Steps: 223 | Train Loss: 0.0483511 Vali Loss: 0.0254592 Test Loss: 0.0312638\n",
      "Validation loss decreased (0.075531 --> 0.025459).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0298315\n",
      "\tspeed: 0.0702s/iter; left time: 1527.2272s\n",
      "\titers: 200, epoch: 3 | loss: 0.0257679\n",
      "\tspeed: 0.0334s/iter; left time: 723.0111s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.99s\n",
      "Steps: 223 | Train Loss: 0.0275876 Vali Loss: 0.0222081 Test Loss: 0.0273541\n",
      "Validation loss decreased (0.025459 --> 0.022208).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0234629\n",
      "\tspeed: 0.0692s/iter; left time: 1489.9470s\n",
      "\titers: 200, epoch: 4 | loss: 0.0214712\n",
      "\tspeed: 0.0341s/iter; left time: 731.5178s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.78s\n",
      "Steps: 223 | Train Loss: 0.0232781 Vali Loss: 0.0205097 Test Loss: 0.0334549\n",
      "Validation loss decreased (0.022208 --> 0.020510).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0214271\n",
      "\tspeed: 0.0745s/iter; left time: 1588.2192s\n",
      "\titers: 200, epoch: 5 | loss: 0.0183603\n",
      "\tspeed: 0.0337s/iter; left time: 714.4726s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.65s\n",
      "Steps: 223 | Train Loss: 0.0199682 Vali Loss: 0.0189914 Test Loss: 0.0321025\n",
      "Validation loss decreased (0.020510 --> 0.018991).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0175757\n",
      "\tspeed: 0.0692s/iter; left time: 1459.1214s\n",
      "\titers: 200, epoch: 6 | loss: 0.0184193\n",
      "\tspeed: 0.0323s/iter; left time: 678.7248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0184786 Vali Loss: 0.0183154 Test Loss: 0.0325361\n",
      "Validation loss decreased (0.018991 --> 0.018315).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0176433\n",
      "\tspeed: 0.0775s/iter; left time: 1616.3857s\n",
      "\titers: 200, epoch: 7 | loss: 0.0172115\n",
      "\tspeed: 0.0415s/iter; left time: 862.6001s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:09.13s\n",
      "Steps: 223 | Train Loss: 0.0178102 Vali Loss: 0.0178938 Test Loss: 0.0310357\n",
      "Validation loss decreased (0.018315 --> 0.017894).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0161289\n",
      "\tspeed: 0.0718s/iter; left time: 1481.0796s\n",
      "\titers: 200, epoch: 8 | loss: 0.0173672\n",
      "\tspeed: 0.0288s/iter; left time: 591.4928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.57s\n",
      "Steps: 223 | Train Loss: 0.0174180 Vali Loss: 0.0177719 Test Loss: 0.0310839\n",
      "Validation loss decreased (0.017894 --> 0.017772).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0172031\n",
      "\tspeed: 0.0690s/iter; left time: 1409.4120s\n",
      "\titers: 200, epoch: 9 | loss: 0.0161021\n",
      "\tspeed: 0.0389s/iter; left time: 790.4316s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:08.40s\n",
      "Steps: 223 | Train Loss: 0.0170769 Vali Loss: 0.0179210 Test Loss: 0.0304198\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0170793\n",
      "\tspeed: 0.0620s/iter; left time: 1251.1784s\n",
      "\titers: 200, epoch: 10 | loss: 0.0183556\n",
      "\tspeed: 0.0327s/iter; left time: 657.5804s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 223 | Train Loss: 0.0170377 Vali Loss: 0.0174098 Test Loss: 0.0347562\n",
      "Validation loss decreased (0.017772 --> 0.017410).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0170728\n",
      "\tspeed: 0.0485s/iter; left time: 968.3480s\n",
      "\titers: 200, epoch: 11 | loss: 0.0171281\n",
      "\tspeed: 0.0201s/iter; left time: 399.0089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 223 | Train Loss: 0.0183533 Vali Loss: 0.0177499 Test Loss: 0.0281153\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0142452\n",
      "\tspeed: 0.0703s/iter; left time: 1389.1458s\n",
      "\titers: 200, epoch: 12 | loss: 0.0168665\n",
      "\tspeed: 0.0360s/iter; left time: 707.7703s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.15s\n",
      "Steps: 223 | Train Loss: 0.0166846 Vali Loss: 0.0173330 Test Loss: 0.0298354\n",
      "Validation loss decreased (0.017410 --> 0.017333).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0161570\n",
      "\tspeed: 0.0699s/iter; left time: 1365.5925s\n",
      "\titers: 200, epoch: 13 | loss: 0.0165948\n",
      "\tspeed: 0.0424s/iter; left time: 824.4303s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:08.68s\n",
      "Steps: 223 | Train Loss: 0.0166812 Vali Loss: 0.0173812 Test Loss: 0.0305343\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0160646\n",
      "\tspeed: 0.0774s/iter; left time: 1493.8002s\n",
      "\titers: 200, epoch: 14 | loss: 0.0174675\n",
      "\tspeed: 0.0330s/iter; left time: 634.2622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:08.72s\n",
      "Steps: 223 | Train Loss: 0.0164507 Vali Loss: 0.0174721 Test Loss: 0.0310623\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0159258\n",
      "\tspeed: 0.0688s/iter; left time: 1311.7023s\n",
      "\titers: 200, epoch: 15 | loss: 0.0180293\n",
      "\tspeed: 0.0328s/iter; left time: 622.8340s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 223 | Train Loss: 0.0163814 Vali Loss: 0.0171970 Test Loss: 0.0328074\n",
      "Validation loss decreased (0.017333 --> 0.017197).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0171973\n",
      "\tspeed: 0.0649s/iter; left time: 1223.1667s\n",
      "\titers: 200, epoch: 16 | loss: 0.0169949\n",
      "\tspeed: 0.0351s/iter; left time: 658.2256s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.76s\n",
      "Steps: 223 | Train Loss: 0.0166250 Vali Loss: 0.0173631 Test Loss: 0.0302975\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0154431\n",
      "\tspeed: 0.0701s/iter; left time: 1306.1770s\n",
      "\titers: 200, epoch: 17 | loss: 0.0158963\n",
      "\tspeed: 0.0330s/iter; left time: 611.2275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.79s\n",
      "Steps: 223 | Train Loss: 0.0163361 Vali Loss: 0.0173100 Test Loss: 0.0320395\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0158747\n",
      "\tspeed: 0.0671s/iter; left time: 1235.7812s\n",
      "\titers: 200, epoch: 18 | loss: 0.0172246\n",
      "\tspeed: 0.0344s/iter; left time: 630.3189s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.96s\n",
      "Steps: 223 | Train Loss: 0.0162627 Vali Loss: 0.0172289 Test Loss: 0.0336265\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0153920\n",
      "\tspeed: 0.0760s/iter; left time: 1381.9362s\n",
      "\titers: 200, epoch: 19 | loss: 0.0159192\n",
      "\tspeed: 0.0335s/iter; left time: 605.1782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.18s\n",
      "Steps: 223 | Train Loss: 0.0161746 Vali Loss: 0.0171079 Test Loss: 0.0315786\n",
      "Validation loss decreased (0.017197 --> 0.017108).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0172178\n",
      "\tspeed: 0.0737s/iter; left time: 1323.1303s\n",
      "\titers: 200, epoch: 20 | loss: 0.0154738\n",
      "\tspeed: 0.0392s/iter; left time: 699.7429s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:09.10s\n",
      "Steps: 223 | Train Loss: 0.0162445 Vali Loss: 0.0171308 Test Loss: 0.0327551\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0156633\n",
      "\tspeed: 0.0701s/iter; left time: 1243.7205s\n",
      "\titers: 200, epoch: 21 | loss: 0.0158444\n",
      "\tspeed: 0.0344s/iter; left time: 606.4581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 223 | Train Loss: 0.0161826 Vali Loss: 0.0171602 Test Loss: 0.0327335\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0160388\n",
      "\tspeed: 0.0645s/iter; left time: 1129.8200s\n",
      "\titers: 200, epoch: 22 | loss: 0.0170065\n",
      "\tspeed: 0.0346s/iter; left time: 603.4638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.81s\n",
      "Steps: 223 | Train Loss: 0.0160776 Vali Loss: 0.0170937 Test Loss: 0.0331659\n",
      "Validation loss decreased (0.017108 --> 0.017094).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0147220\n",
      "\tspeed: 0.0676s/iter; left time: 1168.7368s\n",
      "\titers: 200, epoch: 23 | loss: 0.0174204\n",
      "\tspeed: 0.0379s/iter; left time: 651.7976s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:08.37s\n",
      "Steps: 223 | Train Loss: 0.0160284 Vali Loss: 0.0170745 Test Loss: 0.0332947\n",
      "Validation loss decreased (0.017094 --> 0.017074).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0157670\n",
      "\tspeed: 0.0682s/iter; left time: 1164.6693s\n",
      "\titers: 200, epoch: 24 | loss: 0.0147003\n",
      "\tspeed: 0.0381s/iter; left time: 647.1778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:08.43s\n",
      "Steps: 223 | Train Loss: 0.0159721 Vali Loss: 0.0169896 Test Loss: 0.0325431\n",
      "Validation loss decreased (0.017074 --> 0.016990).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0167779\n",
      "\tspeed: 0.0828s/iter; left time: 1395.6360s\n",
      "\titers: 200, epoch: 25 | loss: 0.0158870\n",
      "\tspeed: 0.0308s/iter; left time: 515.5034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:09.06s\n",
      "Steps: 223 | Train Loss: 0.0160009 Vali Loss: 0.0169696 Test Loss: 0.0342427\n",
      "Validation loss decreased (0.016990 --> 0.016970).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0182935\n",
      "\tspeed: 0.0615s/iter; left time: 1023.0348s\n",
      "\titers: 200, epoch: 26 | loss: 0.0152293\n",
      "\tspeed: 0.0282s/iter; left time: 465.4038s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:06.79s\n",
      "Steps: 223 | Train Loss: 0.0159626 Vali Loss: 0.0170733 Test Loss: 0.0365577\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0150043\n",
      "\tspeed: 0.0706s/iter; left time: 1158.2097s\n",
      "\titers: 200, epoch: 27 | loss: 0.0167594\n",
      "\tspeed: 0.0295s/iter; left time: 481.4555s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:07.15s\n",
      "Steps: 223 | Train Loss: 0.0158955 Vali Loss: 0.0169665 Test Loss: 0.0339735\n",
      "Validation loss decreased (0.016970 --> 0.016967).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0150428\n",
      "\tspeed: 0.0704s/iter; left time: 1139.8537s\n",
      "\titers: 200, epoch: 28 | loss: 0.0171211\n",
      "\tspeed: 0.0347s/iter; left time: 558.4060s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:08.58s\n",
      "Steps: 223 | Train Loss: 0.0160956 Vali Loss: 0.0170350 Test Loss: 0.0318645\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0164731\n",
      "\tspeed: 0.0665s/iter; left time: 1061.1743s\n",
      "\titers: 200, epoch: 29 | loss: 0.0175466\n",
      "\tspeed: 0.0325s/iter; left time: 516.1357s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 223 | Train Loss: 0.0159007 Vali Loss: 0.0170863 Test Loss: 0.0330290\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0160695\n",
      "\tspeed: 0.0689s/iter; left time: 1084.2139s\n",
      "\titers: 200, epoch: 30 | loss: 0.0164711\n",
      "\tspeed: 0.0357s/iter; left time: 557.3875s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:08.21s\n",
      "Steps: 223 | Train Loss: 0.0158933 Vali Loss: 0.0171437 Test Loss: 0.0331343\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0156804\n",
      "\tspeed: 0.0673s/iter; left time: 1044.1861s\n",
      "\titers: 200, epoch: 31 | loss: 0.0155068\n",
      "\tspeed: 0.0339s/iter; left time: 523.1168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:08.21s\n",
      "Steps: 223 | Train Loss: 0.0158197 Vali Loss: 0.0170106 Test Loss: 0.0339011\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0151057\n",
      "\tspeed: 0.0735s/iter; left time: 1122.9904s\n",
      "\titers: 200, epoch: 32 | loss: 0.0165237\n",
      "\tspeed: 0.0378s/iter; left time: 574.1693s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.56s\n",
      "Steps: 223 | Train Loss: 0.0159418 Vali Loss: 0.0170865 Test Loss: 0.0326944\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0154904\n",
      "\tspeed: 0.0659s/iter; left time: 992.6843s\n",
      "\titers: 200, epoch: 33 | loss: 0.0160965\n",
      "\tspeed: 0.0360s/iter; left time: 538.3158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:08.01s\n",
      "Steps: 223 | Train Loss: 0.0159461 Vali Loss: 0.0170641 Test Loss: 0.0318416\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0153600\n",
      "\tspeed: 0.0694s/iter; left time: 1030.3840s\n",
      "\titers: 200, epoch: 34 | loss: 0.0170636\n",
      "\tspeed: 0.0399s/iter; left time: 588.4978s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:08.85s\n",
      "Steps: 223 | Train Loss: 0.0158354 Vali Loss: 0.0170254 Test Loss: 0.0326165\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0164788\n",
      "\tspeed: 0.0662s/iter; left time: 968.0300s\n",
      "\titers: 200, epoch: 35 | loss: 0.0158054\n",
      "\tspeed: 0.0336s/iter; left time: 487.2108s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 223 | Train Loss: 0.0158333 Vali Loss: 0.0169850 Test Loss: 0.0334912\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0153885\n",
      "\tspeed: 0.0706s/iter; left time: 1015.8118s\n",
      "\titers: 200, epoch: 36 | loss: 0.0159685\n",
      "\tspeed: 0.0355s/iter; left time: 507.7441s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:08.75s\n",
      "Steps: 223 | Train Loss: 0.0157919 Vali Loss: 0.0170162 Test Loss: 0.0343361\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0170595\n",
      "\tspeed: 0.0731s/iter; left time: 1035.9503s\n",
      "\titers: 200, epoch: 37 | loss: 0.0156103\n",
      "\tspeed: 0.0357s/iter; left time: 502.7961s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:08.25s\n",
      "Steps: 223 | Train Loss: 0.0157774 Vali Loss: 0.0169519 Test Loss: 0.0330856\n",
      "Validation loss decreased (0.016967 --> 0.016952).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0175597\n",
      "\tspeed: 0.0783s/iter; left time: 1091.6219s\n",
      "\titers: 200, epoch: 38 | loss: 0.0155797\n",
      "\tspeed: 0.0409s/iter; left time: 565.7756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:08.92s\n",
      "Steps: 223 | Train Loss: 0.0158271 Vali Loss: 0.0170436 Test Loss: 0.0333813\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0153492\n",
      "\tspeed: 0.0702s/iter; left time: 964.0592s\n",
      "\titers: 200, epoch: 39 | loss: 0.0160599\n",
      "\tspeed: 0.0322s/iter; left time: 438.9489s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:07.95s\n",
      "Steps: 223 | Train Loss: 0.0157613 Vali Loss: 0.0169965 Test Loss: 0.0336042\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0156489\n",
      "\tspeed: 0.0723s/iter; left time: 976.8041s\n",
      "\titers: 200, epoch: 40 | loss: 0.0156041\n",
      "\tspeed: 0.0354s/iter; left time: 474.4302s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:08.72s\n",
      "Steps: 223 | Train Loss: 0.0159860 Vali Loss: 0.0170430 Test Loss: 0.0325757\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0168099\n",
      "\tspeed: 0.0730s/iter; left time: 970.1701s\n",
      "\titers: 200, epoch: 41 | loss: 0.0156008\n",
      "\tspeed: 0.0353s/iter; left time: 465.9219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:08.92s\n",
      "Steps: 223 | Train Loss: 0.0157786 Vali Loss: 0.0170154 Test Loss: 0.0328521\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0149853\n",
      "\tspeed: 0.0661s/iter; left time: 863.7247s\n",
      "\titers: 200, epoch: 42 | loss: 0.0156122\n",
      "\tspeed: 0.0290s/iter; left time: 376.1919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:07.31s\n",
      "Steps: 223 | Train Loss: 0.0157521 Vali Loss: 0.0169720 Test Loss: 0.0328154\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0155255\n",
      "\tspeed: 0.0742s/iter; left time: 952.0099s\n",
      "\titers: 200, epoch: 43 | loss: 0.0152470\n",
      "\tspeed: 0.0302s/iter; left time: 384.9422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 223 | Train Loss: 0.0157695 Vali Loss: 0.0169560 Test Loss: 0.0332695\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0151010\n",
      "\tspeed: 0.0588s/iter; left time: 741.7916s\n",
      "\titers: 200, epoch: 44 | loss: 0.0157190\n",
      "\tspeed: 0.0386s/iter; left time: 483.1504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:08.11s\n",
      "Steps: 223 | Train Loss: 0.0158228 Vali Loss: 0.0170257 Test Loss: 0.0328763\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0156760\n",
      "\tspeed: 0.0703s/iter; left time: 870.7125s\n",
      "\titers: 200, epoch: 45 | loss: 0.0156764\n",
      "\tspeed: 0.0348s/iter; left time: 427.2998s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:08.05s\n",
      "Steps: 223 | Train Loss: 0.0157282 Vali Loss: 0.0169368 Test Loss: 0.0327699\n",
      "Validation loss decreased (0.016952 --> 0.016937).  Saving model ...\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0161030\n",
      "\tspeed: 0.0820s/iter; left time: 997.5157s\n",
      "\titers: 200, epoch: 46 | loss: 0.0172997\n",
      "\tspeed: 0.0354s/iter; left time: 427.3866s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 223 | Train Loss: 0.0157710 Vali Loss: 0.0169600 Test Loss: 0.0329193\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0147430\n",
      "\tspeed: 0.0737s/iter; left time: 880.4968s\n",
      "\titers: 200, epoch: 47 | loss: 0.0161302\n",
      "\tspeed: 0.0344s/iter; left time: 407.6810s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:08.47s\n",
      "Steps: 223 | Train Loss: 0.0157679 Vali Loss: 0.0169480 Test Loss: 0.0331634\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0164486\n",
      "\tspeed: 0.0698s/iter; left time: 818.3514s\n",
      "\titers: 200, epoch: 48 | loss: 0.0148175\n",
      "\tspeed: 0.0421s/iter; left time: 488.8318s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:08.62s\n",
      "Steps: 223 | Train Loss: 0.0157819 Vali Loss: 0.0169770 Test Loss: 0.0333828\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0163971\n",
      "\tspeed: 0.0709s/iter; left time: 815.3033s\n",
      "\titers: 200, epoch: 49 | loss: 0.0148533\n",
      "\tspeed: 0.0361s/iter; left time: 411.0656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:08.11s\n",
      "Steps: 223 | Train Loss: 0.0157606 Vali Loss: 0.0168984 Test Loss: 0.0325417\n",
      "Validation loss decreased (0.016937 --> 0.016898).  Saving model ...\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0165030\n",
      "\tspeed: 0.0754s/iter; left time: 850.1677s\n",
      "\titers: 200, epoch: 50 | loss: 0.0154998\n",
      "\tspeed: 0.0404s/iter; left time: 450.8843s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:09.23s\n",
      "Steps: 223 | Train Loss: 0.0157381 Vali Loss: 0.0169541 Test Loss: 0.0337758\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0161881\n",
      "\tspeed: 0.0684s/iter; left time: 756.2690s\n",
      "\titers: 200, epoch: 51 | loss: 0.0148654\n",
      "\tspeed: 0.0344s/iter; left time: 376.4242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 223 | Train Loss: 0.0157762 Vali Loss: 0.0169161 Test Loss: 0.0328058\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0165587\n",
      "\tspeed: 0.0710s/iter; left time: 768.9931s\n",
      "\titers: 200, epoch: 52 | loss: 0.0159679\n",
      "\tspeed: 0.0345s/iter; left time: 370.4315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:08.43s\n",
      "Steps: 223 | Train Loss: 0.0157141 Vali Loss: 0.0169452 Test Loss: 0.0326098\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0154703\n",
      "\tspeed: 0.0693s/iter; left time: 735.1127s\n",
      "\titers: 200, epoch: 53 | loss: 0.0147402\n",
      "\tspeed: 0.0390s/iter; left time: 410.0605s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:08.54s\n",
      "Steps: 223 | Train Loss: 0.0157296 Vali Loss: 0.0169105 Test Loss: 0.0328408\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0152968\n",
      "\tspeed: 0.0686s/iter; left time: 712.0448s\n",
      "\titers: 200, epoch: 54 | loss: 0.0161895\n",
      "\tspeed: 0.0345s/iter; left time: 355.1490s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:08.25s\n",
      "Steps: 223 | Train Loss: 0.0157269 Vali Loss: 0.0169528 Test Loss: 0.0334324\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0161664\n",
      "\tspeed: 0.0803s/iter; left time: 815.8882s\n",
      "\titers: 200, epoch: 55 | loss: 0.0158240\n",
      "\tspeed: 0.0360s/iter; left time: 361.9046s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:08.75s\n",
      "Steps: 223 | Train Loss: 0.0157380 Vali Loss: 0.0169289 Test Loss: 0.0334821\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0162431\n",
      "\tspeed: 0.0733s/iter; left time: 728.1013s\n",
      "\titers: 200, epoch: 56 | loss: 0.0165172\n",
      "\tspeed: 0.0352s/iter; left time: 346.0959s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:08.62s\n",
      "Steps: 223 | Train Loss: 0.0157867 Vali Loss: 0.0168710 Test Loss: 0.0329189\n",
      "Validation loss decreased (0.016898 --> 0.016871).  Saving model ...\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0147179\n",
      "\tspeed: 0.0909s/iter; left time: 883.1859s\n",
      "\titers: 200, epoch: 57 | loss: 0.0167741\n",
      "\tspeed: 0.0287s/iter; left time: 275.4427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:08.83s\n",
      "Steps: 223 | Train Loss: 0.0158918 Vali Loss: 0.0170040 Test Loss: 0.0338830\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.0151420\n",
      "\tspeed: 0.0704s/iter; left time: 668.3588s\n",
      "\titers: 200, epoch: 58 | loss: 0.0155338\n",
      "\tspeed: 0.0384s/iter; left time: 360.4754s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 58\n",
      "Cost time: 00h:00m:08.67s\n",
      "Steps: 223 | Train Loss: 0.0157274 Vali Loss: 0.0169733 Test Loss: 0.0335529\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.0157903\n",
      "\tspeed: 0.2107s/iter; left time: 1952.4927s\n",
      "\titers: 200, epoch: 59 | loss: 0.0146947\n",
      "\tspeed: 0.0352s/iter; left time: 322.4601s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 59\n",
      "Cost time: 00h:00m:14.70s\n",
      "Steps: 223 | Train Loss: 0.0157228 Vali Loss: 0.0170033 Test Loss: 0.0348816\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.0163458\n",
      "\tspeed: 0.3186s/iter; left time: 2881.3651s\n",
      "\titers: 200, epoch: 60 | loss: 0.0159373\n",
      "\tspeed: 0.1375s/iter; left time: 1230.1096s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 60\n",
      "Cost time: 00h:00m:30.30s\n",
      "Steps: 223 | Train Loss: 0.0157675 Vali Loss: 0.0168795 Test Loss: 0.0327201\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.0158163\n",
      "\tspeed: 0.0713s/iter; left time: 628.9268s\n",
      "\titers: 200, epoch: 61 | loss: 0.0149162\n",
      "\tspeed: 0.0465s/iter; left time: 405.5445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 61\n",
      "Cost time: 00h:00m:09.82s\n",
      "Steps: 223 | Train Loss: 0.0157603 Vali Loss: 0.0169108 Test Loss: 0.0329584\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.0162279\n",
      "\tspeed: 0.0682s/iter; left time: 586.4561s\n",
      "\titers: 200, epoch: 62 | loss: 0.0151691\n",
      "\tspeed: 0.0337s/iter; left time: 286.5542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 62\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 223 | Train Loss: 0.0157876 Vali Loss: 0.0169134 Test Loss: 0.0335492\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.0156255\n",
      "\tspeed: 0.0708s/iter; left time: 592.9511s\n",
      "\titers: 200, epoch: 63 | loss: 0.0155422\n",
      "\tspeed: 0.0362s/iter; left time: 299.2992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 63\n",
      "Cost time: 00h:00m:08.74s\n",
      "Steps: 223 | Train Loss: 0.0157253 Vali Loss: 0.0169123 Test Loss: 0.0329814\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.0164308\n",
      "\tspeed: 0.0720s/iter; left time: 586.8696s\n",
      "\titers: 200, epoch: 64 | loss: 0.0141026\n",
      "\tspeed: 0.0332s/iter; left time: 267.2577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 64\n",
      "Cost time: 00h:00m:08.07s\n",
      "Steps: 223 | Train Loss: 0.0157296 Vali Loss: 0.0169038 Test Loss: 0.0327831\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.0158460\n",
      "\tspeed: 0.0657s/iter; left time: 520.8375s\n",
      "\titers: 200, epoch: 65 | loss: 0.0152336\n",
      "\tspeed: 0.0415s/iter; left time: 324.7403s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 65\n",
      "Cost time: 00h:00m:08.77s\n",
      "Steps: 223 | Train Loss: 0.0157248 Vali Loss: 0.0169858 Test Loss: 0.0333585\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.0160218\n",
      "\tspeed: 0.0774s/iter; left time: 596.1768s\n",
      "\titers: 200, epoch: 66 | loss: 0.0162508\n",
      "\tspeed: 0.0327s/iter; left time: 248.5706s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 66\n",
      "Cost time: 00h:00m:08.78s\n",
      "Steps: 223 | Train Loss: 0.0157725 Vali Loss: 0.0169603 Test Loss: 0.0329206\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.03291890770196915, rmse:0.181435689330101, mae:0.12120398133993149, rse:0.5330417156219482\n",
      "Intermediate time for ES and pred_len 168: 00h:12m:38.60s\n",
      "Intermediate time for ES: 00h:27m:51.83s\n",
      "Total time: 00h:27m:51.84s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_revin_ES.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            model_id = f\"channel_mixing_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --revin 0 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
