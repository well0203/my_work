{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. TimeLLM](#1-timellm)\n",
    "- [2. TimeLLM](#2-timellm-336)\n",
    "\n",
    "Results for TimeLLM. The first one is default input length 512, the second one: 336."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"2\"\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 512\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_512.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-03 01:12:05,716] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 01:12:06,757] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 01:12:06,757] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 01:12:06,757] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 01:12:06,848] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 01:12:06,848] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 01:12:07,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 01:12:07,584] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 01:12:07,584] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 01:12:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 01:12:07,586] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 01:12:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 01:12:07,942] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 01:12:07,944] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:07,944] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.49 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,122] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 01:12:08,123] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:08,123] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.53 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,123] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 01:12:08,286] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 01:12:08,287] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:08,287] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.42 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1cacb059d0>\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688982\n",
      "\tspeed: 0.1674s/iter; left time: 14941.3446s\n",
      "\titers: 200, epoch: 1 | loss: 0.1398762\n",
      "\tspeed: 0.1227s/iter; left time: 10938.8876s\n",
      "\titers: 300, epoch: 1 | loss: 0.1594033\n",
      "\tspeed: 0.1246s/iter; left time: 11095.0762s\n",
      "\titers: 400, epoch: 1 | loss: 0.1548022\n",
      "\tspeed: 0.1240s/iter; left time: 11033.2634s\n",
      "\titers: 500, epoch: 1 | loss: 0.1380969\n",
      "\tspeed: 0.1244s/iter; left time: 11056.1716s\n",
      "\titers: 600, epoch: 1 | loss: 0.1172558\n",
      "\tspeed: 0.1254s/iter; left time: 11131.3914s\n",
      "\titers: 700, epoch: 1 | loss: 0.1290968\n",
      "\tspeed: 0.1249s/iter; left time: 11072.9375s\n",
      "\titers: 800, epoch: 1 | loss: 0.1323915\n",
      "\tspeed: 0.1237s/iter; left time: 10953.4734s\n",
      "\titers: 900, epoch: 1 | loss: 0.0968730\n",
      "\tspeed: 0.1241s/iter; left time: 10982.1952s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1262417\n",
      "\tspeed: 0.1247s/iter; left time: 11018.8563s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1081160\n",
      "\tspeed: 0.1254s/iter; left time: 11067.6072s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1055638\n",
      "\tspeed: 0.1233s/iter; left time: 10866.4606s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0805435\n",
      "\tspeed: 0.1236s/iter; left time: 10880.7501s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1116071\n",
      "\tspeed: 0.1239s/iter; left time: 10895.0457s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0692826\n",
      "\tspeed: 0.1238s/iter; left time: 10879.3045s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0953681\n",
      "\tspeed: 0.1243s/iter; left time: 10905.0177s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1187560\n",
      "\tspeed: 0.1228s/iter; left time: 10768.5576s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0991687\n",
      "\tspeed: 0.1231s/iter; left time: 10777.3760s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1227836\n",
      "\tspeed: 0.1242s/iter; left time: 10861.5365s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0913499\n",
      "\tspeed: 0.1222s/iter; left time: 10676.2045s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0831885\n",
      "\tspeed: 0.1243s/iter; left time: 10845.1859s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1006144\n",
      "\tspeed: 0.1223s/iter; left time: 10662.3231s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1002881\n",
      "\tspeed: 0.1232s/iter; left time: 10727.6874s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0858188\n",
      "\tspeed: 0.1241s/iter; left time: 10787.8205s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0878481\n",
      "\tspeed: 0.1233s/iter; left time: 10707.2554s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0938092\n",
      "\tspeed: 0.1251s/iter; left time: 10856.3638s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1014624\n",
      "\tspeed: 0.1246s/iter; left time: 10801.6068s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0828544\n",
      "\tspeed: 0.1242s/iter; left time: 10747.8099s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0846184\n",
      "\tspeed: 0.1240s/iter; left time: 10719.4482s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0732674\n",
      "\tspeed: 0.1233s/iter; left time: 10648.1436s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0863261\n",
      "\tspeed: 0.1233s/iter; left time: 10635.2736s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1145399\n",
      "\tspeed: 0.1231s/iter; left time: 10606.0279s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0950736\n",
      "\tspeed: 0.1244s/iter; left time: 10705.7121s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1013337\n",
      "\tspeed: 0.1254s/iter; left time: 10783.4548s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0962714\n",
      "\tspeed: 0.1234s/iter; left time: 10599.2676s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045466\n",
      "\tspeed: 0.1249s/iter; left time: 10713.2908s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0921188\n",
      "\tspeed: 0.1228s/iter; left time: 10516.2821s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0915671\n",
      "\tspeed: 0.1235s/iter; left time: 10562.9555s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059835\n",
      "\tspeed: 0.1255s/iter; left time: 10726.9688s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0626834\n",
      "\tspeed: 0.1229s/iter; left time: 10488.5240s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0807726\n",
      "\tspeed: 0.1229s/iter; left time: 10482.4050s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1075011\n",
      "\tspeed: 0.1222s/iter; left time: 10402.4167s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0953812\n",
      "\tspeed: 0.1234s/iter; left time: 10499.2678s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0955328\n",
      "\tspeed: 0.1237s/iter; left time: 10509.4145s\n",
      "Epoch: 1 cost time: 00h:09m:14.48s\n",
      "Epoch: 1 | Train Loss: 0.1053388 Vali Loss: 0.0971123 Test Loss: 0.0994308\n",
      "Validation loss decreased (inf --> 0.097112).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0704475\n",
      "\tspeed: 1.7948s/iter; left time: 152182.9881s\n",
      "\titers: 200, epoch: 2 | loss: 0.0855533\n",
      "\tspeed: 0.1141s/iter; left time: 9659.3097s\n",
      "\titers: 300, epoch: 2 | loss: 0.0774946\n",
      "\tspeed: 0.1140s/iter; left time: 9640.4392s\n",
      "\titers: 400, epoch: 2 | loss: 0.0883429\n",
      "\tspeed: 0.1123s/iter; left time: 9491.7135s\n",
      "\titers: 500, epoch: 2 | loss: 0.0867326\n",
      "\tspeed: 0.1123s/iter; left time: 9475.7204s\n",
      "\titers: 600, epoch: 2 | loss: 0.0939545\n",
      "\tspeed: 0.1139s/iter; left time: 9597.7267s\n",
      "\titers: 700, epoch: 2 | loss: 0.0739971\n",
      "\tspeed: 0.1128s/iter; left time: 9497.3048s\n",
      "\titers: 800, epoch: 2 | loss: 0.0977335\n",
      "\tspeed: 0.1149s/iter; left time: 9664.4324s\n",
      "\titers: 900, epoch: 2 | loss: 0.0857365\n",
      "\tspeed: 0.1138s/iter; left time: 9555.0482s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0865258\n",
      "\tspeed: 0.1149s/iter; left time: 9639.7231s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0972862\n",
      "\tspeed: 0.1140s/iter; left time: 9554.6080s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0832878\n",
      "\tspeed: 0.1144s/iter; left time: 9574.7462s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0947991\n",
      "\tspeed: 0.1156s/iter; left time: 9662.8164s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0940965\n",
      "\tspeed: 0.1154s/iter; left time: 9639.0286s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0825809\n",
      "\tspeed: 0.1155s/iter; left time: 9628.6063s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0861147\n",
      "\tspeed: 0.1136s/iter; left time: 9465.3499s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1065482\n",
      "\tspeed: 0.1139s/iter; left time: 9479.3903s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0846911\n",
      "\tspeed: 0.1132s/iter; left time: 9403.8697s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1024258\n",
      "\tspeed: 0.1142s/iter; left time: 9480.0655s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0896808\n",
      "\tspeed: 0.1129s/iter; left time: 9358.1236s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1056611\n",
      "\tspeed: 0.1135s/iter; left time: 9397.8205s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0911830\n",
      "\tspeed: 0.1128s/iter; left time: 9331.4414s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0980409\n",
      "\tspeed: 0.1141s/iter; left time: 9421.8908s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0799288\n",
      "\tspeed: 0.1155s/iter; left time: 9529.0887s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0872002\n",
      "\tspeed: 0.1140s/iter; left time: 9391.3248s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0915525\n",
      "\tspeed: 0.1120s/iter; left time: 9218.2102s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1013657\n",
      "\tspeed: 0.1136s/iter; left time: 9339.5619s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0794303\n",
      "\tspeed: 0.1132s/iter; left time: 9294.8333s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0900755\n",
      "\tspeed: 0.1148s/iter; left time: 9411.2400s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0776787\n",
      "\tspeed: 0.1129s/iter; left time: 9244.6546s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1028968\n",
      "\tspeed: 0.1133s/iter; left time: 9268.9056s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0771540\n",
      "\tspeed: 0.1152s/iter; left time: 9411.0392s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0871339\n",
      "\tspeed: 0.1157s/iter; left time: 9437.3403s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1009780\n",
      "\tspeed: 0.1145s/iter; left time: 9332.5256s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0891745\n",
      "\tspeed: 0.1149s/iter; left time: 9348.6727s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0985909\n",
      "\tspeed: 0.1155s/iter; left time: 9389.6240s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1067271\n",
      "\tspeed: 0.1178s/iter; left time: 9560.6694s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0852604\n",
      "\tspeed: 0.1159s/iter; left time: 9395.5152s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0817025\n",
      "\tspeed: 0.1128s/iter; left time: 9132.8653s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0846763\n",
      "\tspeed: 0.1128s/iter; left time: 9121.4540s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0871173\n",
      "\tspeed: 0.1136s/iter; left time: 9175.4806s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0959664\n",
      "\tspeed: 0.1137s/iter; left time: 9174.5179s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0784686\n",
      "\tspeed: 0.1134s/iter; left time: 9141.7126s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0950680\n",
      "\tspeed: 0.1138s/iter; left time: 9156.8940s\n",
      "Epoch: 2 cost time: 00h:08m:30.16s\n",
      "Epoch: 2 | Train Loss: 0.0889246 Vali Loss: 0.0918921 Test Loss: 0.0945349\n",
      "Validation loss decreased (0.097112 --> 0.091892).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1014844\n",
      "\tspeed: 1.6040s/iter; left time: 128842.4170s\n",
      "\titers: 200, epoch: 3 | loss: 0.0968780\n",
      "\tspeed: 0.1137s/iter; left time: 9122.6059s\n",
      "\titers: 300, epoch: 3 | loss: 0.0694327\n",
      "\tspeed: 0.1162s/iter; left time: 9306.7944s\n",
      "\titers: 400, epoch: 3 | loss: 0.0818534\n",
      "\tspeed: 0.1131s/iter; left time: 9049.8084s\n",
      "\titers: 500, epoch: 3 | loss: 0.1179019\n",
      "\tspeed: 0.1105s/iter; left time: 8835.4020s\n",
      "\titers: 600, epoch: 3 | loss: 0.0902409\n",
      "\tspeed: 0.1127s/iter; left time: 8993.7143s\n",
      "\titers: 700, epoch: 3 | loss: 0.0945199\n",
      "\tspeed: 0.1126s/iter; left time: 8973.5594s\n",
      "\titers: 800, epoch: 3 | loss: 0.0943223\n",
      "\tspeed: 0.1126s/iter; left time: 8965.1199s\n",
      "\titers: 900, epoch: 3 | loss: 0.0608129\n",
      "\tspeed: 0.1143s/iter; left time: 9089.2406s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0815312\n",
      "\tspeed: 0.1142s/iter; left time: 9067.5614s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0903181\n",
      "\tspeed: 0.1137s/iter; left time: 9020.1890s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0806858\n",
      "\tspeed: 0.1142s/iter; left time: 9044.4942s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0762755\n",
      "\tspeed: 0.1140s/iter; left time: 9017.3213s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0923751\n",
      "\tspeed: 0.1116s/iter; left time: 8821.8508s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0904863\n",
      "\tspeed: 0.1127s/iter; left time: 8895.6489s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0592667\n",
      "\tspeed: 0.1135s/iter; left time: 8942.9849s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0708085\n",
      "\tspeed: 0.1157s/iter; left time: 9110.4364s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0865551\n",
      "\tspeed: 0.1119s/iter; left time: 8795.0520s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0666124\n",
      "\tspeed: 0.1124s/iter; left time: 8822.9030s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0717531\n",
      "\tspeed: 0.1142s/iter; left time: 8953.9096s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1037164\n",
      "\tspeed: 0.1134s/iter; left time: 8884.8049s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0819335\n",
      "\tspeed: 0.1118s/iter; left time: 8748.3673s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0837101\n",
      "\tspeed: 0.1114s/iter; left time: 8701.8710s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0792184\n",
      "\tspeed: 0.1116s/iter; left time: 8710.8425s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0968816\n",
      "\tspeed: 0.1135s/iter; left time: 8845.6248s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0933213\n",
      "\tspeed: 0.1132s/iter; left time: 8806.5863s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0743180\n",
      "\tspeed: 0.1126s/iter; left time: 8748.4160s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0937308\n",
      "\tspeed: 0.1126s/iter; left time: 8740.1547s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0838305\n",
      "\tspeed: 0.1137s/iter; left time: 8813.0296s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0647273\n",
      "\tspeed: 0.1146s/iter; left time: 8875.4268s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0913395\n",
      "\tspeed: 0.1137s/iter; left time: 8793.1195s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0819122\n",
      "\tspeed: 0.1131s/iter; left time: 8732.8662s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0788185\n",
      "\tspeed: 0.1123s/iter; left time: 8659.5411s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0784381\n",
      "\tspeed: 0.1121s/iter; left time: 8636.4983s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0678186\n",
      "\tspeed: 0.1117s/iter; left time: 8588.7881s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0698414\n",
      "\tspeed: 0.1137s/iter; left time: 8736.1703s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0900844\n",
      "\tspeed: 0.1122s/iter; left time: 8606.1957s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0756922\n",
      "\tspeed: 0.1149s/iter; left time: 8805.2898s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0685307\n",
      "\tspeed: 0.1125s/iter; left time: 8610.4216s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0735308\n",
      "\tspeed: 0.1119s/iter; left time: 8552.9052s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0800254\n",
      "\tspeed: 0.1119s/iter; left time: 8542.2113s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0855180\n",
      "\tspeed: 0.1154s/iter; left time: 8796.0084s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0667157\n",
      "\tspeed: 0.1144s/iter; left time: 8708.6890s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0843554\n",
      "\tspeed: 0.1124s/iter; left time: 8544.1850s\n",
      "Epoch: 3 cost time: 00h:08m:26.09s\n",
      "Epoch: 3 | Train Loss: 0.0853331 Vali Loss: 0.0911640 Test Loss: 0.0942841\n",
      "Validation loss decreased (0.091892 --> 0.091164).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0739329\n",
      "\tspeed: 1.5815s/iter; left time: 119968.3080s\n",
      "\titers: 200, epoch: 4 | loss: 0.1006263\n",
      "\tspeed: 0.1141s/iter; left time: 8647.2176s\n",
      "\titers: 300, epoch: 4 | loss: 0.0738416\n",
      "\tspeed: 0.1155s/iter; left time: 8739.9632s\n",
      "\titers: 400, epoch: 4 | loss: 0.0781351\n",
      "\tspeed: 0.1131s/iter; left time: 8548.7698s\n",
      "\titers: 500, epoch: 4 | loss: 0.0675929\n",
      "\tspeed: 0.1151s/iter; left time: 8687.4375s\n",
      "\titers: 600, epoch: 4 | loss: 0.0722866\n",
      "\tspeed: 0.1138s/iter; left time: 8578.3459s\n",
      "\titers: 700, epoch: 4 | loss: 0.0852794\n",
      "\tspeed: 0.1146s/iter; left time: 8627.3161s\n",
      "\titers: 800, epoch: 4 | loss: 0.0830615\n",
      "\tspeed: 0.1136s/iter; left time: 8539.2426s\n",
      "\titers: 900, epoch: 4 | loss: 0.0746369\n",
      "\tspeed: 0.1127s/iter; left time: 8458.1302s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0714534\n",
      "\tspeed: 0.1134s/iter; left time: 8499.4897s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0721987\n",
      "\tspeed: 0.1133s/iter; left time: 8483.5015s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0848372\n",
      "\tspeed: 0.1142s/iter; left time: 8540.1918s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0901184\n",
      "\tspeed: 0.1126s/iter; left time: 8406.3739s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0873635\n",
      "\tspeed: 0.1127s/iter; left time: 8402.5782s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0672005\n",
      "\tspeed: 0.1121s/iter; left time: 8348.9412s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0815371\n",
      "\tspeed: 0.1119s/iter; left time: 8321.4581s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0967667\n",
      "\tspeed: 0.1143s/iter; left time: 8487.8214s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1003437\n",
      "\tspeed: 0.1123s/iter; left time: 8328.0192s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0772678\n",
      "\tspeed: 0.1121s/iter; left time: 8303.8600s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1023371\n",
      "\tspeed: 0.1112s/iter; left time: 8226.3107s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0959407\n",
      "\tspeed: 0.1126s/iter; left time: 8313.9698s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0707141\n",
      "\tspeed: 0.1121s/iter; left time: 8266.0775s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1181110\n",
      "\tspeed: 0.1130s/iter; left time: 8323.1719s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0988108\n",
      "\tspeed: 0.1115s/iter; left time: 8201.0479s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1022194\n",
      "\tspeed: 0.1111s/iter; left time: 8160.0609s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0755848\n",
      "\tspeed: 0.1114s/iter; left time: 8168.9965s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0852136\n",
      "\tspeed: 0.1122s/iter; left time: 8220.4746s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1073938\n",
      "\tspeed: 0.1111s/iter; left time: 8125.4232s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0776819\n",
      "\tspeed: 0.1122s/iter; left time: 8200.1518s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0915424\n",
      "\tspeed: 0.1117s/iter; left time: 8146.3091s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0914509\n",
      "\tspeed: 0.1155s/iter; left time: 8412.5511s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0769955\n",
      "\tspeed: 0.1168s/iter; left time: 8498.2199s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0912195\n",
      "\tspeed: 0.1177s/iter; left time: 8550.1338s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0864609\n",
      "\tspeed: 0.1169s/iter; left time: 8485.3160s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0855569\n",
      "\tspeed: 0.1181s/iter; left time: 8557.3971s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0880700\n",
      "\tspeed: 0.1181s/iter; left time: 8544.6155s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0775939\n",
      "\tspeed: 0.1193s/iter; left time: 8618.5048s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0855137\n",
      "\tspeed: 0.1171s/iter; left time: 8451.2831s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0877626\n",
      "\tspeed: 0.1153s/iter; left time: 8308.4451s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0802846\n",
      "\tspeed: 0.1172s/iter; left time: 8436.5780s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0870235\n",
      "\tspeed: 0.1202s/iter; left time: 8634.0718s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0716099\n",
      "\tspeed: 0.1207s/iter; left time: 8660.1850s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0780072\n",
      "\tspeed: 0.1156s/iter; left time: 8281.9815s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0948033\n",
      "\tspeed: 0.1183s/iter; left time: 8463.4481s\n",
      "Epoch: 4 cost time: 00h:08m:31.84s\n",
      "Epoch: 4 | Train Loss: 0.0836256 Vali Loss: 0.0897763 Test Loss: 0.0933365\n",
      "Validation loss decreased (0.091164 --> 0.089776).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0797964\n",
      "\tspeed: 1.6464s/iter; left time: 117532.0136s\n",
      "\titers: 200, epoch: 5 | loss: 0.0838478\n",
      "\tspeed: 0.1194s/iter; left time: 8512.3885s\n",
      "\titers: 300, epoch: 5 | loss: 0.0742453\n",
      "\tspeed: 0.1179s/iter; left time: 8390.0387s\n",
      "\titers: 400, epoch: 5 | loss: 0.0888728\n",
      "\tspeed: 0.1203s/iter; left time: 8551.0436s\n",
      "\titers: 500, epoch: 5 | loss: 0.0898506\n",
      "\tspeed: 0.1186s/iter; left time: 8422.4576s\n",
      "\titers: 600, epoch: 5 | loss: 0.0773116\n",
      "\tspeed: 0.1140s/iter; left time: 8081.0151s\n",
      "\titers: 700, epoch: 5 | loss: 0.0626944\n",
      "\tspeed: 0.1191s/iter; left time: 8430.4481s\n",
      "\titers: 800, epoch: 5 | loss: 0.0781008\n",
      "\tspeed: 0.1205s/iter; left time: 8519.7659s\n",
      "\titers: 900, epoch: 5 | loss: 0.0825634\n",
      "\tspeed: 0.1205s/iter; left time: 8509.4591s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0848518\n",
      "\tspeed: 0.1189s/iter; left time: 8377.6728s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0829829\n",
      "\tspeed: 0.1204s/iter; left time: 8472.5536s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0997171\n",
      "\tspeed: 0.1201s/iter; left time: 8443.6639s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0798143\n",
      "\tspeed: 0.1197s/iter; left time: 8402.1515s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0764720\n",
      "\tspeed: 0.1176s/iter; left time: 8243.6340s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0732888\n",
      "\tspeed: 0.1193s/iter; left time: 8351.0889s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0836849\n",
      "\tspeed: 0.1157s/iter; left time: 8085.6975s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0724503\n",
      "\tspeed: 0.1156s/iter; left time: 8069.1478s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0894237\n",
      "\tspeed: 0.1198s/iter; left time: 8348.4672s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0685364\n",
      "\tspeed: 0.1178s/iter; left time: 8200.5653s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0697801\n",
      "\tspeed: 0.1175s/iter; left time: 8164.8331s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0814900\n",
      "\tspeed: 0.1169s/iter; left time: 8113.5792s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0934933\n",
      "\tspeed: 0.1142s/iter; left time: 7911.7020s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861936\n",
      "\tspeed: 0.1145s/iter; left time: 7919.7570s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0938417\n",
      "\tspeed: 0.1149s/iter; left time: 7940.1099s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0799977\n",
      "\tspeed: 0.1189s/iter; left time: 8205.5438s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0855416\n",
      "\tspeed: 0.1195s/iter; left time: 8229.9662s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0920220\n",
      "\tspeed: 0.1158s/iter; left time: 7968.0307s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0856894\n",
      "\tspeed: 0.1187s/iter; left time: 8154.6670s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0774287\n",
      "\tspeed: 0.1181s/iter; left time: 8098.6651s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0683412\n",
      "\tspeed: 0.1165s/iter; left time: 7976.2434s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0973268\n",
      "\tspeed: 0.1172s/iter; left time: 8015.3452s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0831058\n",
      "\tspeed: 0.1146s/iter; left time: 7827.2519s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0682414\n",
      "\tspeed: 0.1170s/iter; left time: 7980.1976s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0845700\n",
      "\tspeed: 0.1196s/iter; left time: 8140.1191s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0771565\n",
      "\tspeed: 0.1187s/iter; left time: 8072.2449s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0795133\n",
      "\tspeed: 0.1174s/iter; left time: 7969.1133s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0733312\n",
      "\tspeed: 0.1185s/iter; left time: 8031.3167s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0908493\n",
      "\tspeed: 0.1188s/iter; left time: 8044.7092s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0670330\n",
      "\tspeed: 0.1188s/iter; left time: 8032.0109s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0631396\n",
      "\tspeed: 0.1181s/iter; left time: 7969.3271s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0828409\n",
      "\tspeed: 0.1209s/iter; left time: 8148.0262s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0800139\n",
      "\tspeed: 0.1177s/iter; left time: 7917.2847s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0884054\n",
      "\tspeed: 0.1186s/iter; left time: 7967.7376s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0831661\n",
      "\tspeed: 0.1184s/iter; left time: 7943.2696s\n",
      "Epoch: 5 cost time: 00h:08m:48.26s\n",
      "Epoch: 5 | Train Loss: 0.0823728 Vali Loss: 0.0899557 Test Loss: 0.0942943\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1001324\n",
      "\tspeed: 1.6218s/iter; left time: 108534.5492s\n",
      "\titers: 200, epoch: 6 | loss: 0.0886922\n",
      "\tspeed: 0.1213s/iter; left time: 8105.9663s\n",
      "\titers: 300, epoch: 6 | loss: 0.0843214\n",
      "\tspeed: 0.1200s/iter; left time: 8005.3656s\n",
      "\titers: 400, epoch: 6 | loss: 0.0734667\n",
      "\tspeed: 0.1185s/iter; left time: 7896.9387s\n",
      "\titers: 500, epoch: 6 | loss: 0.0924280\n",
      "\tspeed: 0.1199s/iter; left time: 7979.1377s\n",
      "\titers: 600, epoch: 6 | loss: 0.0591974\n",
      "\tspeed: 0.1180s/iter; left time: 7835.7974s\n",
      "\titers: 700, epoch: 6 | loss: 0.0633274\n",
      "\tspeed: 0.1191s/iter; left time: 7899.5425s\n",
      "\titers: 800, epoch: 6 | loss: 0.0738303\n",
      "\tspeed: 0.1198s/iter; left time: 7930.7576s\n",
      "\titers: 900, epoch: 6 | loss: 0.1071520\n",
      "\tspeed: 0.1218s/iter; left time: 8053.4950s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0735545\n",
      "\tspeed: 0.1204s/iter; left time: 7946.3154s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0729964\n",
      "\tspeed: 0.1196s/iter; left time: 7881.9557s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0962333\n",
      "\tspeed: 0.1199s/iter; left time: 7892.1226s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0923874\n",
      "\tspeed: 0.1202s/iter; left time: 7901.9868s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0971264\n",
      "\tspeed: 0.1218s/iter; left time: 7992.7890s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0751228\n",
      "\tspeed: 0.1189s/iter; left time: 7790.8841s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0779485\n",
      "\tspeed: 0.1172s/iter; left time: 7668.9793s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0823796\n",
      "\tspeed: 0.1168s/iter; left time: 7627.4802s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0782527\n",
      "\tspeed: 0.1174s/iter; left time: 7659.5757s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0854593\n",
      "\tspeed: 0.1211s/iter; left time: 7887.2417s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0940582\n",
      "\tspeed: 0.1216s/iter; left time: 7906.6408s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0806722\n",
      "\tspeed: 0.1156s/iter; left time: 7503.7319s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0820966\n",
      "\tspeed: 0.1188s/iter; left time: 7698.6315s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0738427\n",
      "\tspeed: 0.1190s/iter; left time: 7704.3061s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0772219\n",
      "\tspeed: 0.1199s/iter; left time: 7749.0500s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0898360\n",
      "\tspeed: 0.1188s/iter; left time: 7662.1246s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0820411\n",
      "\tspeed: 0.1181s/iter; left time: 7605.2153s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0796860\n",
      "\tspeed: 0.1185s/iter; left time: 7624.4024s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989802\n",
      "\tspeed: 0.1191s/iter; left time: 7649.2150s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0941869\n",
      "\tspeed: 0.1154s/iter; left time: 7401.6936s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0823741\n",
      "\tspeed: 0.1177s/iter; left time: 7534.3386s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0726927\n",
      "\tspeed: 0.1189s/iter; left time: 7598.8294s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0867132\n",
      "\tspeed: 0.1197s/iter; left time: 7638.2616s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0877910\n",
      "\tspeed: 0.1177s/iter; left time: 7499.8711s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1008532\n",
      "\tspeed: 0.1194s/iter; left time: 7595.2404s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0882527\n",
      "\tspeed: 0.1193s/iter; left time: 7575.1135s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0798398\n",
      "\tspeed: 0.1182s/iter; left time: 7498.4859s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0708239\n",
      "\tspeed: 0.1203s/iter; left time: 7614.6941s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0720952\n",
      "\tspeed: 0.1156s/iter; left time: 7310.8871s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0832081\n",
      "\tspeed: 0.1173s/iter; left time: 7402.6487s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0772891\n",
      "\tspeed: 0.1173s/iter; left time: 7395.4665s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0615834\n",
      "\tspeed: 0.1181s/iter; left time: 7433.2315s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0647544\n",
      "\tspeed: 0.1197s/iter; left time: 7521.8139s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0650132\n",
      "\tspeed: 0.1182s/iter; left time: 7414.9879s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0766972\n",
      "\tspeed: 0.1205s/iter; left time: 7544.8569s\n",
      "Epoch: 6 cost time: 00h:08m:52.30s\n",
      "Epoch: 6 | Train Loss: 0.0813646 Vali Loss: 0.0895673 Test Loss: 0.0940461\n",
      "Validation loss decreased (0.089776 --> 0.089567).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0841343\n",
      "\tspeed: 1.6530s/iter; left time: 103235.3819s\n",
      "\titers: 200, epoch: 7 | loss: 0.0774401\n",
      "\tspeed: 0.1200s/iter; left time: 7484.1355s\n",
      "\titers: 300, epoch: 7 | loss: 0.0646475\n",
      "\tspeed: 0.1200s/iter; left time: 7471.3773s\n",
      "\titers: 400, epoch: 7 | loss: 0.0766103\n",
      "\tspeed: 0.1180s/iter; left time: 7334.7613s\n",
      "\titers: 500, epoch: 7 | loss: 0.0919630\n",
      "\tspeed: 0.1200s/iter; left time: 7444.7187s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806891\n",
      "\tspeed: 0.1211s/iter; left time: 7504.7698s\n",
      "\titers: 700, epoch: 7 | loss: 0.0947759\n",
      "\tspeed: 0.1203s/iter; left time: 7441.6268s\n",
      "\titers: 800, epoch: 7 | loss: 0.0886568\n",
      "\tspeed: 0.1175s/iter; left time: 7256.5548s\n",
      "\titers: 900, epoch: 7 | loss: 0.0786587\n",
      "\tspeed: 0.1190s/iter; left time: 7336.4875s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0800228\n",
      "\tspeed: 0.1187s/iter; left time: 7306.3839s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0899325\n",
      "\tspeed: 0.1206s/iter; left time: 7411.1211s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0851249\n",
      "\tspeed: 0.1214s/iter; left time: 7446.7356s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0694570\n",
      "\tspeed: 0.1176s/iter; left time: 7206.4126s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0889620\n",
      "\tspeed: 0.1177s/iter; left time: 7198.9649s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0885222\n",
      "\tspeed: 0.1168s/iter; left time: 7131.5664s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0703967\n",
      "\tspeed: 0.1198s/iter; left time: 7302.7558s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0967138\n",
      "\tspeed: 0.1184s/iter; left time: 7206.5879s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0742753\n",
      "\tspeed: 0.1198s/iter; left time: 7275.4256s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0714891\n",
      "\tspeed: 0.1200s/iter; left time: 7276.5838s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0690335\n",
      "\tspeed: 0.1203s/iter; left time: 7285.5115s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0778978\n",
      "\tspeed: 0.1159s/iter; left time: 7006.8084s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0716214\n",
      "\tspeed: 0.1175s/iter; left time: 7091.8659s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0747692\n",
      "\tspeed: 0.1168s/iter; left time: 7040.1868s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0834282\n",
      "\tspeed: 0.1175s/iter; left time: 7065.8316s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0855097\n",
      "\tspeed: 0.1192s/iter; left time: 7161.0485s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0906422\n",
      "\tspeed: 0.1192s/iter; left time: 7149.1376s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0708709\n",
      "\tspeed: 0.1180s/iter; left time: 7059.7972s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0859929\n",
      "\tspeed: 0.1210s/iter; left time: 7230.0027s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0687493\n",
      "\tspeed: 0.1198s/iter; left time: 7147.2599s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0796787\n",
      "\tspeed: 0.1212s/iter; left time: 7216.8474s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0671332\n",
      "\tspeed: 0.1196s/iter; left time: 7109.6244s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0683739\n",
      "\tspeed: 0.1167s/iter; left time: 6927.9818s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0691068\n",
      "\tspeed: 0.1165s/iter; left time: 6902.6622s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0798232\n",
      "\tspeed: 0.1187s/iter; left time: 7021.1931s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0687565\n",
      "\tspeed: 0.1172s/iter; left time: 6919.3256s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0730739\n",
      "\tspeed: 0.1182s/iter; left time: 6967.3658s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0768365\n",
      "\tspeed: 0.1170s/iter; left time: 6888.0754s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0917242\n",
      "\tspeed: 0.1202s/iter; left time: 7063.0170s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0721889\n",
      "\tspeed: 0.1199s/iter; left time: 7035.1511s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1057147\n",
      "\tspeed: 0.1143s/iter; left time: 6692.7616s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0877508\n",
      "\tspeed: 0.1129s/iter; left time: 6600.3418s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0842020\n",
      "\tspeed: 0.1130s/iter; left time: 6595.8754s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0788388\n",
      "\tspeed: 0.1125s/iter; left time: 6555.2348s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0780672\n",
      "\tspeed: 0.1129s/iter; left time: 6565.9990s\n",
      "Epoch: 7 cost time: 00h:08m:48.62s\n",
      "Epoch: 7 | Train Loss: 0.0802900 Vali Loss: 0.0892569 Test Loss: 0.0938624\n",
      "Validation loss decreased (0.089567 --> 0.089257).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0878868\n",
      "\tspeed: 1.5867s/iter; left time: 92005.2740s\n",
      "\titers: 200, epoch: 8 | loss: 0.0819466\n",
      "\tspeed: 0.1146s/iter; left time: 6632.6594s\n",
      "\titers: 300, epoch: 8 | loss: 0.0872314\n",
      "\tspeed: 0.1134s/iter; left time: 6552.0696s\n",
      "\titers: 400, epoch: 8 | loss: 0.0871727\n",
      "\tspeed: 0.1149s/iter; left time: 6629.7226s\n",
      "\titers: 500, epoch: 8 | loss: 0.0690386\n",
      "\tspeed: 0.1146s/iter; left time: 6600.8306s\n",
      "\titers: 600, epoch: 8 | loss: 0.0786107\n",
      "\tspeed: 0.1151s/iter; left time: 6617.8909s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654001\n",
      "\tspeed: 0.1152s/iter; left time: 6608.6635s\n",
      "\titers: 800, epoch: 8 | loss: 0.0706811\n",
      "\tspeed: 0.1148s/iter; left time: 6578.6997s\n",
      "\titers: 900, epoch: 8 | loss: 0.0823400\n",
      "\tspeed: 0.1177s/iter; left time: 6731.2845s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0656414\n",
      "\tspeed: 0.1133s/iter; left time: 6466.9469s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0717802\n",
      "\tspeed: 0.1147s/iter; left time: 6537.9668s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0914649\n",
      "\tspeed: 0.1137s/iter; left time: 6465.6763s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0788195\n",
      "\tspeed: 0.1150s/iter; left time: 6529.7991s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0938862\n",
      "\tspeed: 0.1143s/iter; left time: 6480.2686s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0883940\n",
      "\tspeed: 0.1134s/iter; left time: 6417.5553s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0923529\n",
      "\tspeed: 0.1169s/iter; left time: 6605.7957s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1006962\n",
      "\tspeed: 0.1165s/iter; left time: 6570.9238s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0933289\n",
      "\tspeed: 0.1140s/iter; left time: 6417.5089s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0580641\n",
      "\tspeed: 0.1138s/iter; left time: 6393.3432s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0778803\n",
      "\tspeed: 0.1146s/iter; left time: 6429.3977s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0813645\n",
      "\tspeed: 0.1118s/iter; left time: 6261.6275s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0837200\n",
      "\tspeed: 0.1152s/iter; left time: 6438.9706s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0852636\n",
      "\tspeed: 0.1148s/iter; left time: 6404.3086s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0721436\n",
      "\tspeed: 0.1121s/iter; left time: 6240.5479s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0740744\n",
      "\tspeed: 0.1138s/iter; left time: 6324.6577s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0824070\n",
      "\tspeed: 0.1133s/iter; left time: 6285.7820s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0793054\n",
      "\tspeed: 0.1134s/iter; left time: 6278.7559s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0864588\n",
      "\tspeed: 0.1141s/iter; left time: 6305.7187s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0741376\n",
      "\tspeed: 0.1149s/iter; left time: 6342.0597s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0899623\n",
      "\tspeed: 0.1143s/iter; left time: 6298.8285s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0805951\n",
      "\tspeed: 0.1152s/iter; left time: 6334.0774s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0612919\n",
      "\tspeed: 0.1161s/iter; left time: 6371.3833s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0753164\n",
      "\tspeed: 0.1166s/iter; left time: 6387.3990s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0797729\n",
      "\tspeed: 0.1152s/iter; left time: 6298.6305s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0591326\n",
      "\tspeed: 0.1120s/iter; left time: 6112.7558s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0769536\n",
      "\tspeed: 0.1158s/iter; left time: 6309.2520s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0768749\n",
      "\tspeed: 0.1139s/iter; left time: 6192.3924s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0756438\n",
      "\tspeed: 0.1140s/iter; left time: 6185.8640s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0782435\n",
      "\tspeed: 0.1146s/iter; left time: 6211.7814s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0735168\n",
      "\tspeed: 0.1134s/iter; left time: 6135.4879s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0852071\n",
      "\tspeed: 0.1144s/iter; left time: 6174.2145s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0820105\n",
      "\tspeed: 0.1162s/iter; left time: 6259.5710s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0806806\n",
      "\tspeed: 0.1163s/iter; left time: 6256.4244s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0883520\n",
      "\tspeed: 0.1131s/iter; left time: 6069.3030s\n",
      "Epoch: 8 cost time: 00h:08m:32.31s\n",
      "Epoch: 8 | Train Loss: 0.0793348 Vali Loss: 0.0892188 Test Loss: 0.0953779\n",
      "Validation loss decreased (0.089257 --> 0.089219).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0749750\n",
      "\tspeed: 1.5935s/iter; left time: 85278.7636s\n",
      "\titers: 200, epoch: 9 | loss: 0.0862554\n",
      "\tspeed: 0.1149s/iter; left time: 6139.0793s\n",
      "\titers: 300, epoch: 9 | loss: 0.0647535\n",
      "\tspeed: 0.1153s/iter; left time: 6147.8544s\n",
      "\titers: 400, epoch: 9 | loss: 0.0730983\n",
      "\tspeed: 0.1161s/iter; left time: 6177.1792s\n",
      "\titers: 500, epoch: 9 | loss: 0.0695380\n",
      "\tspeed: 0.1143s/iter; left time: 6073.9151s\n",
      "\titers: 600, epoch: 9 | loss: 0.0691377\n",
      "\tspeed: 0.1138s/iter; left time: 6031.9767s\n",
      "\titers: 700, epoch: 9 | loss: 0.0765528\n",
      "\tspeed: 0.1150s/iter; left time: 6086.4702s\n",
      "\titers: 800, epoch: 9 | loss: 0.0613169\n",
      "\tspeed: 0.1157s/iter; left time: 6112.7932s\n",
      "\titers: 900, epoch: 9 | loss: 0.0732772\n",
      "\tspeed: 0.1128s/iter; left time: 5946.8314s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0607491\n",
      "\tspeed: 0.1141s/iter; left time: 6005.3762s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0597287\n",
      "\tspeed: 0.1145s/iter; left time: 6014.2623s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0944503\n",
      "\tspeed: 0.1158s/iter; left time: 6068.9104s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0612135\n",
      "\tspeed: 0.1167s/iter; left time: 6104.9545s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0633327\n",
      "\tspeed: 0.1161s/iter; left time: 6061.6466s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0837415\n",
      "\tspeed: 0.1161s/iter; left time: 6052.5368s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0839318\n",
      "\tspeed: 0.1145s/iter; left time: 5957.2857s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0879491\n",
      "\tspeed: 0.1153s/iter; left time: 5987.8238s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0665905\n",
      "\tspeed: 0.1159s/iter; left time: 6007.1897s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0713467\n",
      "\tspeed: 0.1132s/iter; left time: 5856.2024s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0669027\n",
      "\tspeed: 0.1109s/iter; left time: 5722.6558s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0749533\n",
      "\tspeed: 0.1125s/iter; left time: 5795.3411s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0823501\n",
      "\tspeed: 0.1112s/iter; left time: 5718.2462s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0919251\n",
      "\tspeed: 0.1139s/iter; left time: 5844.6386s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0902282\n",
      "\tspeed: 0.1126s/iter; left time: 5768.5829s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0695416\n",
      "\tspeed: 0.1117s/iter; left time: 5711.8656s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0872029\n",
      "\tspeed: 0.1133s/iter; left time: 5778.4650s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0668999\n",
      "\tspeed: 0.1119s/iter; left time: 5698.7334s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0708064\n",
      "\tspeed: 0.1127s/iter; left time: 5725.9843s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0776473\n",
      "\tspeed: 0.1123s/iter; left time: 5694.7808s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0897343\n",
      "\tspeed: 0.1122s/iter; left time: 5681.0087s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0689395\n",
      "\tspeed: 0.1124s/iter; left time: 5679.9443s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0872946\n",
      "\tspeed: 0.1139s/iter; left time: 5744.6518s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0869129\n",
      "\tspeed: 0.1114s/iter; left time: 5604.8787s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0875106\n",
      "\tspeed: 0.1127s/iter; left time: 5660.6692s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0698793\n",
      "\tspeed: 0.1125s/iter; left time: 5636.6617s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0638036\n",
      "\tspeed: 0.1135s/iter; left time: 5676.5612s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0692865\n",
      "\tspeed: 0.1122s/iter; left time: 5601.1650s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0771837\n",
      "\tspeed: 0.1136s/iter; left time: 5659.3342s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0842480\n",
      "\tspeed: 0.1127s/iter; left time: 5605.2598s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0725595\n",
      "\tspeed: 0.1126s/iter; left time: 5588.1288s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0908936\n",
      "\tspeed: 0.1121s/iter; left time: 5549.9859s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0893085\n",
      "\tspeed: 0.1131s/iter; left time: 5589.3018s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0930462\n",
      "\tspeed: 0.1122s/iter; left time: 5534.2020s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0681519\n",
      "\tspeed: 0.1128s/iter; left time: 5549.7471s\n",
      "Epoch: 9 cost time: 00h:08m:28.34s\n",
      "Epoch: 9 | Train Loss: 0.0783692 Vali Loss: 0.0899408 Test Loss: 0.0950079\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0844053\n",
      "\tspeed: 1.5703s/iter; left time: 77023.2284s\n",
      "\titers: 200, epoch: 10 | loss: 0.1195817\n",
      "\tspeed: 0.1156s/iter; left time: 5658.7668s\n",
      "\titers: 300, epoch: 10 | loss: 0.0776654\n",
      "\tspeed: 0.1157s/iter; left time: 5653.6352s\n",
      "\titers: 400, epoch: 10 | loss: 0.0721945\n",
      "\tspeed: 0.1144s/iter; left time: 5577.8280s\n",
      "\titers: 500, epoch: 10 | loss: 0.0767422\n",
      "\tspeed: 0.1146s/iter; left time: 5576.6377s\n",
      "\titers: 600, epoch: 10 | loss: 0.0819118\n",
      "\tspeed: 0.1156s/iter; left time: 5610.0326s\n",
      "\titers: 700, epoch: 10 | loss: 0.0688704\n",
      "\tspeed: 0.1144s/iter; left time: 5540.6791s\n",
      "\titers: 800, epoch: 10 | loss: 0.1010805\n",
      "\tspeed: 0.1162s/iter; left time: 5619.3736s\n",
      "\titers: 900, epoch: 10 | loss: 0.0890266\n",
      "\tspeed: 0.1134s/iter; left time: 5473.4191s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0743395\n",
      "\tspeed: 0.1156s/iter; left time: 5567.6560s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0623439\n",
      "\tspeed: 0.1154s/iter; left time: 5542.6937s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0886193\n",
      "\tspeed: 0.1170s/iter; left time: 5610.6100s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0903579\n",
      "\tspeed: 0.1158s/iter; left time: 5541.5628s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0970001\n",
      "\tspeed: 0.1163s/iter; left time: 5550.9740s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0763897\n",
      "\tspeed: 0.1151s/iter; left time: 5483.7668s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0659897\n",
      "\tspeed: 0.1148s/iter; left time: 5460.3132s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0725043\n",
      "\tspeed: 0.1165s/iter; left time: 5526.4024s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0614413\n",
      "\tspeed: 0.1146s/iter; left time: 5425.1157s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0763310\n",
      "\tspeed: 0.1134s/iter; left time: 5356.3255s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0819842\n",
      "\tspeed: 0.1137s/iter; left time: 5362.4754s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0654081\n",
      "\tspeed: 0.1132s/iter; left time: 5325.6774s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0653410\n",
      "\tspeed: 0.1146s/iter; left time: 5380.6889s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0690481\n",
      "\tspeed: 0.1151s/iter; left time: 5390.4317s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0936227\n",
      "\tspeed: 0.1119s/iter; left time: 5232.1446s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0709767\n",
      "\tspeed: 0.1150s/iter; left time: 5366.5604s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0593837\n",
      "\tspeed: 0.1114s/iter; left time: 5186.2645s\n",
      "\titers: 2700, epoch: 10 | loss: 0.1029781\n",
      "\tspeed: 0.1120s/iter; left time: 5201.3187s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0660925\n",
      "\tspeed: 0.1124s/iter; left time: 5209.6535s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0810724\n",
      "\tspeed: 0.1154s/iter; left time: 5335.9264s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0861325\n",
      "\tspeed: 0.1147s/iter; left time: 5292.0603s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0749175\n",
      "\tspeed: 0.1137s/iter; left time: 5237.6337s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0772474\n",
      "\tspeed: 0.1164s/iter; left time: 5349.7385s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0663424\n",
      "\tspeed: 0.1137s/iter; left time: 5214.7671s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0681792\n",
      "\tspeed: 0.1141s/iter; left time: 5221.7219s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0826567\n",
      "\tspeed: 0.1140s/iter; left time: 5203.0824s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0750645\n",
      "\tspeed: 0.1131s/iter; left time: 5151.3906s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0935027\n",
      "\tspeed: 0.1151s/iter; left time: 5229.2264s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0815245\n",
      "\tspeed: 0.1150s/iter; left time: 5214.8541s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0724220\n",
      "\tspeed: 0.1144s/iter; left time: 5177.2966s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0900971\n",
      "\tspeed: 0.1154s/iter; left time: 5208.8186s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0711545\n",
      "\tspeed: 0.1136s/iter; left time: 5116.8096s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0712163\n",
      "\tspeed: 0.1162s/iter; left time: 5225.1819s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0879692\n",
      "\tspeed: 0.1138s/iter; left time: 5103.8100s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0614536\n",
      "\tspeed: 0.1149s/iter; left time: 5140.8918s\n",
      "Epoch: 10 cost time: 00h:08m:32.57s\n",
      "Epoch: 10 | Train Loss: 0.0775587 Vali Loss: 0.0896562 Test Loss: 0.0955003\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0843296\n",
      "\tspeed: 1.5739s/iter; left time: 70164.0240s\n",
      "\titers: 200, epoch: 11 | loss: 0.0733311\n",
      "\tspeed: 0.1145s/iter; left time: 5093.0879s\n",
      "\titers: 300, epoch: 11 | loss: 0.0601665\n",
      "\tspeed: 0.1165s/iter; left time: 5171.7973s\n",
      "\titers: 400, epoch: 11 | loss: 0.0716488\n",
      "\tspeed: 0.1158s/iter; left time: 5126.9430s\n",
      "\titers: 500, epoch: 11 | loss: 0.0750899\n",
      "\tspeed: 0.1165s/iter; left time: 5148.0669s\n",
      "\titers: 600, epoch: 11 | loss: 0.0896189\n",
      "\tspeed: 0.1153s/iter; left time: 5083.1043s\n",
      "\titers: 700, epoch: 11 | loss: 0.0716856\n",
      "\tspeed: 0.1139s/iter; left time: 5009.0067s\n",
      "\titers: 800, epoch: 11 | loss: 0.0796735\n",
      "\tspeed: 0.1153s/iter; left time: 5057.4234s\n",
      "\titers: 900, epoch: 11 | loss: 0.0532320\n",
      "\tspeed: 0.1148s/iter; left time: 5027.0354s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0739114\n",
      "\tspeed: 0.1165s/iter; left time: 5090.5324s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0947660\n",
      "\tspeed: 0.1156s/iter; left time: 5038.4828s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0835808\n",
      "\tspeed: 0.1145s/iter; left time: 4977.6359s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0754700\n",
      "\tspeed: 0.1149s/iter; left time: 4984.0475s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0658226\n",
      "\tspeed: 0.1150s/iter; left time: 4975.6781s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0543421\n",
      "\tspeed: 0.1149s/iter; left time: 4960.4601s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0710039\n",
      "\tspeed: 0.1151s/iter; left time: 4957.1768s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0833827\n",
      "\tspeed: 0.1159s/iter; left time: 4982.1454s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0674744\n",
      "\tspeed: 0.1155s/iter; left time: 4951.2929s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0692966\n",
      "\tspeed: 0.1154s/iter; left time: 4936.1148s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0941740\n",
      "\tspeed: 0.1148s/iter; left time: 4899.7874s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0860964\n",
      "\tspeed: 0.1159s/iter; left time: 4934.5398s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0810799\n",
      "\tspeed: 0.1154s/iter; left time: 4901.1828s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0745259\n",
      "\tspeed: 0.1157s/iter; left time: 4902.2137s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0968850\n",
      "\tspeed: 0.1147s/iter; left time: 4850.8245s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0774877\n",
      "\tspeed: 0.1128s/iter; left time: 4759.7731s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0719790\n",
      "\tspeed: 0.1129s/iter; left time: 4750.8537s\n",
      "\titers: 2700, epoch: 11 | loss: 0.0613416\n",
      "\tspeed: 0.1133s/iter; left time: 4755.9070s\n",
      "\titers: 2800, epoch: 11 | loss: 0.0705149\n",
      "\tspeed: 0.1153s/iter; left time: 4828.2350s\n",
      "\titers: 2900, epoch: 11 | loss: 0.0600997\n",
      "\tspeed: 0.1193s/iter; left time: 4982.8862s\n",
      "\titers: 3000, epoch: 11 | loss: 0.0838152\n",
      "\tspeed: 0.1120s/iter; left time: 4666.8976s\n",
      "\titers: 3100, epoch: 11 | loss: 0.0801776\n",
      "\tspeed: 0.1128s/iter; left time: 4691.7418s\n",
      "\titers: 3200, epoch: 11 | loss: 0.0818057\n",
      "\tspeed: 0.1120s/iter; left time: 4647.3603s\n",
      "\titers: 3300, epoch: 11 | loss: 0.0967920\n",
      "\tspeed: 0.1119s/iter; left time: 4631.3189s\n",
      "\titers: 3400, epoch: 11 | loss: 0.0851540\n",
      "\tspeed: 0.1118s/iter; left time: 4617.2067s\n",
      "\titers: 3500, epoch: 11 | loss: 0.0771672\n",
      "\tspeed: 0.1114s/iter; left time: 4589.0794s\n",
      "\titers: 3600, epoch: 11 | loss: 0.0722381\n",
      "\tspeed: 0.1124s/iter; left time: 4615.8247s\n",
      "\titers: 3700, epoch: 11 | loss: 0.0777316\n",
      "\tspeed: 0.1131s/iter; left time: 4634.9475s\n",
      "\titers: 3800, epoch: 11 | loss: 0.0811023\n",
      "\tspeed: 0.1122s/iter; left time: 4587.1586s\n",
      "\titers: 3900, epoch: 11 | loss: 0.0592589\n",
      "\tspeed: 0.1135s/iter; left time: 4629.6078s\n",
      "\titers: 4000, epoch: 11 | loss: 0.0921532\n",
      "\tspeed: 0.1125s/iter; left time: 4574.9062s\n",
      "\titers: 4100, epoch: 11 | loss: 0.0703947\n",
      "\tspeed: 0.1131s/iter; left time: 4591.6095s\n",
      "\titers: 4200, epoch: 11 | loss: 0.0643912\n",
      "\tspeed: 0.1144s/iter; left time: 4629.1487s\n",
      "\titers: 4300, epoch: 11 | loss: 0.0791592\n",
      "\tspeed: 0.1142s/iter; left time: 4612.0469s\n",
      "\titers: 4400, epoch: 11 | loss: 0.0683983\n",
      "\tspeed: 0.1123s/iter; left time: 4524.4935s\n",
      "Epoch: 11 cost time: 00h:08m:31.37s\n",
      "Epoch: 11 | Train Loss: 0.0766384 Vali Loss: 0.0910036 Test Loss: 0.0970744\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0710792\n",
      "\tspeed: 1.5636s/iter; left time: 62721.0444s\n",
      "\titers: 200, epoch: 12 | loss: 0.0842273\n",
      "\tspeed: 0.1133s/iter; left time: 4533.8282s\n",
      "\titers: 300, epoch: 12 | loss: 0.0629760\n",
      "\tspeed: 0.1142s/iter; left time: 4557.5035s\n",
      "\titers: 400, epoch: 12 | loss: 0.0810391\n",
      "\tspeed: 0.1137s/iter; left time: 4525.0781s\n",
      "\titers: 500, epoch: 12 | loss: 0.1013014\n",
      "\tspeed: 0.1144s/iter; left time: 4542.3369s\n",
      "\titers: 600, epoch: 12 | loss: 0.0584819\n",
      "\tspeed: 0.1137s/iter; left time: 4503.4232s\n",
      "\titers: 700, epoch: 12 | loss: 0.0732251\n",
      "\tspeed: 0.1132s/iter; left time: 4471.8834s\n",
      "\titers: 800, epoch: 12 | loss: 0.0754556\n",
      "\tspeed: 0.1120s/iter; left time: 4413.5894s\n",
      "\titers: 900, epoch: 12 | loss: 0.0640929\n",
      "\tspeed: 0.1135s/iter; left time: 4461.7356s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0760040\n",
      "\tspeed: 0.1133s/iter; left time: 4442.3115s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0927217\n",
      "\tspeed: 0.1133s/iter; left time: 4433.2293s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0876345\n",
      "\tspeed: 0.1150s/iter; left time: 4487.2739s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0746380\n",
      "\tspeed: 0.1136s/iter; left time: 4422.2023s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0757993\n",
      "\tspeed: 0.1150s/iter; left time: 4463.8057s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0810570\n",
      "\tspeed: 0.1153s/iter; left time: 4464.1727s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0619826\n",
      "\tspeed: 0.1155s/iter; left time: 4460.3334s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0746262\n",
      "\tspeed: 0.1122s/iter; left time: 4321.5653s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0823452\n",
      "\tspeed: 0.1131s/iter; left time: 4343.0094s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0711706\n",
      "\tspeed: 0.1119s/iter; left time: 4286.5702s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0654976\n",
      "\tspeed: 0.1114s/iter; left time: 4257.2980s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0801418\n",
      "\tspeed: 0.1130s/iter; left time: 4308.3075s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0796125\n",
      "\tspeed: 0.1143s/iter; left time: 4345.9646s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0880586\n",
      "\tspeed: 0.1140s/iter; left time: 4322.5583s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0664747\n",
      "\tspeed: 0.1128s/iter; left time: 4266.0571s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685178\n",
      "\tspeed: 0.1133s/iter; left time: 4272.3721s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0550742\n",
      "\tspeed: 0.1131s/iter; left time: 4252.6892s\n",
      "\titers: 2700, epoch: 12 | loss: 0.0827314\n",
      "\tspeed: 0.1116s/iter; left time: 4186.3833s\n",
      "\titers: 2800, epoch: 12 | loss: 0.0665130\n",
      "\tspeed: 0.1129s/iter; left time: 4225.7916s\n",
      "\titers: 2900, epoch: 12 | loss: 0.0725770\n",
      "\tspeed: 0.1153s/iter; left time: 4301.2409s\n",
      "\titers: 3000, epoch: 12 | loss: 0.0761487\n",
      "\tspeed: 0.1131s/iter; left time: 4207.9793s\n",
      "\titers: 3100, epoch: 12 | loss: 0.0791091\n",
      "\tspeed: 0.1113s/iter; left time: 4129.1158s\n",
      "\titers: 3200, epoch: 12 | loss: 0.0671570\n",
      "\tspeed: 0.1132s/iter; left time: 4188.6470s\n",
      "\titers: 3300, epoch: 12 | loss: 0.0749104\n",
      "\tspeed: 0.1108s/iter; left time: 4090.0929s\n",
      "\titers: 3400, epoch: 12 | loss: 0.0909807\n",
      "\tspeed: 0.1113s/iter; left time: 4097.7149s\n",
      "\titers: 3500, epoch: 12 | loss: 0.0923854\n",
      "\tspeed: 0.1126s/iter; left time: 4132.9393s\n",
      "\titers: 3600, epoch: 12 | loss: 0.0831650\n",
      "\tspeed: 0.1131s/iter; left time: 4139.3141s\n",
      "\titers: 3700, epoch: 12 | loss: 0.0813926\n",
      "\tspeed: 0.1130s/iter; left time: 4127.1017s\n",
      "\titers: 3800, epoch: 12 | loss: 0.0719712\n",
      "\tspeed: 0.1118s/iter; left time: 4070.7835s\n",
      "\titers: 3900, epoch: 12 | loss: 0.0697064\n",
      "\tspeed: 0.1118s/iter; left time: 4060.5863s\n",
      "\titers: 4000, epoch: 12 | loss: 0.0884542\n",
      "\tspeed: 0.1128s/iter; left time: 4084.2641s\n",
      "\titers: 4100, epoch: 12 | loss: 0.0673367\n",
      "\tspeed: 0.1129s/iter; left time: 4075.9061s\n",
      "\titers: 4200, epoch: 12 | loss: 0.0795303\n",
      "\tspeed: 0.1130s/iter; left time: 4069.6429s\n",
      "\titers: 4300, epoch: 12 | loss: 0.0724135\n",
      "\tspeed: 0.1119s/iter; left time: 4018.1664s\n",
      "\titers: 4400, epoch: 12 | loss: 0.0837259\n",
      "\tspeed: 0.1092s/iter; left time: 3912.5087s\n",
      "Epoch: 12 cost time: 00h:08m:26.23s\n",
      "Epoch: 12 | Train Loss: 0.0758073 Vali Loss: 0.0911360 Test Loss: 0.0977819\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0696547\n",
      "\tspeed: 1.5893s/iter; left time: 56649.7380s\n",
      "\titers: 200, epoch: 13 | loss: 0.0783792\n",
      "\tspeed: 0.1172s/iter; left time: 4164.4258s\n",
      "\titers: 300, epoch: 13 | loss: 0.0612777\n",
      "\tspeed: 0.1179s/iter; left time: 4178.9516s\n",
      "\titers: 400, epoch: 13 | loss: 0.0669738\n",
      "\tspeed: 0.1177s/iter; left time: 4158.8186s\n",
      "\titers: 500, epoch: 13 | loss: 0.0753981\n",
      "\tspeed: 0.1177s/iter; left time: 4147.4482s\n",
      "\titers: 600, epoch: 13 | loss: 0.0703675\n",
      "\tspeed: 0.1153s/iter; left time: 4053.7467s\n",
      "\titers: 700, epoch: 13 | loss: 0.0807081\n",
      "\tspeed: 0.1164s/iter; left time: 4077.7579s\n",
      "\titers: 800, epoch: 13 | loss: 0.0631655\n",
      "\tspeed: 0.1182s/iter; left time: 4130.4168s\n",
      "\titers: 900, epoch: 13 | loss: 0.0665464\n",
      "\tspeed: 0.1170s/iter; left time: 4075.9742s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0889466\n",
      "\tspeed: 0.1150s/iter; left time: 3996.6718s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0937114\n",
      "\tspeed: 0.1159s/iter; left time: 4013.9217s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0855564\n",
      "\tspeed: 0.1143s/iter; left time: 3949.4810s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0802188\n",
      "\tspeed: 0.1161s/iter; left time: 3999.4400s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0752418\n",
      "\tspeed: 0.1170s/iter; left time: 4019.6635s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0920139\n",
      "\tspeed: 0.1172s/iter; left time: 4013.8930s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0802863\n",
      "\tspeed: 0.1177s/iter; left time: 4019.6514s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0784608\n",
      "\tspeed: 0.1200s/iter; left time: 4086.0264s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0880799\n",
      "\tspeed: 0.1172s/iter; left time: 3979.4404s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0902761\n",
      "\tspeed: 0.1177s/iter; left time: 3984.6787s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0855317\n",
      "\tspeed: 0.1162s/iter; left time: 3920.8780s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0778534\n",
      "\tspeed: 0.1169s/iter; left time: 3934.1143s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0732491\n",
      "\tspeed: 0.1166s/iter; left time: 3910.2686s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0718696\n",
      "\tspeed: 0.1174s/iter; left time: 3927.0261s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643891\n",
      "\tspeed: 0.1167s/iter; left time: 3891.0356s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0634482\n",
      "\tspeed: 0.1176s/iter; left time: 3909.0249s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0696910\n",
      "\tspeed: 0.1181s/iter; left time: 3915.0711s\n",
      "\titers: 2700, epoch: 13 | loss: 0.0817821\n",
      "\tspeed: 0.1158s/iter; left time: 3825.6326s\n",
      "\titers: 2800, epoch: 13 | loss: 0.0640855\n",
      "\tspeed: 0.1157s/iter; left time: 3811.4909s\n",
      "\titers: 2900, epoch: 13 | loss: 0.0694307\n",
      "\tspeed: 0.1160s/iter; left time: 3810.3136s\n",
      "\titers: 3000, epoch: 13 | loss: 0.0864336\n",
      "\tspeed: 0.1171s/iter; left time: 3835.8686s\n",
      "\titers: 3100, epoch: 13 | loss: 0.0733568\n",
      "\tspeed: 0.1175s/iter; left time: 3835.4461s\n",
      "\titers: 3200, epoch: 13 | loss: 0.0625381\n",
      "\tspeed: 0.1159s/iter; left time: 3770.5600s\n",
      "\titers: 3300, epoch: 13 | loss: 0.0875661\n",
      "\tspeed: 0.1187s/iter; left time: 3852.2250s\n",
      "\titers: 3400, epoch: 13 | loss: 0.0675188\n",
      "\tspeed: 0.1180s/iter; left time: 3818.2630s\n",
      "\titers: 3500, epoch: 13 | loss: 0.0647626\n",
      "\tspeed: 0.1167s/iter; left time: 3763.5709s\n",
      "\titers: 3600, epoch: 13 | loss: 0.0939891\n",
      "\tspeed: 0.1169s/iter; left time: 3756.8278s\n",
      "\titers: 3700, epoch: 13 | loss: 0.0905394\n",
      "\tspeed: 0.1158s/iter; left time: 3709.2596s\n",
      "\titers: 3800, epoch: 13 | loss: 0.0685588\n",
      "\tspeed: 0.1173s/iter; left time: 3748.1331s\n",
      "\titers: 3900, epoch: 13 | loss: 0.0648391\n",
      "\tspeed: 0.1175s/iter; left time: 3740.8344s\n",
      "\titers: 4000, epoch: 13 | loss: 0.0833651\n",
      "\tspeed: 0.1160s/iter; left time: 3682.8446s\n",
      "\titers: 4100, epoch: 13 | loss: 0.0707878\n",
      "\tspeed: 0.1176s/iter; left time: 3722.4901s\n",
      "\titers: 4200, epoch: 13 | loss: 0.0837740\n",
      "\tspeed: 0.1175s/iter; left time: 3707.5046s\n",
      "\titers: 4300, epoch: 13 | loss: 0.0526903\n",
      "\tspeed: 0.1160s/iter; left time: 3649.0515s\n",
      "\titers: 4400, epoch: 13 | loss: 0.0681901\n",
      "\tspeed: 0.1168s/iter; left time: 3661.2333s\n",
      "Epoch: 13 cost time: 00h:08m:43.12s\n",
      "Epoch: 13 | Train Loss: 0.0749393 Vali Loss: 0.0928503 Test Loss: 0.1000453\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02273400127887726, rmse:0.15077798068523407, mae:0.09537788480520248, rse:0.5325193405151367\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 24: 02h:24m:39.95s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-03 03:36:47,323] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 03:36:48,374] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 03:36:48,374] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 03:36:48,374] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 03:36:48,464] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 03:36:48,464] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 03:36:49,208] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 03:36:49,210] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 03:36:49,210] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 03:36:49,211] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 03:36:49,211] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 03:36:49,212] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 03:36:49,660] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 03:36:49,661] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,661] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.86 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,845] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 03:36:49,846] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,846] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.8 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,846] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 03:36:49,986] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 03:36:49,987] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,987] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.83 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5e609dadd0>\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1857200\n",
      "\tspeed: 0.1749s/iter; left time: 15577.3743s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687170\n",
      "\tspeed: 0.1288s/iter; left time: 11457.6404s\n",
      "\titers: 300, epoch: 1 | loss: 0.1724844\n",
      "\tspeed: 0.1281s/iter; left time: 11376.5133s\n",
      "\titers: 400, epoch: 1 | loss: 0.1656757\n",
      "\tspeed: 0.1285s/iter; left time: 11406.8328s\n",
      "\titers: 500, epoch: 1 | loss: 0.1675965\n",
      "\tspeed: 0.1308s/iter; left time: 11595.5959s\n",
      "\titers: 600, epoch: 1 | loss: 0.1443687\n",
      "\tspeed: 0.1283s/iter; left time: 11360.4195s\n",
      "\titers: 700, epoch: 1 | loss: 0.1422241\n",
      "\tspeed: 0.1294s/iter; left time: 11444.9010s\n",
      "\titers: 800, epoch: 1 | loss: 0.1226619\n",
      "\tspeed: 0.1311s/iter; left time: 11583.9627s\n",
      "\titers: 900, epoch: 1 | loss: 0.1330729\n",
      "\tspeed: 0.1320s/iter; left time: 11645.3274s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1138731\n",
      "\tspeed: 0.1279s/iter; left time: 11277.5582s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1071478\n",
      "\tspeed: 0.1286s/iter; left time: 11321.9161s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1303404\n",
      "\tspeed: 0.1277s/iter; left time: 11226.3168s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1238685\n",
      "\tspeed: 0.1295s/iter; left time: 11371.7322s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1032463\n",
      "\tspeed: 0.1281s/iter; left time: 11238.4410s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1194469\n",
      "\tspeed: 0.1278s/iter; left time: 11204.7918s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1207578\n",
      "\tspeed: 0.1295s/iter; left time: 11336.0300s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1394709\n",
      "\tspeed: 0.1296s/iter; left time: 11329.0578s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1160002\n",
      "\tspeed: 0.1284s/iter; left time: 11216.9853s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1103131\n",
      "\tspeed: 0.1283s/iter; left time: 11196.2989s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1130958\n",
      "\tspeed: 0.1258s/iter; left time: 10963.6410s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1089309\n",
      "\tspeed: 0.1278s/iter; left time: 11119.7526s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1244033\n",
      "\tspeed: 0.1307s/iter; left time: 11360.0311s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1284142\n",
      "\tspeed: 0.1305s/iter; left time: 11330.0133s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1186614\n",
      "\tspeed: 0.1310s/iter; left time: 11359.6028s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1169964\n",
      "\tspeed: 0.1281s/iter; left time: 11094.6381s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1132365\n",
      "\tspeed: 0.1305s/iter; left time: 11294.7278s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1326390\n",
      "\tspeed: 0.1299s/iter; left time: 11227.3974s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1291778\n",
      "\tspeed: 0.1271s/iter; left time: 10971.6094s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1308049\n",
      "\tspeed: 0.1309s/iter; left time: 11287.2538s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1142576\n",
      "\tspeed: 0.1295s/iter; left time: 11153.1138s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0980470\n",
      "\tspeed: 0.1265s/iter; left time: 10886.4749s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1026124\n",
      "\tspeed: 0.1282s/iter; left time: 11013.8344s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1019026\n",
      "\tspeed: 0.1289s/iter; left time: 11065.1453s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1156720\n",
      "\tspeed: 0.1297s/iter; left time: 11123.8672s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1253485\n",
      "\tspeed: 0.1298s/iter; left time: 11116.5148s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1046039\n",
      "\tspeed: 0.1302s/iter; left time: 11137.7349s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1413063\n",
      "\tspeed: 0.1318s/iter; left time: 11263.1589s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1070143\n",
      "\tspeed: 0.1292s/iter; left time: 11023.9143s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1359391\n",
      "\tspeed: 0.1286s/iter; left time: 10961.5411s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1120188\n",
      "\tspeed: 0.1300s/iter; left time: 11066.3841s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1373022\n",
      "\tspeed: 0.1296s/iter; left time: 11017.7127s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1048007\n",
      "\tspeed: 0.1290s/iter; left time: 10959.8018s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1046956\n",
      "\tspeed: 0.1312s/iter; left time: 11129.0249s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1285643\n",
      "\tspeed: 0.1288s/iter; left time: 10912.1558s\n",
      "Epoch: 1 cost time: 00h:09m:37.13s\n",
      "Epoch: 1 | Train Loss: 0.1245267 Vali Loss: 0.1197564 Test Loss: 0.1277876\n",
      "Validation loss decreased (inf --> 0.119756).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1362824\n",
      "\tspeed: 1.8577s/iter; left time: 157131.2041s\n",
      "\titers: 200, epoch: 2 | loss: 0.1034659\n",
      "\tspeed: 0.1181s/iter; left time: 9979.1355s\n",
      "\titers: 300, epoch: 2 | loss: 0.1078680\n",
      "\tspeed: 0.1176s/iter; left time: 9924.3209s\n",
      "\titers: 400, epoch: 2 | loss: 0.1090891\n",
      "\tspeed: 0.1186s/iter; left time: 9997.2466s\n",
      "\titers: 500, epoch: 2 | loss: 0.0995298\n",
      "\tspeed: 0.1178s/iter; left time: 9912.9384s\n",
      "\titers: 600, epoch: 2 | loss: 0.1253273\n",
      "\tspeed: 0.1185s/iter; left time: 9967.8242s\n",
      "\titers: 700, epoch: 2 | loss: 0.1308996\n",
      "\tspeed: 0.1184s/iter; left time: 9945.0339s\n",
      "\titers: 800, epoch: 2 | loss: 0.1199580\n",
      "\tspeed: 0.1172s/iter; left time: 9827.3807s\n",
      "\titers: 900, epoch: 2 | loss: 0.1126412\n",
      "\tspeed: 0.1192s/iter; left time: 9990.0129s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1188750\n",
      "\tspeed: 0.1181s/iter; left time: 9884.5111s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1189725\n",
      "\tspeed: 0.1185s/iter; left time: 9905.0032s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1217855\n",
      "\tspeed: 0.1181s/iter; left time: 9861.4114s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1159035\n",
      "\tspeed: 0.1170s/iter; left time: 9752.8977s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0984060\n",
      "\tspeed: 0.1196s/iter; left time: 9960.2194s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0966178\n",
      "\tspeed: 0.1195s/iter; left time: 9938.6067s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1044325\n",
      "\tspeed: 0.1182s/iter; left time: 9816.7587s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1260282\n",
      "\tspeed: 0.1179s/iter; left time: 9784.6770s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1024257\n",
      "\tspeed: 0.1167s/iter; left time: 9671.8226s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0874435\n",
      "\tspeed: 0.1193s/iter; left time: 9873.2542s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1199540\n",
      "\tspeed: 0.1158s/iter; left time: 9576.7465s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1297425\n",
      "\tspeed: 0.1176s/iter; left time: 9711.1511s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1000140\n",
      "\tspeed: 0.1163s/iter; left time: 9589.7147s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1035635\n",
      "\tspeed: 0.1180s/iter; left time: 9723.6046s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1033574\n",
      "\tspeed: 0.1183s/iter; left time: 9735.6153s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1072888\n",
      "\tspeed: 0.1175s/iter; left time: 9658.7560s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1190963\n",
      "\tspeed: 0.1174s/iter; left time: 9636.4540s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1066877\n",
      "\tspeed: 0.1181s/iter; left time: 9686.3330s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1129730\n",
      "\tspeed: 0.1128s/iter; left time: 9237.4035s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1384397\n",
      "\tspeed: 0.1173s/iter; left time: 9596.9501s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1114533\n",
      "\tspeed: 0.1192s/iter; left time: 9732.7134s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1163946\n",
      "\tspeed: 0.1176s/iter; left time: 9597.6541s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1102893\n",
      "\tspeed: 0.1175s/iter; left time: 9575.9749s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0999768\n",
      "\tspeed: 0.1159s/iter; left time: 9432.5478s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0817148\n",
      "\tspeed: 0.1176s/iter; left time: 9559.8251s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1148444\n",
      "\tspeed: 0.1182s/iter; left time: 9593.3125s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1212692\n",
      "\tspeed: 0.1160s/iter; left time: 9403.9745s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1155532\n",
      "\tspeed: 0.1158s/iter; left time: 9379.7291s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1284737\n",
      "\tspeed: 0.1156s/iter; left time: 9351.2049s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1278360\n",
      "\tspeed: 0.1177s/iter; left time: 9505.1319s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092012\n",
      "\tspeed: 0.1165s/iter; left time: 9396.7610s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0933397\n",
      "\tspeed: 0.1175s/iter; left time: 9467.4941s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1042761\n",
      "\tspeed: 0.1169s/iter; left time: 9407.1105s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1036103\n",
      "\tspeed: 0.1155s/iter; left time: 9285.9710s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0886638\n",
      "\tspeed: 0.1162s/iter; left time: 9326.1864s\n",
      "Epoch: 2 cost time: 00h:08m:44.44s\n",
      "Epoch: 2 | Train Loss: 0.1098625 Vali Loss: 0.1185164 Test Loss: 0.1282422\n",
      "Validation loss decreased (0.119756 --> 0.118516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1121855\n",
      "\tspeed: 1.6093s/iter; left time: 128949.3745s\n",
      "\titers: 200, epoch: 3 | loss: 0.0993601\n",
      "\tspeed: 0.1182s/iter; left time: 9455.5526s\n",
      "\titers: 300, epoch: 3 | loss: 0.0954742\n",
      "\tspeed: 0.1185s/iter; left time: 9471.8841s\n",
      "\titers: 400, epoch: 3 | loss: 0.1022189\n",
      "\tspeed: 0.1167s/iter; left time: 9318.9562s\n",
      "\titers: 500, epoch: 3 | loss: 0.1028623\n",
      "\tspeed: 0.1179s/iter; left time: 9400.1779s\n",
      "\titers: 600, epoch: 3 | loss: 0.1073780\n",
      "\tspeed: 0.1188s/iter; left time: 9455.7653s\n",
      "\titers: 700, epoch: 3 | loss: 0.1252172\n",
      "\tspeed: 0.1184s/iter; left time: 9415.0870s\n",
      "\titers: 800, epoch: 3 | loss: 0.0985473\n",
      "\tspeed: 0.1175s/iter; left time: 9332.2909s\n",
      "\titers: 900, epoch: 3 | loss: 0.1068274\n",
      "\tspeed: 0.1197s/iter; left time: 9496.0076s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1010967\n",
      "\tspeed: 0.1183s/iter; left time: 9374.0746s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1209155\n",
      "\tspeed: 0.1169s/iter; left time: 9252.1070s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1471111\n",
      "\tspeed: 0.1176s/iter; left time: 9295.8203s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0935464\n",
      "\tspeed: 0.1189s/iter; left time: 9381.0565s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1135274\n",
      "\tspeed: 0.1186s/iter; left time: 9352.5755s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0995740\n",
      "\tspeed: 0.1168s/iter; left time: 9194.3072s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0949781\n",
      "\tspeed: 0.1172s/iter; left time: 9212.4557s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1161713\n",
      "\tspeed: 0.1180s/iter; left time: 9264.1771s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1313455\n",
      "\tspeed: 0.1162s/iter; left time: 9111.7358s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1055434\n",
      "\tspeed: 0.1190s/iter; left time: 9317.3203s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1189179\n",
      "\tspeed: 0.1176s/iter; left time: 9198.3426s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0911239\n",
      "\tspeed: 0.1181s/iter; left time: 9227.0430s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1390544\n",
      "\tspeed: 0.1160s/iter; left time: 9050.4505s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976654\n",
      "\tspeed: 0.1182s/iter; left time: 9211.7403s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1079981\n",
      "\tspeed: 0.1198s/iter; left time: 9321.4916s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1111981\n",
      "\tspeed: 0.1177s/iter; left time: 9145.8877s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1110975\n",
      "\tspeed: 0.1151s/iter; left time: 8936.2508s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1392076\n",
      "\tspeed: 0.1162s/iter; left time: 9005.2980s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1271442\n",
      "\tspeed: 0.1160s/iter; left time: 8980.1116s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1141754\n",
      "\tspeed: 0.1152s/iter; left time: 8905.0857s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1144832\n",
      "\tspeed: 0.1152s/iter; left time: 8899.4490s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0868490\n",
      "\tspeed: 0.1183s/iter; left time: 9124.7607s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1013458\n",
      "\tspeed: 0.1181s/iter; left time: 9098.0172s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0867222\n",
      "\tspeed: 0.1157s/iter; left time: 8899.0869s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0821788\n",
      "\tspeed: 0.1162s/iter; left time: 8927.4794s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1050519\n",
      "\tspeed: 0.1156s/iter; left time: 8873.3044s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1225686\n",
      "\tspeed: 0.1185s/iter; left time: 9080.4483s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1083653\n",
      "\tspeed: 0.1174s/iter; left time: 8983.1473s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0852168\n",
      "\tspeed: 0.1169s/iter; left time: 8936.1334s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1460541\n",
      "\tspeed: 0.1177s/iter; left time: 8986.7995s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0892489\n",
      "\tspeed: 0.1159s/iter; left time: 8836.6297s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1043740\n",
      "\tspeed: 0.1151s/iter; left time: 8763.5466s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1233985\n",
      "\tspeed: 0.1171s/iter; left time: 8903.0900s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1201573\n",
      "\tspeed: 0.1143s/iter; left time: 8682.1961s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0884113\n",
      "\tspeed: 0.1163s/iter; left time: 8815.0683s\n",
      "Epoch: 3 cost time: 00h:08m:43.20s\n",
      "Epoch: 3 | Train Loss: 0.1059200 Vali Loss: 0.1199043 Test Loss: 0.1320190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1017862\n",
      "\tspeed: 1.5936s/iter; left time: 120585.6451s\n",
      "\titers: 200, epoch: 4 | loss: 0.1054816\n",
      "\tspeed: 0.1159s/iter; left time: 8755.5299s\n",
      "\titers: 300, epoch: 4 | loss: 0.1008089\n",
      "\tspeed: 0.1170s/iter; left time: 8831.5805s\n",
      "\titers: 400, epoch: 4 | loss: 0.1103006\n",
      "\tspeed: 0.1153s/iter; left time: 8688.9430s\n",
      "\titers: 500, epoch: 4 | loss: 0.1008416\n",
      "\tspeed: 0.1145s/iter; left time: 8619.2124s\n",
      "\titers: 600, epoch: 4 | loss: 0.0872107\n",
      "\tspeed: 0.1160s/iter; left time: 8722.1032s\n",
      "\titers: 700, epoch: 4 | loss: 0.1059361\n",
      "\tspeed: 0.1195s/iter; left time: 8968.0293s\n",
      "\titers: 800, epoch: 4 | loss: 0.1094390\n",
      "\tspeed: 0.1181s/iter; left time: 8851.8553s\n",
      "\titers: 900, epoch: 4 | loss: 0.1180137\n",
      "\tspeed: 0.1181s/iter; left time: 8844.5917s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1055246\n",
      "\tspeed: 0.1197s/iter; left time: 8947.2815s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1000077\n",
      "\tspeed: 0.1173s/iter; left time: 8759.0721s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0994589\n",
      "\tspeed: 0.1172s/iter; left time: 8735.9921s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1067580\n",
      "\tspeed: 0.1171s/iter; left time: 8721.4622s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0998731\n",
      "\tspeed: 0.1158s/iter; left time: 8613.1813s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1033912\n",
      "\tspeed: 0.1119s/iter; left time: 8308.1780s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0937374\n",
      "\tspeed: 0.1107s/iter; left time: 8208.9945s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0970818\n",
      "\tspeed: 0.1124s/iter; left time: 8327.5368s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1050920\n",
      "\tspeed: 0.1120s/iter; left time: 8285.6897s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1291669\n",
      "\tspeed: 0.1121s/iter; left time: 8279.0598s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0771152\n",
      "\tspeed: 0.1133s/iter; left time: 8356.6795s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0940207\n",
      "\tspeed: 0.1136s/iter; left time: 8366.1464s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0884763\n",
      "\tspeed: 0.1108s/iter; left time: 8149.7752s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0902145\n",
      "\tspeed: 0.1114s/iter; left time: 8187.5958s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1145035\n",
      "\tspeed: 0.1129s/iter; left time: 8281.5547s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1050488\n",
      "\tspeed: 0.1144s/iter; left time: 8378.4388s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0954266\n",
      "\tspeed: 0.1140s/iter; left time: 8343.4925s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0995934\n",
      "\tspeed: 0.1122s/iter; left time: 8201.3794s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1045068\n",
      "\tspeed: 0.1108s/iter; left time: 8082.1594s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0896091\n",
      "\tspeed: 0.1110s/iter; left time: 8087.5701s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1133813\n",
      "\tspeed: 0.1100s/iter; left time: 8006.8918s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1004615\n",
      "\tspeed: 0.1144s/iter; left time: 8311.1747s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1033023\n",
      "\tspeed: 0.1111s/iter; left time: 8059.7486s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1076910\n",
      "\tspeed: 0.1126s/iter; left time: 8158.9434s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0782520\n",
      "\tspeed: 0.1114s/iter; left time: 8063.5273s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0946225\n",
      "\tspeed: 0.1116s/iter; left time: 8068.2659s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1047806\n",
      "\tspeed: 0.1135s/iter; left time: 8194.1758s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0901383\n",
      "\tspeed: 0.1132s/iter; left time: 8157.3959s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0860445\n",
      "\tspeed: 0.1126s/iter; left time: 8101.8086s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0885248\n",
      "\tspeed: 0.1116s/iter; left time: 8023.3105s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1189710\n",
      "\tspeed: 0.1109s/iter; left time: 7958.3027s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0907215\n",
      "\tspeed: 0.1112s/iter; left time: 7967.6398s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1236514\n",
      "\tspeed: 0.1111s/iter; left time: 7950.3721s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1064944\n",
      "\tspeed: 0.1120s/iter; left time: 8004.7838s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0968458\n",
      "\tspeed: 0.1119s/iter; left time: 7984.6857s\n",
      "Epoch: 4 cost time: 00h:08m:27.07s\n",
      "Epoch: 4 | Train Loss: 0.1021763 Vali Loss: 0.1203913 Test Loss: 0.1309304\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0960487\n",
      "\tspeed: 1.5315s/iter; left time: 109064.0538s\n",
      "\titers: 200, epoch: 5 | loss: 0.0997009\n",
      "\tspeed: 0.1119s/iter; left time: 7958.2026s\n",
      "\titers: 300, epoch: 5 | loss: 0.1327493\n",
      "\tspeed: 0.1136s/iter; left time: 8067.9951s\n",
      "\titers: 400, epoch: 5 | loss: 0.1110552\n",
      "\tspeed: 0.1112s/iter; left time: 7884.0786s\n",
      "\titers: 500, epoch: 5 | loss: 0.0997856\n",
      "\tspeed: 0.1119s/iter; left time: 7927.2165s\n",
      "\titers: 600, epoch: 5 | loss: 0.1085910\n",
      "\tspeed: 0.1141s/iter; left time: 8068.7005s\n",
      "\titers: 700, epoch: 5 | loss: 0.0906752\n",
      "\tspeed: 0.1124s/iter; left time: 7937.0686s\n",
      "\titers: 800, epoch: 5 | loss: 0.0881957\n",
      "\tspeed: 0.1132s/iter; left time: 7983.2926s\n",
      "\titers: 900, epoch: 5 | loss: 0.1219619\n",
      "\tspeed: 0.1158s/iter; left time: 8154.0782s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1071277\n",
      "\tspeed: 0.1137s/iter; left time: 7997.3908s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1084025\n",
      "\tspeed: 0.1156s/iter; left time: 8117.4470s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0964981\n",
      "\tspeed: 0.1152s/iter; left time: 8076.2562s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0842596\n",
      "\tspeed: 0.1136s/iter; left time: 7952.2301s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0926249\n",
      "\tspeed: 0.1134s/iter; left time: 7929.3991s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0907346\n",
      "\tspeed: 0.1130s/iter; left time: 7888.9182s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1106110\n",
      "\tspeed: 0.1145s/iter; left time: 7981.8965s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0988417\n",
      "\tspeed: 0.1146s/iter; left time: 7978.9288s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0920804\n",
      "\tspeed: 0.1143s/iter; left time: 7944.8159s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0919630\n",
      "\tspeed: 0.1155s/iter; left time: 8015.5406s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1004907\n",
      "\tspeed: 0.1155s/iter; left time: 8005.2387s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1181042\n",
      "\tspeed: 0.1144s/iter; left time: 7914.6309s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0876759\n",
      "\tspeed: 0.1135s/iter; left time: 7840.9967s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1073024\n",
      "\tspeed: 0.1145s/iter; left time: 7900.6209s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0821408\n",
      "\tspeed: 0.1135s/iter; left time: 7818.9375s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0953626\n",
      "\tspeed: 0.1153s/iter; left time: 7932.7214s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1104295\n",
      "\tspeed: 0.1141s/iter; left time: 7843.2803s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1122409\n",
      "\tspeed: 0.1150s/iter; left time: 7888.2967s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0743636\n",
      "\tspeed: 0.1139s/iter; left time: 7800.6765s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0947080\n",
      "\tspeed: 0.1125s/iter; left time: 7699.2210s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1082617\n",
      "\tspeed: 0.1138s/iter; left time: 7774.5218s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0972924\n",
      "\tspeed: 0.1158s/iter; left time: 7897.4971s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1161426\n",
      "\tspeed: 0.1147s/iter; left time: 7810.0341s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0934898\n",
      "\tspeed: 0.1141s/iter; left time: 7761.1548s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0890894\n",
      "\tspeed: 0.1152s/iter; left time: 7820.6728s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0774576\n",
      "\tspeed: 0.1154s/iter; left time: 7827.0229s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1041150\n",
      "\tspeed: 0.1168s/iter; left time: 7905.8663s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0764355\n",
      "\tspeed: 0.1134s/iter; left time: 7667.8056s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0967276\n",
      "\tspeed: 0.1140s/iter; left time: 7694.3834s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1057422\n",
      "\tspeed: 0.1142s/iter; left time: 7697.6299s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1008978\n",
      "\tspeed: 0.1144s/iter; left time: 7701.1571s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1179423\n",
      "\tspeed: 0.1149s/iter; left time: 7720.0400s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1178501\n",
      "\tspeed: 0.1099s/iter; left time: 7376.8081s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0911569\n",
      "\tspeed: 0.1127s/iter; left time: 7551.5480s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0975429\n",
      "\tspeed: 0.1136s/iter; left time: 7599.4730s\n",
      "Epoch: 5 cost time: 00h:08m:29.17s\n",
      "Epoch: 5 | Train Loss: 0.0987026 Vali Loss: 0.1216048 Test Loss: 0.1345417\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1011339\n",
      "\tspeed: 1.5360s/iter; left time: 102536.0272s\n",
      "\titers: 200, epoch: 6 | loss: 0.0883259\n",
      "\tspeed: 0.1171s/iter; left time: 7804.2875s\n",
      "\titers: 300, epoch: 6 | loss: 0.1075289\n",
      "\tspeed: 0.1141s/iter; left time: 7594.8624s\n",
      "\titers: 400, epoch: 6 | loss: 0.0989359\n",
      "\tspeed: 0.1163s/iter; left time: 7728.1429s\n",
      "\titers: 500, epoch: 6 | loss: 0.0862649\n",
      "\tspeed: 0.1143s/iter; left time: 7586.5485s\n",
      "\titers: 600, epoch: 6 | loss: 0.1002253\n",
      "\tspeed: 0.1150s/iter; left time: 7621.8291s\n",
      "\titers: 700, epoch: 6 | loss: 0.1083656\n",
      "\tspeed: 0.1156s/iter; left time: 7648.8588s\n",
      "\titers: 800, epoch: 6 | loss: 0.0928279\n",
      "\tspeed: 0.1160s/iter; left time: 7662.9690s\n",
      "\titers: 900, epoch: 6 | loss: 0.0807987\n",
      "\tspeed: 0.1151s/iter; left time: 7593.2415s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0948872\n",
      "\tspeed: 0.1148s/iter; left time: 7563.3529s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0827360\n",
      "\tspeed: 0.1164s/iter; left time: 7655.3289s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1174165\n",
      "\tspeed: 0.1145s/iter; left time: 7518.5707s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0992087\n",
      "\tspeed: 0.1147s/iter; left time: 7520.0064s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0848289\n",
      "\tspeed: 0.1166s/iter; left time: 7629.1240s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0922957\n",
      "\tspeed: 0.1152s/iter; left time: 7528.3518s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1061229\n",
      "\tspeed: 0.1166s/iter; left time: 7606.1201s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0937682\n",
      "\tspeed: 0.1160s/iter; left time: 7561.1492s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1026462\n",
      "\tspeed: 0.1133s/iter; left time: 7371.2713s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0818370\n",
      "\tspeed: 0.1157s/iter; left time: 7513.2731s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1049387\n",
      "\tspeed: 0.1159s/iter; left time: 7519.7116s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0856962\n",
      "\tspeed: 0.1147s/iter; left time: 7428.7141s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1132535\n",
      "\tspeed: 0.1156s/iter; left time: 7473.8852s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0780297\n",
      "\tspeed: 0.1136s/iter; left time: 7334.1105s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0942708\n",
      "\tspeed: 0.1139s/iter; left time: 7340.2087s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0917941\n",
      "\tspeed: 0.1145s/iter; left time: 7366.6319s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0872721\n",
      "\tspeed: 0.1154s/iter; left time: 7417.8992s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0825186\n",
      "\tspeed: 0.1150s/iter; left time: 7377.3480s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0905587\n",
      "\tspeed: 0.1162s/iter; left time: 7444.0894s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0887696\n",
      "\tspeed: 0.1138s/iter; left time: 7277.6593s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1022261\n",
      "\tspeed: 0.1136s/iter; left time: 7256.6604s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1016905\n",
      "\tspeed: 0.1158s/iter; left time: 7382.1130s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0889834\n",
      "\tspeed: 0.1147s/iter; left time: 7304.1106s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0868730\n",
      "\tspeed: 0.1146s/iter; left time: 7285.4443s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0905508\n",
      "\tspeed: 0.1136s/iter; left time: 7207.7473s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0885148\n",
      "\tspeed: 0.1152s/iter; left time: 7298.9761s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0922447\n",
      "\tspeed: 0.1151s/iter; left time: 7280.8357s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0886584\n",
      "\tspeed: 0.1145s/iter; left time: 7228.4714s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0922629\n",
      "\tspeed: 0.1146s/iter; left time: 7225.4081s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0939470\n",
      "\tspeed: 0.1130s/iter; left time: 7113.7216s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0905015\n",
      "\tspeed: 0.1145s/iter; left time: 7194.3585s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0758674\n",
      "\tspeed: 0.1167s/iter; left time: 7321.7619s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0957208\n",
      "\tspeed: 0.1136s/iter; left time: 7117.3013s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1053216\n",
      "\tspeed: 0.1134s/iter; left time: 7095.8461s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0906008\n",
      "\tspeed: 0.1147s/iter; left time: 7163.6950s\n",
      "Epoch: 6 cost time: 00h:08m:32.77s\n",
      "Epoch: 6 | Train Loss: 0.0954798 Vali Loss: 0.1227273 Test Loss: 0.1344287\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1033562\n",
      "\tspeed: 1.5449s/iter; left time: 96242.8774s\n",
      "\titers: 200, epoch: 7 | loss: 0.0896851\n",
      "\tspeed: 0.1128s/iter; left time: 7014.1229s\n",
      "\titers: 300, epoch: 7 | loss: 0.0913225\n",
      "\tspeed: 0.1154s/iter; left time: 7166.6403s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853823\n",
      "\tspeed: 0.1144s/iter; left time: 7095.4587s\n",
      "\titers: 500, epoch: 7 | loss: 0.0857644\n",
      "\tspeed: 0.1135s/iter; left time: 7026.9296s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806916\n",
      "\tspeed: 0.1141s/iter; left time: 7053.9584s\n",
      "\titers: 700, epoch: 7 | loss: 0.1021493\n",
      "\tspeed: 0.1169s/iter; left time: 7212.0236s\n",
      "\titers: 800, epoch: 7 | loss: 0.0732420\n",
      "\tspeed: 0.1147s/iter; left time: 7066.7822s\n",
      "\titers: 900, epoch: 7 | loss: 0.0850529\n",
      "\tspeed: 0.1133s/iter; left time: 6969.6772s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0993610\n",
      "\tspeed: 0.1148s/iter; left time: 7050.6510s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1097831\n",
      "\tspeed: 0.1150s/iter; left time: 7049.1661s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1054772\n",
      "\tspeed: 0.1153s/iter; left time: 7054.5038s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0870716\n",
      "\tspeed: 0.1149s/iter; left time: 7022.7027s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0907186\n",
      "\tspeed: 0.1148s/iter; left time: 7003.8567s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0836305\n",
      "\tspeed: 0.1153s/iter; left time: 7019.3724s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0796124\n",
      "\tspeed: 0.1154s/iter; left time: 7013.8048s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0783959\n",
      "\tspeed: 0.1152s/iter; left time: 6994.2668s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1006658\n",
      "\tspeed: 0.1179s/iter; left time: 7146.0686s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0884392\n",
      "\tspeed: 0.1131s/iter; left time: 6844.4431s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0906714\n",
      "\tspeed: 0.1152s/iter; left time: 6955.5823s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0930545\n",
      "\tspeed: 0.1124s/iter; left time: 6774.6318s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0999339\n",
      "\tspeed: 0.1150s/iter; left time: 6923.0216s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0763448\n",
      "\tspeed: 0.1141s/iter; left time: 6858.2620s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0873501\n",
      "\tspeed: 0.1148s/iter; left time: 6890.8708s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0835626\n",
      "\tspeed: 0.1134s/iter; left time: 6789.8264s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0899396\n",
      "\tspeed: 0.1144s/iter; left time: 6843.9955s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0973071\n",
      "\tspeed: 0.1155s/iter; left time: 6893.7186s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0889941\n",
      "\tspeed: 0.1119s/iter; left time: 6669.7559s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0885513\n",
      "\tspeed: 0.1140s/iter; left time: 6784.7081s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0899405\n",
      "\tspeed: 0.1142s/iter; left time: 6782.1823s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0913069\n",
      "\tspeed: 0.1141s/iter; left time: 6766.5576s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0929181\n",
      "\tspeed: 0.1131s/iter; left time: 6693.3679s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0748527\n",
      "\tspeed: 0.1134s/iter; left time: 6702.6012s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0883430\n",
      "\tspeed: 0.1127s/iter; left time: 6649.7692s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0995779\n",
      "\tspeed: 0.1146s/iter; left time: 6752.0857s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0879578\n",
      "\tspeed: 0.1146s/iter; left time: 6737.0533s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0936343\n",
      "\tspeed: 0.1164s/iter; left time: 6831.2676s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0912004\n",
      "\tspeed: 0.1137s/iter; left time: 6662.9285s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0808091\n",
      "\tspeed: 0.1143s/iter; left time: 6686.6893s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1068037\n",
      "\tspeed: 0.1160s/iter; left time: 6773.0787s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0853559\n",
      "\tspeed: 0.1127s/iter; left time: 6573.1499s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1038158\n",
      "\tspeed: 0.1145s/iter; left time: 6666.0709s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1103765\n",
      "\tspeed: 0.1153s/iter; left time: 6698.0690s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0968011\n",
      "\tspeed: 0.1133s/iter; left time: 6573.2121s\n",
      "Epoch: 7 cost time: 00h:08m:30.89s\n",
      "Epoch: 7 | Train Loss: 0.0925967 Vali Loss: 0.1245577 Test Loss: 0.1345884\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03578183427453041, rmse:0.18916086852550507, mae:0.12824216485023499, rse:0.6698496341705322\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 96: 01h:19m:09.94s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-03 04:55:56,825] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 04:55:57,946] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 04:55:57,946] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 04:55:57,946] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 04:55:58,038] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 04:55:58,038] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 04:55:58,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 04:55:58,774] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 04:55:58,774] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 04:55:58,775] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 04:55:58,775] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 04:55:58,775] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 04:55:59,162] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 04:55:59,163] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,194] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 88.96 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,359] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 04:55:59,360] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 88.97 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,360] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 04:55:59,504] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 04:55:59,505] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,505] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 89.03 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01f9fd6f50>\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1492427\n",
      "\tspeed: 0.1689s/iter; left time: 14998.9946s\n",
      "\titers: 200, epoch: 1 | loss: 0.1605359\n",
      "\tspeed: 0.1261s/iter; left time: 11190.3225s\n",
      "\titers: 300, epoch: 1 | loss: 0.1671229\n",
      "\tspeed: 0.1253s/iter; left time: 11106.0189s\n",
      "\titers: 400, epoch: 1 | loss: 0.1654047\n",
      "\tspeed: 0.1255s/iter; left time: 11113.6227s\n",
      "\titers: 500, epoch: 1 | loss: 0.1528231\n",
      "\tspeed: 0.1259s/iter; left time: 11130.4096s\n",
      "\titers: 600, epoch: 1 | loss: 0.1641308\n",
      "\tspeed: 0.1259s/iter; left time: 11120.4534s\n",
      "\titers: 700, epoch: 1 | loss: 0.1460070\n",
      "\tspeed: 0.1265s/iter; left time: 11163.3958s\n",
      "\titers: 800, epoch: 1 | loss: 0.1177307\n",
      "\tspeed: 0.1276s/iter; left time: 11242.2914s\n",
      "\titers: 900, epoch: 1 | loss: 0.1349947\n",
      "\tspeed: 0.1269s/iter; left time: 11173.5874s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1090521\n",
      "\tspeed: 0.1261s/iter; left time: 11086.5921s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1334336\n",
      "\tspeed: 0.1265s/iter; left time: 11109.3633s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1300411\n",
      "\tspeed: 0.1276s/iter; left time: 11197.1827s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1205211\n",
      "\tspeed: 0.1266s/iter; left time: 11093.1295s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1304281\n",
      "\tspeed: 0.1244s/iter; left time: 10886.0260s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1271410\n",
      "\tspeed: 0.1264s/iter; left time: 11049.2675s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1058479\n",
      "\tspeed: 0.1275s/iter; left time: 11129.7255s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1209870\n",
      "\tspeed: 0.1265s/iter; left time: 11029.6078s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1023959\n",
      "\tspeed: 0.1272s/iter; left time: 11078.7808s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1413429\n",
      "\tspeed: 0.1274s/iter; left time: 11089.8278s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1427599\n",
      "\tspeed: 0.1268s/iter; left time: 11019.0196s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1329999\n",
      "\tspeed: 0.1256s/iter; left time: 10905.1808s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1120558\n",
      "\tspeed: 0.1267s/iter; left time: 10986.1723s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0989436\n",
      "\tspeed: 0.1277s/iter; left time: 11060.9135s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1160462\n",
      "\tspeed: 0.1281s/iter; left time: 11079.1538s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1234167\n",
      "\tspeed: 0.1267s/iter; left time: 10953.3932s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1299602\n",
      "\tspeed: 0.1275s/iter; left time: 11009.6709s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1306166\n",
      "\tspeed: 0.1254s/iter; left time: 10812.0762s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1211876\n",
      "\tspeed: 0.1246s/iter; left time: 10732.5673s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1157711\n",
      "\tspeed: 0.1257s/iter; left time: 10811.2061s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0999164\n",
      "\tspeed: 0.1260s/iter; left time: 10828.2032s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1516199\n",
      "\tspeed: 0.1257s/iter; left time: 10790.8558s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1097987\n",
      "\tspeed: 0.1227s/iter; left time: 10516.8003s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1195762\n",
      "\tspeed: 0.1243s/iter; left time: 10638.8691s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1289778\n",
      "\tspeed: 0.1240s/iter; left time: 10602.2915s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1021916\n",
      "\tspeed: 0.1258s/iter; left time: 10744.1492s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1183287\n",
      "\tspeed: 0.1243s/iter; left time: 10606.4077s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1355938\n",
      "\tspeed: 0.1235s/iter; left time: 10522.3467s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1252127\n",
      "\tspeed: 0.1223s/iter; left time: 10407.7603s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1068365\n",
      "\tspeed: 0.1246s/iter; left time: 10597.5062s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1084060\n",
      "\tspeed: 0.1245s/iter; left time: 10571.2230s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1262050\n",
      "\tspeed: 0.1235s/iter; left time: 10477.3395s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1099667\n",
      "\tspeed: 0.1241s/iter; left time: 10516.0039s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1208501\n",
      "\tspeed: 0.1237s/iter; left time: 10468.0181s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1325888\n",
      "\tspeed: 0.1255s/iter; left time: 10609.7889s\n",
      "Epoch: 1 cost time: 00h:09m:20.32s\n",
      "Epoch: 1 | Train Loss: 0.1270849 Vali Loss: 0.1233267 Test Loss: 0.1336193\n",
      "Validation loss decreased (inf --> 0.123327).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1183373\n",
      "\tspeed: 1.8223s/iter; left time: 153754.1186s\n",
      "\titers: 200, epoch: 2 | loss: 0.1098894\n",
      "\tspeed: 0.1182s/iter; left time: 9964.4478s\n",
      "\titers: 300, epoch: 2 | loss: 0.1131674\n",
      "\tspeed: 0.1166s/iter; left time: 9818.2258s\n",
      "\titers: 400, epoch: 2 | loss: 0.1160400\n",
      "\tspeed: 0.1172s/iter; left time: 9850.7146s\n",
      "\titers: 500, epoch: 2 | loss: 0.0886714\n",
      "\tspeed: 0.1199s/iter; left time: 10065.0167s\n",
      "\titers: 600, epoch: 2 | loss: 0.1193221\n",
      "\tspeed: 0.1171s/iter; left time: 9825.4766s\n",
      "\titers: 700, epoch: 2 | loss: 0.1303138\n",
      "\tspeed: 0.1174s/iter; left time: 9832.2253s\n",
      "\titers: 800, epoch: 2 | loss: 0.1092902\n",
      "\tspeed: 0.1191s/iter; left time: 9966.0618s\n",
      "\titers: 900, epoch: 2 | loss: 0.0996466\n",
      "\tspeed: 0.1227s/iter; left time: 10250.5498s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1157148\n",
      "\tspeed: 0.1179s/iter; left time: 9845.2584s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1112141\n",
      "\tspeed: 0.1194s/iter; left time: 9953.4943s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1180607\n",
      "\tspeed: 0.1182s/iter; left time: 9846.0720s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0878751\n",
      "\tspeed: 0.1193s/iter; left time: 9920.3713s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1204318\n",
      "\tspeed: 0.1198s/iter; left time: 9951.3580s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1055659\n",
      "\tspeed: 0.1207s/iter; left time: 10015.5060s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1180415\n",
      "\tspeed: 0.1220s/iter; left time: 10108.0111s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1256569\n",
      "\tspeed: 0.1165s/iter; left time: 9646.1022s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1117147\n",
      "\tspeed: 0.1159s/iter; left time: 9583.7666s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1046314\n",
      "\tspeed: 0.1190s/iter; left time: 9824.2144s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1071824\n",
      "\tspeed: 0.1218s/iter; left time: 10045.4236s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1128184\n",
      "\tspeed: 0.1198s/iter; left time: 9872.1925s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1227209\n",
      "\tspeed: 0.1191s/iter; left time: 9795.1126s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1197821\n",
      "\tspeed: 0.1197s/iter; left time: 9834.7361s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1249823\n",
      "\tspeed: 0.1185s/iter; left time: 9729.3528s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1261758\n",
      "\tspeed: 0.1177s/iter; left time: 9645.1390s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1112454\n",
      "\tspeed: 0.1169s/iter; left time: 9568.3370s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1138095\n",
      "\tspeed: 0.1158s/iter; left time: 9466.3586s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0994604\n",
      "\tspeed: 0.1175s/iter; left time: 9597.1147s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1096554\n",
      "\tspeed: 0.1142s/iter; left time: 9316.7782s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1068463\n",
      "\tspeed: 0.1170s/iter; left time: 9529.2329s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1206979\n",
      "\tspeed: 0.1152s/iter; left time: 9372.1133s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1085434\n",
      "\tspeed: 0.1208s/iter; left time: 9814.7812s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1045549\n",
      "\tspeed: 0.1196s/iter; left time: 9709.0210s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1207838\n",
      "\tspeed: 0.1189s/iter; left time: 9640.1564s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1154747\n",
      "\tspeed: 0.1164s/iter; left time: 9424.9487s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1074588\n",
      "\tspeed: 0.1167s/iter; left time: 9435.2399s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1014723\n",
      "\tspeed: 0.1171s/iter; left time: 9457.4684s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0964471\n",
      "\tspeed: 0.1169s/iter; left time: 9432.4725s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1228269\n",
      "\tspeed: 0.1173s/iter; left time: 9447.5816s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1099236\n",
      "\tspeed: 0.1169s/iter; left time: 9405.8898s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171050\n",
      "\tspeed: 0.1216s/iter; left time: 9775.9550s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1104717\n",
      "\tspeed: 0.1183s/iter; left time: 9499.1964s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1126500\n",
      "\tspeed: 0.1213s/iter; left time: 9726.5423s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1323302\n",
      "\tspeed: 0.1198s/iter; left time: 9595.5465s\n",
      "Epoch: 2 cost time: 00h:08m:47.56s\n",
      "Epoch: 2 | Train Loss: 0.1139773 Vali Loss: 0.1234052 Test Loss: 0.1342462\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1082153\n",
      "\tspeed: 1.5796s/iter; left time: 126255.4090s\n",
      "\titers: 200, epoch: 3 | loss: 0.1202841\n",
      "\tspeed: 0.1189s/iter; left time: 9493.6879s\n",
      "\titers: 300, epoch: 3 | loss: 0.0996917\n",
      "\tspeed: 0.1184s/iter; left time: 9435.9582s\n",
      "\titers: 400, epoch: 3 | loss: 0.1064174\n",
      "\tspeed: 0.1157s/iter; left time: 9214.8919s\n",
      "\titers: 500, epoch: 3 | loss: 0.1139071\n",
      "\tspeed: 0.1180s/iter; left time: 9386.0109s\n",
      "\titers: 600, epoch: 3 | loss: 0.1215968\n",
      "\tspeed: 0.1142s/iter; left time: 9071.2138s\n",
      "\titers: 700, epoch: 3 | loss: 0.1096018\n",
      "\tspeed: 0.1197s/iter; left time: 9495.2993s\n",
      "\titers: 800, epoch: 3 | loss: 0.1310975\n",
      "\tspeed: 0.1203s/iter; left time: 9530.2566s\n",
      "\titers: 900, epoch: 3 | loss: 0.1056969\n",
      "\tspeed: 0.1183s/iter; left time: 9362.9065s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1081555\n",
      "\tspeed: 0.1158s/iter; left time: 9150.0619s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0982659\n",
      "\tspeed: 0.1183s/iter; left time: 9336.1627s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1056586\n",
      "\tspeed: 0.1197s/iter; left time: 9436.5511s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1162706\n",
      "\tspeed: 0.1196s/iter; left time: 9419.4681s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1264145\n",
      "\tspeed: 0.1210s/iter; left time: 9512.3302s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1110645\n",
      "\tspeed: 0.1198s/iter; left time: 9411.0207s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1157964\n",
      "\tspeed: 0.1202s/iter; left time: 9428.6409s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1245955\n",
      "\tspeed: 0.1179s/iter; left time: 9236.0999s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1219978\n",
      "\tspeed: 0.1211s/iter; left time: 9474.4907s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1215639\n",
      "\tspeed: 0.1194s/iter; left time: 9329.4055s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1222039\n",
      "\tspeed: 0.1194s/iter; left time: 9318.2211s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0978573\n",
      "\tspeed: 0.1180s/iter; left time: 9193.5410s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1149430\n",
      "\tspeed: 0.1199s/iter; left time: 9330.3611s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1145440\n",
      "\tspeed: 0.1201s/iter; left time: 9334.5904s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0993142\n",
      "\tspeed: 0.1208s/iter; left time: 9376.5978s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0932033\n",
      "\tspeed: 0.1171s/iter; left time: 9076.9066s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1083821\n",
      "\tspeed: 0.1178s/iter; left time: 9120.5453s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1158013\n",
      "\tspeed: 0.1144s/iter; left time: 8849.0927s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1378757\n",
      "\tspeed: 0.1162s/iter; left time: 8974.1123s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0924771\n",
      "\tspeed: 0.1162s/iter; left time: 8961.4239s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1077057\n",
      "\tspeed: 0.1180s/iter; left time: 9090.4405s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1156989\n",
      "\tspeed: 0.1167s/iter; left time: 8979.6128s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1000350\n",
      "\tspeed: 0.1160s/iter; left time: 8909.7129s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1146565\n",
      "\tspeed: 0.1172s/iter; left time: 8990.0227s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1231436\n",
      "\tspeed: 0.1170s/iter; left time: 8968.6194s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1224817\n",
      "\tspeed: 0.1151s/iter; left time: 8805.5514s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1114092\n",
      "\tspeed: 0.1154s/iter; left time: 8821.8669s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1154894\n",
      "\tspeed: 0.1148s/iter; left time: 8760.2005s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0983998\n",
      "\tspeed: 0.1184s/iter; left time: 9027.7861s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1058075\n",
      "\tspeed: 0.1176s/iter; left time: 8949.7973s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1094842\n",
      "\tspeed: 0.1186s/iter; left time: 9016.7895s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1029801\n",
      "\tspeed: 0.1215s/iter; left time: 9222.0609s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1099910\n",
      "\tspeed: 0.1177s/iter; left time: 8921.4090s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1054428\n",
      "\tspeed: 0.1200s/iter; left time: 9088.1881s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0913317\n",
      "\tspeed: 0.1180s/iter; left time: 8925.4389s\n",
      "Epoch: 3 cost time: 00h:08m:45.78s\n",
      "Epoch: 3 | Train Loss: 0.1093807 Vali Loss: 0.1236754 Test Loss: 0.1367024\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0959466\n",
      "\tspeed: 1.5738s/iter; left time: 118798.5643s\n",
      "\titers: 200, epoch: 4 | loss: 0.0858854\n",
      "\tspeed: 0.1189s/iter; left time: 8961.9629s\n",
      "\titers: 300, epoch: 4 | loss: 0.0893636\n",
      "\tspeed: 0.1198s/iter; left time: 9019.7581s\n",
      "\titers: 400, epoch: 4 | loss: 0.1088450\n",
      "\tspeed: 0.1180s/iter; left time: 8870.8322s\n",
      "\titers: 500, epoch: 4 | loss: 0.1087373\n",
      "\tspeed: 0.1165s/iter; left time: 8747.7988s\n",
      "\titers: 600, epoch: 4 | loss: 0.1005550\n",
      "\tspeed: 0.1151s/iter; left time: 8630.8889s\n",
      "\titers: 700, epoch: 4 | loss: 0.0969683\n",
      "\tspeed: 0.1181s/iter; left time: 8846.1387s\n",
      "\titers: 800, epoch: 4 | loss: 0.1045174\n",
      "\tspeed: 0.1171s/iter; left time: 8754.9369s\n",
      "\titers: 900, epoch: 4 | loss: 0.1086594\n",
      "\tspeed: 0.1166s/iter; left time: 8704.5463s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1303164\n",
      "\tspeed: 0.1176s/iter; left time: 8774.3390s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1121879\n",
      "\tspeed: 0.1182s/iter; left time: 8802.4399s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1189122\n",
      "\tspeed: 0.1178s/iter; left time: 8761.6614s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1055549\n",
      "\tspeed: 0.1178s/iter; left time: 8754.0308s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1178071\n",
      "\tspeed: 0.1213s/iter; left time: 8998.8108s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1012621\n",
      "\tspeed: 0.1200s/iter; left time: 8890.9761s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1049342\n",
      "\tspeed: 0.1199s/iter; left time: 8871.7970s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1118138\n",
      "\tspeed: 0.1209s/iter; left time: 8934.2080s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1249317\n",
      "\tspeed: 0.1209s/iter; left time: 8916.8846s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1314284\n",
      "\tspeed: 0.1186s/iter; left time: 8740.8187s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1265800\n",
      "\tspeed: 0.1188s/iter; left time: 8738.2363s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1018833\n",
      "\tspeed: 0.1207s/iter; left time: 8866.4914s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1211214\n",
      "\tspeed: 0.1200s/iter; left time: 8803.3129s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0848068\n",
      "\tspeed: 0.1199s/iter; left time: 8789.0981s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1038890\n",
      "\tspeed: 0.1198s/iter; left time: 8769.5086s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1123954\n",
      "\tspeed: 0.1190s/iter; left time: 8694.3077s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0989040\n",
      "\tspeed: 0.1209s/iter; left time: 8822.2843s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1005402\n",
      "\tspeed: 0.1177s/iter; left time: 8575.7761s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1054526\n",
      "\tspeed: 0.1200s/iter; left time: 8734.2448s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1114677\n",
      "\tspeed: 0.1160s/iter; left time: 8431.7920s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1075196\n",
      "\tspeed: 0.1189s/iter; left time: 8628.7340s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0856683\n",
      "\tspeed: 0.1195s/iter; left time: 8658.9518s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1058800\n",
      "\tspeed: 0.1180s/iter; left time: 8543.6849s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1083319\n",
      "\tspeed: 0.1208s/iter; left time: 8732.2973s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0981609\n",
      "\tspeed: 0.1202s/iter; left time: 8678.0036s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0921983\n",
      "\tspeed: 0.1207s/iter; left time: 8697.5703s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1049358\n",
      "\tspeed: 0.1205s/iter; left time: 8675.4825s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0929390\n",
      "\tspeed: 0.1195s/iter; left time: 8590.2117s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0982588\n",
      "\tspeed: 0.1194s/iter; left time: 8569.5723s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918985\n",
      "\tspeed: 0.1207s/iter; left time: 8649.6845s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1195650\n",
      "\tspeed: 0.1191s/iter; left time: 8526.0042s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1090801\n",
      "\tspeed: 0.1204s/iter; left time: 8606.8944s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1110307\n",
      "\tspeed: 0.1192s/iter; left time: 8505.5317s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1110436\n",
      "\tspeed: 0.1168s/iter; left time: 8327.9308s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1014118\n",
      "\tspeed: 0.1167s/iter; left time: 8308.2562s\n",
      "Epoch: 4 cost time: 00h:08m:50.08s\n",
      "Epoch: 4 | Train Loss: 0.1046495 Vali Loss: 0.1247817 Test Loss: 0.1472365\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1008341\n",
      "\tspeed: 1.5943s/iter; left time: 113251.2470s\n",
      "\titers: 200, epoch: 5 | loss: 0.1144764\n",
      "\tspeed: 0.1212s/iter; left time: 8596.8570s\n",
      "\titers: 300, epoch: 5 | loss: 0.1179324\n",
      "\tspeed: 0.1224s/iter; left time: 8667.5363s\n",
      "\titers: 400, epoch: 5 | loss: 0.1061344\n",
      "\tspeed: 0.1203s/iter; left time: 8507.9813s\n",
      "\titers: 500, epoch: 5 | loss: 0.0869319\n",
      "\tspeed: 0.1168s/iter; left time: 8251.2865s\n",
      "\titers: 600, epoch: 5 | loss: 0.0929676\n",
      "\tspeed: 0.1206s/iter; left time: 8505.0312s\n",
      "\titers: 700, epoch: 5 | loss: 0.1068786\n",
      "\tspeed: 0.1206s/iter; left time: 8494.8491s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935982\n",
      "\tspeed: 0.1185s/iter; left time: 8337.9691s\n",
      "\titers: 900, epoch: 5 | loss: 0.1054375\n",
      "\tspeed: 0.1175s/iter; left time: 8254.9225s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1086309\n",
      "\tspeed: 0.1202s/iter; left time: 8432.1821s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1122420\n",
      "\tspeed: 0.1192s/iter; left time: 8346.6720s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1004656\n",
      "\tspeed: 0.1199s/iter; left time: 8382.4266s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0959615\n",
      "\tspeed: 0.1190s/iter; left time: 8312.2799s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1126448\n",
      "\tspeed: 0.1158s/iter; left time: 8076.4031s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0902287\n",
      "\tspeed: 0.1192s/iter; left time: 8298.4404s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0997650\n",
      "\tspeed: 0.1182s/iter; left time: 8220.6378s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1175994\n",
      "\tspeed: 0.1196s/iter; left time: 8304.8478s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1059145\n",
      "\tspeed: 0.1186s/iter; left time: 8226.3011s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0982435\n",
      "\tspeed: 0.1187s/iter; left time: 8218.5163s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1148793\n",
      "\tspeed: 0.1204s/iter; left time: 8326.4242s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1237828\n",
      "\tspeed: 0.1166s/iter; left time: 8049.6795s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0931523\n",
      "\tspeed: 0.1141s/iter; left time: 7862.8554s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861305\n",
      "\tspeed: 0.1160s/iter; left time: 7987.8383s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0990851\n",
      "\tspeed: 0.1203s/iter; left time: 8266.9898s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0847341\n",
      "\tspeed: 0.1185s/iter; left time: 8131.6616s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1076903\n",
      "\tspeed: 0.1178s/iter; left time: 8076.7580s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1186636\n",
      "\tspeed: 0.1187s/iter; left time: 8124.0679s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0813073\n",
      "\tspeed: 0.1194s/iter; left time: 8161.1644s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0954584\n",
      "\tspeed: 0.1186s/iter; left time: 8091.0418s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0905743\n",
      "\tspeed: 0.1183s/iter; left time: 8063.9963s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1002901\n",
      "\tspeed: 0.1187s/iter; left time: 8075.8405s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0847461\n",
      "\tspeed: 0.1158s/iter; left time: 7868.1699s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1024117\n",
      "\tspeed: 0.1141s/iter; left time: 7742.7515s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0998646\n",
      "\tspeed: 0.1148s/iter; left time: 7772.8822s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0978041\n",
      "\tspeed: 0.1169s/iter; left time: 7909.1294s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1110223\n",
      "\tspeed: 0.1153s/iter; left time: 7785.4547s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0857197\n",
      "\tspeed: 0.1136s/iter; left time: 7663.1865s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0900995\n",
      "\tspeed: 0.1147s/iter; left time: 7720.9080s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0950421\n",
      "\tspeed: 0.1149s/iter; left time: 7724.4726s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1002941\n",
      "\tspeed: 0.1176s/iter; left time: 7894.1738s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1072492\n",
      "\tspeed: 0.1189s/iter; left time: 7973.0274s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0923533\n",
      "\tspeed: 0.1184s/iter; left time: 7922.1517s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0978186\n",
      "\tspeed: 0.1173s/iter; left time: 7837.3911s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0912306\n",
      "\tspeed: 0.1186s/iter; left time: 7914.4866s\n",
      "Epoch: 5 cost time: 00h:08m:46.01s\n",
      "Epoch: 5 | Train Loss: 0.1001263 Vali Loss: 0.1304869 Test Loss: 0.1511040\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1040925\n",
      "\tspeed: 1.5616s/iter; left time: 103988.1173s\n",
      "\titers: 200, epoch: 6 | loss: 0.1047199\n",
      "\tspeed: 0.1154s/iter; left time: 7674.7197s\n",
      "\titers: 300, epoch: 6 | loss: 0.0804931\n",
      "\tspeed: 0.1161s/iter; left time: 7705.3513s\n",
      "\titers: 400, epoch: 6 | loss: 0.0875467\n",
      "\tspeed: 0.1160s/iter; left time: 7688.9225s\n",
      "\titers: 500, epoch: 6 | loss: 0.0984672\n",
      "\tspeed: 0.1171s/iter; left time: 7747.7005s\n",
      "\titers: 600, epoch: 6 | loss: 0.1009006\n",
      "\tspeed: 0.1151s/iter; left time: 7604.0128s\n",
      "\titers: 700, epoch: 6 | loss: 0.0918718\n",
      "\tspeed: 0.1153s/iter; left time: 7606.5829s\n",
      "\titers: 800, epoch: 6 | loss: 0.1056673\n",
      "\tspeed: 0.1163s/iter; left time: 7662.1389s\n",
      "\titers: 900, epoch: 6 | loss: 0.0862563\n",
      "\tspeed: 0.1173s/iter; left time: 7717.6531s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0999702\n",
      "\tspeed: 0.1183s/iter; left time: 7772.3439s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0961189\n",
      "\tspeed: 0.1175s/iter; left time: 7709.2899s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1147054\n",
      "\tspeed: 0.1196s/iter; left time: 7832.8036s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0912160\n",
      "\tspeed: 0.1159s/iter; left time: 7580.3703s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0981800\n",
      "\tspeed: 0.1178s/iter; left time: 7691.0740s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0838766\n",
      "\tspeed: 0.1163s/iter; left time: 7580.5629s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0883116\n",
      "\tspeed: 0.1173s/iter; left time: 7636.6964s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0896513\n",
      "\tspeed: 0.1184s/iter; left time: 7691.9413s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0974333\n",
      "\tspeed: 0.1184s/iter; left time: 7682.3544s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0899982\n",
      "\tspeed: 0.1191s/iter; left time: 7717.4354s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0925732\n",
      "\tspeed: 0.1177s/iter; left time: 7611.6916s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0867949\n",
      "\tspeed: 0.1157s/iter; left time: 7475.8082s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1011756\n",
      "\tspeed: 0.1154s/iter; left time: 7445.3419s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0931246\n",
      "\tspeed: 0.1148s/iter; left time: 7393.9080s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0912003\n",
      "\tspeed: 0.1158s/iter; left time: 7443.0243s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0813842\n",
      "\tspeed: 0.1183s/iter; left time: 7595.5862s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0974265\n",
      "\tspeed: 0.1168s/iter; left time: 7488.7034s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1037935\n",
      "\tspeed: 0.1178s/iter; left time: 7539.9889s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0945832\n",
      "\tspeed: 0.1156s/iter; left time: 7388.0370s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0993364\n",
      "\tspeed: 0.1164s/iter; left time: 7422.9639s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0983878\n",
      "\tspeed: 0.1167s/iter; left time: 7434.3003s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0967573\n",
      "\tspeed: 0.1179s/iter; left time: 7497.4195s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0809352\n",
      "\tspeed: 0.1174s/iter; left time: 7454.3270s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0907727\n",
      "\tspeed: 0.1185s/iter; left time: 7510.3753s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1019389\n",
      "\tspeed: 0.1167s/iter; left time: 7387.1968s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1059106\n",
      "\tspeed: 0.1177s/iter; left time: 7434.7280s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0917989\n",
      "\tspeed: 0.1153s/iter; left time: 7276.6796s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0842525\n",
      "\tspeed: 0.1166s/iter; left time: 7343.0432s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0876396\n",
      "\tspeed: 0.1160s/iter; left time: 7297.2793s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1118588\n",
      "\tspeed: 0.1165s/iter; left time: 7313.6204s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0991682\n",
      "\tspeed: 0.1169s/iter; left time: 7331.3833s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0930842\n",
      "\tspeed: 0.1147s/iter; left time: 7177.1510s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0981919\n",
      "\tspeed: 0.1146s/iter; left time: 7161.5941s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0870969\n",
      "\tspeed: 0.1152s/iter; left time: 7187.5492s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0964762\n",
      "\tspeed: 0.1156s/iter; left time: 7198.3402s\n",
      "Epoch: 6 cost time: 00h:08m:39.51s\n",
      "Epoch: 6 | Train Loss: 0.0960726 Vali Loss: 0.1291619 Test Loss: 0.1511165\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03767301142215729, rmse:0.19409537315368652, mae:0.1336192935705185, rse:0.6876434087753296\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 168: 01h:09m:06.59s\n",
      "\n",
      "Intermediate time for DE: 04h:52m:56.47s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-03 06:05:07,960] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 06:05:09,402] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 06:05:09,402] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 06:05:09,402] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 06:05:09,558] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 06:05:09,558] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 06:05:10,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 06:05:10,277] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 06:05:10,277] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 06:05:10,278] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 06:05:10,279] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 06:05:10,279] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 06:05:10,610] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 06:05:10,611] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,611] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.89 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,736] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 06:05:10,737] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,737] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.89 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,737] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 06:05:10,860] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 06:05:10,861] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,861] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.87 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f31c2cc07d0>\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 06:05:10,866] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1466742\n",
      "\tspeed: 0.1748s/iter; left time: 15598.4857s\n",
      "\titers: 200, epoch: 1 | loss: 0.1377795\n",
      "\tspeed: 0.1298s/iter; left time: 11573.9787s\n",
      "\titers: 300, epoch: 1 | loss: 0.1637115\n",
      "\tspeed: 0.1289s/iter; left time: 11478.2904s\n",
      "\titers: 400, epoch: 1 | loss: 0.1077301\n",
      "\tspeed: 0.1275s/iter; left time: 11342.6149s\n",
      "\titers: 500, epoch: 1 | loss: 0.1226833\n",
      "\tspeed: 0.1276s/iter; left time: 11334.4573s\n",
      "\titers: 600, epoch: 1 | loss: 0.1199201\n",
      "\tspeed: 0.1312s/iter; left time: 11642.2717s\n",
      "\titers: 700, epoch: 1 | loss: 0.1068131\n",
      "\tspeed: 0.1287s/iter; left time: 11414.0931s\n",
      "\titers: 800, epoch: 1 | loss: 0.1092960\n",
      "\tspeed: 0.1320s/iter; left time: 11692.7568s\n",
      "\titers: 900, epoch: 1 | loss: 0.0919815\n",
      "\tspeed: 0.1286s/iter; left time: 11373.6150s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1038064\n",
      "\tspeed: 0.1298s/iter; left time: 11468.7619s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0846845\n",
      "\tspeed: 0.1312s/iter; left time: 11578.5412s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0949691\n",
      "\tspeed: 0.1288s/iter; left time: 11353.3324s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0737247\n",
      "\tspeed: 0.1311s/iter; left time: 11545.0504s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1031198\n",
      "\tspeed: 0.1302s/iter; left time: 11452.3215s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0780963\n",
      "\tspeed: 0.1307s/iter; left time: 11482.6928s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978673\n",
      "\tspeed: 0.1296s/iter; left time: 11371.6500s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1247796\n",
      "\tspeed: 0.1308s/iter; left time: 11464.9652s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062685\n",
      "\tspeed: 0.1303s/iter; left time: 11404.8978s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1049088\n",
      "\tspeed: 0.1276s/iter; left time: 11160.2830s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0870789\n",
      "\tspeed: 0.1304s/iter; left time: 11394.5054s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0814199\n",
      "\tspeed: 0.1311s/iter; left time: 11437.0125s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0885388\n",
      "\tspeed: 0.1297s/iter; left time: 11307.0506s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0876981\n",
      "\tspeed: 0.1284s/iter; left time: 11179.8422s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0750865\n",
      "\tspeed: 0.1304s/iter; left time: 11339.8303s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0770076\n",
      "\tspeed: 0.1299s/iter; left time: 11285.7984s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0930856\n",
      "\tspeed: 0.1271s/iter; left time: 11031.3798s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0914229\n",
      "\tspeed: 0.1238s/iter; left time: 10727.8290s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0742800\n",
      "\tspeed: 0.1251s/iter; left time: 10831.5647s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0699159\n",
      "\tspeed: 0.1299s/iter; left time: 11227.7913s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0888919\n",
      "\tspeed: 0.1286s/iter; left time: 11104.4819s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0732918\n",
      "\tspeed: 0.1276s/iter; left time: 11008.2922s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0881244\n",
      "\tspeed: 0.1289s/iter; left time: 11106.4858s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0822907\n",
      "\tspeed: 0.1264s/iter; left time: 10876.1296s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0753193\n",
      "\tspeed: 0.1298s/iter; left time: 11160.7985s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0816539\n",
      "\tspeed: 0.1282s/iter; left time: 11005.9994s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0759263\n",
      "\tspeed: 0.1307s/iter; left time: 11205.8377s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0969475\n",
      "\tspeed: 0.1297s/iter; left time: 11113.0399s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0810152\n",
      "\tspeed: 0.1271s/iter; left time: 10871.4882s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0943139\n",
      "\tspeed: 0.1299s/iter; left time: 11097.8455s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0698772\n",
      "\tspeed: 0.1268s/iter; left time: 10825.6037s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0704658\n",
      "\tspeed: 0.1269s/iter; left time: 10819.6445s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1035011\n",
      "\tspeed: 0.1271s/iter; left time: 10823.0052s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0949263\n",
      "\tspeed: 0.1275s/iter; left time: 10844.8182s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0885897\n",
      "\tspeed: 0.1288s/iter; left time: 10939.3516s\n",
      "Epoch: 1 cost time: 00h:09m:37.08s\n",
      "Epoch: 1 | Train Loss: 0.0973537 Vali Loss: 0.0928246 Test Loss: 0.1050737\n",
      "Validation loss decreased (inf --> 0.092825).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0701588\n",
      "\tspeed: 1.8726s/iter; left time: 158786.1147s\n",
      "\titers: 200, epoch: 2 | loss: 0.1044648\n",
      "\tspeed: 0.1163s/iter; left time: 9848.0479s\n",
      "\titers: 300, epoch: 2 | loss: 0.0884274\n",
      "\tspeed: 0.1176s/iter; left time: 9950.3132s\n",
      "\titers: 400, epoch: 2 | loss: 0.1147017\n",
      "\tspeed: 0.1188s/iter; left time: 10040.6889s\n",
      "\titers: 500, epoch: 2 | loss: 0.0937675\n",
      "\tspeed: 0.1192s/iter; left time: 10056.2784s\n",
      "\titers: 600, epoch: 2 | loss: 0.0921706\n",
      "\tspeed: 0.1180s/iter; left time: 9950.1293s\n",
      "\titers: 700, epoch: 2 | loss: 0.0930738\n",
      "\tspeed: 0.1198s/iter; left time: 10087.4863s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000150\n",
      "\tspeed: 0.1200s/iter; left time: 10094.0531s\n",
      "\titers: 900, epoch: 2 | loss: 0.0975661\n",
      "\tspeed: 0.1182s/iter; left time: 9925.3564s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0700671\n",
      "\tspeed: 0.1170s/iter; left time: 9816.9412s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0929756\n",
      "\tspeed: 0.1192s/iter; left time: 9988.0118s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0745666\n",
      "\tspeed: 0.1180s/iter; left time: 9879.2296s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0924439\n",
      "\tspeed: 0.1171s/iter; left time: 9789.0249s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1111560\n",
      "\tspeed: 0.1153s/iter; left time: 9628.5784s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0793124\n",
      "\tspeed: 0.1199s/iter; left time: 10001.1410s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0794686\n",
      "\tspeed: 0.1185s/iter; left time: 9867.7083s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0875069\n",
      "\tspeed: 0.1171s/iter; left time: 9738.6008s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0863800\n",
      "\tspeed: 0.1146s/iter; left time: 9522.1638s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0864655\n",
      "\tspeed: 0.1185s/iter; left time: 9833.4448s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0874661\n",
      "\tspeed: 0.1182s/iter; left time: 9800.8815s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0986785\n",
      "\tspeed: 0.1171s/iter; left time: 9692.0347s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0935980\n",
      "\tspeed: 0.1163s/iter; left time: 9617.8344s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0926060\n",
      "\tspeed: 0.1199s/iter; left time: 9902.1111s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731041\n",
      "\tspeed: 0.1162s/iter; left time: 9588.0168s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0878800\n",
      "\tspeed: 0.1154s/iter; left time: 9504.3980s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1000494\n",
      "\tspeed: 0.1144s/iter; left time: 9412.6264s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0909730\n",
      "\tspeed: 0.1134s/iter; left time: 9324.6489s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1120826\n",
      "\tspeed: 0.1131s/iter; left time: 9284.1912s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0753991\n",
      "\tspeed: 0.1170s/iter; left time: 9593.3664s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0918223\n",
      "\tspeed: 0.1166s/iter; left time: 9550.6203s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1169008\n",
      "\tspeed: 0.1160s/iter; left time: 9491.9086s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0799625\n",
      "\tspeed: 0.1160s/iter; left time: 9477.0554s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0924724\n",
      "\tspeed: 0.1156s/iter; left time: 9434.9133s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0893005\n",
      "\tspeed: 0.1156s/iter; left time: 9423.6612s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0870048\n",
      "\tspeed: 0.1142s/iter; left time: 9291.7373s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0749805\n",
      "\tspeed: 0.1153s/iter; left time: 9374.1188s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0773853\n",
      "\tspeed: 0.1178s/iter; left time: 9567.4743s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0819646\n",
      "\tspeed: 0.1175s/iter; left time: 9526.6254s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0823601\n",
      "\tspeed: 0.1144s/iter; left time: 9265.6871s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0686158\n",
      "\tspeed: 0.1143s/iter; left time: 9248.0084s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0982631\n",
      "\tspeed: 0.1139s/iter; left time: 9199.3269s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0787107\n",
      "\tspeed: 0.1149s/iter; left time: 9272.7571s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0879422\n",
      "\tspeed: 0.1146s/iter; left time: 9238.3802s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0896147\n",
      "\tspeed: 0.1152s/iter; left time: 9276.6456s\n",
      "Epoch: 2 cost time: 00h:08m:41.97s\n",
      "Epoch: 2 | Train Loss: 0.0860910 Vali Loss: 0.0922121 Test Loss: 0.1065872\n",
      "Validation loss decreased (0.092825 --> 0.092212).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0907432\n",
      "\tspeed: 1.6204s/iter; left time: 130160.2971s\n",
      "\titers: 200, epoch: 3 | loss: 0.0779589\n",
      "\tspeed: 0.1152s/iter; left time: 9241.5971s\n",
      "\titers: 300, epoch: 3 | loss: 0.0885364\n",
      "\tspeed: 0.1163s/iter; left time: 9316.8426s\n",
      "\titers: 400, epoch: 3 | loss: 0.0848934\n",
      "\tspeed: 0.1171s/iter; left time: 9367.1094s\n",
      "\titers: 500, epoch: 3 | loss: 0.0842507\n",
      "\tspeed: 0.1172s/iter; left time: 9364.6484s\n",
      "\titers: 600, epoch: 3 | loss: 0.0788545\n",
      "\tspeed: 0.1169s/iter; left time: 9328.3574s\n",
      "\titers: 700, epoch: 3 | loss: 0.0803074\n",
      "\tspeed: 0.1174s/iter; left time: 9359.3329s\n",
      "\titers: 800, epoch: 3 | loss: 0.0902780\n",
      "\tspeed: 0.1153s/iter; left time: 9179.2263s\n",
      "\titers: 900, epoch: 3 | loss: 0.0700542\n",
      "\tspeed: 0.1165s/iter; left time: 9260.9572s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0909660\n",
      "\tspeed: 0.1148s/iter; left time: 9119.6276s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0937846\n",
      "\tspeed: 0.1147s/iter; left time: 9097.6639s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0607558\n",
      "\tspeed: 0.1154s/iter; left time: 9145.7810s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0803862\n",
      "\tspeed: 0.1159s/iter; left time: 9167.5892s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0932646\n",
      "\tspeed: 0.1168s/iter; left time: 9233.6737s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0733327\n",
      "\tspeed: 0.1160s/iter; left time: 9157.2614s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0595024\n",
      "\tspeed: 0.1156s/iter; left time: 9109.5161s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0652814\n",
      "\tspeed: 0.1152s/iter; left time: 9072.5026s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0982525\n",
      "\tspeed: 0.1133s/iter; left time: 8909.1675s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0716447\n",
      "\tspeed: 0.1150s/iter; left time: 9034.0922s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0835190\n",
      "\tspeed: 0.1154s/iter; left time: 9052.4635s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0834547\n",
      "\tspeed: 0.1173s/iter; left time: 9187.7496s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0948817\n",
      "\tspeed: 0.1146s/iter; left time: 8965.3189s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0844729\n",
      "\tspeed: 0.1131s/iter; left time: 8832.9053s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0765860\n",
      "\tspeed: 0.1153s/iter; left time: 8993.3148s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0919627\n",
      "\tspeed: 0.1158s/iter; left time: 9022.1140s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0797675\n",
      "\tspeed: 0.1170s/iter; left time: 9102.4720s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0741416\n",
      "\tspeed: 0.1155s/iter; left time: 8973.8882s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0829531\n",
      "\tspeed: 0.1155s/iter; left time: 8968.6057s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0889192\n",
      "\tspeed: 0.1147s/iter; left time: 8889.3441s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0789738\n",
      "\tspeed: 0.1158s/iter; left time: 8968.2427s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0949245\n",
      "\tspeed: 0.1167s/iter; left time: 9026.3304s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0981605\n",
      "\tspeed: 0.1178s/iter; left time: 9098.8648s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0789458\n",
      "\tspeed: 0.1169s/iter; left time: 9016.7586s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0741549\n",
      "\tspeed: 0.1187s/iter; left time: 9139.5161s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0773597\n",
      "\tspeed: 0.1182s/iter; left time: 9091.3377s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0935745\n",
      "\tspeed: 0.1173s/iter; left time: 9012.3308s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0769829\n",
      "\tspeed: 0.1185s/iter; left time: 9089.8660s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0949859\n",
      "\tspeed: 0.1183s/iter; left time: 9068.1966s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0808080\n",
      "\tspeed: 0.1185s/iter; left time: 9066.9092s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0665414\n",
      "\tspeed: 0.1160s/iter; left time: 8861.8437s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0754559\n",
      "\tspeed: 0.1176s/iter; left time: 8973.6887s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0699142\n",
      "\tspeed: 0.1166s/iter; left time: 8885.9167s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0632910\n",
      "\tspeed: 0.1188s/iter; left time: 9041.2206s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0808305\n",
      "\tspeed: 0.1158s/iter; left time: 8802.9235s\n",
      "Epoch: 3 cost time: 00h:08m:40.38s\n",
      "Epoch: 3 | Train Loss: 0.0838997 Vali Loss: 0.0905142 Test Loss: 0.1035439\n",
      "Validation loss decreased (0.092212 --> 0.090514).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0914321\n",
      "\tspeed: 1.6342s/iter; left time: 123966.1902s\n",
      "\titers: 200, epoch: 4 | loss: 0.0923484\n",
      "\tspeed: 0.1178s/iter; left time: 8923.0128s\n",
      "\titers: 300, epoch: 4 | loss: 0.0886503\n",
      "\tspeed: 0.1152s/iter; left time: 8712.2391s\n",
      "\titers: 400, epoch: 4 | loss: 0.0890385\n",
      "\tspeed: 0.1155s/iter; left time: 8726.3773s\n",
      "\titers: 500, epoch: 4 | loss: 0.0889764\n",
      "\tspeed: 0.1156s/iter; left time: 8719.1197s\n",
      "\titers: 600, epoch: 4 | loss: 0.0682504\n",
      "\tspeed: 0.1178s/iter; left time: 8874.6318s\n",
      "\titers: 700, epoch: 4 | loss: 0.0858520\n",
      "\tspeed: 0.1163s/iter; left time: 8755.5159s\n",
      "\titers: 800, epoch: 4 | loss: 0.0807579\n",
      "\tspeed: 0.1139s/iter; left time: 8558.9584s\n",
      "\titers: 900, epoch: 4 | loss: 0.0771627\n",
      "\tspeed: 0.1157s/iter; left time: 8685.2748s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0891313\n",
      "\tspeed: 0.1158s/iter; left time: 8678.3248s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0808600\n",
      "\tspeed: 0.1133s/iter; left time: 8481.0700s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1001335\n",
      "\tspeed: 0.1148s/iter; left time: 8579.0647s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1131133\n",
      "\tspeed: 0.1136s/iter; left time: 8482.4180s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0778967\n",
      "\tspeed: 0.1156s/iter; left time: 8617.9859s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0659208\n",
      "\tspeed: 0.1143s/iter; left time: 8512.1282s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0821595\n",
      "\tspeed: 0.1144s/iter; left time: 8504.2124s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0919877\n",
      "\tspeed: 0.1148s/iter; left time: 8525.1535s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0923949\n",
      "\tspeed: 0.1144s/iter; left time: 8486.7110s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0738678\n",
      "\tspeed: 0.1155s/iter; left time: 8557.1487s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0819451\n",
      "\tspeed: 0.1135s/iter; left time: 8393.9204s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0814312\n",
      "\tspeed: 0.1126s/iter; left time: 8317.2582s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0888639\n",
      "\tspeed: 0.1129s/iter; left time: 8323.5396s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1003453\n",
      "\tspeed: 0.1129s/iter; left time: 8318.3946s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0999414\n",
      "\tspeed: 0.1145s/iter; left time: 8425.5708s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0742211\n",
      "\tspeed: 0.1128s/iter; left time: 8283.9718s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0718067\n",
      "\tspeed: 0.1142s/iter; left time: 8377.3046s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0911036\n",
      "\tspeed: 0.1134s/iter; left time: 8308.4250s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877281\n",
      "\tspeed: 0.1117s/iter; left time: 8169.8015s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0797919\n",
      "\tspeed: 0.1138s/iter; left time: 8314.7661s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0775673\n",
      "\tspeed: 0.1138s/iter; left time: 8305.5354s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0879129\n",
      "\tspeed: 0.1121s/iter; left time: 8167.1228s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0780287\n",
      "\tspeed: 0.1125s/iter; left time: 8183.4866s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0876071\n",
      "\tspeed: 0.1155s/iter; left time: 8394.3315s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0757971\n",
      "\tspeed: 0.1134s/iter; left time: 8226.7189s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0894305\n",
      "\tspeed: 0.1156s/iter; left time: 8377.4186s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0851934\n",
      "\tspeed: 0.1136s/iter; left time: 8218.1792s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0877569\n",
      "\tspeed: 0.1131s/iter; left time: 8175.3755s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0781913\n",
      "\tspeed: 0.1133s/iter; left time: 8172.6620s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0874085\n",
      "\tspeed: 0.1160s/iter; left time: 8359.8425s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0781264\n",
      "\tspeed: 0.1144s/iter; left time: 8235.0631s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0745134\n",
      "\tspeed: 0.1133s/iter; left time: 8138.9936s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0732169\n",
      "\tspeed: 0.1151s/iter; left time: 8258.0444s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0699919\n",
      "\tspeed: 0.1143s/iter; left time: 8192.2432s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0951408\n",
      "\tspeed: 0.1139s/iter; left time: 8151.1855s\n",
      "Epoch: 4 cost time: 00h:08m:31.76s\n",
      "Epoch: 4 | Train Loss: 0.0825824 Vali Loss: 0.0908698 Test Loss: 0.1047861\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0933834\n",
      "\tspeed: 1.5999s/iter; left time: 114213.8981s\n",
      "\titers: 200, epoch: 5 | loss: 0.0743399\n",
      "\tspeed: 0.1159s/iter; left time: 8264.3638s\n",
      "\titers: 300, epoch: 5 | loss: 0.0725531\n",
      "\tspeed: 0.1166s/iter; left time: 8303.3212s\n",
      "\titers: 400, epoch: 5 | loss: 0.0881155\n",
      "\tspeed: 0.1159s/iter; left time: 8242.0165s\n",
      "\titers: 500, epoch: 5 | loss: 0.0945252\n",
      "\tspeed: 0.1146s/iter; left time: 8136.5189s\n",
      "\titers: 600, epoch: 5 | loss: 0.0865966\n",
      "\tspeed: 0.1161s/iter; left time: 8226.7902s\n",
      "\titers: 700, epoch: 5 | loss: 0.0829508\n",
      "\tspeed: 0.1164s/iter; left time: 8240.0986s\n",
      "\titers: 800, epoch: 5 | loss: 0.0887444\n",
      "\tspeed: 0.1174s/iter; left time: 8298.8268s\n",
      "\titers: 900, epoch: 5 | loss: 0.1051521\n",
      "\tspeed: 0.1152s/iter; left time: 8132.9371s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0809733\n",
      "\tspeed: 0.1146s/iter; left time: 8075.1920s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0754441\n",
      "\tspeed: 0.1171s/iter; left time: 8239.9571s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0971770\n",
      "\tspeed: 0.1164s/iter; left time: 8179.7599s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0774984\n",
      "\tspeed: 0.1165s/iter; left time: 8179.5923s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0693193\n",
      "\tspeed: 0.1139s/iter; left time: 7981.7273s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0622493\n",
      "\tspeed: 0.1144s/iter; left time: 8004.3232s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0728563\n",
      "\tspeed: 0.1140s/iter; left time: 7964.1308s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0667490\n",
      "\tspeed: 0.1165s/iter; left time: 8131.8763s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0948240\n",
      "\tspeed: 0.1157s/iter; left time: 8066.1557s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0625855\n",
      "\tspeed: 0.1140s/iter; left time: 7935.2409s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0731169\n",
      "\tspeed: 0.1153s/iter; left time: 8012.9702s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0691220\n",
      "\tspeed: 0.1131s/iter; left time: 7850.1405s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0799258\n",
      "\tspeed: 0.1139s/iter; left time: 7891.7664s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0892581\n",
      "\tspeed: 0.1147s/iter; left time: 7938.9441s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0849659\n",
      "\tspeed: 0.1138s/iter; left time: 7859.0180s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0802421\n",
      "\tspeed: 0.1131s/iter; left time: 7804.0505s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0774631\n",
      "\tspeed: 0.1140s/iter; left time: 7852.8952s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0812496\n",
      "\tspeed: 0.1143s/iter; left time: 7862.3026s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0752958\n",
      "\tspeed: 0.1138s/iter; left time: 7819.8376s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0816401\n",
      "\tspeed: 0.1172s/iter; left time: 8041.8530s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0708815\n",
      "\tspeed: 0.1164s/iter; left time: 7969.1932s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1038611\n",
      "\tspeed: 0.1152s/iter; left time: 7878.3293s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0952071\n",
      "\tspeed: 0.1161s/iter; left time: 7926.1036s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0748858\n",
      "\tspeed: 0.1175s/iter; left time: 8012.6982s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0783696\n",
      "\tspeed: 0.1139s/iter; left time: 7758.4646s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0848109\n",
      "\tspeed: 0.1161s/iter; left time: 7894.9146s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0840811\n",
      "\tspeed: 0.1160s/iter; left time: 7871.9308s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0691066\n",
      "\tspeed: 0.1172s/iter; left time: 7944.3060s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0793731\n",
      "\tspeed: 0.1158s/iter; left time: 7835.1559s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0673810\n",
      "\tspeed: 0.1163s/iter; left time: 7860.3347s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0856006\n",
      "\tspeed: 0.1140s/iter; left time: 7696.4913s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0628996\n",
      "\tspeed: 0.1153s/iter; left time: 7768.3951s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0903348\n",
      "\tspeed: 0.1154s/iter; left time: 7765.9766s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0866672\n",
      "\tspeed: 0.1165s/iter; left time: 7825.4286s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0803752\n",
      "\tspeed: 0.1151s/iter; left time: 7723.7072s\n",
      "Epoch: 5 cost time: 00h:08m:36.25s\n",
      "Epoch: 5 | Train Loss: 0.0813836 Vali Loss: 0.0900973 Test Loss: 0.1040331\n",
      "Validation loss decreased (0.090514 --> 0.090097).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0863184\n",
      "\tspeed: 1.6079s/iter; left time: 107602.2371s\n",
      "\titers: 200, epoch: 6 | loss: 0.0890369\n",
      "\tspeed: 0.1166s/iter; left time: 7792.9815s\n",
      "\titers: 300, epoch: 6 | loss: 0.0835939\n",
      "\tspeed: 0.1167s/iter; left time: 7788.6726s\n",
      "\titers: 400, epoch: 6 | loss: 0.0703793\n",
      "\tspeed: 0.1153s/iter; left time: 7681.3484s\n",
      "\titers: 500, epoch: 6 | loss: 0.0827197\n",
      "\tspeed: 0.1163s/iter; left time: 7737.2977s\n",
      "\titers: 600, epoch: 6 | loss: 0.0714470\n",
      "\tspeed: 0.1183s/iter; left time: 7858.6330s\n",
      "\titers: 700, epoch: 6 | loss: 0.0747727\n",
      "\tspeed: 0.1166s/iter; left time: 7734.6799s\n",
      "\titers: 800, epoch: 6 | loss: 0.0732784\n",
      "\tspeed: 0.1169s/iter; left time: 7739.5273s\n",
      "\titers: 900, epoch: 6 | loss: 0.0771534\n",
      "\tspeed: 0.1158s/iter; left time: 7658.9947s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0741465\n",
      "\tspeed: 0.1157s/iter; left time: 7640.8549s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0746656\n",
      "\tspeed: 0.1148s/iter; left time: 7567.3418s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0931855\n",
      "\tspeed: 0.1191s/iter; left time: 7837.7050s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0977927\n",
      "\tspeed: 0.1163s/iter; left time: 7644.3140s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0876032\n",
      "\tspeed: 0.1166s/iter; left time: 7651.1682s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0801408\n",
      "\tspeed: 0.1177s/iter; left time: 7712.7444s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721369\n",
      "\tspeed: 0.1177s/iter; left time: 7696.8397s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0922342\n",
      "\tspeed: 0.1171s/iter; left time: 7648.3469s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0814926\n",
      "\tspeed: 0.1143s/iter; left time: 7452.2997s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0799462\n",
      "\tspeed: 0.1161s/iter; left time: 7563.4943s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0895512\n",
      "\tspeed: 0.1159s/iter; left time: 7538.6946s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0853165\n",
      "\tspeed: 0.1170s/iter; left time: 7596.2929s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1018588\n",
      "\tspeed: 0.1164s/iter; left time: 7545.7194s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0753619\n",
      "\tspeed: 0.1164s/iter; left time: 7536.7268s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0675035\n",
      "\tspeed: 0.1160s/iter; left time: 7493.0730s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0995846\n",
      "\tspeed: 0.1147s/iter; left time: 7398.4242s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0759167\n",
      "\tspeed: 0.1147s/iter; left time: 7387.5417s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0709326\n",
      "\tspeed: 0.1167s/iter; left time: 7503.3345s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1127540\n",
      "\tspeed: 0.1168s/iter; left time: 7503.3535s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0813409\n",
      "\tspeed: 0.1158s/iter; left time: 7424.7262s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0786561\n",
      "\tspeed: 0.1145s/iter; left time: 7330.6350s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0826245\n",
      "\tspeed: 0.1170s/iter; left time: 7480.9854s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0986693\n",
      "\tspeed: 0.1163s/iter; left time: 7423.9302s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0849674\n",
      "\tspeed: 0.1173s/iter; left time: 7471.3527s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0744465\n",
      "\tspeed: 0.1141s/iter; left time: 7260.0666s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0913548\n",
      "\tspeed: 0.1164s/iter; left time: 7393.4716s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0656657\n",
      "\tspeed: 0.1173s/iter; left time: 7438.1420s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0794955\n",
      "\tspeed: 0.1132s/iter; left time: 7168.1313s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0760554\n",
      "\tspeed: 0.1145s/iter; left time: 7239.6268s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0779494\n",
      "\tspeed: 0.1147s/iter; left time: 7242.1353s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0917149\n",
      "\tspeed: 0.1142s/iter; left time: 7199.8447s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0724742\n",
      "\tspeed: 0.1148s/iter; left time: 7224.6386s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0769409\n",
      "\tspeed: 0.1161s/iter; left time: 7291.8852s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0667945\n",
      "\tspeed: 0.1167s/iter; left time: 7317.3923s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0704586\n",
      "\tspeed: 0.1129s/iter; left time: 7072.5778s\n",
      "Epoch: 6 cost time: 00h:08m:38.83s\n",
      "Epoch: 6 | Train Loss: 0.0803718 Vali Loss: 0.0905898 Test Loss: 0.1044702\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0694771\n",
      "\tspeed: 1.5992s/iter; left time: 99877.1591s\n",
      "\titers: 200, epoch: 7 | loss: 0.0701124\n",
      "\tspeed: 0.1165s/iter; left time: 7262.2913s\n",
      "\titers: 300, epoch: 7 | loss: 0.0611065\n",
      "\tspeed: 0.1148s/iter; left time: 7147.6222s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853083\n",
      "\tspeed: 0.1183s/iter; left time: 7353.4378s\n",
      "\titers: 500, epoch: 7 | loss: 0.0865817\n",
      "\tspeed: 0.1156s/iter; left time: 7176.3244s\n",
      "\titers: 600, epoch: 7 | loss: 0.0731895\n",
      "\tspeed: 0.1170s/iter; left time: 7250.7958s\n",
      "\titers: 700, epoch: 7 | loss: 0.1010686\n",
      "\tspeed: 0.1172s/iter; left time: 7250.9657s\n",
      "\titers: 800, epoch: 7 | loss: 0.0807473\n",
      "\tspeed: 0.1166s/iter; left time: 7200.2405s\n",
      "\titers: 900, epoch: 7 | loss: 0.0729549\n",
      "\tspeed: 0.1158s/iter; left time: 7137.6264s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0784136\n",
      "\tspeed: 0.1160s/iter; left time: 7142.9028s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0915089\n",
      "\tspeed: 0.1162s/iter; left time: 7138.6109s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0813137\n",
      "\tspeed: 0.1176s/iter; left time: 7214.8011s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0710811\n",
      "\tspeed: 0.1147s/iter; left time: 7023.3959s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674738\n",
      "\tspeed: 0.1169s/iter; left time: 7146.4507s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0731219\n",
      "\tspeed: 0.1159s/iter; left time: 7076.6751s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0795863\n",
      "\tspeed: 0.1175s/iter; left time: 7159.0093s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0768837\n",
      "\tspeed: 0.1178s/iter; left time: 7169.3604s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0973881\n",
      "\tspeed: 0.1162s/iter; left time: 7058.1413s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0784142\n",
      "\tspeed: 0.1173s/iter; left time: 7111.9250s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0736109\n",
      "\tspeed: 0.1175s/iter; left time: 7113.4931s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0847631\n",
      "\tspeed: 0.1163s/iter; left time: 7030.1128s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0700621\n",
      "\tspeed: 0.1185s/iter; left time: 7152.6374s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0826743\n",
      "\tspeed: 0.1178s/iter; left time: 7096.9864s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0855349\n",
      "\tspeed: 0.1179s/iter; left time: 7089.6478s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0894471\n",
      "\tspeed: 0.1159s/iter; left time: 6961.7498s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0709336\n",
      "\tspeed: 0.1170s/iter; left time: 7014.0372s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0635979\n",
      "\tspeed: 0.1142s/iter; left time: 6838.1543s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0727365\n",
      "\tspeed: 0.1165s/iter; left time: 6960.3037s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0891747\n",
      "\tspeed: 0.1137s/iter; left time: 6781.6957s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0914883\n",
      "\tspeed: 0.1150s/iter; left time: 6848.7837s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0926042\n",
      "\tspeed: 0.1167s/iter; left time: 6940.1020s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0622337\n",
      "\tspeed: 0.1151s/iter; left time: 6832.7122s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0735094\n",
      "\tspeed: 0.1146s/iter; left time: 6793.1364s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0951522\n",
      "\tspeed: 0.1152s/iter; left time: 6813.5322s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0720976\n",
      "\tspeed: 0.1153s/iter; left time: 6807.6841s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0690883\n",
      "\tspeed: 0.1146s/iter; left time: 6753.2053s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0841284\n",
      "\tspeed: 0.1187s/iter; left time: 6985.1423s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0866462\n",
      "\tspeed: 0.1156s/iter; left time: 6790.0288s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0677450\n",
      "\tspeed: 0.1156s/iter; left time: 6780.3658s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0808895\n",
      "\tspeed: 0.1148s/iter; left time: 6723.6332s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0839261\n",
      "\tspeed: 0.1135s/iter; left time: 6633.9557s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0723493\n",
      "\tspeed: 0.1171s/iter; left time: 6835.8888s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0775747\n",
      "\tspeed: 0.1183s/iter; left time: 6893.1300s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0767770\n",
      "\tspeed: 0.1166s/iter; left time: 6778.6296s\n",
      "Epoch: 7 cost time: 00h:08m:40.24s\n",
      "Epoch: 7 | Train Loss: 0.0794193 Vali Loss: 0.0901961 Test Loss: 0.1045509\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0895511\n",
      "\tspeed: 1.6001s/iter; left time: 92783.0760s\n",
      "\titers: 200, epoch: 8 | loss: 0.0866904\n",
      "\tspeed: 0.1176s/iter; left time: 6804.9920s\n",
      "\titers: 300, epoch: 8 | loss: 0.0797954\n",
      "\tspeed: 0.1173s/iter; left time: 6778.7685s\n",
      "\titers: 400, epoch: 8 | loss: 0.0820298\n",
      "\tspeed: 0.1157s/iter; left time: 6671.7858s\n",
      "\titers: 500, epoch: 8 | loss: 0.0783670\n",
      "\tspeed: 0.1153s/iter; left time: 6637.8132s\n",
      "\titers: 600, epoch: 8 | loss: 0.0902877\n",
      "\tspeed: 0.1160s/iter; left time: 6667.4776s\n",
      "\titers: 700, epoch: 8 | loss: 0.0648628\n",
      "\tspeed: 0.1193s/iter; left time: 6846.3671s\n",
      "\titers: 800, epoch: 8 | loss: 0.0814682\n",
      "\tspeed: 0.1170s/iter; left time: 6705.1745s\n",
      "\titers: 900, epoch: 8 | loss: 0.0808761\n",
      "\tspeed: 0.1179s/iter; left time: 6740.2376s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0752130\n",
      "\tspeed: 0.1156s/iter; left time: 6597.0368s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0748815\n",
      "\tspeed: 0.1141s/iter; left time: 6500.1897s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0795547\n",
      "\tspeed: 0.1146s/iter; left time: 6521.2980s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0994396\n",
      "\tspeed: 0.1153s/iter; left time: 6548.6293s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0792517\n",
      "\tspeed: 0.1165s/iter; left time: 6602.8411s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0784374\n",
      "\tspeed: 0.1173s/iter; left time: 6636.2352s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0859944\n",
      "\tspeed: 0.1158s/iter; left time: 6538.8571s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0819602\n",
      "\tspeed: 0.1172s/iter; left time: 6609.0135s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0977695\n",
      "\tspeed: 0.1156s/iter; left time: 6505.3635s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0645730\n",
      "\tspeed: 0.1171s/iter; left time: 6578.6172s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0558155\n",
      "\tspeed: 0.1168s/iter; left time: 6549.4902s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0748824\n",
      "\tspeed: 0.1157s/iter; left time: 6478.2495s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0751968\n",
      "\tspeed: 0.1162s/iter; left time: 6494.5979s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0775747\n",
      "\tspeed: 0.1148s/iter; left time: 6404.6842s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0743494\n",
      "\tspeed: 0.1139s/iter; left time: 6342.7048s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0847968\n",
      "\tspeed: 0.1157s/iter; left time: 6433.2662s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0757934\n",
      "\tspeed: 0.1164s/iter; left time: 6456.4094s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0962444\n",
      "\tspeed: 0.1137s/iter; left time: 6297.2839s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0798034\n",
      "\tspeed: 0.1155s/iter; left time: 6386.9256s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0616579\n",
      "\tspeed: 0.1153s/iter; left time: 6360.0942s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0749053\n",
      "\tspeed: 0.1160s/iter; left time: 6388.2991s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0779971\n",
      "\tspeed: 0.1161s/iter; left time: 6384.3218s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0703546\n",
      "\tspeed: 0.1156s/iter; left time: 6344.9051s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0924814\n",
      "\tspeed: 0.1184s/iter; left time: 6485.7043s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0760914\n",
      "\tspeed: 0.1151s/iter; left time: 6293.5055s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0540513\n",
      "\tspeed: 0.1156s/iter; left time: 6309.0888s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0791183\n",
      "\tspeed: 0.1166s/iter; left time: 6350.9989s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0715009\n",
      "\tspeed: 0.1170s/iter; left time: 6365.5592s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0779355\n",
      "\tspeed: 0.1161s/iter; left time: 6304.4201s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0706417\n",
      "\tspeed: 0.1178s/iter; left time: 6382.4899s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0706924\n",
      "\tspeed: 0.1169s/iter; left time: 6324.4682s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0704471\n",
      "\tspeed: 0.1175s/iter; left time: 6342.1722s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0729290\n",
      "\tspeed: 0.1144s/iter; left time: 6166.8255s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0735274\n",
      "\tspeed: 0.1167s/iter; left time: 6279.3577s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0819775\n",
      "\tspeed: 0.1163s/iter; left time: 6245.3451s\n",
      "Epoch: 8 cost time: 00h:08m:39.75s\n",
      "Epoch: 8 | Train Loss: 0.0784829 Vali Loss: 0.0906655 Test Loss: 0.1073768\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0807247\n",
      "\tspeed: 1.5920s/iter; left time: 85200.9096s\n",
      "\titers: 200, epoch: 9 | loss: 0.0876306\n",
      "\tspeed: 0.1159s/iter; left time: 6188.4256s\n",
      "\titers: 300, epoch: 9 | loss: 0.0690702\n",
      "\tspeed: 0.1141s/iter; left time: 6083.2244s\n",
      "\titers: 400, epoch: 9 | loss: 0.0685977\n",
      "\tspeed: 0.1155s/iter; left time: 6144.9507s\n",
      "\titers: 500, epoch: 9 | loss: 0.1049898\n",
      "\tspeed: 0.1168s/iter; left time: 6202.2266s\n",
      "\titers: 600, epoch: 9 | loss: 0.0862811\n",
      "\tspeed: 0.1132s/iter; left time: 5999.1441s\n",
      "\titers: 700, epoch: 9 | loss: 0.0700331\n",
      "\tspeed: 0.1152s/iter; left time: 6093.7406s\n",
      "\titers: 800, epoch: 9 | loss: 0.0694490\n",
      "\tspeed: 0.1144s/iter; left time: 6041.9345s\n",
      "\titers: 900, epoch: 9 | loss: 0.0769776\n",
      "\tspeed: 0.1146s/iter; left time: 6043.7254s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0690558\n",
      "\tspeed: 0.1141s/iter; left time: 6005.1500s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0718805\n",
      "\tspeed: 0.1166s/iter; left time: 6125.6806s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0790950\n",
      "\tspeed: 0.1166s/iter; left time: 6113.3885s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0878323\n",
      "\tspeed: 0.1139s/iter; left time: 5957.3476s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0601362\n",
      "\tspeed: 0.1162s/iter; left time: 6066.5406s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0813463\n",
      "\tspeed: 0.1156s/iter; left time: 6026.6967s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0808004\n",
      "\tspeed: 0.1173s/iter; left time: 6101.3994s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0846742\n",
      "\tspeed: 0.1160s/iter; left time: 6021.5311s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0694577\n",
      "\tspeed: 0.1165s/iter; left time: 6037.4243s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0717922\n",
      "\tspeed: 0.1162s/iter; left time: 6010.5074s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0688320\n",
      "\tspeed: 0.1155s/iter; left time: 5960.9380s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0924541\n",
      "\tspeed: 0.1158s/iter; left time: 5965.4880s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0821416\n",
      "\tspeed: 0.1134s/iter; left time: 5831.2938s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0760271\n",
      "\tspeed: 0.1136s/iter; left time: 5831.3681s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0767931\n",
      "\tspeed: 0.1146s/iter; left time: 5867.6329s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0697705\n",
      "\tspeed: 0.1166s/iter; left time: 5959.7000s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0924100\n",
      "\tspeed: 0.1174s/iter; left time: 5988.3751s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0637605\n",
      "\tspeed: 0.1151s/iter; left time: 5862.7900s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0797227\n",
      "\tspeed: 0.1144s/iter; left time: 5814.0759s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0813500\n",
      "\tspeed: 0.1136s/iter; left time: 5762.7865s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0780612\n",
      "\tspeed: 0.1135s/iter; left time: 5746.4798s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0720700\n",
      "\tspeed: 0.1135s/iter; left time: 5735.5167s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0765664\n",
      "\tspeed: 0.1161s/iter; left time: 5855.5431s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0852109\n",
      "\tspeed: 0.1160s/iter; left time: 5839.0464s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0872862\n",
      "\tspeed: 0.1130s/iter; left time: 5674.4628s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0673751\n",
      "\tspeed: 0.1135s/iter; left time: 5689.5537s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0731839\n",
      "\tspeed: 0.1136s/iter; left time: 5680.6715s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0864574\n",
      "\tspeed: 0.1145s/iter; left time: 5713.9607s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0817628\n",
      "\tspeed: 0.1132s/iter; left time: 5638.7590s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0986147\n",
      "\tspeed: 0.1148s/iter; left time: 5706.3174s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0703890\n",
      "\tspeed: 0.1135s/iter; left time: 5632.6065s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0744173\n",
      "\tspeed: 0.1171s/iter; left time: 5796.8049s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0785395\n",
      "\tspeed: 0.1150s/iter; left time: 5681.7597s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0744364\n",
      "\tspeed: 0.1147s/iter; left time: 5655.6415s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0595877\n",
      "\tspeed: 0.1140s/iter; left time: 5610.7776s\n",
      "Epoch: 9 cost time: 00h:08m:34.31s\n",
      "Epoch: 9 | Train Loss: 0.0773867 Vali Loss: 0.0910278 Test Loss: 0.1069496\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0671151\n",
      "\tspeed: 1.5785s/iter; left time: 77423.5353s\n",
      "\titers: 200, epoch: 10 | loss: 0.0828441\n",
      "\tspeed: 0.1145s/iter; left time: 5605.6839s\n",
      "\titers: 300, epoch: 10 | loss: 0.0783897\n",
      "\tspeed: 0.1136s/iter; left time: 5546.8106s\n",
      "\titers: 400, epoch: 10 | loss: 0.0834453\n",
      "\tspeed: 0.1153s/iter; left time: 5620.6085s\n",
      "\titers: 500, epoch: 10 | loss: 0.0739451\n",
      "\tspeed: 0.1143s/iter; left time: 5561.1659s\n",
      "\titers: 600, epoch: 10 | loss: 0.0635547\n",
      "\tspeed: 0.1166s/iter; left time: 5660.8866s\n",
      "\titers: 700, epoch: 10 | loss: 0.0789368\n",
      "\tspeed: 0.1144s/iter; left time: 5543.8684s\n",
      "\titers: 800, epoch: 10 | loss: 0.0980061\n",
      "\tspeed: 0.1155s/iter; left time: 5582.6527s\n",
      "\titers: 900, epoch: 10 | loss: 0.0827744\n",
      "\tspeed: 0.1135s/iter; left time: 5478.4677s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0858516\n",
      "\tspeed: 0.1160s/iter; left time: 5586.9409s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0609591\n",
      "\tspeed: 0.1159s/iter; left time: 5570.3575s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0810569\n",
      "\tspeed: 0.1169s/iter; left time: 5603.0406s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0937729\n",
      "\tspeed: 0.1171s/iter; left time: 5602.3511s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0835219\n",
      "\tspeed: 0.1161s/iter; left time: 5545.6390s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0743610\n",
      "\tspeed: 0.1172s/iter; left time: 5584.3104s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0712053\n",
      "\tspeed: 0.1170s/iter; left time: 5562.3177s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0691880\n",
      "\tspeed: 0.1129s/iter; left time: 5357.6994s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0734778\n",
      "\tspeed: 0.1141s/iter; left time: 5404.1841s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0740059\n",
      "\tspeed: 0.1151s/iter; left time: 5439.5805s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0803476\n",
      "\tspeed: 0.1189s/iter; left time: 5607.9472s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0573778\n",
      "\tspeed: 0.1133s/iter; left time: 5331.1378s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633182\n",
      "\tspeed: 0.1135s/iter; left time: 5330.4922s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0607398\n",
      "\tspeed: 0.1139s/iter; left time: 5334.9009s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0995448\n",
      "\tspeed: 0.1147s/iter; left time: 5363.4710s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0664814\n",
      "\tspeed: 0.1141s/iter; left time: 5324.1224s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0725956\n",
      "\tspeed: 0.1151s/iter; left time: 5357.9144s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0891252\n",
      "\tspeed: 0.1147s/iter; left time: 5329.2590s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0708456\n",
      "\tspeed: 0.1129s/iter; left time: 5231.2750s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0679720\n",
      "\tspeed: 0.1145s/iter; left time: 5294.0154s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0656617\n",
      "\tspeed: 0.1147s/iter; left time: 5293.4144s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0661887\n",
      "\tspeed: 0.1137s/iter; left time: 5235.6263s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0758295\n",
      "\tspeed: 0.1158s/iter; left time: 5322.2497s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0698776\n",
      "\tspeed: 0.1148s/iter; left time: 5263.9330s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0789215\n",
      "\tspeed: 0.1148s/iter; left time: 5251.9707s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0749588\n",
      "\tspeed: 0.1140s/iter; left time: 5202.1299s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1019792\n",
      "\tspeed: 0.1160s/iter; left time: 5285.0743s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0910538\n",
      "\tspeed: 0.1161s/iter; left time: 5275.2146s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0882741\n",
      "\tspeed: 0.1137s/iter; left time: 5154.9439s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0684295\n",
      "\tspeed: 0.1169s/iter; left time: 5291.0654s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0834578\n",
      "\tspeed: 0.1151s/iter; left time: 5197.8577s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0795520\n",
      "\tspeed: 0.1142s/iter; left time: 5146.1165s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0775932\n",
      "\tspeed: 0.1176s/iter; left time: 5286.7117s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0749520\n",
      "\tspeed: 0.1152s/iter; left time: 5168.5459s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0773880\n",
      "\tspeed: 0.1140s/iter; left time: 5103.3444s\n",
      "Epoch: 10 cost time: 00h:08m:34.73s\n",
      "Epoch: 10 | Train Loss: 0.0764753 Vali Loss: 0.0915059 Test Loss: 0.1088750\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.025583267211914062, rmse:0.1599477082490921, mae:0.10403311252593994, rse:0.5514940619468689\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 24: 01h:52m:46.62s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-03 07:57:52,023] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 07:57:53,303] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 07:57:53,303] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 07:57:53,303] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 07:57:53,403] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 07:57:53,403] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 07:57:54,256] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 07:57:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 07:57:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 07:57:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 07:57:54,259] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 07:57:54,259] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 07:57:54,707] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 07:57:54,708] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:54,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.75 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:54,878] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 07:57:54,879] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:54,879] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.77 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:54,879] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 07:57:55,059] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 07:57:55,060] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:55,060] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.72 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 07:57:55,062] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a74e9add0>\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688266\n",
      "\tspeed: 0.1785s/iter; left time: 15893.5365s\n",
      "\titers: 200, epoch: 1 | loss: 0.1466758\n",
      "\tspeed: 0.1255s/iter; left time: 11163.3362s\n",
      "\titers: 300, epoch: 1 | loss: 0.1657802\n",
      "\tspeed: 0.1288s/iter; left time: 11443.9814s\n",
      "\titers: 400, epoch: 1 | loss: 0.1365485\n",
      "\tspeed: 0.1305s/iter; left time: 11583.9117s\n",
      "\titers: 500, epoch: 1 | loss: 0.1343239\n",
      "\tspeed: 0.1286s/iter; left time: 11396.9528s\n",
      "\titers: 600, epoch: 1 | loss: 0.1273262\n",
      "\tspeed: 0.1305s/iter; left time: 11555.1903s\n",
      "\titers: 700, epoch: 1 | loss: 0.1142251\n",
      "\tspeed: 0.1300s/iter; left time: 11495.5934s\n",
      "\titers: 800, epoch: 1 | loss: 0.1129106\n",
      "\tspeed: 0.1282s/iter; left time: 11327.4622s\n",
      "\titers: 900, epoch: 1 | loss: 0.1081580\n",
      "\tspeed: 0.1301s/iter; left time: 11483.9961s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1082976\n",
      "\tspeed: 0.1288s/iter; left time: 11355.2133s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1104180\n",
      "\tspeed: 0.1282s/iter; left time: 11289.5287s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1293156\n",
      "\tspeed: 0.1285s/iter; left time: 11304.2771s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1199676\n",
      "\tspeed: 0.1286s/iter; left time: 11298.3167s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0853492\n",
      "\tspeed: 0.1277s/iter; left time: 11204.6884s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1185155\n",
      "\tspeed: 0.1289s/iter; left time: 11294.9506s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1091379\n",
      "\tspeed: 0.1280s/iter; left time: 11201.8787s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1387339\n",
      "\tspeed: 0.1293s/iter; left time: 11306.8202s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1115096\n",
      "\tspeed: 0.1266s/iter; left time: 11058.3400s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1182949\n",
      "\tspeed: 0.1303s/iter; left time: 11364.1194s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1110286\n",
      "\tspeed: 0.1287s/iter; left time: 11218.9095s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1118804\n",
      "\tspeed: 0.1277s/iter; left time: 11113.0769s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1165919\n",
      "\tspeed: 0.1274s/iter; left time: 11074.5511s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1117706\n",
      "\tspeed: 0.1301s/iter; left time: 11299.2092s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1129438\n",
      "\tspeed: 0.1294s/iter; left time: 11222.3812s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1135126\n",
      "\tspeed: 0.1281s/iter; left time: 11096.9533s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1112166\n",
      "\tspeed: 0.1286s/iter; left time: 11128.1423s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1203404\n",
      "\tspeed: 0.1266s/iter; left time: 10943.4566s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1182374\n",
      "\tspeed: 0.1295s/iter; left time: 11184.3534s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1069434\n",
      "\tspeed: 0.1288s/iter; left time: 11103.8088s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1128676\n",
      "\tspeed: 0.1270s/iter; left time: 10942.3124s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1112418\n",
      "\tspeed: 0.1259s/iter; left time: 10836.4119s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1109535\n",
      "\tspeed: 0.1280s/iter; left time: 11004.6363s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1022410\n",
      "\tspeed: 0.1280s/iter; left time: 10991.1581s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1067109\n",
      "\tspeed: 0.1288s/iter; left time: 11039.7652s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1093592\n",
      "\tspeed: 0.1301s/iter; left time: 11142.3125s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1050644\n",
      "\tspeed: 0.1300s/iter; left time: 11120.9045s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1453399\n",
      "\tspeed: 0.1292s/iter; left time: 11037.9253s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1064153\n",
      "\tspeed: 0.1306s/iter; left time: 11142.7100s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1275871\n",
      "\tspeed: 0.1284s/iter; left time: 10940.9372s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1056856\n",
      "\tspeed: 0.1266s/iter; left time: 10782.8795s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1163018\n",
      "\tspeed: 0.1286s/iter; left time: 10939.7400s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1195716\n",
      "\tspeed: 0.1284s/iter; left time: 10907.8794s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1011549\n",
      "\tspeed: 0.1282s/iter; left time: 10878.4997s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1301714\n",
      "\tspeed: 0.1273s/iter; left time: 10789.5248s\n",
      "Epoch: 1 cost time: 00h:09m:34.14s\n",
      "Epoch: 1 | Train Loss: 0.1159376 Vali Loss: 0.1171800 Test Loss: 0.1386980\n",
      "Validation loss decreased (inf --> 0.117180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1323460\n",
      "\tspeed: 1.8177s/iter; left time: 153750.1873s\n",
      "\titers: 200, epoch: 2 | loss: 0.0998212\n",
      "\tspeed: 0.1151s/iter; left time: 9726.5930s\n",
      "\titers: 300, epoch: 2 | loss: 0.1072860\n",
      "\tspeed: 0.1144s/iter; left time: 9650.6818s\n",
      "\titers: 400, epoch: 2 | loss: 0.1047579\n",
      "\tspeed: 0.1165s/iter; left time: 9817.5607s\n",
      "\titers: 500, epoch: 2 | loss: 0.0988569\n",
      "\tspeed: 0.1131s/iter; left time: 9518.1567s\n",
      "\titers: 600, epoch: 2 | loss: 0.1400726\n",
      "\tspeed: 0.1153s/iter; left time: 9696.2221s\n",
      "\titers: 700, epoch: 2 | loss: 0.1279359\n",
      "\tspeed: 0.1146s/iter; left time: 9621.8230s\n",
      "\titers: 800, epoch: 2 | loss: 0.1120330\n",
      "\tspeed: 0.1140s/iter; left time: 9561.5629s\n",
      "\titers: 900, epoch: 2 | loss: 0.1051929\n",
      "\tspeed: 0.1129s/iter; left time: 9458.3001s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1209462\n",
      "\tspeed: 0.1157s/iter; left time: 9681.0412s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1275426\n",
      "\tspeed: 0.1137s/iter; left time: 9505.8381s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1159983\n",
      "\tspeed: 0.1157s/iter; left time: 9656.7284s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1053278\n",
      "\tspeed: 0.1147s/iter; left time: 9564.5253s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0974504\n",
      "\tspeed: 0.1146s/iter; left time: 9545.8152s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0961491\n",
      "\tspeed: 0.1162s/iter; left time: 9668.4279s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1127011\n",
      "\tspeed: 0.1150s/iter; left time: 9551.3628s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1136763\n",
      "\tspeed: 0.1160s/iter; left time: 9629.7080s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1032368\n",
      "\tspeed: 0.1138s/iter; left time: 9430.6147s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0907084\n",
      "\tspeed: 0.1128s/iter; left time: 9339.1341s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1084257\n",
      "\tspeed: 0.1156s/iter; left time: 9560.5969s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1318485\n",
      "\tspeed: 0.1146s/iter; left time: 9465.4027s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1092268\n",
      "\tspeed: 0.1150s/iter; left time: 9482.2905s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0971450\n",
      "\tspeed: 0.1159s/iter; left time: 9546.3702s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1082254\n",
      "\tspeed: 0.1141s/iter; left time: 9386.9575s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1057774\n",
      "\tspeed: 0.1130s/iter; left time: 9288.2027s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1233867\n",
      "\tspeed: 0.1132s/iter; left time: 9294.7662s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1121541\n",
      "\tspeed: 0.1137s/iter; left time: 9322.9370s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1023491\n",
      "\tspeed: 0.1140s/iter; left time: 9334.0303s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1325897\n",
      "\tspeed: 0.1140s/iter; left time: 9320.0574s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0932151\n",
      "\tspeed: 0.1154s/iter; left time: 9425.6305s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1183982\n",
      "\tspeed: 0.1155s/iter; left time: 9426.6047s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1071068\n",
      "\tspeed: 0.1148s/iter; left time: 9355.3898s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0916324\n",
      "\tspeed: 0.1159s/iter; left time: 9432.0369s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0934265\n",
      "\tspeed: 0.1107s/iter; left time: 8998.8114s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1121869\n",
      "\tspeed: 0.1131s/iter; left time: 9182.5247s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1178536\n",
      "\tspeed: 0.1135s/iter; left time: 9204.1838s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1048463\n",
      "\tspeed: 0.1135s/iter; left time: 9189.4847s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1119690\n",
      "\tspeed: 0.1135s/iter; left time: 9180.8183s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1302823\n",
      "\tspeed: 0.1146s/iter; left time: 9255.9061s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1027673\n",
      "\tspeed: 0.1133s/iter; left time: 9141.1086s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1058483\n",
      "\tspeed: 0.1124s/iter; left time: 9057.8689s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1003960\n",
      "\tspeed: 0.1139s/iter; left time: 9169.8420s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0997679\n",
      "\tspeed: 0.1143s/iter; left time: 9190.6403s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1080339\n",
      "\tspeed: 0.1127s/iter; left time: 9048.9508s\n",
      "Epoch: 2 cost time: 00h:08m:30.12s\n",
      "Epoch: 2 | Train Loss: 0.1063434 Vali Loss: 0.1159233 Test Loss: 0.1404574\n",
      "Validation loss decreased (0.117180 --> 0.115923).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0953313\n",
      "\tspeed: 1.5298s/iter; left time: 122579.7741s\n",
      "\titers: 200, epoch: 3 | loss: 0.0960641\n",
      "\tspeed: 0.1129s/iter; left time: 9037.5760s\n",
      "\titers: 300, epoch: 3 | loss: 0.1168496\n",
      "\tspeed: 0.1128s/iter; left time: 9018.4111s\n",
      "\titers: 400, epoch: 3 | loss: 0.1128009\n",
      "\tspeed: 0.1139s/iter; left time: 9091.4012s\n",
      "\titers: 500, epoch: 3 | loss: 0.1003084\n",
      "\tspeed: 0.1134s/iter; left time: 9037.9726s\n",
      "\titers: 600, epoch: 3 | loss: 0.1153115\n",
      "\tspeed: 0.1150s/iter; left time: 9153.9593s\n",
      "\titers: 700, epoch: 3 | loss: 0.1167073\n",
      "\tspeed: 0.1128s/iter; left time: 8969.6655s\n",
      "\titers: 800, epoch: 3 | loss: 0.1098146\n",
      "\tspeed: 0.1131s/iter; left time: 8983.1531s\n",
      "\titers: 900, epoch: 3 | loss: 0.0888241\n",
      "\tspeed: 0.1131s/iter; left time: 8970.7544s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963814\n",
      "\tspeed: 0.1139s/iter; left time: 9022.4195s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1247230\n",
      "\tspeed: 0.1133s/iter; left time: 8968.2897s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1112344\n",
      "\tspeed: 0.1125s/iter; left time: 8891.8323s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1090816\n",
      "\tspeed: 0.1131s/iter; left time: 8923.5137s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1014220\n",
      "\tspeed: 0.1129s/iter; left time: 8897.4957s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1205137\n",
      "\tspeed: 0.1142s/iter; left time: 8989.3578s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0969793\n",
      "\tspeed: 0.1143s/iter; left time: 8985.5839s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1139035\n",
      "\tspeed: 0.1153s/iter; left time: 9057.3622s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1128580\n",
      "\tspeed: 0.1135s/iter; left time: 8903.3702s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1043471\n",
      "\tspeed: 0.1139s/iter; left time: 8917.9254s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0987270\n",
      "\tspeed: 0.1139s/iter; left time: 8912.4455s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0881278\n",
      "\tspeed: 0.1126s/iter; left time: 8796.8159s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1081288\n",
      "\tspeed: 0.1120s/iter; left time: 8738.4418s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976076\n",
      "\tspeed: 0.1126s/iter; left time: 8772.7511s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0961220\n",
      "\tspeed: 0.1128s/iter; left time: 8775.3532s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0965293\n",
      "\tspeed: 0.1127s/iter; left time: 8760.7689s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1095369\n",
      "\tspeed: 0.1120s/iter; left time: 8697.7938s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1212904\n",
      "\tspeed: 0.1144s/iter; left time: 8865.4812s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1003948\n",
      "\tspeed: 0.1133s/iter; left time: 8775.0465s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1081474\n",
      "\tspeed: 0.1133s/iter; left time: 8761.8701s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1015081\n",
      "\tspeed: 0.1129s/iter; left time: 8715.9515s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0989254\n",
      "\tspeed: 0.1135s/iter; left time: 8754.3035s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1039556\n",
      "\tspeed: 0.1128s/iter; left time: 8692.0616s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0903572\n",
      "\tspeed: 0.1132s/iter; left time: 8709.2199s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0759418\n",
      "\tspeed: 0.1130s/iter; left time: 8682.8255s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1176762\n",
      "\tspeed: 0.1120s/iter; left time: 8595.1412s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0941180\n",
      "\tspeed: 0.1109s/iter; left time: 8494.2782s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1044800\n",
      "\tspeed: 0.1118s/iter; left time: 8553.7069s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1119280\n",
      "\tspeed: 0.1118s/iter; left time: 8543.0616s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1174034\n",
      "\tspeed: 0.1113s/iter; left time: 8495.1171s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0906112\n",
      "\tspeed: 0.1126s/iter; left time: 8584.3571s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1110326\n",
      "\tspeed: 0.1132s/iter; left time: 8617.3934s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1211934\n",
      "\tspeed: 0.1132s/iter; left time: 8603.3143s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1134209\n",
      "\tspeed: 0.1138s/iter; left time: 8643.2430s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0881434\n",
      "\tspeed: 0.1132s/iter; left time: 8582.5215s\n",
      "Epoch: 3 cost time: 00h:08m:24.63s\n",
      "Epoch: 3 | Train Loss: 0.1022726 Vali Loss: 0.1180037 Test Loss: 0.1440704\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0873546\n",
      "\tspeed: 1.5100s/iter; left time: 114263.3071s\n",
      "\titers: 200, epoch: 4 | loss: 0.1036950\n",
      "\tspeed: 0.1139s/iter; left time: 8609.2430s\n",
      "\titers: 300, epoch: 4 | loss: 0.0934119\n",
      "\tspeed: 0.1139s/iter; left time: 8593.5786s\n",
      "\titers: 400, epoch: 4 | loss: 0.1056632\n",
      "\tspeed: 0.1150s/iter; left time: 8664.7593s\n",
      "\titers: 500, epoch: 4 | loss: 0.0883508\n",
      "\tspeed: 0.1135s/iter; left time: 8540.5182s\n",
      "\titers: 600, epoch: 4 | loss: 0.0899846\n",
      "\tspeed: 0.1128s/iter; left time: 8478.9874s\n",
      "\titers: 700, epoch: 4 | loss: 0.0998617\n",
      "\tspeed: 0.1132s/iter; left time: 8494.6099s\n",
      "\titers: 800, epoch: 4 | loss: 0.0895698\n",
      "\tspeed: 0.1122s/iter; left time: 8408.1319s\n",
      "\titers: 900, epoch: 4 | loss: 0.1108057\n",
      "\tspeed: 0.1133s/iter; left time: 8481.2006s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1005503\n",
      "\tspeed: 0.1139s/iter; left time: 8514.6839s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0851609\n",
      "\tspeed: 0.1138s/iter; left time: 8497.9852s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0922965\n",
      "\tspeed: 0.1133s/iter; left time: 8452.3653s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0940839\n",
      "\tspeed: 0.1130s/iter; left time: 8413.9763s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0969528\n",
      "\tspeed: 0.1134s/iter; left time: 8436.9304s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0909017\n",
      "\tspeed: 0.1125s/iter; left time: 8358.9278s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0886021\n",
      "\tspeed: 0.1141s/iter; left time: 8463.9709s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0961353\n",
      "\tspeed: 0.1132s/iter; left time: 8384.6226s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0882971\n",
      "\tspeed: 0.1139s/iter; left time: 8427.3077s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0977799\n",
      "\tspeed: 0.1143s/iter; left time: 8446.6530s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0880991\n",
      "\tspeed: 0.1126s/iter; left time: 8309.3337s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0910579\n",
      "\tspeed: 0.1132s/iter; left time: 8340.1650s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0977751\n",
      "\tspeed: 0.1129s/iter; left time: 8305.7125s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0931557\n",
      "\tspeed: 0.1141s/iter; left time: 8380.7850s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1089414\n",
      "\tspeed: 0.1128s/iter; left time: 8275.7434s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1060289\n",
      "\tspeed: 0.1136s/iter; left time: 8326.6336s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0834869\n",
      "\tspeed: 0.1109s/iter; left time: 8113.2451s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1122872\n",
      "\tspeed: 0.1139s/iter; left time: 8325.9781s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0878354\n",
      "\tspeed: 0.1136s/iter; left time: 8292.4274s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0907886\n",
      "\tspeed: 0.1146s/iter; left time: 8353.3154s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0977618\n",
      "\tspeed: 0.1131s/iter; left time: 8230.6683s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0923466\n",
      "\tspeed: 0.1129s/iter; left time: 8204.9562s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0991563\n",
      "\tspeed: 0.1173s/iter; left time: 8508.9227s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1049987\n",
      "\tspeed: 0.1162s/iter; left time: 8419.4591s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0708893\n",
      "\tspeed: 0.1140s/iter; left time: 8252.0366s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0792143\n",
      "\tspeed: 0.1137s/iter; left time: 8215.6924s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0895646\n",
      "\tspeed: 0.1126s/iter; left time: 8129.8788s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1049187\n",
      "\tspeed: 0.1127s/iter; left time: 8124.3975s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0842904\n",
      "\tspeed: 0.1129s/iter; left time: 8126.8559s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0941178\n",
      "\tspeed: 0.1108s/iter; left time: 7961.7936s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1109897\n",
      "\tspeed: 0.1139s/iter; left time: 8177.1761s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1001586\n",
      "\tspeed: 0.1125s/iter; left time: 8065.4055s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1093832\n",
      "\tspeed: 0.1127s/iter; left time: 8068.1410s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0876529\n",
      "\tspeed: 0.1123s/iter; left time: 8028.4879s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952323\n",
      "\tspeed: 0.1132s/iter; left time: 8081.8479s\n",
      "Epoch: 4 cost time: 00h:08m:25.73s\n",
      "Epoch: 4 | Train Loss: 0.0973941 Vali Loss: 0.1181420 Test Loss: 0.1481285\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0899206\n",
      "\tspeed: 1.5235s/iter; left time: 108491.2324s\n",
      "\titers: 200, epoch: 5 | loss: 0.1038604\n",
      "\tspeed: 0.1154s/iter; left time: 8205.3218s\n",
      "\titers: 300, epoch: 5 | loss: 0.1103195\n",
      "\tspeed: 0.1156s/iter; left time: 8207.7192s\n",
      "\titers: 400, epoch: 5 | loss: 0.0988987\n",
      "\tspeed: 0.1153s/iter; left time: 8173.8791s\n",
      "\titers: 500, epoch: 5 | loss: 0.1051342\n",
      "\tspeed: 0.1136s/iter; left time: 8046.3911s\n",
      "\titers: 600, epoch: 5 | loss: 0.0994484\n",
      "\tspeed: 0.1174s/iter; left time: 8298.3677s\n",
      "\titers: 700, epoch: 5 | loss: 0.0807362\n",
      "\tspeed: 0.1152s/iter; left time: 8132.8384s\n",
      "\titers: 800, epoch: 5 | loss: 0.1006454\n",
      "\tspeed: 0.1152s/iter; left time: 8125.9078s\n",
      "\titers: 900, epoch: 5 | loss: 0.1117228\n",
      "\tspeed: 0.1140s/iter; left time: 8025.7115s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0958334\n",
      "\tspeed: 0.1154s/iter; left time: 8114.8124s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0945488\n",
      "\tspeed: 0.1149s/iter; left time: 8068.0753s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0999299\n",
      "\tspeed: 0.1168s/iter; left time: 8190.4201s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0833874\n",
      "\tspeed: 0.1151s/iter; left time: 8061.9571s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0800731\n",
      "\tspeed: 0.1166s/iter; left time: 8150.3149s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0873000\n",
      "\tspeed: 0.1170s/iter; left time: 8169.8289s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0974982\n",
      "\tspeed: 0.1148s/iter; left time: 8005.9197s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0832424\n",
      "\tspeed: 0.1148s/iter; left time: 7990.4724s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0964874\n",
      "\tspeed: 0.1142s/iter; left time: 7938.4620s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1015488\n",
      "\tspeed: 0.1148s/iter; left time: 7971.9532s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0962706\n",
      "\tspeed: 0.1139s/iter; left time: 7893.5208s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0944651\n",
      "\tspeed: 0.1158s/iter; left time: 8014.9157s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0883529\n",
      "\tspeed: 0.1142s/iter; left time: 7892.4674s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0954471\n",
      "\tspeed: 0.1152s/iter; left time: 7952.3328s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0827175\n",
      "\tspeed: 0.1146s/iter; left time: 7899.6119s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1006347\n",
      "\tspeed: 0.1144s/iter; left time: 7870.0949s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0951255\n",
      "\tspeed: 0.1165s/iter; left time: 8005.8141s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1033527\n",
      "\tspeed: 0.1175s/iter; left time: 8060.1569s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0756043\n",
      "\tspeed: 0.1147s/iter; left time: 7858.7596s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0850169\n",
      "\tspeed: 0.1152s/iter; left time: 7882.4580s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1090293\n",
      "\tspeed: 0.1144s/iter; left time: 7814.8763s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0975026\n",
      "\tspeed: 0.1145s/iter; left time: 7813.4333s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0954872\n",
      "\tspeed: 0.1149s/iter; left time: 7823.0406s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0834685\n",
      "\tspeed: 0.1166s/iter; left time: 7931.9783s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0841382\n",
      "\tspeed: 0.1164s/iter; left time: 7902.3602s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0745072\n",
      "\tspeed: 0.1160s/iter; left time: 7863.0053s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1053615\n",
      "\tspeed: 0.1157s/iter; left time: 7837.6776s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0791975\n",
      "\tspeed: 0.1165s/iter; left time: 7879.8089s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0998375\n",
      "\tspeed: 0.1157s/iter; left time: 7814.3196s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0949914\n",
      "\tspeed: 0.1157s/iter; left time: 7801.0721s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0968361\n",
      "\tspeed: 0.1149s/iter; left time: 7735.8557s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0942416\n",
      "\tspeed: 0.1143s/iter; left time: 7685.6907s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1008289\n",
      "\tspeed: 0.1138s/iter; left time: 7634.2870s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0779671\n",
      "\tspeed: 0.1151s/iter; left time: 7711.2709s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0973403\n",
      "\tspeed: 0.1155s/iter; left time: 7725.3286s\n",
      "Epoch: 5 cost time: 00h:08m:34.07s\n",
      "Epoch: 5 | Train Loss: 0.0930669 Vali Loss: 0.1177695 Test Loss: 0.1494393\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0950565\n",
      "\tspeed: 1.5478s/iter; left time: 103326.1281s\n",
      "\titers: 200, epoch: 6 | loss: 0.0880194\n",
      "\tspeed: 0.1140s/iter; left time: 7598.5819s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845609\n",
      "\tspeed: 0.1146s/iter; left time: 7628.5804s\n",
      "\titers: 400, epoch: 6 | loss: 0.0856797\n",
      "\tspeed: 0.1171s/iter; left time: 7784.0110s\n",
      "\titers: 500, epoch: 6 | loss: 0.0811084\n",
      "\tspeed: 0.1152s/iter; left time: 7641.6682s\n",
      "\titers: 600, epoch: 6 | loss: 0.1066922\n",
      "\tspeed: 0.1153s/iter; left time: 7640.4823s\n",
      "\titers: 700, epoch: 6 | loss: 0.0946200\n",
      "\tspeed: 0.1154s/iter; left time: 7632.8492s\n",
      "\titers: 800, epoch: 6 | loss: 0.1000760\n",
      "\tspeed: 0.1148s/iter; left time: 7581.1448s\n",
      "\titers: 900, epoch: 6 | loss: 0.0695545\n",
      "\tspeed: 0.1170s/iter; left time: 7717.6166s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1009072\n",
      "\tspeed: 0.1155s/iter; left time: 7606.8553s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0820811\n",
      "\tspeed: 0.1148s/iter; left time: 7547.9258s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1012075\n",
      "\tspeed: 0.1142s/iter; left time: 7495.7848s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0863402\n",
      "\tspeed: 0.1158s/iter; left time: 7588.2879s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0826545\n",
      "\tspeed: 0.1159s/iter; left time: 7587.4863s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0777465\n",
      "\tspeed: 0.1156s/iter; left time: 7557.8843s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1057802\n",
      "\tspeed: 0.1166s/iter; left time: 7611.9507s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0942861\n",
      "\tspeed: 0.1162s/iter; left time: 7569.9625s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0920153\n",
      "\tspeed: 0.1164s/iter; left time: 7574.8956s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0782236\n",
      "\tspeed: 0.1161s/iter; left time: 7542.9847s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0907045\n",
      "\tspeed: 0.1116s/iter; left time: 7237.2041s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0759282\n",
      "\tspeed: 0.1137s/iter; left time: 7359.8177s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1017066\n",
      "\tspeed: 0.1148s/iter; left time: 7422.9566s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0679502\n",
      "\tspeed: 0.1181s/iter; left time: 7621.3176s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0776927\n",
      "\tspeed: 0.1167s/iter; left time: 7525.2174s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0888472\n",
      "\tspeed: 0.1147s/iter; left time: 7383.1680s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0768875\n",
      "\tspeed: 0.1166s/iter; left time: 7491.8912s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0782935\n",
      "\tspeed: 0.1167s/iter; left time: 7489.6762s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0903737\n",
      "\tspeed: 0.1143s/iter; left time: 7319.0551s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1004810\n",
      "\tspeed: 0.1151s/iter; left time: 7362.3392s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0948790\n",
      "\tspeed: 0.1163s/iter; left time: 7428.4829s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0886455\n",
      "\tspeed: 0.1128s/iter; left time: 7194.6490s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0854608\n",
      "\tspeed: 0.1158s/iter; left time: 7370.2266s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0769016\n",
      "\tspeed: 0.1162s/iter; left time: 7384.1257s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0817375\n",
      "\tspeed: 0.1164s/iter; left time: 7384.1548s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0888851\n",
      "\tspeed: 0.1152s/iter; left time: 7295.7853s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0727457\n",
      "\tspeed: 0.1156s/iter; left time: 7311.4228s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0903441\n",
      "\tspeed: 0.1161s/iter; left time: 7334.4639s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0950212\n",
      "\tspeed: 0.1164s/iter; left time: 7338.4941s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0870087\n",
      "\tspeed: 0.1164s/iter; left time: 7330.3570s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0871792\n",
      "\tspeed: 0.1141s/iter; left time: 7170.3075s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0932686\n",
      "\tspeed: 0.1145s/iter; left time: 7187.2149s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0874465\n",
      "\tspeed: 0.1179s/iter; left time: 7389.2674s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0889675\n",
      "\tspeed: 0.1146s/iter; left time: 7171.8204s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0926063\n",
      "\tspeed: 0.1141s/iter; left time: 7129.0473s\n",
      "Epoch: 6 cost time: 00h:08m:35.30s\n",
      "Epoch: 6 | Train Loss: 0.0893757 Vali Loss: 0.1196648 Test Loss: 0.1541289\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1048594\n",
      "\tspeed: 1.5670s/iter; left time: 97624.5618s\n",
      "\titers: 200, epoch: 7 | loss: 0.0843566\n",
      "\tspeed: 0.1158s/iter; left time: 7202.4942s\n",
      "\titers: 300, epoch: 7 | loss: 0.0917804\n",
      "\tspeed: 0.1159s/iter; left time: 7197.7076s\n",
      "\titers: 400, epoch: 7 | loss: 0.0795149\n",
      "\tspeed: 0.1163s/iter; left time: 7208.8362s\n",
      "\titers: 500, epoch: 7 | loss: 0.0842911\n",
      "\tspeed: 0.1164s/iter; left time: 7204.4799s\n",
      "\titers: 600, epoch: 7 | loss: 0.0841490\n",
      "\tspeed: 0.1165s/iter; left time: 7197.0453s\n",
      "\titers: 700, epoch: 7 | loss: 0.1009502\n",
      "\tspeed: 0.1176s/iter; left time: 7256.1310s\n",
      "\titers: 800, epoch: 7 | loss: 0.0739675\n",
      "\tspeed: 0.1168s/iter; left time: 7195.5046s\n",
      "\titers: 900, epoch: 7 | loss: 0.0888571\n",
      "\tspeed: 0.1167s/iter; left time: 7176.0577s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0870971\n",
      "\tspeed: 0.1167s/iter; left time: 7162.7047s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0928913\n",
      "\tspeed: 0.1164s/iter; left time: 7136.7999s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0886939\n",
      "\tspeed: 0.1136s/iter; left time: 6953.2413s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0893422\n",
      "\tspeed: 0.1159s/iter; left time: 7082.8468s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0847866\n",
      "\tspeed: 0.1163s/iter; left time: 7092.0799s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0769473\n",
      "\tspeed: 0.1159s/iter; left time: 7060.6891s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0924601\n",
      "\tspeed: 0.1148s/iter; left time: 6981.6211s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0808676\n",
      "\tspeed: 0.1129s/iter; left time: 6850.2870s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0922723\n",
      "\tspeed: 0.1167s/iter; left time: 7071.4805s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0814242\n",
      "\tspeed: 0.1156s/iter; left time: 6992.7185s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0870774\n",
      "\tspeed: 0.1151s/iter; left time: 6950.2598s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0811431\n",
      "\tspeed: 0.1156s/iter; left time: 6970.0987s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0951526\n",
      "\tspeed: 0.1132s/iter; left time: 6812.4758s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0774082\n",
      "\tspeed: 0.1148s/iter; left time: 6898.2062s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0838267\n",
      "\tspeed: 0.1152s/iter; left time: 6909.5983s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0840728\n",
      "\tspeed: 0.1140s/iter; left time: 6825.7495s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0857305\n",
      "\tspeed: 0.1154s/iter; left time: 6900.0571s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0911212\n",
      "\tspeed: 0.1161s/iter; left time: 6931.8133s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0788338\n",
      "\tspeed: 0.1136s/iter; left time: 6770.5995s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0812203\n",
      "\tspeed: 0.1129s/iter; left time: 6717.0057s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0968771\n",
      "\tspeed: 0.1153s/iter; left time: 6846.5521s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0815360\n",
      "\tspeed: 0.1158s/iter; left time: 6866.4748s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0820606\n",
      "\tspeed: 0.1159s/iter; left time: 6862.3245s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0775566\n",
      "\tspeed: 0.1134s/iter; left time: 6703.6955s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1014951\n",
      "\tspeed: 0.1149s/iter; left time: 6777.4195s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0893218\n",
      "\tspeed: 0.1148s/iter; left time: 6759.9584s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0940796\n",
      "\tspeed: 0.1172s/iter; left time: 6888.9346s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0836638\n",
      "\tspeed: 0.1131s/iter; left time: 6636.1693s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1027704\n",
      "\tspeed: 0.1150s/iter; left time: 6736.3065s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0680492\n",
      "\tspeed: 0.1139s/iter; left time: 6663.9134s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1034615\n",
      "\tspeed: 0.1145s/iter; left time: 6683.8426s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0949323\n",
      "\tspeed: 0.1138s/iter; left time: 6632.1384s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0865043\n",
      "\tspeed: 0.1150s/iter; left time: 6693.9101s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0960433\n",
      "\tspeed: 0.1149s/iter; left time: 6677.9859s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0965981\n",
      "\tspeed: 0.1142s/iter; left time: 6624.5261s\n",
      "Epoch: 7 cost time: 00h:08m:34.14s\n",
      "Epoch: 7 | Train Loss: 0.0862290 Vali Loss: 0.1190629 Test Loss: 0.1528910\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04200141876935959, rmse:0.2049424797296524, mae:0.14045733213424683, rse:0.7086868286132812\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 96: 01h:18m:33.66s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-03 09:16:27,227] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 09:16:28,647] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 09:16:28,647] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 09:16:28,648] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 09:16:28,773] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 09:16:28,773] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 09:16:29,576] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 09:16:29,577] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 09:16:29,577] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 09:16:29,579] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 09:16:29,579] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 09:16:29,579] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 09:16:29,986] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 09:16:29,987] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:29,988] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.5 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,171] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 09:16:30,172] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:30,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.51 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,172] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 09:16:30,348] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 09:16:30,349] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:30,349] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.51 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,350] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f80738ae1d0>\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1482348\n",
      "\tspeed: 0.1806s/iter; left time: 16043.0744s\n",
      "\titers: 200, epoch: 1 | loss: 0.1453139\n",
      "\tspeed: 0.1298s/iter; left time: 11516.6845s\n",
      "\titers: 300, epoch: 1 | loss: 0.1553313\n",
      "\tspeed: 0.1296s/iter; left time: 11486.2147s\n",
      "\titers: 400, epoch: 1 | loss: 0.1554058\n",
      "\tspeed: 0.1298s/iter; left time: 11493.2830s\n",
      "\titers: 500, epoch: 1 | loss: 0.1471658\n",
      "\tspeed: 0.1276s/iter; left time: 11279.4548s\n",
      "\titers: 600, epoch: 1 | loss: 0.1542638\n",
      "\tspeed: 0.1299s/iter; left time: 11473.4496s\n",
      "\titers: 700, epoch: 1 | loss: 0.1336013\n",
      "\tspeed: 0.1302s/iter; left time: 11485.4159s\n",
      "\titers: 800, epoch: 1 | loss: 0.1162374\n",
      "\tspeed: 0.1307s/iter; left time: 11521.5088s\n",
      "\titers: 900, epoch: 1 | loss: 0.1247154\n",
      "\tspeed: 0.1290s/iter; left time: 11358.7186s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1020275\n",
      "\tspeed: 0.1299s/iter; left time: 11422.7040s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1375454\n",
      "\tspeed: 0.1282s/iter; left time: 11259.6940s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1161896\n",
      "\tspeed: 0.1314s/iter; left time: 11523.9214s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1136029\n",
      "\tspeed: 0.1286s/iter; left time: 11267.5807s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1167578\n",
      "\tspeed: 0.1286s/iter; left time: 11253.9748s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1177376\n",
      "\tspeed: 0.1304s/iter; left time: 11401.8118s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1085610\n",
      "\tspeed: 0.1304s/iter; left time: 11385.5133s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1154755\n",
      "\tspeed: 0.1287s/iter; left time: 11223.4444s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0951475\n",
      "\tspeed: 0.1291s/iter; left time: 11248.3587s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1343727\n",
      "\tspeed: 0.1315s/iter; left time: 11440.2881s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1305830\n",
      "\tspeed: 0.1288s/iter; left time: 11194.6836s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1169467\n",
      "\tspeed: 0.1273s/iter; left time: 11050.9776s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1000786\n",
      "\tspeed: 0.1265s/iter; left time: 10966.5075s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0901878\n",
      "\tspeed: 0.1276s/iter; left time: 11055.7600s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1336366\n",
      "\tspeed: 0.1273s/iter; left time: 11016.1030s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1131255\n",
      "\tspeed: 0.1289s/iter; left time: 11142.5916s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1157848\n",
      "\tspeed: 0.1275s/iter; left time: 11003.2673s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1087803\n",
      "\tspeed: 0.1296s/iter; left time: 11171.4330s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1142708\n",
      "\tspeed: 0.1293s/iter; left time: 11135.9459s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1243213\n",
      "\tspeed: 0.1301s/iter; left time: 11193.6051s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0919846\n",
      "\tspeed: 0.1320s/iter; left time: 11342.2288s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1445014\n",
      "\tspeed: 0.1291s/iter; left time: 11081.8479s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0993669\n",
      "\tspeed: 0.1289s/iter; left time: 11046.5269s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1163438\n",
      "\tspeed: 0.1293s/iter; left time: 11069.4295s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1209128\n",
      "\tspeed: 0.1283s/iter; left time: 10975.8670s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0993587\n",
      "\tspeed: 0.1268s/iter; left time: 10831.2122s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1136707\n",
      "\tspeed: 0.1281s/iter; left time: 10929.0588s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1139015\n",
      "\tspeed: 0.1290s/iter; left time: 10996.0999s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1168825\n",
      "\tspeed: 0.1306s/iter; left time: 11112.7088s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059878\n",
      "\tspeed: 0.1280s/iter; left time: 10882.4153s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1127900\n",
      "\tspeed: 0.1280s/iter; left time: 10865.7985s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1130453\n",
      "\tspeed: 0.1272s/iter; left time: 10787.6265s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1120209\n",
      "\tspeed: 0.1293s/iter; left time: 10953.2413s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1165368\n",
      "\tspeed: 0.1280s/iter; left time: 10831.6520s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1169618\n",
      "\tspeed: 0.1292s/iter; left time: 10917.5637s\n",
      "Epoch: 1 cost time: 00h:09m:35.04s\n",
      "Epoch: 1 | Train Loss: 0.1205897 Vali Loss: 0.1209952 Test Loss: 0.1438364\n",
      "Validation loss decreased (inf --> 0.120995).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1197857\n",
      "\tspeed: 1.8114s/iter; left time: 152836.4726s\n",
      "\titers: 200, epoch: 2 | loss: 0.1099122\n",
      "\tspeed: 0.1168s/iter; left time: 9845.0516s\n",
      "\titers: 300, epoch: 2 | loss: 0.1050249\n",
      "\tspeed: 0.1172s/iter; left time: 9866.8842s\n",
      "\titers: 400, epoch: 2 | loss: 0.1127598\n",
      "\tspeed: 0.1170s/iter; left time: 9837.4411s\n",
      "\titers: 500, epoch: 2 | loss: 0.0847004\n",
      "\tspeed: 0.1180s/iter; left time: 9909.6726s\n",
      "\titers: 600, epoch: 2 | loss: 0.1266375\n",
      "\tspeed: 0.1166s/iter; left time: 9776.1746s\n",
      "\titers: 700, epoch: 2 | loss: 0.1209411\n",
      "\tspeed: 0.1175s/iter; left time: 9841.5933s\n",
      "\titers: 800, epoch: 2 | loss: 0.1068922\n",
      "\tspeed: 0.1167s/iter; left time: 9763.5481s\n",
      "\titers: 900, epoch: 2 | loss: 0.0993572\n",
      "\tspeed: 0.1166s/iter; left time: 9743.6605s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1088792\n",
      "\tspeed: 0.1193s/iter; left time: 9954.9116s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1103762\n",
      "\tspeed: 0.1184s/iter; left time: 9871.9279s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1110987\n",
      "\tspeed: 0.1178s/iter; left time: 9809.1376s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0816215\n",
      "\tspeed: 0.1163s/iter; left time: 9669.8693s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1149543\n",
      "\tspeed: 0.1170s/iter; left time: 9723.7857s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1127167\n",
      "\tspeed: 0.1155s/iter; left time: 9583.8694s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1143629\n",
      "\tspeed: 0.1171s/iter; left time: 9706.3140s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1134126\n",
      "\tspeed: 0.1158s/iter; left time: 9581.8043s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1078830\n",
      "\tspeed: 0.1166s/iter; left time: 9640.7901s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0992961\n",
      "\tspeed: 0.1160s/iter; left time: 9576.2030s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1067948\n",
      "\tspeed: 0.1166s/iter; left time: 9617.4982s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1038578\n",
      "\tspeed: 0.1145s/iter; left time: 9432.7129s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177262\n",
      "\tspeed: 0.1165s/iter; left time: 9584.8832s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1152344\n",
      "\tspeed: 0.1140s/iter; left time: 9365.2021s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1077954\n",
      "\tspeed: 0.1157s/iter; left time: 9498.0317s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1127032\n",
      "\tspeed: 0.1171s/iter; left time: 9602.6803s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1118686\n",
      "\tspeed: 0.1122s/iter; left time: 9184.2086s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1096208\n",
      "\tspeed: 0.1129s/iter; left time: 9234.7673s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0965174\n",
      "\tspeed: 0.1125s/iter; left time: 9190.5343s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1007073\n",
      "\tspeed: 0.1129s/iter; left time: 9208.9160s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0923950\n",
      "\tspeed: 0.1153s/iter; left time: 9395.5281s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1115430\n",
      "\tspeed: 0.1138s/iter; left time: 9260.0025s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1126868\n",
      "\tspeed: 0.1146s/iter; left time: 9311.4516s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1034839\n",
      "\tspeed: 0.1118s/iter; left time: 9072.6644s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1078405\n",
      "\tspeed: 0.1113s/iter; left time: 9022.3053s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1037599\n",
      "\tspeed: 0.1137s/iter; left time: 9208.3424s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0952058\n",
      "\tspeed: 0.1122s/iter; left time: 9076.4295s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1051804\n",
      "\tspeed: 0.1120s/iter; left time: 9045.5504s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1044087\n",
      "\tspeed: 0.1130s/iter; left time: 9113.2990s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1066130\n",
      "\tspeed: 0.1147s/iter; left time: 9241.7066s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092677\n",
      "\tspeed: 0.1133s/iter; left time: 9121.1625s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1083696\n",
      "\tspeed: 0.1133s/iter; left time: 9110.3547s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1197105\n",
      "\tspeed: 0.1132s/iter; left time: 9089.6904s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1068081\n",
      "\tspeed: 0.1117s/iter; left time: 8954.9869s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1261329\n",
      "\tspeed: 0.1128s/iter; left time: 9030.0277s\n",
      "Epoch: 2 cost time: 00h:08m:32.27s\n",
      "Epoch: 2 | Train Loss: 0.1098766 Vali Loss: 0.1225332 Test Loss: 0.1477604\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0998430\n",
      "\tspeed: 1.5405s/iter; left time: 123129.9321s\n",
      "\titers: 200, epoch: 3 | loss: 0.1029584\n",
      "\tspeed: 0.1128s/iter; left time: 9008.1590s\n",
      "\titers: 300, epoch: 3 | loss: 0.0965041\n",
      "\tspeed: 0.1161s/iter; left time: 9253.1586s\n",
      "\titers: 400, epoch: 3 | loss: 0.1077358\n",
      "\tspeed: 0.1167s/iter; left time: 9291.2182s\n",
      "\titers: 500, epoch: 3 | loss: 0.1056079\n",
      "\tspeed: 0.1153s/iter; left time: 9165.7626s\n",
      "\titers: 600, epoch: 3 | loss: 0.1190159\n",
      "\tspeed: 0.1167s/iter; left time: 9271.6581s\n",
      "\titers: 700, epoch: 3 | loss: 0.1028953\n",
      "\tspeed: 0.1166s/iter; left time: 9248.6437s\n",
      "\titers: 800, epoch: 3 | loss: 0.1252995\n",
      "\tspeed: 0.1160s/iter; left time: 9193.5876s\n",
      "\titers: 900, epoch: 3 | loss: 0.0910322\n",
      "\tspeed: 0.1143s/iter; left time: 9045.1347s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1096593\n",
      "\tspeed: 0.1166s/iter; left time: 9214.4819s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0958329\n",
      "\tspeed: 0.1161s/iter; left time: 9163.4822s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1051666\n",
      "\tspeed: 0.1139s/iter; left time: 8978.8459s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1145009\n",
      "\tspeed: 0.1157s/iter; left time: 9111.8886s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1089370\n",
      "\tspeed: 0.1166s/iter; left time: 9170.4829s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0982486\n",
      "\tspeed: 0.1153s/iter; left time: 9055.8982s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1116064\n",
      "\tspeed: 0.1162s/iter; left time: 9112.0542s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1152646\n",
      "\tspeed: 0.1147s/iter; left time: 8981.9711s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1033108\n",
      "\tspeed: 0.1157s/iter; left time: 9047.4058s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1160483\n",
      "\tspeed: 0.1145s/iter; left time: 8942.6018s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1133725\n",
      "\tspeed: 0.1152s/iter; left time: 8988.5962s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0946887\n",
      "\tspeed: 0.1162s/iter; left time: 9056.8138s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1068842\n",
      "\tspeed: 0.1158s/iter; left time: 9008.9040s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1091909\n",
      "\tspeed: 0.1141s/iter; left time: 8869.8343s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0988755\n",
      "\tspeed: 0.1144s/iter; left time: 8880.5515s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0938124\n",
      "\tspeed: 0.1152s/iter; left time: 8929.7448s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1091160\n",
      "\tspeed: 0.1153s/iter; left time: 8929.1321s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1095193\n",
      "\tspeed: 0.1152s/iter; left time: 8908.6028s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1060785\n",
      "\tspeed: 0.1142s/iter; left time: 8823.2657s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0896944\n",
      "\tspeed: 0.1151s/iter; left time: 8875.4527s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0934717\n",
      "\tspeed: 0.1164s/iter; left time: 8969.1079s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1221983\n",
      "\tspeed: 0.1152s/iter; left time: 8861.5937s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0963738\n",
      "\tspeed: 0.1147s/iter; left time: 8815.4265s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1190489\n",
      "\tspeed: 0.1161s/iter; left time: 8908.4538s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1036496\n",
      "\tspeed: 0.1165s/iter; left time: 8928.5374s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1124387\n",
      "\tspeed: 0.1168s/iter; left time: 8939.4267s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1170041\n",
      "\tspeed: 0.1155s/iter; left time: 8827.6652s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1089111\n",
      "\tspeed: 0.1159s/iter; left time: 8850.0768s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0951907\n",
      "\tspeed: 0.1147s/iter; left time: 8746.3940s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0979321\n",
      "\tspeed: 0.1144s/iter; left time: 8706.1174s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0999806\n",
      "\tspeed: 0.1163s/iter; left time: 8842.1246s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0940368\n",
      "\tspeed: 0.1148s/iter; left time: 8719.9476s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0999659\n",
      "\tspeed: 0.1162s/iter; left time: 8810.3419s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0951753\n",
      "\tspeed: 0.1153s/iter; left time: 8732.1293s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0947775\n",
      "\tspeed: 0.1145s/iter; left time: 8662.4110s\n",
      "Epoch: 3 cost time: 00h:08m:33.85s\n",
      "Epoch: 3 | Train Loss: 0.1043304 Vali Loss: 0.1245715 Test Loss: 0.1501074\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0944092\n",
      "\tspeed: 1.5497s/iter; left time: 116975.3279s\n",
      "\titers: 200, epoch: 4 | loss: 0.0867483\n",
      "\tspeed: 0.1149s/iter; left time: 8658.0705s\n",
      "\titers: 300, epoch: 4 | loss: 0.1064334\n",
      "\tspeed: 0.1155s/iter; left time: 8698.3918s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956620\n",
      "\tspeed: 0.1158s/iter; left time: 8705.6986s\n",
      "\titers: 500, epoch: 4 | loss: 0.1117885\n",
      "\tspeed: 0.1166s/iter; left time: 8758.2921s\n",
      "\titers: 600, epoch: 4 | loss: 0.1063067\n",
      "\tspeed: 0.1150s/iter; left time: 8626.7746s\n",
      "\titers: 700, epoch: 4 | loss: 0.0915639\n",
      "\tspeed: 0.1156s/iter; left time: 8655.7974s\n",
      "\titers: 800, epoch: 4 | loss: 0.1026455\n",
      "\tspeed: 0.1153s/iter; left time: 8624.9928s\n",
      "\titers: 900, epoch: 4 | loss: 0.1035407\n",
      "\tspeed: 0.1159s/iter; left time: 8652.7572s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1075997\n",
      "\tspeed: 0.1163s/iter; left time: 8675.9252s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0891981\n",
      "\tspeed: 0.1140s/iter; left time: 8491.5195s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1041374\n",
      "\tspeed: 0.1134s/iter; left time: 8438.0519s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1022310\n",
      "\tspeed: 0.1133s/iter; left time: 8418.8444s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1072065\n",
      "\tspeed: 0.1122s/iter; left time: 8324.7086s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0838472\n",
      "\tspeed: 0.1134s/iter; left time: 8403.0457s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0923450\n",
      "\tspeed: 0.1137s/iter; left time: 8410.4056s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1015804\n",
      "\tspeed: 0.1152s/iter; left time: 8511.1904s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1218250\n",
      "\tspeed: 0.1150s/iter; left time: 8486.1731s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1150391\n",
      "\tspeed: 0.1155s/iter; left time: 8510.3849s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1164713\n",
      "\tspeed: 0.1148s/iter; left time: 8449.4671s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1034276\n",
      "\tspeed: 0.1131s/iter; left time: 8312.4625s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0898664\n",
      "\tspeed: 0.1181s/iter; left time: 8665.1293s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0815171\n",
      "\tspeed: 0.1187s/iter; left time: 8698.5061s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1059342\n",
      "\tspeed: 0.1173s/iter; left time: 8586.7144s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1042058\n",
      "\tspeed: 0.1165s/iter; left time: 8515.5548s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0878983\n",
      "\tspeed: 0.1162s/iter; left time: 8481.3466s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0832504\n",
      "\tspeed: 0.1187s/iter; left time: 8649.1016s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0833858\n",
      "\tspeed: 0.1179s/iter; left time: 8580.0004s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1033833\n",
      "\tspeed: 0.1149s/iter; left time: 8354.4193s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0952015\n",
      "\tspeed: 0.1168s/iter; left time: 8477.1869s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0781978\n",
      "\tspeed: 0.1149s/iter; left time: 8326.1763s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1017636\n",
      "\tspeed: 0.1138s/iter; left time: 8238.4425s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1010068\n",
      "\tspeed: 0.1156s/iter; left time: 8357.3458s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1017999\n",
      "\tspeed: 0.1184s/iter; left time: 8548.1644s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0878085\n",
      "\tspeed: 0.1168s/iter; left time: 8419.6193s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0985364\n",
      "\tspeed: 0.1170s/iter; left time: 8422.9713s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0910970\n",
      "\tspeed: 0.1139s/iter; left time: 8190.7131s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0913839\n",
      "\tspeed: 0.1141s/iter; left time: 8192.0792s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918390\n",
      "\tspeed: 0.1143s/iter; left time: 8189.9435s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1008365\n",
      "\tspeed: 0.1174s/iter; left time: 8401.5474s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0926459\n",
      "\tspeed: 0.1177s/iter; left time: 8410.5593s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0991397\n",
      "\tspeed: 0.1154s/iter; left time: 8239.6806s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0954166\n",
      "\tspeed: 0.1142s/iter; left time: 8141.7228s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0878920\n",
      "\tspeed: 0.1161s/iter; left time: 8266.2998s\n",
      "Epoch: 4 cost time: 00h:08m:34.28s\n",
      "Epoch: 4 | Train Loss: 0.0984264 Vali Loss: 0.1281532 Test Loss: 0.1525455\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1027972\n",
      "\tspeed: 1.5516s/iter; left time: 110220.2287s\n",
      "\titers: 200, epoch: 5 | loss: 0.1047456\n",
      "\tspeed: 0.1157s/iter; left time: 8208.4461s\n",
      "\titers: 300, epoch: 5 | loss: 0.0907510\n",
      "\tspeed: 0.1167s/iter; left time: 8265.7405s\n",
      "\titers: 400, epoch: 5 | loss: 0.0943936\n",
      "\tspeed: 0.1172s/iter; left time: 8292.2366s\n",
      "\titers: 500, epoch: 5 | loss: 0.0821496\n",
      "\tspeed: 0.1161s/iter; left time: 8200.0644s\n",
      "\titers: 600, epoch: 5 | loss: 0.0936060\n",
      "\tspeed: 0.1160s/iter; left time: 8185.4850s\n",
      "\titers: 700, epoch: 5 | loss: 0.0996595\n",
      "\tspeed: 0.1171s/iter; left time: 8250.5821s\n",
      "\titers: 800, epoch: 5 | loss: 0.0979459\n",
      "\tspeed: 0.1154s/iter; left time: 8119.3812s\n",
      "\titers: 900, epoch: 5 | loss: 0.1001264\n",
      "\tspeed: 0.1133s/iter; left time: 7958.2826s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1090219\n",
      "\tspeed: 0.1141s/iter; left time: 8005.5584s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1011967\n",
      "\tspeed: 0.1139s/iter; left time: 7978.4549s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0915010\n",
      "\tspeed: 0.1158s/iter; left time: 8096.2294s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1061302\n",
      "\tspeed: 0.1162s/iter; left time: 8116.5520s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1118270\n",
      "\tspeed: 0.1164s/iter; left time: 8118.4551s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0901711\n",
      "\tspeed: 0.1157s/iter; left time: 8055.1593s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0990031\n",
      "\tspeed: 0.1164s/iter; left time: 8095.2933s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1073154\n",
      "\tspeed: 0.1151s/iter; left time: 7990.1939s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0957215\n",
      "\tspeed: 0.1146s/iter; left time: 7948.0382s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0969568\n",
      "\tspeed: 0.1147s/iter; left time: 7938.2746s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1063399\n",
      "\tspeed: 0.1152s/iter; left time: 7965.5585s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1092876\n",
      "\tspeed: 0.1184s/iter; left time: 8170.8366s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0891375\n",
      "\tspeed: 0.1147s/iter; left time: 7910.2817s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0779103\n",
      "\tspeed: 0.1175s/iter; left time: 8087.8896s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0807228\n",
      "\tspeed: 0.1168s/iter; left time: 8025.9800s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0742790\n",
      "\tspeed: 0.1168s/iter; left time: 8016.6586s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0908970\n",
      "\tspeed: 0.1147s/iter; left time: 7858.3526s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1129368\n",
      "\tspeed: 0.1143s/iter; left time: 7820.8059s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0733277\n",
      "\tspeed: 0.1184s/iter; left time: 8090.8699s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1080936\n",
      "\tspeed: 0.1183s/iter; left time: 8070.8115s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0774377\n",
      "\tspeed: 0.1167s/iter; left time: 7949.2214s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0847868\n",
      "\tspeed: 0.1155s/iter; left time: 7861.0918s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0902236\n",
      "\tspeed: 0.1149s/iter; left time: 7805.2258s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0947069\n",
      "\tspeed: 0.1163s/iter; left time: 7886.0970s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0958610\n",
      "\tspeed: 0.1161s/iter; left time: 7863.7970s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0908997\n",
      "\tspeed: 0.1148s/iter; left time: 7767.2626s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0939663\n",
      "\tspeed: 0.1141s/iter; left time: 7708.7375s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0827342\n",
      "\tspeed: 0.1151s/iter; left time: 7761.5226s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1009660\n",
      "\tspeed: 0.1139s/iter; left time: 7667.6180s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0889867\n",
      "\tspeed: 0.1142s/iter; left time: 7681.4587s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0949150\n",
      "\tspeed: 0.1134s/iter; left time: 7613.2535s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1014487\n",
      "\tspeed: 0.1142s/iter; left time: 7656.9560s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0968869\n",
      "\tspeed: 0.1153s/iter; left time: 7716.1146s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0901529\n",
      "\tspeed: 0.1157s/iter; left time: 7732.6608s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0915154\n",
      "\tspeed: 0.1180s/iter; left time: 7871.9444s\n",
      "Epoch: 5 cost time: 00h:08m:34.78s\n",
      "Epoch: 5 | Train Loss: 0.0934047 Vali Loss: 0.1274233 Test Loss: 0.1514446\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0924059\n",
      "\tspeed: 1.5476s/iter; left time: 103055.2063s\n",
      "\titers: 200, epoch: 6 | loss: 0.0953507\n",
      "\tspeed: 0.1191s/iter; left time: 7915.8442s\n",
      "\titers: 300, epoch: 6 | loss: 0.0743414\n",
      "\tspeed: 0.1163s/iter; left time: 7718.9871s\n",
      "\titers: 400, epoch: 6 | loss: 0.0888031\n",
      "\tspeed: 0.1185s/iter; left time: 7852.1903s\n",
      "\titers: 500, epoch: 6 | loss: 0.1043609\n",
      "\tspeed: 0.1168s/iter; left time: 7733.2412s\n",
      "\titers: 600, epoch: 6 | loss: 0.0847825\n",
      "\tspeed: 0.1188s/iter; left time: 7854.5862s\n",
      "\titers: 700, epoch: 6 | loss: 0.1001649\n",
      "\tspeed: 0.1176s/iter; left time: 7758.1293s\n",
      "\titers: 800, epoch: 6 | loss: 0.1008426\n",
      "\tspeed: 0.1162s/iter; left time: 7657.0937s\n",
      "\titers: 900, epoch: 6 | loss: 0.0914951\n",
      "\tspeed: 0.1168s/iter; left time: 7683.7625s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0961552\n",
      "\tspeed: 0.1188s/iter; left time: 7801.7333s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0863231\n",
      "\tspeed: 0.1186s/iter; left time: 7781.1236s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0925186\n",
      "\tspeed: 0.1202s/iter; left time: 7871.2631s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0857735\n",
      "\tspeed: 0.1174s/iter; left time: 7675.7391s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0983386\n",
      "\tspeed: 0.1202s/iter; left time: 7851.0174s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0876124\n",
      "\tspeed: 0.1204s/iter; left time: 7846.9987s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0781590\n",
      "\tspeed: 0.1156s/iter; left time: 7526.7923s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0933763\n",
      "\tspeed: 0.1205s/iter; left time: 7832.0395s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0982791\n",
      "\tspeed: 0.1149s/iter; left time: 7454.8488s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0728418\n",
      "\tspeed: 0.1143s/iter; left time: 7402.4854s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0914764\n",
      "\tspeed: 0.1159s/iter; left time: 7497.4194s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0815687\n",
      "\tspeed: 0.1173s/iter; left time: 7575.4451s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0897236\n",
      "\tspeed: 0.1172s/iter; left time: 7557.0015s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0844050\n",
      "\tspeed: 0.1166s/iter; left time: 7508.0284s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0881767\n",
      "\tspeed: 0.1161s/iter; left time: 7461.0308s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0834182\n",
      "\tspeed: 0.1222s/iter; left time: 7841.6178s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0942059\n",
      "\tspeed: 0.1146s/iter; left time: 7342.8139s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0970327\n",
      "\tspeed: 0.1168s/iter; left time: 7472.0707s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989593\n",
      "\tspeed: 0.1172s/iter; left time: 7486.8818s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0968164\n",
      "\tspeed: 0.1192s/iter; left time: 7603.9717s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0916561\n",
      "\tspeed: 0.1167s/iter; left time: 7429.6563s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0938281\n",
      "\tspeed: 0.1172s/iter; left time: 7455.4710s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0751842\n",
      "\tspeed: 0.1199s/iter; left time: 7613.1808s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0846767\n",
      "\tspeed: 0.1195s/iter; left time: 7572.9081s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0979014\n",
      "\tspeed: 0.1223s/iter; left time: 7737.9127s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0967091\n",
      "\tspeed: 0.1144s/iter; left time: 7230.6336s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0888710\n",
      "\tspeed: 0.1178s/iter; left time: 7430.2501s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0812877\n",
      "\tspeed: 0.1143s/iter; left time: 7198.5841s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0757051\n",
      "\tspeed: 0.1140s/iter; left time: 7168.3648s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0898106\n",
      "\tspeed: 0.1154s/iter; left time: 7244.5771s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1019688\n",
      "\tspeed: 0.1157s/iter; left time: 7252.7306s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0886026\n",
      "\tspeed: 0.1212s/iter; left time: 7585.3348s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0841476\n",
      "\tspeed: 0.1170s/iter; left time: 7309.4672s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0868416\n",
      "\tspeed: 0.1157s/iter; left time: 7217.7561s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0976313\n",
      "\tspeed: 0.1141s/iter; left time: 7107.8399s\n",
      "Epoch: 6 cost time: 00h:08m:42.56s\n",
      "Epoch: 6 | Train Loss: 0.0896105 Vali Loss: 0.1277131 Test Loss: 0.1531950\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04277150332927704, rmse:0.20681272447109222, mae:0.14383640885353088, rse:0.7167763113975525\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 168: 01h:08m:23.04s\n",
      "\n",
      "Intermediate time for GB: 04h:19m:43.32s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 10:24:52,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 10:24:54,700] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 10:24:54,700] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 10:24:54,700] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 10:24:54,895] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 10:24:54,895] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 10:24:55,670] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 10:24:55,671] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 10:24:55,672] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 10:24:55,673] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 10:24:55,673] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 10:24:55,673] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 10:24:56,136] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 10:24:56,138] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.22 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,274] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 10:24:56,275] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,275] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.16 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,275] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 10:24:56,398] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 10:24:56,399] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,399] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.13 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f755575a050>\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1790063\n",
      "\tspeed: 0.1804s/iter; left time: 9654.8354s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687252\n",
      "\tspeed: 0.1288s/iter; left time: 6882.6025s\n",
      "\titers: 300, epoch: 1 | loss: 0.1309537\n",
      "\tspeed: 0.1273s/iter; left time: 6785.8478s\n",
      "\titers: 400, epoch: 1 | loss: 0.1245550\n",
      "\tspeed: 0.1274s/iter; left time: 6779.2307s\n",
      "\titers: 500, epoch: 1 | loss: 0.1051500\n",
      "\tspeed: 0.1296s/iter; left time: 6883.5540s\n",
      "\titers: 600, epoch: 1 | loss: 0.0886984\n",
      "\tspeed: 0.1291s/iter; left time: 6847.2720s\n",
      "\titers: 700, epoch: 1 | loss: 0.0941334\n",
      "\tspeed: 0.1298s/iter; left time: 6866.7539s\n",
      "\titers: 800, epoch: 1 | loss: 0.0780938\n",
      "\tspeed: 0.1254s/iter; left time: 6625.6681s\n",
      "\titers: 900, epoch: 1 | loss: 0.0840373\n",
      "\tspeed: 0.1287s/iter; left time: 6784.1158s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0893998\n",
      "\tspeed: 0.1293s/iter; left time: 6803.2337s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0767727\n",
      "\tspeed: 0.1283s/iter; left time: 6739.6440s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0874630\n",
      "\tspeed: 0.1275s/iter; left time: 6685.8349s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0802690\n",
      "\tspeed: 0.1263s/iter; left time: 6608.3775s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0984690\n",
      "\tspeed: 0.1274s/iter; left time: 6652.6463s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0814931\n",
      "\tspeed: 0.1302s/iter; left time: 6785.3352s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0775438\n",
      "\tspeed: 0.1344s/iter; left time: 6989.4507s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801652\n",
      "\tspeed: 0.1321s/iter; left time: 6860.4821s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0743842\n",
      "\tspeed: 0.1277s/iter; left time: 6618.4710s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0854426\n",
      "\tspeed: 0.1287s/iter; left time: 6654.7200s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0780391\n",
      "\tspeed: 0.1286s/iter; left time: 6638.0561s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0724551\n",
      "\tspeed: 0.1272s/iter; left time: 6552.3398s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0770094\n",
      "\tspeed: 0.1267s/iter; left time: 6515.2400s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0864393\n",
      "\tspeed: 0.1267s/iter; left time: 6501.0342s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0777969\n",
      "\tspeed: 0.1283s/iter; left time: 6572.1778s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0745067\n",
      "\tspeed: 0.1275s/iter; left time: 6518.0973s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0927263\n",
      "\tspeed: 0.1279s/iter; left time: 6526.9623s\n",
      "Epoch: 1 cost time: 00h:05m:45.68s\n",
      "Epoch: 1 | Train Loss: 0.0993755 Vali Loss: 0.0662276 Test Loss: 0.0754743\n",
      "Validation loss decreased (inf --> 0.066228).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0936529\n",
      "\tspeed: 1.2303s/iter; left time: 62549.7002s\n",
      "\titers: 200, epoch: 2 | loss: 0.0912237\n",
      "\tspeed: 0.1184s/iter; left time: 6010.0761s\n",
      "\titers: 300, epoch: 2 | loss: 0.0807760\n",
      "\tspeed: 0.1178s/iter; left time: 5966.7274s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864775\n",
      "\tspeed: 0.1187s/iter; left time: 5997.6103s\n",
      "\titers: 500, epoch: 2 | loss: 0.0780736\n",
      "\tspeed: 0.1146s/iter; left time: 5781.9483s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745789\n",
      "\tspeed: 0.1154s/iter; left time: 5809.9201s\n",
      "\titers: 700, epoch: 2 | loss: 0.0697930\n",
      "\tspeed: 0.1180s/iter; left time: 5926.9884s\n",
      "\titers: 800, epoch: 2 | loss: 0.0780450\n",
      "\tspeed: 0.1161s/iter; left time: 5821.2941s\n",
      "\titers: 900, epoch: 2 | loss: 0.0729557\n",
      "\tspeed: 0.1163s/iter; left time: 5819.5825s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0800805\n",
      "\tspeed: 0.1147s/iter; left time: 5727.8370s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0736994\n",
      "\tspeed: 0.1143s/iter; left time: 5696.9979s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0718818\n",
      "\tspeed: 0.1132s/iter; left time: 5631.1617s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0728408\n",
      "\tspeed: 0.1143s/iter; left time: 5672.7104s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0732936\n",
      "\tspeed: 0.1155s/iter; left time: 5721.5886s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0862661\n",
      "\tspeed: 0.1155s/iter; left time: 5708.7566s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0885015\n",
      "\tspeed: 0.1165s/iter; left time: 5746.3741s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0699046\n",
      "\tspeed: 0.1194s/iter; left time: 5877.9799s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0673015\n",
      "\tspeed: 0.1178s/iter; left time: 5786.2836s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0852279\n",
      "\tspeed: 0.1161s/iter; left time: 5693.6767s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0779036\n",
      "\tspeed: 0.1150s/iter; left time: 5628.4731s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0709876\n",
      "\tspeed: 0.1149s/iter; left time: 5610.1158s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0774782\n",
      "\tspeed: 0.1137s/iter; left time: 5539.6278s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0751852\n",
      "\tspeed: 0.1162s/iter; left time: 5654.1786s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0819853\n",
      "\tspeed: 0.1158s/iter; left time: 5620.3485s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0652728\n",
      "\tspeed: 0.1170s/iter; left time: 5668.2419s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0770506\n",
      "\tspeed: 0.1167s/iter; left time: 5641.0626s\n",
      "Epoch: 2 cost time: 00h:05m:11.76s\n",
      "Epoch: 2 | Train Loss: 0.0773266 Vali Loss: 0.0630829 Test Loss: 0.0720199\n",
      "Validation loss decreased (0.066228 --> 0.063083).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0664506\n",
      "\tspeed: 1.0860s/iter; left time: 52298.8217s\n",
      "\titers: 200, epoch: 3 | loss: 0.0780226\n",
      "\tspeed: 0.1148s/iter; left time: 5515.8168s\n",
      "\titers: 300, epoch: 3 | loss: 0.0809838\n",
      "\tspeed: 0.1146s/iter; left time: 5496.9904s\n",
      "\titers: 400, epoch: 3 | loss: 0.0751593\n",
      "\tspeed: 0.1146s/iter; left time: 5482.8314s\n",
      "\titers: 500, epoch: 3 | loss: 0.0690148\n",
      "\tspeed: 0.1186s/iter; left time: 5664.2265s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651948\n",
      "\tspeed: 0.1200s/iter; left time: 5717.9900s\n",
      "\titers: 700, epoch: 3 | loss: 0.0696828\n",
      "\tspeed: 0.1192s/iter; left time: 5669.5340s\n",
      "\titers: 800, epoch: 3 | loss: 0.0909723\n",
      "\tspeed: 0.1180s/iter; left time: 5601.0442s\n",
      "\titers: 900, epoch: 3 | loss: 0.0693156\n",
      "\tspeed: 0.1179s/iter; left time: 5583.0343s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0763046\n",
      "\tspeed: 0.1202s/iter; left time: 5681.1260s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0654815\n",
      "\tspeed: 0.1174s/iter; left time: 5538.4323s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0816431\n",
      "\tspeed: 0.1205s/iter; left time: 5671.2245s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0731011\n",
      "\tspeed: 0.1176s/iter; left time: 5522.5890s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0798532\n",
      "\tspeed: 0.1202s/iter; left time: 5634.7652s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0781588\n",
      "\tspeed: 0.1197s/iter; left time: 5594.8528s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0688082\n",
      "\tspeed: 0.1186s/iter; left time: 5535.3495s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0722890\n",
      "\tspeed: 0.1157s/iter; left time: 5385.5811s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0578647\n",
      "\tspeed: 0.1150s/iter; left time: 5343.9319s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0831098\n",
      "\tspeed: 0.1192s/iter; left time: 5524.3157s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0657302\n",
      "\tspeed: 0.1168s/iter; left time: 5401.8942s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0686098\n",
      "\tspeed: 0.1184s/iter; left time: 5463.8973s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0793400\n",
      "\tspeed: 0.1222s/iter; left time: 5627.4213s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0739723\n",
      "\tspeed: 0.1189s/iter; left time: 5464.7096s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0883555\n",
      "\tspeed: 0.1152s/iter; left time: 5282.4396s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0636316\n",
      "\tspeed: 0.1158s/iter; left time: 5297.6672s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0704639\n",
      "\tspeed: 0.1139s/iter; left time: 5199.1854s\n",
      "Epoch: 3 cost time: 00h:05m:15.64s\n",
      "Epoch: 3 | Train Loss: 0.0731855 Vali Loss: 0.0623539 Test Loss: 0.0720562\n",
      "Validation loss decreased (0.063083 --> 0.062354).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0695748\n",
      "\tspeed: 1.0455s/iter; left time: 47545.2099s\n",
      "\titers: 200, epoch: 4 | loss: 0.0786053\n",
      "\tspeed: 0.1144s/iter; left time: 5191.6691s\n",
      "\titers: 300, epoch: 4 | loss: 0.0584018\n",
      "\tspeed: 0.1150s/iter; left time: 5205.4502s\n",
      "\titers: 400, epoch: 4 | loss: 0.0929034\n",
      "\tspeed: 0.1155s/iter; left time: 5216.2159s\n",
      "\titers: 500, epoch: 4 | loss: 0.0779471\n",
      "\tspeed: 0.1114s/iter; left time: 5020.0233s\n",
      "\titers: 600, epoch: 4 | loss: 0.0695291\n",
      "\tspeed: 0.1130s/iter; left time: 5082.2797s\n",
      "\titers: 700, epoch: 4 | loss: 0.0666811\n",
      "\tspeed: 0.1151s/iter; left time: 5166.2757s\n",
      "\titers: 800, epoch: 4 | loss: 0.0734229\n",
      "\tspeed: 0.1154s/iter; left time: 5167.0127s\n",
      "\titers: 900, epoch: 4 | loss: 0.0612417\n",
      "\tspeed: 0.1138s/iter; left time: 5084.3318s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0748140\n",
      "\tspeed: 0.1142s/iter; left time: 5088.6718s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0748632\n",
      "\tspeed: 0.1153s/iter; left time: 5127.5359s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0688734\n",
      "\tspeed: 0.1159s/iter; left time: 5144.1363s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0741981\n",
      "\tspeed: 0.1160s/iter; left time: 5137.8647s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0658868\n",
      "\tspeed: 0.1148s/iter; left time: 5072.0988s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0664924\n",
      "\tspeed: 0.1150s/iter; left time: 5068.6878s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0765673\n",
      "\tspeed: 0.1144s/iter; left time: 5031.6526s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0755662\n",
      "\tspeed: 0.1157s/iter; left time: 5074.7618s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0777128\n",
      "\tspeed: 0.1148s/iter; left time: 5026.6644s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0851511\n",
      "\tspeed: 0.1154s/iter; left time: 5040.0799s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0781193\n",
      "\tspeed: 0.1161s/iter; left time: 5057.9787s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0825148\n",
      "\tspeed: 0.1156s/iter; left time: 5025.7944s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0718397\n",
      "\tspeed: 0.1155s/iter; left time: 5009.7955s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0678933\n",
      "\tspeed: 0.1169s/iter; left time: 5058.4917s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0727424\n",
      "\tspeed: 0.1153s/iter; left time: 4979.7414s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0754367\n",
      "\tspeed: 0.1147s/iter; left time: 4941.4695s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0694987\n",
      "\tspeed: 0.1147s/iter; left time: 4927.9194s\n",
      "Epoch: 4 cost time: 00h:05m:08.58s\n",
      "Epoch: 4 | Train Loss: 0.0712121 Vali Loss: 0.0611709 Test Loss: 0.0710917\n",
      "Validation loss decreased (0.062354 --> 0.061171).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0630679\n",
      "\tspeed: 1.0569s/iter; left time: 45230.5762s\n",
      "\titers: 200, epoch: 5 | loss: 0.0689823\n",
      "\tspeed: 0.1152s/iter; left time: 4917.2996s\n",
      "\titers: 300, epoch: 5 | loss: 0.0810203\n",
      "\tspeed: 0.1162s/iter; left time: 4949.1596s\n",
      "\titers: 400, epoch: 5 | loss: 0.0599910\n",
      "\tspeed: 0.1150s/iter; left time: 4886.2999s\n",
      "\titers: 500, epoch: 5 | loss: 0.0720650\n",
      "\tspeed: 0.1143s/iter; left time: 4845.0679s\n",
      "\titers: 600, epoch: 5 | loss: 0.0723064\n",
      "\tspeed: 0.1147s/iter; left time: 4852.3544s\n",
      "\titers: 700, epoch: 5 | loss: 0.0714625\n",
      "\tspeed: 0.1139s/iter; left time: 4807.6518s\n",
      "\titers: 800, epoch: 5 | loss: 0.0736922\n",
      "\tspeed: 0.1128s/iter; left time: 4748.6478s\n",
      "\titers: 900, epoch: 5 | loss: 0.0730744\n",
      "\tspeed: 0.1134s/iter; left time: 4760.4105s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0762590\n",
      "\tspeed: 0.1141s/iter; left time: 4781.9349s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0776123\n",
      "\tspeed: 0.1133s/iter; left time: 4735.9502s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0766415\n",
      "\tspeed: 0.1127s/iter; left time: 4697.1781s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0700914\n",
      "\tspeed: 0.1137s/iter; left time: 4729.9397s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0647124\n",
      "\tspeed: 0.1127s/iter; left time: 4676.9359s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0750828\n",
      "\tspeed: 0.1134s/iter; left time: 4694.3295s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0726999\n",
      "\tspeed: 0.1137s/iter; left time: 4695.2950s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0699233\n",
      "\tspeed: 0.1123s/iter; left time: 4626.5963s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0737871\n",
      "\tspeed: 0.1139s/iter; left time: 4682.4217s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0672794\n",
      "\tspeed: 0.1147s/iter; left time: 4701.4137s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0721373\n",
      "\tspeed: 0.1153s/iter; left time: 4716.3622s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0577321\n",
      "\tspeed: 0.1148s/iter; left time: 4682.7861s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0599433\n",
      "\tspeed: 0.1149s/iter; left time: 4677.9945s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0708436\n",
      "\tspeed: 0.1131s/iter; left time: 4590.2444s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0647120\n",
      "\tspeed: 0.1143s/iter; left time: 4627.6918s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0707248\n",
      "\tspeed: 0.1142s/iter; left time: 4613.0119s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0699116\n",
      "\tspeed: 0.1138s/iter; left time: 4585.3660s\n",
      "Epoch: 5 cost time: 00h:05m:06.02s\n",
      "Epoch: 5 | Train Loss: 0.0699275 Vali Loss: 0.0589704 Test Loss: 0.0681652\n",
      "Validation loss decreased (0.061171 --> 0.058970).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0585817\n",
      "\tspeed: 1.0444s/iter; left time: 41895.8387s\n",
      "\titers: 200, epoch: 6 | loss: 0.0743628\n",
      "\tspeed: 0.1150s/iter; left time: 4601.7528s\n",
      "\titers: 300, epoch: 6 | loss: 0.0816830\n",
      "\tspeed: 0.1144s/iter; left time: 4568.0115s\n",
      "\titers: 400, epoch: 6 | loss: 0.0686249\n",
      "\tspeed: 0.1157s/iter; left time: 4606.7715s\n",
      "\titers: 500, epoch: 6 | loss: 0.0658081\n",
      "\tspeed: 0.1160s/iter; left time: 4607.8605s\n",
      "\titers: 600, epoch: 6 | loss: 0.0699370\n",
      "\tspeed: 0.1123s/iter; left time: 4448.1751s\n",
      "\titers: 700, epoch: 6 | loss: 0.0579729\n",
      "\tspeed: 0.1141s/iter; left time: 4509.3703s\n",
      "\titers: 800, epoch: 6 | loss: 0.0594627\n",
      "\tspeed: 0.1135s/iter; left time: 4473.9968s\n",
      "\titers: 900, epoch: 6 | loss: 0.0765333\n",
      "\tspeed: 0.1126s/iter; left time: 4425.4338s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0679004\n",
      "\tspeed: 0.1127s/iter; left time: 4419.3486s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0628908\n",
      "\tspeed: 0.1142s/iter; left time: 4465.3444s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0643912\n",
      "\tspeed: 0.1134s/iter; left time: 4422.9509s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0718672\n",
      "\tspeed: 0.1122s/iter; left time: 4366.1406s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0650968\n",
      "\tspeed: 0.1133s/iter; left time: 4397.1992s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0803147\n",
      "\tspeed: 0.1126s/iter; left time: 4357.9873s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721307\n",
      "\tspeed: 0.1128s/iter; left time: 4355.1406s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0664947\n",
      "\tspeed: 0.1121s/iter; left time: 4318.4247s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0713947\n",
      "\tspeed: 0.1131s/iter; left time: 4345.8420s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0666180\n",
      "\tspeed: 0.1140s/iter; left time: 4367.4006s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0742579\n",
      "\tspeed: 0.1155s/iter; left time: 4415.1730s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0744588\n",
      "\tspeed: 0.1138s/iter; left time: 4336.3663s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0583553\n",
      "\tspeed: 0.1137s/iter; left time: 4324.2063s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0706729\n",
      "\tspeed: 0.1135s/iter; left time: 4304.5950s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0550006\n",
      "\tspeed: 0.1142s/iter; left time: 4317.8107s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0605696\n",
      "\tspeed: 0.1146s/iter; left time: 4324.0003s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0701861\n",
      "\tspeed: 0.1142s/iter; left time: 4296.4502s\n",
      "Epoch: 6 cost time: 00h:05m:05.76s\n",
      "Epoch: 6 | Train Loss: 0.0689073 Vali Loss: 0.0598308 Test Loss: 0.0697114\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0657653\n",
      "\tspeed: 1.0371s/iter; left time: 38823.0358s\n",
      "\titers: 200, epoch: 7 | loss: 0.0665484\n",
      "\tspeed: 0.1143s/iter; left time: 4265.6868s\n",
      "\titers: 300, epoch: 7 | loss: 0.0624240\n",
      "\tspeed: 0.1151s/iter; left time: 4287.0299s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643224\n",
      "\tspeed: 0.1143s/iter; left time: 4245.1658s\n",
      "\titers: 500, epoch: 7 | loss: 0.0836057\n",
      "\tspeed: 0.1148s/iter; left time: 4250.6829s\n",
      "\titers: 600, epoch: 7 | loss: 0.0660850\n",
      "\tspeed: 0.1122s/iter; left time: 4144.5433s\n",
      "\titers: 700, epoch: 7 | loss: 0.0869880\n",
      "\tspeed: 0.1148s/iter; left time: 4227.2054s\n",
      "\titers: 800, epoch: 7 | loss: 0.0704906\n",
      "\tspeed: 0.1132s/iter; left time: 4157.9783s\n",
      "\titers: 900, epoch: 7 | loss: 0.0647015\n",
      "\tspeed: 0.1136s/iter; left time: 4162.2106s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0806929\n",
      "\tspeed: 0.1146s/iter; left time: 4187.5710s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0759010\n",
      "\tspeed: 0.1140s/iter; left time: 4153.3982s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0615634\n",
      "\tspeed: 0.1137s/iter; left time: 4131.3824s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0764042\n",
      "\tspeed: 0.1122s/iter; left time: 4064.2429s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674644\n",
      "\tspeed: 0.1125s/iter; left time: 4065.6722s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0740616\n",
      "\tspeed: 0.1134s/iter; left time: 4084.7985s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0792665\n",
      "\tspeed: 0.1136s/iter; left time: 4082.3793s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0690767\n",
      "\tspeed: 0.1127s/iter; left time: 4039.7644s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0598226\n",
      "\tspeed: 0.1149s/iter; left time: 4104.9907s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0678398\n",
      "\tspeed: 0.1154s/iter; left time: 4113.3813s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0747984\n",
      "\tspeed: 0.1150s/iter; left time: 4087.7855s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0712984\n",
      "\tspeed: 0.1151s/iter; left time: 4077.3569s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0657289\n",
      "\tspeed: 0.1137s/iter; left time: 4016.5526s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0730863\n",
      "\tspeed: 0.1156s/iter; left time: 4073.4537s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0687535\n",
      "\tspeed: 0.1143s/iter; left time: 4015.7164s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0626324\n",
      "\tspeed: 0.1146s/iter; left time: 4016.0510s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0694073\n",
      "\tspeed: 0.1139s/iter; left time: 3980.2275s\n",
      "Epoch: 7 cost time: 00h:05m:06.51s\n",
      "Epoch: 7 | Train Loss: 0.0681056 Vali Loss: 0.0597853 Test Loss: 0.0696563\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0955697\n",
      "\tspeed: 1.0430s/iter; left time: 36246.7374s\n",
      "\titers: 200, epoch: 8 | loss: 0.0681316\n",
      "\tspeed: 0.1134s/iter; left time: 3929.1527s\n",
      "\titers: 300, epoch: 8 | loss: 0.0796526\n",
      "\tspeed: 0.1125s/iter; left time: 3887.7026s\n",
      "\titers: 400, epoch: 8 | loss: 0.0672699\n",
      "\tspeed: 0.1137s/iter; left time: 3916.2965s\n",
      "\titers: 500, epoch: 8 | loss: 0.0579451\n",
      "\tspeed: 0.1137s/iter; left time: 3905.9708s\n",
      "\titers: 600, epoch: 8 | loss: 0.0730144\n",
      "\tspeed: 0.1146s/iter; left time: 3924.0622s\n",
      "\titers: 700, epoch: 8 | loss: 0.0561597\n",
      "\tspeed: 0.1131s/iter; left time: 3861.1952s\n",
      "\titers: 800, epoch: 8 | loss: 0.0630030\n",
      "\tspeed: 0.1123s/iter; left time: 3823.9748s\n",
      "\titers: 900, epoch: 8 | loss: 0.0696586\n",
      "\tspeed: 0.1131s/iter; left time: 3839.3908s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0760989\n",
      "\tspeed: 0.1129s/iter; left time: 3823.1975s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0601159\n",
      "\tspeed: 0.1125s/iter; left time: 3798.2438s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0697419\n",
      "\tspeed: 0.1127s/iter; left time: 3792.1555s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0581929\n",
      "\tspeed: 0.1156s/iter; left time: 3878.0686s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0630530\n",
      "\tspeed: 0.1134s/iter; left time: 3792.1820s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0653922\n",
      "\tspeed: 0.1125s/iter; left time: 3752.6265s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0719701\n",
      "\tspeed: 0.1122s/iter; left time: 3729.8680s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0626506\n",
      "\tspeed: 0.1124s/iter; left time: 3727.5302s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0601200\n",
      "\tspeed: 0.1121s/iter; left time: 3706.5486s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0739461\n",
      "\tspeed: 0.1125s/iter; left time: 3706.8998s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0698656\n",
      "\tspeed: 0.1148s/iter; left time: 3772.4910s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0620731\n",
      "\tspeed: 0.1124s/iter; left time: 3681.6874s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0645657\n",
      "\tspeed: 0.1139s/iter; left time: 3719.4823s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0703310\n",
      "\tspeed: 0.1129s/iter; left time: 3676.4973s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0673083\n",
      "\tspeed: 0.1124s/iter; left time: 3646.4198s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0710015\n",
      "\tspeed: 0.1135s/iter; left time: 3673.5000s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0631191\n",
      "\tspeed: 0.1134s/iter; left time: 3656.2917s\n",
      "Epoch: 8 cost time: 00h:05m:04.16s\n",
      "Epoch: 8 | Train Loss: 0.0675166 Vali Loss: 0.0581114 Test Loss: 0.0668070\n",
      "Validation loss decreased (0.058970 --> 0.058111).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0710958\n",
      "\tspeed: 1.0400s/iter; left time: 33355.3824s\n",
      "\titers: 200, epoch: 9 | loss: 0.0710134\n",
      "\tspeed: 0.1117s/iter; left time: 3570.8230s\n",
      "\titers: 300, epoch: 9 | loss: 0.0564387\n",
      "\tspeed: 0.1126s/iter; left time: 3588.6726s\n",
      "\titers: 400, epoch: 9 | loss: 0.0593314\n",
      "\tspeed: 0.1127s/iter; left time: 3581.2274s\n",
      "\titers: 500, epoch: 9 | loss: 0.0549250\n",
      "\tspeed: 0.1135s/iter; left time: 3594.9364s\n",
      "\titers: 600, epoch: 9 | loss: 0.0573312\n",
      "\tspeed: 0.1121s/iter; left time: 3538.6037s\n",
      "\titers: 700, epoch: 9 | loss: 0.0749223\n",
      "\tspeed: 0.1143s/iter; left time: 3597.2976s\n",
      "\titers: 800, epoch: 9 | loss: 0.0616827\n",
      "\tspeed: 0.1156s/iter; left time: 3627.1548s\n",
      "\titers: 900, epoch: 9 | loss: 0.0696181\n",
      "\tspeed: 0.1130s/iter; left time: 3533.0747s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0527223\n",
      "\tspeed: 0.1131s/iter; left time: 3524.5021s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0648112\n",
      "\tspeed: 0.1121s/iter; left time: 3484.1837s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0737804\n",
      "\tspeed: 0.1123s/iter; left time: 3478.6223s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0596698\n",
      "\tspeed: 0.1130s/iter; left time: 3488.1215s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0660387\n",
      "\tspeed: 0.1121s/iter; left time: 3449.5615s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0742668\n",
      "\tspeed: 0.1149s/iter; left time: 3523.9911s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0563254\n",
      "\tspeed: 0.1146s/iter; left time: 3504.2331s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0555071\n",
      "\tspeed: 0.1133s/iter; left time: 3453.3819s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0757507\n",
      "\tspeed: 0.1136s/iter; left time: 3451.1903s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0711249\n",
      "\tspeed: 0.1131s/iter; left time: 3423.7884s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0721804\n",
      "\tspeed: 0.1134s/iter; left time: 3420.5568s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0665950\n",
      "\tspeed: 0.1139s/iter; left time: 3425.0314s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0581865\n",
      "\tspeed: 0.1138s/iter; left time: 3411.8899s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0679084\n",
      "\tspeed: 0.1130s/iter; left time: 3376.8615s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0549888\n",
      "\tspeed: 0.1133s/iter; left time: 3372.3990s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0570390\n",
      "\tspeed: 0.1135s/iter; left time: 3368.7344s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0600057\n",
      "\tspeed: 0.1133s/iter; left time: 3350.1827s\n",
      "Epoch: 9 cost time: 00h:05m:04.05s\n",
      "Epoch: 9 | Train Loss: 0.0669224 Vali Loss: 0.0591406 Test Loss: 0.0683875\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0737776\n",
      "\tspeed: 1.0245s/iter; left time: 30110.7750s\n",
      "\titers: 200, epoch: 10 | loss: 0.0724111\n",
      "\tspeed: 0.1137s/iter; left time: 3329.1865s\n",
      "\titers: 300, epoch: 10 | loss: 0.0616018\n",
      "\tspeed: 0.1138s/iter; left time: 3321.2091s\n",
      "\titers: 400, epoch: 10 | loss: 0.0517321\n",
      "\tspeed: 0.1130s/iter; left time: 3287.4474s\n",
      "\titers: 500, epoch: 10 | loss: 0.0636202\n",
      "\tspeed: 0.1132s/iter; left time: 3281.2723s\n",
      "\titers: 600, epoch: 10 | loss: 0.0686904\n",
      "\tspeed: 0.1132s/iter; left time: 3270.4862s\n",
      "\titers: 700, epoch: 10 | loss: 0.0717656\n",
      "\tspeed: 0.1130s/iter; left time: 3254.5680s\n",
      "\titers: 800, epoch: 10 | loss: 0.0680537\n",
      "\tspeed: 0.1145s/iter; left time: 3284.0216s\n",
      "\titers: 900, epoch: 10 | loss: 0.0653778\n",
      "\tspeed: 0.1123s/iter; left time: 3211.9863s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0478307\n",
      "\tspeed: 0.1131s/iter; left time: 3222.6684s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0693082\n",
      "\tspeed: 0.1129s/iter; left time: 3204.8677s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0719121\n",
      "\tspeed: 0.1142s/iter; left time: 3231.4140s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0745866\n",
      "\tspeed: 0.1141s/iter; left time: 3215.3624s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0660974\n",
      "\tspeed: 0.1120s/iter; left time: 3146.3648s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0666839\n",
      "\tspeed: 0.1148s/iter; left time: 3213.9156s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0883639\n",
      "\tspeed: 0.1116s/iter; left time: 3113.4350s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0611924\n",
      "\tspeed: 0.1139s/iter; left time: 3166.0606s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0584188\n",
      "\tspeed: 0.1134s/iter; left time: 3139.2218s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0601275\n",
      "\tspeed: 0.1138s/iter; left time: 3138.8746s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0625480\n",
      "\tspeed: 0.1141s/iter; left time: 3136.6568s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0705743\n",
      "\tspeed: 0.1130s/iter; left time: 3095.4025s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633123\n",
      "\tspeed: 0.1131s/iter; left time: 3086.8010s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0581706\n",
      "\tspeed: 0.1119s/iter; left time: 3042.6933s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0554645\n",
      "\tspeed: 0.1136s/iter; left time: 3076.7753s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0550835\n",
      "\tspeed: 0.1134s/iter; left time: 3060.9786s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0590457\n",
      "\tspeed: 0.1126s/iter; left time: 3028.8592s\n",
      "Epoch: 10 cost time: 00h:05m:03.96s\n",
      "Epoch: 10 | Train Loss: 0.0663446 Vali Loss: 0.0583170 Test Loss: 0.0684556\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0767739\n",
      "\tspeed: 1.0372s/iter; left time: 27704.6867s\n",
      "\titers: 200, epoch: 11 | loss: 0.0565607\n",
      "\tspeed: 0.1145s/iter; left time: 3047.2512s\n",
      "\titers: 300, epoch: 11 | loss: 0.0709955\n",
      "\tspeed: 0.1149s/iter; left time: 3046.1056s\n",
      "\titers: 400, epoch: 11 | loss: 0.0625823\n",
      "\tspeed: 0.1138s/iter; left time: 3004.5073s\n",
      "\titers: 500, epoch: 11 | loss: 0.0661458\n",
      "\tspeed: 0.1146s/iter; left time: 3014.9425s\n",
      "\titers: 600, epoch: 11 | loss: 0.0606733\n",
      "\tspeed: 0.1151s/iter; left time: 3016.2975s\n",
      "\titers: 700, epoch: 11 | loss: 0.0683432\n",
      "\tspeed: 0.1140s/iter; left time: 2975.9419s\n",
      "\titers: 800, epoch: 11 | loss: 0.0626959\n",
      "\tspeed: 0.1138s/iter; left time: 2959.4348s\n",
      "\titers: 900, epoch: 11 | loss: 0.0729644\n",
      "\tspeed: 0.1129s/iter; left time: 2924.6392s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0603989\n",
      "\tspeed: 0.1131s/iter; left time: 2919.0391s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0806362\n",
      "\tspeed: 0.1139s/iter; left time: 2928.9323s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0656467\n",
      "\tspeed: 0.1131s/iter; left time: 2896.3354s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0590451\n",
      "\tspeed: 0.1133s/iter; left time: 2890.3598s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0639197\n",
      "\tspeed: 0.1121s/iter; left time: 2848.7497s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0649276\n",
      "\tspeed: 0.1138s/iter; left time: 2881.1500s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0672397\n",
      "\tspeed: 0.1133s/iter; left time: 2856.4783s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0771958\n",
      "\tspeed: 0.1128s/iter; left time: 2832.8237s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0633096\n",
      "\tspeed: 0.1133s/iter; left time: 2833.8208s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0575457\n",
      "\tspeed: 0.1132s/iter; left time: 2819.3834s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0709095\n",
      "\tspeed: 0.1137s/iter; left time: 2821.8423s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0625228\n",
      "\tspeed: 0.1124s/iter; left time: 2776.7867s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0722681\n",
      "\tspeed: 0.1124s/iter; left time: 2765.1727s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0579663\n",
      "\tspeed: 0.1139s/iter; left time: 2790.6840s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0612664\n",
      "\tspeed: 0.1118s/iter; left time: 2728.5146s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0591755\n",
      "\tspeed: 0.1124s/iter; left time: 2732.7433s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0699752\n",
      "\tspeed: 0.1148s/iter; left time: 2778.2495s\n",
      "Epoch: 11 cost time: 00h:05m:04.77s\n",
      "Epoch: 11 | Train Loss: 0.0658757 Vali Loss: 0.0578032 Test Loss: 0.0670076\n",
      "Validation loss decreased (0.058111 --> 0.057803).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0724038\n",
      "\tspeed: 1.0561s/iter; left time: 25377.5917s\n",
      "\titers: 200, epoch: 12 | loss: 0.0599244\n",
      "\tspeed: 0.1167s/iter; left time: 2791.5539s\n",
      "\titers: 300, epoch: 12 | loss: 0.0652725\n",
      "\tspeed: 0.1146s/iter; left time: 2730.2562s\n",
      "\titers: 400, epoch: 12 | loss: 0.0576787\n",
      "\tspeed: 0.1151s/iter; left time: 2731.3073s\n",
      "\titers: 500, epoch: 12 | loss: 0.0657673\n",
      "\tspeed: 0.1162s/iter; left time: 2745.4736s\n",
      "\titers: 600, epoch: 12 | loss: 0.0662366\n",
      "\tspeed: 0.1159s/iter; left time: 2726.3024s\n",
      "\titers: 700, epoch: 12 | loss: 0.0807744\n",
      "\tspeed: 0.1156s/iter; left time: 2708.6135s\n",
      "\titers: 800, epoch: 12 | loss: 0.0683301\n",
      "\tspeed: 0.1149s/iter; left time: 2681.5224s\n",
      "\titers: 900, epoch: 12 | loss: 0.0653859\n",
      "\tspeed: 0.1139s/iter; left time: 2645.7059s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0721641\n",
      "\tspeed: 0.1158s/iter; left time: 2678.6462s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0629323\n",
      "\tspeed: 0.1133s/iter; left time: 2609.9367s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0567239\n",
      "\tspeed: 0.1160s/iter; left time: 2660.6701s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0589142\n",
      "\tspeed: 0.1145s/iter; left time: 2614.6360s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0682594\n",
      "\tspeed: 0.1172s/iter; left time: 2664.7349s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0660052\n",
      "\tspeed: 0.1145s/iter; left time: 2591.8247s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0597542\n",
      "\tspeed: 0.1149s/iter; left time: 2588.6791s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0569995\n",
      "\tspeed: 0.1162s/iter; left time: 2605.3605s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0621652\n",
      "\tspeed: 0.1139s/iter; left time: 2543.5189s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0574851\n",
      "\tspeed: 0.1154s/iter; left time: 2566.3783s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0754092\n",
      "\tspeed: 0.1168s/iter; left time: 2583.9628s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0705758\n",
      "\tspeed: 0.1139s/iter; left time: 2508.5002s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0638501\n",
      "\tspeed: 0.1144s/iter; left time: 2509.4028s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0607917\n",
      "\tspeed: 0.1150s/iter; left time: 2509.3821s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0704667\n",
      "\tspeed: 0.1134s/iter; left time: 2465.0269s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0649680\n",
      "\tspeed: 0.1152s/iter; left time: 2492.0219s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0622617\n",
      "\tspeed: 0.1135s/iter; left time: 2443.2286s\n",
      "Epoch: 12 cost time: 00h:05m:09.35s\n",
      "Epoch: 12 | Train Loss: 0.0654141 Vali Loss: 0.0592872 Test Loss: 0.0696159\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0571517\n",
      "\tspeed: 1.0461s/iter; left time: 22332.6199s\n",
      "\titers: 200, epoch: 13 | loss: 0.0563359\n",
      "\tspeed: 0.1160s/iter; left time: 2465.5303s\n",
      "\titers: 300, epoch: 13 | loss: 0.0752222\n",
      "\tspeed: 0.1144s/iter; left time: 2420.4436s\n",
      "\titers: 400, epoch: 13 | loss: 0.0552250\n",
      "\tspeed: 0.1159s/iter; left time: 2440.2397s\n",
      "\titers: 500, epoch: 13 | loss: 0.0716464\n",
      "\tspeed: 0.1161s/iter; left time: 2431.9619s\n",
      "\titers: 600, epoch: 13 | loss: 0.0713721\n",
      "\tspeed: 0.1143s/iter; left time: 2383.0557s\n",
      "\titers: 700, epoch: 13 | loss: 0.0675928\n",
      "\tspeed: 0.1141s/iter; left time: 2367.1671s\n",
      "\titers: 800, epoch: 13 | loss: 0.0716297\n",
      "\tspeed: 0.1131s/iter; left time: 2335.4889s\n",
      "\titers: 900, epoch: 13 | loss: 0.0656308\n",
      "\tspeed: 0.1131s/iter; left time: 2324.4347s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0822458\n",
      "\tspeed: 0.1156s/iter; left time: 2364.7438s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0752804\n",
      "\tspeed: 0.1129s/iter; left time: 2298.0280s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0652262\n",
      "\tspeed: 0.1172s/iter; left time: 2373.8849s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0598756\n",
      "\tspeed: 0.1151s/iter; left time: 2318.4410s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0691182\n",
      "\tspeed: 0.1149s/iter; left time: 2303.1170s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0635491\n",
      "\tspeed: 0.1145s/iter; left time: 2283.3282s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0895910\n",
      "\tspeed: 0.1124s/iter; left time: 2231.5296s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0710886\n",
      "\tspeed: 0.1151s/iter; left time: 2272.3943s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0558507\n",
      "\tspeed: 0.1159s/iter; left time: 2276.5376s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0647579\n",
      "\tspeed: 0.1128s/iter; left time: 2204.6798s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0736879\n",
      "\tspeed: 0.1139s/iter; left time: 2214.5192s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0558424\n",
      "\tspeed: 0.1143s/iter; left time: 2211.3625s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0630306\n",
      "\tspeed: 0.1142s/iter; left time: 2197.4649s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0580184\n",
      "\tspeed: 0.1155s/iter; left time: 2211.6236s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643669\n",
      "\tspeed: 0.1134s/iter; left time: 2160.5518s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0530160\n",
      "\tspeed: 0.1140s/iter; left time: 2160.1064s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0617090\n",
      "\tspeed: 0.1137s/iter; left time: 2142.7455s\n",
      "Epoch: 13 cost time: 00h:05m:07.74s\n",
      "Epoch: 13 | Train Loss: 0.0649905 Vali Loss: 0.0579648 Test Loss: 0.0673596\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0698010\n",
      "\tspeed: 1.0505s/iter; left time: 19609.8385s\n",
      "\titers: 200, epoch: 14 | loss: 0.0725076\n",
      "\tspeed: 0.1178s/iter; left time: 2187.9124s\n",
      "\titers: 300, epoch: 14 | loss: 0.0715118\n",
      "\tspeed: 0.1160s/iter; left time: 2143.0169s\n",
      "\titers: 400, epoch: 14 | loss: 0.0609961\n",
      "\tspeed: 0.1183s/iter; left time: 2172.8697s\n",
      "\titers: 500, epoch: 14 | loss: 0.0552948\n",
      "\tspeed: 0.1142s/iter; left time: 2087.0287s\n",
      "\titers: 600, epoch: 14 | loss: 0.0565410\n",
      "\tspeed: 0.1163s/iter; left time: 2113.0871s\n",
      "\titers: 700, epoch: 14 | loss: 0.0706945\n",
      "\tspeed: 0.1147s/iter; left time: 2072.3260s\n",
      "\titers: 800, epoch: 14 | loss: 0.0687059\n",
      "\tspeed: 0.1151s/iter; left time: 2067.9418s\n",
      "\titers: 900, epoch: 14 | loss: 0.0700648\n",
      "\tspeed: 0.1139s/iter; left time: 2035.3535s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0679078\n",
      "\tspeed: 0.1145s/iter; left time: 2034.5733s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0623200\n",
      "\tspeed: 0.1180s/iter; left time: 2084.1258s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0606130\n",
      "\tspeed: 0.1154s/iter; left time: 2026.6224s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0608021\n",
      "\tspeed: 0.1154s/iter; left time: 2015.2278s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0654804\n",
      "\tspeed: 0.1158s/iter; left time: 2011.5886s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0613914\n",
      "\tspeed: 0.1146s/iter; left time: 1978.1197s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0539721\n",
      "\tspeed: 0.1188s/iter; left time: 2040.0403s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0673941\n",
      "\tspeed: 0.1140s/iter; left time: 1945.7779s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0644400\n",
      "\tspeed: 0.1151s/iter; left time: 1952.6388s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0617564\n",
      "\tspeed: 0.1189s/iter; left time: 2004.8755s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0573019\n",
      "\tspeed: 0.1138s/iter; left time: 1907.5353s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0627567\n",
      "\tspeed: 0.1157s/iter; left time: 1927.8315s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0622240\n",
      "\tspeed: 0.1143s/iter; left time: 1894.0622s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0786616\n",
      "\tspeed: 0.1130s/iter; left time: 1860.1168s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0765208\n",
      "\tspeed: 0.1167s/iter; left time: 1909.5444s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0689429\n",
      "\tspeed: 0.1148s/iter; left time: 1867.9114s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0559283\n",
      "\tspeed: 0.1152s/iter; left time: 1862.1300s\n",
      "Epoch: 14 cost time: 00h:05m:10.55s\n",
      "Epoch: 14 | Train Loss: 0.0645770 Vali Loss: 0.0574409 Test Loss: 0.0664566\n",
      "Validation loss decreased (0.057803 --> 0.057441).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0564922\n",
      "\tspeed: 1.0504s/iter; left time: 16792.2142s\n",
      "\titers: 200, epoch: 15 | loss: 0.0621809\n",
      "\tspeed: 0.1152s/iter; left time: 1830.9058s\n",
      "\titers: 300, epoch: 15 | loss: 0.0641433\n",
      "\tspeed: 0.1150s/iter; left time: 1814.7649s\n",
      "\titers: 400, epoch: 15 | loss: 0.0578732\n",
      "\tspeed: 0.1162s/iter; left time: 1823.4217s\n",
      "\titers: 500, epoch: 15 | loss: 0.0655350\n",
      "\tspeed: 0.1155s/iter; left time: 1800.4331s\n",
      "\titers: 600, epoch: 15 | loss: 0.0686196\n",
      "\tspeed: 0.1140s/iter; left time: 1765.6390s\n",
      "\titers: 700, epoch: 15 | loss: 0.0824154\n",
      "\tspeed: 0.1150s/iter; left time: 1769.5573s\n",
      "\titers: 800, epoch: 15 | loss: 0.0650468\n",
      "\tspeed: 0.1172s/iter; left time: 1790.8980s\n",
      "\titers: 900, epoch: 15 | loss: 0.0688387\n",
      "\tspeed: 0.1168s/iter; left time: 1773.9111s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0771232\n",
      "\tspeed: 0.1136s/iter; left time: 1714.3128s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0594005\n",
      "\tspeed: 0.1151s/iter; left time: 1724.9768s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0690505\n",
      "\tspeed: 0.1168s/iter; left time: 1738.1331s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0685179\n",
      "\tspeed: 0.1155s/iter; left time: 1707.5917s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0714361\n",
      "\tspeed: 0.1157s/iter; left time: 1698.7921s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0690282\n",
      "\tspeed: 0.1158s/iter; left time: 1689.5013s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0619706\n",
      "\tspeed: 0.1154s/iter; left time: 1672.0780s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0708041\n",
      "\tspeed: 0.1165s/iter; left time: 1676.4341s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0704809\n",
      "\tspeed: 0.1164s/iter; left time: 1662.5459s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0758016\n",
      "\tspeed: 0.1157s/iter; left time: 1641.6784s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0559394\n",
      "\tspeed: 0.1133s/iter; left time: 1596.5836s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0584624\n",
      "\tspeed: 0.1185s/iter; left time: 1657.0702s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0783027\n",
      "\tspeed: 0.1177s/iter; left time: 1634.3422s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0668146\n",
      "\tspeed: 0.1158s/iter; left time: 1596.5994s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0637044\n",
      "\tspeed: 0.1169s/iter; left time: 1600.2406s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0603980\n",
      "\tspeed: 0.1133s/iter; left time: 1539.4111s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0710332\n",
      "\tspeed: 0.1172s/iter; left time: 1580.7251s\n",
      "Epoch: 15 cost time: 00h:05m:11.05s\n",
      "Epoch: 15 | Train Loss: 0.0641804 Vali Loss: 0.0605943 Test Loss: 0.0706357\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0574767\n",
      "\tspeed: 1.0832s/iter; left time: 14413.1043s\n",
      "\titers: 200, epoch: 16 | loss: 0.0573915\n",
      "\tspeed: 0.1138s/iter; left time: 1503.4549s\n",
      "\titers: 300, epoch: 16 | loss: 0.0506497\n",
      "\tspeed: 0.1197s/iter; left time: 1568.4148s\n",
      "\titers: 400, epoch: 16 | loss: 0.0601425\n",
      "\tspeed: 0.1245s/iter; left time: 1619.3223s\n",
      "\titers: 500, epoch: 16 | loss: 0.0669079\n",
      "\tspeed: 0.1170s/iter; left time: 1510.5657s\n",
      "\titers: 600, epoch: 16 | loss: 0.0705684\n",
      "\tspeed: 0.1265s/iter; left time: 1619.5410s\n",
      "\titers: 700, epoch: 16 | loss: 0.0603802\n",
      "\tspeed: 0.1172s/iter; left time: 1489.1899s\n",
      "\titers: 800, epoch: 16 | loss: 0.0707469\n",
      "\tspeed: 0.1186s/iter; left time: 1494.8500s\n",
      "\titers: 900, epoch: 16 | loss: 0.0820457\n",
      "\tspeed: 0.1133s/iter; left time: 1417.4639s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0660811\n",
      "\tspeed: 0.1144s/iter; left time: 1418.7206s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0602575\n",
      "\tspeed: 0.1211s/iter; left time: 1490.2298s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0644291\n",
      "\tspeed: 0.1163s/iter; left time: 1419.6789s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0740635\n",
      "\tspeed: 0.1166s/iter; left time: 1411.3760s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0634106\n",
      "\tspeed: 0.1186s/iter; left time: 1423.7740s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0686077\n",
      "\tspeed: 0.1129s/iter; left time: 1344.6050s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0558413\n",
      "\tspeed: 0.1153s/iter; left time: 1361.7066s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0561398\n",
      "\tspeed: 0.1176s/iter; left time: 1376.6157s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0640715\n",
      "\tspeed: 0.1212s/iter; left time: 1407.1287s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0615180\n",
      "\tspeed: 0.1137s/iter; left time: 1308.5983s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0579631\n",
      "\tspeed: 0.1218s/iter; left time: 1388.9642s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0742498\n",
      "\tspeed: 0.1371s/iter; left time: 1550.2575s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0673364\n",
      "\tspeed: 0.1217s/iter; left time: 1364.0987s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0641308\n",
      "\tspeed: 0.1156s/iter; left time: 1283.5332s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0701997\n",
      "\tspeed: 0.1180s/iter; left time: 1298.6027s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0703389\n",
      "\tspeed: 0.1198s/iter; left time: 1307.0178s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0659333\n",
      "\tspeed: 0.1143s/iter; left time: 1234.6238s\n",
      "Epoch: 16 cost time: 00h:05m:18.40s\n",
      "Epoch: 16 | Train Loss: 0.0638276 Vali Loss: 0.0593083 Test Loss: 0.0686335\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0632415\n",
      "\tspeed: 1.0655s/iter; left time: 11321.3357s\n",
      "\titers: 200, epoch: 17 | loss: 0.0688461\n",
      "\tspeed: 0.1274s/iter; left time: 1340.6059s\n",
      "\titers: 300, epoch: 17 | loss: 0.0631828\n",
      "\tspeed: 0.1190s/iter; left time: 1240.7238s\n",
      "\titers: 400, epoch: 17 | loss: 0.0736408\n",
      "\tspeed: 0.1165s/iter; left time: 1203.3470s\n",
      "\titers: 500, epoch: 17 | loss: 0.0770576\n",
      "\tspeed: 0.1157s/iter; left time: 1182.5244s\n",
      "\titers: 600, epoch: 17 | loss: 0.0678533\n",
      "\tspeed: 0.1210s/iter; left time: 1224.7393s\n",
      "\titers: 700, epoch: 17 | loss: 0.0549429\n",
      "\tspeed: 0.1198s/iter; left time: 1200.9915s\n",
      "\titers: 800, epoch: 17 | loss: 0.0515392\n",
      "\tspeed: 0.1164s/iter; left time: 1155.6964s\n",
      "\titers: 900, epoch: 17 | loss: 0.0676008\n",
      "\tspeed: 0.1183s/iter; left time: 1162.2364s\n",
      "\titers: 1000, epoch: 17 | loss: 0.0647659\n",
      "\tspeed: 0.1275s/iter; left time: 1239.9174s\n",
      "\titers: 1100, epoch: 17 | loss: 0.0694209\n",
      "\tspeed: 0.1152s/iter; left time: 1108.8508s\n",
      "\titers: 1200, epoch: 17 | loss: 0.0576050\n",
      "\tspeed: 0.1232s/iter; left time: 1173.7085s\n",
      "\titers: 1300, epoch: 17 | loss: 0.0682291\n",
      "\tspeed: 0.1285s/iter; left time: 1211.3266s\n",
      "\titers: 1400, epoch: 17 | loss: 0.0641747\n",
      "\tspeed: 0.1159s/iter; left time: 1080.6953s\n",
      "\titers: 1500, epoch: 17 | loss: 0.0657903\n",
      "\tspeed: 0.1203s/iter; left time: 1109.7518s\n",
      "\titers: 1600, epoch: 17 | loss: 0.0568783\n",
      "\tspeed: 0.1165s/iter; left time: 1063.3112s\n",
      "\titers: 1700, epoch: 17 | loss: 0.0658871\n",
      "\tspeed: 0.1160s/iter; left time: 1047.2816s\n",
      "\titers: 1800, epoch: 17 | loss: 0.0624544\n",
      "\tspeed: 0.1171s/iter; left time: 1044.9825s\n",
      "\titers: 1900, epoch: 17 | loss: 0.0611300\n",
      "\tspeed: 0.1184s/iter; left time: 1044.9180s\n",
      "\titers: 2000, epoch: 17 | loss: 0.0487223\n",
      "\tspeed: 0.1155s/iter; left time: 1007.8602s\n",
      "\titers: 2100, epoch: 17 | loss: 0.0584179\n",
      "\tspeed: 0.1158s/iter; left time: 998.5470s\n",
      "\titers: 2200, epoch: 17 | loss: 0.0644555\n",
      "\tspeed: 0.1157s/iter; left time: 986.0106s\n",
      "\titers: 2300, epoch: 17 | loss: 0.0529996\n",
      "\tspeed: 0.1187s/iter; left time: 1000.3853s\n",
      "\titers: 2400, epoch: 17 | loss: 0.0567144\n",
      "\tspeed: 0.1177s/iter; left time: 980.1682s\n",
      "\titers: 2500, epoch: 17 | loss: 0.0710186\n",
      "\tspeed: 0.1150s/iter; left time: 946.1899s\n",
      "\titers: 2600, epoch: 17 | loss: 0.0712463\n",
      "\tspeed: 0.1362s/iter; left time: 1106.6550s\n",
      "Epoch: 17 cost time: 00h:05m:20.52s\n",
      "Epoch: 17 | Train Loss: 0.0634753 Vali Loss: 0.0594251 Test Loss: 0.0680345\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0608606\n",
      "\tspeed: 1.0592s/iter; left time: 8414.4599s\n",
      "\titers: 200, epoch: 18 | loss: 0.0630744\n",
      "\tspeed: 0.1173s/iter; left time: 920.3750s\n",
      "\titers: 300, epoch: 18 | loss: 0.0573201\n",
      "\tspeed: 0.1164s/iter; left time: 901.6809s\n",
      "\titers: 400, epoch: 18 | loss: 0.0669892\n",
      "\tspeed: 0.1149s/iter; left time: 878.1572s\n",
      "\titers: 500, epoch: 18 | loss: 0.0642769\n",
      "\tspeed: 0.1175s/iter; left time: 886.4048s\n",
      "\titers: 600, epoch: 18 | loss: 0.0601702\n",
      "\tspeed: 0.1171s/iter; left time: 871.5879s\n",
      "\titers: 700, epoch: 18 | loss: 0.0813551\n",
      "\tspeed: 0.1213s/iter; left time: 891.1741s\n",
      "\titers: 800, epoch: 18 | loss: 0.0769780\n",
      "\tspeed: 0.1153s/iter; left time: 835.3674s\n",
      "\titers: 900, epoch: 18 | loss: 0.0527711\n",
      "\tspeed: 0.1147s/iter; left time: 819.4615s\n",
      "\titers: 1000, epoch: 18 | loss: 0.0642087\n",
      "\tspeed: 0.1179s/iter; left time: 830.5596s\n",
      "\titers: 1100, epoch: 18 | loss: 0.0753492\n",
      "\tspeed: 0.1156s/iter; left time: 802.4335s\n",
      "\titers: 1200, epoch: 18 | loss: 0.0503302\n",
      "\tspeed: 0.1153s/iter; left time: 789.4083s\n",
      "\titers: 1300, epoch: 18 | loss: 0.0638088\n",
      "\tspeed: 0.1176s/iter; left time: 793.3942s\n",
      "\titers: 1400, epoch: 18 | loss: 0.0659324\n",
      "\tspeed: 0.1136s/iter; left time: 755.0098s\n",
      "\titers: 1500, epoch: 18 | loss: 0.0653730\n",
      "\tspeed: 0.1465s/iter; left time: 958.7802s\n",
      "\titers: 1600, epoch: 18 | loss: 0.0739145\n",
      "\tspeed: 0.1208s/iter; left time: 778.5894s\n",
      "\titers: 1700, epoch: 18 | loss: 0.0535310\n",
      "\tspeed: 0.1176s/iter; left time: 746.3211s\n",
      "\titers: 1800, epoch: 18 | loss: 0.0655848\n",
      "\tspeed: 0.1177s/iter; left time: 735.2285s\n",
      "\titers: 1900, epoch: 18 | loss: 0.0589777\n",
      "\tspeed: 0.1218s/iter; left time: 748.3657s\n",
      "\titers: 2000, epoch: 18 | loss: 0.0685786\n",
      "\tspeed: 0.1147s/iter; left time: 693.5202s\n",
      "\titers: 2100, epoch: 18 | loss: 0.0555254\n",
      "\tspeed: 0.1135s/iter; left time: 674.5372s\n",
      "\titers: 2200, epoch: 18 | loss: 0.0631094\n",
      "\tspeed: 0.1178s/iter; left time: 688.4124s\n",
      "\titers: 2300, epoch: 18 | loss: 0.0528505\n",
      "\tspeed: 0.1173s/iter; left time: 673.8333s\n",
      "\titers: 2400, epoch: 18 | loss: 0.0643650\n",
      "\tspeed: 0.1150s/iter; left time: 648.8403s\n",
      "\titers: 2500, epoch: 18 | loss: 0.0647579\n",
      "\tspeed: 0.1146s/iter; left time: 635.5729s\n",
      "\titers: 2600, epoch: 18 | loss: 0.0699769\n",
      "\tspeed: 0.1192s/iter; left time: 649.0398s\n",
      "Epoch: 18 cost time: 00h:05m:16.65s\n",
      "Epoch: 18 | Train Loss: 0.0629919 Vali Loss: 0.0601041 Test Loss: 0.0698167\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0543833\n",
      "\tspeed: 1.0633s/iter; left time: 5595.9750s\n",
      "\titers: 200, epoch: 19 | loss: 0.0554405\n",
      "\tspeed: 0.1168s/iter; left time: 602.8683s\n",
      "\titers: 300, epoch: 19 | loss: 0.0701018\n",
      "\tspeed: 0.1165s/iter; left time: 589.9046s\n",
      "\titers: 400, epoch: 19 | loss: 0.0615413\n",
      "\tspeed: 0.1210s/iter; left time: 600.4576s\n",
      "\titers: 500, epoch: 19 | loss: 0.0614020\n",
      "\tspeed: 0.1151s/iter; left time: 559.5217s\n",
      "\titers: 600, epoch: 19 | loss: 0.0573202\n",
      "\tspeed: 0.1146s/iter; left time: 545.8222s\n",
      "\titers: 700, epoch: 19 | loss: 0.0611640\n",
      "\tspeed: 0.1167s/iter; left time: 544.2196s\n",
      "\titers: 800, epoch: 19 | loss: 0.0607653\n",
      "\tspeed: 0.1140s/iter; left time: 520.1647s\n",
      "\titers: 900, epoch: 19 | loss: 0.0566523\n",
      "\tspeed: 0.1147s/iter; left time: 512.0958s\n",
      "\titers: 1000, epoch: 19 | loss: 0.0622958\n",
      "\tspeed: 0.1178s/iter; left time: 514.1242s\n",
      "\titers: 1100, epoch: 19 | loss: 0.0550250\n",
      "\tspeed: 0.1133s/iter; left time: 483.1861s\n",
      "\titers: 1200, epoch: 19 | loss: 0.0663746\n",
      "\tspeed: 0.1155s/iter; left time: 480.6642s\n",
      "\titers: 1300, epoch: 19 | loss: 0.0650760\n",
      "\tspeed: 0.1151s/iter; left time: 467.5133s\n",
      "\titers: 1400, epoch: 19 | loss: 0.0588802\n",
      "\tspeed: 0.1159s/iter; left time: 459.2294s\n",
      "\titers: 1500, epoch: 19 | loss: 0.0603579\n",
      "\tspeed: 0.1202s/iter; left time: 464.4885s\n",
      "\titers: 1600, epoch: 19 | loss: 0.0569299\n",
      "\tspeed: 0.1162s/iter; left time: 437.3346s\n",
      "\titers: 1700, epoch: 19 | loss: 0.0491759\n",
      "\tspeed: 0.1141s/iter; left time: 417.7748s\n",
      "\titers: 1800, epoch: 19 | loss: 0.0864070\n",
      "\tspeed: 0.1154s/iter; left time: 411.2592s\n",
      "\titers: 1900, epoch: 19 | loss: 0.0639838\n",
      "\tspeed: 0.1170s/iter; left time: 405.0338s\n",
      "\titers: 2000, epoch: 19 | loss: 0.0655531\n",
      "\tspeed: 0.1135s/iter; left time: 381.5999s\n",
      "\titers: 2100, epoch: 19 | loss: 0.0550640\n",
      "\tspeed: 0.1187s/iter; left time: 387.3000s\n",
      "\titers: 2200, epoch: 19 | loss: 0.0624830\n",
      "\tspeed: 0.1146s/iter; left time: 362.3764s\n",
      "\titers: 2300, epoch: 19 | loss: 0.0577416\n",
      "\tspeed: 0.1153s/iter; left time: 353.3156s\n",
      "\titers: 2400, epoch: 19 | loss: 0.0651264\n",
      "\tspeed: 0.1176s/iter; left time: 348.4014s\n",
      "\titers: 2500, epoch: 19 | loss: 0.0614737\n",
      "\tspeed: 0.1146s/iter; left time: 328.0820s\n",
      "\titers: 2600, epoch: 19 | loss: 0.0525950\n",
      "\tspeed: 0.1160s/iter; left time: 320.5934s\n",
      "Epoch: 19 cost time: 00h:05m:11.75s\n",
      "Epoch: 19 | Train Loss: 0.0626410 Vali Loss: 0.0593832 Test Loss: 0.0687746\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010672738775610924, rmse:0.10330894589424133, mae:0.0664566159248352, rse:0.3033173382282257\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 24: 02h:06m:51.14s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 12:31:39,413] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 12:31:40,501] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 12:31:40,502] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 12:31:40,502] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 12:31:40,610] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 12:31:40,611] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 12:31:41,354] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 12:31:41,355] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 12:31:41,355] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 12:31:41,357] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 12:31:41,357] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 12:31:41,357] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 12:31:41,749] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 12:31:41,750] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:41,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.95 GB, percent = 54.9%\n",
      "[2024-11-03 12:31:42,004] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 12:31:42,006] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:42,006] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 415.18 GB, percent = 55.0%\n",
      "[2024-11-03 12:31:42,006] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 12:31:42,208] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 12:31:42,209] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:42,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 416.07 GB, percent = 55.1%\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 12:31:42,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 12:31:42,211] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f347c58d590>\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1911721\n",
      "\tspeed: 0.2250s/iter; left time: 12010.3608s\n",
      "\titers: 200, epoch: 1 | loss: 0.1927420\n",
      "\tspeed: 0.1312s/iter; left time: 6991.0572s\n",
      "\titers: 300, epoch: 1 | loss: 0.1694221\n",
      "\tspeed: 0.1279s/iter; left time: 6803.3074s\n",
      "\titers: 400, epoch: 1 | loss: 0.1720468\n",
      "\tspeed: 0.1280s/iter; left time: 6792.1212s\n",
      "\titers: 500, epoch: 1 | loss: 0.1218105\n",
      "\tspeed: 0.1259s/iter; left time: 6671.8709s\n",
      "\titers: 600, epoch: 1 | loss: 0.1114848\n",
      "\tspeed: 0.1327s/iter; left time: 7017.3328s\n",
      "\titers: 700, epoch: 1 | loss: 0.1140701\n",
      "\tspeed: 0.1261s/iter; left time: 6657.5580s\n",
      "\titers: 800, epoch: 1 | loss: 0.1041137\n",
      "\tspeed: 0.1255s/iter; left time: 6613.8594s\n",
      "\titers: 900, epoch: 1 | loss: 0.1242962\n",
      "\tspeed: 0.1287s/iter; left time: 6765.5096s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1122826\n",
      "\tspeed: 0.1289s/iter; left time: 6764.8339s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1078091\n",
      "\tspeed: 0.1265s/iter; left time: 6627.9105s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0886153\n",
      "\tspeed: 0.1313s/iter; left time: 6863.7768s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1067653\n",
      "\tspeed: 0.1269s/iter; left time: 6623.9458s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1063584\n",
      "\tspeed: 0.1258s/iter; left time: 6553.2690s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1006202\n",
      "\tspeed: 0.1279s/iter; left time: 6649.1573s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0992378\n",
      "\tspeed: 0.1268s/iter; left time: 6578.8237s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1159680\n",
      "\tspeed: 0.1262s/iter; left time: 6533.5496s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0989420\n",
      "\tspeed: 0.1287s/iter; left time: 6648.8436s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1210326\n",
      "\tspeed: 0.1272s/iter; left time: 6559.0411s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0897738\n",
      "\tspeed: 0.1263s/iter; left time: 6502.1211s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1054735\n",
      "\tspeed: 0.1254s/iter; left time: 6444.7193s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0856309\n",
      "\tspeed: 0.1277s/iter; left time: 6549.6717s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1068027\n",
      "\tspeed: 0.1290s/iter; left time: 6600.0312s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0811701\n",
      "\tspeed: 0.1258s/iter; left time: 6424.3765s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0951465\n",
      "\tspeed: 0.1272s/iter; left time: 6486.0592s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1027023\n",
      "\tspeed: 0.1281s/iter; left time: 6518.3456s\n",
      "Epoch: 1 cost time: 00h:05m:47.64s\n",
      "Epoch: 1 | Train Loss: 0.1174485 Vali Loss: 0.0865509 Test Loss: 0.1004628\n",
      "Validation loss decreased (inf --> 0.086551).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1076370\n",
      "\tspeed: 1.2004s/iter; left time: 60869.2661s\n",
      "\titers: 200, epoch: 2 | loss: 0.1094375\n",
      "\tspeed: 0.1190s/iter; left time: 6020.3157s\n",
      "\titers: 300, epoch: 2 | loss: 0.0885852\n",
      "\tspeed: 0.1138s/iter; left time: 5749.9829s\n",
      "\titers: 400, epoch: 2 | loss: 0.0921508\n",
      "\tspeed: 0.1151s/iter; left time: 5802.3581s\n",
      "\titers: 500, epoch: 2 | loss: 0.0874121\n",
      "\tspeed: 0.1181s/iter; left time: 5942.7607s\n",
      "\titers: 600, epoch: 2 | loss: 0.0942486\n",
      "\tspeed: 0.1156s/iter; left time: 5804.2839s\n",
      "\titers: 700, epoch: 2 | loss: 0.0875211\n",
      "\tspeed: 0.1157s/iter; left time: 5798.5184s\n",
      "\titers: 800, epoch: 2 | loss: 0.0975673\n",
      "\tspeed: 0.1169s/iter; left time: 5844.5824s\n",
      "\titers: 900, epoch: 2 | loss: 0.0873782\n",
      "\tspeed: 0.1145s/iter; left time: 5715.6672s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0910329\n",
      "\tspeed: 0.1148s/iter; left time: 5717.4644s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0909847\n",
      "\tspeed: 0.1147s/iter; left time: 5702.5887s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0977218\n",
      "\tspeed: 0.1141s/iter; left time: 5662.2298s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0851496\n",
      "\tspeed: 0.1118s/iter; left time: 5535.0687s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0878843\n",
      "\tspeed: 0.1116s/iter; left time: 5515.6216s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0895761\n",
      "\tspeed: 0.1254s/iter; left time: 6182.0052s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0866336\n",
      "\tspeed: 0.1183s/iter; left time: 5819.2809s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1094462\n",
      "\tspeed: 0.1163s/iter; left time: 5712.2202s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1022051\n",
      "\tspeed: 0.1145s/iter; left time: 5609.4801s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0886707\n",
      "\tspeed: 0.1180s/iter; left time: 5770.6991s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0902492\n",
      "\tspeed: 0.1182s/iter; left time: 5768.7588s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0888425\n",
      "\tspeed: 0.1190s/iter; left time: 5794.1530s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0878865\n",
      "\tspeed: 0.1177s/iter; left time: 5722.5241s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0795095\n",
      "\tspeed: 0.1194s/iter; left time: 5789.5118s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0914220\n",
      "\tspeed: 0.1162s/iter; left time: 5622.8384s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0818894\n",
      "\tspeed: 0.1161s/iter; left time: 5607.8173s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0820524\n",
      "\tspeed: 0.1184s/iter; left time: 5705.3554s\n",
      "Epoch: 2 cost time: 00h:05m:12.15s\n",
      "Epoch: 2 | Train Loss: 0.0935248 Vali Loss: 0.0811140 Test Loss: 0.0944012\n",
      "Validation loss decreased (0.086551 --> 0.081114).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0952333\n",
      "\tspeed: 1.0778s/iter; left time: 51768.2510s\n",
      "\titers: 200, epoch: 3 | loss: 0.0934332\n",
      "\tspeed: 0.1083s/iter; left time: 5191.1988s\n",
      "\titers: 300, epoch: 3 | loss: 0.0905903\n",
      "\tspeed: 0.1008s/iter; left time: 4819.9429s\n",
      "\titers: 400, epoch: 3 | loss: 0.0940271\n",
      "\tspeed: 0.1110s/iter; left time: 5296.5709s\n",
      "\titers: 500, epoch: 3 | loss: 0.0887500\n",
      "\tspeed: 0.0976s/iter; left time: 4648.3151s\n",
      "\titers: 600, epoch: 3 | loss: 0.0991548\n",
      "\tspeed: 0.0925s/iter; left time: 4397.1942s\n",
      "\titers: 700, epoch: 3 | loss: 0.0992021\n",
      "\tspeed: 0.0981s/iter; left time: 4651.3183s\n",
      "\titers: 800, epoch: 3 | loss: 0.0928352\n",
      "\tspeed: 0.0910s/iter; left time: 4306.5169s\n",
      "\titers: 900, epoch: 3 | loss: 0.0962132\n",
      "\tspeed: 0.0898s/iter; left time: 4241.9536s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963722\n",
      "\tspeed: 0.0940s/iter; left time: 4430.7202s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0940291\n",
      "\tspeed: 0.0908s/iter; left time: 4269.6858s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0946871\n",
      "\tspeed: 0.0876s/iter; left time: 4112.2190s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0897035\n",
      "\tspeed: 0.0935s/iter; left time: 4379.2322s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0817999\n",
      "\tspeed: 0.1109s/iter; left time: 5182.1395s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0934589\n",
      "\tspeed: 0.1107s/iter; left time: 5164.5937s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1075991\n",
      "\tspeed: 0.1093s/iter; left time: 5087.5857s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029885\n",
      "\tspeed: 0.1141s/iter; left time: 5297.1691s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0796154\n",
      "\tspeed: 0.1162s/iter; left time: 5385.4164s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1007038\n",
      "\tspeed: 0.1152s/iter; left time: 5328.2288s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0810669\n",
      "\tspeed: 0.1161s/iter; left time: 5353.9627s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1120119\n",
      "\tspeed: 0.1162s/iter; left time: 5349.2830s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1014793\n",
      "\tspeed: 0.1159s/iter; left time: 5322.9236s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0782545\n",
      "\tspeed: 0.1156s/iter; left time: 5298.5233s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0805253\n",
      "\tspeed: 0.1144s/iter; left time: 5229.9599s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0871867\n",
      "\tspeed: 0.1147s/iter; left time: 5233.2774s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0826894\n",
      "\tspeed: 0.1139s/iter; left time: 5186.5886s\n",
      "Epoch: 3 cost time: 00h:04m:44.91s\n",
      "Epoch: 3 | Train Loss: 0.0897070 Vali Loss: 0.0801801 Test Loss: 0.0940483\n",
      "Validation loss decreased (0.081114 --> 0.080180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0769089\n",
      "\tspeed: 1.0491s/iter; left time: 47586.7382s\n",
      "\titers: 200, epoch: 4 | loss: 0.0909401\n",
      "\tspeed: 0.1141s/iter; left time: 5162.4431s\n",
      "\titers: 300, epoch: 4 | loss: 0.0769373\n",
      "\tspeed: 0.1169s/iter; left time: 5280.5800s\n",
      "\titers: 400, epoch: 4 | loss: 0.0873026\n",
      "\tspeed: 0.1158s/iter; left time: 5219.3674s\n",
      "\titers: 500, epoch: 4 | loss: 0.0947193\n",
      "\tspeed: 0.1174s/iter; left time: 5280.2416s\n",
      "\titers: 600, epoch: 4 | loss: 0.0831833\n",
      "\tspeed: 0.1155s/iter; left time: 5180.0694s\n",
      "\titers: 700, epoch: 4 | loss: 0.0980527\n",
      "\tspeed: 0.1161s/iter; left time: 5194.4162s\n",
      "\titers: 800, epoch: 4 | loss: 0.0867212\n",
      "\tspeed: 0.1164s/iter; left time: 5197.0489s\n",
      "\titers: 900, epoch: 4 | loss: 0.0815352\n",
      "\tspeed: 0.1140s/iter; left time: 5078.6952s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0801502\n",
      "\tspeed: 0.1139s/iter; left time: 5064.2236s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0883447\n",
      "\tspeed: 0.1138s/iter; left time: 5048.9823s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0877123\n",
      "\tspeed: 0.1137s/iter; left time: 5032.9758s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0836246\n",
      "\tspeed: 0.1149s/iter; left time: 5076.0069s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0986883\n",
      "\tspeed: 0.1153s/iter; left time: 5082.1712s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0836472\n",
      "\tspeed: 0.1137s/iter; left time: 4999.1454s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0853868\n",
      "\tspeed: 0.1142s/iter; left time: 5007.7051s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0907262\n",
      "\tspeed: 0.1127s/iter; left time: 4932.0601s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0784356\n",
      "\tspeed: 0.1122s/iter; left time: 4899.6599s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0888463\n",
      "\tspeed: 0.1158s/iter; left time: 5043.7710s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0986566\n",
      "\tspeed: 0.1152s/iter; left time: 5008.2468s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0922876\n",
      "\tspeed: 0.1123s/iter; left time: 4870.9778s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0821516\n",
      "\tspeed: 0.1154s/iter; left time: 4991.4455s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0900341\n",
      "\tspeed: 0.1147s/iter; left time: 4952.2516s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0951754\n",
      "\tspeed: 0.1142s/iter; left time: 4916.3450s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0837031\n",
      "\tspeed: 0.1139s/iter; left time: 4891.1125s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0842158\n",
      "\tspeed: 0.1163s/iter; left time: 4986.0906s\n",
      "Epoch: 4 cost time: 00h:05m:07.49s\n",
      "Epoch: 4 | Train Loss: 0.0869057 Vali Loss: 0.0800106 Test Loss: 0.0955628\n",
      "Validation loss decreased (0.080180 --> 0.080011).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0871619\n",
      "\tspeed: 1.0465s/iter; left time: 44668.5079s\n",
      "\titers: 200, epoch: 5 | loss: 0.1035479\n",
      "\tspeed: 0.1166s/iter; left time: 4966.4781s\n",
      "\titers: 300, epoch: 5 | loss: 0.0778850\n",
      "\tspeed: 0.1167s/iter; left time: 4960.0059s\n",
      "\titers: 400, epoch: 5 | loss: 0.0712645\n",
      "\tspeed: 0.1169s/iter; left time: 4953.1657s\n",
      "\titers: 500, epoch: 5 | loss: 0.0783551\n",
      "\tspeed: 0.1165s/iter; left time: 4925.0533s\n",
      "\titers: 600, epoch: 5 | loss: 0.0905214\n",
      "\tspeed: 0.1162s/iter; left time: 4901.8905s\n",
      "\titers: 700, epoch: 5 | loss: 0.0817809\n",
      "\tspeed: 0.1151s/iter; left time: 4844.6803s\n",
      "\titers: 800, epoch: 5 | loss: 0.0813011\n",
      "\tspeed: 0.1165s/iter; left time: 4890.1612s\n",
      "\titers: 900, epoch: 5 | loss: 0.0899853\n",
      "\tspeed: 0.1141s/iter; left time: 4779.5406s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0864681\n",
      "\tspeed: 0.1146s/iter; left time: 4789.0321s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0875253\n",
      "\tspeed: 0.1164s/iter; left time: 4852.1821s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0901085\n",
      "\tspeed: 0.1122s/iter; left time: 4666.6779s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0639921\n",
      "\tspeed: 0.1127s/iter; left time: 4674.1220s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0969867\n",
      "\tspeed: 0.1155s/iter; left time: 4780.2230s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0761939\n",
      "\tspeed: 0.1140s/iter; left time: 4707.7693s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0842133\n",
      "\tspeed: 0.1135s/iter; left time: 4674.0158s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0735740\n",
      "\tspeed: 0.1144s/iter; left time: 4701.2770s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0814190\n",
      "\tspeed: 0.1134s/iter; left time: 4647.7697s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0850187\n",
      "\tspeed: 0.1138s/iter; left time: 4651.2324s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0746921\n",
      "\tspeed: 0.1143s/iter; left time: 4661.2029s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0881317\n",
      "\tspeed: 0.1139s/iter; left time: 4633.6248s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0845793\n",
      "\tspeed: 0.1158s/iter; left time: 4699.8150s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0848546\n",
      "\tspeed: 0.1141s/iter; left time: 4618.5652s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0750976\n",
      "\tspeed: 0.1138s/iter; left time: 4594.6637s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0834830\n",
      "\tspeed: 0.1144s/iter; left time: 4609.4049s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0875506\n",
      "\tspeed: 0.1140s/iter; left time: 4579.0886s\n",
      "Epoch: 5 cost time: 00h:05m:07.39s\n",
      "Epoch: 5 | Train Loss: 0.0848302 Vali Loss: 0.0812559 Test Loss: 0.0953243\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1068747\n",
      "\tspeed: 1.0150s/iter; left time: 40609.2939s\n",
      "\titers: 200, epoch: 6 | loss: 0.0783328\n",
      "\tspeed: 0.1153s/iter; left time: 4602.4415s\n",
      "\titers: 300, epoch: 6 | loss: 0.0739404\n",
      "\tspeed: 0.1158s/iter; left time: 4608.7733s\n",
      "\titers: 400, epoch: 6 | loss: 0.0745426\n",
      "\tspeed: 0.1151s/iter; left time: 4570.9422s\n",
      "\titers: 500, epoch: 6 | loss: 0.0807710\n",
      "\tspeed: 0.1141s/iter; left time: 4521.2147s\n",
      "\titers: 600, epoch: 6 | loss: 0.0781820\n",
      "\tspeed: 0.1145s/iter; left time: 4522.0859s\n",
      "\titers: 700, epoch: 6 | loss: 0.0852243\n",
      "\tspeed: 0.1138s/iter; left time: 4486.1345s\n",
      "\titers: 800, epoch: 6 | loss: 0.0808892\n",
      "\tspeed: 0.1159s/iter; left time: 4556.8919s\n",
      "\titers: 900, epoch: 6 | loss: 0.0863012\n",
      "\tspeed: 0.1144s/iter; left time: 4487.2771s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0845646\n",
      "\tspeed: 0.1148s/iter; left time: 4490.8618s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0786446\n",
      "\tspeed: 0.1156s/iter; left time: 4508.3062s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0738035\n",
      "\tspeed: 0.1137s/iter; left time: 4424.7178s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0962725\n",
      "\tspeed: 0.1139s/iter; left time: 4419.6932s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0873117\n",
      "\tspeed: 0.1157s/iter; left time: 4477.5946s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0892730\n",
      "\tspeed: 0.1148s/iter; left time: 4430.8446s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0773154\n",
      "\tspeed: 0.1143s/iter; left time: 4400.2482s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0777241\n",
      "\tspeed: 0.1152s/iter; left time: 4426.3397s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0818046\n",
      "\tspeed: 0.1160s/iter; left time: 4445.0211s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0856868\n",
      "\tspeed: 0.1147s/iter; left time: 4384.3465s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849679\n",
      "\tspeed: 0.1143s/iter; left time: 4357.2855s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0735170\n",
      "\tspeed: 0.1158s/iter; left time: 4401.3630s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0838584\n",
      "\tspeed: 0.1133s/iter; left time: 4295.5436s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0831294\n",
      "\tspeed: 0.1176s/iter; left time: 4447.5343s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0829203\n",
      "\tspeed: 0.1143s/iter; left time: 4309.3404s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0855648\n",
      "\tspeed: 0.1149s/iter; left time: 4320.9223s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0735175\n",
      "\tspeed: 0.1140s/iter; left time: 4276.1456s\n",
      "Epoch: 6 cost time: 00h:05m:07.62s\n",
      "Epoch: 6 | Train Loss: 0.0828304 Vali Loss: 0.0819640 Test Loss: 0.0980078\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0902265\n",
      "\tspeed: 1.0371s/iter; left time: 38722.3254s\n",
      "\titers: 200, epoch: 7 | loss: 0.0986805\n",
      "\tspeed: 0.1142s/iter; left time: 4250.9448s\n",
      "\titers: 300, epoch: 7 | loss: 0.0778703\n",
      "\tspeed: 0.1127s/iter; left time: 4186.3723s\n",
      "\titers: 400, epoch: 7 | loss: 0.0782931\n",
      "\tspeed: 0.1160s/iter; left time: 4295.4712s\n",
      "\titers: 500, epoch: 7 | loss: 0.0746187\n",
      "\tspeed: 0.1161s/iter; left time: 4289.4106s\n",
      "\titers: 600, epoch: 7 | loss: 0.0827935\n",
      "\tspeed: 0.1153s/iter; left time: 4245.6709s\n",
      "\titers: 700, epoch: 7 | loss: 0.0739331\n",
      "\tspeed: 0.1138s/iter; left time: 4180.5216s\n",
      "\titers: 800, epoch: 7 | loss: 0.0809667\n",
      "\tspeed: 0.1159s/iter; left time: 4245.3415s\n",
      "\titers: 900, epoch: 7 | loss: 0.0907432\n",
      "\tspeed: 0.1135s/iter; left time: 4148.0988s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0945175\n",
      "\tspeed: 0.1139s/iter; left time: 4151.0506s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0788761\n",
      "\tspeed: 0.1161s/iter; left time: 4218.0290s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0823496\n",
      "\tspeed: 0.1136s/iter; left time: 4115.4184s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0750478\n",
      "\tspeed: 0.1144s/iter; left time: 4134.4843s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0663611\n",
      "\tspeed: 0.1139s/iter; left time: 4105.3062s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0791697\n",
      "\tspeed: 0.1154s/iter; left time: 4146.3280s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0843642\n",
      "\tspeed: 0.1140s/iter; left time: 4085.1494s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0820326\n",
      "\tspeed: 0.1147s/iter; left time: 4097.9003s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0798094\n",
      "\tspeed: 0.1149s/iter; left time: 4093.0102s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0762156\n",
      "\tspeed: 0.1138s/iter; left time: 4045.6408s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0743921\n",
      "\tspeed: 0.1145s/iter; left time: 4058.3544s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0891102\n",
      "\tspeed: 0.1150s/iter; left time: 4062.2655s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0900708\n",
      "\tspeed: 0.1123s/iter; left time: 3958.3714s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0966768\n",
      "\tspeed: 0.1148s/iter; left time: 4035.3253s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0648704\n",
      "\tspeed: 0.1094s/iter; left time: 3832.9021s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0843473\n",
      "\tspeed: 0.1263s/iter; left time: 4411.6532s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0859990\n",
      "\tspeed: 0.1143s/iter; left time: 3981.3376s\n",
      "Epoch: 7 cost time: 00h:05m:07.73s\n",
      "Epoch: 7 | Train Loss: 0.0812575 Vali Loss: 0.0815965 Test Loss: 0.0964520\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0794488\n",
      "\tspeed: 1.0163s/iter; left time: 35228.0566s\n",
      "\titers: 200, epoch: 8 | loss: 0.0848540\n",
      "\tspeed: 0.1132s/iter; left time: 3911.2718s\n",
      "\titers: 300, epoch: 8 | loss: 0.0769302\n",
      "\tspeed: 0.1125s/iter; left time: 3876.9898s\n",
      "\titers: 400, epoch: 8 | loss: 0.0856332\n",
      "\tspeed: 0.1128s/iter; left time: 3877.8341s\n",
      "\titers: 500, epoch: 8 | loss: 0.0824820\n",
      "\tspeed: 0.1137s/iter; left time: 3895.0860s\n",
      "\titers: 600, epoch: 8 | loss: 0.0854290\n",
      "\tspeed: 0.1154s/iter; left time: 3943.4592s\n",
      "\titers: 700, epoch: 8 | loss: 0.0854288\n",
      "\tspeed: 0.1132s/iter; left time: 3857.4683s\n",
      "\titers: 800, epoch: 8 | loss: 0.0680083\n",
      "\tspeed: 0.1152s/iter; left time: 3914.0443s\n",
      "\titers: 900, epoch: 8 | loss: 0.0768383\n",
      "\tspeed: 0.1128s/iter; left time: 3819.4727s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0693779\n",
      "\tspeed: 0.1141s/iter; left time: 3852.3100s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0754587\n",
      "\tspeed: 0.1131s/iter; left time: 3805.9977s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0802685\n",
      "\tspeed: 0.1123s/iter; left time: 3770.7072s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0774171\n",
      "\tspeed: 0.1106s/iter; left time: 3700.4081s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754229\n",
      "\tspeed: 0.1129s/iter; left time: 3767.4139s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0888831\n",
      "\tspeed: 0.1134s/iter; left time: 3771.3276s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0705786\n",
      "\tspeed: 0.1153s/iter; left time: 3823.0475s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0788224\n",
      "\tspeed: 0.1161s/iter; left time: 3838.7923s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0689727\n",
      "\tspeed: 0.1123s/iter; left time: 3702.2267s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0841972\n",
      "\tspeed: 0.1137s/iter; left time: 3736.1949s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0773610\n",
      "\tspeed: 0.1131s/iter; left time: 3705.2475s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0929009\n",
      "\tspeed: 0.1137s/iter; left time: 3715.3777s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0882238\n",
      "\tspeed: 0.1135s/iter; left time: 3697.1907s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0731173\n",
      "\tspeed: 0.1146s/iter; left time: 3718.7553s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0878904\n",
      "\tspeed: 0.1116s/iter; left time: 3611.7746s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0880044\n",
      "\tspeed: 0.1123s/iter; left time: 3624.7379s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0914146\n",
      "\tspeed: 0.1131s/iter; left time: 3638.1944s\n",
      "Epoch: 8 cost time: 00h:05m:03.87s\n",
      "Epoch: 8 | Train Loss: 0.0798089 Vali Loss: 0.0830694 Test Loss: 0.0950187\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0826529\n",
      "\tspeed: 1.0051s/iter; left time: 32153.6951s\n",
      "\titers: 200, epoch: 9 | loss: 0.0691043\n",
      "\tspeed: 0.1136s/iter; left time: 3623.4213s\n",
      "\titers: 300, epoch: 9 | loss: 0.0827234\n",
      "\tspeed: 0.1129s/iter; left time: 3589.9077s\n",
      "\titers: 400, epoch: 9 | loss: 0.0850475\n",
      "\tspeed: 0.1135s/iter; left time: 3597.6932s\n",
      "\titers: 500, epoch: 9 | loss: 0.0775021\n",
      "\tspeed: 0.1146s/iter; left time: 3619.6377s\n",
      "\titers: 600, epoch: 9 | loss: 0.0795878\n",
      "\tspeed: 0.1145s/iter; left time: 3605.7645s\n",
      "\titers: 700, epoch: 9 | loss: 0.0837808\n",
      "\tspeed: 0.1141s/iter; left time: 3581.4114s\n",
      "\titers: 800, epoch: 9 | loss: 0.0798738\n",
      "\tspeed: 0.1141s/iter; left time: 3568.5339s\n",
      "\titers: 900, epoch: 9 | loss: 0.0779057\n",
      "\tspeed: 0.1147s/iter; left time: 3576.2283s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0727953\n",
      "\tspeed: 0.1138s/iter; left time: 3537.9186s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0807575\n",
      "\tspeed: 0.1118s/iter; left time: 3463.9758s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0787443\n",
      "\tspeed: 0.1131s/iter; left time: 3492.1398s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0788000\n",
      "\tspeed: 0.1126s/iter; left time: 3466.7881s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0729088\n",
      "\tspeed: 0.1134s/iter; left time: 3480.7516s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0709907\n",
      "\tspeed: 0.1127s/iter; left time: 3447.6636s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0749613\n",
      "\tspeed: 0.1132s/iter; left time: 3450.1363s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0788948\n",
      "\tspeed: 0.1122s/iter; left time: 3410.4812s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0830306\n",
      "\tspeed: 0.1127s/iter; left time: 3412.3783s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0778918\n",
      "\tspeed: 0.1129s/iter; left time: 3409.4986s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0789471\n",
      "\tspeed: 0.1115s/iter; left time: 3355.3329s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0754702\n",
      "\tspeed: 0.1164s/iter; left time: 3489.8922s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0705925\n",
      "\tspeed: 0.1147s/iter; left time: 3428.4263s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0731009\n",
      "\tspeed: 0.1136s/iter; left time: 3383.9702s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0785734\n",
      "\tspeed: 0.1141s/iter; left time: 3388.0698s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0922928\n",
      "\tspeed: 0.1156s/iter; left time: 3419.5757s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0796332\n",
      "\tspeed: 0.1155s/iter; left time: 3406.9806s\n",
      "Epoch: 9 cost time: 00h:05m:04.53s\n",
      "Epoch: 9 | Train Loss: 0.0784978 Vali Loss: 0.0850671 Test Loss: 0.0985102\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02088882215321064, rmse:0.14452965557575226, mae:0.09556279331445694, rse:0.4245525300502777\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 96: 01h:00m:14.20s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 13:31:51,060] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 13:31:51,984] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 13:31:51,984] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 13:31:51,984] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 13:31:52,087] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 13:31:52,087] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 13:31:52,844] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 13:31:52,845] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 13:31:52,845] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 13:31:52,846] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 13:31:52,846] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 13:31:52,846] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 13:31:52,846] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 13:31:53,247] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 13:31:53,248] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,248] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.46 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,434] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 13:31:53,435] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,435] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.48 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,435] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 13:31:53,587] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 13:31:53,588] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.46 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 13:31:53,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 13:31:53,590] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efcb907f850>\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2009432\n",
      "\tspeed: 0.2035s/iter; left time: 10833.1145s\n",
      "\titers: 200, epoch: 1 | loss: 0.2018673\n",
      "\tspeed: 0.1293s/iter; left time: 6868.6140s\n",
      "\titers: 300, epoch: 1 | loss: 0.1664647\n",
      "\tspeed: 0.1290s/iter; left time: 6843.2504s\n",
      "\titers: 400, epoch: 1 | loss: 0.1907074\n",
      "\tspeed: 0.1283s/iter; left time: 6792.1821s\n",
      "\titers: 500, epoch: 1 | loss: 0.1524435\n",
      "\tspeed: 0.1282s/iter; left time: 6775.9449s\n",
      "\titers: 600, epoch: 1 | loss: 0.1277965\n",
      "\tspeed: 0.1273s/iter; left time: 6713.1010s\n",
      "\titers: 700, epoch: 1 | loss: 0.1040149\n",
      "\tspeed: 0.1279s/iter; left time: 6735.2929s\n",
      "\titers: 800, epoch: 1 | loss: 0.1075614\n",
      "\tspeed: 0.1278s/iter; left time: 6714.1902s\n",
      "\titers: 900, epoch: 1 | loss: 0.1123559\n",
      "\tspeed: 0.1285s/iter; left time: 6738.3145s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1132720\n",
      "\tspeed: 0.1271s/iter; left time: 6650.2271s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0960071\n",
      "\tspeed: 0.1284s/iter; left time: 6706.3182s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1164301\n",
      "\tspeed: 0.1278s/iter; left time: 6661.5176s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1128735\n",
      "\tspeed: 0.1285s/iter; left time: 6687.8148s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1040649\n",
      "\tspeed: 0.1297s/iter; left time: 6737.2592s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1173225\n",
      "\tspeed: 0.1280s/iter; left time: 6633.6710s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1031440\n",
      "\tspeed: 0.1295s/iter; left time: 6699.5237s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0966299\n",
      "\tspeed: 0.1276s/iter; left time: 6591.9157s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1064342\n",
      "\tspeed: 0.1291s/iter; left time: 6654.9832s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0987400\n",
      "\tspeed: 0.1275s/iter; left time: 6559.7447s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1042845\n",
      "\tspeed: 0.1294s/iter; left time: 6643.9934s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0977351\n",
      "\tspeed: 0.1279s/iter; left time: 6552.6752s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0965085\n",
      "\tspeed: 0.1264s/iter; left time: 6463.5276s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0990303\n",
      "\tspeed: 0.1272s/iter; left time: 6491.7162s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1100298\n",
      "\tspeed: 0.1277s/iter; left time: 6505.7362s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0987217\n",
      "\tspeed: 0.1287s/iter; left time: 6541.4817s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1110397\n",
      "\tspeed: 0.1283s/iter; left time: 6507.8580s\n",
      "Epoch: 1 cost time: 00h:05m:46.31s\n",
      "Epoch: 1 | Train Loss: 0.1213860 Vali Loss: 0.0894020 Test Loss: 0.1034134\n",
      "Validation loss decreased (inf --> 0.089402).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0940944\n",
      "\tspeed: 1.1672s/iter; left time: 59029.4659s\n",
      "\titers: 200, epoch: 2 | loss: 0.0937534\n",
      "\tspeed: 0.1127s/iter; left time: 5687.0277s\n",
      "\titers: 300, epoch: 2 | loss: 0.1080840\n",
      "\tspeed: 0.1129s/iter; left time: 5685.3026s\n",
      "\titers: 400, epoch: 2 | loss: 0.0966061\n",
      "\tspeed: 0.1141s/iter; left time: 5735.0344s\n",
      "\titers: 500, epoch: 2 | loss: 0.1093938\n",
      "\tspeed: 0.1147s/iter; left time: 5756.6175s\n",
      "\titers: 600, epoch: 2 | loss: 0.0923856\n",
      "\tspeed: 0.1165s/iter; left time: 5835.2127s\n",
      "\titers: 700, epoch: 2 | loss: 0.0916145\n",
      "\tspeed: 0.1149s/iter; left time: 5742.0539s\n",
      "\titers: 800, epoch: 2 | loss: 0.0939326\n",
      "\tspeed: 0.1145s/iter; left time: 5708.3946s\n",
      "\titers: 900, epoch: 2 | loss: 0.1001902\n",
      "\tspeed: 0.1131s/iter; left time: 5629.7220s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0946725\n",
      "\tspeed: 0.1164s/iter; left time: 5780.2450s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0976080\n",
      "\tspeed: 0.1134s/iter; left time: 5622.9334s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017308\n",
      "\tspeed: 0.1144s/iter; left time: 5662.1506s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1020761\n",
      "\tspeed: 0.1132s/iter; left time: 5588.9233s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0914178\n",
      "\tspeed: 0.1127s/iter; left time: 5554.0966s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1076127\n",
      "\tspeed: 0.1107s/iter; left time: 5443.4879s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0961113\n",
      "\tspeed: 0.1147s/iter; left time: 5627.7270s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1049188\n",
      "\tspeed: 0.1157s/iter; left time: 5664.6585s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0905886\n",
      "\tspeed: 0.1133s/iter; left time: 5536.0354s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0959672\n",
      "\tspeed: 0.1155s/iter; left time: 5631.1644s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0967659\n",
      "\tspeed: 0.1152s/iter; left time: 5605.1883s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1091169\n",
      "\tspeed: 0.1165s/iter; left time: 5656.5862s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0912857\n",
      "\tspeed: 0.1129s/iter; left time: 5471.1685s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0814347\n",
      "\tspeed: 0.1148s/iter; left time: 5555.4270s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1000843\n",
      "\tspeed: 0.1147s/iter; left time: 5537.4035s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0908363\n",
      "\tspeed: 0.1124s/iter; left time: 5415.2498s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0960613\n",
      "\tspeed: 0.1152s/iter; left time: 5539.6381s\n",
      "Epoch: 2 cost time: 00h:05m:05.02s\n",
      "Epoch: 2 | Train Loss: 0.0975315 Vali Loss: 0.0867351 Test Loss: 0.0988254\n",
      "Validation loss decreased (0.089402 --> 0.086735).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0956383\n",
      "\tspeed: 1.0260s/iter; left time: 49153.2021s\n",
      "\titers: 200, epoch: 3 | loss: 0.0700900\n",
      "\tspeed: 0.1136s/iter; left time: 5432.9049s\n",
      "\titers: 300, epoch: 3 | loss: 0.0907799\n",
      "\tspeed: 0.1157s/iter; left time: 5521.0043s\n",
      "\titers: 400, epoch: 3 | loss: 0.1059120\n",
      "\tspeed: 0.1128s/iter; left time: 5371.1919s\n",
      "\titers: 500, epoch: 3 | loss: 0.0847025\n",
      "\tspeed: 0.1155s/iter; left time: 5489.1741s\n",
      "\titers: 600, epoch: 3 | loss: 0.1060325\n",
      "\tspeed: 0.1141s/iter; left time: 5408.1395s\n",
      "\titers: 700, epoch: 3 | loss: 0.0842078\n",
      "\tspeed: 0.1145s/iter; left time: 5418.1948s\n",
      "\titers: 800, epoch: 3 | loss: 0.1065284\n",
      "\tspeed: 0.1155s/iter; left time: 5451.8367s\n",
      "\titers: 900, epoch: 3 | loss: 0.0883381\n",
      "\tspeed: 0.1152s/iter; left time: 5427.6164s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0973774\n",
      "\tspeed: 0.1147s/iter; left time: 5392.8294s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1029167\n",
      "\tspeed: 0.1151s/iter; left time: 5398.3411s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1063046\n",
      "\tspeed: 0.1145s/iter; left time: 5361.3742s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0910941\n",
      "\tspeed: 0.1125s/iter; left time: 5255.2026s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0989402\n",
      "\tspeed: 0.1167s/iter; left time: 5436.8592s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0897562\n",
      "\tspeed: 0.1155s/iter; left time: 5372.7001s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1038726\n",
      "\tspeed: 0.1128s/iter; left time: 5234.9290s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029803\n",
      "\tspeed: 0.1167s/iter; left time: 5402.0971s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0976344\n",
      "\tspeed: 0.1143s/iter; left time: 5281.2633s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0954050\n",
      "\tspeed: 0.1146s/iter; left time: 5285.8710s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0907561\n",
      "\tspeed: 0.1151s/iter; left time: 5297.6428s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1184174\n",
      "\tspeed: 0.1146s/iter; left time: 5259.1909s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0903510\n",
      "\tspeed: 0.1169s/iter; left time: 5356.7739s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0935952\n",
      "\tspeed: 0.1136s/iter; left time: 5190.0479s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0925637\n",
      "\tspeed: 0.1134s/iter; left time: 5174.0763s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0878491\n",
      "\tspeed: 0.1141s/iter; left time: 5191.0910s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1013690\n",
      "\tspeed: 0.1146s/iter; left time: 5203.8776s\n",
      "Epoch: 3 cost time: 00h:05m:06.35s\n",
      "Epoch: 3 | Train Loss: 0.0937625 Vali Loss: 0.0846537 Test Loss: 0.0967681\n",
      "Validation loss decreased (0.086735 --> 0.084654).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0943716\n",
      "\tspeed: 1.0024s/iter; left time: 45349.0421s\n",
      "\titers: 200, epoch: 4 | loss: 0.0859998\n",
      "\tspeed: 0.1120s/iter; left time: 5056.7647s\n",
      "\titers: 300, epoch: 4 | loss: 0.0874903\n",
      "\tspeed: 0.1157s/iter; left time: 5209.7013s\n",
      "\titers: 400, epoch: 4 | loss: 0.1020916\n",
      "\tspeed: 0.1141s/iter; left time: 5128.7587s\n",
      "\titers: 500, epoch: 4 | loss: 0.0874067\n",
      "\tspeed: 0.1157s/iter; left time: 5186.5889s\n",
      "\titers: 600, epoch: 4 | loss: 0.1072563\n",
      "\tspeed: 0.1163s/iter; left time: 5201.6022s\n",
      "\titers: 700, epoch: 4 | loss: 0.0964151\n",
      "\tspeed: 0.1148s/iter; left time: 5122.9166s\n",
      "\titers: 800, epoch: 4 | loss: 0.0861687\n",
      "\tspeed: 0.1151s/iter; left time: 5128.6994s\n",
      "\titers: 900, epoch: 4 | loss: 0.0826063\n",
      "\tspeed: 0.1158s/iter; left time: 5148.3643s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0798693\n",
      "\tspeed: 0.1149s/iter; left time: 5095.4728s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0928330\n",
      "\tspeed: 0.1145s/iter; left time: 5063.9366s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0862791\n",
      "\tspeed: 0.1143s/iter; left time: 5045.4097s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0830672\n",
      "\tspeed: 0.1141s/iter; left time: 5023.8441s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0836192\n",
      "\tspeed: 0.1138s/iter; left time: 4998.9174s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0898090\n",
      "\tspeed: 0.1153s/iter; left time: 5053.3962s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0897531\n",
      "\tspeed: 0.1150s/iter; left time: 5031.5050s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0794996\n",
      "\tspeed: 0.1137s/iter; left time: 4962.9002s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0978874\n",
      "\tspeed: 0.1142s/iter; left time: 4973.5847s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0965451\n",
      "\tspeed: 0.1134s/iter; left time: 4927.6185s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0926374\n",
      "\tspeed: 0.1135s/iter; left time: 4919.1444s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1069342\n",
      "\tspeed: 0.1117s/iter; left time: 4830.5466s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0779019\n",
      "\tspeed: 0.1142s/iter; left time: 4927.8059s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0973944\n",
      "\tspeed: 0.1117s/iter; left time: 4808.6873s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0905181\n",
      "\tspeed: 0.1153s/iter; left time: 4952.0385s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0938979\n",
      "\tspeed: 0.1141s/iter; left time: 4887.8665s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0907974\n",
      "\tspeed: 0.1140s/iter; left time: 4873.3340s\n",
      "Epoch: 4 cost time: 00h:05m:05.20s\n",
      "Epoch: 4 | Train Loss: 0.0911554 Vali Loss: 0.0860957 Test Loss: 0.0986955\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0987655\n",
      "\tspeed: 0.9952s/iter; left time: 42367.6231s\n",
      "\titers: 200, epoch: 5 | loss: 0.0867120\n",
      "\tspeed: 0.1171s/iter; left time: 4973.4730s\n",
      "\titers: 300, epoch: 5 | loss: 0.0962949\n",
      "\tspeed: 0.1150s/iter; left time: 4872.4313s\n",
      "\titers: 400, epoch: 5 | loss: 0.0800745\n",
      "\tspeed: 0.1159s/iter; left time: 4899.5939s\n",
      "\titers: 500, epoch: 5 | loss: 0.0932057\n",
      "\tspeed: 0.1154s/iter; left time: 4867.5819s\n",
      "\titers: 600, epoch: 5 | loss: 0.0953641\n",
      "\tspeed: 0.1147s/iter; left time: 4824.5822s\n",
      "\titers: 700, epoch: 5 | loss: 0.0878022\n",
      "\tspeed: 0.1167s/iter; left time: 4897.0108s\n",
      "\titers: 800, epoch: 5 | loss: 0.0875066\n",
      "\tspeed: 0.1152s/iter; left time: 4824.5467s\n",
      "\titers: 900, epoch: 5 | loss: 0.0935306\n",
      "\tspeed: 0.1156s/iter; left time: 4827.2523s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0829855\n",
      "\tspeed: 0.1151s/iter; left time: 4795.5210s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0839508\n",
      "\tspeed: 0.1156s/iter; left time: 4807.3895s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0897008\n",
      "\tspeed: 0.1156s/iter; left time: 4792.3140s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0881073\n",
      "\tspeed: 0.1146s/iter; left time: 4742.8585s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0853253\n",
      "\tspeed: 0.1140s/iter; left time: 4704.8949s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0889603\n",
      "\tspeed: 0.1154s/iter; left time: 4752.0199s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1002251\n",
      "\tspeed: 0.1152s/iter; left time: 4732.4010s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1032137\n",
      "\tspeed: 0.1144s/iter; left time: 4687.8406s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0937006\n",
      "\tspeed: 0.1135s/iter; left time: 4638.2335s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0855721\n",
      "\tspeed: 0.1146s/iter; left time: 4671.3233s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0733401\n",
      "\tspeed: 0.1138s/iter; left time: 4628.5156s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0945749\n",
      "\tspeed: 0.1134s/iter; left time: 4600.9319s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0890472\n",
      "\tspeed: 0.1146s/iter; left time: 4636.7185s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0819968\n",
      "\tspeed: 0.1150s/iter; left time: 4642.4987s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0929638\n",
      "\tspeed: 0.1133s/iter; left time: 4561.2040s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0771377\n",
      "\tspeed: 0.1118s/iter; left time: 4492.8096s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0927820\n",
      "\tspeed: 0.1145s/iter; left time: 4588.9597s\n",
      "Epoch: 5 cost time: 00h:05m:06.40s\n",
      "Epoch: 5 | Train Loss: 0.0889107 Vali Loss: 0.0877582 Test Loss: 0.1025786\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0778098\n",
      "\tspeed: 0.9945s/iter; left time: 39685.6441s\n",
      "\titers: 200, epoch: 6 | loss: 0.0997249\n",
      "\tspeed: 0.1172s/iter; left time: 4664.5360s\n",
      "\titers: 300, epoch: 6 | loss: 0.0860444\n",
      "\tspeed: 0.1130s/iter; left time: 4487.8382s\n",
      "\titers: 400, epoch: 6 | loss: 0.0840693\n",
      "\tspeed: 0.1132s/iter; left time: 4483.2478s\n",
      "\titers: 500, epoch: 6 | loss: 0.0925132\n",
      "\tspeed: 0.1152s/iter; left time: 4551.5005s\n",
      "\titers: 600, epoch: 6 | loss: 0.0898937\n",
      "\tspeed: 0.1136s/iter; left time: 4477.7601s\n",
      "\titers: 700, epoch: 6 | loss: 0.0814919\n",
      "\tspeed: 0.1136s/iter; left time: 4463.2690s\n",
      "\titers: 800, epoch: 6 | loss: 0.1018386\n",
      "\tspeed: 0.1140s/iter; left time: 4470.2427s\n",
      "\titers: 900, epoch: 6 | loss: 0.0892932\n",
      "\tspeed: 0.1119s/iter; left time: 4376.0153s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1022972\n",
      "\tspeed: 0.1144s/iter; left time: 4463.6517s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0904634\n",
      "\tspeed: 0.1123s/iter; left time: 4368.4022s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0951061\n",
      "\tspeed: 0.1129s/iter; left time: 4379.7294s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0853480\n",
      "\tspeed: 0.1134s/iter; left time: 4387.5211s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0741242\n",
      "\tspeed: 0.1127s/iter; left time: 4350.3364s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0868103\n",
      "\tspeed: 0.1131s/iter; left time: 4355.5334s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0908482\n",
      "\tspeed: 0.1155s/iter; left time: 4437.4801s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0931770\n",
      "\tspeed: 0.1144s/iter; left time: 4381.3060s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0878102\n",
      "\tspeed: 0.1139s/iter; left time: 4351.3232s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0864693\n",
      "\tspeed: 0.1136s/iter; left time: 4328.0597s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849704\n",
      "\tspeed: 0.1143s/iter; left time: 4343.3339s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0827954\n",
      "\tspeed: 0.1146s/iter; left time: 4342.5904s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0875449\n",
      "\tspeed: 0.1159s/iter; left time: 4380.5153s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0922521\n",
      "\tspeed: 0.1128s/iter; left time: 4252.8108s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0752860\n",
      "\tspeed: 0.1127s/iter; left time: 4238.0111s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0840485\n",
      "\tspeed: 0.1136s/iter; left time: 4262.2877s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0845857\n",
      "\tspeed: 0.1137s/iter; left time: 4253.1654s\n",
      "Epoch: 6 cost time: 00h:05m:04.20s\n",
      "Epoch: 6 | Train Loss: 0.0867293 Vali Loss: 0.0884304 Test Loss: 0.1025518\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0863808\n",
      "\tspeed: 0.9864s/iter; left time: 36732.8987s\n",
      "\titers: 200, epoch: 7 | loss: 0.0894201\n",
      "\tspeed: 0.1121s/iter; left time: 4164.6086s\n",
      "\titers: 300, epoch: 7 | loss: 0.0827672\n",
      "\tspeed: 0.1130s/iter; left time: 4184.5254s\n",
      "\titers: 400, epoch: 7 | loss: 0.0769152\n",
      "\tspeed: 0.1119s/iter; left time: 4134.6707s\n",
      "\titers: 500, epoch: 7 | loss: 0.0987583\n",
      "\tspeed: 0.1143s/iter; left time: 4212.1974s\n",
      "\titers: 600, epoch: 7 | loss: 0.0983056\n",
      "\tspeed: 0.1149s/iter; left time: 4219.9864s\n",
      "\titers: 700, epoch: 7 | loss: 0.0806711\n",
      "\tspeed: 0.1140s/iter; left time: 4177.7662s\n",
      "\titers: 800, epoch: 7 | loss: 0.0801147\n",
      "\tspeed: 0.1122s/iter; left time: 4100.5414s\n",
      "\titers: 900, epoch: 7 | loss: 0.0864820\n",
      "\tspeed: 0.1124s/iter; left time: 4094.9165s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0967652\n",
      "\tspeed: 0.1134s/iter; left time: 4119.4826s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0854905\n",
      "\tspeed: 0.1120s/iter; left time: 4058.1003s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0898021\n",
      "\tspeed: 0.1120s/iter; left time: 4047.9327s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0775288\n",
      "\tspeed: 0.1125s/iter; left time: 4052.7830s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0811367\n",
      "\tspeed: 0.1133s/iter; left time: 4073.1715s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0821564\n",
      "\tspeed: 0.1130s/iter; left time: 4048.8283s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0834867\n",
      "\tspeed: 0.1149s/iter; left time: 4105.0264s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0906567\n",
      "\tspeed: 0.1133s/iter; left time: 4037.1594s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0855301\n",
      "\tspeed: 0.1132s/iter; left time: 4024.6562s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0763849\n",
      "\tspeed: 0.1151s/iter; left time: 4080.5070s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0703013\n",
      "\tspeed: 0.1135s/iter; left time: 4012.6210s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0814764\n",
      "\tspeed: 0.1135s/iter; left time: 4000.1259s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0879330\n",
      "\tspeed: 0.1136s/iter; left time: 3992.9013s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0835790\n",
      "\tspeed: 0.1154s/iter; left time: 4043.7016s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0896361\n",
      "\tspeed: 0.1151s/iter; left time: 4023.0234s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0880383\n",
      "\tspeed: 0.1147s/iter; left time: 3994.3573s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0784809\n",
      "\tspeed: 0.1152s/iter; left time: 4000.8033s\n",
      "Epoch: 7 cost time: 00h:05m:03.25s\n",
      "Epoch: 7 | Train Loss: 0.0847558 Vali Loss: 0.0862495 Test Loss: 0.0994056\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0741981\n",
      "\tspeed: 0.9933s/iter; left time: 34339.2805s\n",
      "\titers: 200, epoch: 8 | loss: 0.0925967\n",
      "\tspeed: 0.1136s/iter; left time: 3917.0845s\n",
      "\titers: 300, epoch: 8 | loss: 0.0885595\n",
      "\tspeed: 0.1148s/iter; left time: 3946.5618s\n",
      "\titers: 400, epoch: 8 | loss: 0.0859595\n",
      "\tspeed: 0.1137s/iter; left time: 3896.4052s\n",
      "\titers: 500, epoch: 8 | loss: 0.0826866\n",
      "\tspeed: 0.1135s/iter; left time: 3878.4150s\n",
      "\titers: 600, epoch: 8 | loss: 0.0911786\n",
      "\tspeed: 0.1126s/iter; left time: 3838.0058s\n",
      "\titers: 700, epoch: 8 | loss: 0.0777338\n",
      "\tspeed: 0.1148s/iter; left time: 3901.0649s\n",
      "\titers: 800, epoch: 8 | loss: 0.0907550\n",
      "\tspeed: 0.1159s/iter; left time: 3924.6959s\n",
      "\titers: 900, epoch: 8 | loss: 0.0851398\n",
      "\tspeed: 0.1143s/iter; left time: 3860.4160s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0833690\n",
      "\tspeed: 0.1141s/iter; left time: 3841.0356s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0865234\n",
      "\tspeed: 0.1131s/iter; left time: 3797.9051s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0920289\n",
      "\tspeed: 0.1137s/iter; left time: 3805.3657s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0871832\n",
      "\tspeed: 0.1141s/iter; left time: 3807.8263s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754475\n",
      "\tspeed: 0.1139s/iter; left time: 3791.0663s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0724243\n",
      "\tspeed: 0.1130s/iter; left time: 3748.3517s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0741268\n",
      "\tspeed: 0.1149s/iter; left time: 3801.4922s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0789550\n",
      "\tspeed: 0.1151s/iter; left time: 3795.2618s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0848181\n",
      "\tspeed: 0.1131s/iter; left time: 3719.2967s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0913343\n",
      "\tspeed: 0.1155s/iter; left time: 3786.3674s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0802559\n",
      "\tspeed: 0.1130s/iter; left time: 3693.1555s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0726332\n",
      "\tspeed: 0.1144s/iter; left time: 3725.4667s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0801809\n",
      "\tspeed: 0.1176s/iter; left time: 3818.9217s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0922091\n",
      "\tspeed: 0.1157s/iter; left time: 3746.9757s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0758863\n",
      "\tspeed: 0.1144s/iter; left time: 3692.1940s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0947716\n",
      "\tspeed: 0.1168s/iter; left time: 3757.6151s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0881028\n",
      "\tspeed: 0.1149s/iter; left time: 3685.7139s\n",
      "Epoch: 8 cost time: 00h:05m:05.67s\n",
      "Epoch: 8 | Train Loss: 0.0829597 Vali Loss: 0.0875380 Test Loss: 0.1024083\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02114104852080345, rmse:0.14539961516857147, mae:0.09676817059516907, rse:0.4269324839115143\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 168: 00h:53m:27.00s\n",
      "\n",
      "Intermediate time for ES: 04h:00m:32.34s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 14:25:18,527] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 14:25:19,550] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 14:25:19,551] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 14:25:19,551] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 14:25:19,613] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 14:25:19,613] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 14:25:20,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 14:25:20,312] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 14:25:20,312] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 14:25:20,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 14:25:20,313] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 14:25:20,313] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 14:25:20,667] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 14:25:20,669] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:20,702] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:20,874] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 14:25:20,875] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:20,876] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.52 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:20,876] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 14:25:21,034] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 14:25:21,035] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:21,035] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.42 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 14:25:21,037] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a91d56f50>\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1535002\n",
      "\tspeed: 0.1670s/iter; left time: 8937.9413s\n",
      "\titers: 200, epoch: 1 | loss: 0.1159472\n",
      "\tspeed: 0.1275s/iter; left time: 6809.3841s\n",
      "\titers: 300, epoch: 1 | loss: 0.1058322\n",
      "\tspeed: 0.1286s/iter; left time: 6857.4477s\n",
      "\titers: 400, epoch: 1 | loss: 0.0828711\n",
      "\tspeed: 0.1281s/iter; left time: 6815.4143s\n",
      "\titers: 500, epoch: 1 | loss: 0.0717254\n",
      "\tspeed: 0.1262s/iter; left time: 6705.7068s\n",
      "\titers: 600, epoch: 1 | loss: 0.0627306\n",
      "\tspeed: 0.1277s/iter; left time: 6771.5508s\n",
      "\titers: 700, epoch: 1 | loss: 0.0745939\n",
      "\tspeed: 0.1279s/iter; left time: 6769.5948s\n",
      "\titers: 800, epoch: 1 | loss: 0.0592833\n",
      "\tspeed: 0.1284s/iter; left time: 6781.5981s\n",
      "\titers: 900, epoch: 1 | loss: 0.0581057\n",
      "\tspeed: 0.1259s/iter; left time: 6636.9253s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0899070\n",
      "\tspeed: 0.1285s/iter; left time: 6763.9433s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0486953\n",
      "\tspeed: 0.1277s/iter; left time: 6706.1418s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0688207\n",
      "\tspeed: 0.1277s/iter; left time: 6694.8470s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0650532\n",
      "\tspeed: 0.1284s/iter; left time: 6719.3383s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0587849\n",
      "\tspeed: 0.1262s/iter; left time: 6591.1690s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0618791\n",
      "\tspeed: 0.1259s/iter; left time: 6559.9799s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0509475\n",
      "\tspeed: 0.1254s/iter; left time: 6524.6139s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0735843\n",
      "\tspeed: 0.1276s/iter; left time: 6624.1152s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0482276\n",
      "\tspeed: 0.1256s/iter; left time: 6506.3246s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0562918\n",
      "\tspeed: 0.1279s/iter; left time: 6613.9771s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0636575\n",
      "\tspeed: 0.1298s/iter; left time: 6700.3561s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0499355\n",
      "\tspeed: 0.1273s/iter; left time: 6560.9881s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0701026\n",
      "\tspeed: 0.1284s/iter; left time: 6600.1536s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0674836\n",
      "\tspeed: 0.1258s/iter; left time: 6457.8720s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0581415\n",
      "\tspeed: 0.1270s/iter; left time: 6507.2497s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0673574\n",
      "\tspeed: 0.1290s/iter; left time: 6595.6348s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0543020\n",
      "\tspeed: 0.1304s/iter; left time: 6651.2315s\n",
      "Epoch: 1 cost time: 00h:05m:42.77s\n",
      "Epoch: 1 | Train Loss: 0.0739179 Vali Loss: 0.0605835 Test Loss: 0.0654644\n",
      "Validation loss decreased (inf --> 0.060583).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0619027\n",
      "\tspeed: 1.1911s/iter; left time: 60556.0340s\n",
      "\titers: 200, epoch: 2 | loss: 0.0812935\n",
      "\tspeed: 0.1142s/iter; left time: 5796.6521s\n",
      "\titers: 300, epoch: 2 | loss: 0.0682675\n",
      "\tspeed: 0.1137s/iter; left time: 5756.8308s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864105\n",
      "\tspeed: 0.1148s/iter; left time: 5803.4670s\n",
      "\titers: 500, epoch: 2 | loss: 0.0529756\n",
      "\tspeed: 0.1137s/iter; left time: 5734.8439s\n",
      "\titers: 600, epoch: 2 | loss: 0.0543025\n",
      "\tspeed: 0.1122s/iter; left time: 5647.7195s\n",
      "\titers: 700, epoch: 2 | loss: 0.0502084\n",
      "\tspeed: 0.1135s/iter; left time: 5704.4916s\n",
      "\titers: 800, epoch: 2 | loss: 0.0579205\n",
      "\tspeed: 0.1151s/iter; left time: 5773.4711s\n",
      "\titers: 900, epoch: 2 | loss: 0.0560098\n",
      "\tspeed: 0.1139s/iter; left time: 5697.6910s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0432923\n",
      "\tspeed: 0.1140s/iter; left time: 5695.6111s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0533719\n",
      "\tspeed: 0.1131s/iter; left time: 5635.8386s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0699260\n",
      "\tspeed: 0.1132s/iter; left time: 5630.1798s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0667628\n",
      "\tspeed: 0.1142s/iter; left time: 5667.3855s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0512179\n",
      "\tspeed: 0.1156s/iter; left time: 5729.1609s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0579801\n",
      "\tspeed: 0.1151s/iter; left time: 5689.2001s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0674562\n",
      "\tspeed: 0.1125s/iter; left time: 5552.5770s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0445822\n",
      "\tspeed: 0.1139s/iter; left time: 5607.3974s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0552087\n",
      "\tspeed: 0.1140s/iter; left time: 5602.9960s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0708642\n",
      "\tspeed: 0.1177s/iter; left time: 5773.8578s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0525380\n",
      "\tspeed: 0.1167s/iter; left time: 5713.1574s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0523798\n",
      "\tspeed: 0.1136s/iter; left time: 5549.7491s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0721295\n",
      "\tspeed: 0.1166s/iter; left time: 5685.0079s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0610736\n",
      "\tspeed: 0.1142s/iter; left time: 5555.7468s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0564372\n",
      "\tspeed: 0.1164s/iter; left time: 5648.3881s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0445440\n",
      "\tspeed: 0.1162s/iter; left time: 5627.6453s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0545508\n",
      "\tspeed: 0.1149s/iter; left time: 5555.9755s\n",
      "Epoch: 2 cost time: 00h:05m:07.46s\n",
      "Epoch: 2 | Train Loss: 0.0589849 Vali Loss: 0.0589729 Test Loss: 0.0641147\n",
      "Validation loss decreased (0.060583 --> 0.058973).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0636515\n",
      "\tspeed: 1.0464s/iter; left time: 50393.2997s\n",
      "\titers: 200, epoch: 3 | loss: 0.0587107\n",
      "\tspeed: 0.1175s/iter; left time: 5646.8515s\n",
      "\titers: 300, epoch: 3 | loss: 0.0566106\n",
      "\tspeed: 0.1164s/iter; left time: 5582.5851s\n",
      "\titers: 400, epoch: 3 | loss: 0.0612787\n",
      "\tspeed: 0.1159s/iter; left time: 5546.6321s\n",
      "\titers: 500, epoch: 3 | loss: 0.0468024\n",
      "\tspeed: 0.1167s/iter; left time: 5574.9883s\n",
      "\titers: 600, epoch: 3 | loss: 0.0613940\n",
      "\tspeed: 0.1163s/iter; left time: 5541.0090s\n",
      "\titers: 700, epoch: 3 | loss: 0.0506664\n",
      "\tspeed: 0.1168s/iter; left time: 5554.4904s\n",
      "\titers: 800, epoch: 3 | loss: 0.0535050\n",
      "\tspeed: 0.1146s/iter; left time: 5439.7039s\n",
      "\titers: 900, epoch: 3 | loss: 0.0552201\n",
      "\tspeed: 0.1144s/iter; left time: 5418.4324s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0510626\n",
      "\tspeed: 0.1163s/iter; left time: 5494.0462s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0414352\n",
      "\tspeed: 0.1163s/iter; left time: 5485.1237s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0635938\n",
      "\tspeed: 0.1138s/iter; left time: 5357.0654s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0423877\n",
      "\tspeed: 0.1161s/iter; left time: 5452.5070s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0537974\n",
      "\tspeed: 0.1151s/iter; left time: 5392.1644s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0664770\n",
      "\tspeed: 0.1158s/iter; left time: 5414.7966s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0489237\n",
      "\tspeed: 0.1161s/iter; left time: 5418.3764s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0435316\n",
      "\tspeed: 0.1177s/iter; left time: 5479.9241s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0485576\n",
      "\tspeed: 0.1181s/iter; left time: 5487.7930s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0613631\n",
      "\tspeed: 0.1161s/iter; left time: 5382.2814s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0441004\n",
      "\tspeed: 0.1166s/iter; left time: 5391.9324s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0450289\n",
      "\tspeed: 0.1163s/iter; left time: 5370.0735s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0547356\n",
      "\tspeed: 0.1152s/iter; left time: 5304.7146s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0599108\n",
      "\tspeed: 0.1152s/iter; left time: 5295.8818s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0434626\n",
      "\tspeed: 0.1159s/iter; left time: 5313.3749s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0669618\n",
      "\tspeed: 0.1147s/iter; left time: 5246.7893s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0644993\n",
      "\tspeed: 0.1144s/iter; left time: 5222.2977s\n",
      "Epoch: 3 cost time: 00h:05m:11.03s\n",
      "Epoch: 3 | Train Loss: 0.0563925 Vali Loss: 0.0578105 Test Loss: 0.0628996\n",
      "Validation loss decreased (0.058973 --> 0.057811).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0500212\n",
      "\tspeed: 1.0506s/iter; left time: 47778.1646s\n",
      "\titers: 200, epoch: 4 | loss: 0.0662548\n",
      "\tspeed: 0.1175s/iter; left time: 5330.5472s\n",
      "\titers: 300, epoch: 4 | loss: 0.0432065\n",
      "\tspeed: 0.1171s/iter; left time: 5300.3254s\n",
      "\titers: 400, epoch: 4 | loss: 0.0570253\n",
      "\tspeed: 0.1166s/iter; left time: 5266.3735s\n",
      "\titers: 500, epoch: 4 | loss: 0.0514172\n",
      "\tspeed: 0.1160s/iter; left time: 5229.6123s\n",
      "\titers: 600, epoch: 4 | loss: 0.0459452\n",
      "\tspeed: 0.1163s/iter; left time: 5231.5868s\n",
      "\titers: 700, epoch: 4 | loss: 0.0489160\n",
      "\tspeed: 0.1175s/iter; left time: 5272.4026s\n",
      "\titers: 800, epoch: 4 | loss: 0.0543107\n",
      "\tspeed: 0.1173s/iter; left time: 5251.5969s\n",
      "\titers: 900, epoch: 4 | loss: 0.0444514\n",
      "\tspeed: 0.1172s/iter; left time: 5235.3134s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0546588\n",
      "\tspeed: 0.1172s/iter; left time: 5224.0163s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0588563\n",
      "\tspeed: 0.1181s/iter; left time: 5253.8665s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0534603\n",
      "\tspeed: 0.1166s/iter; left time: 5174.8999s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0535597\n",
      "\tspeed: 0.1171s/iter; left time: 5183.7953s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0503390\n",
      "\tspeed: 0.1161s/iter; left time: 5130.1821s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0542765\n",
      "\tspeed: 0.1151s/iter; left time: 5072.2718s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0546105\n",
      "\tspeed: 0.1151s/iter; left time: 5059.6824s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0538735\n",
      "\tspeed: 0.1161s/iter; left time: 5095.0027s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0518390\n",
      "\tspeed: 0.1178s/iter; left time: 5156.7007s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0519650\n",
      "\tspeed: 0.1165s/iter; left time: 5089.3941s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0477860\n",
      "\tspeed: 0.1168s/iter; left time: 5092.0510s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0585879\n",
      "\tspeed: 0.1162s/iter; left time: 5052.5469s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0599166\n",
      "\tspeed: 0.1159s/iter; left time: 5026.3131s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0504315\n",
      "\tspeed: 0.1145s/iter; left time: 4956.8512s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0512709\n",
      "\tspeed: 0.1140s/iter; left time: 4922.5263s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0432640\n",
      "\tspeed: 0.1139s/iter; left time: 4905.8741s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0526977\n",
      "\tspeed: 0.1152s/iter; left time: 4953.0588s\n",
      "Epoch: 4 cost time: 00h:05m:11.89s\n",
      "Epoch: 4 | Train Loss: 0.0547071 Vali Loss: 0.0565705 Test Loss: 0.0614690\n",
      "Validation loss decreased (0.057811 --> 0.056570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0500493\n",
      "\tspeed: 1.0484s/iter; left time: 44866.7391s\n",
      "\titers: 200, epoch: 5 | loss: 0.0452871\n",
      "\tspeed: 0.1145s/iter; left time: 4888.1574s\n",
      "\titers: 300, epoch: 5 | loss: 0.0708607\n",
      "\tspeed: 0.1138s/iter; left time: 4846.1708s\n",
      "\titers: 400, epoch: 5 | loss: 0.0393255\n",
      "\tspeed: 0.1172s/iter; left time: 4982.5589s\n",
      "\titers: 500, epoch: 5 | loss: 0.0584477\n",
      "\tspeed: 0.1150s/iter; left time: 4875.3948s\n",
      "\titers: 600, epoch: 5 | loss: 0.0544610\n",
      "\tspeed: 0.1140s/iter; left time: 4821.9702s\n",
      "\titers: 700, epoch: 5 | loss: 0.0538923\n",
      "\tspeed: 0.1139s/iter; left time: 4805.8009s\n",
      "\titers: 800, epoch: 5 | loss: 0.0542743\n",
      "\tspeed: 0.1144s/iter; left time: 4815.9957s\n",
      "\titers: 900, epoch: 5 | loss: 0.0646305\n",
      "\tspeed: 0.1151s/iter; left time: 4835.4620s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0515460\n",
      "\tspeed: 0.1158s/iter; left time: 4850.9438s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0574071\n",
      "\tspeed: 0.1162s/iter; left time: 4856.9851s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0493728\n",
      "\tspeed: 0.1156s/iter; left time: 4819.2445s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0618911\n",
      "\tspeed: 0.1169s/iter; left time: 4862.4262s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0536328\n",
      "\tspeed: 0.1164s/iter; left time: 4829.5067s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0613508\n",
      "\tspeed: 0.1165s/iter; left time: 4823.5586s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0503679\n",
      "\tspeed: 0.1169s/iter; left time: 4825.6196s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0521905\n",
      "\tspeed: 0.1185s/iter; left time: 4882.8882s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0556836\n",
      "\tspeed: 0.1160s/iter; left time: 4767.8640s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0641983\n",
      "\tspeed: 0.1143s/iter; left time: 4683.9989s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0585940\n",
      "\tspeed: 0.1147s/iter; left time: 4690.2370s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0435131\n",
      "\tspeed: 0.1145s/iter; left time: 4672.2849s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0479925\n",
      "\tspeed: 0.1142s/iter; left time: 4646.0430s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0534774\n",
      "\tspeed: 0.1155s/iter; left time: 4689.6705s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0596892\n",
      "\tspeed: 0.1144s/iter; left time: 4633.2802s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0679414\n",
      "\tspeed: 0.1129s/iter; left time: 4560.5640s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0447534\n",
      "\tspeed: 0.1147s/iter; left time: 4622.3254s\n",
      "Epoch: 5 cost time: 00h:05m:09.46s\n",
      "Epoch: 5 | Train Loss: 0.0534883 Vali Loss: 0.0565160 Test Loss: 0.0616658\n",
      "Validation loss decreased (0.056570 --> 0.056516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0517764\n",
      "\tspeed: 1.0445s/iter; left time: 41900.6783s\n",
      "\titers: 200, epoch: 6 | loss: 0.0638383\n",
      "\tspeed: 0.1134s/iter; left time: 4537.3457s\n",
      "\titers: 300, epoch: 6 | loss: 0.0642103\n",
      "\tspeed: 0.1128s/iter; left time: 4502.1621s\n",
      "\titers: 400, epoch: 6 | loss: 0.0472873\n",
      "\tspeed: 0.1133s/iter; left time: 4512.5762s\n",
      "\titers: 500, epoch: 6 | loss: 0.0434228\n",
      "\tspeed: 0.1128s/iter; left time: 4479.2753s\n",
      "\titers: 600, epoch: 6 | loss: 0.0501320\n",
      "\tspeed: 0.1136s/iter; left time: 4501.4273s\n",
      "\titers: 700, epoch: 6 | loss: 0.0372537\n",
      "\tspeed: 0.1124s/iter; left time: 4441.1197s\n",
      "\titers: 800, epoch: 6 | loss: 0.0626084\n",
      "\tspeed: 0.1113s/iter; left time: 4388.8442s\n",
      "\titers: 900, epoch: 6 | loss: 0.0585214\n",
      "\tspeed: 0.1117s/iter; left time: 4392.3550s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0676461\n",
      "\tspeed: 0.1126s/iter; left time: 4416.9388s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0532079\n",
      "\tspeed: 0.1122s/iter; left time: 4388.2556s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0608200\n",
      "\tspeed: 0.1120s/iter; left time: 4369.7022s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0481969\n",
      "\tspeed: 0.1127s/iter; left time: 4384.0525s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0451223\n",
      "\tspeed: 0.1121s/iter; left time: 4351.5105s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0623270\n",
      "\tspeed: 0.1142s/iter; left time: 4422.8118s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0533873\n",
      "\tspeed: 0.1136s/iter; left time: 4385.8150s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0445180\n",
      "\tspeed: 0.1126s/iter; left time: 4336.7563s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0662053\n",
      "\tspeed: 0.1132s/iter; left time: 4348.3974s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0493251\n",
      "\tspeed: 0.1128s/iter; left time: 4320.9003s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0448093\n",
      "\tspeed: 0.1136s/iter; left time: 4340.1308s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0606518\n",
      "\tspeed: 0.1118s/iter; left time: 4261.2007s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0587236\n",
      "\tspeed: 0.1131s/iter; left time: 4299.3428s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0420482\n",
      "\tspeed: 0.1136s/iter; left time: 4305.8230s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0426998\n",
      "\tspeed: 0.1142s/iter; left time: 4319.9757s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0466667\n",
      "\tspeed: 0.1155s/iter; left time: 4355.6813s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0537168\n",
      "\tspeed: 0.1127s/iter; left time: 4240.1491s\n",
      "Epoch: 6 cost time: 00h:05m:03.61s\n",
      "Epoch: 6 | Train Loss: 0.0525643 Vali Loss: 0.0556040 Test Loss: 0.0610864\n",
      "Validation loss decreased (0.056516 --> 0.055604).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0668757\n",
      "\tspeed: 1.0519s/iter; left time: 39376.3420s\n",
      "\titers: 200, epoch: 7 | loss: 0.0545903\n",
      "\tspeed: 0.1115s/iter; left time: 4163.8687s\n",
      "\titers: 300, epoch: 7 | loss: 0.0470054\n",
      "\tspeed: 0.1123s/iter; left time: 4181.6989s\n",
      "\titers: 400, epoch: 7 | loss: 0.0559453\n",
      "\tspeed: 0.1150s/iter; left time: 4270.1900s\n",
      "\titers: 500, epoch: 7 | loss: 0.0481483\n",
      "\tspeed: 0.1164s/iter; left time: 4309.3933s\n",
      "\titers: 600, epoch: 7 | loss: 0.0419734\n",
      "\tspeed: 0.1134s/iter; left time: 4187.1318s\n",
      "\titers: 700, epoch: 7 | loss: 0.0507655\n",
      "\tspeed: 0.1132s/iter; left time: 4171.0630s\n",
      "\titers: 800, epoch: 7 | loss: 0.0553637\n",
      "\tspeed: 0.1127s/iter; left time: 4138.8237s\n",
      "\titers: 900, epoch: 7 | loss: 0.0417836\n",
      "\tspeed: 0.1129s/iter; left time: 4136.5673s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0594274\n",
      "\tspeed: 0.1124s/iter; left time: 4107.9578s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0467496\n",
      "\tspeed: 0.1130s/iter; left time: 4116.9479s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0645316\n",
      "\tspeed: 0.1121s/iter; left time: 4074.7185s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0443441\n",
      "\tspeed: 0.1142s/iter; left time: 4136.5834s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0480593\n",
      "\tspeed: 0.1133s/iter; left time: 4093.1993s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0565358\n",
      "\tspeed: 0.1144s/iter; left time: 4122.1659s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0584448\n",
      "\tspeed: 0.1122s/iter; left time: 4030.6157s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0491271\n",
      "\tspeed: 0.1134s/iter; left time: 4063.1363s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0501366\n",
      "\tspeed: 0.1122s/iter; left time: 4011.0066s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0490542\n",
      "\tspeed: 0.1133s/iter; left time: 4035.9678s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0508287\n",
      "\tspeed: 0.1151s/iter; left time: 4088.5964s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0515859\n",
      "\tspeed: 0.1139s/iter; left time: 4036.4593s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0453970\n",
      "\tspeed: 0.1135s/iter; left time: 4009.3773s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0537278\n",
      "\tspeed: 0.1134s/iter; left time: 3996.6944s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0542764\n",
      "\tspeed: 0.1138s/iter; left time: 3998.1799s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0465567\n",
      "\tspeed: 0.1138s/iter; left time: 3985.5715s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0497594\n",
      "\tspeed: 0.1147s/iter; left time: 4006.3793s\n",
      "Epoch: 7 cost time: 00h:05m:04.75s\n",
      "Epoch: 7 | Train Loss: 0.0518482 Vali Loss: 0.0550725 Test Loss: 0.0603381\n",
      "Validation loss decreased (0.055604 --> 0.055073).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0724908\n",
      "\tspeed: 1.0599s/iter; left time: 36834.1324s\n",
      "\titers: 200, epoch: 8 | loss: 0.0371657\n",
      "\tspeed: 0.1160s/iter; left time: 4019.2294s\n",
      "\titers: 300, epoch: 8 | loss: 0.0593067\n",
      "\tspeed: 0.1154s/iter; left time: 3985.9655s\n",
      "\titers: 400, epoch: 8 | loss: 0.0512719\n",
      "\tspeed: 0.1166s/iter; left time: 4017.0193s\n",
      "\titers: 500, epoch: 8 | loss: 0.0472560\n",
      "\tspeed: 0.1165s/iter; left time: 4003.6044s\n",
      "\titers: 600, epoch: 8 | loss: 0.0475125\n",
      "\tspeed: 0.1151s/iter; left time: 3942.3386s\n",
      "\titers: 700, epoch: 8 | loss: 0.0528827\n",
      "\tspeed: 0.1138s/iter; left time: 3886.4899s\n",
      "\titers: 800, epoch: 8 | loss: 0.0542871\n",
      "\tspeed: 0.1135s/iter; left time: 3865.0685s\n",
      "\titers: 900, epoch: 8 | loss: 0.0525813\n",
      "\tspeed: 0.1143s/iter; left time: 3880.3574s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0576860\n",
      "\tspeed: 0.1137s/iter; left time: 3849.6669s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0429319\n",
      "\tspeed: 0.1154s/iter; left time: 3896.3183s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0528561\n",
      "\tspeed: 0.1133s/iter; left time: 3813.9504s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0534785\n",
      "\tspeed: 0.1126s/iter; left time: 3778.5499s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0459838\n",
      "\tspeed: 0.1152s/iter; left time: 3855.0111s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0495800\n",
      "\tspeed: 0.1134s/iter; left time: 3780.7558s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0621961\n",
      "\tspeed: 0.1122s/iter; left time: 3730.8639s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0502770\n",
      "\tspeed: 0.1138s/iter; left time: 3771.8794s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0546408\n",
      "\tspeed: 0.1121s/iter; left time: 3704.1273s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0553437\n",
      "\tspeed: 0.1153s/iter; left time: 3800.1403s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0498420\n",
      "\tspeed: 0.1125s/iter; left time: 3694.8584s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0456712\n",
      "\tspeed: 0.1123s/iter; left time: 3676.7870s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0570142\n",
      "\tspeed: 0.1142s/iter; left time: 3729.3105s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0586175\n",
      "\tspeed: 0.1147s/iter; left time: 3734.0547s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0472034\n",
      "\tspeed: 0.1149s/iter; left time: 3727.6649s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0537372\n",
      "\tspeed: 0.1137s/iter; left time: 3677.3987s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0509919\n",
      "\tspeed: 0.1121s/iter; left time: 3615.2861s\n",
      "Epoch: 8 cost time: 00h:05m:06.39s\n",
      "Epoch: 8 | Train Loss: 0.0511598 Vali Loss: 0.0561661 Test Loss: 0.0617564\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0486406\n",
      "\tspeed: 1.0277s/iter; left time: 32960.2609s\n",
      "\titers: 200, epoch: 9 | loss: 0.0556457\n",
      "\tspeed: 0.1133s/iter; left time: 3623.7339s\n",
      "\titers: 300, epoch: 9 | loss: 0.0395175\n",
      "\tspeed: 0.1130s/iter; left time: 3601.6127s\n",
      "\titers: 400, epoch: 9 | loss: 0.0557750\n",
      "\tspeed: 0.1131s/iter; left time: 3594.7021s\n",
      "\titers: 500, epoch: 9 | loss: 0.0424190\n",
      "\tspeed: 0.1129s/iter; left time: 3575.5721s\n",
      "\titers: 600, epoch: 9 | loss: 0.0351209\n",
      "\tspeed: 0.1124s/iter; left time: 3547.6705s\n",
      "\titers: 700, epoch: 9 | loss: 0.0446073\n",
      "\tspeed: 0.1135s/iter; left time: 3572.3584s\n",
      "\titers: 800, epoch: 9 | loss: 0.0384792\n",
      "\tspeed: 0.1122s/iter; left time: 3520.8111s\n",
      "\titers: 900, epoch: 9 | loss: 0.0720906\n",
      "\tspeed: 0.1121s/iter; left time: 3505.5232s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0432853\n",
      "\tspeed: 0.1126s/iter; left time: 3511.5431s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0520263\n",
      "\tspeed: 0.1140s/iter; left time: 3542.9423s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0406994\n",
      "\tspeed: 0.1136s/iter; left time: 3518.8090s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0368552\n",
      "\tspeed: 0.1128s/iter; left time: 3483.4962s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0537488\n",
      "\tspeed: 0.1140s/iter; left time: 3509.3689s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0565366\n",
      "\tspeed: 0.1129s/iter; left time: 3462.6149s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0400267\n",
      "\tspeed: 0.1144s/iter; left time: 3497.2432s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0363783\n",
      "\tspeed: 0.1132s/iter; left time: 3450.2868s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0432666\n",
      "\tspeed: 0.1121s/iter; left time: 3405.0745s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0467523\n",
      "\tspeed: 0.1126s/iter; left time: 3408.0894s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0596244\n",
      "\tspeed: 0.1132s/iter; left time: 3416.5046s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0576644\n",
      "\tspeed: 0.1129s/iter; left time: 3394.4302s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0496884\n",
      "\tspeed: 0.1102s/iter; left time: 3301.5614s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0494345\n",
      "\tspeed: 0.1091s/iter; left time: 3259.4055s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0386619\n",
      "\tspeed: 0.1149s/iter; left time: 3420.6504s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0411887\n",
      "\tspeed: 0.1115s/iter; left time: 3308.3598s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0443211\n",
      "\tspeed: 0.1138s/iter; left time: 3365.7200s\n",
      "Epoch: 9 cost time: 00h:05m:03.14s\n",
      "Epoch: 9 | Train Loss: 0.0506318 Vali Loss: 0.0546863 Test Loss: 0.0599749\n",
      "Validation loss decreased (0.055073 --> 0.054686).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0549749\n",
      "\tspeed: 1.0393s/iter; left time: 30545.8957s\n",
      "\titers: 200, epoch: 10 | loss: 0.0383406\n",
      "\tspeed: 0.1121s/iter; left time: 3282.3379s\n",
      "\titers: 300, epoch: 10 | loss: 0.0472804\n",
      "\tspeed: 0.1125s/iter; left time: 3283.6315s\n",
      "\titers: 400, epoch: 10 | loss: 0.0422506\n",
      "\tspeed: 0.1118s/iter; left time: 3253.6303s\n",
      "\titers: 500, epoch: 10 | loss: 0.0550114\n",
      "\tspeed: 0.1137s/iter; left time: 3296.0525s\n",
      "\titers: 600, epoch: 10 | loss: 0.0441338\n",
      "\tspeed: 0.1127s/iter; left time: 3255.9567s\n",
      "\titers: 700, epoch: 10 | loss: 0.0540485\n",
      "\tspeed: 0.1117s/iter; left time: 3215.9544s\n",
      "\titers: 800, epoch: 10 | loss: 0.0546766\n",
      "\tspeed: 0.1133s/iter; left time: 3252.0772s\n",
      "\titers: 900, epoch: 10 | loss: 0.0499431\n",
      "\tspeed: 0.1119s/iter; left time: 3198.8956s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0570361\n",
      "\tspeed: 0.1122s/iter; left time: 3195.4844s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0438565\n",
      "\tspeed: 0.1123s/iter; left time: 3188.3446s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0453070\n",
      "\tspeed: 0.1106s/iter; left time: 3127.9442s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0541585\n",
      "\tspeed: 0.1130s/iter; left time: 3186.5914s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0488800\n",
      "\tspeed: 0.1114s/iter; left time: 3129.6275s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0653748\n",
      "\tspeed: 0.1134s/iter; left time: 3174.8546s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0513718\n",
      "\tspeed: 0.1116s/iter; left time: 3113.5575s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0519211\n",
      "\tspeed: 0.1117s/iter; left time: 3103.9028s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0510913\n",
      "\tspeed: 0.1127s/iter; left time: 3121.0756s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0787238\n",
      "\tspeed: 0.1153s/iter; left time: 3181.8783s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0443497\n",
      "\tspeed: 0.1139s/iter; left time: 3130.7918s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0386278\n",
      "\tspeed: 0.1122s/iter; left time: 3073.5192s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0553838\n",
      "\tspeed: 0.1118s/iter; left time: 3051.9252s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0513475\n",
      "\tspeed: 0.1139s/iter; left time: 3097.5671s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0396648\n",
      "\tspeed: 0.1116s/iter; left time: 3024.2607s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0409211\n",
      "\tspeed: 0.1113s/iter; left time: 3004.2385s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0425374\n",
      "\tspeed: 0.1118s/iter; left time: 3007.5790s\n",
      "Epoch: 10 cost time: 00h:05m:02.03s\n",
      "Epoch: 10 | Train Loss: 0.0501928 Vali Loss: 0.0552204 Test Loss: 0.0600623\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0620002\n",
      "\tspeed: 1.0251s/iter; left time: 27382.6088s\n",
      "\titers: 200, epoch: 11 | loss: 0.0374523\n",
      "\tspeed: 0.1119s/iter; left time: 2977.7641s\n",
      "\titers: 300, epoch: 11 | loss: 0.0529428\n",
      "\tspeed: 0.1136s/iter; left time: 3011.8567s\n",
      "\titers: 400, epoch: 11 | loss: 0.0386790\n",
      "\tspeed: 0.1130s/iter; left time: 2983.6250s\n",
      "\titers: 500, epoch: 11 | loss: 0.0355124\n",
      "\tspeed: 0.1119s/iter; left time: 2943.1978s\n",
      "\titers: 600, epoch: 11 | loss: 0.0515568\n",
      "\tspeed: 0.1151s/iter; left time: 3017.2496s\n",
      "\titers: 700, epoch: 11 | loss: 0.0656706\n",
      "\tspeed: 0.1176s/iter; left time: 3071.1696s\n",
      "\titers: 800, epoch: 11 | loss: 0.0524726\n",
      "\tspeed: 0.1126s/iter; left time: 2929.5851s\n",
      "\titers: 900, epoch: 11 | loss: 0.0677181\n",
      "\tspeed: 0.1138s/iter; left time: 2949.0683s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0548737\n",
      "\tspeed: 0.1144s/iter; left time: 2951.5983s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0569288\n",
      "\tspeed: 0.1127s/iter; left time: 2898.6880s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0609900\n",
      "\tspeed: 0.1115s/iter; left time: 2856.0894s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0399404\n",
      "\tspeed: 0.1144s/iter; left time: 2918.1807s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0586750\n",
      "\tspeed: 0.1136s/iter; left time: 2886.1428s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0501058\n",
      "\tspeed: 0.1107s/iter; left time: 2800.8886s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0411009\n",
      "\tspeed: 0.1105s/iter; left time: 2785.1531s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0643648\n",
      "\tspeed: 0.1137s/iter; left time: 2854.6794s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0525917\n",
      "\tspeed: 0.1141s/iter; left time: 2854.7075s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0451737\n",
      "\tspeed: 0.1122s/iter; left time: 2794.4115s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0579281\n",
      "\tspeed: 0.1119s/iter; left time: 2777.5283s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0540055\n",
      "\tspeed: 0.1122s/iter; left time: 2771.3486s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0435182\n",
      "\tspeed: 0.1105s/iter; left time: 2718.7439s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0541128\n",
      "\tspeed: 0.1131s/iter; left time: 2771.3440s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0472108\n",
      "\tspeed: 0.1135s/iter; left time: 2771.5390s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0526714\n",
      "\tspeed: 0.1147s/iter; left time: 2789.4014s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0492443\n",
      "\tspeed: 0.1118s/iter; left time: 2706.8650s\n",
      "Epoch: 11 cost time: 00h:05m:03.58s\n",
      "Epoch: 11 | Train Loss: 0.0496507 Vali Loss: 0.0557215 Test Loss: 0.0603047\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0461969\n",
      "\tspeed: 1.0286s/iter; left time: 24718.1822s\n",
      "\titers: 200, epoch: 12 | loss: 0.0482488\n",
      "\tspeed: 0.1127s/iter; left time: 2697.5579s\n",
      "\titers: 300, epoch: 12 | loss: 0.0463038\n",
      "\tspeed: 0.1140s/iter; left time: 2716.9514s\n",
      "\titers: 400, epoch: 12 | loss: 0.0601207\n",
      "\tspeed: 0.1123s/iter; left time: 2663.8539s\n",
      "\titers: 500, epoch: 12 | loss: 0.0524425\n",
      "\tspeed: 0.1156s/iter; left time: 2731.6391s\n",
      "\titers: 600, epoch: 12 | loss: 0.0494985\n",
      "\tspeed: 0.1158s/iter; left time: 2724.5077s\n",
      "\titers: 700, epoch: 12 | loss: 0.0530444\n",
      "\tspeed: 0.1140s/iter; left time: 2671.1297s\n",
      "\titers: 800, epoch: 12 | loss: 0.0502007\n",
      "\tspeed: 0.1148s/iter; left time: 2677.7831s\n",
      "\titers: 900, epoch: 12 | loss: 0.0386652\n",
      "\tspeed: 0.1154s/iter; left time: 2679.8984s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0524950\n",
      "\tspeed: 0.1140s/iter; left time: 2637.0591s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0360246\n",
      "\tspeed: 0.1154s/iter; left time: 2657.0871s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0573246\n",
      "\tspeed: 0.1153s/iter; left time: 2643.9142s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0448311\n",
      "\tspeed: 0.1121s/iter; left time: 2558.4108s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0468765\n",
      "\tspeed: 0.1130s/iter; left time: 2568.5121s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0427486\n",
      "\tspeed: 0.1146s/iter; left time: 2592.7664s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0422873\n",
      "\tspeed: 0.1144s/iter; left time: 2576.6648s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0424657\n",
      "\tspeed: 0.1082s/iter; left time: 2427.6676s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0512158\n",
      "\tspeed: 0.1091s/iter; left time: 2436.9783s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0482876\n",
      "\tspeed: 0.1118s/iter; left time: 2485.1582s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0403065\n",
      "\tspeed: 0.1119s/iter; left time: 2476.3739s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0512401\n",
      "\tspeed: 0.1121s/iter; left time: 2470.0594s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0528152\n",
      "\tspeed: 0.1136s/iter; left time: 2490.4916s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0479994\n",
      "\tspeed: 0.1122s/iter; left time: 2448.9373s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0547172\n",
      "\tspeed: 0.1116s/iter; left time: 2426.1544s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0471118\n",
      "\tspeed: 0.1123s/iter; left time: 2429.9498s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0501922\n",
      "\tspeed: 0.1138s/iter; left time: 2449.2207s\n",
      "Epoch: 12 cost time: 00h:05m:03.90s\n",
      "Epoch: 12 | Train Loss: 0.0491253 Vali Loss: 0.0558346 Test Loss: 0.0616492\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0454289\n",
      "\tspeed: 1.0255s/iter; left time: 21893.8210s\n",
      "\titers: 200, epoch: 13 | loss: 0.0566784\n",
      "\tspeed: 0.1133s/iter; left time: 2406.8231s\n",
      "\titers: 300, epoch: 13 | loss: 0.0492385\n",
      "\tspeed: 0.1141s/iter; left time: 2413.8053s\n",
      "\titers: 400, epoch: 13 | loss: 0.0413138\n",
      "\tspeed: 0.1131s/iter; left time: 2380.9215s\n",
      "\titers: 500, epoch: 13 | loss: 0.0430361\n",
      "\tspeed: 0.1147s/iter; left time: 2401.8599s\n",
      "\titers: 600, epoch: 13 | loss: 0.0513702\n",
      "\tspeed: 0.1137s/iter; left time: 2370.2280s\n",
      "\titers: 700, epoch: 13 | loss: 0.0502950\n",
      "\tspeed: 0.1152s/iter; left time: 2391.1957s\n",
      "\titers: 800, epoch: 13 | loss: 0.0534758\n",
      "\tspeed: 0.1164s/iter; left time: 2402.5526s\n",
      "\titers: 900, epoch: 13 | loss: 0.0518013\n",
      "\tspeed: 0.1142s/iter; left time: 2346.3902s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0583335\n",
      "\tspeed: 0.1140s/iter; left time: 2331.8613s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0650049\n",
      "\tspeed: 0.1123s/iter; left time: 2285.9362s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0528312\n",
      "\tspeed: 0.1121s/iter; left time: 2270.7913s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0473740\n",
      "\tspeed: 0.1127s/iter; left time: 2270.3789s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0452519\n",
      "\tspeed: 0.1119s/iter; left time: 2243.0886s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0424276\n",
      "\tspeed: 0.1122s/iter; left time: 2237.7048s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0521983\n",
      "\tspeed: 0.1140s/iter; left time: 2262.3137s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0471075\n",
      "\tspeed: 0.1110s/iter; left time: 2192.8490s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0435151\n",
      "\tspeed: 0.1114s/iter; left time: 2188.5153s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0494255\n",
      "\tspeed: 0.1151s/iter; left time: 2249.1457s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0469670\n",
      "\tspeed: 0.1146s/iter; left time: 2228.1475s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0472251\n",
      "\tspeed: 0.1128s/iter; left time: 2183.4210s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0486188\n",
      "\tspeed: 0.1151s/iter; left time: 2215.8062s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0509845\n",
      "\tspeed: 0.1136s/iter; left time: 2175.1362s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0457661\n",
      "\tspeed: 0.1142s/iter; left time: 2174.7222s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0508229\n",
      "\tspeed: 0.1117s/iter; left time: 2116.8378s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0558406\n",
      "\tspeed: 0.1130s/iter; left time: 2129.6985s\n",
      "Epoch: 13 cost time: 00h:05m:04.57s\n",
      "Epoch: 13 | Train Loss: 0.0487104 Vali Loss: 0.0579919 Test Loss: 0.0634963\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0413323\n",
      "\tspeed: 1.0277s/iter; left time: 19185.7892s\n",
      "\titers: 200, epoch: 14 | loss: 0.0421818\n",
      "\tspeed: 0.1115s/iter; left time: 2070.8073s\n",
      "\titers: 300, epoch: 14 | loss: 0.0630368\n",
      "\tspeed: 0.1124s/iter; left time: 2076.6077s\n",
      "\titers: 400, epoch: 14 | loss: 0.0455646\n",
      "\tspeed: 0.1125s/iter; left time: 2067.1773s\n",
      "\titers: 500, epoch: 14 | loss: 0.0373397\n",
      "\tspeed: 0.1103s/iter; left time: 2014.1091s\n",
      "\titers: 600, epoch: 14 | loss: 0.0462459\n",
      "\tspeed: 0.1112s/iter; left time: 2020.9447s\n",
      "\titers: 700, epoch: 14 | loss: 0.0535143\n",
      "\tspeed: 0.1137s/iter; left time: 2054.4473s\n",
      "\titers: 800, epoch: 14 | loss: 0.0559032\n",
      "\tspeed: 0.1129s/iter; left time: 2028.6877s\n",
      "\titers: 900, epoch: 14 | loss: 0.0622907\n",
      "\tspeed: 0.1120s/iter; left time: 2000.8022s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0430358\n",
      "\tspeed: 0.1112s/iter; left time: 1975.0108s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0412004\n",
      "\tspeed: 0.1117s/iter; left time: 1974.1785s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0447963\n",
      "\tspeed: 0.1132s/iter; left time: 1988.1058s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0473538\n",
      "\tspeed: 0.1146s/iter; left time: 2002.2731s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0410066\n",
      "\tspeed: 0.1133s/iter; left time: 1967.6612s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0531902\n",
      "\tspeed: 0.1126s/iter; left time: 1945.0240s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0443050\n",
      "\tspeed: 0.1142s/iter; left time: 1960.8417s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0529488\n",
      "\tspeed: 0.1144s/iter; left time: 1953.0810s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0568460\n",
      "\tspeed: 0.1163s/iter; left time: 1973.1694s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0442665\n",
      "\tspeed: 0.1132s/iter; left time: 1908.8723s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0497327\n",
      "\tspeed: 0.1127s/iter; left time: 1889.8253s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0499501\n",
      "\tspeed: 0.1154s/iter; left time: 1922.7709s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0507595\n",
      "\tspeed: 0.1127s/iter; left time: 1867.6123s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0538237\n",
      "\tspeed: 0.1122s/iter; left time: 1848.2104s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0460961\n",
      "\tspeed: 0.1136s/iter; left time: 1859.4444s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0627327\n",
      "\tspeed: 0.1098s/iter; left time: 1786.1661s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0460155\n",
      "\tspeed: 0.1111s/iter; left time: 1796.8610s\n",
      "Epoch: 14 cost time: 00h:05m:02.79s\n",
      "Epoch: 14 | Train Loss: 0.0482707 Vali Loss: 0.0567447 Test Loss: 0.0618993\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.011070186272263527, rmse:0.10521495342254639, mae:0.059974875301122665, rse:0.40591442584991455\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 24: 01h:32m:37.52s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 15:57:56,806] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 15:57:57,848] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 15:57:57,848] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 15:57:57,848] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 15:57:57,978] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 15:57:57,979] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 15:57:58,762] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 15:57:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 15:57:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 15:57:58,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 15:57:58,766] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 15:57:58,766] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 15:57:59,171] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 15:57:59,172] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.58 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,319] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 15:57:59,320] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,320] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.63 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,320] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 15:57:59,489] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 15:57:59,490] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,490] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.67 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 15:57:59,492] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5a92bc61d0>\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 15:57:59,496] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1506957\n",
      "\tspeed: 0.1707s/iter; left time: 9113.0167s\n",
      "\titers: 200, epoch: 1 | loss: 0.1479850\n",
      "\tspeed: 0.1272s/iter; left time: 6776.0640s\n",
      "\titers: 300, epoch: 1 | loss: 0.1137763\n",
      "\tspeed: 0.1266s/iter; left time: 6732.0131s\n",
      "\titers: 400, epoch: 1 | loss: 0.1076533\n",
      "\tspeed: 0.1272s/iter; left time: 6750.7454s\n",
      "\titers: 500, epoch: 1 | loss: 0.0889011\n",
      "\tspeed: 0.1260s/iter; left time: 6677.0404s\n",
      "\titers: 600, epoch: 1 | loss: 0.0769084\n",
      "\tspeed: 0.1259s/iter; left time: 6658.3469s\n",
      "\titers: 700, epoch: 1 | loss: 0.0896481\n",
      "\tspeed: 0.1268s/iter; left time: 6693.0274s\n",
      "\titers: 800, epoch: 1 | loss: 0.0794844\n",
      "\tspeed: 0.1264s/iter; left time: 6658.0271s\n",
      "\titers: 900, epoch: 1 | loss: 0.0815863\n",
      "\tspeed: 0.1259s/iter; left time: 6619.0603s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0779093\n",
      "\tspeed: 0.1268s/iter; left time: 6653.2277s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0663487\n",
      "\tspeed: 0.1268s/iter; left time: 6639.5392s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0627032\n",
      "\tspeed: 0.1270s/iter; left time: 6641.1303s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0696527\n",
      "\tspeed: 0.1250s/iter; left time: 6525.2010s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0784859\n",
      "\tspeed: 0.1281s/iter; left time: 6671.6439s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0762833\n",
      "\tspeed: 0.1253s/iter; left time: 6512.1729s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0648971\n",
      "\tspeed: 0.1261s/iter; left time: 6542.8828s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801568\n",
      "\tspeed: 0.1264s/iter; left time: 6544.9091s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0775711\n",
      "\tspeed: 0.1235s/iter; left time: 6383.1070s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0935053\n",
      "\tspeed: 0.1275s/iter; left time: 6577.2110s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0565456\n",
      "\tspeed: 0.1257s/iter; left time: 6472.9286s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0903167\n",
      "\tspeed: 0.1253s/iter; left time: 6438.8915s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0752582\n",
      "\tspeed: 0.1283s/iter; left time: 6581.6070s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0761704\n",
      "\tspeed: 0.1280s/iter; left time: 6552.6275s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0646984\n",
      "\tspeed: 0.1251s/iter; left time: 6390.5272s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0703335\n",
      "\tspeed: 0.1260s/iter; left time: 6425.0778s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0783555\n",
      "\tspeed: 0.1272s/iter; left time: 6472.8357s\n",
      "Epoch: 1 cost time: 00h:05m:39.18s\n",
      "Epoch: 1 | Train Loss: 0.0856850 Vali Loss: 0.0756439 Test Loss: 0.0843909\n",
      "Validation loss decreased (inf --> 0.075644).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0820017\n",
      "\tspeed: 1.1856s/iter; left time: 60116.7690s\n",
      "\titers: 200, epoch: 2 | loss: 0.0687233\n",
      "\tspeed: 0.1123s/iter; left time: 5684.0124s\n",
      "\titers: 300, epoch: 2 | loss: 0.0772514\n",
      "\tspeed: 0.1136s/iter; left time: 5735.1321s\n",
      "\titers: 400, epoch: 2 | loss: 0.0848139\n",
      "\tspeed: 0.1172s/iter; left time: 5908.0864s\n",
      "\titers: 500, epoch: 2 | loss: 0.0686562\n",
      "\tspeed: 0.1156s/iter; left time: 5817.7403s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745008\n",
      "\tspeed: 0.1163s/iter; left time: 5841.3102s\n",
      "\titers: 700, epoch: 2 | loss: 0.0605222\n",
      "\tspeed: 0.1161s/iter; left time: 5817.0332s\n",
      "\titers: 800, epoch: 2 | loss: 0.0737291\n",
      "\tspeed: 0.1160s/iter; left time: 5800.6933s\n",
      "\titers: 900, epoch: 2 | loss: 0.0731786\n",
      "\tspeed: 0.1150s/iter; left time: 5737.7970s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0705629\n",
      "\tspeed: 0.1147s/iter; left time: 5714.8002s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0684758\n",
      "\tspeed: 0.1153s/iter; left time: 5733.4341s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017207\n",
      "\tspeed: 0.1130s/iter; left time: 5603.5418s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0608158\n",
      "\tspeed: 0.1151s/iter; left time: 5699.1413s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0733602\n",
      "\tspeed: 0.1129s/iter; left time: 5576.6890s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0823769\n",
      "\tspeed: 0.1142s/iter; left time: 5632.0723s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0670033\n",
      "\tspeed: 0.1133s/iter; left time: 5574.8166s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0635963\n",
      "\tspeed: 0.1143s/iter; left time: 5613.5456s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0702989\n",
      "\tspeed: 0.1162s/iter; left time: 5693.8001s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0682929\n",
      "\tspeed: 0.1155s/iter; left time: 5650.0819s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0600829\n",
      "\tspeed: 0.1143s/iter; left time: 5577.4614s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0654664\n",
      "\tspeed: 0.1160s/iter; left time: 5650.9377s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0597576\n",
      "\tspeed: 0.1135s/iter; left time: 5518.1442s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0667420\n",
      "\tspeed: 0.1160s/iter; left time: 5629.2045s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0765030\n",
      "\tspeed: 0.1169s/iter; left time: 5656.5137s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0623692\n",
      "\tspeed: 0.1144s/iter; left time: 5527.0673s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0587819\n",
      "\tspeed: 0.1134s/iter; left time: 5467.1744s\n",
      "Epoch: 2 cost time: 00h:05m:07.74s\n",
      "Epoch: 2 | Train Loss: 0.0710048 Vali Loss: 0.0745253 Test Loss: 0.0838230\n",
      "Validation loss decreased (0.075644 --> 0.074525).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0658879\n",
      "\tspeed: 1.0551s/iter; left time: 50679.4805s\n",
      "\titers: 200, epoch: 3 | loss: 0.0615703\n",
      "\tspeed: 0.1147s/iter; left time: 5498.9434s\n",
      "\titers: 300, epoch: 3 | loss: 0.0726208\n",
      "\tspeed: 0.1124s/iter; left time: 5374.3410s\n",
      "\titers: 400, epoch: 3 | loss: 0.0762616\n",
      "\tspeed: 0.1152s/iter; left time: 5497.1977s\n",
      "\titers: 500, epoch: 3 | loss: 0.0660659\n",
      "\tspeed: 0.1135s/iter; left time: 5404.0749s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651958\n",
      "\tspeed: 0.1157s/iter; left time: 5500.9620s\n",
      "\titers: 700, epoch: 3 | loss: 0.0897581\n",
      "\tspeed: 0.1144s/iter; left time: 5425.0195s\n",
      "\titers: 800, epoch: 3 | loss: 0.0652855\n",
      "\tspeed: 0.1165s/iter; left time: 5513.4883s\n",
      "\titers: 900, epoch: 3 | loss: 0.0668169\n",
      "\tspeed: 0.1150s/iter; left time: 5429.7491s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0626059\n",
      "\tspeed: 0.1127s/iter; left time: 5313.1327s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0549380\n",
      "\tspeed: 0.1151s/iter; left time: 5414.6272s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0706437\n",
      "\tspeed: 0.1127s/iter; left time: 5287.4627s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0740227\n",
      "\tspeed: 0.1168s/iter; left time: 5471.6214s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0664061\n",
      "\tspeed: 0.1158s/iter; left time: 5410.8156s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0757374\n",
      "\tspeed: 0.1141s/iter; left time: 5320.4736s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0855467\n",
      "\tspeed: 0.1139s/iter; left time: 5301.0619s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0756636\n",
      "\tspeed: 0.1169s/iter; left time: 5428.4163s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0538661\n",
      "\tspeed: 0.1141s/iter; left time: 5286.8755s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0783621\n",
      "\tspeed: 0.1174s/iter; left time: 5426.1842s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0645575\n",
      "\tspeed: 0.1167s/iter; left time: 5384.5331s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0941539\n",
      "\tspeed: 0.1144s/iter; left time: 5266.8994s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0742499\n",
      "\tspeed: 0.1152s/iter; left time: 5292.9174s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0604884\n",
      "\tspeed: 0.1164s/iter; left time: 5336.3567s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0683079\n",
      "\tspeed: 0.1133s/iter; left time: 5182.0659s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0758141\n",
      "\tspeed: 0.1135s/iter; left time: 5177.5742s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0615954\n",
      "\tspeed: 0.1145s/iter; left time: 5213.3850s\n",
      "Epoch: 3 cost time: 00h:05m:07.65s\n",
      "Epoch: 3 | Train Loss: 0.0687785 Vali Loss: 0.0725997 Test Loss: 0.0817405\n",
      "Validation loss decreased (0.074525 --> 0.072600).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0635042\n",
      "\tspeed: 1.0342s/iter; left time: 46908.5323s\n",
      "\titers: 200, epoch: 4 | loss: 0.0766480\n",
      "\tspeed: 0.1169s/iter; left time: 5290.6569s\n",
      "\titers: 300, epoch: 4 | loss: 0.0603194\n",
      "\tspeed: 0.1149s/iter; left time: 5190.9310s\n",
      "\titers: 400, epoch: 4 | loss: 0.0879604\n",
      "\tspeed: 0.1153s/iter; left time: 5196.5941s\n",
      "\titers: 500, epoch: 4 | loss: 0.0716096\n",
      "\tspeed: 0.1173s/iter; left time: 5274.9817s\n",
      "\titers: 600, epoch: 4 | loss: 0.0661810\n",
      "\tspeed: 0.1123s/iter; left time: 5036.5761s\n",
      "\titers: 700, epoch: 4 | loss: 0.0630168\n",
      "\tspeed: 0.1146s/iter; left time: 5130.1413s\n",
      "\titers: 800, epoch: 4 | loss: 0.0750177\n",
      "\tspeed: 0.1139s/iter; left time: 5085.6809s\n",
      "\titers: 900, epoch: 4 | loss: 0.0560872\n",
      "\tspeed: 0.1120s/iter; left time: 4991.7414s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0561997\n",
      "\tspeed: 0.1138s/iter; left time: 5061.1343s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0634589\n",
      "\tspeed: 0.1102s/iter; left time: 4889.6764s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0602584\n",
      "\tspeed: 0.1117s/iter; left time: 4944.1367s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0809753\n",
      "\tspeed: 0.1122s/iter; left time: 4952.7185s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0696282\n",
      "\tspeed: 0.1121s/iter; left time: 4940.2120s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0582918\n",
      "\tspeed: 0.1145s/iter; left time: 5032.7009s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0587403\n",
      "\tspeed: 0.1132s/iter; left time: 4966.3179s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0575944\n",
      "\tspeed: 0.1112s/iter; left time: 4867.8195s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0667402\n",
      "\tspeed: 0.1137s/iter; left time: 4962.0644s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0664581\n",
      "\tspeed: 0.1148s/iter; left time: 5002.5965s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0745811\n",
      "\tspeed: 0.1129s/iter; left time: 4907.8576s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0695823\n",
      "\tspeed: 0.1111s/iter; left time: 4817.2379s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0542657\n",
      "\tspeed: 0.1129s/iter; left time: 4882.6145s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0670997\n",
      "\tspeed: 0.1105s/iter; left time: 4770.8912s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0595331\n",
      "\tspeed: 0.1125s/iter; left time: 4843.8712s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0590853\n",
      "\tspeed: 0.1134s/iter; left time: 4869.7242s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0622148\n",
      "\tspeed: 0.1115s/iter; left time: 4778.2491s\n",
      "Epoch: 4 cost time: 00h:05m:03.21s\n",
      "Epoch: 4 | Train Loss: 0.0669450 Vali Loss: 0.0741470 Test Loss: 0.0824094\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0655506\n",
      "\tspeed: 1.0279s/iter; left time: 43876.3073s\n",
      "\titers: 200, epoch: 5 | loss: 0.0827420\n",
      "\tspeed: 0.1139s/iter; left time: 4849.6463s\n",
      "\titers: 300, epoch: 5 | loss: 0.0619815\n",
      "\tspeed: 0.1149s/iter; left time: 4883.1241s\n",
      "\titers: 400, epoch: 5 | loss: 0.0519550\n",
      "\tspeed: 0.1118s/iter; left time: 4738.8475s\n",
      "\titers: 500, epoch: 5 | loss: 0.0678731\n",
      "\tspeed: 0.1125s/iter; left time: 4758.4126s\n",
      "\titers: 600, epoch: 5 | loss: 0.0686385\n",
      "\tspeed: 0.1133s/iter; left time: 4778.2636s\n",
      "\titers: 700, epoch: 5 | loss: 0.0715512\n",
      "\tspeed: 0.1117s/iter; left time: 4700.8591s\n",
      "\titers: 800, epoch: 5 | loss: 0.0702961\n",
      "\tspeed: 0.1136s/iter; left time: 4770.0126s\n",
      "\titers: 900, epoch: 5 | loss: 0.0748347\n",
      "\tspeed: 0.1141s/iter; left time: 4778.6170s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0546004\n",
      "\tspeed: 0.1121s/iter; left time: 4682.2282s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0699323\n",
      "\tspeed: 0.1169s/iter; left time: 4871.2229s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0556661\n",
      "\tspeed: 0.1123s/iter; left time: 4670.5481s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0645581\n",
      "\tspeed: 0.1148s/iter; left time: 4764.2947s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0688608\n",
      "\tspeed: 0.1150s/iter; left time: 4758.5611s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0485171\n",
      "\tspeed: 0.1118s/iter; left time: 4615.0795s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0664087\n",
      "\tspeed: 0.1126s/iter; left time: 4638.3682s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0548556\n",
      "\tspeed: 0.1134s/iter; left time: 4659.2041s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0654618\n",
      "\tspeed: 0.1146s/iter; left time: 4695.4469s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0673945\n",
      "\tspeed: 0.1149s/iter; left time: 4697.6498s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0469344\n",
      "\tspeed: 0.1129s/iter; left time: 4602.5989s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0672865\n",
      "\tspeed: 0.1125s/iter; left time: 4578.5228s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0703256\n",
      "\tspeed: 0.1153s/iter; left time: 4679.2620s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0843584\n",
      "\tspeed: 0.1133s/iter; left time: 4586.4834s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0603486\n",
      "\tspeed: 0.1109s/iter; left time: 4477.8991s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761832\n",
      "\tspeed: 0.1141s/iter; left time: 4597.2399s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0514573\n",
      "\tspeed: 0.1127s/iter; left time: 4528.0888s\n",
      "Epoch: 5 cost time: 00h:05m:03.59s\n",
      "Epoch: 5 | Train Loss: 0.0649131 Vali Loss: 0.0754854 Test Loss: 0.0844793\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0773710\n",
      "\tspeed: 1.0069s/iter; left time: 40285.6555s\n",
      "\titers: 200, epoch: 6 | loss: 0.0613844\n",
      "\tspeed: 0.1138s/iter; left time: 4542.1712s\n",
      "\titers: 300, epoch: 6 | loss: 0.0576228\n",
      "\tspeed: 0.1128s/iter; left time: 4491.8380s\n",
      "\titers: 400, epoch: 6 | loss: 0.0596324\n",
      "\tspeed: 0.1127s/iter; left time: 4476.8733s\n",
      "\titers: 500, epoch: 6 | loss: 0.0579083\n",
      "\tspeed: 0.1122s/iter; left time: 4442.8573s\n",
      "\titers: 600, epoch: 6 | loss: 0.0726682\n",
      "\tspeed: 0.1135s/iter; left time: 4482.6344s\n",
      "\titers: 700, epoch: 6 | loss: 0.0726198\n",
      "\tspeed: 0.1146s/iter; left time: 4516.1612s\n",
      "\titers: 800, epoch: 6 | loss: 0.0645957\n",
      "\tspeed: 0.1146s/iter; left time: 4504.1012s\n",
      "\titers: 900, epoch: 6 | loss: 0.0725977\n",
      "\tspeed: 0.1156s/iter; left time: 4531.2633s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0638610\n",
      "\tspeed: 0.1151s/iter; left time: 4501.1788s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0556347\n",
      "\tspeed: 0.1140s/iter; left time: 4446.7623s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0472695\n",
      "\tspeed: 0.1152s/iter; left time: 4481.4860s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0792933\n",
      "\tspeed: 0.1131s/iter; left time: 4389.8681s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0675426\n",
      "\tspeed: 0.1117s/iter; left time: 4322.6630s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0533914\n",
      "\tspeed: 0.1132s/iter; left time: 4371.7503s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0623400\n",
      "\tspeed: 0.1111s/iter; left time: 4278.8181s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0586607\n",
      "\tspeed: 0.1123s/iter; left time: 4311.9919s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0620784\n",
      "\tspeed: 0.1118s/iter; left time: 4283.5127s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0596254\n",
      "\tspeed: 0.1134s/iter; left time: 4334.5241s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0587260\n",
      "\tspeed: 0.1135s/iter; left time: 4326.0426s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0641739\n",
      "\tspeed: 0.1125s/iter; left time: 4277.7994s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0735683\n",
      "\tspeed: 0.1132s/iter; left time: 4293.3688s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0535805\n",
      "\tspeed: 0.1139s/iter; left time: 4307.2052s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0671832\n",
      "\tspeed: 0.1129s/iter; left time: 4257.8680s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0640754\n",
      "\tspeed: 0.1123s/iter; left time: 4224.7128s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0475231\n",
      "\tspeed: 0.1124s/iter; left time: 4216.1819s\n",
      "Epoch: 6 cost time: 00h:05m:03.11s\n",
      "Epoch: 6 | Train Loss: 0.0630431 Vali Loss: 0.0751659 Test Loss: 0.0838745\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0737136\n",
      "\tspeed: 1.0189s/iter; left time: 38040.9426s\n",
      "\titers: 200, epoch: 7 | loss: 0.0773198\n",
      "\tspeed: 0.1144s/iter; left time: 4260.6138s\n",
      "\titers: 300, epoch: 7 | loss: 0.0562561\n",
      "\tspeed: 0.1137s/iter; left time: 4224.2310s\n",
      "\titers: 400, epoch: 7 | loss: 0.0674500\n",
      "\tspeed: 0.1141s/iter; left time: 4224.7118s\n",
      "\titers: 500, epoch: 7 | loss: 0.0538294\n",
      "\tspeed: 0.1153s/iter; left time: 4259.2230s\n",
      "\titers: 600, epoch: 7 | loss: 0.0683504\n",
      "\tspeed: 0.1155s/iter; left time: 4254.6560s\n",
      "\titers: 700, epoch: 7 | loss: 0.0571279\n",
      "\tspeed: 0.1155s/iter; left time: 4244.5785s\n",
      "\titers: 800, epoch: 7 | loss: 0.0532890\n",
      "\tspeed: 0.1145s/iter; left time: 4193.3060s\n",
      "\titers: 900, epoch: 7 | loss: 0.0591477\n",
      "\tspeed: 0.1146s/iter; left time: 4187.5830s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0772009\n",
      "\tspeed: 0.1166s/iter; left time: 4248.5963s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0556193\n",
      "\tspeed: 0.1131s/iter; left time: 4110.9963s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0560169\n",
      "\tspeed: 0.1131s/iter; left time: 4097.1813s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0597722\n",
      "\tspeed: 0.1163s/iter; left time: 4204.4354s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0604149\n",
      "\tspeed: 0.1141s/iter; left time: 4111.2249s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0788255\n",
      "\tspeed: 0.1133s/iter; left time: 4070.2651s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0704609\n",
      "\tspeed: 0.1124s/iter; left time: 4028.6042s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0629530\n",
      "\tspeed: 0.1124s/iter; left time: 4015.4502s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0610598\n",
      "\tspeed: 0.1121s/iter; left time: 3994.9430s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0446289\n",
      "\tspeed: 0.1151s/iter; left time: 4089.8119s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0594098\n",
      "\tspeed: 0.1126s/iter; left time: 3991.0313s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0708012\n",
      "\tspeed: 0.1122s/iter; left time: 3965.9981s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0720370\n",
      "\tspeed: 0.1143s/iter; left time: 4026.7334s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0789985\n",
      "\tspeed: 0.1111s/iter; left time: 3903.4207s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0545131\n",
      "\tspeed: 0.1114s/iter; left time: 3902.6692s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0678101\n",
      "\tspeed: 0.1142s/iter; left time: 3990.9587s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0619310\n",
      "\tspeed: 0.1112s/iter; left time: 3873.8694s\n",
      "Epoch: 7 cost time: 00h:05m:04.62s\n",
      "Epoch: 7 | Train Loss: 0.0613680 Vali Loss: 0.0761664 Test Loss: 0.0845503\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0591804\n",
      "\tspeed: 1.0174s/iter; left time: 35265.2915s\n",
      "\titers: 200, epoch: 8 | loss: 0.0601895\n",
      "\tspeed: 0.1139s/iter; left time: 3936.2679s\n",
      "\titers: 300, epoch: 8 | loss: 0.0537779\n",
      "\tspeed: 0.1147s/iter; left time: 3951.2009s\n",
      "\titers: 400, epoch: 8 | loss: 0.0596471\n",
      "\tspeed: 0.1150s/iter; left time: 3952.8149s\n",
      "\titers: 500, epoch: 8 | loss: 0.0748389\n",
      "\tspeed: 0.1152s/iter; left time: 3946.7594s\n",
      "\titers: 600, epoch: 8 | loss: 0.0619526\n",
      "\tspeed: 0.1142s/iter; left time: 3900.6351s\n",
      "\titers: 700, epoch: 8 | loss: 0.0492099\n",
      "\tspeed: 0.1158s/iter; left time: 3943.2424s\n",
      "\titers: 800, epoch: 8 | loss: 0.0553380\n",
      "\tspeed: 0.1146s/iter; left time: 3893.2388s\n",
      "\titers: 900, epoch: 8 | loss: 0.0660264\n",
      "\tspeed: 0.1124s/iter; left time: 3804.8857s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0512587\n",
      "\tspeed: 0.1160s/iter; left time: 3916.6093s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0606477\n",
      "\tspeed: 0.1154s/iter; left time: 3885.3929s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0488810\n",
      "\tspeed: 0.1150s/iter; left time: 3860.4352s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0636803\n",
      "\tspeed: 0.1152s/iter; left time: 3854.7517s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0603122\n",
      "\tspeed: 0.1152s/iter; left time: 3843.0386s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0753196\n",
      "\tspeed: 0.1143s/iter; left time: 3802.7447s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0476781\n",
      "\tspeed: 0.1152s/iter; left time: 3820.4807s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0543786\n",
      "\tspeed: 0.1175s/iter; left time: 3885.4586s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0479689\n",
      "\tspeed: 0.1144s/iter; left time: 3772.0123s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0567113\n",
      "\tspeed: 0.1152s/iter; left time: 3786.4991s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0584764\n",
      "\tspeed: 0.1148s/iter; left time: 3762.3402s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0674268\n",
      "\tspeed: 0.1149s/iter; left time: 3753.6997s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0617867\n",
      "\tspeed: 0.1140s/iter; left time: 3711.0942s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0588714\n",
      "\tspeed: 0.1135s/iter; left time: 3684.6010s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0716897\n",
      "\tspeed: 0.1161s/iter; left time: 3756.1475s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0649153\n",
      "\tspeed: 0.1134s/iter; left time: 3658.8856s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0538609\n",
      "\tspeed: 0.1148s/iter; left time: 3691.1308s\n",
      "Epoch: 8 cost time: 00h:05m:07.49s\n",
      "Epoch: 8 | Train Loss: 0.0601105 Vali Loss: 0.0785164 Test Loss: 0.0872316\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018459701910614967, rmse:0.13586649298667908, mae:0.08174050599336624, rse:0.5256097912788391\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 96: 00h:53m:37.48s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 16:51:34,589] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 16:51:35,763] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 16:51:35,764] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 16:51:35,764] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 16:51:35,879] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 16:51:35,879] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 16:51:36,608] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 16:51:36,609] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 16:51:36,609] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 16:51:36,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 16:51:36,611] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 16:51:36,611] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 16:51:37,007] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 16:51:37,008] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 312.97 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,202] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 16:51:37,203] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,203] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 313.01 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,203] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 16:51:37,377] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 16:51:37,378] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,379] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 313.01 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd374ba9ed0>\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1461806\n",
      "\tspeed: 0.1728s/iter; left time: 9201.6091s\n",
      "\titers: 200, epoch: 1 | loss: 0.1501896\n",
      "\tspeed: 0.1258s/iter; left time: 6685.5717s\n",
      "\titers: 300, epoch: 1 | loss: 0.1255645\n",
      "\tspeed: 0.1265s/iter; left time: 6707.1471s\n",
      "\titers: 400, epoch: 1 | loss: 0.0948358\n",
      "\tspeed: 0.1274s/iter; left time: 6742.0429s\n",
      "\titers: 500, epoch: 1 | loss: 0.0860671\n",
      "\tspeed: 0.1254s/iter; left time: 6627.6459s\n",
      "\titers: 600, epoch: 1 | loss: 0.0797759\n",
      "\tspeed: 0.1268s/iter; left time: 6686.3868s\n",
      "\titers: 700, epoch: 1 | loss: 0.0779733\n",
      "\tspeed: 0.1257s/iter; left time: 6618.0314s\n",
      "\titers: 800, epoch: 1 | loss: 0.0854992\n",
      "\tspeed: 0.1223s/iter; left time: 6426.5638s\n",
      "\titers: 900, epoch: 1 | loss: 0.0824629\n",
      "\tspeed: 0.1239s/iter; left time: 6498.8014s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0905522\n",
      "\tspeed: 0.1266s/iter; left time: 6625.9316s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0848468\n",
      "\tspeed: 0.1262s/iter; left time: 6591.7380s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0831938\n",
      "\tspeed: 0.1253s/iter; left time: 6531.3858s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0840276\n",
      "\tspeed: 0.1238s/iter; left time: 6443.5033s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0715065\n",
      "\tspeed: 0.1244s/iter; left time: 6459.2688s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0868700\n",
      "\tspeed: 0.1237s/iter; left time: 6412.8650s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0816183\n",
      "\tspeed: 0.1259s/iter; left time: 6516.6899s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0746674\n",
      "\tspeed: 0.1258s/iter; left time: 6496.2027s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0842547\n",
      "\tspeed: 0.1277s/iter; left time: 6581.8055s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0750883\n",
      "\tspeed: 0.1244s/iter; left time: 6396.8053s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0767707\n",
      "\tspeed: 0.1264s/iter; left time: 6489.2698s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0823143\n",
      "\tspeed: 0.1248s/iter; left time: 6394.4898s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0773541\n",
      "\tspeed: 0.1260s/iter; left time: 6441.5222s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0822798\n",
      "\tspeed: 0.1244s/iter; left time: 6351.4653s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0697680\n",
      "\tspeed: 0.1264s/iter; left time: 6440.4329s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0781708\n",
      "\tspeed: 0.1239s/iter; left time: 6299.2218s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0923201\n",
      "\tspeed: 0.1256s/iter; left time: 6372.9236s\n",
      "Epoch: 1 cost time: 00h:05m:35.64s\n",
      "Epoch: 1 | Train Loss: 0.0882067 Vali Loss: 0.0789939 Test Loss: 0.0885378\n",
      "Validation loss decreased (inf --> 0.078994).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0682366\n",
      "\tspeed: 1.1571s/iter; left time: 58517.2971s\n",
      "\titers: 200, epoch: 2 | loss: 0.0756340\n",
      "\tspeed: 0.1126s/iter; left time: 5682.0940s\n",
      "\titers: 300, epoch: 2 | loss: 0.0757687\n",
      "\tspeed: 0.1163s/iter; left time: 5856.8647s\n",
      "\titers: 400, epoch: 2 | loss: 0.0809062\n",
      "\tspeed: 0.1147s/iter; left time: 5765.0768s\n",
      "\titers: 500, epoch: 2 | loss: 0.0785947\n",
      "\tspeed: 0.1158s/iter; left time: 5810.3476s\n",
      "\titers: 600, epoch: 2 | loss: 0.0788290\n",
      "\tspeed: 0.1148s/iter; left time: 5748.4740s\n",
      "\titers: 700, epoch: 2 | loss: 0.0643081\n",
      "\tspeed: 0.1140s/iter; left time: 5699.4764s\n",
      "\titers: 800, epoch: 2 | loss: 0.0724190\n",
      "\tspeed: 0.1153s/iter; left time: 5749.9291s\n",
      "\titers: 900, epoch: 2 | loss: 0.0765032\n",
      "\tspeed: 0.1134s/iter; left time: 5645.3463s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0680958\n",
      "\tspeed: 0.1127s/iter; left time: 5596.1018s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0636572\n",
      "\tspeed: 0.1140s/iter; left time: 5650.9959s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0647527\n",
      "\tspeed: 0.1138s/iter; left time: 5628.4425s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0695712\n",
      "\tspeed: 0.1125s/iter; left time: 5556.3360s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0781222\n",
      "\tspeed: 0.1108s/iter; left time: 5459.3519s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0884119\n",
      "\tspeed: 0.1148s/iter; left time: 5646.0649s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0732782\n",
      "\tspeed: 0.1128s/iter; left time: 5537.6323s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0834294\n",
      "\tspeed: 0.1125s/iter; left time: 5510.1069s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0762573\n",
      "\tspeed: 0.1135s/iter; left time: 5545.8452s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0638730\n",
      "\tspeed: 0.1122s/iter; left time: 5473.9790s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0719824\n",
      "\tspeed: 0.1118s/iter; left time: 5440.0921s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0811448\n",
      "\tspeed: 0.1130s/iter; left time: 5487.5219s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0702735\n",
      "\tspeed: 0.1139s/iter; left time: 5522.9308s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0598425\n",
      "\tspeed: 0.1125s/iter; left time: 5441.5109s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0801101\n",
      "\tspeed: 0.1135s/iter; left time: 5481.4252s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0596308\n",
      "\tspeed: 0.1117s/iter; left time: 5383.4181s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0712510\n",
      "\tspeed: 0.1133s/iter; left time: 5448.0658s\n",
      "Epoch: 2 cost time: 00h:05m:03.32s\n",
      "Epoch: 2 | Train Loss: 0.0743254 Vali Loss: 0.0777038 Test Loss: 0.0871744\n",
      "Validation loss decreased (0.078994 --> 0.077704).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0675267\n",
      "\tspeed: 1.0175s/iter; left time: 48745.3355s\n",
      "\titers: 200, epoch: 3 | loss: 0.0488273\n",
      "\tspeed: 0.1142s/iter; left time: 5457.8495s\n",
      "\titers: 300, epoch: 3 | loss: 0.0692624\n",
      "\tspeed: 0.1129s/iter; left time: 5385.2999s\n",
      "\titers: 400, epoch: 3 | loss: 0.0781940\n",
      "\tspeed: 0.1140s/iter; left time: 5425.0041s\n",
      "\titers: 500, epoch: 3 | loss: 0.0633163\n",
      "\tspeed: 0.1146s/iter; left time: 5446.1456s\n",
      "\titers: 600, epoch: 3 | loss: 0.0786292\n",
      "\tspeed: 0.1144s/iter; left time: 5422.7847s\n",
      "\titers: 700, epoch: 3 | loss: 0.0660528\n",
      "\tspeed: 0.1128s/iter; left time: 5334.5753s\n",
      "\titers: 800, epoch: 3 | loss: 0.0873496\n",
      "\tspeed: 0.1138s/iter; left time: 5373.4053s\n",
      "\titers: 900, epoch: 3 | loss: 0.0665047\n",
      "\tspeed: 0.1121s/iter; left time: 5280.7909s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0717470\n",
      "\tspeed: 0.1128s/iter; left time: 5303.9321s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0673361\n",
      "\tspeed: 0.1147s/iter; left time: 5382.1805s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0878783\n",
      "\tspeed: 0.1125s/iter; left time: 5267.7362s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0672138\n",
      "\tspeed: 0.1147s/iter; left time: 5357.9900s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0739039\n",
      "\tspeed: 0.1151s/iter; left time: 5365.0421s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0644442\n",
      "\tspeed: 0.1117s/iter; left time: 5195.0276s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0913087\n",
      "\tspeed: 0.1122s/iter; left time: 5206.8303s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0859381\n",
      "\tspeed: 0.1127s/iter; left time: 5219.3783s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0756321\n",
      "\tspeed: 0.1105s/iter; left time: 5107.9640s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0888247\n",
      "\tspeed: 0.1094s/iter; left time: 5043.7622s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0595378\n",
      "\tspeed: 0.1115s/iter; left time: 5127.8967s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0935414\n",
      "\tspeed: 0.1130s/iter; left time: 5188.4238s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0669725\n",
      "\tspeed: 0.1131s/iter; left time: 5178.5974s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0605008\n",
      "\tspeed: 0.1144s/iter; left time: 5227.8167s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0831451\n",
      "\tspeed: 0.1111s/iter; left time: 5066.6465s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0697700\n",
      "\tspeed: 0.1123s/iter; left time: 5112.1039s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0661712\n",
      "\tspeed: 0.1122s/iter; left time: 5095.2971s\n",
      "Epoch: 3 cost time: 00h:05m:01.88s\n",
      "Epoch: 3 | Train Loss: 0.0718358 Vali Loss: 0.0789278 Test Loss: 0.0876987\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0648829\n",
      "\tspeed: 0.9919s/iter; left time: 44872.7863s\n",
      "\titers: 200, epoch: 4 | loss: 0.0765537\n",
      "\tspeed: 0.1145s/iter; left time: 5167.3367s\n",
      "\titers: 300, epoch: 4 | loss: 0.0551690\n",
      "\tspeed: 0.1123s/iter; left time: 5059.4413s\n",
      "\titers: 400, epoch: 4 | loss: 0.0743216\n",
      "\tspeed: 0.1145s/iter; left time: 5144.3220s\n",
      "\titers: 500, epoch: 4 | loss: 0.0566139\n",
      "\tspeed: 0.1147s/iter; left time: 5143.2865s\n",
      "\titers: 600, epoch: 4 | loss: 0.0790194\n",
      "\tspeed: 0.1133s/iter; left time: 5069.6998s\n",
      "\titers: 700, epoch: 4 | loss: 0.0843454\n",
      "\tspeed: 0.1173s/iter; left time: 5234.2535s\n",
      "\titers: 800, epoch: 4 | loss: 0.0670473\n",
      "\tspeed: 0.1154s/iter; left time: 5138.4053s\n",
      "\titers: 900, epoch: 4 | loss: 0.0606182\n",
      "\tspeed: 0.1156s/iter; left time: 5136.3097s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0734311\n",
      "\tspeed: 0.1161s/iter; left time: 5148.5885s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0760170\n",
      "\tspeed: 0.1164s/iter; left time: 5150.3318s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0643949\n",
      "\tspeed: 0.1147s/iter; left time: 5062.8959s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0647174\n",
      "\tspeed: 0.1145s/iter; left time: 5043.0254s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0552365\n",
      "\tspeed: 0.1156s/iter; left time: 5077.9105s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0723340\n",
      "\tspeed: 0.1150s/iter; left time: 5040.2806s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0671190\n",
      "\tspeed: 0.1131s/iter; left time: 4948.4789s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0635535\n",
      "\tspeed: 0.1122s/iter; left time: 4895.0363s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0668017\n",
      "\tspeed: 0.1162s/iter; left time: 5058.7632s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0685256\n",
      "\tspeed: 0.1150s/iter; left time: 4994.0688s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0721530\n",
      "\tspeed: 0.1153s/iter; left time: 4996.1174s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0882780\n",
      "\tspeed: 0.1124s/iter; left time: 4859.0463s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0493336\n",
      "\tspeed: 0.1148s/iter; left time: 4954.4873s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0750259\n",
      "\tspeed: 0.1153s/iter; left time: 4964.6474s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0622560\n",
      "\tspeed: 0.1117s/iter; left time: 4798.4092s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0759433\n",
      "\tspeed: 0.1141s/iter; left time: 4887.5694s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0722388\n",
      "\tspeed: 0.1123s/iter; left time: 4800.1817s\n",
      "Epoch: 4 cost time: 00h:05m:05.82s\n",
      "Epoch: 4 | Train Loss: 0.0692130 Vali Loss: 0.0806310 Test Loss: 0.0891718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0739205\n",
      "\tspeed: 1.0042s/iter; left time: 42753.4276s\n",
      "\titers: 200, epoch: 5 | loss: 0.0671281\n",
      "\tspeed: 0.1143s/iter; left time: 4853.5783s\n",
      "\titers: 300, epoch: 5 | loss: 0.0831495\n",
      "\tspeed: 0.1165s/iter; left time: 4937.0348s\n",
      "\titers: 400, epoch: 5 | loss: 0.0562803\n",
      "\tspeed: 0.1164s/iter; left time: 4918.9635s\n",
      "\titers: 500, epoch: 5 | loss: 0.0612629\n",
      "\tspeed: 0.1151s/iter; left time: 4855.1824s\n",
      "\titers: 600, epoch: 5 | loss: 0.0725645\n",
      "\tspeed: 0.1160s/iter; left time: 4880.8346s\n",
      "\titers: 700, epoch: 5 | loss: 0.0736510\n",
      "\tspeed: 0.1155s/iter; left time: 4846.2801s\n",
      "\titers: 800, epoch: 5 | loss: 0.0621689\n",
      "\tspeed: 0.1132s/iter; left time: 4738.0259s\n",
      "\titers: 900, epoch: 5 | loss: 0.0703253\n",
      "\tspeed: 0.1148s/iter; left time: 4794.1539s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0583030\n",
      "\tspeed: 0.1140s/iter; left time: 4751.3104s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0703888\n",
      "\tspeed: 0.1148s/iter; left time: 4771.1987s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0585797\n",
      "\tspeed: 0.1134s/iter; left time: 4704.2690s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0742435\n",
      "\tspeed: 0.1126s/iter; left time: 4659.8960s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0719956\n",
      "\tspeed: 0.1149s/iter; left time: 4742.0219s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0606002\n",
      "\tspeed: 0.1151s/iter; left time: 4737.2574s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0692182\n",
      "\tspeed: 0.1123s/iter; left time: 4611.1768s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0654579\n",
      "\tspeed: 0.1119s/iter; left time: 4585.1680s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0808130\n",
      "\tspeed: 0.1136s/iter; left time: 4642.1484s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0635292\n",
      "\tspeed: 0.1130s/iter; left time: 4606.2225s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0533435\n",
      "\tspeed: 0.1117s/iter; left time: 4544.1873s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0679128\n",
      "\tspeed: 0.1130s/iter; left time: 4586.4621s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0734313\n",
      "\tspeed: 0.1126s/iter; left time: 4556.3801s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0613091\n",
      "\tspeed: 0.1141s/iter; left time: 4607.2712s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0723060\n",
      "\tspeed: 0.1129s/iter; left time: 4547.2335s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0571138\n",
      "\tspeed: 0.1128s/iter; left time: 4530.7935s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0643532\n",
      "\tspeed: 0.1128s/iter; left time: 4520.1549s\n",
      "Epoch: 5 cost time: 00h:05m:04.35s\n",
      "Epoch: 5 | Train Loss: 0.0666898 Vali Loss: 0.0816376 Test Loss: 0.0911526\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0592945\n",
      "\tspeed: 0.9876s/iter; left time: 39411.1732s\n",
      "\titers: 200, epoch: 6 | loss: 0.0701825\n",
      "\tspeed: 0.1144s/iter; left time: 4555.2524s\n",
      "\titers: 300, epoch: 6 | loss: 0.0733728\n",
      "\tspeed: 0.1133s/iter; left time: 4498.4615s\n",
      "\titers: 400, epoch: 6 | loss: 0.0500985\n",
      "\tspeed: 0.1121s/iter; left time: 4440.6552s\n",
      "\titers: 500, epoch: 6 | loss: 0.0708840\n",
      "\tspeed: 0.1141s/iter; left time: 4508.6746s\n",
      "\titers: 600, epoch: 6 | loss: 0.0746798\n",
      "\tspeed: 0.1151s/iter; left time: 4534.9810s\n",
      "\titers: 700, epoch: 6 | loss: 0.0672090\n",
      "\tspeed: 0.1126s/iter; left time: 4426.7414s\n",
      "\titers: 800, epoch: 6 | loss: 0.0675267\n",
      "\tspeed: 0.1126s/iter; left time: 4416.2322s\n",
      "\titers: 900, epoch: 6 | loss: 0.0650174\n",
      "\tspeed: 0.1131s/iter; left time: 4422.3891s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0685931\n",
      "\tspeed: 0.1135s/iter; left time: 4427.0639s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0609383\n",
      "\tspeed: 0.1119s/iter; left time: 4351.8524s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0727255\n",
      "\tspeed: 0.1125s/iter; left time: 4365.8285s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0623817\n",
      "\tspeed: 0.1120s/iter; left time: 4335.5399s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603095\n",
      "\tspeed: 0.1135s/iter; left time: 4380.0029s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0660724\n",
      "\tspeed: 0.1131s/iter; left time: 4353.1481s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0712819\n",
      "\tspeed: 0.1116s/iter; left time: 4285.8897s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0682647\n",
      "\tspeed: 0.1127s/iter; left time: 4318.5119s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0655491\n",
      "\tspeed: 0.1132s/iter; left time: 4325.5446s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0615094\n",
      "\tspeed: 0.1136s/iter; left time: 4329.7846s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0686113\n",
      "\tspeed: 0.1129s/iter; left time: 4292.6018s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0684695\n",
      "\tspeed: 0.1131s/iter; left time: 4286.8611s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0678513\n",
      "\tspeed: 0.1119s/iter; left time: 4230.2983s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0722934\n",
      "\tspeed: 0.1124s/iter; left time: 4237.9501s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0518117\n",
      "\tspeed: 0.1115s/iter; left time: 4194.3563s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0615523\n",
      "\tspeed: 0.1132s/iter; left time: 4244.6177s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0523729\n",
      "\tspeed: 0.1137s/iter; left time: 4252.3758s\n",
      "Epoch: 6 cost time: 00h:05m:01.92s\n",
      "Epoch: 6 | Train Loss: 0.0646321 Vali Loss: 0.0833404 Test Loss: 0.0923264\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0626126\n",
      "\tspeed: 0.9871s/iter; left time: 36759.9347s\n",
      "\titers: 200, epoch: 7 | loss: 0.0635089\n",
      "\tspeed: 0.1141s/iter; left time: 4239.2426s\n",
      "\titers: 300, epoch: 7 | loss: 0.0645225\n",
      "\tspeed: 0.1116s/iter; left time: 4132.2544s\n",
      "\titers: 400, epoch: 7 | loss: 0.0587169\n",
      "\tspeed: 0.1136s/iter; left time: 4194.6714s\n",
      "\titers: 500, epoch: 7 | loss: 0.0679099\n",
      "\tspeed: 0.1126s/iter; left time: 4147.4079s\n",
      "\titers: 600, epoch: 7 | loss: 0.0761093\n",
      "\tspeed: 0.1128s/iter; left time: 4143.3336s\n",
      "\titers: 700, epoch: 7 | loss: 0.0614073\n",
      "\tspeed: 0.1146s/iter; left time: 4200.4443s\n",
      "\titers: 800, epoch: 7 | loss: 0.0606054\n",
      "\tspeed: 0.1138s/iter; left time: 4156.8729s\n",
      "\titers: 900, epoch: 7 | loss: 0.0640666\n",
      "\tspeed: 0.1133s/iter; left time: 4128.3271s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0652940\n",
      "\tspeed: 0.1120s/iter; left time: 4068.4261s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0591778\n",
      "\tspeed: 0.1137s/iter; left time: 4122.0008s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0599352\n",
      "\tspeed: 0.1132s/iter; left time: 4091.2681s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0674185\n",
      "\tspeed: 0.1136s/iter; left time: 4094.0902s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0645608\n",
      "\tspeed: 0.1127s/iter; left time: 4048.6741s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0626351\n",
      "\tspeed: 0.1126s/iter; left time: 4036.2563s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0564335\n",
      "\tspeed: 0.1140s/iter; left time: 4075.1212s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0628764\n",
      "\tspeed: 0.1123s/iter; left time: 4002.8423s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0579112\n",
      "\tspeed: 0.1133s/iter; left time: 4025.9566s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0576657\n",
      "\tspeed: 0.1140s/iter; left time: 4039.2995s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0645774\n",
      "\tspeed: 0.1120s/iter; left time: 3958.6573s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0611573\n",
      "\tspeed: 0.1133s/iter; left time: 3991.7090s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0668616\n",
      "\tspeed: 0.1108s/iter; left time: 3891.7385s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0692711\n",
      "\tspeed: 0.1121s/iter; left time: 3927.3236s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0704450\n",
      "\tspeed: 0.1122s/iter; left time: 3920.1121s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0594643\n",
      "\tspeed: 0.1145s/iter; left time: 3987.4141s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0567257\n",
      "\tspeed: 0.1143s/iter; left time: 3971.3102s\n",
      "Epoch: 7 cost time: 00h:05m:01.84s\n",
      "Epoch: 7 | Train Loss: 0.0629600 Vali Loss: 0.0832727 Test Loss: 0.0938695\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02040581777691841, rmse:0.142848938703537, mae:0.08717440813779831, rse:0.5534131526947021\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 168: 00h:46m:39.29s\n",
      "\n",
      "Intermediate time for FR: 03h:12m:54.28s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 17:38:13,775] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 17:38:14,961] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 17:38:14,962] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 17:38:14,962] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 17:38:15,055] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 17:38:15,055] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 17:38:15,742] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 17:38:15,744] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 17:38:15,744] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 17:38:15,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 17:38:15,745] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 17:38:15,745] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 17:38:16,138] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 17:38:16,141] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,141] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.71 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,321] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 17:38:16,322] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,322] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.71 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,322] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 17:38:16,520] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 17:38:16,521] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,521] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.6 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7e6cfcd050>\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1961632\n",
      "\tspeed: 0.1721s/iter; left time: 9212.2875s\n",
      "\titers: 200, epoch: 1 | loss: 0.1897899\n",
      "\tspeed: 0.1240s/iter; left time: 6625.8756s\n",
      "\titers: 300, epoch: 1 | loss: 0.1400119\n",
      "\tspeed: 0.1264s/iter; left time: 6740.8285s\n",
      "\titers: 400, epoch: 1 | loss: 0.1414553\n",
      "\tspeed: 0.1249s/iter; left time: 6646.2304s\n",
      "\titers: 500, epoch: 1 | loss: 0.1245781\n",
      "\tspeed: 0.1244s/iter; left time: 6608.3626s\n",
      "\titers: 600, epoch: 1 | loss: 0.0947767\n",
      "\tspeed: 0.1283s/iter; left time: 6802.8842s\n",
      "\titers: 700, epoch: 1 | loss: 0.0984841\n",
      "\tspeed: 0.1260s/iter; left time: 6670.1009s\n",
      "\titers: 800, epoch: 1 | loss: 0.0828382\n",
      "\tspeed: 0.1247s/iter; left time: 6585.5102s\n",
      "\titers: 900, epoch: 1 | loss: 0.0719742\n",
      "\tspeed: 0.1284s/iter; left time: 6768.0768s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1195307\n",
      "\tspeed: 0.1332s/iter; left time: 7008.2822s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0817368\n",
      "\tspeed: 0.1278s/iter; left time: 6709.7699s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0921901\n",
      "\tspeed: 0.1293s/iter; left time: 6778.1289s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0989637\n",
      "\tspeed: 0.1268s/iter; left time: 6632.0273s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0795155\n",
      "\tspeed: 0.1257s/iter; left time: 6565.8398s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0898348\n",
      "\tspeed: 0.1275s/iter; left time: 6645.7793s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0705952\n",
      "\tspeed: 0.1250s/iter; left time: 6502.1500s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0890884\n",
      "\tspeed: 0.1274s/iter; left time: 6616.2715s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0708546\n",
      "\tspeed: 0.1278s/iter; left time: 6621.3609s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0823215\n",
      "\tspeed: 0.1279s/iter; left time: 6614.7988s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0903587\n",
      "\tspeed: 0.1271s/iter; left time: 6558.6651s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0667403\n",
      "\tspeed: 0.1263s/iter; left time: 6506.3228s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0840566\n",
      "\tspeed: 0.1270s/iter; left time: 6530.3758s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1007551\n",
      "\tspeed: 0.1270s/iter; left time: 6517.5440s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0791763\n",
      "\tspeed: 0.1250s/iter; left time: 6404.0763s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0936417\n",
      "\tspeed: 0.1246s/iter; left time: 6368.7051s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0681735\n",
      "\tspeed: 0.1263s/iter; left time: 6441.6549s\n",
      "Epoch: 1 cost time: 00h:05m:40.50s\n",
      "Epoch: 1 | Train Loss: 0.1042742 Vali Loss: 0.0697548 Test Loss: 0.0724680\n",
      "Validation loss decreased (inf --> 0.069755).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0968200\n",
      "\tspeed: 1.1964s/iter; left time: 60823.4643s\n",
      "\titers: 200, epoch: 2 | loss: 0.0908526\n",
      "\tspeed: 0.1135s/iter; left time: 5759.7286s\n",
      "\titers: 300, epoch: 2 | loss: 0.0880430\n",
      "\tspeed: 0.1132s/iter; left time: 5733.4311s\n",
      "\titers: 400, epoch: 2 | loss: 0.0909087\n",
      "\tspeed: 0.1148s/iter; left time: 5800.0278s\n",
      "\titers: 500, epoch: 2 | loss: 0.0755832\n",
      "\tspeed: 0.1150s/iter; left time: 5800.8973s\n",
      "\titers: 600, epoch: 2 | loss: 0.0719677\n",
      "\tspeed: 0.1130s/iter; left time: 5690.1754s\n",
      "\titers: 700, epoch: 2 | loss: 0.0632761\n",
      "\tspeed: 0.1161s/iter; left time: 5833.7360s\n",
      "\titers: 800, epoch: 2 | loss: 0.0799158\n",
      "\tspeed: 0.1135s/iter; left time: 5689.4282s\n",
      "\titers: 900, epoch: 2 | loss: 0.0778257\n",
      "\tspeed: 0.1120s/iter; left time: 5604.9770s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0686750\n",
      "\tspeed: 0.1139s/iter; left time: 5689.7352s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0759580\n",
      "\tspeed: 0.1121s/iter; left time: 5589.1426s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0850438\n",
      "\tspeed: 0.1132s/iter; left time: 5628.8998s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0768398\n",
      "\tspeed: 0.1126s/iter; left time: 5591.4223s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0734043\n",
      "\tspeed: 0.1126s/iter; left time: 5576.2087s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0702756\n",
      "\tspeed: 0.1134s/iter; left time: 5608.4505s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0901470\n",
      "\tspeed: 0.1134s/iter; left time: 5595.7804s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0736003\n",
      "\tspeed: 0.1132s/iter; left time: 5574.8807s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0812662\n",
      "\tspeed: 0.1141s/iter; left time: 5607.1686s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0856911\n",
      "\tspeed: 0.1117s/iter; left time: 5479.8548s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0710515\n",
      "\tspeed: 0.1123s/iter; left time: 5494.9948s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0731606\n",
      "\tspeed: 0.1108s/iter; left time: 5413.7384s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0908238\n",
      "\tspeed: 0.1137s/iter; left time: 5541.7092s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0716156\n",
      "\tspeed: 0.1145s/iter; left time: 5569.4548s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0662761\n",
      "\tspeed: 0.1124s/iter; left time: 5454.4864s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0705364\n",
      "\tspeed: 0.1146s/iter; left time: 5552.5707s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0787798\n",
      "\tspeed: 0.1157s/iter; left time: 5591.1339s\n",
      "Epoch: 2 cost time: 00h:05m:04.83s\n",
      "Epoch: 2 | Train Loss: 0.0792366 Vali Loss: 0.0630193 Test Loss: 0.0664905\n",
      "Validation loss decreased (0.069755 --> 0.063019).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0837417\n",
      "\tspeed: 1.0566s/iter; left time: 50887.1933s\n",
      "\titers: 200, epoch: 3 | loss: 0.0720257\n",
      "\tspeed: 0.1153s/iter; left time: 5539.1675s\n",
      "\titers: 300, epoch: 3 | loss: 0.0743371\n",
      "\tspeed: 0.1143s/iter; left time: 5483.9913s\n",
      "\titers: 400, epoch: 3 | loss: 0.0783527\n",
      "\tspeed: 0.1124s/iter; left time: 5378.7126s\n",
      "\titers: 500, epoch: 3 | loss: 0.0714736\n",
      "\tspeed: 0.1124s/iter; left time: 5367.3301s\n",
      "\titers: 600, epoch: 3 | loss: 0.0732643\n",
      "\tspeed: 0.1158s/iter; left time: 5517.5317s\n",
      "\titers: 700, epoch: 3 | loss: 0.0646055\n",
      "\tspeed: 0.1127s/iter; left time: 5357.9279s\n",
      "\titers: 800, epoch: 3 | loss: 0.0739734\n",
      "\tspeed: 0.1117s/iter; left time: 5300.4545s\n",
      "\titers: 900, epoch: 3 | loss: 0.0694846\n",
      "\tspeed: 0.1164s/iter; left time: 5513.7908s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0750011\n",
      "\tspeed: 0.1145s/iter; left time: 5409.1384s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0742698\n",
      "\tspeed: 0.1131s/iter; left time: 5331.3288s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0718982\n",
      "\tspeed: 0.1133s/iter; left time: 5332.9434s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0686082\n",
      "\tspeed: 0.1150s/iter; left time: 5398.8493s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0775685\n",
      "\tspeed: 0.1141s/iter; left time: 5346.2458s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0676080\n",
      "\tspeed: 0.1129s/iter; left time: 5281.1198s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0703222\n",
      "\tspeed: 0.1147s/iter; left time: 5351.9548s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0706295\n",
      "\tspeed: 0.1164s/iter; left time: 5419.2978s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0611054\n",
      "\tspeed: 0.1123s/iter; left time: 5216.7241s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0822397\n",
      "\tspeed: 0.1122s/iter; left time: 5203.7787s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0630606\n",
      "\tspeed: 0.1133s/iter; left time: 5243.2936s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0664413\n",
      "\tspeed: 0.1144s/iter; left time: 5280.1509s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0754516\n",
      "\tspeed: 0.1126s/iter; left time: 5183.9776s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0628932\n",
      "\tspeed: 0.1117s/iter; left time: 5135.4868s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0689445\n",
      "\tspeed: 0.1132s/iter; left time: 5193.1520s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0753874\n",
      "\tspeed: 0.1143s/iter; left time: 5231.9445s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0803154\n",
      "\tspeed: 0.1117s/iter; left time: 5100.1547s\n",
      "Epoch: 3 cost time: 00h:05m:05.16s\n",
      "Epoch: 3 | Train Loss: 0.0754979 Vali Loss: 0.0626933 Test Loss: 0.0662443\n",
      "Validation loss decreased (0.063019 --> 0.062693).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0701278\n",
      "\tspeed: 1.0424s/iter; left time: 47406.5237s\n",
      "\titers: 200, epoch: 4 | loss: 0.0728249\n",
      "\tspeed: 0.1145s/iter; left time: 5197.6080s\n",
      "\titers: 300, epoch: 4 | loss: 0.0778461\n",
      "\tspeed: 0.1130s/iter; left time: 5117.2262s\n",
      "\titers: 400, epoch: 4 | loss: 0.0785829\n",
      "\tspeed: 0.1132s/iter; left time: 5116.0930s\n",
      "\titers: 500, epoch: 4 | loss: 0.0790459\n",
      "\tspeed: 0.1153s/iter; left time: 5197.8971s\n",
      "\titers: 600, epoch: 4 | loss: 0.0710924\n",
      "\tspeed: 0.1143s/iter; left time: 5139.3973s\n",
      "\titers: 700, epoch: 4 | loss: 0.0721118\n",
      "\tspeed: 0.1152s/iter; left time: 5171.2332s\n",
      "\titers: 800, epoch: 4 | loss: 0.0836879\n",
      "\tspeed: 0.1165s/iter; left time: 5217.0912s\n",
      "\titers: 900, epoch: 4 | loss: 0.0670347\n",
      "\tspeed: 0.1127s/iter; left time: 5037.3830s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0732893\n",
      "\tspeed: 0.1135s/iter; left time: 5058.3337s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0734157\n",
      "\tspeed: 0.1130s/iter; left time: 5026.3469s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0836176\n",
      "\tspeed: 0.1125s/iter; left time: 4993.3019s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0914069\n",
      "\tspeed: 0.1140s/iter; left time: 5049.6116s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0554709\n",
      "\tspeed: 0.1130s/iter; left time: 4994.0839s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0638680\n",
      "\tspeed: 0.1121s/iter; left time: 4940.7801s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0843564\n",
      "\tspeed: 0.1153s/iter; left time: 5072.0975s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0867873\n",
      "\tspeed: 0.1154s/iter; left time: 5062.8955s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0825667\n",
      "\tspeed: 0.1119s/iter; left time: 4899.0202s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0808909\n",
      "\tspeed: 0.1123s/iter; left time: 4903.3557s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0671010\n",
      "\tspeed: 0.1135s/iter; left time: 4947.8210s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0800280\n",
      "\tspeed: 0.1121s/iter; left time: 4873.7499s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0652930\n",
      "\tspeed: 0.1110s/iter; left time: 4816.3385s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0597176\n",
      "\tspeed: 0.1110s/iter; left time: 4805.4423s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0868424\n",
      "\tspeed: 0.1124s/iter; left time: 4852.0543s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0782121\n",
      "\tspeed: 0.1127s/iter; left time: 4856.6246s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0802435\n",
      "\tspeed: 0.1120s/iter; left time: 4812.7082s\n",
      "Epoch: 4 cost time: 00h:05m:04.23s\n",
      "Epoch: 4 | Train Loss: 0.0732161 Vali Loss: 0.0602605 Test Loss: 0.0639531\n",
      "Validation loss decreased (0.062693 --> 0.060260).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0794265\n",
      "\tspeed: 1.0445s/iter; left time: 44703.2173s\n",
      "\titers: 200, epoch: 5 | loss: 0.0705508\n",
      "\tspeed: 0.1145s/iter; left time: 4889.2588s\n",
      "\titers: 300, epoch: 5 | loss: 0.0829213\n",
      "\tspeed: 0.1137s/iter; left time: 4841.8229s\n",
      "\titers: 400, epoch: 5 | loss: 0.0772983\n",
      "\tspeed: 0.1103s/iter; left time: 4687.0188s\n",
      "\titers: 500, epoch: 5 | loss: 0.0697670\n",
      "\tspeed: 0.1129s/iter; left time: 4788.1117s\n",
      "\titers: 600, epoch: 5 | loss: 0.0615391\n",
      "\tspeed: 0.1130s/iter; left time: 4779.3203s\n",
      "\titers: 700, epoch: 5 | loss: 0.0567918\n",
      "\tspeed: 0.1124s/iter; left time: 4742.9584s\n",
      "\titers: 800, epoch: 5 | loss: 0.0847099\n",
      "\tspeed: 0.1118s/iter; left time: 4707.3078s\n",
      "\titers: 900, epoch: 5 | loss: 0.0704188\n",
      "\tspeed: 0.1122s/iter; left time: 4713.4398s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0620770\n",
      "\tspeed: 0.1124s/iter; left time: 4708.3377s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0741380\n",
      "\tspeed: 0.1131s/iter; left time: 4725.7189s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0661858\n",
      "\tspeed: 0.1152s/iter; left time: 4802.3252s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0621480\n",
      "\tspeed: 0.1118s/iter; left time: 4652.4158s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0629700\n",
      "\tspeed: 0.1152s/iter; left time: 4779.4661s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0820492\n",
      "\tspeed: 0.1127s/iter; left time: 4665.4697s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0610860\n",
      "\tspeed: 0.1122s/iter; left time: 4633.4398s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0861114\n",
      "\tspeed: 0.1111s/iter; left time: 4575.4531s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0606930\n",
      "\tspeed: 0.1111s/iter; left time: 4567.4049s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0612833\n",
      "\tspeed: 0.1121s/iter; left time: 4597.4820s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0786318\n",
      "\tspeed: 0.1110s/iter; left time: 4539.6443s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0779789\n",
      "\tspeed: 0.1123s/iter; left time: 4580.0710s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0667817\n",
      "\tspeed: 0.1111s/iter; left time: 4523.4657s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0773585\n",
      "\tspeed: 0.1128s/iter; left time: 4581.3611s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0830474\n",
      "\tspeed: 0.1119s/iter; left time: 4533.0059s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0766403\n",
      "\tspeed: 0.1110s/iter; left time: 4485.0244s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0594169\n",
      "\tspeed: 0.1122s/iter; left time: 4521.0120s\n",
      "Epoch: 5 cost time: 00h:05m:01.95s\n",
      "Epoch: 5 | Train Loss: 0.0718611 Vali Loss: 0.0608224 Test Loss: 0.0644853\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0779639\n",
      "\tspeed: 1.0313s/iter; left time: 41373.1934s\n",
      "\titers: 200, epoch: 6 | loss: 0.0661459\n",
      "\tspeed: 0.1129s/iter; left time: 4519.0567s\n",
      "\titers: 300, epoch: 6 | loss: 0.0925239\n",
      "\tspeed: 0.1145s/iter; left time: 4568.8113s\n",
      "\titers: 400, epoch: 6 | loss: 0.0681258\n",
      "\tspeed: 0.1124s/iter; left time: 4473.6384s\n",
      "\titers: 500, epoch: 6 | loss: 0.0705238\n",
      "\tspeed: 0.1128s/iter; left time: 4479.2027s\n",
      "\titers: 600, epoch: 6 | loss: 0.0755477\n",
      "\tspeed: 0.1134s/iter; left time: 4494.4193s\n",
      "\titers: 700, epoch: 6 | loss: 0.0716280\n",
      "\tspeed: 0.1112s/iter; left time: 4395.6017s\n",
      "\titers: 800, epoch: 6 | loss: 0.0619959\n",
      "\tspeed: 0.1134s/iter; left time: 4469.7612s\n",
      "\titers: 900, epoch: 6 | loss: 0.0714130\n",
      "\tspeed: 0.1125s/iter; left time: 4421.3718s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0737989\n",
      "\tspeed: 0.1145s/iter; left time: 4490.0219s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0621136\n",
      "\tspeed: 0.1151s/iter; left time: 4501.3607s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0685738\n",
      "\tspeed: 0.1155s/iter; left time: 4504.9048s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0633010\n",
      "\tspeed: 0.1145s/iter; left time: 4454.9581s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603377\n",
      "\tspeed: 0.1131s/iter; left time: 4388.6346s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0727599\n",
      "\tspeed: 0.1139s/iter; left time: 4409.1309s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0715380\n",
      "\tspeed: 0.1111s/iter; left time: 4289.6488s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0616430\n",
      "\tspeed: 0.1125s/iter; left time: 4333.5642s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0798777\n",
      "\tspeed: 0.1129s/iter; left time: 4335.9876s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0765420\n",
      "\tspeed: 0.1119s/iter; left time: 4286.7227s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0671012\n",
      "\tspeed: 0.1099s/iter; left time: 4200.8745s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0616646\n",
      "\tspeed: 0.1135s/iter; left time: 4325.0934s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0681724\n",
      "\tspeed: 0.1132s/iter; left time: 4302.4291s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0847921\n",
      "\tspeed: 0.1123s/iter; left time: 4259.4616s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0787449\n",
      "\tspeed: 0.1131s/iter; left time: 4275.7450s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0638925\n",
      "\tspeed: 0.1127s/iter; left time: 4251.6605s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0637040\n",
      "\tspeed: 0.1121s/iter; left time: 4218.4933s\n",
      "Epoch: 6 cost time: 00h:05m:03.44s\n",
      "Epoch: 6 | Train Loss: 0.0710204 Vali Loss: 0.0602071 Test Loss: 0.0637255\n",
      "Validation loss decreased (0.060260 --> 0.060207).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0805032\n",
      "\tspeed: 1.0514s/iter; left time: 39357.4660s\n",
      "\titers: 200, epoch: 7 | loss: 0.0928424\n",
      "\tspeed: 0.1134s/iter; left time: 4234.1236s\n",
      "\titers: 300, epoch: 7 | loss: 0.0699223\n",
      "\tspeed: 0.1140s/iter; left time: 4245.8493s\n",
      "\titers: 400, epoch: 7 | loss: 0.0772188\n",
      "\tspeed: 0.1138s/iter; left time: 4225.0050s\n",
      "\titers: 500, epoch: 7 | loss: 0.0806367\n",
      "\tspeed: 0.1140s/iter; left time: 4222.1278s\n",
      "\titers: 600, epoch: 7 | loss: 0.0634284\n",
      "\tspeed: 0.1119s/iter; left time: 4131.5210s\n",
      "\titers: 700, epoch: 7 | loss: 0.0699541\n",
      "\tspeed: 0.1147s/iter; left time: 4223.7646s\n",
      "\titers: 800, epoch: 7 | loss: 0.0892019\n",
      "\tspeed: 0.1146s/iter; left time: 4210.1537s\n",
      "\titers: 900, epoch: 7 | loss: 0.0712903\n",
      "\tspeed: 0.1136s/iter; left time: 4163.4128s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0747735\n",
      "\tspeed: 0.1142s/iter; left time: 4170.9985s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0782191\n",
      "\tspeed: 0.1127s/iter; left time: 4106.8679s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0647047\n",
      "\tspeed: 0.1116s/iter; left time: 4055.0160s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0666225\n",
      "\tspeed: 0.1144s/iter; left time: 4146.6489s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843146\n",
      "\tspeed: 0.1130s/iter; left time: 4081.5626s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0681136\n",
      "\tspeed: 0.1137s/iter; left time: 4097.0857s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0701010\n",
      "\tspeed: 0.1120s/iter; left time: 4026.2285s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0843430\n",
      "\tspeed: 0.1126s/iter; left time: 4034.4264s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0538953\n",
      "\tspeed: 0.1131s/iter; left time: 4042.4488s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0655326\n",
      "\tspeed: 0.1124s/iter; left time: 4006.9932s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0805343\n",
      "\tspeed: 0.1128s/iter; left time: 4006.6221s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0631403\n",
      "\tspeed: 0.1123s/iter; left time: 3980.9580s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0604495\n",
      "\tspeed: 0.1122s/iter; left time: 3964.3230s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0642110\n",
      "\tspeed: 0.1123s/iter; left time: 3958.0783s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0664487\n",
      "\tspeed: 0.1137s/iter; left time: 3995.3425s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0816911\n",
      "\tspeed: 0.1134s/iter; left time: 3972.5020s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0661209\n",
      "\tspeed: 0.1129s/iter; left time: 3944.3765s\n",
      "Epoch: 7 cost time: 00h:05m:03.94s\n",
      "Epoch: 7 | Train Loss: 0.0702127 Vali Loss: 0.0597712 Test Loss: 0.0634713\n",
      "Validation loss decreased (0.060207 --> 0.059771).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0825887\n",
      "\tspeed: 1.0613s/iter; left time: 36883.3912s\n",
      "\titers: 200, epoch: 8 | loss: 0.0750446\n",
      "\tspeed: 0.1157s/iter; left time: 4008.4034s\n",
      "\titers: 300, epoch: 8 | loss: 0.0714472\n",
      "\tspeed: 0.1147s/iter; left time: 3964.0194s\n",
      "\titers: 400, epoch: 8 | loss: 0.0683220\n",
      "\tspeed: 0.1138s/iter; left time: 3919.4924s\n",
      "\titers: 500, epoch: 8 | loss: 0.0658177\n",
      "\tspeed: 0.1144s/iter; left time: 3930.1622s\n",
      "\titers: 600, epoch: 8 | loss: 0.0671118\n",
      "\tspeed: 0.1134s/iter; left time: 3886.0113s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654151\n",
      "\tspeed: 0.1139s/iter; left time: 3888.8765s\n",
      "\titers: 800, epoch: 8 | loss: 0.0633116\n",
      "\tspeed: 0.1150s/iter; left time: 3916.3384s\n",
      "\titers: 900, epoch: 8 | loss: 0.0762280\n",
      "\tspeed: 0.1129s/iter; left time: 3834.4540s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0904688\n",
      "\tspeed: 0.1132s/iter; left time: 3833.6557s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0685867\n",
      "\tspeed: 0.1157s/iter; left time: 3904.7728s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0753753\n",
      "\tspeed: 0.1157s/iter; left time: 3892.1284s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0869639\n",
      "\tspeed: 0.1128s/iter; left time: 3783.3242s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0656767\n",
      "\tspeed: 0.1127s/iter; left time: 3769.6066s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0751337\n",
      "\tspeed: 0.1129s/iter; left time: 3767.1325s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0769074\n",
      "\tspeed: 0.1114s/iter; left time: 3705.3295s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0628735\n",
      "\tspeed: 0.1138s/iter; left time: 3774.2191s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0630130\n",
      "\tspeed: 0.1123s/iter; left time: 3712.3849s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0680705\n",
      "\tspeed: 0.1126s/iter; left time: 3710.9236s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0652012\n",
      "\tspeed: 0.1123s/iter; left time: 3690.3335s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0650971\n",
      "\tspeed: 0.1104s/iter; left time: 3616.4226s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0778571\n",
      "\tspeed: 0.1124s/iter; left time: 3669.9809s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0902112\n",
      "\tspeed: 0.1117s/iter; left time: 3637.1336s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0791112\n",
      "\tspeed: 0.1114s/iter; left time: 3616.3970s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0677406\n",
      "\tspeed: 0.1130s/iter; left time: 3655.9542s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0605854\n",
      "\tspeed: 0.1125s/iter; left time: 3628.7290s\n",
      "Epoch: 8 cost time: 00h:05m:04.33s\n",
      "Epoch: 8 | Train Loss: 0.0696060 Vali Loss: 0.0590203 Test Loss: 0.0629085\n",
      "Validation loss decreased (0.059771 --> 0.059020).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0732988\n",
      "\tspeed: 1.0469s/iter; left time: 33575.7486s\n",
      "\titers: 200, epoch: 9 | loss: 0.0632006\n",
      "\tspeed: 0.1134s/iter; left time: 3625.6311s\n",
      "\titers: 300, epoch: 9 | loss: 0.0675089\n",
      "\tspeed: 0.1133s/iter; left time: 3610.2588s\n",
      "\titers: 400, epoch: 9 | loss: 0.0753807\n",
      "\tspeed: 0.1126s/iter; left time: 3576.9251s\n",
      "\titers: 500, epoch: 9 | loss: 0.0688046\n",
      "\tspeed: 0.1139s/iter; left time: 3606.3103s\n",
      "\titers: 600, epoch: 9 | loss: 0.0563322\n",
      "\tspeed: 0.1114s/iter; left time: 3518.7991s\n",
      "\titers: 700, epoch: 9 | loss: 0.0637789\n",
      "\tspeed: 0.1147s/iter; left time: 3609.5353s\n",
      "\titers: 800, epoch: 9 | loss: 0.0552997\n",
      "\tspeed: 0.1143s/iter; left time: 3585.2895s\n",
      "\titers: 900, epoch: 9 | loss: 0.0730415\n",
      "\tspeed: 0.1131s/iter; left time: 3536.3882s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0624509\n",
      "\tspeed: 0.1131s/iter; left time: 3525.1544s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0611722\n",
      "\tspeed: 0.1142s/iter; left time: 3548.2124s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0624793\n",
      "\tspeed: 0.1136s/iter; left time: 3518.4102s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0554433\n",
      "\tspeed: 0.1126s/iter; left time: 3474.9651s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0854482\n",
      "\tspeed: 0.1130s/iter; left time: 3478.8142s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1009781\n",
      "\tspeed: 0.1139s/iter; left time: 3494.9430s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0588896\n",
      "\tspeed: 0.1126s/iter; left time: 3443.7265s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0664771\n",
      "\tspeed: 0.1125s/iter; left time: 3427.2529s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0875199\n",
      "\tspeed: 0.1135s/iter; left time: 3447.8911s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0652644\n",
      "\tspeed: 0.1130s/iter; left time: 3421.9041s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0729210\n",
      "\tspeed: 0.1112s/iter; left time: 3355.2659s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0595015\n",
      "\tspeed: 0.1128s/iter; left time: 3391.7246s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0645168\n",
      "\tspeed: 0.1130s/iter; left time: 3388.3325s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0664069\n",
      "\tspeed: 0.1108s/iter; left time: 3310.9877s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0560363\n",
      "\tspeed: 0.1127s/iter; left time: 3354.7876s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0640769\n",
      "\tspeed: 0.1137s/iter; left time: 3373.7109s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0609662\n",
      "\tspeed: 0.1146s/iter; left time: 3390.4395s\n",
      "Epoch: 9 cost time: 00h:05m:04.13s\n",
      "Epoch: 9 | Train Loss: 0.0688502 Vali Loss: 0.0579161 Test Loss: 0.0620261\n",
      "Validation loss decreased (0.059020 --> 0.057916).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0692350\n",
      "\tspeed: 1.0613s/iter; left time: 31192.3696s\n",
      "\titers: 200, epoch: 10 | loss: 0.0792829\n",
      "\tspeed: 0.1143s/iter; left time: 3349.2427s\n",
      "\titers: 300, epoch: 10 | loss: 0.0651196\n",
      "\tspeed: 0.1137s/iter; left time: 3317.6927s\n",
      "\titers: 400, epoch: 10 | loss: 0.0529624\n",
      "\tspeed: 0.1138s/iter; left time: 3309.5114s\n",
      "\titers: 500, epoch: 10 | loss: 0.0596995\n",
      "\tspeed: 0.1145s/iter; left time: 3320.1693s\n",
      "\titers: 600, epoch: 10 | loss: 0.0596051\n",
      "\tspeed: 0.1158s/iter; left time: 3346.7888s\n",
      "\titers: 700, epoch: 10 | loss: 0.0729908\n",
      "\tspeed: 0.1113s/iter; left time: 3205.6438s\n",
      "\titers: 800, epoch: 10 | loss: 0.0748923\n",
      "\tspeed: 0.1140s/iter; left time: 3270.4554s\n",
      "\titers: 900, epoch: 10 | loss: 0.0764702\n",
      "\tspeed: 0.1167s/iter; left time: 3335.9231s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0590842\n",
      "\tspeed: 0.1168s/iter; left time: 3327.6925s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0709747\n",
      "\tspeed: 0.1160s/iter; left time: 3292.8934s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0812026\n",
      "\tspeed: 0.1153s/iter; left time: 3260.7871s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0673435\n",
      "\tspeed: 0.1149s/iter; left time: 3238.6523s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0555878\n",
      "\tspeed: 0.1158s/iter; left time: 3251.9341s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0766573\n",
      "\tspeed: 0.1145s/iter; left time: 3205.1303s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0764899\n",
      "\tspeed: 0.1143s/iter; left time: 3187.6596s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0618597\n",
      "\tspeed: 0.1160s/iter; left time: 3223.2066s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0638845\n",
      "\tspeed: 0.1111s/iter; left time: 3077.8686s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0900016\n",
      "\tspeed: 0.1143s/iter; left time: 3152.6443s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0711123\n",
      "\tspeed: 0.1149s/iter; left time: 3158.2682s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0715640\n",
      "\tspeed: 0.1122s/iter; left time: 3072.3091s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0749826\n",
      "\tspeed: 0.1123s/iter; left time: 3064.6542s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0696980\n",
      "\tspeed: 0.1130s/iter; left time: 3071.7559s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0574644\n",
      "\tspeed: 0.1118s/iter; left time: 3029.9863s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0694450\n",
      "\tspeed: 0.1117s/iter; left time: 3014.1432s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0577533\n",
      "\tspeed: 0.1135s/iter; left time: 3051.3840s\n",
      "Epoch: 10 cost time: 00h:05m:06.70s\n",
      "Epoch: 10 | Train Loss: 0.0683724 Vali Loss: 0.0582843 Test Loss: 0.0620225\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0733247\n",
      "\tspeed: 1.0370s/iter; left time: 27698.2191s\n",
      "\titers: 200, epoch: 11 | loss: 0.0593850\n",
      "\tspeed: 0.1133s/iter; left time: 3016.3245s\n",
      "\titers: 300, epoch: 11 | loss: 0.0672786\n",
      "\tspeed: 0.1143s/iter; left time: 3028.9575s\n",
      "\titers: 400, epoch: 11 | loss: 0.0505884\n",
      "\tspeed: 0.1162s/iter; left time: 3067.8889s\n",
      "\titers: 500, epoch: 11 | loss: 0.0644879\n",
      "\tspeed: 0.1131s/iter; left time: 2976.2848s\n",
      "\titers: 600, epoch: 11 | loss: 0.0813994\n",
      "\tspeed: 0.1134s/iter; left time: 2972.4012s\n",
      "\titers: 700, epoch: 11 | loss: 0.0774051\n",
      "\tspeed: 0.1157s/iter; left time: 3021.8245s\n",
      "\titers: 800, epoch: 11 | loss: 0.0700469\n",
      "\tspeed: 0.1136s/iter; left time: 2954.5585s\n",
      "\titers: 900, epoch: 11 | loss: 0.0562205\n",
      "\tspeed: 0.1144s/iter; left time: 2965.5106s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0744258\n",
      "\tspeed: 0.1142s/iter; left time: 2947.7942s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0834967\n",
      "\tspeed: 0.1150s/iter; left time: 2957.1412s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0731772\n",
      "\tspeed: 0.1142s/iter; left time: 2924.9462s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0543189\n",
      "\tspeed: 0.1162s/iter; left time: 2964.6617s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0754633\n",
      "\tspeed: 0.1160s/iter; left time: 2947.0532s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0645236\n",
      "\tspeed: 0.1141s/iter; left time: 2888.1280s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0708787\n",
      "\tspeed: 0.1155s/iter; left time: 2911.1337s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0754559\n",
      "\tspeed: 0.1132s/iter; left time: 2842.4902s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0719775\n",
      "\tspeed: 0.1122s/iter; left time: 2804.9962s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0709032\n",
      "\tspeed: 0.1151s/iter; left time: 2867.6776s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0738048\n",
      "\tspeed: 0.1125s/iter; left time: 2791.8765s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0666654\n",
      "\tspeed: 0.1133s/iter; left time: 2800.5792s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0716899\n",
      "\tspeed: 0.1127s/iter; left time: 2774.4368s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0751313\n",
      "\tspeed: 0.1113s/iter; left time: 2728.3758s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0650967\n",
      "\tspeed: 0.1115s/iter; left time: 2722.6908s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0566285\n",
      "\tspeed: 0.1131s/iter; left time: 2748.4264s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0661598\n",
      "\tspeed: 0.1119s/iter; left time: 2709.0795s\n",
      "Epoch: 11 cost time: 00h:05m:06.02s\n",
      "Epoch: 11 | Train Loss: 0.0677539 Vali Loss: 0.0579457 Test Loss: 0.0615977\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0564539\n",
      "\tspeed: 1.0320s/iter; left time: 24800.0710s\n",
      "\titers: 200, epoch: 12 | loss: 0.0552143\n",
      "\tspeed: 0.1127s/iter; left time: 2696.2229s\n",
      "\titers: 300, epoch: 12 | loss: 0.0666377\n",
      "\tspeed: 0.1130s/iter; left time: 2693.7457s\n",
      "\titers: 400, epoch: 12 | loss: 0.0854366\n",
      "\tspeed: 0.1118s/iter; left time: 2653.1796s\n",
      "\titers: 500, epoch: 12 | loss: 0.0836129\n",
      "\tspeed: 0.1138s/iter; left time: 2688.2405s\n",
      "\titers: 600, epoch: 12 | loss: 0.0733599\n",
      "\tspeed: 0.1144s/iter; left time: 2692.4083s\n",
      "\titers: 700, epoch: 12 | loss: 0.0519824\n",
      "\tspeed: 0.1141s/iter; left time: 2674.1239s\n",
      "\titers: 800, epoch: 12 | loss: 0.0707659\n",
      "\tspeed: 0.1120s/iter; left time: 2612.7351s\n",
      "\titers: 900, epoch: 12 | loss: 0.0599417\n",
      "\tspeed: 0.1124s/iter; left time: 2610.4100s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0641551\n",
      "\tspeed: 0.1119s/iter; left time: 2587.2098s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0645704\n",
      "\tspeed: 0.1141s/iter; left time: 2627.2422s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0506950\n",
      "\tspeed: 0.1148s/iter; left time: 2631.2853s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0640931\n",
      "\tspeed: 0.1143s/iter; left time: 2610.5703s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0689833\n",
      "\tspeed: 0.1145s/iter; left time: 2601.8537s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0576060\n",
      "\tspeed: 0.1140s/iter; left time: 2580.7300s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0531667\n",
      "\tspeed: 0.1152s/iter; left time: 2595.6259s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0634687\n",
      "\tspeed: 0.1156s/iter; left time: 2592.7872s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0635579\n",
      "\tspeed: 0.1152s/iter; left time: 2571.7915s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0655776\n",
      "\tspeed: 0.1167s/iter; left time: 2594.8326s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0597161\n",
      "\tspeed: 0.1159s/iter; left time: 2565.2309s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0718565\n",
      "\tspeed: 0.1146s/iter; left time: 2525.0952s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0863532\n",
      "\tspeed: 0.1146s/iter; left time: 2512.7847s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0655009\n",
      "\tspeed: 0.1138s/iter; left time: 2483.6130s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0780334\n",
      "\tspeed: 0.1132s/iter; left time: 2459.7022s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685403\n",
      "\tspeed: 0.1140s/iter; left time: 2465.9565s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0669672\n",
      "\tspeed: 0.1128s/iter; left time: 2428.6642s\n",
      "Epoch: 12 cost time: 00h:05m:06.14s\n",
      "Epoch: 12 | Train Loss: 0.0672516 Vali Loss: 0.0583924 Test Loss: 0.0623229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0646751\n",
      "\tspeed: 1.0364s/iter; left time: 22126.2339s\n",
      "\titers: 200, epoch: 13 | loss: 0.0718063\n",
      "\tspeed: 0.1127s/iter; left time: 2394.7919s\n",
      "\titers: 300, epoch: 13 | loss: 0.0704412\n",
      "\tspeed: 0.1128s/iter; left time: 2385.4888s\n",
      "\titers: 400, epoch: 13 | loss: 0.0566958\n",
      "\tspeed: 0.1143s/iter; left time: 2406.1945s\n",
      "\titers: 500, epoch: 13 | loss: 0.0677867\n",
      "\tspeed: 0.1114s/iter; left time: 2334.0660s\n",
      "\titers: 600, epoch: 13 | loss: 0.0662406\n",
      "\tspeed: 0.1131s/iter; left time: 2358.9155s\n",
      "\titers: 700, epoch: 13 | loss: 0.0692414\n",
      "\tspeed: 0.1158s/iter; left time: 2403.1566s\n",
      "\titers: 800, epoch: 13 | loss: 0.0744722\n",
      "\tspeed: 0.1136s/iter; left time: 2345.5722s\n",
      "\titers: 900, epoch: 13 | loss: 0.0682855\n",
      "\tspeed: 0.1139s/iter; left time: 2339.8397s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0692520\n",
      "\tspeed: 0.1138s/iter; left time: 2327.3567s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0754888\n",
      "\tspeed: 0.1127s/iter; left time: 2293.1628s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0644453\n",
      "\tspeed: 0.1117s/iter; left time: 2262.0843s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0570012\n",
      "\tspeed: 0.1152s/iter; left time: 2321.7263s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0658361\n",
      "\tspeed: 0.1138s/iter; left time: 2282.3398s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0681140\n",
      "\tspeed: 0.1120s/iter; left time: 2234.3692s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0861393\n",
      "\tspeed: 0.1131s/iter; left time: 2244.6265s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0685929\n",
      "\tspeed: 0.1121s/iter; left time: 2213.1584s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0834594\n",
      "\tspeed: 0.1121s/iter; left time: 2201.6722s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0550227\n",
      "\tspeed: 0.1143s/iter; left time: 2234.3334s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0614033\n",
      "\tspeed: 0.1144s/iter; left time: 2225.0598s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0588906\n",
      "\tspeed: 0.1141s/iter; left time: 2207.2866s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0599210\n",
      "\tspeed: 0.1156s/iter; left time: 2224.4833s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0557182\n",
      "\tspeed: 0.1158s/iter; left time: 2216.8033s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0523902\n",
      "\tspeed: 0.1137s/iter; left time: 2165.3673s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0614735\n",
      "\tspeed: 0.1128s/iter; left time: 2138.0992s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0746370\n",
      "\tspeed: 0.1149s/iter; left time: 2166.1827s\n",
      "Epoch: 13 cost time: 00h:05m:05.24s\n",
      "Epoch: 13 | Train Loss: 0.0666866 Vali Loss: 0.0598697 Test Loss: 0.0634292\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0550120\n",
      "\tspeed: 1.0248s/iter; left time: 19130.7921s\n",
      "\titers: 200, epoch: 14 | loss: 0.0765724\n",
      "\tspeed: 0.1136s/iter; left time: 2109.4309s\n",
      "\titers: 300, epoch: 14 | loss: 0.0594586\n",
      "\tspeed: 0.1121s/iter; left time: 2069.9351s\n",
      "\titers: 400, epoch: 14 | loss: 0.0718415\n",
      "\tspeed: 0.1145s/iter; left time: 2102.6518s\n",
      "\titers: 500, epoch: 14 | loss: 0.0611447\n",
      "\tspeed: 0.1153s/iter; left time: 2106.0231s\n",
      "\titers: 600, epoch: 14 | loss: 0.0523001\n",
      "\tspeed: 0.1114s/iter; left time: 2023.0540s\n",
      "\titers: 700, epoch: 14 | loss: 0.0722972\n",
      "\tspeed: 0.1141s/iter; left time: 2060.6737s\n",
      "\titers: 800, epoch: 14 | loss: 0.0649306\n",
      "\tspeed: 0.1160s/iter; left time: 2083.8280s\n",
      "\titers: 900, epoch: 14 | loss: 0.0694602\n",
      "\tspeed: 0.1121s/iter; left time: 2003.6791s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0615104\n",
      "\tspeed: 0.1132s/iter; left time: 2011.5344s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0770956\n",
      "\tspeed: 0.1140s/iter; left time: 2014.7615s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0567148\n",
      "\tspeed: 0.1149s/iter; left time: 2019.0759s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0728452\n",
      "\tspeed: 0.1126s/iter; left time: 1967.7282s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0593382\n",
      "\tspeed: 0.1136s/iter; left time: 1972.6366s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0824114\n",
      "\tspeed: 0.1135s/iter; left time: 1960.3904s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0644274\n",
      "\tspeed: 0.1138s/iter; left time: 1954.0745s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0727649\n",
      "\tspeed: 0.1132s/iter; left time: 1932.2008s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0557669\n",
      "\tspeed: 0.1159s/iter; left time: 1966.0186s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0580711\n",
      "\tspeed: 0.1141s/iter; left time: 1925.0778s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0541300\n",
      "\tspeed: 0.1131s/iter; left time: 1896.9430s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0559602\n",
      "\tspeed: 0.1127s/iter; left time: 1878.8913s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0641657\n",
      "\tspeed: 0.1133s/iter; left time: 1876.6334s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0765522\n",
      "\tspeed: 0.1120s/iter; left time: 1843.8689s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0656291\n",
      "\tspeed: 0.1125s/iter; left time: 1841.0284s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0698191\n",
      "\tspeed: 0.1118s/iter; left time: 1818.2485s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0704088\n",
      "\tspeed: 0.1125s/iter; left time: 1819.4298s\n",
      "Epoch: 14 cost time: 00h:05m:04.54s\n",
      "Epoch: 14 | Train Loss: 0.0661358 Vali Loss: 0.0595082 Test Loss: 0.0636313\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010770452208817005, rmse:0.10378079116344452, mae:0.06202606484293938, rse:0.39211392402648926\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 24: 01h:32m:28.32s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 19:10:42,525] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 19:10:43,613] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 19:10:43,613] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 19:10:43,613] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 19:10:43,698] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 19:10:43,698] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 19:10:44,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 19:10:44,423] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 19:10:44,423] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 19:10:44,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 19:10:44,425] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 19:10:44,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 19:10:44,816] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 19:10:44,817] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:44,817] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.89 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:44,985] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 19:10:44,986] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:44,986] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.96 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:44,986] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 19:10:45,157] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 19:10:45,158] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:45,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.98 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 19:10:45,160] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f2cac6550>\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2074100\n",
      "\tspeed: 0.1698s/iter; left time: 9065.7885s\n",
      "\titers: 200, epoch: 1 | loss: 0.1933736\n",
      "\tspeed: 0.1209s/iter; left time: 6441.2101s\n",
      "\titers: 300, epoch: 1 | loss: 0.1680937\n",
      "\tspeed: 0.1243s/iter; left time: 6612.0969s\n",
      "\titers: 400, epoch: 1 | loss: 0.1478600\n",
      "\tspeed: 0.1233s/iter; left time: 6543.0074s\n",
      "\titers: 500, epoch: 1 | loss: 0.1329422\n",
      "\tspeed: 0.1241s/iter; left time: 6574.7097s\n",
      "\titers: 600, epoch: 1 | loss: 0.1080209\n",
      "\tspeed: 0.1237s/iter; left time: 6541.7857s\n",
      "\titers: 700, epoch: 1 | loss: 0.1268893\n",
      "\tspeed: 0.1274s/iter; left time: 6724.4386s\n",
      "\titers: 800, epoch: 1 | loss: 0.1043758\n",
      "\tspeed: 0.1244s/iter; left time: 6552.9860s\n",
      "\titers: 900, epoch: 1 | loss: 0.1108791\n",
      "\tspeed: 0.1236s/iter; left time: 6499.0257s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1154346\n",
      "\tspeed: 0.1242s/iter; left time: 6520.1496s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0978668\n",
      "\tspeed: 0.1230s/iter; left time: 6444.5262s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0894702\n",
      "\tspeed: 0.1251s/iter; left time: 6540.3517s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1008194\n",
      "\tspeed: 0.1253s/iter; left time: 6537.7925s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1065734\n",
      "\tspeed: 0.1244s/iter; left time: 6480.2460s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1051371\n",
      "\tspeed: 0.1249s/iter; left time: 6494.8085s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0991306\n",
      "\tspeed: 0.1247s/iter; left time: 6467.0995s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1286744\n",
      "\tspeed: 0.1231s/iter; left time: 6375.7148s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1012800\n",
      "\tspeed: 0.1206s/iter; left time: 6234.7451s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1123592\n",
      "\tspeed: 0.1227s/iter; left time: 6330.6444s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0761735\n",
      "\tspeed: 0.1225s/iter; left time: 6308.5215s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1169404\n",
      "\tspeed: 0.1229s/iter; left time: 6316.9931s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1165787\n",
      "\tspeed: 0.1224s/iter; left time: 6277.8256s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1044903\n",
      "\tspeed: 0.1213s/iter; left time: 6206.7813s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0961953\n",
      "\tspeed: 0.1252s/iter; left time: 6397.0164s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1058684\n",
      "\tspeed: 0.1251s/iter; left time: 6377.4777s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0999430\n",
      "\tspeed: 0.1232s/iter; left time: 6268.4567s\n",
      "Epoch: 1 cost time: 00h:05m:32.20s\n",
      "Epoch: 1 | Train Loss: 0.1198931 Vali Loss: 0.0841650 Test Loss: 0.0884351\n",
      "Validation loss decreased (inf --> 0.084165).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1127564\n",
      "\tspeed: 1.1665s/iter; left time: 59149.8786s\n",
      "\titers: 200, epoch: 2 | loss: 0.0858049\n",
      "\tspeed: 0.1124s/iter; left time: 5687.4056s\n",
      "\titers: 300, epoch: 2 | loss: 0.0845444\n",
      "\tspeed: 0.1103s/iter; left time: 5568.9101s\n",
      "\titers: 400, epoch: 2 | loss: 0.0956632\n",
      "\tspeed: 0.1131s/iter; left time: 5699.8288s\n",
      "\titers: 500, epoch: 2 | loss: 0.0922268\n",
      "\tspeed: 0.1148s/iter; left time: 5775.3422s\n",
      "\titers: 600, epoch: 2 | loss: 0.0877490\n",
      "\tspeed: 0.1141s/iter; left time: 5730.1084s\n",
      "\titers: 700, epoch: 2 | loss: 0.0789305\n",
      "\tspeed: 0.1129s/iter; left time: 5658.8953s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000123\n",
      "\tspeed: 0.1131s/iter; left time: 5655.9864s\n",
      "\titers: 900, epoch: 2 | loss: 0.1008030\n",
      "\tspeed: 0.1136s/iter; left time: 5670.5816s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0849907\n",
      "\tspeed: 0.1112s/iter; left time: 5537.6149s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0847871\n",
      "\tspeed: 0.1123s/iter; left time: 5580.6413s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1110906\n",
      "\tspeed: 0.1117s/iter; left time: 5543.3750s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0902140\n",
      "\tspeed: 0.1116s/iter; left time: 5526.9080s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1075663\n",
      "\tspeed: 0.1122s/iter; left time: 5543.3304s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0813371\n",
      "\tspeed: 0.1151s/iter; left time: 5674.4658s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0910764\n",
      "\tspeed: 0.1130s/iter; left time: 5562.6063s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0874856\n",
      "\tspeed: 0.1129s/iter; left time: 5545.6735s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1003650\n",
      "\tspeed: 0.1132s/iter; left time: 5548.0881s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0853289\n",
      "\tspeed: 0.1134s/iter; left time: 5544.9215s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0867084\n",
      "\tspeed: 0.1125s/iter; left time: 5489.3666s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0767493\n",
      "\tspeed: 0.1130s/iter; left time: 5503.8645s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0793778\n",
      "\tspeed: 0.1144s/iter; left time: 5559.9907s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0787130\n",
      "\tspeed: 0.1138s/iter; left time: 5518.6469s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0944487\n",
      "\tspeed: 0.1144s/iter; left time: 5536.3076s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0862754\n",
      "\tspeed: 0.1154s/iter; left time: 5573.3013s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0699872\n",
      "\tspeed: 0.1159s/iter; left time: 5585.2136s\n",
      "Epoch: 2 cost time: 00h:05m:03.44s\n",
      "Epoch: 2 | Train Loss: 0.0951379 Vali Loss: 0.0802616 Test Loss: 0.0852105\n",
      "Validation loss decreased (0.084165 --> 0.080262).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0913631\n",
      "\tspeed: 1.0426s/iter; left time: 50079.0402s\n",
      "\titers: 200, epoch: 3 | loss: 0.0941604\n",
      "\tspeed: 0.1148s/iter; left time: 5503.5955s\n",
      "\titers: 300, epoch: 3 | loss: 0.0937610\n",
      "\tspeed: 0.1146s/iter; left time: 5481.5325s\n",
      "\titers: 400, epoch: 3 | loss: 0.0962613\n",
      "\tspeed: 0.1151s/iter; left time: 5493.5033s\n",
      "\titers: 500, epoch: 3 | loss: 0.1008476\n",
      "\tspeed: 0.1144s/iter; left time: 5446.8518s\n",
      "\titers: 600, epoch: 3 | loss: 0.1072278\n",
      "\tspeed: 0.1149s/iter; left time: 5461.5053s\n",
      "\titers: 700, epoch: 3 | loss: 0.0809577\n",
      "\tspeed: 0.1160s/iter; left time: 5500.3849s\n",
      "\titers: 800, epoch: 3 | loss: 0.0869908\n",
      "\tspeed: 0.1144s/iter; left time: 5417.0870s\n",
      "\titers: 900, epoch: 3 | loss: 0.1061004\n",
      "\tspeed: 0.1130s/iter; left time: 5337.2130s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1013325\n",
      "\tspeed: 0.1142s/iter; left time: 5382.7579s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0924709\n",
      "\tspeed: 0.1162s/iter; left time: 5467.3840s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0991043\n",
      "\tspeed: 0.1144s/iter; left time: 5368.7871s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1008329\n",
      "\tspeed: 0.1141s/iter; left time: 5345.9778s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0831598\n",
      "\tspeed: 0.1160s/iter; left time: 5421.5980s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1090582\n",
      "\tspeed: 0.1150s/iter; left time: 5364.5424s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1031286\n",
      "\tspeed: 0.1160s/iter; left time: 5395.8119s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0970030\n",
      "\tspeed: 0.1178s/iter; left time: 5468.1474s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0891229\n",
      "\tspeed: 0.1160s/iter; left time: 5376.3383s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0952215\n",
      "\tspeed: 0.1142s/iter; left time: 5281.1271s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0831259\n",
      "\tspeed: 0.1140s/iter; left time: 5257.9225s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1124142\n",
      "\tspeed: 0.1154s/iter; left time: 5312.1351s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1149694\n",
      "\tspeed: 0.1158s/iter; left time: 5321.2063s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0783673\n",
      "\tspeed: 0.1138s/iter; left time: 5214.8664s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0836947\n",
      "\tspeed: 0.1117s/iter; left time: 5108.9117s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0893349\n",
      "\tspeed: 0.1122s/iter; left time: 5118.3652s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0813950\n",
      "\tspeed: 0.1163s/iter; left time: 5294.5987s\n",
      "Epoch: 3 cost time: 00h:05m:08.15s\n",
      "Epoch: 3 | Train Loss: 0.0912028 Vali Loss: 0.0809163 Test Loss: 0.0860651\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0766129\n",
      "\tspeed: 1.0843s/iter; left time: 49181.3535s\n",
      "\titers: 200, epoch: 4 | loss: 0.1203493\n",
      "\tspeed: 0.1202s/iter; left time: 5437.9964s\n",
      "\titers: 300, epoch: 4 | loss: 0.0836161\n",
      "\tspeed: 0.1240s/iter; left time: 5599.3588s\n",
      "\titers: 400, epoch: 4 | loss: 0.0988006\n",
      "\tspeed: 0.1207s/iter; left time: 5437.8985s\n",
      "\titers: 500, epoch: 4 | loss: 0.0826896\n",
      "\tspeed: 0.1217s/iter; left time: 5472.9887s\n",
      "\titers: 600, epoch: 4 | loss: 0.0930348\n",
      "\tspeed: 0.1205s/iter; left time: 5403.7398s\n",
      "\titers: 700, epoch: 4 | loss: 0.0988240\n",
      "\tspeed: 0.1237s/iter; left time: 5534.5948s\n",
      "\titers: 800, epoch: 4 | loss: 0.0805480\n",
      "\tspeed: 0.1199s/iter; left time: 5354.3449s\n",
      "\titers: 900, epoch: 4 | loss: 0.0826487\n",
      "\tspeed: 0.1188s/iter; left time: 5293.6080s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0710772\n",
      "\tspeed: 0.1274s/iter; left time: 5665.2333s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0831637\n",
      "\tspeed: 0.1225s/iter; left time: 5435.6249s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0868283\n",
      "\tspeed: 0.1206s/iter; left time: 5336.0502s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1062559\n",
      "\tspeed: 0.1246s/iter; left time: 5502.1697s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0913987\n",
      "\tspeed: 0.1224s/iter; left time: 5394.5232s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0884490\n",
      "\tspeed: 0.1231s/iter; left time: 5411.4485s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0850192\n",
      "\tspeed: 0.1212s/iter; left time: 5317.7873s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0802786\n",
      "\tspeed: 0.1164s/iter; left time: 5095.0696s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0982129\n",
      "\tspeed: 0.1213s/iter; left time: 5295.9952s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0785628\n",
      "\tspeed: 0.1255s/iter; left time: 5467.6361s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0918725\n",
      "\tspeed: 0.1253s/iter; left time: 5444.4988s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0956076\n",
      "\tspeed: 0.1261s/iter; left time: 5466.6697s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0837968\n",
      "\tspeed: 0.1245s/iter; left time: 5384.6818s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1122651\n",
      "\tspeed: 0.1187s/iter; left time: 5123.9010s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0925200\n",
      "\tspeed: 0.1248s/iter; left time: 5374.0130s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0905496\n",
      "\tspeed: 0.1263s/iter; left time: 5426.8022s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0913031\n",
      "\tspeed: 0.1276s/iter; left time: 5469.0224s\n",
      "Epoch: 4 cost time: 00h:05m:28.50s\n",
      "Epoch: 4 | Train Loss: 0.0887905 Vali Loss: 0.0801354 Test Loss: 0.0864428\n",
      "Validation loss decreased (0.080262 --> 0.080135).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0905916\n",
      "\tspeed: 1.0963s/iter; left time: 46795.9889s\n",
      "\titers: 200, epoch: 5 | loss: 0.0949473\n",
      "\tspeed: 0.1195s/iter; left time: 5087.5571s\n",
      "\titers: 300, epoch: 5 | loss: 0.0949715\n",
      "\tspeed: 0.1325s/iter; left time: 5630.9090s\n",
      "\titers: 400, epoch: 5 | loss: 0.0939259\n",
      "\tspeed: 0.1198s/iter; left time: 5077.4282s\n",
      "\titers: 500, epoch: 5 | loss: 0.0792014\n",
      "\tspeed: 0.1198s/iter; left time: 5065.8468s\n",
      "\titers: 600, epoch: 5 | loss: 0.1021533\n",
      "\tspeed: 0.1211s/iter; left time: 5107.1698s\n",
      "\titers: 700, epoch: 5 | loss: 0.0821311\n",
      "\tspeed: 0.1221s/iter; left time: 5138.0385s\n",
      "\titers: 800, epoch: 5 | loss: 0.0785077\n",
      "\tspeed: 0.1227s/iter; left time: 5152.3935s\n",
      "\titers: 900, epoch: 5 | loss: 0.0880126\n",
      "\tspeed: 0.1273s/iter; left time: 5331.1461s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0763944\n",
      "\tspeed: 0.1233s/iter; left time: 5152.4257s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0758381\n",
      "\tspeed: 0.1199s/iter; left time: 4997.8302s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0830716\n",
      "\tspeed: 0.1227s/iter; left time: 5101.1974s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0800337\n",
      "\tspeed: 0.1215s/iter; left time: 5039.0160s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0928819\n",
      "\tspeed: 0.1201s/iter; left time: 4972.1698s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0703389\n",
      "\tspeed: 0.1223s/iter; left time: 5049.8767s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0925038\n",
      "\tspeed: 0.1186s/iter; left time: 4885.4127s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0736254\n",
      "\tspeed: 0.1254s/iter; left time: 5152.7273s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0827256\n",
      "\tspeed: 0.1205s/iter; left time: 4939.7981s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0770165\n",
      "\tspeed: 0.1248s/iter; left time: 5100.7339s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0748519\n",
      "\tspeed: 0.1214s/iter; left time: 4951.9783s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0818176\n",
      "\tspeed: 0.1238s/iter; left time: 5038.1865s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0815690\n",
      "\tspeed: 0.1294s/iter; left time: 5250.8622s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1056789\n",
      "\tspeed: 0.1219s/iter; left time: 4933.6225s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0869661\n",
      "\tspeed: 0.1203s/iter; left time: 4858.3086s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0985921\n",
      "\tspeed: 0.1241s/iter; left time: 5000.0069s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0684036\n",
      "\tspeed: 0.1212s/iter; left time: 4869.5123s\n",
      "Epoch: 5 cost time: 00h:05m:28.74s\n",
      "Epoch: 5 | Train Loss: 0.0866818 Vali Loss: 0.0797001 Test Loss: 0.0868311\n",
      "Validation loss decreased (0.080135 --> 0.079700).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1188883\n",
      "\tspeed: 1.0801s/iter; left time: 43215.1434s\n",
      "\titers: 200, epoch: 6 | loss: 0.0875507\n",
      "\tspeed: 0.1203s/iter; left time: 4802.4070s\n",
      "\titers: 300, epoch: 6 | loss: 0.0807057\n",
      "\tspeed: 0.1217s/iter; left time: 4846.0808s\n",
      "\titers: 400, epoch: 6 | loss: 0.0704877\n",
      "\tspeed: 0.1247s/iter; left time: 4951.6835s\n",
      "\titers: 500, epoch: 6 | loss: 0.0778315\n",
      "\tspeed: 0.1205s/iter; left time: 4772.8739s\n",
      "\titers: 600, epoch: 6 | loss: 0.0950918\n",
      "\tspeed: 0.1252s/iter; left time: 4948.2841s\n",
      "\titers: 700, epoch: 6 | loss: 0.0813851\n",
      "\tspeed: 0.1257s/iter; left time: 4954.3791s\n",
      "\titers: 800, epoch: 6 | loss: 0.0890235\n",
      "\tspeed: 0.1145s/iter; left time: 4502.3709s\n",
      "\titers: 900, epoch: 6 | loss: 0.0926933\n",
      "\tspeed: 0.1220s/iter; left time: 4782.3417s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0856443\n",
      "\tspeed: 0.1242s/iter; left time: 4856.5758s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0886120\n",
      "\tspeed: 0.1291s/iter; left time: 5037.0379s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0603817\n",
      "\tspeed: 0.1206s/iter; left time: 4691.3295s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0879021\n",
      "\tspeed: 0.1214s/iter; left time: 4710.2026s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1013661\n",
      "\tspeed: 0.1237s/iter; left time: 4790.2180s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0748302\n",
      "\tspeed: 0.1179s/iter; left time: 4550.8062s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0890142\n",
      "\tspeed: 0.1261s/iter; left time: 4855.4099s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0706096\n",
      "\tspeed: 0.1201s/iter; left time: 4611.3015s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0830537\n",
      "\tspeed: 0.1210s/iter; left time: 4634.2511s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0720259\n",
      "\tspeed: 0.1204s/iter; left time: 4602.0593s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0876311\n",
      "\tspeed: 0.1194s/iter; left time: 4549.9251s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0846190\n",
      "\tspeed: 0.1153s/iter; left time: 4381.5095s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0942824\n",
      "\tspeed: 0.1197s/iter; left time: 4537.4921s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0858440\n",
      "\tspeed: 0.1261s/iter; left time: 4767.5138s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0759794\n",
      "\tspeed: 0.1173s/iter; left time: 4423.4440s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0880383\n",
      "\tspeed: 0.1215s/iter; left time: 4568.3632s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0596848\n",
      "\tspeed: 0.1271s/iter; left time: 4768.3723s\n",
      "Epoch: 6 cost time: 00h:05m:27.11s\n",
      "Epoch: 6 | Train Loss: 0.0846411 Vali Loss: 0.0798222 Test Loss: 0.0859858\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0920638\n",
      "\tspeed: 1.0889s/iter; left time: 40654.9404s\n",
      "\titers: 200, epoch: 7 | loss: 0.1065395\n",
      "\tspeed: 0.1244s/iter; left time: 4630.9616s\n",
      "\titers: 300, epoch: 7 | loss: 0.0627838\n",
      "\tspeed: 0.1228s/iter; left time: 4560.1871s\n",
      "\titers: 400, epoch: 7 | loss: 0.0834983\n",
      "\tspeed: 0.1213s/iter; left time: 4493.4944s\n",
      "\titers: 500, epoch: 7 | loss: 0.0813256\n",
      "\tspeed: 0.1187s/iter; left time: 4383.3373s\n",
      "\titers: 600, epoch: 7 | loss: 0.0936052\n",
      "\tspeed: 0.1211s/iter; left time: 4461.3375s\n",
      "\titers: 700, epoch: 7 | loss: 0.0897368\n",
      "\tspeed: 0.1188s/iter; left time: 4363.0497s\n",
      "\titers: 800, epoch: 7 | loss: 0.0712877\n",
      "\tspeed: 0.1177s/iter; left time: 4311.6725s\n",
      "\titers: 900, epoch: 7 | loss: 0.0922524\n",
      "\tspeed: 0.1268s/iter; left time: 4634.0975s\n",
      "\titers: 1000, epoch: 7 | loss: 0.1083432\n",
      "\tspeed: 0.1203s/iter; left time: 4384.3760s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0795755\n",
      "\tspeed: 0.1130s/iter; left time: 4104.9531s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0830358\n",
      "\tspeed: 0.1221s/iter; left time: 4423.9972s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0777278\n",
      "\tspeed: 0.1252s/iter; left time: 4522.7798s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0791193\n",
      "\tspeed: 0.1157s/iter; left time: 4169.6404s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0937434\n",
      "\tspeed: 0.1221s/iter; left time: 4389.5757s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0894421\n",
      "\tspeed: 0.1193s/iter; left time: 4273.6029s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0789753\n",
      "\tspeed: 0.1208s/iter; left time: 4316.0113s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0762554\n",
      "\tspeed: 0.1264s/iter; left time: 4503.6826s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0578167\n",
      "\tspeed: 0.1265s/iter; left time: 4497.0209s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0893710\n",
      "\tspeed: 0.1247s/iter; left time: 4420.4862s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0829187\n",
      "\tspeed: 0.1202s/iter; left time: 4246.8789s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1052914\n",
      "\tspeed: 0.1248s/iter; left time: 4396.5759s\n",
      "\titers: 2300, epoch: 7 | loss: 0.1023362\n",
      "\tspeed: 0.1258s/iter; left time: 4421.6090s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0633124\n",
      "\tspeed: 0.1253s/iter; left time: 4390.6589s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0897403\n",
      "\tspeed: 0.1147s/iter; left time: 4008.3266s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0877524\n",
      "\tspeed: 0.1172s/iter; left time: 4084.1017s\n",
      "Epoch: 7 cost time: 00h:05m:25.04s\n",
      "Epoch: 7 | Train Loss: 0.0827914 Vali Loss: 0.0802376 Test Loss: 0.0865027\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0741210\n",
      "\tspeed: 1.0234s/iter; left time: 35472.6250s\n",
      "\titers: 200, epoch: 8 | loss: 0.0855546\n",
      "\tspeed: 0.1134s/iter; left time: 3919.1548s\n",
      "\titers: 300, epoch: 8 | loss: 0.0862117\n",
      "\tspeed: 0.1145s/iter; left time: 3944.8104s\n",
      "\titers: 400, epoch: 8 | loss: 0.0810832\n",
      "\tspeed: 0.1152s/iter; left time: 3958.8356s\n",
      "\titers: 500, epoch: 8 | loss: 0.0992447\n",
      "\tspeed: 0.1152s/iter; left time: 3947.5852s\n",
      "\titers: 600, epoch: 8 | loss: 0.0759280\n",
      "\tspeed: 0.1157s/iter; left time: 3951.6902s\n",
      "\titers: 700, epoch: 8 | loss: 0.0706180\n",
      "\tspeed: 0.1156s/iter; left time: 3939.2951s\n",
      "\titers: 800, epoch: 8 | loss: 0.0777954\n",
      "\tspeed: 0.1154s/iter; left time: 3918.7120s\n",
      "\titers: 900, epoch: 8 | loss: 0.0899965\n",
      "\tspeed: 0.1138s/iter; left time: 3854.2748s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0714895\n",
      "\tspeed: 0.1137s/iter; left time: 3838.9223s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0831028\n",
      "\tspeed: 0.1137s/iter; left time: 3828.9141s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0776870\n",
      "\tspeed: 0.1142s/iter; left time: 3834.2195s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0858583\n",
      "\tspeed: 0.1170s/iter; left time: 3916.1538s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0750962\n",
      "\tspeed: 0.1143s/iter; left time: 3813.1582s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0934992\n",
      "\tspeed: 0.1136s/iter; left time: 3777.5693s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0680919\n",
      "\tspeed: 0.1178s/iter; left time: 3905.2622s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0809952\n",
      "\tspeed: 0.1163s/iter; left time: 3844.2126s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0644422\n",
      "\tspeed: 0.1131s/iter; left time: 3727.5083s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0749647\n",
      "\tspeed: 0.1140s/iter; left time: 3746.4699s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0769117\n",
      "\tspeed: 0.1155s/iter; left time: 3783.7151s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0895205\n",
      "\tspeed: 0.1166s/iter; left time: 3809.9989s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0856249\n",
      "\tspeed: 0.1153s/iter; left time: 3755.7292s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0781523\n",
      "\tspeed: 0.1150s/iter; left time: 3731.7042s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0775857\n",
      "\tspeed: 0.1160s/iter; left time: 3754.0185s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0808454\n",
      "\tspeed: 0.1143s/iter; left time: 3686.3988s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0844256\n",
      "\tspeed: 0.1156s/iter; left time: 3716.4597s\n",
      "Epoch: 8 cost time: 00h:05m:08.22s\n",
      "Epoch: 8 | Train Loss: 0.0808754 Vali Loss: 0.0805583 Test Loss: 0.0866693\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0862420\n",
      "\tspeed: 1.0197s/iter; left time: 32618.5266s\n",
      "\titers: 200, epoch: 9 | loss: 0.0791138\n",
      "\tspeed: 0.1145s/iter; left time: 3651.2684s\n",
      "\titers: 300, epoch: 9 | loss: 0.0770254\n",
      "\tspeed: 0.1177s/iter; left time: 3741.3610s\n",
      "\titers: 400, epoch: 9 | loss: 0.0769348\n",
      "\tspeed: 0.1145s/iter; left time: 3629.4086s\n",
      "\titers: 500, epoch: 9 | loss: 0.0745477\n",
      "\tspeed: 0.1163s/iter; left time: 3672.3151s\n",
      "\titers: 600, epoch: 9 | loss: 0.0885054\n",
      "\tspeed: 0.1173s/iter; left time: 3693.0668s\n",
      "\titers: 700, epoch: 9 | loss: 0.0813209\n",
      "\tspeed: 0.1145s/iter; left time: 3595.4994s\n",
      "\titers: 800, epoch: 9 | loss: 0.0839284\n",
      "\tspeed: 0.1154s/iter; left time: 3609.6311s\n",
      "\titers: 900, epoch: 9 | loss: 0.0726186\n",
      "\tspeed: 0.1157s/iter; left time: 3610.0175s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0716390\n",
      "\tspeed: 0.1154s/iter; left time: 3587.6555s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0776923\n",
      "\tspeed: 0.1136s/iter; left time: 3519.9077s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0776154\n",
      "\tspeed: 0.1131s/iter; left time: 3492.6586s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0860990\n",
      "\tspeed: 0.1160s/iter; left time: 3570.2019s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0690729\n",
      "\tspeed: 0.1139s/iter; left time: 3496.1104s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0735717\n",
      "\tspeed: 0.1116s/iter; left time: 3412.3343s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0749671\n",
      "\tspeed: 0.1152s/iter; left time: 3511.0997s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0911820\n",
      "\tspeed: 0.1138s/iter; left time: 3458.6774s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0734640\n",
      "\tspeed: 0.1157s/iter; left time: 3505.0243s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0728578\n",
      "\tspeed: 0.1153s/iter; left time: 3481.0690s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0706099\n",
      "\tspeed: 0.1147s/iter; left time: 3452.3748s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0727396\n",
      "\tspeed: 0.1155s/iter; left time: 3462.6380s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0882496\n",
      "\tspeed: 0.1152s/iter; left time: 3443.8173s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0900344\n",
      "\tspeed: 0.1159s/iter; left time: 3451.6548s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0780393\n",
      "\tspeed: 0.1155s/iter; left time: 3428.7190s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0849080\n",
      "\tspeed: 0.1150s/iter; left time: 3403.7123s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0787509\n",
      "\tspeed: 0.1151s/iter; left time: 3394.7966s\n",
      "Epoch: 9 cost time: 00h:05m:08.25s\n",
      "Epoch: 9 | Train Loss: 0.0790067 Vali Loss: 0.0807893 Test Loss: 0.0869210\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0815329\n",
      "\tspeed: 1.0193s/iter; left time: 29881.4935s\n",
      "\titers: 200, epoch: 10 | loss: 0.0723726\n",
      "\tspeed: 0.1165s/iter; left time: 3403.3836s\n",
      "\titers: 300, epoch: 10 | loss: 0.0746200\n",
      "\tspeed: 0.1142s/iter; left time: 3324.3854s\n",
      "\titers: 400, epoch: 10 | loss: 0.0674459\n",
      "\tspeed: 0.1128s/iter; left time: 3273.8902s\n",
      "\titers: 500, epoch: 10 | loss: 0.0791470\n",
      "\tspeed: 0.1145s/iter; left time: 3310.3644s\n",
      "\titers: 600, epoch: 10 | loss: 0.0762916\n",
      "\tspeed: 0.1190s/iter; left time: 3428.5021s\n",
      "\titers: 700, epoch: 10 | loss: 0.0842900\n",
      "\tspeed: 0.1147s/iter; left time: 3294.8868s\n",
      "\titers: 800, epoch: 10 | loss: 0.0661934\n",
      "\tspeed: 0.1133s/iter; left time: 3242.7917s\n",
      "\titers: 900, epoch: 10 | loss: 0.0785321\n",
      "\tspeed: 0.1158s/iter; left time: 3300.6932s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0710449\n",
      "\tspeed: 0.1159s/iter; left time: 3294.2681s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0821774\n",
      "\tspeed: 0.1136s/iter; left time: 3215.6312s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0766501\n",
      "\tspeed: 0.1129s/iter; left time: 3184.4876s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0781828\n",
      "\tspeed: 0.1144s/iter; left time: 3216.5979s\n",
      "\titers: 1400, epoch: 10 | loss: 0.1004712\n",
      "\tspeed: 0.1140s/iter; left time: 3194.0989s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0783593\n",
      "\tspeed: 0.1131s/iter; left time: 3158.2657s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0835777\n",
      "\tspeed: 0.1176s/iter; left time: 3271.0312s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0684503\n",
      "\tspeed: 0.1142s/iter; left time: 3165.4326s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0780601\n",
      "\tspeed: 0.1144s/iter; left time: 3159.8146s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0802295\n",
      "\tspeed: 0.1141s/iter; left time: 3140.6876s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0798366\n",
      "\tspeed: 0.1135s/iter; left time: 3110.5581s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0814212\n",
      "\tspeed: 0.1143s/iter; left time: 3121.5129s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0743018\n",
      "\tspeed: 0.1154s/iter; left time: 3139.6100s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0766831\n",
      "\tspeed: 0.1115s/iter; left time: 3024.2273s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0754861\n",
      "\tspeed: 0.1085s/iter; left time: 2931.5074s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0673950\n",
      "\tspeed: 0.1141s/iter; left time: 3071.9885s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0699747\n",
      "\tspeed: 0.1135s/iter; left time: 3044.8236s\n",
      "Epoch: 10 cost time: 00h:05m:05.94s\n",
      "Epoch: 10 | Train Loss: 0.0774372 Vali Loss: 0.0823554 Test Loss: 0.0883417\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019759919494390488, rmse:0.14056998491287231, mae:0.08683112263679504, rse:0.5315272808074951\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 96: 01h:08m:14.72s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 20:18:56,405] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 20:18:57,458] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 20:18:57,458] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 20:18:57,458] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 20:18:57,560] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 20:18:57,561] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 20:18:58,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 20:18:58,201] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 20:18:58,201] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 20:18:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 20:18:58,202] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 20:18:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 20:18:58,588] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 20:18:58,589] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.29 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,741] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 20:18:58,742] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,743] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.33 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,743] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 20:18:58,910] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 20:18:58,911] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,911] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.34 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 20:18:58,913] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc06dfa3110>\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2159382\n",
      "\tspeed: 0.1688s/iter; left time: 8987.9947s\n",
      "\titers: 200, epoch: 1 | loss: 0.2169471\n",
      "\tspeed: 0.1266s/iter; left time: 6725.1401s\n",
      "\titers: 300, epoch: 1 | loss: 0.1850180\n",
      "\tspeed: 0.1284s/iter; left time: 6811.7981s\n",
      "\titers: 400, epoch: 1 | loss: 0.1335363\n",
      "\tspeed: 0.1263s/iter; left time: 6688.5373s\n",
      "\titers: 500, epoch: 1 | loss: 0.1362359\n",
      "\tspeed: 0.1277s/iter; left time: 6745.5131s\n",
      "\titers: 600, epoch: 1 | loss: 0.1168138\n",
      "\tspeed: 0.1284s/iter; left time: 6770.0533s\n",
      "\titers: 700, epoch: 1 | loss: 0.1086815\n",
      "\tspeed: 0.1276s/iter; left time: 6718.7430s\n",
      "\titers: 800, epoch: 1 | loss: 0.1136740\n",
      "\tspeed: 0.1258s/iter; left time: 6611.2388s\n",
      "\titers: 900, epoch: 1 | loss: 0.1117396\n",
      "\tspeed: 0.1259s/iter; left time: 6603.8215s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1147921\n",
      "\tspeed: 0.1270s/iter; left time: 6647.8004s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1080129\n",
      "\tspeed: 0.1250s/iter; left time: 6528.0576s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1242537\n",
      "\tspeed: 0.1255s/iter; left time: 6544.4916s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1189426\n",
      "\tspeed: 0.1259s/iter; left time: 6549.9950s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0914909\n",
      "\tspeed: 0.1247s/iter; left time: 6476.1490s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1163723\n",
      "\tspeed: 0.1269s/iter; left time: 6580.0809s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1010581\n",
      "\tspeed: 0.1263s/iter; left time: 6535.9959s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0958032\n",
      "\tspeed: 0.1242s/iter; left time: 6414.0186s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0969449\n",
      "\tspeed: 0.1264s/iter; left time: 6515.8776s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0934629\n",
      "\tspeed: 0.1251s/iter; left time: 6433.0175s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1103176\n",
      "\tspeed: 0.1234s/iter; left time: 6336.3626s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1056070\n",
      "\tspeed: 0.1263s/iter; left time: 6473.2160s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1026355\n",
      "\tspeed: 0.1239s/iter; left time: 6335.1837s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1064908\n",
      "\tspeed: 0.1255s/iter; left time: 6407.7300s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1117751\n",
      "\tspeed: 0.1287s/iter; left time: 6554.4221s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1078130\n",
      "\tspeed: 0.1267s/iter; left time: 6440.2062s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1026039\n",
      "\tspeed: 0.1235s/iter; left time: 6264.4007s\n",
      "Epoch: 1 cost time: 00h:05m:37.43s\n",
      "Epoch: 1 | Train Loss: 0.1204411 Vali Loss: 0.0876045 Test Loss: 0.0915034\n",
      "Validation loss decreased (inf --> 0.087604).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0972830\n",
      "\tspeed: 1.1562s/iter; left time: 58472.3019s\n",
      "\titers: 200, epoch: 2 | loss: 0.0874374\n",
      "\tspeed: 0.1141s/iter; left time: 5757.8750s\n",
      "\titers: 300, epoch: 2 | loss: 0.0931379\n",
      "\tspeed: 0.1170s/iter; left time: 5896.2167s\n",
      "\titers: 400, epoch: 2 | loss: 0.1136137\n",
      "\tspeed: 0.1134s/iter; left time: 5699.3104s\n",
      "\titers: 500, epoch: 2 | loss: 0.1128266\n",
      "\tspeed: 0.1152s/iter; left time: 5782.3263s\n",
      "\titers: 600, epoch: 2 | loss: 0.1001407\n",
      "\tspeed: 0.1126s/iter; left time: 5639.9692s\n",
      "\titers: 700, epoch: 2 | loss: 0.0840503\n",
      "\tspeed: 0.1158s/iter; left time: 5788.7249s\n",
      "\titers: 800, epoch: 2 | loss: 0.0859213\n",
      "\tspeed: 0.1116s/iter; left time: 5564.6505s\n",
      "\titers: 900, epoch: 2 | loss: 0.1103960\n",
      "\tspeed: 0.1139s/iter; left time: 5670.9123s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0919214\n",
      "\tspeed: 0.1186s/iter; left time: 5891.5262s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0923761\n",
      "\tspeed: 0.1154s/iter; left time: 5719.5178s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0950010\n",
      "\tspeed: 0.1120s/iter; left time: 5539.8004s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0965657\n",
      "\tspeed: 0.1130s/iter; left time: 5578.7222s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0979968\n",
      "\tspeed: 0.1154s/iter; left time: 5686.1529s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1063472\n",
      "\tspeed: 0.1147s/iter; left time: 5640.1787s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1021644\n",
      "\tspeed: 0.1122s/iter; left time: 5506.7581s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1312233\n",
      "\tspeed: 0.1127s/iter; left time: 5521.2830s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0861884\n",
      "\tspeed: 0.1138s/iter; left time: 5564.2598s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0921369\n",
      "\tspeed: 0.1164s/iter; left time: 5677.3142s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0864059\n",
      "\tspeed: 0.1137s/iter; left time: 5535.1938s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1039751\n",
      "\tspeed: 0.1140s/iter; left time: 5537.6795s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1015228\n",
      "\tspeed: 0.1138s/iter; left time: 5514.8010s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0781528\n",
      "\tspeed: 0.1124s/iter; left time: 5438.5495s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0949763\n",
      "\tspeed: 0.1110s/iter; left time: 5360.2614s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0828070\n",
      "\tspeed: 0.1103s/iter; left time: 5313.6583s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0963483\n",
      "\tspeed: 0.1130s/iter; left time: 5430.7824s\n",
      "Epoch: 2 cost time: 00h:05m:03.97s\n",
      "Epoch: 2 | Train Loss: 0.0979559 Vali Loss: 0.0849390 Test Loss: 0.0888758\n",
      "Validation loss decreased (0.087604 --> 0.084939).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0922209\n",
      "\tspeed: 1.0196s/iter; left time: 48844.0855s\n",
      "\titers: 200, epoch: 3 | loss: 0.0644474\n",
      "\tspeed: 0.1129s/iter; left time: 5399.0376s\n",
      "\titers: 300, epoch: 3 | loss: 0.1041549\n",
      "\tspeed: 0.1143s/iter; left time: 5450.7034s\n",
      "\titers: 400, epoch: 3 | loss: 0.1109336\n",
      "\tspeed: 0.1132s/iter; left time: 5388.7711s\n",
      "\titers: 500, epoch: 3 | loss: 0.1064243\n",
      "\tspeed: 0.1131s/iter; left time: 5373.5663s\n",
      "\titers: 600, epoch: 3 | loss: 0.0900191\n",
      "\tspeed: 0.1145s/iter; left time: 5429.0218s\n",
      "\titers: 700, epoch: 3 | loss: 0.0858806\n",
      "\tspeed: 0.1136s/iter; left time: 5374.8444s\n",
      "\titers: 800, epoch: 3 | loss: 0.1083349\n",
      "\tspeed: 0.1112s/iter; left time: 5249.5299s\n",
      "\titers: 900, epoch: 3 | loss: 0.1029609\n",
      "\tspeed: 0.1125s/iter; left time: 5301.7130s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0872559\n",
      "\tspeed: 0.1136s/iter; left time: 5340.4887s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0983983\n",
      "\tspeed: 0.1154s/iter; left time: 5411.2824s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1123618\n",
      "\tspeed: 0.1119s/iter; left time: 5239.3426s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0855980\n",
      "\tspeed: 0.1120s/iter; left time: 5231.0785s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0841101\n",
      "\tspeed: 0.1118s/iter; left time: 5209.5184s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0859778\n",
      "\tspeed: 0.1124s/iter; left time: 5226.8275s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1071441\n",
      "\tspeed: 0.1128s/iter; left time: 5235.5990s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0972808\n",
      "\tspeed: 0.1125s/iter; left time: 5209.3522s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0949871\n",
      "\tspeed: 0.1119s/iter; left time: 5170.4263s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1109390\n",
      "\tspeed: 0.1138s/iter; left time: 5244.8320s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0801679\n",
      "\tspeed: 0.1160s/iter; left time: 5336.6812s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1097496\n",
      "\tspeed: 0.1120s/iter; left time: 5142.8642s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0926881\n",
      "\tspeed: 0.1125s/iter; left time: 5151.9609s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0823457\n",
      "\tspeed: 0.1121s/iter; left time: 5125.4586s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0971059\n",
      "\tspeed: 0.1141s/iter; left time: 5201.8496s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0847562\n",
      "\tspeed: 0.1130s/iter; left time: 5144.3606s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0877398\n",
      "\tspeed: 0.1137s/iter; left time: 5161.3596s\n",
      "Epoch: 3 cost time: 00h:05m:02.27s\n",
      "Epoch: 3 | Train Loss: 0.0939890 Vali Loss: 0.0849982 Test Loss: 0.0889016\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0809531\n",
      "\tspeed: 1.0044s/iter; left time: 45438.6496s\n",
      "\titers: 200, epoch: 4 | loss: 0.1035346\n",
      "\tspeed: 0.1128s/iter; left time: 5093.4320s\n",
      "\titers: 300, epoch: 4 | loss: 0.0931954\n",
      "\tspeed: 0.1140s/iter; left time: 5133.4622s\n",
      "\titers: 400, epoch: 4 | loss: 0.0936391\n",
      "\tspeed: 0.1149s/iter; left time: 5163.9889s\n",
      "\titers: 500, epoch: 4 | loss: 0.0769154\n",
      "\tspeed: 0.1148s/iter; left time: 5146.0266s\n",
      "\titers: 600, epoch: 4 | loss: 0.1011256\n",
      "\tspeed: 0.1127s/iter; left time: 5042.1903s\n",
      "\titers: 700, epoch: 4 | loss: 0.1071842\n",
      "\tspeed: 0.1131s/iter; left time: 5050.2225s\n",
      "\titers: 800, epoch: 4 | loss: 0.0825684\n",
      "\tspeed: 0.1130s/iter; left time: 5032.1413s\n",
      "\titers: 900, epoch: 4 | loss: 0.0715358\n",
      "\tspeed: 0.1175s/iter; left time: 5222.1823s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0911235\n",
      "\tspeed: 0.1122s/iter; left time: 4976.4855s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0931380\n",
      "\tspeed: 0.1121s/iter; left time: 4957.5722s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0906525\n",
      "\tspeed: 0.1126s/iter; left time: 4970.0985s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0844347\n",
      "\tspeed: 0.1125s/iter; left time: 4952.3628s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0868495\n",
      "\tspeed: 0.1140s/iter; left time: 5010.1955s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0898872\n",
      "\tspeed: 0.1120s/iter; left time: 4909.2841s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0944612\n",
      "\tspeed: 0.1123s/iter; left time: 4912.6020s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0744505\n",
      "\tspeed: 0.1140s/iter; left time: 4976.5667s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0840901\n",
      "\tspeed: 0.1121s/iter; left time: 4879.4054s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0785889\n",
      "\tspeed: 0.1157s/iter; left time: 5027.3227s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0908988\n",
      "\tspeed: 0.1145s/iter; left time: 4963.6864s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1051483\n",
      "\tspeed: 0.1126s/iter; left time: 4870.4880s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0749808\n",
      "\tspeed: 0.1124s/iter; left time: 4847.8524s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0964035\n",
      "\tspeed: 0.1123s/iter; left time: 4833.6900s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0797054\n",
      "\tspeed: 0.1128s/iter; left time: 4841.5484s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0989241\n",
      "\tspeed: 0.1122s/iter; left time: 4808.3043s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0763184\n",
      "\tspeed: 0.1124s/iter; left time: 4803.4697s\n",
      "Epoch: 4 cost time: 00h:05m:02.73s\n",
      "Epoch: 4 | Train Loss: 0.0912334 Vali Loss: 0.0860498 Test Loss: 0.0906333\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1012761\n",
      "\tspeed: 0.9978s/iter; left time: 42479.9415s\n",
      "\titers: 200, epoch: 5 | loss: 0.0788181\n",
      "\tspeed: 0.1127s/iter; left time: 4784.7008s\n",
      "\titers: 300, epoch: 5 | loss: 0.0916079\n",
      "\tspeed: 0.1132s/iter; left time: 4795.1335s\n",
      "\titers: 400, epoch: 5 | loss: 0.0774744\n",
      "\tspeed: 0.1142s/iter; left time: 4828.3127s\n",
      "\titers: 500, epoch: 5 | loss: 0.0913393\n",
      "\tspeed: 0.1145s/iter; left time: 4827.0348s\n",
      "\titers: 600, epoch: 5 | loss: 0.0951685\n",
      "\tspeed: 0.1133s/iter; left time: 4768.1042s\n",
      "\titers: 700, epoch: 5 | loss: 0.0887147\n",
      "\tspeed: 0.1131s/iter; left time: 4745.9684s\n",
      "\titers: 800, epoch: 5 | loss: 0.0910846\n",
      "\tspeed: 0.1139s/iter; left time: 4768.2605s\n",
      "\titers: 900, epoch: 5 | loss: 0.0891837\n",
      "\tspeed: 0.1128s/iter; left time: 4711.0590s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0790758\n",
      "\tspeed: 0.1133s/iter; left time: 4722.2869s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0895170\n",
      "\tspeed: 0.1145s/iter; left time: 4760.3129s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0942188\n",
      "\tspeed: 0.1131s/iter; left time: 4690.9434s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0948133\n",
      "\tspeed: 0.1126s/iter; left time: 4659.9481s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0987849\n",
      "\tspeed: 0.1167s/iter; left time: 4815.8104s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0812767\n",
      "\tspeed: 0.1149s/iter; left time: 4732.1020s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0904108\n",
      "\tspeed: 0.1145s/iter; left time: 4700.9119s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1042102\n",
      "\tspeed: 0.1130s/iter; left time: 4631.5456s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0898959\n",
      "\tspeed: 0.1130s/iter; left time: 4617.0331s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0911765\n",
      "\tspeed: 0.1132s/iter; left time: 4616.6145s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0696084\n",
      "\tspeed: 0.1135s/iter; left time: 4617.9026s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0943199\n",
      "\tspeed: 0.1152s/iter; left time: 4672.8086s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0874961\n",
      "\tspeed: 0.1113s/iter; left time: 4505.1616s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0919802\n",
      "\tspeed: 0.1122s/iter; left time: 4529.2977s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0831057\n",
      "\tspeed: 0.1130s/iter; left time: 4550.3049s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0805359\n",
      "\tspeed: 0.1137s/iter; left time: 4569.3736s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0857960\n",
      "\tspeed: 0.1137s/iter; left time: 4554.3336s\n",
      "Epoch: 5 cost time: 00h:05m:03.27s\n",
      "Epoch: 5 | Train Loss: 0.0882231 Vali Loss: 0.0871280 Test Loss: 0.0905764\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0777886\n",
      "\tspeed: 0.9981s/iter; left time: 39828.6754s\n",
      "\titers: 200, epoch: 6 | loss: 0.0941440\n",
      "\tspeed: 0.1153s/iter; left time: 4590.3012s\n",
      "\titers: 300, epoch: 6 | loss: 0.0958172\n",
      "\tspeed: 0.1138s/iter; left time: 4518.4601s\n",
      "\titers: 400, epoch: 6 | loss: 0.0725196\n",
      "\tspeed: 0.1136s/iter; left time: 4497.4792s\n",
      "\titers: 500, epoch: 6 | loss: 0.0889861\n",
      "\tspeed: 0.1136s/iter; left time: 4486.5884s\n",
      "\titers: 600, epoch: 6 | loss: 0.0903357\n",
      "\tspeed: 0.1140s/iter; left time: 4494.2167s\n",
      "\titers: 700, epoch: 6 | loss: 0.0943096\n",
      "\tspeed: 0.1158s/iter; left time: 4552.7988s\n",
      "\titers: 800, epoch: 6 | loss: 0.0846683\n",
      "\tspeed: 0.1139s/iter; left time: 4465.7063s\n",
      "\titers: 900, epoch: 6 | loss: 0.0848038\n",
      "\tspeed: 0.1174s/iter; left time: 4591.0282s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0868105\n",
      "\tspeed: 0.1137s/iter; left time: 4436.4564s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0821280\n",
      "\tspeed: 0.1144s/iter; left time: 4449.0651s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0953802\n",
      "\tspeed: 0.1142s/iter; left time: 4430.3935s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0808989\n",
      "\tspeed: 0.1159s/iter; left time: 4487.6126s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0898427\n",
      "\tspeed: 0.1138s/iter; left time: 4394.5421s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0939664\n",
      "\tspeed: 0.1166s/iter; left time: 4490.2968s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0925003\n",
      "\tspeed: 0.1140s/iter; left time: 4380.1347s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0959591\n",
      "\tspeed: 0.1141s/iter; left time: 4369.6036s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0839964\n",
      "\tspeed: 0.1109s/iter; left time: 4235.9092s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0757705\n",
      "\tspeed: 0.1123s/iter; left time: 4278.5917s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0871647\n",
      "\tspeed: 0.1121s/iter; left time: 4261.2119s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0874589\n",
      "\tspeed: 0.1125s/iter; left time: 4263.0298s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0961731\n",
      "\tspeed: 0.1130s/iter; left time: 4273.7239s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0795345\n",
      "\tspeed: 0.1139s/iter; left time: 4296.2716s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0825180\n",
      "\tspeed: 0.1130s/iter; left time: 4251.1629s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0753969\n",
      "\tspeed: 0.1122s/iter; left time: 4208.6607s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0764885\n",
      "\tspeed: 0.1132s/iter; left time: 4234.7952s\n",
      "Epoch: 6 cost time: 00h:05m:04.35s\n",
      "Epoch: 6 | Train Loss: 0.0857598 Vali Loss: 0.0890246 Test Loss: 0.0932542\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0879309\n",
      "\tspeed: 1.0021s/iter; left time: 37316.4604s\n",
      "\titers: 200, epoch: 7 | loss: 0.0852456\n",
      "\tspeed: 0.1156s/iter; left time: 4293.4604s\n",
      "\titers: 300, epoch: 7 | loss: 0.0813645\n",
      "\tspeed: 0.1148s/iter; left time: 4253.5320s\n",
      "\titers: 400, epoch: 7 | loss: 0.0748249\n",
      "\tspeed: 0.1139s/iter; left time: 4207.1884s\n",
      "\titers: 500, epoch: 7 | loss: 0.0818025\n",
      "\tspeed: 0.1149s/iter; left time: 4232.5474s\n",
      "\titers: 600, epoch: 7 | loss: 0.0957716\n",
      "\tspeed: 0.1157s/iter; left time: 4249.1404s\n",
      "\titers: 700, epoch: 7 | loss: 0.0731218\n",
      "\tspeed: 0.1143s/iter; left time: 4186.5099s\n",
      "\titers: 800, epoch: 7 | loss: 0.0727848\n",
      "\tspeed: 0.1144s/iter; left time: 4181.6516s\n",
      "\titers: 900, epoch: 7 | loss: 0.0822178\n",
      "\tspeed: 0.1148s/iter; left time: 4181.4862s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0866405\n",
      "\tspeed: 0.1143s/iter; left time: 4152.4235s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0839373\n",
      "\tspeed: 0.1160s/iter; left time: 4202.3776s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0864395\n",
      "\tspeed: 0.1117s/iter; left time: 4035.0252s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1001431\n",
      "\tspeed: 0.1156s/iter; left time: 4165.4831s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0870203\n",
      "\tspeed: 0.1135s/iter; left time: 4079.4270s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0783804\n",
      "\tspeed: 0.1138s/iter; left time: 4079.6902s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0840335\n",
      "\tspeed: 0.1152s/iter; left time: 4116.2249s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0848608\n",
      "\tspeed: 0.1136s/iter; left time: 4048.5709s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0887816\n",
      "\tspeed: 0.1140s/iter; left time: 4049.6803s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0716856\n",
      "\tspeed: 0.1126s/iter; left time: 3990.8076s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0809039\n",
      "\tspeed: 0.1142s/iter; left time: 4034.9774s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0809592\n",
      "\tspeed: 0.1132s/iter; left time: 3990.2337s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0854933\n",
      "\tspeed: 0.1133s/iter; left time: 3982.5259s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0758755\n",
      "\tspeed: 0.1122s/iter; left time: 3932.4847s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0947762\n",
      "\tspeed: 0.1110s/iter; left time: 3877.5687s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0809882\n",
      "\tspeed: 0.1130s/iter; left time: 3937.3034s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0749397\n",
      "\tspeed: 0.1143s/iter; left time: 3971.5130s\n",
      "Epoch: 7 cost time: 00h:05m:04.25s\n",
      "Epoch: 7 | Train Loss: 0.0833416 Vali Loss: 0.0886686 Test Loss: 0.0912572\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019771769642829895, rmse:0.14061212539672852, mae:0.08887572586536407, rse:0.531994640827179\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 168: 00h:46m:46.48s\n",
      "\n",
      "Intermediate time for IT: 03h:27m:29.51s\n",
      "\n",
      "Total time: 19h:53m:35.94s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>0.1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.2049</td>\n",
       "      <td>0.1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1445</td>\n",
       "      <td>0.0956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.0968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.0817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0227  0.1508  0.0954\n",
       "        96        0.0358  0.1892  0.1282\n",
       "        168       0.0377  0.1941  0.1336\n",
       "GB      24        0.0256  0.1599  0.1040\n",
       "        96        0.0420  0.2049  0.1405\n",
       "        168       0.0428  0.2068  0.1438\n",
       "ES      24        0.0107  0.1033  0.0665\n",
       "        96        0.0209  0.1445  0.0956\n",
       "        168       0.0211  0.1454  0.0968\n",
       "FR      24        0.0111  0.1052  0.0600\n",
       "        96        0.0185  0.1359  0.0817\n",
       "        168       0.0204  0.1428  0.0872\n",
       "IT      24        0.0108  0.1038  0.0620\n",
       "        96        0.0198  0.1406  0.0868\n",
       "        168       0.0198  0.1406  0.0889"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM/512'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm.csv'))\n",
    "timellm_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TimeLLM 336\n",
    "\n",
    "Sequence length 336."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "model = \"TimeLLM\"\n",
    "seq_len = 336\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_336.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-02 04:05:41,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 04:05:42,950] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 04:05:42,950] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 04:05:42,950] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 04:05:43,048] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 04:05:43,048] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 04:05:43,713] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 04:05:43,714] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 04:05:43,714] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 04:05:43,716] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 04:05:43,716] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 04:05:43,716] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 04:05:44,034] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 04:05:44,035] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,062] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,198] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 04:05:44,199] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,200] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,200] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 04:05:44,312] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 04:05:44,313] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,313] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 04:05:44,314] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6394bf6dd0>\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1751984\n",
      "\tspeed: 0.1735s/iter; left time: 15584.1708s\n",
      "\titers: 200, epoch: 1 | loss: 0.1696132\n",
      "\tspeed: 0.1309s/iter; left time: 11748.6723s\n",
      "\titers: 300, epoch: 1 | loss: 0.1651162\n",
      "\tspeed: 0.1313s/iter; left time: 11767.3423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1374285\n",
      "\tspeed: 0.1307s/iter; left time: 11699.3748s\n",
      "\titers: 500, epoch: 1 | loss: 0.0960834\n",
      "\tspeed: 0.1272s/iter; left time: 11378.5702s\n",
      "\titers: 600, epoch: 1 | loss: 0.0871328\n",
      "\tspeed: 0.1259s/iter; left time: 11242.6723s\n",
      "\titers: 700, epoch: 1 | loss: 0.1093957\n",
      "\tspeed: 0.1296s/iter; left time: 11562.0570s\n",
      "\titers: 800, epoch: 1 | loss: 0.0864986\n",
      "\tspeed: 0.1166s/iter; left time: 10388.0754s\n",
      "\titers: 900, epoch: 1 | loss: 0.1236022\n",
      "\tspeed: 0.1189s/iter; left time: 10586.8357s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1236182\n",
      "\tspeed: 0.1289s/iter; left time: 11463.7493s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1068401\n",
      "\tspeed: 0.1264s/iter; left time: 11230.1935s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1080805\n",
      "\tspeed: 0.1299s/iter; left time: 11522.8253s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1073034\n",
      "\tspeed: 0.1300s/iter; left time: 11524.5034s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0848949\n",
      "\tspeed: 0.1256s/iter; left time: 11122.0103s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1067144\n",
      "\tspeed: 0.1307s/iter; left time: 11555.8828s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0850505\n",
      "\tspeed: 0.1298s/iter; left time: 11467.4007s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0994569\n",
      "\tspeed: 0.1308s/iter; left time: 11535.5314s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062451\n",
      "\tspeed: 0.1303s/iter; left time: 11481.1729s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0976172\n",
      "\tspeed: 0.1307s/iter; left time: 11501.9078s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0936665\n",
      "\tspeed: 0.1300s/iter; left time: 11431.7189s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1149349\n",
      "\tspeed: 0.1103s/iter; left time: 9690.5952s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0990795\n",
      "\tspeed: 0.1246s/iter; left time: 10931.3024s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1037625\n",
      "\tspeed: 0.1204s/iter; left time: 10549.2546s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0966860\n",
      "\tspeed: 0.1097s/iter; left time: 9603.1471s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1019908\n",
      "\tspeed: 0.1092s/iter; left time: 9544.9901s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0780252\n",
      "\tspeed: 0.1089s/iter; left time: 9510.2132s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0944925\n",
      "\tspeed: 0.1090s/iter; left time: 9509.5887s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0935264\n",
      "\tspeed: 0.1088s/iter; left time: 9475.9938s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0859544\n",
      "\tspeed: 0.1091s/iter; left time: 9490.9660s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0855883\n",
      "\tspeed: 0.1229s/iter; left time: 10684.9461s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0829413\n",
      "\tspeed: 0.1277s/iter; left time: 11088.7397s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0780405\n",
      "\tspeed: 0.1285s/iter; left time: 11141.1512s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0817463\n",
      "\tspeed: 0.1259s/iter; left time: 10907.2366s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1099318\n",
      "\tspeed: 0.1302s/iter; left time: 11264.9607s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0809699\n",
      "\tspeed: 0.1291s/iter; left time: 11154.1947s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1028950\n",
      "\tspeed: 0.1301s/iter; left time: 11227.4162s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0928097\n",
      "\tspeed: 0.1261s/iter; left time: 10874.1060s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0896632\n",
      "\tspeed: 0.1254s/iter; left time: 10800.9263s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0757574\n",
      "\tspeed: 0.1244s/iter; left time: 10697.8411s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0994942\n",
      "\tspeed: 0.1246s/iter; left time: 10707.7015s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0882582\n",
      "\tspeed: 0.1291s/iter; left time: 11080.5087s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1168685\n",
      "\tspeed: 0.1298s/iter; left time: 11126.1313s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1015711\n",
      "\tspeed: 0.1307s/iter; left time: 11189.2471s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0811718\n",
      "\tspeed: 0.1312s/iter; left time: 11220.3463s\n",
      "Epoch: 1 cost time: 00h:09m:22.97s\n",
      "Epoch: 1 | Train Loss: 0.1040335 Vali Loss: 0.0961287 Test Loss: 0.0975929\n",
      "Validation loss decreased (inf --> 0.096129).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0796660\n",
      "\tspeed: 1.7371s/iter; left time: 148215.5008s\n",
      "\titers: 200, epoch: 2 | loss: 0.0931822\n",
      "\tspeed: 0.1176s/iter; left time: 10020.1966s\n",
      "\titers: 300, epoch: 2 | loss: 0.1017061\n",
      "\tspeed: 0.1195s/iter; left time: 10173.8280s\n",
      "\titers: 400, epoch: 2 | loss: 0.0949261\n",
      "\tspeed: 0.1142s/iter; left time: 9710.3122s\n",
      "\titers: 500, epoch: 2 | loss: 0.1168831\n",
      "\tspeed: 0.1137s/iter; left time: 9659.6283s\n",
      "\titers: 600, epoch: 2 | loss: 0.0772928\n",
      "\tspeed: 0.1134s/iter; left time: 9616.1355s\n",
      "\titers: 700, epoch: 2 | loss: 0.0832478\n",
      "\tspeed: 0.1129s/iter; left time: 9569.1699s\n",
      "\titers: 800, epoch: 2 | loss: 0.0981834\n",
      "\tspeed: 0.1166s/iter; left time: 9870.3455s\n",
      "\titers: 900, epoch: 2 | loss: 0.1037944\n",
      "\tspeed: 0.1193s/iter; left time: 10082.1692s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0856554\n",
      "\tspeed: 0.1135s/iter; left time: 9583.9284s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1003807\n",
      "\tspeed: 0.1193s/iter; left time: 10059.9384s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0889630\n",
      "\tspeed: 0.1208s/iter; left time: 10176.5481s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0855101\n",
      "\tspeed: 0.1167s/iter; left time: 9820.3638s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1153450\n",
      "\tspeed: 0.1152s/iter; left time: 9675.8513s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0956274\n",
      "\tspeed: 0.1067s/iter; left time: 8955.2998s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1119400\n",
      "\tspeed: 0.0961s/iter; left time: 8058.9779s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0828846\n",
      "\tspeed: 0.0962s/iter; left time: 8051.7727s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1111595\n",
      "\tspeed: 0.0961s/iter; left time: 8037.0539s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1014844\n",
      "\tspeed: 0.0963s/iter; left time: 8041.7422s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0966399\n",
      "\tspeed: 0.1111s/iter; left time: 9270.6148s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0997675\n",
      "\tspeed: 0.1125s/iter; left time: 9376.3954s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1031985\n",
      "\tspeed: 0.1125s/iter; left time: 9362.4143s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0954874\n",
      "\tspeed: 0.1132s/iter; left time: 9409.2818s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0890880\n",
      "\tspeed: 0.1094s/iter; left time: 9080.3776s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1061457\n",
      "\tspeed: 0.1010s/iter; left time: 8373.6327s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1096236\n",
      "\tspeed: 0.1133s/iter; left time: 9380.5049s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1097501\n",
      "\tspeed: 0.1127s/iter; left time: 9323.8457s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0776054\n",
      "\tspeed: 0.1133s/iter; left time: 9358.0218s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0837930\n",
      "\tspeed: 0.1125s/iter; left time: 9279.9873s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1149195\n",
      "\tspeed: 0.1127s/iter; left time: 9293.0983s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0764869\n",
      "\tspeed: 0.1148s/iter; left time: 9453.8410s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0779453\n",
      "\tspeed: 0.1141s/iter; left time: 9384.4575s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0975494\n",
      "\tspeed: 0.1144s/iter; left time: 9393.0479s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0804761\n",
      "\tspeed: 0.1153s/iter; left time: 9453.6404s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0919167\n",
      "\tspeed: 0.1151s/iter; left time: 9426.7968s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0746915\n",
      "\tspeed: 0.1150s/iter; left time: 9409.6590s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0837285\n",
      "\tspeed: 0.1133s/iter; left time: 9257.2814s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0917258\n",
      "\tspeed: 0.1142s/iter; left time: 9318.9132s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0715970\n",
      "\tspeed: 0.1145s/iter; left time: 9331.8389s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0857370\n",
      "\tspeed: 0.1136s/iter; left time: 9248.2053s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0920830\n",
      "\tspeed: 0.1140s/iter; left time: 9268.6945s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0768602\n",
      "\tspeed: 0.1140s/iter; left time: 9257.3731s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0806141\n",
      "\tspeed: 0.1144s/iter; left time: 9283.5268s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0979972\n",
      "\tspeed: 0.1138s/iter; left time: 9222.8458s\n",
      "Epoch: 2 cost time: 00h:08m:25.36s\n",
      "Epoch: 2 | Train Loss: 0.0885479 Vali Loss: 0.0917196 Test Loss: 0.0938499\n",
      "Validation loss decreased (0.096129 --> 0.091720).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0962270\n",
      "\tspeed: 1.5176s/iter; left time: 122666.1800s\n",
      "\titers: 200, epoch: 3 | loss: 0.0868109\n",
      "\tspeed: 0.1207s/iter; left time: 9740.0078s\n",
      "\titers: 300, epoch: 3 | loss: 0.0884279\n",
      "\tspeed: 0.1213s/iter; left time: 9781.4377s\n",
      "\titers: 400, epoch: 3 | loss: 0.0981889\n",
      "\tspeed: 0.1211s/iter; left time: 9751.3365s\n",
      "\titers: 500, epoch: 3 | loss: 0.0899171\n",
      "\tspeed: 0.1207s/iter; left time: 9709.9214s\n",
      "\titers: 600, epoch: 3 | loss: 0.0844401\n",
      "\tspeed: 0.1206s/iter; left time: 9689.5479s\n",
      "\titers: 700, epoch: 3 | loss: 0.0752369\n",
      "\tspeed: 0.1187s/iter; left time: 9520.6208s\n",
      "\titers: 800, epoch: 3 | loss: 0.1060660\n",
      "\tspeed: 0.1208s/iter; left time: 9680.7391s\n",
      "\titers: 900, epoch: 3 | loss: 0.0833049\n",
      "\tspeed: 0.1208s/iter; left time: 9668.7466s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0819281\n",
      "\tspeed: 0.1203s/iter; left time: 9615.2239s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0777918\n",
      "\tspeed: 0.1209s/iter; left time: 9652.0103s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0780214\n",
      "\tspeed: 0.1177s/iter; left time: 9387.0852s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0893477\n",
      "\tspeed: 0.1156s/iter; left time: 9203.5842s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1029789\n",
      "\tspeed: 0.1209s/iter; left time: 9618.1192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0834389\n",
      "\tspeed: 0.1209s/iter; left time: 9603.3986s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0866824\n",
      "\tspeed: 0.1183s/iter; left time: 9383.2547s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0818847\n",
      "\tspeed: 0.1208s/iter; left time: 9572.4971s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0906848\n",
      "\tspeed: 0.1213s/iter; left time: 9599.9935s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1146158\n",
      "\tspeed: 0.1195s/iter; left time: 9447.6256s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1075108\n",
      "\tspeed: 0.1056s/iter; left time: 8332.5540s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0804141\n",
      "\tspeed: 0.1122s/iter; left time: 8843.5249s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0912658\n",
      "\tspeed: 0.1183s/iter; left time: 9313.1841s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0801835\n",
      "\tspeed: 0.1185s/iter; left time: 9316.9041s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0916704\n",
      "\tspeed: 0.1210s/iter; left time: 9504.5358s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0994615\n",
      "\tspeed: 0.1174s/iter; left time: 9204.7229s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1034542\n",
      "\tspeed: 0.1202s/iter; left time: 9416.0022s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0897656\n",
      "\tspeed: 0.1209s/iter; left time: 9459.9946s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1089268\n",
      "\tspeed: 0.1208s/iter; left time: 9440.5900s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0789208\n",
      "\tspeed: 0.1212s/iter; left time: 9457.2291s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0762717\n",
      "\tspeed: 0.1165s/iter; left time: 9082.1749s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0869183\n",
      "\tspeed: 0.1207s/iter; left time: 9395.2146s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0751872\n",
      "\tspeed: 0.1154s/iter; left time: 8971.9236s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0878178\n",
      "\tspeed: 0.1213s/iter; left time: 9413.0328s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0656829\n",
      "\tspeed: 0.1207s/iter; left time: 9357.4105s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0847644\n",
      "\tspeed: 0.1191s/iter; left time: 9220.4443s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0789355\n",
      "\tspeed: 0.1208s/iter; left time: 9342.9600s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0871209\n",
      "\tspeed: 0.1179s/iter; left time: 9103.4060s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0978919\n",
      "\tspeed: 0.1205s/iter; left time: 9290.4399s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0924827\n",
      "\tspeed: 0.1217s/iter; left time: 9377.2864s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0813397\n",
      "\tspeed: 0.1194s/iter; left time: 9185.8177s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0784988\n",
      "\tspeed: 0.1202s/iter; left time: 9236.4669s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0991811\n",
      "\tspeed: 0.1209s/iter; left time: 9276.9542s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0861519\n",
      "\tspeed: 0.1210s/iter; left time: 9272.3883s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0760844\n",
      "\tspeed: 0.1200s/iter; left time: 9186.2755s\n",
      "Epoch: 3 cost time: 00h:08m:57.62s\n",
      "Epoch: 3 | Train Loss: 0.0849499 Vali Loss: 0.0901408 Test Loss: 0.0926513\n",
      "Validation loss decreased (0.091720 --> 0.090141).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1111427\n",
      "\tspeed: 1.5235s/iter; left time: 116290.6463s\n",
      "\titers: 200, epoch: 4 | loss: 0.0753410\n",
      "\tspeed: 0.1209s/iter; left time: 9216.7592s\n",
      "\titers: 300, epoch: 4 | loss: 0.0888413\n",
      "\tspeed: 0.1208s/iter; left time: 9197.5835s\n",
      "\titers: 400, epoch: 4 | loss: 0.0714968\n",
      "\tspeed: 0.1192s/iter; left time: 9065.1911s\n",
      "\titers: 500, epoch: 4 | loss: 0.0833365\n",
      "\tspeed: 0.1151s/iter; left time: 8739.0584s\n",
      "\titers: 600, epoch: 4 | loss: 0.0899843\n",
      "\tspeed: 0.1184s/iter; left time: 8976.1350s\n",
      "\titers: 700, epoch: 4 | loss: 0.0789592\n",
      "\tspeed: 0.1167s/iter; left time: 8841.6298s\n",
      "\titers: 800, epoch: 4 | loss: 0.0856384\n",
      "\tspeed: 0.1136s/iter; left time: 8592.4849s\n",
      "\titers: 900, epoch: 4 | loss: 0.0778323\n",
      "\tspeed: 0.1137s/iter; left time: 8585.3265s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0816742\n",
      "\tspeed: 0.1140s/iter; left time: 8599.2045s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0672043\n",
      "\tspeed: 0.1184s/iter; left time: 8920.9904s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0746364\n",
      "\tspeed: 0.1208s/iter; left time: 9087.3051s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0835876\n",
      "\tspeed: 0.1208s/iter; left time: 9074.5038s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0832614\n",
      "\tspeed: 0.1192s/iter; left time: 8945.4002s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0914626\n",
      "\tspeed: 0.1205s/iter; left time: 9032.8420s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0810632\n",
      "\tspeed: 0.1167s/iter; left time: 8731.8975s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0825682\n",
      "\tspeed: 0.1207s/iter; left time: 9019.9769s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0768290\n",
      "\tspeed: 0.1207s/iter; left time: 9009.0520s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1031834\n",
      "\tspeed: 0.1204s/iter; left time: 8974.2922s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0810642\n",
      "\tspeed: 0.1205s/iter; left time: 8968.9662s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1011447\n",
      "\tspeed: 0.1185s/iter; left time: 8809.8841s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0945584\n",
      "\tspeed: 0.1131s/iter; left time: 8393.2932s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0859294\n",
      "\tspeed: 0.1130s/iter; left time: 8373.3779s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0821534\n",
      "\tspeed: 0.1129s/iter; left time: 8356.1091s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0808725\n",
      "\tspeed: 0.1137s/iter; left time: 8405.3245s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0857516\n",
      "\tspeed: 0.1193s/iter; left time: 8808.3368s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0724460\n",
      "\tspeed: 0.1210s/iter; left time: 8918.9110s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0676562\n",
      "\tspeed: 0.1206s/iter; left time: 8881.6904s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0927680\n",
      "\tspeed: 0.1205s/iter; left time: 8858.0213s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0894543\n",
      "\tspeed: 0.1152s/iter; left time: 8461.9201s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0880243\n",
      "\tspeed: 0.1147s/iter; left time: 8409.3481s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0814270\n",
      "\tspeed: 0.1162s/iter; left time: 8507.9933s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0787635\n",
      "\tspeed: 0.1172s/iter; left time: 8570.4009s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0685097\n",
      "\tspeed: 0.1157s/iter; left time: 8452.7793s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0689826\n",
      "\tspeed: 0.1148s/iter; left time: 8372.5090s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1017262\n",
      "\tspeed: 0.1141s/iter; left time: 8312.6962s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0673666\n",
      "\tspeed: 0.1152s/iter; left time: 8376.7681s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0698210\n",
      "\tspeed: 0.1127s/iter; left time: 8184.9480s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0750384\n",
      "\tspeed: 0.1125s/iter; left time: 8157.1098s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0919478\n",
      "\tspeed: 0.1131s/iter; left time: 8189.3309s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0796407\n",
      "\tspeed: 0.0963s/iter; left time: 6968.1981s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0891761\n",
      "\tspeed: 0.1107s/iter; left time: 7993.9332s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0734849\n",
      "\tspeed: 0.1142s/iter; left time: 8236.1714s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0780938\n",
      "\tspeed: 0.1141s/iter; left time: 8221.2131s\n",
      "Epoch: 4 cost time: 00h:08m:43.54s\n",
      "Epoch: 4 | Train Loss: 0.0830537 Vali Loss: 0.0887350 Test Loss: 0.0919589\n",
      "Validation loss decreased (0.090141 --> 0.088735).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0724721\n",
      "\tspeed: 1.5060s/iter; left time: 108189.1676s\n",
      "\titers: 200, epoch: 5 | loss: 0.0816491\n",
      "\tspeed: 0.1202s/iter; left time: 8623.4440s\n",
      "\titers: 300, epoch: 5 | loss: 0.0865343\n",
      "\tspeed: 0.1167s/iter; left time: 8360.7473s\n",
      "\titers: 400, epoch: 5 | loss: 0.0748772\n",
      "\tspeed: 0.1129s/iter; left time: 8075.5640s\n",
      "\titers: 500, epoch: 5 | loss: 0.0879068\n",
      "\tspeed: 0.1137s/iter; left time: 8123.7226s\n",
      "\titers: 600, epoch: 5 | loss: 0.0776411\n",
      "\tspeed: 0.1134s/iter; left time: 8086.9740s\n",
      "\titers: 700, epoch: 5 | loss: 0.0837840\n",
      "\tspeed: 0.1134s/iter; left time: 8076.2918s\n",
      "\titers: 800, epoch: 5 | loss: 0.0696601\n",
      "\tspeed: 0.1163s/iter; left time: 8271.1694s\n",
      "\titers: 900, epoch: 5 | loss: 0.0858637\n",
      "\tspeed: 0.1159s/iter; left time: 8233.1103s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0875993\n",
      "\tspeed: 0.1137s/iter; left time: 8063.5210s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0880472\n",
      "\tspeed: 0.1159s/iter; left time: 8211.4916s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0787003\n",
      "\tspeed: 0.1140s/iter; left time: 8060.8014s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0790513\n",
      "\tspeed: 0.1149s/iter; left time: 8119.3239s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0656777\n",
      "\tspeed: 0.1213s/iter; left time: 8558.4860s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0692728\n",
      "\tspeed: 0.1212s/iter; left time: 8534.1719s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0867893\n",
      "\tspeed: 0.1185s/iter; left time: 8337.5441s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0772494\n",
      "\tspeed: 0.1151s/iter; left time: 8082.2563s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1018692\n",
      "\tspeed: 0.1172s/iter; left time: 8223.1548s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0822593\n",
      "\tspeed: 0.1211s/iter; left time: 8482.3100s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0723740\n",
      "\tspeed: 0.1204s/iter; left time: 8419.7675s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0827404\n",
      "\tspeed: 0.1177s/iter; left time: 8221.5918s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0853322\n",
      "\tspeed: 0.1207s/iter; left time: 8418.3833s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0850059\n",
      "\tspeed: 0.1212s/iter; left time: 8441.9546s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0737549\n",
      "\tspeed: 0.1212s/iter; left time: 8430.9857s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0851119\n",
      "\tspeed: 0.1207s/iter; left time: 8377.8651s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0840694\n",
      "\tspeed: 0.1202s/iter; left time: 8334.7514s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0876200\n",
      "\tspeed: 0.1209s/iter; left time: 8368.1360s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0861114\n",
      "\tspeed: 0.1200s/iter; left time: 8293.5198s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0650319\n",
      "\tspeed: 0.1171s/iter; left time: 8086.0150s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0898532\n",
      "\tspeed: 0.1208s/iter; left time: 8327.2123s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0896070\n",
      "\tspeed: 0.1181s/iter; left time: 8131.1862s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0580150\n",
      "\tspeed: 0.1154s/iter; left time: 7934.0417s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0968894\n",
      "\tspeed: 0.1192s/iter; left time: 8182.1327s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0927546\n",
      "\tspeed: 0.1209s/iter; left time: 8286.1156s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0884232\n",
      "\tspeed: 0.1204s/iter; left time: 8241.1341s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0773740\n",
      "\tspeed: 0.1149s/iter; left time: 7848.5766s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0735403\n",
      "\tspeed: 0.1157s/iter; left time: 7893.2557s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0843595\n",
      "\tspeed: 0.1185s/iter; left time: 8071.4490s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0890049\n",
      "\tspeed: 0.1212s/iter; left time: 8247.9663s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0733620\n",
      "\tspeed: 0.1180s/iter; left time: 8016.0967s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0826425\n",
      "\tspeed: 0.1195s/iter; left time: 8108.4777s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0942001\n",
      "\tspeed: 0.1175s/iter; left time: 7958.0431s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0662682\n",
      "\tspeed: 0.1208s/iter; left time: 8169.3140s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0639044\n",
      "\tspeed: 0.1202s/iter; left time: 8118.1535s\n",
      "Epoch: 5 cost time: 00h:08m:51.87s\n",
      "Epoch: 5 | Train Loss: 0.0819060 Vali Loss: 0.0899136 Test Loss: 0.0937484\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0834547\n",
      "\tspeed: 1.5025s/iter; left time: 101181.7385s\n",
      "\titers: 200, epoch: 6 | loss: 0.0788915\n",
      "\tspeed: 0.1156s/iter; left time: 7776.1217s\n",
      "\titers: 300, epoch: 6 | loss: 0.0766931\n",
      "\tspeed: 0.1185s/iter; left time: 7954.5460s\n",
      "\titers: 400, epoch: 6 | loss: 0.0621577\n",
      "\tspeed: 0.1160s/iter; left time: 7779.0340s\n",
      "\titers: 500, epoch: 6 | loss: 0.0798744\n",
      "\tspeed: 0.1150s/iter; left time: 7700.7145s\n",
      "\titers: 600, epoch: 6 | loss: 0.0737847\n",
      "\tspeed: 0.1144s/iter; left time: 7647.0265s\n",
      "\titers: 700, epoch: 6 | loss: 0.0843354\n",
      "\tspeed: 0.1210s/iter; left time: 8075.2940s\n",
      "\titers: 800, epoch: 6 | loss: 0.0722700\n",
      "\tspeed: 0.1209s/iter; left time: 8057.6134s\n",
      "\titers: 900, epoch: 6 | loss: 0.0824605\n",
      "\tspeed: 0.1207s/iter; left time: 8032.2200s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0940322\n",
      "\tspeed: 0.1201s/iter; left time: 7979.0409s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0984112\n",
      "\tspeed: 0.1182s/iter; left time: 7840.0682s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0816496\n",
      "\tspeed: 0.1212s/iter; left time: 8029.8885s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0897281\n",
      "\tspeed: 0.1208s/iter; left time: 7991.1979s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0707488\n",
      "\tspeed: 0.1212s/iter; left time: 8002.9449s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0827402\n",
      "\tspeed: 0.1209s/iter; left time: 7973.2661s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0788377\n",
      "\tspeed: 0.1208s/iter; left time: 7952.6845s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0708660\n",
      "\tspeed: 0.1208s/iter; left time: 7939.3029s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0858712\n",
      "\tspeed: 0.1144s/iter; left time: 7512.0945s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0694004\n",
      "\tspeed: 0.1157s/iter; left time: 7582.9526s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0808107\n",
      "\tspeed: 0.1208s/iter; left time: 7903.0900s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0673134\n",
      "\tspeed: 0.1139s/iter; left time: 7440.9977s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0682330\n",
      "\tspeed: 0.1151s/iter; left time: 7506.4159s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0918810\n",
      "\tspeed: 0.1209s/iter; left time: 7874.9710s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0851345\n",
      "\tspeed: 0.1208s/iter; left time: 7855.7979s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0815403\n",
      "\tspeed: 0.1209s/iter; left time: 7854.2735s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0941485\n",
      "\tspeed: 0.1208s/iter; left time: 7834.8281s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0907783\n",
      "\tspeed: 0.1209s/iter; left time: 7829.0331s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0615061\n",
      "\tspeed: 0.1212s/iter; left time: 7834.7400s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0898525\n",
      "\tspeed: 0.1187s/iter; left time: 7662.9349s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0763716\n",
      "\tspeed: 0.1052s/iter; left time: 6779.6397s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0835483\n",
      "\tspeed: 0.1076s/iter; left time: 6922.0729s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0802400\n",
      "\tspeed: 0.1186s/iter; left time: 7616.6857s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0705647\n",
      "\tspeed: 0.1210s/iter; left time: 7760.8542s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0637157\n",
      "\tspeed: 0.1209s/iter; left time: 7740.7803s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0887412\n",
      "\tspeed: 0.1211s/iter; left time: 7743.5506s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0846283\n",
      "\tspeed: 0.1158s/iter; left time: 7395.1846s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0793136\n",
      "\tspeed: 0.1203s/iter; left time: 7668.3205s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0903979\n",
      "\tspeed: 0.1206s/iter; left time: 7674.0616s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0845485\n",
      "\tspeed: 0.1208s/iter; left time: 7674.5317s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0887016\n",
      "\tspeed: 0.1209s/iter; left time: 7668.5008s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0647517\n",
      "\tspeed: 0.1188s/iter; left time: 7525.8907s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0810043\n",
      "\tspeed: 0.1141s/iter; left time: 7215.1798s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0847861\n",
      "\tspeed: 0.1206s/iter; left time: 7615.9642s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0607698\n",
      "\tspeed: 0.1209s/iter; left time: 7620.7176s\n",
      "Epoch: 6 cost time: 00h:08m:53.65s\n",
      "Epoch: 6 | Train Loss: 0.0808758 Vali Loss: 0.0904147 Test Loss: 0.0949829\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0776564\n",
      "\tspeed: 1.5165s/iter; left time: 95306.6113s\n",
      "\titers: 200, epoch: 7 | loss: 0.0784396\n",
      "\tspeed: 0.1208s/iter; left time: 7578.0014s\n",
      "\titers: 300, epoch: 7 | loss: 0.0935769\n",
      "\tspeed: 0.1209s/iter; left time: 7571.0067s\n",
      "\titers: 400, epoch: 7 | loss: 0.1011236\n",
      "\tspeed: 0.1210s/iter; left time: 7569.2014s\n",
      "\titers: 500, epoch: 7 | loss: 0.0993084\n",
      "\tspeed: 0.1199s/iter; left time: 7488.3781s\n",
      "\titers: 600, epoch: 7 | loss: 0.0678284\n",
      "\tspeed: 0.1153s/iter; left time: 7190.7073s\n",
      "\titers: 700, epoch: 7 | loss: 0.0915019\n",
      "\tspeed: 0.1158s/iter; left time: 7210.0226s\n",
      "\titers: 800, epoch: 7 | loss: 0.0908480\n",
      "\tspeed: 0.1212s/iter; left time: 7534.5951s\n",
      "\titers: 900, epoch: 7 | loss: 0.1084777\n",
      "\tspeed: 0.1211s/iter; left time: 7514.2712s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0742308\n",
      "\tspeed: 0.1202s/iter; left time: 7445.9882s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0900566\n",
      "\tspeed: 0.1200s/iter; left time: 7419.1822s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0717520\n",
      "\tspeed: 0.1169s/iter; left time: 7216.7687s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0896325\n",
      "\tspeed: 0.1197s/iter; left time: 7377.1597s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0751720\n",
      "\tspeed: 0.1205s/iter; left time: 7416.6328s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0697805\n",
      "\tspeed: 0.1168s/iter; left time: 7178.5825s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0777683\n",
      "\tspeed: 0.1154s/iter; left time: 7076.2840s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0931582\n",
      "\tspeed: 0.1211s/iter; left time: 7415.2672s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0872888\n",
      "\tspeed: 0.1206s/iter; left time: 7373.5675s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0675946\n",
      "\tspeed: 0.1210s/iter; left time: 7385.9729s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0718096\n",
      "\tspeed: 0.1211s/iter; left time: 7381.1314s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0726633\n",
      "\tspeed: 0.1209s/iter; left time: 7358.4123s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0895870\n",
      "\tspeed: 0.1173s/iter; left time: 7128.2462s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0828907\n",
      "\tspeed: 0.1143s/iter; left time: 6931.1229s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0906809\n",
      "\tspeed: 0.1088s/iter; left time: 6585.8205s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0836006\n",
      "\tspeed: 0.1187s/iter; left time: 7173.4855s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0730073\n",
      "\tspeed: 0.0993s/iter; left time: 5990.1222s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0711934\n",
      "\tspeed: 0.1122s/iter; left time: 6760.5423s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0955372\n",
      "\tspeed: 0.1142s/iter; left time: 6867.4480s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0695751\n",
      "\tspeed: 0.1138s/iter; left time: 6831.3028s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0690845\n",
      "\tspeed: 0.1166s/iter; left time: 6987.5757s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0937775\n",
      "\tspeed: 0.1209s/iter; left time: 7234.3981s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1030593\n",
      "\tspeed: 0.1212s/iter; left time: 7242.7967s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0797456\n",
      "\tspeed: 0.1211s/iter; left time: 7225.2851s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0739272\n",
      "\tspeed: 0.1209s/iter; left time: 7199.8657s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0762674\n",
      "\tspeed: 0.1210s/iter; left time: 7192.6481s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0890733\n",
      "\tspeed: 0.1211s/iter; left time: 7186.6449s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0628304\n",
      "\tspeed: 0.1172s/iter; left time: 6940.8726s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0651186\n",
      "\tspeed: 0.1178s/iter; left time: 6968.0551s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0825545\n",
      "\tspeed: 0.1209s/iter; left time: 7139.1120s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0898772\n",
      "\tspeed: 0.1208s/iter; left time: 7121.8930s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0976483\n",
      "\tspeed: 0.1194s/iter; left time: 7024.7355s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0855372\n",
      "\tspeed: 0.1156s/iter; left time: 6789.4006s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0670111\n",
      "\tspeed: 0.1152s/iter; left time: 6754.4861s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0837409\n",
      "\tspeed: 0.1206s/iter; left time: 7062.9214s\n",
      "Epoch: 7 cost time: 00h:08m:52.25s\n",
      "Epoch: 7 | Train Loss: 0.0799692 Vali Loss: 0.0892061 Test Loss: 0.0939316\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0858032\n",
      "\tspeed: 1.4986s/iter; left time: 87440.8666s\n",
      "\titers: 200, epoch: 8 | loss: 0.0891609\n",
      "\tspeed: 0.1206s/iter; left time: 7025.2453s\n",
      "\titers: 300, epoch: 8 | loss: 0.0738470\n",
      "\tspeed: 0.1218s/iter; left time: 7085.3730s\n",
      "\titers: 400, epoch: 8 | loss: 0.0654960\n",
      "\tspeed: 0.1156s/iter; left time: 6710.4905s\n",
      "\titers: 500, epoch: 8 | loss: 0.0660103\n",
      "\tspeed: 0.1153s/iter; left time: 6682.4981s\n",
      "\titers: 600, epoch: 8 | loss: 0.0857095\n",
      "\tspeed: 0.1220s/iter; left time: 7056.9123s\n",
      "\titers: 700, epoch: 8 | loss: 0.0678255\n",
      "\tspeed: 0.1206s/iter; left time: 6962.1952s\n",
      "\titers: 800, epoch: 8 | loss: 0.0832420\n",
      "\tspeed: 0.1209s/iter; left time: 6970.8135s\n",
      "\titers: 900, epoch: 8 | loss: 0.0736113\n",
      "\tspeed: 0.1211s/iter; left time: 6969.5437s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0566616\n",
      "\tspeed: 0.1201s/iter; left time: 6897.1962s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0793718\n",
      "\tspeed: 0.1152s/iter; left time: 6607.7398s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0917201\n",
      "\tspeed: 0.1205s/iter; left time: 6898.9591s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0691053\n",
      "\tspeed: 0.1183s/iter; left time: 6761.3995s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0720860\n",
      "\tspeed: 0.1199s/iter; left time: 6840.1601s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0792802\n",
      "\tspeed: 0.1200s/iter; left time: 6835.0838s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0826451\n",
      "\tspeed: 0.1216s/iter; left time: 6915.1612s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0810083\n",
      "\tspeed: 0.1210s/iter; left time: 6867.3659s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1009962\n",
      "\tspeed: 0.1204s/iter; left time: 6820.7911s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0958357\n",
      "\tspeed: 0.1183s/iter; left time: 6689.8956s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0718374\n",
      "\tspeed: 0.1189s/iter; left time: 6709.0451s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0927144\n",
      "\tspeed: 0.1200s/iter; left time: 6761.6151s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0949822\n",
      "\tspeed: 0.1210s/iter; left time: 6807.8392s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0678577\n",
      "\tspeed: 0.1204s/iter; left time: 6761.3589s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0812559\n",
      "\tspeed: 0.1168s/iter; left time: 6543.9464s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0822914\n",
      "\tspeed: 0.1195s/iter; left time: 6684.1770s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0643865\n",
      "\tspeed: 0.1171s/iter; left time: 6541.8708s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0984127\n",
      "\tspeed: 0.1209s/iter; left time: 6740.7699s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0899339\n",
      "\tspeed: 0.1208s/iter; left time: 6724.1681s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0574796\n",
      "\tspeed: 0.1213s/iter; left time: 6738.5486s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0782979\n",
      "\tspeed: 0.1184s/iter; left time: 6563.7757s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0836753\n",
      "\tspeed: 0.1175s/iter; left time: 6503.8366s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0640419\n",
      "\tspeed: 0.1172s/iter; left time: 6473.2469s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0771721\n",
      "\tspeed: 0.1215s/iter; left time: 6700.6903s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0778379\n",
      "\tspeed: 0.1222s/iter; left time: 6727.0175s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0805037\n",
      "\tspeed: 0.1208s/iter; left time: 6635.9473s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0769201\n",
      "\tspeed: 0.1209s/iter; left time: 6632.4699s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0943394\n",
      "\tspeed: 0.1207s/iter; left time: 6608.6204s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0953691\n",
      "\tspeed: 0.1208s/iter; left time: 6600.3599s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0673332\n",
      "\tspeed: 0.1209s/iter; left time: 6596.0071s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0977372\n",
      "\tspeed: 0.1215s/iter; left time: 6616.9175s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0735301\n",
      "\tspeed: 0.1205s/iter; left time: 6550.7294s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0813443\n",
      "\tspeed: 0.1207s/iter; left time: 6548.4466s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0507200\n",
      "\tspeed: 0.1207s/iter; left time: 6534.2047s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0746277\n",
      "\tspeed: 0.1213s/iter; left time: 6557.3912s\n",
      "Epoch: 8 cost time: 00h:08m:59.78s\n",
      "Epoch: 8 | Train Loss: 0.0791088 Vali Loss: 0.0891922 Test Loss: 0.0933998\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0883823\n",
      "\tspeed: 1.4926s/iter; left time: 80382.3245s\n",
      "\titers: 200, epoch: 9 | loss: 0.0742780\n",
      "\tspeed: 0.1143s/iter; left time: 6142.0410s\n",
      "\titers: 300, epoch: 9 | loss: 0.0729224\n",
      "\tspeed: 0.1201s/iter; left time: 6441.2238s\n",
      "\titers: 400, epoch: 9 | loss: 0.0771885\n",
      "\tspeed: 0.1207s/iter; left time: 6463.8236s\n",
      "\titers: 500, epoch: 9 | loss: 0.0633648\n",
      "\tspeed: 0.1197s/iter; left time: 6400.8904s\n",
      "\titers: 600, epoch: 9 | loss: 0.0877349\n",
      "\tspeed: 0.1186s/iter; left time: 6325.5573s\n",
      "\titers: 700, epoch: 9 | loss: 0.0688003\n",
      "\tspeed: 0.1209s/iter; left time: 6437.5326s\n",
      "\titers: 800, epoch: 9 | loss: 0.0829574\n",
      "\tspeed: 0.1209s/iter; left time: 6426.9728s\n",
      "\titers: 900, epoch: 9 | loss: 0.0724092\n",
      "\tspeed: 0.1204s/iter; left time: 6387.5727s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0991572\n",
      "\tspeed: 0.1208s/iter; left time: 6396.3747s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0887690\n",
      "\tspeed: 0.1202s/iter; left time: 6351.6722s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0728898\n",
      "\tspeed: 0.1207s/iter; left time: 6365.8803s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0847884\n",
      "\tspeed: 0.1209s/iter; left time: 6367.3844s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0939002\n",
      "\tspeed: 0.1167s/iter; left time: 6131.6376s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0808990\n",
      "\tspeed: 0.1090s/iter; left time: 5716.1509s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0823213\n",
      "\tspeed: 0.1135s/iter; left time: 5939.6949s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0796168\n",
      "\tspeed: 0.1195s/iter; left time: 6245.6759s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0721289\n",
      "\tspeed: 0.1208s/iter; left time: 6300.5310s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0679318\n",
      "\tspeed: 0.1194s/iter; left time: 6216.6185s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0717794\n",
      "\tspeed: 0.1184s/iter; left time: 6152.9969s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0890689\n",
      "\tspeed: 0.1140s/iter; left time: 5910.6588s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0657449\n",
      "\tspeed: 0.1195s/iter; left time: 6186.4854s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0594654\n",
      "\tspeed: 0.1209s/iter; left time: 6245.2868s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0709143\n",
      "\tspeed: 0.1209s/iter; left time: 6233.7558s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0705610\n",
      "\tspeed: 0.1191s/iter; left time: 6130.5124s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0897841\n",
      "\tspeed: 0.1159s/iter; left time: 5952.4614s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0816959\n",
      "\tspeed: 0.1198s/iter; left time: 6140.9849s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0750316\n",
      "\tspeed: 0.1207s/iter; left time: 6176.0496s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0815326\n",
      "\tspeed: 0.1208s/iter; left time: 6169.1837s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0938492\n",
      "\tspeed: 0.1186s/iter; left time: 6043.0369s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0800535\n",
      "\tspeed: 0.1145s/iter; left time: 5825.0162s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0835484\n",
      "\tspeed: 0.1131s/iter; left time: 5737.9751s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0548027\n",
      "\tspeed: 0.1160s/iter; left time: 5875.0380s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0590655\n",
      "\tspeed: 0.1189s/iter; left time: 6010.3775s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0721123\n",
      "\tspeed: 0.1180s/iter; left time: 5954.1996s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0877456\n",
      "\tspeed: 0.1141s/iter; left time: 5744.7532s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0746561\n",
      "\tspeed: 0.1183s/iter; left time: 5943.7809s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0877061\n",
      "\tspeed: 0.1192s/iter; left time: 5979.5753s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0804281\n",
      "\tspeed: 0.1201s/iter; left time: 6011.7280s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0666070\n",
      "\tspeed: 0.1166s/iter; left time: 5823.0049s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0842516\n",
      "\tspeed: 0.1181s/iter; left time: 5886.4829s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0634511\n",
      "\tspeed: 0.1156s/iter; left time: 5751.5241s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0862772\n",
      "\tspeed: 0.1136s/iter; left time: 5639.1242s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0666628\n",
      "\tspeed: 0.1130s/iter; left time: 5600.0875s\n",
      "Epoch: 9 cost time: 00h:08m:51.02s\n",
      "Epoch: 9 | Train Loss: 0.0782179 Vali Loss: 0.0899105 Test Loss: 0.0940169\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02172875590622425, rmse:0.14740677177906036, mae:0.0919589027762413, rse:0.5206128358840942\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 24: 01h:40m:54.77s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 143525\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-02 05:46:36,884] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 05:46:38,092] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 05:46:38,092] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 05:46:38,093] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 05:46:38,196] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 05:46:38,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 05:46:38,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 05:46:38,864] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 05:46:38,864] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 05:46:38,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 05:46:38,865] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 05:46:38,865] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 05:46:39,195] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 05:46:39,196] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,197] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,323] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 05:46:39,324] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,324] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,324] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 05:46:39,441] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 05:46:39,443] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,443] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 05:46:39,444] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9778b9ae50>\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1796905\n",
      "\tspeed: 0.1577s/iter; left time: 14133.7593s\n",
      "\titers: 200, epoch: 1 | loss: 0.1528330\n",
      "\tspeed: 0.1278s/iter; left time: 11439.1865s\n",
      "\titers: 300, epoch: 1 | loss: 0.1636411\n",
      "\tspeed: 0.1276s/iter; left time: 11403.7132s\n",
      "\titers: 400, epoch: 1 | loss: 0.1717221\n",
      "\tspeed: 0.1243s/iter; left time: 11104.3602s\n",
      "\titers: 500, epoch: 1 | loss: 0.1625754\n",
      "\tspeed: 0.1295s/iter; left time: 11553.6505s\n",
      "\titers: 600, epoch: 1 | loss: 0.1756469\n",
      "\tspeed: 0.1281s/iter; left time: 11417.1809s\n",
      "\titers: 700, epoch: 1 | loss: 0.1374525\n",
      "\tspeed: 0.1274s/iter; left time: 11335.4492s\n",
      "\titers: 800, epoch: 1 | loss: 0.1207612\n",
      "\tspeed: 0.1274s/iter; left time: 11324.7311s\n",
      "\titers: 900, epoch: 1 | loss: 0.1471106\n",
      "\tspeed: 0.1264s/iter; left time: 11227.5124s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1346920\n",
      "\tspeed: 0.1189s/iter; left time: 10544.4150s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1261072\n",
      "\tspeed: 0.1074s/iter; left time: 9516.7964s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1214257\n",
      "\tspeed: 0.1180s/iter; left time: 10447.1125s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1278190\n",
      "\tspeed: 0.1280s/iter; left time: 11318.8812s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1183581\n",
      "\tspeed: 0.1278s/iter; left time: 11283.1554s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1059927\n",
      "\tspeed: 0.1275s/iter; left time: 11248.9772s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1225744\n",
      "\tspeed: 0.1286s/iter; left time: 11326.9879s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1184389\n",
      "\tspeed: 0.1284s/iter; left time: 11298.2002s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1273916\n",
      "\tspeed: 0.1280s/iter; left time: 11254.2115s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1305389\n",
      "\tspeed: 0.1272s/iter; left time: 11166.4987s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1092692\n",
      "\tspeed: 0.1274s/iter; left time: 11172.1297s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1127294\n",
      "\tspeed: 0.1260s/iter; left time: 11039.1152s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1192679\n",
      "\tspeed: 0.1277s/iter; left time: 11177.4816s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1197385\n",
      "\tspeed: 0.1074s/iter; left time: 9389.1119s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1230457\n",
      "\tspeed: 0.1143s/iter; left time: 9981.0137s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1455796\n",
      "\tspeed: 0.1242s/iter; left time: 10829.2640s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1400923\n",
      "\tspeed: 0.1104s/iter; left time: 9615.3961s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1138726\n",
      "\tspeed: 0.1156s/iter; left time: 10056.6128s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1174629\n",
      "\tspeed: 0.1278s/iter; left time: 11106.0899s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1184106\n",
      "\tspeed: 0.1274s/iter; left time: 11058.7338s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1312887\n",
      "\tspeed: 0.1269s/iter; left time: 11005.5283s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1324136\n",
      "\tspeed: 0.1278s/iter; left time: 11071.3846s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1211721\n",
      "\tspeed: 0.1211s/iter; left time: 10473.4759s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1178138\n",
      "\tspeed: 0.1249s/iter; left time: 10794.9082s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1224443\n",
      "\tspeed: 0.1261s/iter; left time: 10881.3066s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1183141\n",
      "\tspeed: 0.1274s/iter; left time: 10980.9751s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1242890\n",
      "\tspeed: 0.1273s/iter; left time: 10958.4074s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1144471\n",
      "\tspeed: 0.1281s/iter; left time: 11012.8003s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1015655\n",
      "\tspeed: 0.1274s/iter; left time: 10939.5922s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1111848\n",
      "\tspeed: 0.1269s/iter; left time: 10891.3807s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1316575\n",
      "\tspeed: 0.1284s/iter; left time: 11005.7130s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1088002\n",
      "\tspeed: 0.1266s/iter; left time: 10841.2923s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1164419\n",
      "\tspeed: 0.1271s/iter; left time: 10870.0765s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1259046\n",
      "\tspeed: 0.1272s/iter; left time: 10862.7851s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1085604\n",
      "\tspeed: 0.1257s/iter; left time: 10722.8313s\n",
      "Epoch: 1 cost time: 00h:09m:20.10s\n",
      "Epoch: 1 | Train Loss: 0.1268652 Vali Loss: 0.1215782 Test Loss: 0.1305961\n",
      "Validation loss decreased (inf --> 0.121578).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0937665\n",
      "\tspeed: 1.7150s/iter; left time: 145973.5313s\n",
      "\titers: 200, epoch: 2 | loss: 0.1341150\n",
      "\tspeed: 0.1166s/iter; left time: 9911.0785s\n",
      "\titers: 300, epoch: 2 | loss: 0.1132426\n",
      "\tspeed: 0.1154s/iter; left time: 9796.2151s\n",
      "\titers: 400, epoch: 2 | loss: 0.1143301\n",
      "\tspeed: 0.1156s/iter; left time: 9801.1368s\n",
      "\titers: 500, epoch: 2 | loss: 0.0936279\n",
      "\tspeed: 0.1152s/iter; left time: 9760.1670s\n",
      "\titers: 600, epoch: 2 | loss: 0.1210652\n",
      "\tspeed: 0.1165s/iter; left time: 9855.5831s\n",
      "\titers: 700, epoch: 2 | loss: 0.1088079\n",
      "\tspeed: 0.1156s/iter; left time: 9767.0333s\n",
      "\titers: 800, epoch: 2 | loss: 0.1030090\n",
      "\tspeed: 0.1160s/iter; left time: 9795.4755s\n",
      "\titers: 900, epoch: 2 | loss: 0.1108489\n",
      "\tspeed: 0.1152s/iter; left time: 9711.7447s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1083097\n",
      "\tspeed: 0.1155s/iter; left time: 9728.4191s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1051919\n",
      "\tspeed: 0.1161s/iter; left time: 9761.8822s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1058292\n",
      "\tspeed: 0.1166s/iter; left time: 9794.1186s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1199586\n",
      "\tspeed: 0.1125s/iter; left time: 9438.4597s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1058512\n",
      "\tspeed: 0.1161s/iter; left time: 9733.0043s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1033324\n",
      "\tspeed: 0.1153s/iter; left time: 9651.2989s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1068781\n",
      "\tspeed: 0.1155s/iter; left time: 9653.7709s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1302516\n",
      "\tspeed: 0.1162s/iter; left time: 9703.8041s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0881851\n",
      "\tspeed: 0.1156s/iter; left time: 9644.9497s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1368857\n",
      "\tspeed: 0.1155s/iter; left time: 9624.0595s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1057539\n",
      "\tspeed: 0.1158s/iter; left time: 9637.0728s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1266332\n",
      "\tspeed: 0.1150s/iter; left time: 9559.3286s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1175085\n",
      "\tspeed: 0.1074s/iter; left time: 8920.0190s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0942677\n",
      "\tspeed: 0.1158s/iter; left time: 9605.4521s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1120734\n",
      "\tspeed: 0.1093s/iter; left time: 9050.4594s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1228208\n",
      "\tspeed: 0.1142s/iter; left time: 9444.7890s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1139171\n",
      "\tspeed: 0.1172s/iter; left time: 9685.9896s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1344807\n",
      "\tspeed: 0.1079s/iter; left time: 8904.6353s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0970470\n",
      "\tspeed: 0.1166s/iter; left time: 9607.0546s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1221787\n",
      "\tspeed: 0.1143s/iter; left time: 9411.3897s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1317635\n",
      "\tspeed: 0.1162s/iter; left time: 9555.0655s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1116549\n",
      "\tspeed: 0.1154s/iter; left time: 9472.8397s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1177123\n",
      "\tspeed: 0.1151s/iter; left time: 9437.1712s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1062045\n",
      "\tspeed: 0.1152s/iter; left time: 9438.3001s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1368778\n",
      "\tspeed: 0.1154s/iter; left time: 9442.2239s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1050819\n",
      "\tspeed: 0.1156s/iter; left time: 9450.0167s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0943935\n",
      "\tspeed: 0.1152s/iter; left time: 9398.9143s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1096257\n",
      "\tspeed: 0.1152s/iter; left time: 9387.5670s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1289937\n",
      "\tspeed: 0.1157s/iter; left time: 9420.3089s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1044328\n",
      "\tspeed: 0.1159s/iter; left time: 9425.1984s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1009454\n",
      "\tspeed: 0.1122s/iter; left time: 9109.3106s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1349109\n",
      "\tspeed: 0.1134s/iter; left time: 9199.0742s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1130775\n",
      "\tspeed: 0.1154s/iter; left time: 9346.9434s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0991431\n",
      "\tspeed: 0.0968s/iter; left time: 7830.6714s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1101747\n",
      "\tspeed: 0.0967s/iter; left time: 7816.9526s\n",
      "Epoch: 2 cost time: 00h:08m:31.10s\n",
      "Epoch: 2 | Train Loss: 0.1110525 Vali Loss: 0.1208038 Test Loss: 0.1302489\n",
      "Validation loss decreased (0.121578 --> 0.120804).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1169660\n",
      "\tspeed: 1.4572s/iter; left time: 117493.7390s\n",
      "\titers: 200, epoch: 3 | loss: 0.1078827\n",
      "\tspeed: 0.1170s/iter; left time: 9421.6242s\n",
      "\titers: 300, epoch: 3 | loss: 0.1039631\n",
      "\tspeed: 0.1156s/iter; left time: 9301.5319s\n",
      "\titers: 400, epoch: 3 | loss: 0.1180947\n",
      "\tspeed: 0.1156s/iter; left time: 9285.7093s\n",
      "\titers: 500, epoch: 3 | loss: 0.1180327\n",
      "\tspeed: 0.1161s/iter; left time: 9313.3744s\n",
      "\titers: 600, epoch: 3 | loss: 0.0924053\n",
      "\tspeed: 0.1146s/iter; left time: 9185.8182s\n",
      "\titers: 700, epoch: 3 | loss: 0.1207208\n",
      "\tspeed: 0.1144s/iter; left time: 9155.2845s\n",
      "\titers: 800, epoch: 3 | loss: 0.1209414\n",
      "\tspeed: 0.1149s/iter; left time: 9184.5775s\n",
      "\titers: 900, epoch: 3 | loss: 0.1271856\n",
      "\tspeed: 0.1151s/iter; left time: 9192.0041s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1029982\n",
      "\tspeed: 0.1169s/iter; left time: 9321.4557s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1220193\n",
      "\tspeed: 0.1147s/iter; left time: 9136.6739s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0876944\n",
      "\tspeed: 0.1166s/iter; left time: 9272.6736s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0800447\n",
      "\tspeed: 0.1177s/iter; left time: 9346.4810s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1022078\n",
      "\tspeed: 0.1156s/iter; left time: 9167.7192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0931360\n",
      "\tspeed: 0.1172s/iter; left time: 9286.0300s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0966400\n",
      "\tspeed: 0.1152s/iter; left time: 9119.7205s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1089689\n",
      "\tspeed: 0.1145s/iter; left time: 9049.5219s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1262760\n",
      "\tspeed: 0.1156s/iter; left time: 9123.7408s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1214038\n",
      "\tspeed: 0.1169s/iter; left time: 9216.6460s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1131916\n",
      "\tspeed: 0.1180s/iter; left time: 9291.4135s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0910320\n",
      "\tspeed: 0.1178s/iter; left time: 9262.6729s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1071264\n",
      "\tspeed: 0.1164s/iter; left time: 9138.9622s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1228675\n",
      "\tspeed: 0.1169s/iter; left time: 9169.1644s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1094589\n",
      "\tspeed: 0.1165s/iter; left time: 9124.5763s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0970292\n",
      "\tspeed: 0.1196s/iter; left time: 9357.8439s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1208833\n",
      "\tspeed: 0.1177s/iter; left time: 9196.2377s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1006875\n",
      "\tspeed: 0.1178s/iter; left time: 9192.0300s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0891459\n",
      "\tspeed: 0.1157s/iter; left time: 9016.8123s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1074533\n",
      "\tspeed: 0.1165s/iter; left time: 9070.8716s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1291854\n",
      "\tspeed: 0.1162s/iter; left time: 9029.8660s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1223493\n",
      "\tspeed: 0.1171s/iter; left time: 9092.2994s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1235415\n",
      "\tspeed: 0.1169s/iter; left time: 9061.9804s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0978686\n",
      "\tspeed: 0.1164s/iter; left time: 9014.0719s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1255539\n",
      "\tspeed: 0.1168s/iter; left time: 9030.1651s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0985570\n",
      "\tspeed: 0.1172s/iter; left time: 9052.8040s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0875186\n",
      "\tspeed: 0.1137s/iter; left time: 8770.0572s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0979343\n",
      "\tspeed: 0.1147s/iter; left time: 8833.0443s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1091715\n",
      "\tspeed: 0.1158s/iter; left time: 8909.1244s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1004776\n",
      "\tspeed: 0.1161s/iter; left time: 8921.2142s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0880002\n",
      "\tspeed: 0.1155s/iter; left time: 8859.3549s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1066579\n",
      "\tspeed: 0.1155s/iter; left time: 8847.4811s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1162613\n",
      "\tspeed: 0.1103s/iter; left time: 8442.6629s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1064103\n",
      "\tspeed: 0.1093s/iter; left time: 8355.1387s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1064245\n",
      "\tspeed: 0.1126s/iter; left time: 8592.9947s\n",
      "Epoch: 3 cost time: 00h:08m:40.34s\n",
      "Epoch: 3 | Train Loss: 0.1070935 Vali Loss: 0.1214075 Test Loss: 0.1343616\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0998068\n",
      "\tspeed: 1.4499s/iter; left time: 110404.1637s\n",
      "\titers: 200, epoch: 4 | loss: 0.0908893\n",
      "\tspeed: 0.1157s/iter; left time: 8798.1010s\n",
      "\titers: 300, epoch: 4 | loss: 0.1107343\n",
      "\tspeed: 0.1172s/iter; left time: 8901.5089s\n",
      "\titers: 400, epoch: 4 | loss: 0.1227403\n",
      "\tspeed: 0.1148s/iter; left time: 8707.7615s\n",
      "\titers: 500, epoch: 4 | loss: 0.1063603\n",
      "\tspeed: 0.1164s/iter; left time: 8817.2421s\n",
      "\titers: 600, epoch: 4 | loss: 0.1203433\n",
      "\tspeed: 0.1154s/iter; left time: 8731.4206s\n",
      "\titers: 700, epoch: 4 | loss: 0.1365832\n",
      "\tspeed: 0.1154s/iter; left time: 8720.3389s\n",
      "\titers: 800, epoch: 4 | loss: 0.0913843\n",
      "\tspeed: 0.1152s/iter; left time: 8693.4082s\n",
      "\titers: 900, epoch: 4 | loss: 0.1054852\n",
      "\tspeed: 0.1156s/iter; left time: 8711.6342s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0973554\n",
      "\tspeed: 0.1146s/iter; left time: 8626.5125s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0843453\n",
      "\tspeed: 0.1155s/iter; left time: 8677.2394s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0939158\n",
      "\tspeed: 0.1066s/iter; left time: 8001.1407s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0991926\n",
      "\tspeed: 0.1078s/iter; left time: 8079.6909s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0926170\n",
      "\tspeed: 0.1164s/iter; left time: 8710.1360s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0980558\n",
      "\tspeed: 0.1150s/iter; left time: 8592.9991s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1088924\n",
      "\tspeed: 0.1165s/iter; left time: 8692.9161s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1093120\n",
      "\tspeed: 0.1152s/iter; left time: 8586.7060s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1005844\n",
      "\tspeed: 0.1150s/iter; left time: 8557.7215s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1107281\n",
      "\tspeed: 0.1146s/iter; left time: 8519.5482s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1060532\n",
      "\tspeed: 0.1163s/iter; left time: 8637.1994s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0904001\n",
      "\tspeed: 0.1151s/iter; left time: 8531.3305s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1109696\n",
      "\tspeed: 0.1162s/iter; left time: 8605.2239s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1122366\n",
      "\tspeed: 0.0992s/iter; left time: 7336.7084s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0989136\n",
      "\tspeed: 0.1142s/iter; left time: 8431.5912s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0947601\n",
      "\tspeed: 0.1094s/iter; left time: 8071.1090s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1021152\n",
      "\tspeed: 0.1161s/iter; left time: 8547.9752s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1003583\n",
      "\tspeed: 0.1147s/iter; left time: 8435.3343s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1050423\n",
      "\tspeed: 0.1150s/iter; left time: 8446.1468s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0937110\n",
      "\tspeed: 0.1147s/iter; left time: 8410.0946s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1102281\n",
      "\tspeed: 0.1137s/iter; left time: 8324.9367s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1013796\n",
      "\tspeed: 0.1164s/iter; left time: 8513.4038s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0967339\n",
      "\tspeed: 0.1167s/iter; left time: 8526.7652s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1165496\n",
      "\tspeed: 0.1159s/iter; left time: 8452.3091s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1168198\n",
      "\tspeed: 0.1157s/iter; left time: 8426.7270s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0818577\n",
      "\tspeed: 0.1156s/iter; left time: 8406.8334s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0995615\n",
      "\tspeed: 0.1147s/iter; left time: 8332.7640s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1094128\n",
      "\tspeed: 0.1161s/iter; left time: 8419.6498s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0862050\n",
      "\tspeed: 0.1141s/iter; left time: 8262.7599s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0992920\n",
      "\tspeed: 0.1144s/iter; left time: 8276.9377s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1077046\n",
      "\tspeed: 0.1143s/iter; left time: 8259.7362s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0952680\n",
      "\tspeed: 0.1147s/iter; left time: 8273.2550s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1204872\n",
      "\tspeed: 0.1153s/iter; left time: 8305.8857s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1002148\n",
      "\tspeed: 0.1159s/iter; left time: 8335.4637s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0981565\n",
      "\tspeed: 0.1051s/iter; left time: 7553.5532s\n",
      "Epoch: 4 cost time: 00h:08m:32.22s\n",
      "Epoch: 4 | Train Loss: 0.1034841 Vali Loss: 0.1205657 Test Loss: 0.1323852\n",
      "Validation loss decreased (0.120804 --> 0.120566).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1074458\n",
      "\tspeed: 1.4627s/iter; left time: 104821.1352s\n",
      "\titers: 200, epoch: 5 | loss: 0.1058937\n",
      "\tspeed: 0.1138s/iter; left time: 8143.2592s\n",
      "\titers: 300, epoch: 5 | loss: 0.1061051\n",
      "\tspeed: 0.1151s/iter; left time: 8225.7194s\n",
      "\titers: 400, epoch: 5 | loss: 0.1060566\n",
      "\tspeed: 0.1141s/iter; left time: 8142.5112s\n",
      "\titers: 500, epoch: 5 | loss: 0.1082695\n",
      "\tspeed: 0.1147s/iter; left time: 8172.9705s\n",
      "\titers: 600, epoch: 5 | loss: 0.0998992\n",
      "\tspeed: 0.1160s/iter; left time: 8253.3638s\n",
      "\titers: 700, epoch: 5 | loss: 0.0936923\n",
      "\tspeed: 0.1155s/iter; left time: 8203.9940s\n",
      "\titers: 800, epoch: 5 | loss: 0.0949209\n",
      "\tspeed: 0.1158s/iter; left time: 8217.0675s\n",
      "\titers: 900, epoch: 5 | loss: 0.1189355\n",
      "\tspeed: 0.1003s/iter; left time: 7110.8602s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1086110\n",
      "\tspeed: 0.1004s/iter; left time: 7103.0959s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1019219\n",
      "\tspeed: 0.1086s/iter; left time: 7671.5343s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0848749\n",
      "\tspeed: 0.1152s/iter; left time: 8130.4813s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0860293\n",
      "\tspeed: 0.1161s/iter; left time: 8180.9625s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0869483\n",
      "\tspeed: 0.1174s/iter; left time: 8260.7919s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0946057\n",
      "\tspeed: 0.1175s/iter; left time: 8256.7838s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0962429\n",
      "\tspeed: 0.1163s/iter; left time: 8156.7721s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0948651\n",
      "\tspeed: 0.1148s/iter; left time: 8041.5830s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0928815\n",
      "\tspeed: 0.1178s/iter; left time: 8239.3558s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1203045\n",
      "\tspeed: 0.1154s/iter; left time: 8060.0481s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0950496\n",
      "\tspeed: 0.1155s/iter; left time: 8055.3206s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1085438\n",
      "\tspeed: 0.1163s/iter; left time: 8099.8594s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0956611\n",
      "\tspeed: 0.1155s/iter; left time: 8034.8774s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1001744\n",
      "\tspeed: 0.1175s/iter; left time: 8164.6710s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0964360\n",
      "\tspeed: 0.1170s/iter; left time: 8115.6164s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0808083\n",
      "\tspeed: 0.1161s/iter; left time: 8039.4640s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1084981\n",
      "\tspeed: 0.1153s/iter; left time: 7970.9301s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0833636\n",
      "\tspeed: 0.1156s/iter; left time: 7985.1967s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0960330\n",
      "\tspeed: 0.1157s/iter; left time: 7978.9113s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1077119\n",
      "\tspeed: 0.1151s/iter; left time: 7924.2324s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1022597\n",
      "\tspeed: 0.1148s/iter; left time: 7892.3061s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0829771\n",
      "\tspeed: 0.1138s/iter; left time: 7811.7347s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1020444\n",
      "\tspeed: 0.1139s/iter; left time: 7812.5222s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0871775\n",
      "\tspeed: 0.1146s/iter; left time: 7844.7267s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0848032\n",
      "\tspeed: 0.1055s/iter; left time: 7215.2609s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1116961\n",
      "\tspeed: 0.1134s/iter; left time: 7740.9983s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0996750\n",
      "\tspeed: 0.1150s/iter; left time: 7837.7535s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0903397\n",
      "\tspeed: 0.1167s/iter; left time: 7940.6002s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0819974\n",
      "\tspeed: 0.1170s/iter; left time: 7948.9063s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1081164\n",
      "\tspeed: 0.1148s/iter; left time: 7789.3585s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0874876\n",
      "\tspeed: 0.1128s/iter; left time: 7643.4199s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1060955\n",
      "\tspeed: 0.1146s/iter; left time: 7752.6454s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1013475\n",
      "\tspeed: 0.1142s/iter; left time: 7712.8244s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0901750\n",
      "\tspeed: 0.1146s/iter; left time: 7733.9478s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0789233\n",
      "\tspeed: 0.1155s/iter; left time: 7777.1008s\n",
      "Epoch: 5 cost time: 00h:08m:33.46s\n",
      "Epoch: 5 | Train Loss: 0.1003134 Vali Loss: 0.1239620 Test Loss: 0.1393968\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1080867\n",
      "\tspeed: 1.4367s/iter; left time: 96512.7582s\n",
      "\titers: 200, epoch: 6 | loss: 0.0835378\n",
      "\tspeed: 0.0964s/iter; left time: 6466.7118s\n",
      "\titers: 300, epoch: 6 | loss: 0.0942729\n",
      "\tspeed: 0.1157s/iter; left time: 7749.9778s\n",
      "\titers: 400, epoch: 6 | loss: 0.0856797\n",
      "\tspeed: 0.1176s/iter; left time: 7867.5322s\n",
      "\titers: 500, epoch: 6 | loss: 0.1078456\n",
      "\tspeed: 0.1151s/iter; left time: 7684.6486s\n",
      "\titers: 600, epoch: 6 | loss: 0.0982793\n",
      "\tspeed: 0.1155s/iter; left time: 7701.8178s\n",
      "\titers: 700, epoch: 6 | loss: 0.0959515\n",
      "\tspeed: 0.1153s/iter; left time: 7677.6807s\n",
      "\titers: 800, epoch: 6 | loss: 0.0882172\n",
      "\tspeed: 0.1154s/iter; left time: 7670.4021s\n",
      "\titers: 900, epoch: 6 | loss: 0.1032411\n",
      "\tspeed: 0.1157s/iter; left time: 7677.2243s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0916246\n",
      "\tspeed: 0.1144s/iter; left time: 7583.0094s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1052714\n",
      "\tspeed: 0.1147s/iter; left time: 7589.3536s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1026374\n",
      "\tspeed: 0.1131s/iter; left time: 7472.7404s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0844505\n",
      "\tspeed: 0.1157s/iter; left time: 7633.2029s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1079274\n",
      "\tspeed: 0.1151s/iter; left time: 7579.2040s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1132590\n",
      "\tspeed: 0.1150s/iter; left time: 7561.5080s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0864766\n",
      "\tspeed: 0.1174s/iter; left time: 7710.4596s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0859761\n",
      "\tspeed: 0.1161s/iter; left time: 7614.6673s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0804487\n",
      "\tspeed: 0.1173s/iter; left time: 7680.2563s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1012686\n",
      "\tspeed: 0.1132s/iter; left time: 7403.0954s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1165255\n",
      "\tspeed: 0.1149s/iter; left time: 7497.9783s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1132169\n",
      "\tspeed: 0.1162s/iter; left time: 7570.7355s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0798103\n",
      "\tspeed: 0.1155s/iter; left time: 7513.9598s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0966341\n",
      "\tspeed: 0.1159s/iter; left time: 7528.6904s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1069327\n",
      "\tspeed: 0.1148s/iter; left time: 7450.4132s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0875472\n",
      "\tspeed: 0.1152s/iter; left time: 7460.0947s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1107147\n",
      "\tspeed: 0.1157s/iter; left time: 7482.2903s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1130749\n",
      "\tspeed: 0.1159s/iter; left time: 7484.2634s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0955565\n",
      "\tspeed: 0.1170s/iter; left time: 7542.6318s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1047895\n",
      "\tspeed: 0.1165s/iter; left time: 7498.4011s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1028518\n",
      "\tspeed: 0.1150s/iter; left time: 7393.8324s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0772144\n",
      "\tspeed: 0.1179s/iter; left time: 7566.3026s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1066739\n",
      "\tspeed: 0.1046s/iter; left time: 6704.5906s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1067312\n",
      "\tspeed: 0.1149s/iter; left time: 7351.1931s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0982327\n",
      "\tspeed: 0.1157s/iter; left time: 7387.9009s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1066106\n",
      "\tspeed: 0.1128s/iter; left time: 7196.3515s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1092574\n",
      "\tspeed: 0.1153s/iter; left time: 7343.8735s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1125807\n",
      "\tspeed: 0.1151s/iter; left time: 7319.3994s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0988214\n",
      "\tspeed: 0.1149s/iter; left time: 7293.2887s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0866890\n",
      "\tspeed: 0.1149s/iter; left time: 7283.4518s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0922776\n",
      "\tspeed: 0.1156s/iter; left time: 7315.6730s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1005166\n",
      "\tspeed: 0.1143s/iter; left time: 7220.2949s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0808731\n",
      "\tspeed: 0.1142s/iter; left time: 7205.7780s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0855673\n",
      "\tspeed: 0.1162s/iter; left time: 7315.5184s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1201236\n",
      "\tspeed: 0.1161s/iter; left time: 7298.1147s\n",
      "Epoch: 6 cost time: 00h:08m:34.03s\n",
      "Epoch: 6 | Train Loss: 0.0972888 Vali Loss: 0.1255503 Test Loss: 0.1417521\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0924498\n",
      "\tspeed: 1.4499s/iter; left time: 90895.6183s\n",
      "\titers: 200, epoch: 7 | loss: 0.1178812\n",
      "\tspeed: 0.1154s/iter; left time: 7223.4450s\n",
      "\titers: 300, epoch: 7 | loss: 0.0915430\n",
      "\tspeed: 0.1141s/iter; left time: 7127.9345s\n",
      "\titers: 400, epoch: 7 | loss: 0.0889588\n",
      "\tspeed: 0.1117s/iter; left time: 6967.4425s\n",
      "\titers: 500, epoch: 7 | loss: 0.0823657\n",
      "\tspeed: 0.1092s/iter; left time: 6805.2207s\n",
      "\titers: 600, epoch: 7 | loss: 0.1029791\n",
      "\tspeed: 0.1162s/iter; left time: 7227.7682s\n",
      "\titers: 700, epoch: 7 | loss: 0.0904151\n",
      "\tspeed: 0.1155s/iter; left time: 7168.4667s\n",
      "\titers: 800, epoch: 7 | loss: 0.1373906\n",
      "\tspeed: 0.1147s/iter; left time: 7108.8489s\n",
      "\titers: 900, epoch: 7 | loss: 0.0987064\n",
      "\tspeed: 0.1150s/iter; left time: 7116.0598s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0955451\n",
      "\tspeed: 0.1145s/iter; left time: 7072.5929s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0812314\n",
      "\tspeed: 0.1156s/iter; left time: 7133.7479s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1071944\n",
      "\tspeed: 0.1165s/iter; left time: 7177.3804s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1112599\n",
      "\tspeed: 0.1133s/iter; left time: 6967.3636s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0859119\n",
      "\tspeed: 0.0965s/iter; left time: 5924.9031s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0963456\n",
      "\tspeed: 0.0979s/iter; left time: 6003.2388s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0927925\n",
      "\tspeed: 0.1155s/iter; left time: 7066.2155s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1041150\n",
      "\tspeed: 0.1158s/iter; left time: 7075.9964s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0919541\n",
      "\tspeed: 0.1151s/iter; left time: 7021.1483s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0924811\n",
      "\tspeed: 0.0964s/iter; left time: 5872.0157s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0916734\n",
      "\tspeed: 0.1116s/iter; left time: 6783.0743s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0851152\n",
      "\tspeed: 0.1072s/iter; left time: 6504.1584s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0713027\n",
      "\tspeed: 0.1148s/iter; left time: 6956.5648s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0756128\n",
      "\tspeed: 0.1156s/iter; left time: 6990.4095s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0799808\n",
      "\tspeed: 0.1154s/iter; left time: 6968.6508s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0849434\n",
      "\tspeed: 0.1138s/iter; left time: 6863.1466s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0794628\n",
      "\tspeed: 0.1151s/iter; left time: 6926.4028s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0931070\n",
      "\tspeed: 0.1150s/iter; left time: 6909.9753s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1051028\n",
      "\tspeed: 0.1161s/iter; left time: 6965.8407s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1156529\n",
      "\tspeed: 0.1153s/iter; left time: 6907.9654s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0889909\n",
      "\tspeed: 0.1139s/iter; left time: 6812.9981s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0863874\n",
      "\tspeed: 0.1157s/iter; left time: 6908.3129s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0882615\n",
      "\tspeed: 0.1150s/iter; left time: 6855.6170s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0995574\n",
      "\tspeed: 0.1155s/iter; left time: 6869.2117s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0981942\n",
      "\tspeed: 0.1167s/iter; left time: 6928.9357s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1015216\n",
      "\tspeed: 0.1157s/iter; left time: 6858.1393s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0947982\n",
      "\tspeed: 0.1156s/iter; left time: 6841.4885s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1041567\n",
      "\tspeed: 0.1163s/iter; left time: 6872.8146s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0952044\n",
      "\tspeed: 0.1136s/iter; left time: 6698.5930s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0995766\n",
      "\tspeed: 0.1144s/iter; left time: 6734.4282s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0990296\n",
      "\tspeed: 0.1152s/iter; left time: 6772.2692s\n",
      "\titers: 4100, epoch: 7 | loss: 0.1016947\n",
      "\tspeed: 0.1158s/iter; left time: 6794.1004s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1040053\n",
      "\tspeed: 0.1145s/iter; left time: 6707.6129s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0988609\n",
      "\tspeed: 0.1154s/iter; left time: 6748.8651s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0869684\n",
      "\tspeed: 0.1148s/iter; left time: 6705.4233s\n",
      "Epoch: 7 cost time: 00h:08m:29.48s\n",
      "Epoch: 7 | Train Loss: 0.0946513 Vali Loss: 0.1245566 Test Loss: 0.1390361\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0918592\n",
      "\tspeed: 1.4296s/iter; left time: 83208.9369s\n",
      "\titers: 200, epoch: 8 | loss: 0.0835139\n",
      "\tspeed: 0.1151s/iter; left time: 6685.1469s\n",
      "\titers: 300, epoch: 8 | loss: 0.0830909\n",
      "\tspeed: 0.1116s/iter; left time: 6470.8564s\n",
      "\titers: 400, epoch: 8 | loss: 0.0773382\n",
      "\tspeed: 0.1140s/iter; left time: 6599.3565s\n",
      "\titers: 500, epoch: 8 | loss: 0.0999175\n",
      "\tspeed: 0.1146s/iter; left time: 6627.2344s\n",
      "\titers: 600, epoch: 8 | loss: 0.0910697\n",
      "\tspeed: 0.1148s/iter; left time: 6624.5143s\n",
      "\titers: 700, epoch: 8 | loss: 0.0925420\n",
      "\tspeed: 0.1106s/iter; left time: 6371.0272s\n",
      "\titers: 800, epoch: 8 | loss: 0.0842696\n",
      "\tspeed: 0.1029s/iter; left time: 5918.4271s\n",
      "\titers: 900, epoch: 8 | loss: 0.1030085\n",
      "\tspeed: 0.1137s/iter; left time: 6527.5101s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0857712\n",
      "\tspeed: 0.1155s/iter; left time: 6619.3193s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0929789\n",
      "\tspeed: 0.1149s/iter; left time: 6575.7293s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0848481\n",
      "\tspeed: 0.1145s/iter; left time: 6537.3795s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0990944\n",
      "\tspeed: 0.1163s/iter; left time: 6631.5699s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0971840\n",
      "\tspeed: 0.1146s/iter; left time: 6518.8950s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0843722\n",
      "\tspeed: 0.1154s/iter; left time: 6553.7758s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0780336\n",
      "\tspeed: 0.1153s/iter; left time: 6537.3273s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0831821\n",
      "\tspeed: 0.1145s/iter; left time: 6479.4710s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0875997\n",
      "\tspeed: 0.1154s/iter; left time: 6523.5061s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0869278\n",
      "\tspeed: 0.1149s/iter; left time: 6479.1692s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1146180\n",
      "\tspeed: 0.1152s/iter; left time: 6487.1368s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0729130\n",
      "\tspeed: 0.1153s/iter; left time: 6479.4428s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0905604\n",
      "\tspeed: 0.1149s/iter; left time: 6443.9184s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0900590\n",
      "\tspeed: 0.1151s/iter; left time: 6444.2595s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1031543\n",
      "\tspeed: 0.1156s/iter; left time: 6460.4231s\n",
      "\titers: 2500, epoch: 8 | loss: 0.1029483\n",
      "\tspeed: 0.1149s/iter; left time: 6414.3145s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0795364\n",
      "\tspeed: 0.1156s/iter; left time: 6440.5292s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1111625\n",
      "\tspeed: 0.1165s/iter; left time: 6477.0374s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0862386\n",
      "\tspeed: 0.1157s/iter; left time: 6422.2466s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1006412\n",
      "\tspeed: 0.1155s/iter; left time: 6401.8719s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0985600\n",
      "\tspeed: 0.1148s/iter; left time: 6347.0249s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0790469\n",
      "\tspeed: 0.1148s/iter; left time: 6335.9774s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1037440\n",
      "\tspeed: 0.1138s/iter; left time: 6272.0306s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0927985\n",
      "\tspeed: 0.1136s/iter; left time: 6250.9363s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0887386\n",
      "\tspeed: 0.1141s/iter; left time: 6266.1795s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0954302\n",
      "\tspeed: 0.1143s/iter; left time: 6263.9716s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0882672\n",
      "\tspeed: 0.1138s/iter; left time: 6224.1715s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0840252\n",
      "\tspeed: 0.1159s/iter; left time: 6328.9136s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0825819\n",
      "\tspeed: 0.1165s/iter; left time: 6349.1319s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0817300\n",
      "\tspeed: 0.1174s/iter; left time: 6386.4907s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0904208\n",
      "\tspeed: 0.1162s/iter; left time: 6312.9097s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0837452\n",
      "\tspeed: 0.1152s/iter; left time: 6244.6775s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0895528\n",
      "\tspeed: 0.1163s/iter; left time: 6290.0829s\n",
      "\titers: 4300, epoch: 8 | loss: 0.1021302\n",
      "\tspeed: 0.1163s/iter; left time: 6279.4962s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0824312\n",
      "\tspeed: 0.1157s/iter; left time: 6235.1450s\n",
      "Epoch: 8 cost time: 00h:08m:33.60s\n",
      "Epoch: 8 | Train Loss: 0.0923321 Vali Loss: 0.1244166 Test Loss: 0.1393789\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0860157\n",
      "\tspeed: 1.4456s/iter; left time: 77658.5198s\n",
      "\titers: 200, epoch: 9 | loss: 0.0827482\n",
      "\tspeed: 0.1145s/iter; left time: 6139.4815s\n",
      "\titers: 300, epoch: 9 | loss: 0.0886363\n",
      "\tspeed: 0.1163s/iter; left time: 6222.9455s\n",
      "\titers: 400, epoch: 9 | loss: 0.1117299\n",
      "\tspeed: 0.1188s/iter; left time: 6347.3701s\n",
      "\titers: 500, epoch: 9 | loss: 0.0810957\n",
      "\tspeed: 0.1137s/iter; left time: 6065.0390s\n",
      "\titers: 600, epoch: 9 | loss: 0.0957763\n",
      "\tspeed: 0.1145s/iter; left time: 6093.8977s\n",
      "\titers: 700, epoch: 9 | loss: 0.1021896\n",
      "\tspeed: 0.1150s/iter; left time: 6108.0329s\n",
      "\titers: 800, epoch: 9 | loss: 0.0900560\n",
      "\tspeed: 0.1154s/iter; left time: 6120.9355s\n",
      "\titers: 900, epoch: 9 | loss: 0.0941611\n",
      "\tspeed: 0.1150s/iter; left time: 6084.2274s\n",
      "\titers: 1000, epoch: 9 | loss: 0.1047803\n",
      "\tspeed: 0.1155s/iter; left time: 6101.7430s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0963440\n",
      "\tspeed: 0.1166s/iter; left time: 6148.0789s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0966223\n",
      "\tspeed: 0.1084s/iter; left time: 5703.9226s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0858766\n",
      "\tspeed: 0.1122s/iter; left time: 5892.2736s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0874112\n",
      "\tspeed: 0.1171s/iter; left time: 6136.1091s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0868891\n",
      "\tspeed: 0.1177s/iter; left time: 6159.6734s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0750172\n",
      "\tspeed: 0.1163s/iter; left time: 6073.6480s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0886351\n",
      "\tspeed: 0.1155s/iter; left time: 6018.5100s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0861389\n",
      "\tspeed: 0.1154s/iter; left time: 6003.2281s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0806216\n",
      "\tspeed: 0.1158s/iter; left time: 6012.8285s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0933814\n",
      "\tspeed: 0.1143s/iter; left time: 5922.7467s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0894557\n",
      "\tspeed: 0.1152s/iter; left time: 5956.7469s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0905567\n",
      "\tspeed: 0.1162s/iter; left time: 5997.0745s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0910580\n",
      "\tspeed: 0.1166s/iter; left time: 6008.7362s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0801017\n",
      "\tspeed: 0.1160s/iter; left time: 5966.8233s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0770919\n",
      "\tspeed: 0.1170s/iter; left time: 6003.3777s\n",
      "\titers: 2600, epoch: 9 | loss: 0.1063322\n",
      "\tspeed: 0.1162s/iter; left time: 5949.3625s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0792379\n",
      "\tspeed: 0.1167s/iter; left time: 5964.2041s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0964480\n",
      "\tspeed: 0.1146s/iter; left time: 5848.1760s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0888367\n",
      "\tspeed: 0.1149s/iter; left time: 5849.4877s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0799860\n",
      "\tspeed: 0.1154s/iter; left time: 5862.9364s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0811218\n",
      "\tspeed: 0.1146s/iter; left time: 5811.2544s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0989336\n",
      "\tspeed: 0.1165s/iter; left time: 5896.8224s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0911095\n",
      "\tspeed: 0.1160s/iter; left time: 5862.5736s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0657498\n",
      "\tspeed: 0.1151s/iter; left time: 5802.7048s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0899863\n",
      "\tspeed: 0.1167s/iter; left time: 5871.3044s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0988450\n",
      "\tspeed: 0.1156s/iter; left time: 5807.9330s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0745410\n",
      "\tspeed: 0.1168s/iter; left time: 5854.1877s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0895517\n",
      "\tspeed: 0.1154s/iter; left time: 5770.9914s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0853783\n",
      "\tspeed: 0.1190s/iter; left time: 5941.1843s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0899109\n",
      "\tspeed: 0.1159s/iter; left time: 5771.9665s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0919613\n",
      "\tspeed: 0.1152s/iter; left time: 5728.7361s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0816684\n",
      "\tspeed: 0.1166s/iter; left time: 5787.2196s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0924804\n",
      "\tspeed: 0.1147s/iter; left time: 5679.0589s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0909822\n",
      "\tspeed: 0.1150s/iter; left time: 5684.1695s\n",
      "Epoch: 9 cost time: 00h:08m:38.94s\n",
      "Epoch: 9 | Train Loss: 0.0901876 Vali Loss: 0.1248721 Test Loss: 0.1431583\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.039549555629491806, rmse:0.1988707035779953, mae:0.13238517940044403, rse:0.704233705997467\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 96: 01h:38m:18.51s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 143165\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-02 07:24:55,161] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 07:24:56,387] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 07:24:56,388] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 07:24:56,388] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 07:24:56,497] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 07:24:56,497] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 07:24:57,181] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 07:24:57,182] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 07:24:57,183] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 07:24:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 07:24:57,184] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 07:24:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 07:24:57,498] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 07:24:57,499] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,499] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,616] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 07:24:57,617] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,617] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 07:24:57,729] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 07:24:57,730] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,730] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe52e535310>\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 07:24:57,735] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1796879\n",
      "\tspeed: 0.1726s/iter; left time: 15419.9925s\n",
      "\titers: 200, epoch: 1 | loss: 0.1421639\n",
      "\tspeed: 0.1283s/iter; left time: 11449.1534s\n",
      "\titers: 300, epoch: 1 | loss: 0.1639913\n",
      "\tspeed: 0.1281s/iter; left time: 11420.8693s\n",
      "\titers: 400, epoch: 1 | loss: 0.1615290\n",
      "\tspeed: 0.1281s/iter; left time: 11406.5486s\n",
      "\titers: 500, epoch: 1 | loss: 0.1515372\n",
      "\tspeed: 0.1288s/iter; left time: 11457.7404s\n",
      "\titers: 600, epoch: 1 | loss: 0.1479097\n",
      "\tspeed: 0.1276s/iter; left time: 11341.7462s\n",
      "\titers: 700, epoch: 1 | loss: 0.1270908\n",
      "\tspeed: 0.1274s/iter; left time: 11312.0453s\n",
      "\titers: 800, epoch: 1 | loss: 0.1200892\n",
      "\tspeed: 0.1268s/iter; left time: 11239.8093s\n",
      "\titers: 900, epoch: 1 | loss: 0.1140285\n",
      "\tspeed: 0.1252s/iter; left time: 11088.1436s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1321990\n",
      "\tspeed: 0.1076s/iter; left time: 9519.3615s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1220576\n",
      "\tspeed: 0.1197s/iter; left time: 10577.6750s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1186985\n",
      "\tspeed: 0.1267s/iter; left time: 11179.6567s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1086995\n",
      "\tspeed: 0.1194s/iter; left time: 10525.2684s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1446381\n",
      "\tspeed: 0.1219s/iter; left time: 10730.7447s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1293692\n",
      "\tspeed: 0.1276s/iter; left time: 11220.7259s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1335815\n",
      "\tspeed: 0.1262s/iter; left time: 11084.1032s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1137727\n",
      "\tspeed: 0.1228s/iter; left time: 10779.0151s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1187546\n",
      "\tspeed: 0.1106s/iter; left time: 9695.9841s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1147917\n",
      "\tspeed: 0.1275s/iter; left time: 11166.8687s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1335327\n",
      "\tspeed: 0.1269s/iter; left time: 11098.9680s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1156783\n",
      "\tspeed: 0.1265s/iter; left time: 11051.8360s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1262950\n",
      "\tspeed: 0.1260s/iter; left time: 10998.5858s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1381016\n",
      "\tspeed: 0.1279s/iter; left time: 11152.1526s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1440842\n",
      "\tspeed: 0.1272s/iter; left time: 11073.5955s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1314677\n",
      "\tspeed: 0.1258s/iter; left time: 10941.4413s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1136199\n",
      "\tspeed: 0.1251s/iter; left time: 10868.1463s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1499220\n",
      "\tspeed: 0.1276s/iter; left time: 11072.2368s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1278764\n",
      "\tspeed: 0.1280s/iter; left time: 11093.0776s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1151534\n",
      "\tspeed: 0.1271s/iter; left time: 11001.7898s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1235821\n",
      "\tspeed: 0.1075s/iter; left time: 9291.7831s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1166754\n",
      "\tspeed: 0.1202s/iter; left time: 10382.4649s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1334251\n",
      "\tspeed: 0.1275s/iter; left time: 10999.5686s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1084857\n",
      "\tspeed: 0.1260s/iter; left time: 10859.4316s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1111453\n",
      "\tspeed: 0.1242s/iter; left time: 10693.0066s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1186036\n",
      "\tspeed: 0.1165s/iter; left time: 10018.6891s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1027479\n",
      "\tspeed: 0.1232s/iter; left time: 10577.1327s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1280373\n",
      "\tspeed: 0.1284s/iter; left time: 11015.6281s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1071356\n",
      "\tspeed: 0.1274s/iter; left time: 10911.9701s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1510007\n",
      "\tspeed: 0.1261s/iter; left time: 10787.3226s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1181215\n",
      "\tspeed: 0.1189s/iter; left time: 10163.3112s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1273197\n",
      "\tspeed: 0.1188s/iter; left time: 10142.5712s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1286651\n",
      "\tspeed: 0.1070s/iter; left time: 9124.9525s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1168264\n",
      "\tspeed: 0.1070s/iter; left time: 9108.0538s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1208781\n",
      "\tspeed: 0.1251s/iter; left time: 10639.2787s\n",
      "Epoch: 1 cost time: 00h:09m:13.64s\n",
      "Epoch: 1 | Train Loss: 0.1312128 Vali Loss: 0.1250472 Test Loss: 0.1346043\n",
      "Validation loss decreased (inf --> 0.125047).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1518959\n",
      "\tspeed: 1.6594s/iter; left time: 140860.2401s\n",
      "\titers: 200, epoch: 2 | loss: 0.0964266\n",
      "\tspeed: 0.1082s/iter; left time: 9171.5763s\n",
      "\titers: 300, epoch: 2 | loss: 0.1247060\n",
      "\tspeed: 0.1168s/iter; left time: 9893.8506s\n",
      "\titers: 400, epoch: 2 | loss: 0.1089664\n",
      "\tspeed: 0.1169s/iter; left time: 9885.9224s\n",
      "\titers: 500, epoch: 2 | loss: 0.1381617\n",
      "\tspeed: 0.1180s/iter; left time: 9972.6544s\n",
      "\titers: 600, epoch: 2 | loss: 0.1414172\n",
      "\tspeed: 0.0988s/iter; left time: 8335.2298s\n",
      "\titers: 700, epoch: 2 | loss: 0.1288400\n",
      "\tspeed: 0.1173s/iter; left time: 9884.6857s\n",
      "\titers: 800, epoch: 2 | loss: 0.1145916\n",
      "\tspeed: 0.1160s/iter; left time: 9764.8300s\n",
      "\titers: 900, epoch: 2 | loss: 0.1190638\n",
      "\tspeed: 0.1128s/iter; left time: 9486.3662s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1355297\n",
      "\tspeed: 0.1170s/iter; left time: 9824.2876s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1262201\n",
      "\tspeed: 0.1157s/iter; left time: 9704.0627s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1185752\n",
      "\tspeed: 0.1148s/iter; left time: 9615.0635s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1181296\n",
      "\tspeed: 0.1173s/iter; left time: 9820.5802s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1286746\n",
      "\tspeed: 0.1173s/iter; left time: 9803.8865s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1001056\n",
      "\tspeed: 0.1164s/iter; left time: 9715.2900s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1140635\n",
      "\tspeed: 0.1154s/iter; left time: 9626.1250s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1254133\n",
      "\tspeed: 0.1170s/iter; left time: 9746.7219s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0982282\n",
      "\tspeed: 0.1166s/iter; left time: 9701.4404s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1218288\n",
      "\tspeed: 0.1164s/iter; left time: 9674.1802s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1265479\n",
      "\tspeed: 0.1169s/iter; left time: 9700.0668s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1115575\n",
      "\tspeed: 0.1167s/iter; left time: 9671.6086s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1242154\n",
      "\tspeed: 0.1161s/iter; left time: 9610.3575s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1200421\n",
      "\tspeed: 0.1162s/iter; left time: 9611.5781s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1178377\n",
      "\tspeed: 0.1158s/iter; left time: 9561.1125s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1067296\n",
      "\tspeed: 0.1165s/iter; left time: 9613.9227s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1391483\n",
      "\tspeed: 0.1156s/iter; left time: 9527.7564s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0934009\n",
      "\tspeed: 0.1165s/iter; left time: 9590.4439s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1260687\n",
      "\tspeed: 0.1175s/iter; left time: 9660.2411s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1159549\n",
      "\tspeed: 0.1160s/iter; left time: 9524.8115s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1432488\n",
      "\tspeed: 0.1148s/iter; left time: 9414.2710s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1512252\n",
      "\tspeed: 0.1157s/iter; left time: 9477.0973s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1017725\n",
      "\tspeed: 0.1154s/iter; left time: 9435.5917s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1183511\n",
      "\tspeed: 0.1151s/iter; left time: 9401.9968s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1324611\n",
      "\tspeed: 0.1160s/iter; left time: 9464.5271s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1085706\n",
      "\tspeed: 0.1165s/iter; left time: 9490.2395s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1149569\n",
      "\tspeed: 0.1154s/iter; left time: 9388.3903s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1181808\n",
      "\tspeed: 0.1161s/iter; left time: 9435.1903s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0986252\n",
      "\tspeed: 0.1152s/iter; left time: 9354.0269s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1190111\n",
      "\tspeed: 0.1155s/iter; left time: 9365.0712s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1110677\n",
      "\tspeed: 0.1172s/iter; left time: 9489.5820s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1073243\n",
      "\tspeed: 0.1152s/iter; left time: 9322.0756s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1071260\n",
      "\tspeed: 0.1155s/iter; left time: 9328.9636s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1088651\n",
      "\tspeed: 0.1162s/iter; left time: 9378.4993s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1130303\n",
      "\tspeed: 0.1143s/iter; left time: 9207.5642s\n",
      "Epoch: 2 cost time: 00h:08m:37.30s\n",
      "Epoch: 2 | Train Loss: 0.1167669 Vali Loss: 0.1236090 Test Loss: 0.1348319\n",
      "Validation loss decreased (0.125047 --> 0.123609).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1316700\n",
      "\tspeed: 1.4469s/iter; left time: 116354.8227s\n",
      "\titers: 200, epoch: 3 | loss: 0.1039964\n",
      "\tspeed: 0.1161s/iter; left time: 9324.3219s\n",
      "\titers: 300, epoch: 3 | loss: 0.1126300\n",
      "\tspeed: 0.1174s/iter; left time: 9418.7683s\n",
      "\titers: 400, epoch: 3 | loss: 0.1131036\n",
      "\tspeed: 0.1179s/iter; left time: 9441.8684s\n",
      "\titers: 500, epoch: 3 | loss: 0.1220148\n",
      "\tspeed: 0.1157s/iter; left time: 9255.2142s\n",
      "\titers: 600, epoch: 3 | loss: 0.1027394\n",
      "\tspeed: 0.1173s/iter; left time: 9375.9775s\n",
      "\titers: 700, epoch: 3 | loss: 0.0868261\n",
      "\tspeed: 0.1194s/iter; left time: 9529.9573s\n",
      "\titers: 800, epoch: 3 | loss: 0.1178839\n",
      "\tspeed: 0.1147s/iter; left time: 9145.8728s\n",
      "\titers: 900, epoch: 3 | loss: 0.1133176\n",
      "\tspeed: 0.1156s/iter; left time: 9206.9141s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1266209\n",
      "\tspeed: 0.1094s/iter; left time: 8700.7189s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1104170\n",
      "\tspeed: 0.1161s/iter; left time: 9216.2815s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1124218\n",
      "\tspeed: 0.1152s/iter; left time: 9139.3929s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1273887\n",
      "\tspeed: 0.1155s/iter; left time: 9149.5361s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1296332\n",
      "\tspeed: 0.1165s/iter; left time: 9215.2192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1154967\n",
      "\tspeed: 0.1157s/iter; left time: 9144.4551s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1137325\n",
      "\tspeed: 0.1173s/iter; left time: 9260.2878s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1128048\n",
      "\tspeed: 0.1148s/iter; left time: 9045.8627s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1064127\n",
      "\tspeed: 0.1156s/iter; left time: 9103.3748s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1183627\n",
      "\tspeed: 0.1174s/iter; left time: 9226.3196s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1283379\n",
      "\tspeed: 0.1165s/iter; left time: 9150.0116s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1172018\n",
      "\tspeed: 0.1149s/iter; left time: 9009.6223s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1036158\n",
      "\tspeed: 0.1161s/iter; left time: 9093.2266s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1167738\n",
      "\tspeed: 0.1157s/iter; left time: 9051.7936s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1137663\n",
      "\tspeed: 0.1147s/iter; left time: 8962.3061s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1288111\n",
      "\tspeed: 0.1158s/iter; left time: 9031.7794s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1302198\n",
      "\tspeed: 0.1154s/iter; left time: 8988.4020s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1083783\n",
      "\tspeed: 0.1160s/iter; left time: 9029.4703s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1071675\n",
      "\tspeed: 0.1170s/iter; left time: 9095.9111s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1293719\n",
      "\tspeed: 0.1189s/iter; left time: 9224.6144s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1187557\n",
      "\tspeed: 0.1176s/iter; left time: 9118.7357s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0963806\n",
      "\tspeed: 0.1170s/iter; left time: 9058.7001s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1214169\n",
      "\tspeed: 0.1165s/iter; left time: 9006.0014s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0924854\n",
      "\tspeed: 0.1175s/iter; left time: 9071.1331s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0961636\n",
      "\tspeed: 0.1161s/iter; left time: 8950.3778s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1234925\n",
      "\tspeed: 0.1174s/iter; left time: 9038.1773s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1198209\n",
      "\tspeed: 0.1161s/iter; left time: 8926.9670s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1148769\n",
      "\tspeed: 0.1172s/iter; left time: 9003.4303s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1119351\n",
      "\tspeed: 0.1161s/iter; left time: 8903.0600s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1058953\n",
      "\tspeed: 0.1188s/iter; left time: 9104.9184s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1222074\n",
      "\tspeed: 0.1186s/iter; left time: 9075.6974s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1121319\n",
      "\tspeed: 0.1168s/iter; left time: 8924.4999s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0926314\n",
      "\tspeed: 0.1174s/iter; left time: 8958.3413s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1098126\n",
      "\tspeed: 0.1200s/iter; left time: 9144.0661s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1091134\n",
      "\tspeed: 0.1167s/iter; left time: 8879.8932s\n",
      "Epoch: 3 cost time: 00h:08m:41.53s\n",
      "Epoch: 3 | Train Loss: 0.1130445 Vali Loss: 0.1239033 Test Loss: 0.1380568\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1118852\n",
      "\tspeed: 1.4256s/iter; left time: 108259.1567s\n",
      "\titers: 200, epoch: 4 | loss: 0.1162335\n",
      "\tspeed: 0.1163s/iter; left time: 8822.3548s\n",
      "\titers: 300, epoch: 4 | loss: 0.0995601\n",
      "\tspeed: 0.1157s/iter; left time: 8766.8859s\n",
      "\titers: 400, epoch: 4 | loss: 0.1132471\n",
      "\tspeed: 0.1149s/iter; left time: 8689.4645s\n",
      "\titers: 500, epoch: 4 | loss: 0.1124692\n",
      "\tspeed: 0.1151s/iter; left time: 8698.4065s\n",
      "\titers: 600, epoch: 4 | loss: 0.1041030\n",
      "\tspeed: 0.1161s/iter; left time: 8756.2922s\n",
      "\titers: 700, epoch: 4 | loss: 0.1155552\n",
      "\tspeed: 0.1168s/iter; left time: 8798.0051s\n",
      "\titers: 800, epoch: 4 | loss: 0.1389403\n",
      "\tspeed: 0.1162s/iter; left time: 8744.8484s\n",
      "\titers: 900, epoch: 4 | loss: 0.1264938\n",
      "\tspeed: 0.1158s/iter; left time: 8700.2851s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1071267\n",
      "\tspeed: 0.1144s/iter; left time: 8584.7234s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1268844\n",
      "\tspeed: 0.1149s/iter; left time: 8608.8614s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0896029\n",
      "\tspeed: 0.1144s/iter; left time: 8563.0431s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0967841\n",
      "\tspeed: 0.1154s/iter; left time: 8622.9499s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0985883\n",
      "\tspeed: 0.1175s/iter; left time: 8771.2091s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1171656\n",
      "\tspeed: 0.1145s/iter; left time: 8532.8823s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1105181\n",
      "\tspeed: 0.1167s/iter; left time: 8686.1936s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0927968\n",
      "\tspeed: 0.1149s/iter; left time: 8541.6680s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0862853\n",
      "\tspeed: 0.1159s/iter; left time: 8601.1435s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0791367\n",
      "\tspeed: 0.1150s/iter; left time: 8528.6747s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0964180\n",
      "\tspeed: 0.1163s/iter; left time: 8614.4506s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1272053\n",
      "\tspeed: 0.1150s/iter; left time: 8501.9548s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1248995\n",
      "\tspeed: 0.1149s/iter; left time: 8486.9660s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1133061\n",
      "\tspeed: 0.1163s/iter; left time: 8573.9003s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0944415\n",
      "\tspeed: 0.1160s/iter; left time: 8540.5527s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0903405\n",
      "\tspeed: 0.1155s/iter; left time: 8493.4931s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1078932\n",
      "\tspeed: 0.1163s/iter; left time: 8539.2716s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1252578\n",
      "\tspeed: 0.1151s/iter; left time: 8442.0480s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1257209\n",
      "\tspeed: 0.1174s/iter; left time: 8597.6128s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1187883\n",
      "\tspeed: 0.1158s/iter; left time: 8467.2677s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1017213\n",
      "\tspeed: 0.1168s/iter; left time: 8532.3864s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0994309\n",
      "\tspeed: 0.1160s/iter; left time: 8462.6957s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1110925\n",
      "\tspeed: 0.1155s/iter; left time: 8415.8009s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1359264\n",
      "\tspeed: 0.1148s/iter; left time: 8349.8342s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1213765\n",
      "\tspeed: 0.1159s/iter; left time: 8416.6438s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1151622\n",
      "\tspeed: 0.1152s/iter; left time: 8353.7053s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0981746\n",
      "\tspeed: 0.1146s/iter; left time: 8305.3391s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1020091\n",
      "\tspeed: 0.1042s/iter; left time: 7537.6323s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1195145\n",
      "\tspeed: 0.1161s/iter; left time: 8384.9630s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1091338\n",
      "\tspeed: 0.1151s/iter; left time: 8306.8368s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0933681\n",
      "\tspeed: 0.1152s/iter; left time: 8301.8588s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1090270\n",
      "\tspeed: 0.1157s/iter; left time: 8321.0657s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0897193\n",
      "\tspeed: 0.1150s/iter; left time: 8265.2853s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0790287\n",
      "\tspeed: 0.1148s/iter; left time: 8238.9496s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0960602\n",
      "\tspeed: 0.1144s/iter; left time: 8193.9019s\n",
      "Epoch: 4 cost time: 00h:08m:36.70s\n",
      "Epoch: 4 | Train Loss: 0.1090248 Vali Loss: 0.1242168 Test Loss: 0.1387148\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0958029\n",
      "\tspeed: 1.4214s/iter; left time: 101586.7235s\n",
      "\titers: 200, epoch: 5 | loss: 0.1198548\n",
      "\tspeed: 0.1155s/iter; left time: 8242.3255s\n",
      "\titers: 300, epoch: 5 | loss: 0.0923079\n",
      "\tspeed: 0.1149s/iter; left time: 8191.2940s\n",
      "\titers: 400, epoch: 5 | loss: 0.1043584\n",
      "\tspeed: 0.1146s/iter; left time: 8159.1562s\n",
      "\titers: 500, epoch: 5 | loss: 0.1197232\n",
      "\tspeed: 0.1137s/iter; left time: 8078.0820s\n",
      "\titers: 600, epoch: 5 | loss: 0.0903645\n",
      "\tspeed: 0.1134s/iter; left time: 8051.2072s\n",
      "\titers: 700, epoch: 5 | loss: 0.1047615\n",
      "\tspeed: 0.1144s/iter; left time: 8105.6870s\n",
      "\titers: 800, epoch: 5 | loss: 0.1033622\n",
      "\tspeed: 0.1110s/iter; left time: 7853.1599s\n",
      "\titers: 900, epoch: 5 | loss: 0.1312480\n",
      "\tspeed: 0.1155s/iter; left time: 8158.7599s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1187147\n",
      "\tspeed: 0.1154s/iter; left time: 8140.9617s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1290016\n",
      "\tspeed: 0.1163s/iter; left time: 8193.0191s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1014192\n",
      "\tspeed: 0.1159s/iter; left time: 8155.0929s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1014588\n",
      "\tspeed: 0.1163s/iter; left time: 8170.1980s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1132953\n",
      "\tspeed: 0.1156s/iter; left time: 8113.3698s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0985292\n",
      "\tspeed: 0.1143s/iter; left time: 8007.2994s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0970728\n",
      "\tspeed: 0.1145s/iter; left time: 8014.6014s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1024207\n",
      "\tspeed: 0.1070s/iter; left time: 7476.1594s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1228088\n",
      "\tspeed: 0.1087s/iter; left time: 7581.0328s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1073534\n",
      "\tspeed: 0.1151s/iter; left time: 8021.4381s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1048014\n",
      "\tspeed: 0.1160s/iter; left time: 8072.1069s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0878087\n",
      "\tspeed: 0.1153s/iter; left time: 8013.1637s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0974426\n",
      "\tspeed: 0.1161s/iter; left time: 8053.1329s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1010784\n",
      "\tspeed: 0.1145s/iter; left time: 7929.1990s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1082448\n",
      "\tspeed: 0.1146s/iter; left time: 7928.9050s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1082602\n",
      "\tspeed: 0.1134s/iter; left time: 7835.5237s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1061032\n",
      "\tspeed: 0.1153s/iter; left time: 7950.3988s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1009885\n",
      "\tspeed: 0.1140s/iter; left time: 7851.8386s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1107342\n",
      "\tspeed: 0.1146s/iter; left time: 7883.3197s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1206490\n",
      "\tspeed: 0.1171s/iter; left time: 8043.8140s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0938140\n",
      "\tspeed: 0.1124s/iter; left time: 7704.7116s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0936605\n",
      "\tspeed: 0.1148s/iter; left time: 7862.1151s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1181608\n",
      "\tspeed: 0.1047s/iter; left time: 7156.0041s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1202567\n",
      "\tspeed: 0.1147s/iter; left time: 7827.1282s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1185086\n",
      "\tspeed: 0.1150s/iter; left time: 7838.4699s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1326381\n",
      "\tspeed: 0.1151s/iter; left time: 7833.1310s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1140965\n",
      "\tspeed: 0.1146s/iter; left time: 7791.2881s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0983576\n",
      "\tspeed: 0.1150s/iter; left time: 7804.9407s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1108841\n",
      "\tspeed: 0.1152s/iter; left time: 7808.9477s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1224041\n",
      "\tspeed: 0.1163s/iter; left time: 7867.3697s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1092387\n",
      "\tspeed: 0.1151s/iter; left time: 7780.1666s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1015882\n",
      "\tspeed: 0.1163s/iter; left time: 7843.3600s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1037027\n",
      "\tspeed: 0.1158s/iter; left time: 7800.7583s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1098816\n",
      "\tspeed: 0.1146s/iter; left time: 7710.4665s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0915770\n",
      "\tspeed: 0.1142s/iter; left time: 7673.7397s\n",
      "Epoch: 5 cost time: 00h:08m:32.11s\n",
      "Epoch: 5 | Train Loss: 0.1055163 Vali Loss: 0.1278250 Test Loss: 0.1433094\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1131890\n",
      "\tspeed: 1.4163s/iter; left time: 94886.3687s\n",
      "\titers: 200, epoch: 6 | loss: 0.1149606\n",
      "\tspeed: 0.1160s/iter; left time: 7760.3897s\n",
      "\titers: 300, epoch: 6 | loss: 0.0889585\n",
      "\tspeed: 0.1161s/iter; left time: 7753.3661s\n",
      "\titers: 400, epoch: 6 | loss: 0.0879647\n",
      "\tspeed: 0.1154s/iter; left time: 7696.7286s\n",
      "\titers: 500, epoch: 6 | loss: 0.0849967\n",
      "\tspeed: 0.1151s/iter; left time: 7663.1614s\n",
      "\titers: 600, epoch: 6 | loss: 0.1211026\n",
      "\tspeed: 0.1138s/iter; left time: 7567.4627s\n",
      "\titers: 700, epoch: 6 | loss: 0.0971647\n",
      "\tspeed: 0.1136s/iter; left time: 7543.8119s\n",
      "\titers: 800, epoch: 6 | loss: 0.0980629\n",
      "\tspeed: 0.1142s/iter; left time: 7572.5191s\n",
      "\titers: 900, epoch: 6 | loss: 0.0897351\n",
      "\tspeed: 0.1146s/iter; left time: 7585.9983s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0996988\n",
      "\tspeed: 0.1164s/iter; left time: 7693.5253s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0931935\n",
      "\tspeed: 0.1145s/iter; left time: 7555.4581s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0953249\n",
      "\tspeed: 0.1157s/iter; left time: 7623.4250s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0887816\n",
      "\tspeed: 0.1163s/iter; left time: 7653.2949s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0984268\n",
      "\tspeed: 0.1158s/iter; left time: 7605.3619s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1156681\n",
      "\tspeed: 0.1156s/iter; left time: 7586.0758s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0764754\n",
      "\tspeed: 0.1149s/iter; left time: 7522.5356s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0990757\n",
      "\tspeed: 0.1140s/iter; left time: 7455.4058s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0968827\n",
      "\tspeed: 0.1153s/iter; left time: 7527.6041s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1168354\n",
      "\tspeed: 0.1150s/iter; left time: 7497.9210s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1053651\n",
      "\tspeed: 0.1154s/iter; left time: 7510.2694s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0823664\n",
      "\tspeed: 0.1151s/iter; left time: 7482.7343s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1102771\n",
      "\tspeed: 0.1166s/iter; left time: 7564.2998s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1032612\n",
      "\tspeed: 0.1114s/iter; left time: 7215.5340s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1231637\n",
      "\tspeed: 0.1150s/iter; left time: 7438.2102s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0966471\n",
      "\tspeed: 0.1137s/iter; left time: 7345.7661s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1128988\n",
      "\tspeed: 0.1171s/iter; left time: 7553.1837s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0957244\n",
      "\tspeed: 0.1161s/iter; left time: 7475.1081s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1045579\n",
      "\tspeed: 0.1151s/iter; left time: 7401.1844s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1174107\n",
      "\tspeed: 0.1138s/iter; left time: 7303.1345s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1265308\n",
      "\tspeed: 0.1158s/iter; left time: 7422.9778s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1023416\n",
      "\tspeed: 0.1147s/iter; left time: 7343.3806s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1108819\n",
      "\tspeed: 0.1153s/iter; left time: 7365.2338s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1014752\n",
      "\tspeed: 0.1148s/iter; left time: 7324.4552s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1040500\n",
      "\tspeed: 0.1133s/iter; left time: 7219.1967s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0979548\n",
      "\tspeed: 0.1142s/iter; left time: 7261.3570s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0927388\n",
      "\tspeed: 0.1071s/iter; left time: 6798.5022s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1014158\n",
      "\tspeed: 0.1152s/iter; left time: 7302.8676s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0997907\n",
      "\tspeed: 0.1131s/iter; left time: 7156.3863s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0954398\n",
      "\tspeed: 0.0966s/iter; left time: 6102.8671s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1046737\n",
      "\tspeed: 0.1147s/iter; left time: 7238.7126s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1015742\n",
      "\tspeed: 0.1153s/iter; left time: 7262.5951s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1010522\n",
      "\tspeed: 0.1145s/iter; left time: 7200.6178s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1013126\n",
      "\tspeed: 0.1159s/iter; left time: 7276.7739s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1157126\n",
      "\tspeed: 0.1157s/iter; left time: 7253.7991s\n",
      "Epoch: 6 cost time: 00h:08m:32.30s\n",
      "Epoch: 6 | Train Loss: 0.1023981 Vali Loss: 0.1287997 Test Loss: 0.1417000\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1052988\n",
      "\tspeed: 1.4231s/iter; left time: 88977.0848s\n",
      "\titers: 200, epoch: 7 | loss: 0.0873112\n",
      "\tspeed: 0.1171s/iter; left time: 7307.5313s\n",
      "\titers: 300, epoch: 7 | loss: 0.1031646\n",
      "\tspeed: 0.1071s/iter; left time: 6677.3176s\n",
      "\titers: 400, epoch: 7 | loss: 0.1072783\n",
      "\tspeed: 0.1167s/iter; left time: 7258.5221s\n",
      "\titers: 500, epoch: 7 | loss: 0.1088210\n",
      "\tspeed: 0.1060s/iter; left time: 6582.6507s\n",
      "\titers: 600, epoch: 7 | loss: 0.1105814\n",
      "\tspeed: 0.1077s/iter; left time: 6677.1701s\n",
      "\titers: 700, epoch: 7 | loss: 0.0999586\n",
      "\tspeed: 0.0995s/iter; left time: 6159.3849s\n",
      "\titers: 800, epoch: 7 | loss: 0.1067762\n",
      "\tspeed: 0.1153s/iter; left time: 7125.3779s\n",
      "\titers: 900, epoch: 7 | loss: 0.1046992\n",
      "\tspeed: 0.1151s/iter; left time: 7101.8821s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0837644\n",
      "\tspeed: 0.1085s/iter; left time: 6686.3988s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0945975\n",
      "\tspeed: 0.1145s/iter; left time: 7044.0881s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0875397\n",
      "\tspeed: 0.1145s/iter; left time: 7034.8019s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1284341\n",
      "\tspeed: 0.1173s/iter; left time: 7192.9399s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0821818\n",
      "\tspeed: 0.1187s/iter; left time: 7267.4471s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0969216\n",
      "\tspeed: 0.1176s/iter; left time: 7188.5354s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1076515\n",
      "\tspeed: 0.1160s/iter; left time: 7079.3109s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0943058\n",
      "\tspeed: 0.1171s/iter; left time: 7134.9859s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1208289\n",
      "\tspeed: 0.1172s/iter; left time: 7127.2384s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0957807\n",
      "\tspeed: 0.1169s/iter; left time: 7096.7321s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0940845\n",
      "\tspeed: 0.1146s/iter; left time: 6949.6210s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0939218\n",
      "\tspeed: 0.1155s/iter; left time: 6990.7062s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0780332\n",
      "\tspeed: 0.1163s/iter; left time: 7027.2516s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0861988\n",
      "\tspeed: 0.1159s/iter; left time: 6991.9481s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0930833\n",
      "\tspeed: 0.1150s/iter; left time: 6923.8736s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0916982\n",
      "\tspeed: 0.1156s/iter; left time: 6947.3130s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0973968\n",
      "\tspeed: 0.1179s/iter; left time: 7077.8250s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0994120\n",
      "\tspeed: 0.1165s/iter; left time: 6983.2574s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1021433\n",
      "\tspeed: 0.1164s/iter; left time: 6961.0543s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1025242\n",
      "\tspeed: 0.1172s/iter; left time: 6997.5293s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0995893\n",
      "\tspeed: 0.1159s/iter; left time: 6908.7495s\n",
      "\titers: 3100, epoch: 7 | loss: 0.1242002\n",
      "\tspeed: 0.1170s/iter; left time: 6966.5509s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0999554\n",
      "\tspeed: 0.1165s/iter; left time: 6921.0840s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0944341\n",
      "\tspeed: 0.1157s/iter; left time: 6863.4892s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0867006\n",
      "\tspeed: 0.1150s/iter; left time: 6809.7276s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0829456\n",
      "\tspeed: 0.1168s/iter; left time: 6903.4284s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1114448\n",
      "\tspeed: 0.1150s/iter; left time: 6787.7201s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1034871\n",
      "\tspeed: 0.1146s/iter; left time: 6754.5146s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0981191\n",
      "\tspeed: 0.1161s/iter; left time: 6831.5273s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0835699\n",
      "\tspeed: 0.1052s/iter; left time: 6179.2232s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0970934\n",
      "\tspeed: 0.1162s/iter; left time: 6809.7888s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0923245\n",
      "\tspeed: 0.1168s/iter; left time: 6834.4963s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0830892\n",
      "\tspeed: 0.1151s/iter; left time: 6726.9684s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1040890\n",
      "\tspeed: 0.1190s/iter; left time: 6939.1710s\n",
      "\titers: 4400, epoch: 7 | loss: 0.1054323\n",
      "\tspeed: 0.1153s/iter; left time: 6711.2376s\n",
      "Epoch: 7 cost time: 00h:08m:33.95s\n",
      "Epoch: 7 | Train Loss: 0.0994186 Vali Loss: 0.1318476 Test Loss: 0.1430013\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.039509814232587814, rmse:0.19877076148986816, mae:0.13483189046382904, rse:0.7042074799537659\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 168: 01h:16m:53.78s\n",
      "\n",
      "Intermediate time for DE: 04h:36m:07.06s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-02 08:41:48,941] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 08:41:50,078] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 08:41:50,079] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 08:41:50,079] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 08:41:50,183] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 08:41:50,184] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 08:41:50,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 08:41:50,864] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 08:41:50,864] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 08:41:50,866] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 08:41:50,866] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 08:41:50,866] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 08:41:51,181] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 08:41:51,182] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.42 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,300] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 08:41:51,301] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,302] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.43 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,302] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 08:41:51,417] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 08:41:51,418] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,418] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.43 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,418] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 08:41:51,419] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f66915c2150>\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1480396\n",
      "\tspeed: 0.1702s/iter; left time: 15289.4378s\n",
      "\titers: 200, epoch: 1 | loss: 0.1359396\n",
      "\tspeed: 0.1278s/iter; left time: 11462.0213s\n",
      "\titers: 300, epoch: 1 | loss: 0.1393571\n",
      "\tspeed: 0.1281s/iter; left time: 11476.6610s\n",
      "\titers: 400, epoch: 1 | loss: 0.1382776\n",
      "\tspeed: 0.1272s/iter; left time: 11387.7617s\n",
      "\titers: 500, epoch: 1 | loss: 0.1498523\n",
      "\tspeed: 0.1274s/iter; left time: 11392.2678s\n",
      "\titers: 600, epoch: 1 | loss: 0.1618727\n",
      "\tspeed: 0.1290s/iter; left time: 11522.9930s\n",
      "\titers: 700, epoch: 1 | loss: 0.1502969\n",
      "\tspeed: 0.1276s/iter; left time: 11387.8297s\n",
      "\titers: 800, epoch: 1 | loss: 0.1506559\n",
      "\tspeed: 0.1273s/iter; left time: 11347.5577s\n",
      "\titers: 900, epoch: 1 | loss: 0.1592317\n",
      "\tspeed: 0.1270s/iter; left time: 11304.4541s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1277500\n",
      "\tspeed: 0.1272s/iter; left time: 11310.4209s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1013472\n",
      "\tspeed: 0.1176s/iter; left time: 10443.7474s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0913689\n",
      "\tspeed: 0.1276s/iter; left time: 11323.0255s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1019253\n",
      "\tspeed: 0.1258s/iter; left time: 11146.2666s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0914617\n",
      "\tspeed: 0.1257s/iter; left time: 11124.8529s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1200864\n",
      "\tspeed: 0.1285s/iter; left time: 11359.2716s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0783514\n",
      "\tspeed: 0.1288s/iter; left time: 11377.0559s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0821924\n",
      "\tspeed: 0.1281s/iter; left time: 11305.0679s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1186250\n",
      "\tspeed: 0.1286s/iter; left time: 11334.6789s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1131187\n",
      "\tspeed: 0.1290s/iter; left time: 11357.9233s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0995416\n",
      "\tspeed: 0.1274s/iter; left time: 11200.8495s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1084358\n",
      "\tspeed: 0.1263s/iter; left time: 11089.8330s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1041089\n",
      "\tspeed: 0.1216s/iter; left time: 10669.8667s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0937779\n",
      "\tspeed: 0.1275s/iter; left time: 11172.8503s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0927153\n",
      "\tspeed: 0.1284s/iter; left time: 11238.5934s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0854183\n",
      "\tspeed: 0.1272s/iter; left time: 11121.1070s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0832396\n",
      "\tspeed: 0.1262s/iter; left time: 11015.7884s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0943345\n",
      "\tspeed: 0.1268s/iter; left time: 11055.6223s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0730870\n",
      "\tspeed: 0.1261s/iter; left time: 10982.9476s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0838178\n",
      "\tspeed: 0.1265s/iter; left time: 11009.8294s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1037557\n",
      "\tspeed: 0.1275s/iter; left time: 11079.0149s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0776548\n",
      "\tspeed: 0.1297s/iter; left time: 11257.6756s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0702538\n",
      "\tspeed: 0.1265s/iter; left time: 10968.0991s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0951389\n",
      "\tspeed: 0.1280s/iter; left time: 11083.8968s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0885212\n",
      "\tspeed: 0.1282s/iter; left time: 11091.3444s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0847573\n",
      "\tspeed: 0.1266s/iter; left time: 10939.4751s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0946836\n",
      "\tspeed: 0.1263s/iter; left time: 10904.2457s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0944981\n",
      "\tspeed: 0.1257s/iter; left time: 10838.8317s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0870032\n",
      "\tspeed: 0.1268s/iter; left time: 10923.8423s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0747629\n",
      "\tspeed: 0.1260s/iter; left time: 10842.5188s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0820309\n",
      "\tspeed: 0.1268s/iter; left time: 10893.2202s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0989253\n",
      "\tspeed: 0.1259s/iter; left time: 10808.3815s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1019376\n",
      "\tspeed: 0.1260s/iter; left time: 10804.7401s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0859145\n",
      "\tspeed: 0.1267s/iter; left time: 10852.4174s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0804792\n",
      "\tspeed: 0.1259s/iter; left time: 10770.4280s\n",
      "Epoch: 1 cost time: 00h:09m:31.74s\n",
      "Epoch: 1 | Train Loss: 0.1045278 Vali Loss: 0.0942639 Test Loss: 0.1063175\n",
      "Validation loss decreased (inf --> 0.094264).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0567711\n",
      "\tspeed: 1.7215s/iter; left time: 146883.7925s\n",
      "\titers: 200, epoch: 2 | loss: 0.0760146\n",
      "\tspeed: 0.1167s/iter; left time: 9949.4148s\n",
      "\titers: 300, epoch: 2 | loss: 0.0817795\n",
      "\tspeed: 0.1183s/iter; left time: 10069.3451s\n",
      "\titers: 400, epoch: 2 | loss: 0.0809524\n",
      "\tspeed: 0.1157s/iter; left time: 9835.0170s\n",
      "\titers: 500, epoch: 2 | loss: 0.0880101\n",
      "\tspeed: 0.1165s/iter; left time: 9893.0284s\n",
      "\titers: 600, epoch: 2 | loss: 0.0825832\n",
      "\tspeed: 0.1120s/iter; left time: 9503.7633s\n",
      "\titers: 700, epoch: 2 | loss: 0.0830727\n",
      "\tspeed: 0.1167s/iter; left time: 9884.5535s\n",
      "\titers: 800, epoch: 2 | loss: 0.1017664\n",
      "\tspeed: 0.1153s/iter; left time: 9755.3256s\n",
      "\titers: 900, epoch: 2 | loss: 0.0834542\n",
      "\tspeed: 0.1169s/iter; left time: 9879.2239s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1037458\n",
      "\tspeed: 0.1166s/iter; left time: 9842.4837s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0702419\n",
      "\tspeed: 0.1164s/iter; left time: 9813.7170s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0827916\n",
      "\tspeed: 0.1170s/iter; left time: 9854.5568s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0975119\n",
      "\tspeed: 0.1160s/iter; left time: 9756.9489s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0909550\n",
      "\tspeed: 0.1171s/iter; left time: 9838.3383s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0980282\n",
      "\tspeed: 0.1166s/iter; left time: 9783.2063s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0983221\n",
      "\tspeed: 0.1169s/iter; left time: 9798.2750s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0979449\n",
      "\tspeed: 0.1001s/iter; left time: 8381.6061s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0913448\n",
      "\tspeed: 0.1151s/iter; left time: 9624.4655s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1151030\n",
      "\tspeed: 0.0990s/iter; left time: 8271.8433s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0902165\n",
      "\tspeed: 0.1029s/iter; left time: 8583.1806s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1013854\n",
      "\tspeed: 0.1163s/iter; left time: 9688.5822s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0748697\n",
      "\tspeed: 0.1169s/iter; left time: 9727.6551s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0970995\n",
      "\tspeed: 0.1162s/iter; left time: 9657.8191s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0777804\n",
      "\tspeed: 0.1171s/iter; left time: 9719.2870s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0974897\n",
      "\tspeed: 0.1164s/iter; left time: 9656.5650s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0840274\n",
      "\tspeed: 0.1173s/iter; left time: 9717.8059s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0875587\n",
      "\tspeed: 0.1163s/iter; left time: 9620.4784s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0773716\n",
      "\tspeed: 0.1161s/iter; left time: 9589.8572s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0789836\n",
      "\tspeed: 0.1172s/iter; left time: 9675.8173s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1230920\n",
      "\tspeed: 0.1069s/iter; left time: 8809.7361s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0794076\n",
      "\tspeed: 0.1163s/iter; left time: 9571.7775s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0580312\n",
      "\tspeed: 0.1174s/iter; left time: 9650.7737s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0819194\n",
      "\tspeed: 0.1185s/iter; left time: 9732.8212s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0737068\n",
      "\tspeed: 0.1160s/iter; left time: 9511.0434s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0817206\n",
      "\tspeed: 0.1171s/iter; left time: 9589.7468s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0782916\n",
      "\tspeed: 0.1154s/iter; left time: 9444.9179s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0830257\n",
      "\tspeed: 0.1156s/iter; left time: 9447.7819s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0800915\n",
      "\tspeed: 0.1166s/iter; left time: 9520.3294s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0705851\n",
      "\tspeed: 0.1166s/iter; left time: 9502.4624s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0799635\n",
      "\tspeed: 0.1160s/iter; left time: 9444.6185s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0793858\n",
      "\tspeed: 0.1160s/iter; left time: 9436.8695s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0901380\n",
      "\tspeed: 0.1153s/iter; left time: 9366.7847s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0854981\n",
      "\tspeed: 0.1141s/iter; left time: 9252.7679s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0815358\n",
      "\tspeed: 0.1147s/iter; left time: 9294.2794s\n",
      "Epoch: 2 cost time: 00h:08m:37.60s\n",
      "Epoch: 2 | Train Loss: 0.0874084 Vali Loss: 0.0943880 Test Loss: 0.1067109\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0967912\n",
      "\tspeed: 1.4692s/iter; left time: 118749.9835s\n",
      "\titers: 200, epoch: 3 | loss: 0.0968783\n",
      "\tspeed: 0.1170s/iter; left time: 9444.9843s\n",
      "\titers: 300, epoch: 3 | loss: 0.0831899\n",
      "\tspeed: 0.1130s/iter; left time: 9111.0593s\n",
      "\titers: 400, epoch: 3 | loss: 0.1071719\n",
      "\tspeed: 0.1002s/iter; left time: 8069.0538s\n",
      "\titers: 500, epoch: 3 | loss: 0.0645533\n",
      "\tspeed: 0.1145s/iter; left time: 9206.0888s\n",
      "\titers: 600, epoch: 3 | loss: 0.0737154\n",
      "\tspeed: 0.1159s/iter; left time: 9309.3799s\n",
      "\titers: 700, epoch: 3 | loss: 0.0825373\n",
      "\tspeed: 0.1158s/iter; left time: 9287.1080s\n",
      "\titers: 800, epoch: 3 | loss: 0.0860664\n",
      "\tspeed: 0.1155s/iter; left time: 9252.1500s\n",
      "\titers: 900, epoch: 3 | loss: 0.0782738\n",
      "\tspeed: 0.1165s/iter; left time: 9320.8989s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0969616\n",
      "\tspeed: 0.1144s/iter; left time: 9147.7179s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0766379\n",
      "\tspeed: 0.1160s/iter; left time: 9258.4540s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0865378\n",
      "\tspeed: 0.1171s/iter; left time: 9337.5920s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0749705\n",
      "\tspeed: 0.1167s/iter; left time: 9295.8511s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0806904\n",
      "\tspeed: 0.1173s/iter; left time: 9331.6695s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0795831\n",
      "\tspeed: 0.1159s/iter; left time: 9207.2889s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0955335\n",
      "\tspeed: 0.1153s/iter; left time: 9149.3765s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0916460\n",
      "\tspeed: 0.1150s/iter; left time: 9110.7891s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0964015\n",
      "\tspeed: 0.1154s/iter; left time: 9134.8030s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1013548\n",
      "\tspeed: 0.1146s/iter; left time: 9054.9745s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0869623\n",
      "\tspeed: 0.1157s/iter; left time: 9134.3805s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0899700\n",
      "\tspeed: 0.1149s/iter; left time: 9056.0083s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0860984\n",
      "\tspeed: 0.1153s/iter; left time: 9073.9541s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0814408\n",
      "\tspeed: 0.1150s/iter; left time: 9041.0419s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1089024\n",
      "\tspeed: 0.1159s/iter; left time: 9101.7565s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1013576\n",
      "\tspeed: 0.1147s/iter; left time: 8996.6029s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0859051\n",
      "\tspeed: 0.1149s/iter; left time: 9000.8520s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0691349\n",
      "\tspeed: 0.1153s/iter; left time: 9018.4082s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0932444\n",
      "\tspeed: 0.1172s/iter; left time: 9156.3923s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0649643\n",
      "\tspeed: 0.1152s/iter; left time: 8985.3477s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0698654\n",
      "\tspeed: 0.1167s/iter; left time: 9093.0110s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0942341\n",
      "\tspeed: 0.1162s/iter; left time: 9041.9563s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0748233\n",
      "\tspeed: 0.1176s/iter; left time: 9139.4906s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0922656\n",
      "\tspeed: 0.1159s/iter; left time: 8999.5240s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0845802\n",
      "\tspeed: 0.1149s/iter; left time: 8910.5060s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0817511\n",
      "\tspeed: 0.1145s/iter; left time: 8868.7184s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0760563\n",
      "\tspeed: 0.1152s/iter; left time: 8910.8561s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1167071\n",
      "\tspeed: 0.1158s/iter; left time: 8946.6072s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0771297\n",
      "\tspeed: 0.1146s/iter; left time: 8839.2269s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0774214\n",
      "\tspeed: 0.1131s/iter; left time: 8712.6860s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0806527\n",
      "\tspeed: 0.1131s/iter; left time: 8702.0080s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1028307\n",
      "\tspeed: 0.1151s/iter; left time: 8840.9925s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0881696\n",
      "\tspeed: 0.1156s/iter; left time: 8869.1441s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0913428\n",
      "\tspeed: 0.1154s/iter; left time: 8841.1989s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0837192\n",
      "\tspeed: 0.1158s/iter; left time: 8863.6025s\n",
      "Epoch: 3 cost time: 00h:08m:37.48s\n",
      "Epoch: 3 | Train Loss: 0.0860432 Vali Loss: 0.0918118 Test Loss: 0.1031213\n",
      "Validation loss decreased (0.094264 --> 0.091812).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0954896\n",
      "\tspeed: 1.5194s/iter; left time: 115982.9203s\n",
      "\titers: 200, epoch: 4 | loss: 0.0677183\n",
      "\tspeed: 0.1153s/iter; left time: 8792.1664s\n",
      "\titers: 300, epoch: 4 | loss: 0.0733540\n",
      "\tspeed: 0.1169s/iter; left time: 8903.2544s\n",
      "\titers: 400, epoch: 4 | loss: 0.0691343\n",
      "\tspeed: 0.1165s/iter; left time: 8855.6881s\n",
      "\titers: 500, epoch: 4 | loss: 0.0786122\n",
      "\tspeed: 0.1158s/iter; left time: 8793.4333s\n",
      "\titers: 600, epoch: 4 | loss: 0.0799889\n",
      "\tspeed: 0.1163s/iter; left time: 8821.3362s\n",
      "\titers: 700, epoch: 4 | loss: 0.0815678\n",
      "\tspeed: 0.1158s/iter; left time: 8770.7137s\n",
      "\titers: 800, epoch: 4 | loss: 0.0852794\n",
      "\tspeed: 0.1154s/iter; left time: 8730.4849s\n",
      "\titers: 900, epoch: 4 | loss: 0.1063726\n",
      "\tspeed: 0.1149s/iter; left time: 8676.3405s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0828676\n",
      "\tspeed: 0.1170s/iter; left time: 8826.2468s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0860450\n",
      "\tspeed: 0.1163s/iter; left time: 8757.7761s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0846064\n",
      "\tspeed: 0.1153s/iter; left time: 8677.9395s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0873416\n",
      "\tspeed: 0.1159s/iter; left time: 8705.9777s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0829189\n",
      "\tspeed: 0.1152s/iter; left time: 8641.3069s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0689046\n",
      "\tspeed: 0.1153s/iter; left time: 8636.5696s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0895178\n",
      "\tspeed: 0.1149s/iter; left time: 8601.3822s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0820116\n",
      "\tspeed: 0.1145s/iter; left time: 8555.8162s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0827952\n",
      "\tspeed: 0.1149s/iter; left time: 8571.8162s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1026970\n",
      "\tspeed: 0.1159s/iter; left time: 8640.1371s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0725404\n",
      "\tspeed: 0.1149s/iter; left time: 8555.6678s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0876920\n",
      "\tspeed: 0.1146s/iter; left time: 8520.9992s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1142400\n",
      "\tspeed: 0.1148s/iter; left time: 8518.8770s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0928669\n",
      "\tspeed: 0.1153s/iter; left time: 8550.5162s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0878178\n",
      "\tspeed: 0.1153s/iter; left time: 8533.5576s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0795425\n",
      "\tspeed: 0.1148s/iter; left time: 8490.3157s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0796704\n",
      "\tspeed: 0.1119s/iter; left time: 8262.1208s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0722193\n",
      "\tspeed: 0.1150s/iter; left time: 8476.6537s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0852612\n",
      "\tspeed: 0.1181s/iter; left time: 8693.9337s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0933903\n",
      "\tspeed: 0.1176s/iter; left time: 8645.8405s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0701374\n",
      "\tspeed: 0.1161s/iter; left time: 8525.0474s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1009073\n",
      "\tspeed: 0.1153s/iter; left time: 8457.3872s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0881226\n",
      "\tspeed: 0.1151s/iter; left time: 8425.6172s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0878723\n",
      "\tspeed: 0.1011s/iter; left time: 7397.3712s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0679823\n",
      "\tspeed: 0.1073s/iter; left time: 7839.9911s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0670110\n",
      "\tspeed: 0.1052s/iter; left time: 7669.5679s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1087624\n",
      "\tspeed: 0.1088s/iter; left time: 7922.7925s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0843604\n",
      "\tspeed: 0.1083s/iter; left time: 7879.6771s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0843288\n",
      "\tspeed: 0.1079s/iter; left time: 7837.0638s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0889594\n",
      "\tspeed: 0.1070s/iter; left time: 7758.4626s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0709078\n",
      "\tspeed: 0.1089s/iter; left time: 7890.1776s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0785354\n",
      "\tspeed: 0.1110s/iter; left time: 8031.3928s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1014095\n",
      "\tspeed: 0.1110s/iter; left time: 8020.0764s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0900307\n",
      "\tspeed: 0.1059s/iter; left time: 7638.1102s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0985371\n",
      "\tspeed: 0.1094s/iter; left time: 7878.0186s\n",
      "Epoch: 4 cost time: 00h:08m:29.98s\n",
      "Epoch: 4 | Train Loss: 0.0849872 Vali Loss: 0.0910281 Test Loss: 0.1025915\n",
      "Validation loss decreased (0.091812 --> 0.091028).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0995415\n",
      "\tspeed: 1.5141s/iter; left time: 108771.3119s\n",
      "\titers: 200, epoch: 5 | loss: 0.0702179\n",
      "\tspeed: 0.1096s/iter; left time: 7862.5018s\n",
      "\titers: 300, epoch: 5 | loss: 0.0926327\n",
      "\tspeed: 0.1035s/iter; left time: 7416.0335s\n",
      "\titers: 400, epoch: 5 | loss: 0.0842026\n",
      "\tspeed: 0.1086s/iter; left time: 7768.5542s\n",
      "\titers: 500, epoch: 5 | loss: 0.0849711\n",
      "\tspeed: 0.1071s/iter; left time: 7651.4452s\n",
      "\titers: 600, epoch: 5 | loss: 0.0858472\n",
      "\tspeed: 0.1107s/iter; left time: 7895.3261s\n",
      "\titers: 700, epoch: 5 | loss: 0.0910149\n",
      "\tspeed: 0.1118s/iter; left time: 7961.6434s\n",
      "\titers: 800, epoch: 5 | loss: 0.0706106\n",
      "\tspeed: 0.1095s/iter; left time: 7791.4857s\n",
      "\titers: 900, epoch: 5 | loss: 0.0857396\n",
      "\tspeed: 0.1097s/iter; left time: 7792.7134s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0778304\n",
      "\tspeed: 0.1025s/iter; left time: 7269.0923s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0807632\n",
      "\tspeed: 0.1059s/iter; left time: 7499.7300s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0821455\n",
      "\tspeed: 0.1095s/iter; left time: 7742.7626s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0761781\n",
      "\tspeed: 0.1053s/iter; left time: 7436.2212s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0864105\n",
      "\tspeed: 0.1110s/iter; left time: 7829.8405s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0635816\n",
      "\tspeed: 0.1109s/iter; left time: 7811.5784s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0796172\n",
      "\tspeed: 0.1117s/iter; left time: 7858.3424s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0873830\n",
      "\tspeed: 0.1121s/iter; left time: 7870.4731s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0776808\n",
      "\tspeed: 0.1071s/iter; left time: 7515.0193s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0813784\n",
      "\tspeed: 0.1095s/iter; left time: 7671.4312s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0825241\n",
      "\tspeed: 0.1087s/iter; left time: 7604.7255s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0971226\n",
      "\tspeed: 0.1092s/iter; left time: 7627.9008s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0820535\n",
      "\tspeed: 0.1085s/iter; left time: 7569.6907s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0787062\n",
      "\tspeed: 0.1073s/iter; left time: 7475.1698s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0813770\n",
      "\tspeed: 0.1124s/iter; left time: 7818.5126s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0734397\n",
      "\tspeed: 0.1090s/iter; left time: 7565.5047s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0811693\n",
      "\tspeed: 0.1101s/iter; left time: 7636.4951s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0857649\n",
      "\tspeed: 0.1077s/iter; left time: 7454.6958s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0826442\n",
      "\tspeed: 0.1079s/iter; left time: 7463.0407s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0696512\n",
      "\tspeed: 0.1070s/iter; left time: 7390.0087s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0978351\n",
      "\tspeed: 0.1050s/iter; left time: 7237.8526s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0703570\n",
      "\tspeed: 0.1089s/iter; left time: 7497.3170s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0664298\n",
      "\tspeed: 0.1118s/iter; left time: 7682.5474s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0841569\n",
      "\tspeed: 0.1085s/iter; left time: 7450.3132s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0866328\n",
      "\tspeed: 0.1054s/iter; left time: 7222.5796s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0752724\n",
      "\tspeed: 0.1060s/iter; left time: 7254.2415s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0709817\n",
      "\tspeed: 0.1054s/iter; left time: 7202.8623s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0909998\n",
      "\tspeed: 0.1073s/iter; left time: 7322.7343s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0780020\n",
      "\tspeed: 0.1077s/iter; left time: 7339.9173s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0807215\n",
      "\tspeed: 0.1071s/iter; left time: 7288.4246s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0772602\n",
      "\tspeed: 0.1110s/iter; left time: 7539.4364s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0780594\n",
      "\tspeed: 0.1099s/iter; left time: 7454.3440s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0868828\n",
      "\tspeed: 0.1059s/iter; left time: 7173.3815s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0795707\n",
      "\tspeed: 0.1116s/iter; left time: 7546.4290s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0779867\n",
      "\tspeed: 0.1115s/iter; left time: 7530.1593s\n",
      "Epoch: 5 cost time: 00h:08m:08.52s\n",
      "Epoch: 5 | Train Loss: 0.0839314 Vali Loss: 0.0930410 Test Loss: 0.1055147\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0871269\n",
      "\tspeed: 1.4553s/iter; left time: 97999.8378s\n",
      "\titers: 200, epoch: 6 | loss: 0.0924439\n",
      "\tspeed: 0.1085s/iter; left time: 7298.9540s\n",
      "\titers: 300, epoch: 6 | loss: 0.0767670\n",
      "\tspeed: 0.1099s/iter; left time: 7376.6712s\n",
      "\titers: 400, epoch: 6 | loss: 0.0709401\n",
      "\tspeed: 0.1076s/iter; left time: 7210.7371s\n",
      "\titers: 500, epoch: 6 | loss: 0.0788817\n",
      "\tspeed: 0.1076s/iter; left time: 7204.0856s\n",
      "\titers: 600, epoch: 6 | loss: 0.0702770\n",
      "\tspeed: 0.1099s/iter; left time: 7345.0702s\n",
      "\titers: 700, epoch: 6 | loss: 0.0980311\n",
      "\tspeed: 0.1111s/iter; left time: 7412.3065s\n",
      "\titers: 800, epoch: 6 | loss: 0.0822494\n",
      "\tspeed: 0.1077s/iter; left time: 7176.0579s\n",
      "\titers: 900, epoch: 6 | loss: 0.0677797\n",
      "\tspeed: 0.1082s/iter; left time: 7200.6620s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0886040\n",
      "\tspeed: 0.1012s/iter; left time: 6723.6221s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0871691\n",
      "\tspeed: 0.1067s/iter; left time: 7080.8750s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0784157\n",
      "\tspeed: 0.1088s/iter; left time: 7206.3865s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0943549\n",
      "\tspeed: 0.1088s/iter; left time: 7197.4000s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0763766\n",
      "\tspeed: 0.1080s/iter; left time: 7134.1124s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0764059\n",
      "\tspeed: 0.1043s/iter; left time: 6874.9185s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0744005\n",
      "\tspeed: 0.1080s/iter; left time: 7110.8253s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0694969\n",
      "\tspeed: 0.1099s/iter; left time: 7228.1110s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0677618\n",
      "\tspeed: 0.1077s/iter; left time: 7071.5907s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0911437\n",
      "\tspeed: 0.1043s/iter; left time: 6835.3529s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0920015\n",
      "\tspeed: 0.1050s/iter; left time: 6870.2395s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0828903\n",
      "\tspeed: 0.1066s/iter; left time: 6963.6891s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0758576\n",
      "\tspeed: 0.1048s/iter; left time: 6838.2962s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0807692\n",
      "\tspeed: 0.1054s/iter; left time: 6865.9606s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0943944\n",
      "\tspeed: 0.1056s/iter; left time: 6869.0119s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0872433\n",
      "\tspeed: 0.1052s/iter; left time: 6829.1467s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0959055\n",
      "\tspeed: 0.1082s/iter; left time: 7018.1775s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0809270\n",
      "\tspeed: 0.1070s/iter; left time: 6927.1189s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0640975\n",
      "\tspeed: 0.1080s/iter; left time: 6980.9291s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0803174\n",
      "\tspeed: 0.1088s/iter; left time: 7021.9549s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0886245\n",
      "\tspeed: 0.1073s/iter; left time: 6912.7752s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0835691\n",
      "\tspeed: 0.1070s/iter; left time: 6882.9547s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0802454\n",
      "\tspeed: 0.1089s/iter; left time: 6996.6120s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0707895\n",
      "\tspeed: 0.1076s/iter; left time: 6898.5370s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0747536\n",
      "\tspeed: 0.1047s/iter; left time: 6703.4284s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1065182\n",
      "\tspeed: 0.1062s/iter; left time: 6789.0110s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0878420\n",
      "\tspeed: 0.1058s/iter; left time: 6755.5887s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0774292\n",
      "\tspeed: 0.1043s/iter; left time: 6645.3086s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1086048\n",
      "\tspeed: 0.1087s/iter; left time: 6918.0962s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0799593\n",
      "\tspeed: 0.1091s/iter; left time: 6931.3972s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1003824\n",
      "\tspeed: 0.1062s/iter; left time: 6738.8975s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0854066\n",
      "\tspeed: 0.1048s/iter; left time: 6637.6901s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0746893\n",
      "\tspeed: 0.1052s/iter; left time: 6651.1235s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0869709\n",
      "\tspeed: 0.1079s/iter; left time: 6813.7169s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0689251\n",
      "\tspeed: 0.1046s/iter; left time: 6591.0560s\n",
      "Epoch: 6 cost time: 00h:08m:02.06s\n",
      "Epoch: 6 | Train Loss: 0.0833013 Vali Loss: 0.0915693 Test Loss: 0.1043275\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0705436\n",
      "\tspeed: 1.4515s/iter; left time: 91216.5162s\n",
      "\titers: 200, epoch: 7 | loss: 0.0841148\n",
      "\tspeed: 0.1086s/iter; left time: 6812.7254s\n",
      "\titers: 300, epoch: 7 | loss: 0.0840711\n",
      "\tspeed: 0.1057s/iter; left time: 6619.8859s\n",
      "\titers: 400, epoch: 7 | loss: 0.0786994\n",
      "\tspeed: 0.1054s/iter; left time: 6594.1110s\n",
      "\titers: 500, epoch: 7 | loss: 0.0765329\n",
      "\tspeed: 0.1063s/iter; left time: 6635.6774s\n",
      "\titers: 600, epoch: 7 | loss: 0.0759210\n",
      "\tspeed: 0.1084s/iter; left time: 6759.8873s\n",
      "\titers: 700, epoch: 7 | loss: 0.1062268\n",
      "\tspeed: 0.1092s/iter; left time: 6798.3646s\n",
      "\titers: 800, epoch: 7 | loss: 0.0814249\n",
      "\tspeed: 0.1101s/iter; left time: 6839.5005s\n",
      "\titers: 900, epoch: 7 | loss: 0.0928968\n",
      "\tspeed: 0.1049s/iter; left time: 6510.9280s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0790363\n",
      "\tspeed: 0.1078s/iter; left time: 6675.1641s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0801770\n",
      "\tspeed: 0.1099s/iter; left time: 6794.7556s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0799099\n",
      "\tspeed: 0.1067s/iter; left time: 6590.6523s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1077784\n",
      "\tspeed: 0.1079s/iter; left time: 6651.2303s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0819536\n",
      "\tspeed: 0.1085s/iter; left time: 6676.5453s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0709243\n",
      "\tspeed: 0.1049s/iter; left time: 6448.0119s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0715297\n",
      "\tspeed: 0.1081s/iter; left time: 6629.1512s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0753313\n",
      "\tspeed: 0.1093s/iter; left time: 6696.5055s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0870889\n",
      "\tspeed: 0.1102s/iter; left time: 6739.3079s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0596218\n",
      "\tspeed: 0.1086s/iter; left time: 6630.2702s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0800639\n",
      "\tspeed: 0.1082s/iter; left time: 6594.2838s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0774808\n",
      "\tspeed: 0.1067s/iter; left time: 6494.8406s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0776948\n",
      "\tspeed: 0.1076s/iter; left time: 6533.1254s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0848682\n",
      "\tspeed: 0.1044s/iter; left time: 6332.0964s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0881115\n",
      "\tspeed: 0.1089s/iter; left time: 6590.3729s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0899723\n",
      "\tspeed: 0.1089s/iter; left time: 6582.7269s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0730382\n",
      "\tspeed: 0.1074s/iter; left time: 6480.5943s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0763565\n",
      "\tspeed: 0.1096s/iter; left time: 6602.4794s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0803490\n",
      "\tspeed: 0.1102s/iter; left time: 6626.0847s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0798958\n",
      "\tspeed: 0.1091s/iter; left time: 6551.9400s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0765674\n",
      "\tspeed: 0.1093s/iter; left time: 6552.8049s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0898325\n",
      "\tspeed: 0.1057s/iter; left time: 6323.5382s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0746160\n",
      "\tspeed: 0.1094s/iter; left time: 6537.2984s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0873410\n",
      "\tspeed: 0.1089s/iter; left time: 6496.4498s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0789864\n",
      "\tspeed: 0.1070s/iter; left time: 6369.8082s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0850723\n",
      "\tspeed: 0.1098s/iter; left time: 6526.5437s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0945234\n",
      "\tspeed: 0.1103s/iter; left time: 6544.1685s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0540937\n",
      "\tspeed: 0.1099s/iter; left time: 6512.3603s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0729968\n",
      "\tspeed: 0.1103s/iter; left time: 6525.2250s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0832610\n",
      "\tspeed: 0.1082s/iter; left time: 6386.7499s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0856799\n",
      "\tspeed: 0.1079s/iter; left time: 6358.0768s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0843669\n",
      "\tspeed: 0.1104s/iter; left time: 6494.3987s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0647494\n",
      "\tspeed: 0.1095s/iter; left time: 6429.6583s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0789265\n",
      "\tspeed: 0.1112s/iter; left time: 6520.6726s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0709903\n",
      "\tspeed: 0.1100s/iter; left time: 6441.0770s\n",
      "Epoch: 7 cost time: 00h:08m:08.20s\n",
      "Epoch: 7 | Train Loss: 0.0827715 Vali Loss: 0.0898659 Test Loss: 0.1015000\n",
      "Validation loss decreased (0.091028 --> 0.089866).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0960936\n",
      "\tspeed: 1.5094s/iter; left time: 88072.7849s\n",
      "\titers: 200, epoch: 8 | loss: 0.0740620\n",
      "\tspeed: 0.1050s/iter; left time: 6115.0676s\n",
      "\titers: 300, epoch: 8 | loss: 0.0876887\n",
      "\tspeed: 0.1119s/iter; left time: 6506.2064s\n",
      "\titers: 400, epoch: 8 | loss: 0.0733641\n",
      "\tspeed: 0.1081s/iter; left time: 6274.1387s\n",
      "\titers: 500, epoch: 8 | loss: 0.0889757\n",
      "\tspeed: 0.1083s/iter; left time: 6274.7419s\n",
      "\titers: 600, epoch: 8 | loss: 0.1005570\n",
      "\tspeed: 0.1054s/iter; left time: 6094.9406s\n",
      "\titers: 700, epoch: 8 | loss: 0.0792509\n",
      "\tspeed: 0.1126s/iter; left time: 6501.3859s\n",
      "\titers: 800, epoch: 8 | loss: 0.0805020\n",
      "\tspeed: 0.1068s/iter; left time: 6159.3257s\n",
      "\titers: 900, epoch: 8 | loss: 0.0761237\n",
      "\tspeed: 0.1087s/iter; left time: 6256.3690s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0728325\n",
      "\tspeed: 0.1060s/iter; left time: 6089.1869s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0897990\n",
      "\tspeed: 0.1098s/iter; left time: 6298.3499s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0857851\n",
      "\tspeed: 0.1108s/iter; left time: 6343.7974s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0711524\n",
      "\tspeed: 0.1111s/iter; left time: 6349.4167s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0887766\n",
      "\tspeed: 0.1093s/iter; left time: 6236.5120s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0786460\n",
      "\tspeed: 0.1121s/iter; left time: 6382.6439s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0860998\n",
      "\tspeed: 0.1038s/iter; left time: 5898.5450s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0714980\n",
      "\tspeed: 0.1081s/iter; left time: 6132.1933s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0794795\n",
      "\tspeed: 0.1082s/iter; left time: 6129.5536s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0873640\n",
      "\tspeed: 0.1103s/iter; left time: 6237.7933s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0895032\n",
      "\tspeed: 0.1057s/iter; left time: 5968.5749s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0804572\n",
      "\tspeed: 0.1046s/iter; left time: 5894.8727s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0963350\n",
      "\tspeed: 0.1049s/iter; left time: 5902.4230s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0714696\n",
      "\tspeed: 0.1079s/iter; left time: 6058.4630s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0926667\n",
      "\tspeed: 0.1065s/iter; left time: 5968.7561s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0717168\n",
      "\tspeed: 0.1051s/iter; left time: 5877.8041s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0704690\n",
      "\tspeed: 0.1094s/iter; left time: 6109.9223s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1047586\n",
      "\tspeed: 0.1073s/iter; left time: 5979.3563s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0844329\n",
      "\tspeed: 0.1085s/iter; left time: 6040.4353s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0658040\n",
      "\tspeed: 0.1090s/iter; left time: 6054.4304s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0744485\n",
      "\tspeed: 0.1104s/iter; left time: 6120.4387s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0894325\n",
      "\tspeed: 0.1031s/iter; left time: 5706.3880s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0886287\n",
      "\tspeed: 0.1051s/iter; left time: 5809.2085s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0864769\n",
      "\tspeed: 0.1071s/iter; left time: 5905.0952s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0689103\n",
      "\tspeed: 0.1057s/iter; left time: 5816.6354s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0874014\n",
      "\tspeed: 0.1106s/iter; left time: 6074.7154s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0792102\n",
      "\tspeed: 0.1088s/iter; left time: 5966.3641s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0755558\n",
      "\tspeed: 0.1102s/iter; left time: 6032.5168s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0958932\n",
      "\tspeed: 0.1094s/iter; left time: 5978.0712s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0898063\n",
      "\tspeed: 0.1048s/iter; left time: 5715.3486s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0988585\n",
      "\tspeed: 0.1072s/iter; left time: 5835.3402s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0759023\n",
      "\tspeed: 0.1122s/iter; left time: 6098.8360s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0791041\n",
      "\tspeed: 0.1084s/iter; left time: 5878.6637s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0657694\n",
      "\tspeed: 0.1105s/iter; left time: 5985.4459s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0909220\n",
      "\tspeed: 0.1087s/iter; left time: 5873.7307s\n",
      "Epoch: 8 cost time: 00h:08m:06.34s\n",
      "Epoch: 8 | Train Loss: 0.0823500 Vali Loss: 0.0910530 Test Loss: 0.1034771\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0831056\n",
      "\tspeed: 1.4514s/iter; left time: 78163.0226s\n",
      "\titers: 200, epoch: 9 | loss: 0.0952516\n",
      "\tspeed: 0.1094s/iter; left time: 5881.2074s\n",
      "\titers: 300, epoch: 9 | loss: 0.0744533\n",
      "\tspeed: 0.1068s/iter; left time: 5732.5674s\n",
      "\titers: 400, epoch: 9 | loss: 0.0850896\n",
      "\tspeed: 0.1049s/iter; left time: 5620.3388s\n",
      "\titers: 500, epoch: 9 | loss: 0.0550304\n",
      "\tspeed: 0.1058s/iter; left time: 5652.9427s\n",
      "\titers: 600, epoch: 9 | loss: 0.0789448\n",
      "\tspeed: 0.1081s/iter; left time: 5769.2775s\n",
      "\titers: 700, epoch: 9 | loss: 0.0687364\n",
      "\tspeed: 0.1090s/iter; left time: 5802.5814s\n",
      "\titers: 800, epoch: 9 | loss: 0.0792406\n",
      "\tspeed: 0.1101s/iter; left time: 5850.5956s\n",
      "\titers: 900, epoch: 9 | loss: 0.0835051\n",
      "\tspeed: 0.1086s/iter; left time: 5759.2836s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0730056\n",
      "\tspeed: 0.1064s/iter; left time: 5632.9452s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0977784\n",
      "\tspeed: 0.1065s/iter; left time: 5629.5731s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0776352\n",
      "\tspeed: 0.1103s/iter; left time: 5816.3402s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0933063\n",
      "\tspeed: 0.1051s/iter; left time: 5533.8348s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0774592\n",
      "\tspeed: 0.1051s/iter; left time: 5521.6245s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0733840\n",
      "\tspeed: 0.1095s/iter; left time: 5742.2190s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0761116\n",
      "\tspeed: 0.1027s/iter; left time: 5376.5107s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0698281\n",
      "\tspeed: 0.1076s/iter; left time: 5623.5312s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0795811\n",
      "\tspeed: 0.1061s/iter; left time: 5533.4007s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0838274\n",
      "\tspeed: 0.1069s/iter; left time: 5564.5762s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0694626\n",
      "\tspeed: 0.1096s/iter; left time: 5693.9632s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0945157\n",
      "\tspeed: 0.1065s/iter; left time: 5522.7097s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0914454\n",
      "\tspeed: 0.1068s/iter; left time: 5524.8859s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0576647\n",
      "\tspeed: 0.1093s/iter; left time: 5646.5105s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0809036\n",
      "\tspeed: 0.1080s/iter; left time: 5565.1956s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0779389\n",
      "\tspeed: 0.1088s/iter; left time: 5597.4147s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0879130\n",
      "\tspeed: 0.1061s/iter; left time: 5450.2346s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0842345\n",
      "\tspeed: 0.1090s/iter; left time: 5585.2469s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0857091\n",
      "\tspeed: 0.1087s/iter; left time: 5558.0092s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0836044\n",
      "\tspeed: 0.1111s/iter; left time: 5673.4564s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0857766\n",
      "\tspeed: 0.1081s/iter; left time: 5508.9101s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0825322\n",
      "\tspeed: 0.1088s/iter; left time: 5532.1036s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0827652\n",
      "\tspeed: 0.1090s/iter; left time: 5534.3862s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0923525\n",
      "\tspeed: 0.1109s/iter; left time: 5618.1983s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0894057\n",
      "\tspeed: 0.1113s/iter; left time: 5627.0260s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0685351\n",
      "\tspeed: 0.1100s/iter; left time: 5548.8322s\n",
      "\titers: 3600, epoch: 9 | loss: 0.1009678\n",
      "\tspeed: 0.1096s/iter; left time: 5520.7287s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0932600\n",
      "\tspeed: 0.1091s/iter; left time: 5481.8443s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0711512\n",
      "\tspeed: 0.1107s/iter; left time: 5551.6595s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0905252\n",
      "\tspeed: 0.1082s/iter; left time: 5417.3414s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0613631\n",
      "\tspeed: 0.1092s/iter; left time: 5454.7988s\n",
      "\titers: 4100, epoch: 9 | loss: 0.1000974\n",
      "\tspeed: 0.1096s/iter; left time: 5464.4524s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0895644\n",
      "\tspeed: 0.1071s/iter; left time: 5329.8063s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0789975\n",
      "\tspeed: 0.1096s/iter; left time: 5440.5652s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0666418\n",
      "\tspeed: 0.1103s/iter; left time: 5466.5605s\n",
      "Epoch: 9 cost time: 00h:08m:07.22s\n",
      "Epoch: 9 | Train Loss: 0.0819719 Vali Loss: 0.0893733 Test Loss: 0.1011986\n",
      "Validation loss decreased (0.089866 --> 0.089373).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0804666\n",
      "\tspeed: 1.4863s/iter; left time: 73357.6544s\n",
      "\titers: 200, epoch: 10 | loss: 0.0945602\n",
      "\tspeed: 0.1068s/iter; left time: 5262.0357s\n",
      "\titers: 300, epoch: 10 | loss: 0.0714898\n",
      "\tspeed: 0.1039s/iter; left time: 5109.3410s\n",
      "\titers: 400, epoch: 10 | loss: 0.0826127\n",
      "\tspeed: 0.1065s/iter; left time: 5226.0831s\n",
      "\titers: 500, epoch: 10 | loss: 0.0924424\n",
      "\tspeed: 0.1054s/iter; left time: 5159.2442s\n",
      "\titers: 600, epoch: 10 | loss: 0.0706647\n",
      "\tspeed: 0.1053s/iter; left time: 5144.5541s\n",
      "\titers: 700, epoch: 10 | loss: 0.0966906\n",
      "\tspeed: 0.1032s/iter; left time: 5032.6570s\n",
      "\titers: 800, epoch: 10 | loss: 0.0714759\n",
      "\tspeed: 0.1039s/iter; left time: 5055.0802s\n",
      "\titers: 900, epoch: 10 | loss: 0.0955755\n",
      "\tspeed: 0.1062s/iter; left time: 5158.1279s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0649942\n",
      "\tspeed: 0.1036s/iter; left time: 5019.2797s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0779102\n",
      "\tspeed: 0.1044s/iter; left time: 5050.6463s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0758527\n",
      "\tspeed: 0.1072s/iter; left time: 5174.5789s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0841196\n",
      "\tspeed: 0.1065s/iter; left time: 5130.6830s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0752938\n",
      "\tspeed: 0.1070s/iter; left time: 5141.2591s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0925946\n",
      "\tspeed: 0.1082s/iter; left time: 5189.7579s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0713231\n",
      "\tspeed: 0.1038s/iter; left time: 4969.1839s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0798582\n",
      "\tspeed: 0.1127s/iter; left time: 5380.6080s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0845527\n",
      "\tspeed: 0.1071s/iter; left time: 5105.6895s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0802245\n",
      "\tspeed: 0.1076s/iter; left time: 5118.1966s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0924563\n",
      "\tspeed: 0.1104s/iter; left time: 5237.8094s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0730820\n",
      "\tspeed: 0.1012s/iter; left time: 4792.8719s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0877439\n",
      "\tspeed: 0.1074s/iter; left time: 5076.2265s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0808664\n",
      "\tspeed: 0.1048s/iter; left time: 4940.3052s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0805754\n",
      "\tspeed: 0.1059s/iter; left time: 4983.9050s\n",
      "\titers: 2500, epoch: 10 | loss: 0.1015715\n",
      "\tspeed: 0.1105s/iter; left time: 5190.6395s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0852922\n",
      "\tspeed: 0.1080s/iter; left time: 5059.2391s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0820987\n",
      "\tspeed: 0.1081s/iter; left time: 5054.6425s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0819424\n",
      "\tspeed: 0.1097s/iter; left time: 5116.0070s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0815215\n",
      "\tspeed: 0.1092s/iter; left time: 5083.0218s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0779925\n",
      "\tspeed: 0.1052s/iter; left time: 4888.9427s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0649120\n",
      "\tspeed: 0.1102s/iter; left time: 5108.8914s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0969833\n",
      "\tspeed: 0.1060s/iter; left time: 4901.9683s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0975729\n",
      "\tspeed: 0.1111s/iter; left time: 5128.5113s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0759113\n",
      "\tspeed: 0.1112s/iter; left time: 5121.4709s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0715975\n",
      "\tspeed: 0.1098s/iter; left time: 5048.0698s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0765458\n",
      "\tspeed: 0.1084s/iter; left time: 4972.8561s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0926370\n",
      "\tspeed: 0.1044s/iter; left time: 4778.0693s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0954896\n",
      "\tspeed: 0.1069s/iter; left time: 4880.9876s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0901705\n",
      "\tspeed: 0.1108s/iter; left time: 5048.6446s\n",
      "\titers: 4000, epoch: 10 | loss: 0.1011319\n",
      "\tspeed: 0.1098s/iter; left time: 4990.8753s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0829659\n",
      "\tspeed: 0.1103s/iter; left time: 5002.8850s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0952567\n",
      "\tspeed: 0.1095s/iter; left time: 4953.7826s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0830213\n",
      "\tspeed: 0.1058s/iter; left time: 4779.4080s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0838863\n",
      "\tspeed: 0.1099s/iter; left time: 4949.9624s\n",
      "Epoch: 10 cost time: 00h:08m:03.38s\n",
      "Epoch: 10 | Train Loss: 0.0815905 Vali Loss: 0.0900862 Test Loss: 0.1020951\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0954875\n",
      "\tspeed: 1.4550s/iter; left time: 65271.2623s\n",
      "\titers: 200, epoch: 11 | loss: 0.0963455\n",
      "\tspeed: 0.1072s/iter; left time: 4799.7547s\n",
      "\titers: 300, epoch: 11 | loss: 0.0792705\n",
      "\tspeed: 0.1024s/iter; left time: 4574.7534s\n",
      "\titers: 400, epoch: 11 | loss: 0.0801558\n",
      "\tspeed: 0.1040s/iter; left time: 4635.7560s\n",
      "\titers: 500, epoch: 11 | loss: 0.1015538\n",
      "\tspeed: 0.1042s/iter; left time: 4634.8272s\n",
      "\titers: 600, epoch: 11 | loss: 0.0732061\n",
      "\tspeed: 0.1071s/iter; left time: 4751.7992s\n",
      "\titers: 700, epoch: 11 | loss: 0.0891110\n",
      "\tspeed: 0.1045s/iter; left time: 4625.1676s\n",
      "\titers: 800, epoch: 11 | loss: 0.0999442\n",
      "\tspeed: 0.1068s/iter; left time: 4717.5128s\n",
      "\titers: 900, epoch: 11 | loss: 0.0800853\n",
      "\tspeed: 0.1101s/iter; left time: 4852.0515s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0720467\n",
      "\tspeed: 0.1090s/iter; left time: 4793.5842s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0852719\n",
      "\tspeed: 0.1118s/iter; left time: 4905.7622s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0710463\n",
      "\tspeed: 0.1097s/iter; left time: 4799.9479s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0742209\n",
      "\tspeed: 0.1043s/iter; left time: 4554.0590s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0696262\n",
      "\tspeed: 0.1074s/iter; left time: 4679.7211s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0862678\n",
      "\tspeed: 0.1109s/iter; left time: 4818.6020s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0709689\n",
      "\tspeed: 0.1081s/iter; left time: 4687.5179s\n",
      "\titers: 1700, epoch: 11 | loss: 0.1065298\n",
      "\tspeed: 0.1079s/iter; left time: 4665.8927s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0639064\n",
      "\tspeed: 0.1081s/iter; left time: 4666.2391s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0827991\n",
      "\tspeed: 0.1073s/iter; left time: 4620.0134s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0793500\n",
      "\tspeed: 0.1103s/iter; left time: 4736.9666s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0814809\n",
      "\tspeed: 0.1079s/iter; left time: 4623.5706s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0918499\n",
      "\tspeed: 0.1014s/iter; left time: 4335.5316s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0911202\n",
      "\tspeed: 0.1064s/iter; left time: 4538.3178s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0762489\n",
      "\tspeed: 0.1056s/iter; left time: 4493.1566s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0705035\n",
      "\tspeed: 0.1079s/iter; left time: 4580.0884s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0713519\n",
      "\tspeed: 0.1070s/iter; left time: 4533.2017s\n",
      "\titers: 2700, epoch: 11 | loss: 0.0821046\n",
      "\tspeed: 0.1100s/iter; left time: 4648.8504s\n",
      "\titers: 2800, epoch: 11 | loss: 0.0885580\n",
      "\tspeed: 0.1076s/iter; left time: 4538.2544s\n",
      "\titers: 2900, epoch: 11 | loss: 0.1035070\n",
      "\tspeed: 0.1077s/iter; left time: 4528.9081s\n",
      "\titers: 3000, epoch: 11 | loss: 0.0753226\n",
      "\tspeed: 0.1110s/iter; left time: 4658.2426s\n",
      "\titers: 3100, epoch: 11 | loss: 0.0714463\n",
      "\tspeed: 0.1099s/iter; left time: 4602.4249s\n",
      "\titers: 3200, epoch: 11 | loss: 0.0673974\n",
      "\tspeed: 0.1101s/iter; left time: 4595.9124s\n",
      "\titers: 3300, epoch: 11 | loss: 0.0771680\n",
      "\tspeed: 0.1084s/iter; left time: 4517.1873s\n",
      "\titers: 3400, epoch: 11 | loss: 0.1020880\n",
      "\tspeed: 0.1084s/iter; left time: 4504.3102s\n",
      "\titers: 3500, epoch: 11 | loss: 0.0695742\n",
      "\tspeed: 0.1058s/iter; left time: 4386.2004s\n",
      "\titers: 3600, epoch: 11 | loss: 0.0783400\n",
      "\tspeed: 0.1074s/iter; left time: 4440.7268s\n",
      "\titers: 3700, epoch: 11 | loss: 0.0872828\n",
      "\tspeed: 0.1115s/iter; left time: 4599.9908s\n",
      "\titers: 3800, epoch: 11 | loss: 0.1151122\n",
      "\tspeed: 0.1067s/iter; left time: 4391.6117s\n",
      "\titers: 3900, epoch: 11 | loss: 0.0821990\n",
      "\tspeed: 0.1072s/iter; left time: 4402.9774s\n",
      "\titers: 4000, epoch: 11 | loss: 0.0874506\n",
      "\tspeed: 0.1060s/iter; left time: 4343.0573s\n",
      "\titers: 4100, epoch: 11 | loss: 0.0895397\n",
      "\tspeed: 0.1070s/iter; left time: 4370.8742s\n",
      "\titers: 4200, epoch: 11 | loss: 0.0791727\n",
      "\tspeed: 0.1031s/iter; left time: 4203.4028s\n",
      "\titers: 4300, epoch: 11 | loss: 0.0826887\n",
      "\tspeed: 0.1092s/iter; left time: 4438.7178s\n",
      "\titers: 4400, epoch: 11 | loss: 0.0772736\n",
      "\tspeed: 0.1077s/iter; left time: 4369.1912s\n",
      "Epoch: 11 cost time: 00h:08m:04.10s\n",
      "Epoch: 11 | Train Loss: 0.0812005 Vali Loss: 0.0898660 Test Loss: 0.1020890\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0749976\n",
      "\tspeed: 1.4556s/iter; left time: 58756.9022s\n",
      "\titers: 200, epoch: 12 | loss: 0.0952725\n",
      "\tspeed: 0.1065s/iter; left time: 4289.6273s\n",
      "\titers: 300, epoch: 12 | loss: 0.0830348\n",
      "\tspeed: 0.1096s/iter; left time: 4400.5693s\n",
      "\titers: 400, epoch: 12 | loss: 0.0752531\n",
      "\tspeed: 0.1069s/iter; left time: 4280.9562s\n",
      "\titers: 500, epoch: 12 | loss: 0.0754286\n",
      "\tspeed: 0.1077s/iter; left time: 4304.1247s\n",
      "\titers: 600, epoch: 12 | loss: 0.0688584\n",
      "\tspeed: 0.1023s/iter; left time: 4079.0040s\n",
      "\titers: 700, epoch: 12 | loss: 0.0959318\n",
      "\tspeed: 0.1121s/iter; left time: 4458.9278s\n",
      "\titers: 800, epoch: 12 | loss: 0.0900387\n",
      "\tspeed: 0.1073s/iter; left time: 4256.0677s\n",
      "\titers: 900, epoch: 12 | loss: 0.0738986\n",
      "\tspeed: 0.1089s/iter; left time: 4308.1859s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0676233\n",
      "\tspeed: 0.1106s/iter; left time: 4365.5835s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0989304\n",
      "\tspeed: 0.1078s/iter; left time: 4244.1168s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0711347\n",
      "\tspeed: 0.1088s/iter; left time: 4271.1816s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0813206\n",
      "\tspeed: 0.1084s/iter; left time: 4243.7253s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0788712\n",
      "\tspeed: 0.1089s/iter; left time: 4254.6615s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0956492\n",
      "\tspeed: 0.1081s/iter; left time: 4213.0126s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0699590\n",
      "\tspeed: 0.1080s/iter; left time: 4196.8728s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0868768\n",
      "\tspeed: 0.1063s/iter; left time: 4120.3310s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0729411\n",
      "\tspeed: 0.1082s/iter; left time: 4181.6406s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0975851\n",
      "\tspeed: 0.1111s/iter; left time: 4283.0566s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0993183\n",
      "\tspeed: 0.1085s/iter; left time: 4171.9905s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0732370\n",
      "\tspeed: 0.1108s/iter; left time: 4251.6781s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0761602\n",
      "\tspeed: 0.1122s/iter; left time: 4293.5015s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0807065\n",
      "\tspeed: 0.1080s/iter; left time: 4121.3202s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0800232\n",
      "\tspeed: 0.1035s/iter; left time: 3939.6190s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0855232\n",
      "\tspeed: 0.1056s/iter; left time: 4009.5346s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0662207\n",
      "\tspeed: 0.1056s/iter; left time: 3997.0229s\n",
      "\titers: 2700, epoch: 12 | loss: 0.0743456\n",
      "\tspeed: 0.1110s/iter; left time: 4192.5218s\n",
      "\titers: 2800, epoch: 12 | loss: 0.0744488\n",
      "\tspeed: 0.1060s/iter; left time: 3993.9229s\n",
      "\titers: 2900, epoch: 12 | loss: 0.0875774\n",
      "\tspeed: 0.1047s/iter; left time: 3933.6907s\n",
      "\titers: 3000, epoch: 12 | loss: 0.0959495\n",
      "\tspeed: 0.1060s/iter; left time: 3971.4837s\n",
      "\titers: 3100, epoch: 12 | loss: 0.0766992\n",
      "\tspeed: 0.1080s/iter; left time: 4036.6462s\n",
      "\titers: 3200, epoch: 12 | loss: 0.1069204\n",
      "\tspeed: 0.1120s/iter; left time: 4174.6158s\n",
      "\titers: 3300, epoch: 12 | loss: 0.0903072\n",
      "\tspeed: 0.1115s/iter; left time: 4142.5623s\n",
      "\titers: 3400, epoch: 12 | loss: 0.0947618\n",
      "\tspeed: 0.1073s/iter; left time: 3978.7131s\n",
      "\titers: 3500, epoch: 12 | loss: 0.0750581\n",
      "\tspeed: 0.1036s/iter; left time: 3831.4114s\n",
      "\titers: 3600, epoch: 12 | loss: 0.0677229\n",
      "\tspeed: 0.1084s/iter; left time: 3997.1009s\n",
      "\titers: 3700, epoch: 12 | loss: 0.0825931\n",
      "\tspeed: 0.1081s/iter; left time: 3973.8821s\n",
      "\titers: 3800, epoch: 12 | loss: 0.0663846\n",
      "\tspeed: 0.1060s/iter; left time: 3884.6832s\n",
      "\titers: 3900, epoch: 12 | loss: 0.0917371\n",
      "\tspeed: 0.1075s/iter; left time: 3931.7501s\n",
      "\titers: 4000, epoch: 12 | loss: 0.0844175\n",
      "\tspeed: 0.1066s/iter; left time: 3887.4367s\n",
      "\titers: 4100, epoch: 12 | loss: 0.0886744\n",
      "\tspeed: 0.1089s/iter; left time: 3958.6021s\n",
      "\titers: 4200, epoch: 12 | loss: 0.0929067\n",
      "\tspeed: 0.1079s/iter; left time: 3912.8887s\n",
      "\titers: 4300, epoch: 12 | loss: 0.0852121\n",
      "\tspeed: 0.1067s/iter; left time: 3860.3138s\n",
      "\titers: 4400, epoch: 12 | loss: 0.0680903\n",
      "\tspeed: 0.1112s/iter; left time: 4011.7576s\n",
      "Epoch: 12 cost time: 00h:08m:05.89s\n",
      "Epoch: 12 | Train Loss: 0.0808781 Vali Loss: 0.0895198 Test Loss: 0.1023670\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0941976\n",
      "\tspeed: 1.4546s/iter; left time: 52174.8459s\n",
      "\titers: 200, epoch: 13 | loss: 0.0721684\n",
      "\tspeed: 0.1104s/iter; left time: 3947.4954s\n",
      "\titers: 300, epoch: 13 | loss: 0.0847256\n",
      "\tspeed: 0.1083s/iter; left time: 3864.1808s\n",
      "\titers: 400, epoch: 13 | loss: 0.1075728\n",
      "\tspeed: 0.1071s/iter; left time: 3810.8300s\n",
      "\titers: 500, epoch: 13 | loss: 0.0736173\n",
      "\tspeed: 0.1069s/iter; left time: 3793.3650s\n",
      "\titers: 600, epoch: 13 | loss: 0.0811842\n",
      "\tspeed: 0.1108s/iter; left time: 3918.6544s\n",
      "\titers: 700, epoch: 13 | loss: 0.0647019\n",
      "\tspeed: 0.1064s/iter; left time: 3753.0734s\n",
      "\titers: 800, epoch: 13 | loss: 0.1009579\n",
      "\tspeed: 0.1078s/iter; left time: 3790.5590s\n",
      "\titers: 900, epoch: 13 | loss: 0.0781825\n",
      "\tspeed: 0.1050s/iter; left time: 3682.7212s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0830416\n",
      "\tspeed: 0.1079s/iter; left time: 3774.3421s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0865278\n",
      "\tspeed: 0.1098s/iter; left time: 3828.0344s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0713144\n",
      "\tspeed: 0.1072s/iter; left time: 3725.7387s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0747054\n",
      "\tspeed: 0.1075s/iter; left time: 3726.8043s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0833910\n",
      "\tspeed: 0.1099s/iter; left time: 3798.9485s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0931288\n",
      "\tspeed: 0.1079s/iter; left time: 3718.0445s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0980143\n",
      "\tspeed: 0.1069s/iter; left time: 3675.7590s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0957459\n",
      "\tspeed: 0.1097s/iter; left time: 3758.7045s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0775483\n",
      "\tspeed: 0.1064s/iter; left time: 3634.4467s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0813203\n",
      "\tspeed: 0.1075s/iter; left time: 3663.9863s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0666705\n",
      "\tspeed: 0.1049s/iter; left time: 3562.7860s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0789099\n",
      "\tspeed: 0.1056s/iter; left time: 3577.9141s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0800629\n",
      "\tspeed: 0.1114s/iter; left time: 3760.1997s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0889463\n",
      "\tspeed: 0.1077s/iter; left time: 3625.9342s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0802381\n",
      "\tspeed: 0.1075s/iter; left time: 3609.9553s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0825546\n",
      "\tspeed: 0.1083s/iter; left time: 3625.2111s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0956851\n",
      "\tspeed: 0.1044s/iter; left time: 3485.0140s\n",
      "\titers: 2700, epoch: 13 | loss: 0.0856287\n",
      "\tspeed: 0.1031s/iter; left time: 3429.1250s\n",
      "\titers: 2800, epoch: 13 | loss: 0.1151457\n",
      "\tspeed: 0.1070s/iter; left time: 3549.4356s\n",
      "\titers: 2900, epoch: 13 | loss: 0.0953493\n",
      "\tspeed: 0.1076s/iter; left time: 3558.8930s\n",
      "\titers: 3000, epoch: 13 | loss: 0.0769377\n",
      "\tspeed: 0.1126s/iter; left time: 3712.9364s\n",
      "\titers: 3100, epoch: 13 | loss: 0.0727554\n",
      "\tspeed: 0.1106s/iter; left time: 3635.8976s\n",
      "\titers: 3200, epoch: 13 | loss: 0.0860205\n",
      "\tspeed: 0.1076s/iter; left time: 3526.6133s\n",
      "\titers: 3300, epoch: 13 | loss: 0.0888060\n",
      "\tspeed: 0.1038s/iter; left time: 3389.9225s\n",
      "\titers: 3400, epoch: 13 | loss: 0.0835236\n",
      "\tspeed: 0.1047s/iter; left time: 3410.0364s\n",
      "\titers: 3500, epoch: 13 | loss: 0.0703757\n",
      "\tspeed: 0.1057s/iter; left time: 3431.4315s\n",
      "\titers: 3600, epoch: 13 | loss: 0.0632925\n",
      "\tspeed: 0.1075s/iter; left time: 3478.4818s\n",
      "\titers: 3700, epoch: 13 | loss: 0.0658673\n",
      "\tspeed: 0.1066s/iter; left time: 3440.8265s\n",
      "\titers: 3800, epoch: 13 | loss: 0.0704690\n",
      "\tspeed: 0.1087s/iter; left time: 3498.0750s\n",
      "\titers: 3900, epoch: 13 | loss: 0.0829609\n",
      "\tspeed: 0.1065s/iter; left time: 3413.9607s\n",
      "\titers: 4000, epoch: 13 | loss: 0.0752248\n",
      "\tspeed: 0.1053s/iter; left time: 3365.6733s\n",
      "\titers: 4100, epoch: 13 | loss: 0.0729631\n",
      "\tspeed: 0.1063s/iter; left time: 3386.1781s\n",
      "\titers: 4200, epoch: 13 | loss: 0.0699356\n",
      "\tspeed: 0.1087s/iter; left time: 3453.0041s\n",
      "\titers: 4300, epoch: 13 | loss: 0.0826374\n",
      "\tspeed: 0.1099s/iter; left time: 3481.2287s\n",
      "\titers: 4400, epoch: 13 | loss: 0.0771110\n",
      "\tspeed: 0.1064s/iter; left time: 3359.4156s\n",
      "Epoch: 13 cost time: 00h:08m:04.11s\n",
      "Epoch: 13 | Train Loss: 0.0804917 Vali Loss: 0.0901564 Test Loss: 0.1021227\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0726042\n",
      "\tspeed: 1.4495s/iter; left time: 45476.5048s\n",
      "\titers: 200, epoch: 14 | loss: 0.0950259\n",
      "\tspeed: 0.1076s/iter; left time: 3364.6307s\n",
      "\titers: 300, epoch: 14 | loss: 0.0974067\n",
      "\tspeed: 0.1079s/iter; left time: 3363.5395s\n",
      "\titers: 400, epoch: 14 | loss: 0.0626494\n",
      "\tspeed: 0.1086s/iter; left time: 3373.3374s\n",
      "\titers: 500, epoch: 14 | loss: 0.0716593\n",
      "\tspeed: 0.1055s/iter; left time: 3266.4000s\n",
      "\titers: 600, epoch: 14 | loss: 0.0849337\n",
      "\tspeed: 0.1108s/iter; left time: 3420.4207s\n",
      "\titers: 700, epoch: 14 | loss: 0.0826007\n",
      "\tspeed: 0.1101s/iter; left time: 3387.3099s\n",
      "\titers: 800, epoch: 14 | loss: 0.0776362\n",
      "\tspeed: 0.1077s/iter; left time: 3304.1345s\n",
      "\titers: 900, epoch: 14 | loss: 0.0640049\n",
      "\tspeed: 0.1058s/iter; left time: 3235.4062s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0785243\n",
      "\tspeed: 0.1041s/iter; left time: 3171.9672s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0750521\n",
      "\tspeed: 0.1087s/iter; left time: 3300.1446s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0951579\n",
      "\tspeed: 0.1021s/iter; left time: 3089.9786s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0776445\n",
      "\tspeed: 0.1113s/iter; left time: 3357.0679s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0870477\n",
      "\tspeed: 0.1059s/iter; left time: 3184.9689s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0794556\n",
      "\tspeed: 0.1069s/iter; left time: 3203.1593s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0775292\n",
      "\tspeed: 0.1096s/iter; left time: 3273.3867s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0893918\n",
      "\tspeed: 0.1069s/iter; left time: 3183.1147s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0848042\n",
      "\tspeed: 0.1055s/iter; left time: 3129.6010s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0841384\n",
      "\tspeed: 0.1057s/iter; left time: 3126.2171s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0834215\n",
      "\tspeed: 0.1064s/iter; left time: 3136.4897s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0687327\n",
      "\tspeed: 0.1073s/iter; left time: 3151.7779s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0857227\n",
      "\tspeed: 0.1084s/iter; left time: 3172.1934s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0868264\n",
      "\tspeed: 0.1084s/iter; left time: 3161.2270s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0743876\n",
      "\tspeed: 0.1093s/iter; left time: 3179.0395s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0917093\n",
      "\tspeed: 0.1080s/iter; left time: 3129.3420s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0774212\n",
      "\tspeed: 0.1094s/iter; left time: 3157.7117s\n",
      "\titers: 2700, epoch: 14 | loss: 0.0725287\n",
      "\tspeed: 0.1075s/iter; left time: 3094.0871s\n",
      "\titers: 2800, epoch: 14 | loss: 0.0763098\n",
      "\tspeed: 0.1096s/iter; left time: 3142.6273s\n",
      "\titers: 2900, epoch: 14 | loss: 0.0796201\n",
      "\tspeed: 0.1094s/iter; left time: 3125.3975s\n",
      "\titers: 3000, epoch: 14 | loss: 0.0665898\n",
      "\tspeed: 0.1109s/iter; left time: 3158.3338s\n",
      "\titers: 3100, epoch: 14 | loss: 0.0787124\n",
      "\tspeed: 0.1109s/iter; left time: 3145.5942s\n",
      "\titers: 3200, epoch: 14 | loss: 0.1122939\n",
      "\tspeed: 0.1145s/iter; left time: 3237.6875s\n",
      "\titers: 3300, epoch: 14 | loss: 0.0970755\n",
      "\tspeed: 0.1156s/iter; left time: 3257.6814s\n",
      "\titers: 3400, epoch: 14 | loss: 0.0779423\n",
      "\tspeed: 0.1159s/iter; left time: 3253.2467s\n",
      "\titers: 3500, epoch: 14 | loss: 0.0917225\n",
      "\tspeed: 0.1117s/iter; left time: 3124.7020s\n",
      "\titers: 3600, epoch: 14 | loss: 0.1003116\n",
      "\tspeed: 0.1090s/iter; left time: 3037.3148s\n",
      "\titers: 3700, epoch: 14 | loss: 0.0843237\n",
      "\tspeed: 0.1060s/iter; left time: 2942.8000s\n",
      "\titers: 3800, epoch: 14 | loss: 0.0673942\n",
      "\tspeed: 0.1037s/iter; left time: 2868.8723s\n",
      "\titers: 3900, epoch: 14 | loss: 0.0758277\n",
      "\tspeed: 0.1104s/iter; left time: 3044.4335s\n",
      "\titers: 4000, epoch: 14 | loss: 0.0841069\n",
      "\tspeed: 0.1100s/iter; left time: 3021.1814s\n",
      "\titers: 4100, epoch: 14 | loss: 0.0972049\n",
      "\tspeed: 0.1096s/iter; left time: 2999.6882s\n",
      "\titers: 4200, epoch: 14 | loss: 0.0760971\n",
      "\tspeed: 0.1091s/iter; left time: 2974.1384s\n",
      "\titers: 4300, epoch: 14 | loss: 0.0766167\n",
      "\tspeed: 0.1087s/iter; left time: 2953.0563s\n",
      "\titers: 4400, epoch: 14 | loss: 0.0637466\n",
      "\tspeed: 0.1071s/iter; left time: 2899.3748s\n",
      "Epoch: 14 cost time: 00h:08m:08.22s\n",
      "Epoch: 14 | Train Loss: 0.0801451 Vali Loss: 0.0898751 Test Loss: 0.1022130\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.025205422192811966, rmse:0.1587621569633484, mae:0.10119861364364624, rse:0.5474064350128174\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 24: 02h:27m:18.85s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 143525\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-02 11:09:05,393] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 11:09:06,553] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 11:09:06,553] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 11:09:06,553] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 11:09:06,653] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 11:09:06,654] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 11:09:07,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 11:09:07,352] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 11:09:07,353] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 11:09:07,354] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 11:09:07,354] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 11:09:07,354] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 11:09:07,670] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 11:09:07,671] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,671] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,803] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 11:09:07,804] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,804] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,804] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 11:09:07,925] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 11:09:07,926] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,926] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fea95d33c90>\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1523321\n",
      "\tspeed: 0.1642s/iter; left time: 14716.1394s\n",
      "\titers: 200, epoch: 1 | loss: 0.1570973\n",
      "\tspeed: 0.1209s/iter; left time: 10817.3486s\n",
      "\titers: 300, epoch: 1 | loss: 0.1372597\n",
      "\tspeed: 0.1232s/iter; left time: 11013.3695s\n",
      "\titers: 400, epoch: 1 | loss: 0.1611106\n",
      "\tspeed: 0.1167s/iter; left time: 10422.9751s\n",
      "\titers: 500, epoch: 1 | loss: 0.1438916\n",
      "\tspeed: 0.1254s/iter; left time: 11183.1044s\n",
      "\titers: 600, epoch: 1 | loss: 0.1517782\n",
      "\tspeed: 0.1189s/iter; left time: 10597.6413s\n",
      "\titers: 700, epoch: 1 | loss: 0.1311948\n",
      "\tspeed: 0.1241s/iter; left time: 11042.8284s\n",
      "\titers: 800, epoch: 1 | loss: 0.1038204\n",
      "\tspeed: 0.1179s/iter; left time: 10481.2214s\n",
      "\titers: 900, epoch: 1 | loss: 0.1193201\n",
      "\tspeed: 0.1216s/iter; left time: 10800.0383s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1048014\n",
      "\tspeed: 0.1208s/iter; left time: 10711.8491s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1129711\n",
      "\tspeed: 0.1191s/iter; left time: 10548.4015s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1085250\n",
      "\tspeed: 0.1219s/iter; left time: 10791.5197s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1155873\n",
      "\tspeed: 0.1198s/iter; left time: 10589.9816s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1127517\n",
      "\tspeed: 0.1150s/iter; left time: 10150.9681s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1008264\n",
      "\tspeed: 0.1230s/iter; left time: 10845.5863s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1333221\n",
      "\tspeed: 0.1181s/iter; left time: 10403.3260s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1157868\n",
      "\tspeed: 0.1178s/iter; left time: 10362.5586s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1128253\n",
      "\tspeed: 0.1150s/iter; left time: 10106.5354s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1154400\n",
      "\tspeed: 0.1197s/iter; left time: 10510.2128s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1206072\n",
      "\tspeed: 0.1213s/iter; left time: 10637.7866s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1116072\n",
      "\tspeed: 0.1179s/iter; left time: 10332.3801s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1246088\n",
      "\tspeed: 0.1199s/iter; left time: 10488.1211s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1120683\n",
      "\tspeed: 0.1125s/iter; left time: 9832.5137s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1298170\n",
      "\tspeed: 0.1194s/iter; left time: 10426.5338s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1402837\n",
      "\tspeed: 0.1181s/iter; left time: 10296.0634s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1254882\n",
      "\tspeed: 0.1183s/iter; left time: 10305.0880s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1003666\n",
      "\tspeed: 0.1171s/iter; left time: 10189.6375s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1112671\n",
      "\tspeed: 0.1172s/iter; left time: 10185.2528s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1137624\n",
      "\tspeed: 0.1198s/iter; left time: 10402.4663s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1242562\n",
      "\tspeed: 0.1185s/iter; left time: 10278.3318s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1275978\n",
      "\tspeed: 0.1142s/iter; left time: 9893.9340s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1161146\n",
      "\tspeed: 0.1141s/iter; left time: 9870.8651s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1244878\n",
      "\tspeed: 0.1178s/iter; left time: 10178.6933s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1080193\n",
      "\tspeed: 0.1216s/iter; left time: 10497.7282s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1138757\n",
      "\tspeed: 0.1196s/iter; left time: 10309.6014s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1165035\n",
      "\tspeed: 0.1162s/iter; left time: 10009.1216s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1057020\n",
      "\tspeed: 0.1226s/iter; left time: 10542.4120s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0968045\n",
      "\tspeed: 0.1225s/iter; left time: 10519.0971s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1031482\n",
      "\tspeed: 0.1164s/iter; left time: 9983.3868s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1125728\n",
      "\tspeed: 0.1196s/iter; left time: 10247.4907s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1198684\n",
      "\tspeed: 0.1194s/iter; left time: 10223.1208s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1088336\n",
      "\tspeed: 0.1180s/iter; left time: 10091.9961s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1251617\n",
      "\tspeed: 0.1194s/iter; left time: 10195.8646s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1200401\n",
      "\tspeed: 0.1176s/iter; left time: 10031.5046s\n",
      "Epoch: 1 cost time: 00h:08m:55.29s\n",
      "Epoch: 1 | Train Loss: 0.1178117 Vali Loss: 0.1184595 Test Loss: 0.1412180\n",
      "Validation loss decreased (inf --> 0.118459).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0888638\n",
      "\tspeed: 1.6645s/iter; left time: 141672.5188s\n",
      "\titers: 200, epoch: 2 | loss: 0.1137921\n",
      "\tspeed: 0.1082s/iter; left time: 9202.4579s\n",
      "\titers: 300, epoch: 2 | loss: 0.1233345\n",
      "\tspeed: 0.1080s/iter; left time: 9173.1189s\n",
      "\titers: 400, epoch: 2 | loss: 0.1065918\n",
      "\tspeed: 0.1019s/iter; left time: 8639.2576s\n",
      "\titers: 500, epoch: 2 | loss: 0.0876302\n",
      "\tspeed: 0.1067s/iter; left time: 9043.3081s\n",
      "\titers: 600, epoch: 2 | loss: 0.1139689\n",
      "\tspeed: 0.1069s/iter; left time: 9046.5598s\n",
      "\titers: 700, epoch: 2 | loss: 0.1090599\n",
      "\tspeed: 0.1102s/iter; left time: 9316.3301s\n",
      "\titers: 800, epoch: 2 | loss: 0.0918770\n",
      "\tspeed: 0.1101s/iter; left time: 9290.4094s\n",
      "\titers: 900, epoch: 2 | loss: 0.1054808\n",
      "\tspeed: 0.1096s/iter; left time: 9244.5952s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1240198\n",
      "\tspeed: 0.1094s/iter; left time: 9214.8151s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1188949\n",
      "\tspeed: 0.1060s/iter; left time: 8912.7695s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1082713\n",
      "\tspeed: 0.1068s/iter; left time: 8970.2357s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1121853\n",
      "\tspeed: 0.1107s/iter; left time: 9291.2580s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1057807\n",
      "\tspeed: 0.1094s/iter; left time: 9167.4611s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1165954\n",
      "\tspeed: 0.1027s/iter; left time: 8595.6482s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1163039\n",
      "\tspeed: 0.1082s/iter; left time: 9048.2322s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1402899\n",
      "\tspeed: 0.1080s/iter; left time: 9019.2083s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0893394\n",
      "\tspeed: 0.1091s/iter; left time: 9103.0766s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1250467\n",
      "\tspeed: 0.1082s/iter; left time: 9012.4281s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0956506\n",
      "\tspeed: 0.1055s/iter; left time: 8779.0126s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1079546\n",
      "\tspeed: 0.1074s/iter; left time: 8925.6004s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1003428\n",
      "\tspeed: 0.1034s/iter; left time: 8584.0707s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1021225\n",
      "\tspeed: 0.1088s/iter; left time: 9022.0597s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1186273\n",
      "\tspeed: 0.1034s/iter; left time: 8566.1653s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1165032\n",
      "\tspeed: 0.1061s/iter; left time: 8774.0694s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1051225\n",
      "\tspeed: 0.1063s/iter; left time: 8779.5792s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1007597\n",
      "\tspeed: 0.1062s/iter; left time: 8762.9591s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0979256\n",
      "\tspeed: 0.1066s/iter; left time: 8782.6368s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1156221\n",
      "\tspeed: 0.1103s/iter; left time: 9078.6564s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1229274\n",
      "\tspeed: 0.1086s/iter; left time: 8929.6905s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1010010\n",
      "\tspeed: 0.1101s/iter; left time: 9038.0092s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0914321\n",
      "\tspeed: 0.1050s/iter; left time: 8608.4418s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0996214\n",
      "\tspeed: 0.1090s/iter; left time: 8932.6923s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1225803\n",
      "\tspeed: 0.1057s/iter; left time: 8645.2226s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1037378\n",
      "\tspeed: 0.1081s/iter; left time: 8834.8982s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0835612\n",
      "\tspeed: 0.1081s/iter; left time: 8819.2208s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1148943\n",
      "\tspeed: 0.1078s/iter; left time: 8789.5885s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1152604\n",
      "\tspeed: 0.1082s/iter; left time: 8807.0794s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1060695\n",
      "\tspeed: 0.1106s/iter; left time: 8992.8881s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0958115\n",
      "\tspeed: 0.1089s/iter; left time: 8840.9779s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1165176\n",
      "\tspeed: 0.1086s/iter; left time: 8810.6341s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1040161\n",
      "\tspeed: 0.1081s/iter; left time: 8754.8615s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1187941\n",
      "\tspeed: 0.1120s/iter; left time: 9065.4638s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1056552\n",
      "\tspeed: 0.1088s/iter; left time: 8793.8298s\n",
      "Epoch: 2 cost time: 00h:08m:03.68s\n",
      "Epoch: 2 | Train Loss: 0.1077817 Vali Loss: 0.1173895 Test Loss: 0.1434370\n",
      "Validation loss decreased (0.118459 --> 0.117389).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1310282\n",
      "\tspeed: 1.4511s/iter; left time: 117006.7300s\n",
      "\titers: 200, epoch: 3 | loss: 0.1085388\n",
      "\tspeed: 0.1104s/iter; left time: 8894.4253s\n",
      "\titers: 300, epoch: 3 | loss: 0.1202963\n",
      "\tspeed: 0.1118s/iter; left time: 8993.4722s\n",
      "\titers: 400, epoch: 3 | loss: 0.1198052\n",
      "\tspeed: 0.1101s/iter; left time: 8841.1450s\n",
      "\titers: 500, epoch: 3 | loss: 0.1075269\n",
      "\tspeed: 0.1087s/iter; left time: 8723.8894s\n",
      "\titers: 600, epoch: 3 | loss: 0.0981584\n",
      "\tspeed: 0.1058s/iter; left time: 8480.3468s\n",
      "\titers: 700, epoch: 3 | loss: 0.1196058\n",
      "\tspeed: 0.1064s/iter; left time: 8511.6395s\n",
      "\titers: 800, epoch: 3 | loss: 0.1243347\n",
      "\tspeed: 0.1088s/iter; left time: 8696.4165s\n",
      "\titers: 900, epoch: 3 | loss: 0.1134376\n",
      "\tspeed: 0.1091s/iter; left time: 8712.2458s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1046471\n",
      "\tspeed: 0.1055s/iter; left time: 8408.6168s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1102815\n",
      "\tspeed: 0.1084s/iter; left time: 8632.7852s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0851324\n",
      "\tspeed: 0.1071s/iter; left time: 8521.0054s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0943000\n",
      "\tspeed: 0.1090s/iter; left time: 8655.1238s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0990528\n",
      "\tspeed: 0.1087s/iter; left time: 8621.0262s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0970403\n",
      "\tspeed: 0.1089s/iter; left time: 8625.9391s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0781570\n",
      "\tspeed: 0.1078s/iter; left time: 8528.1395s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0904602\n",
      "\tspeed: 0.1093s/iter; left time: 8634.1985s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1183398\n",
      "\tspeed: 0.1104s/iter; left time: 8716.5300s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1106287\n",
      "\tspeed: 0.1113s/iter; left time: 8772.8133s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1017435\n",
      "\tspeed: 0.1107s/iter; left time: 8712.6664s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0943806\n",
      "\tspeed: 0.1108s/iter; left time: 8715.7095s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1140733\n",
      "\tspeed: 0.1109s/iter; left time: 8710.2082s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1258210\n",
      "\tspeed: 0.1093s/iter; left time: 8571.6508s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0975943\n",
      "\tspeed: 0.1101s/iter; left time: 8623.4105s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1019286\n",
      "\tspeed: 0.1087s/iter; left time: 8507.3984s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1040863\n",
      "\tspeed: 0.1103s/iter; left time: 8615.2479s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1137355\n",
      "\tspeed: 0.1101s/iter; left time: 8591.0793s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1007091\n",
      "\tspeed: 0.1104s/iter; left time: 8604.1474s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1016200\n",
      "\tspeed: 0.1113s/iter; left time: 8660.1653s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1144505\n",
      "\tspeed: 0.1112s/iter; left time: 8642.1682s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1279829\n",
      "\tspeed: 0.1097s/iter; left time: 8513.7131s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1213689\n",
      "\tspeed: 0.1090s/iter; left time: 8451.7399s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1051345\n",
      "\tspeed: 0.1110s/iter; left time: 8597.2565s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1120226\n",
      "\tspeed: 0.1148s/iter; left time: 8874.1319s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1024189\n",
      "\tspeed: 0.1163s/iter; left time: 8980.1940s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0943960\n",
      "\tspeed: 0.1137s/iter; left time: 8772.9666s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1066416\n",
      "\tspeed: 0.1149s/iter; left time: 8854.0322s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0961487\n",
      "\tspeed: 0.1144s/iter; left time: 8803.2502s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0950556\n",
      "\tspeed: 0.1143s/iter; left time: 8778.9043s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1026345\n",
      "\tspeed: 0.1149s/iter; left time: 8814.8270s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1027451\n",
      "\tspeed: 0.1094s/iter; left time: 8386.3927s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1061701\n",
      "\tspeed: 0.1059s/iter; left time: 8105.5696s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0936683\n",
      "\tspeed: 0.1023s/iter; left time: 7818.0689s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1012166\n",
      "\tspeed: 0.1054s/iter; left time: 8046.8008s\n",
      "Epoch: 3 cost time: 00h:08m:13.06s\n",
      "Epoch: 3 | Train Loss: 0.1043771 Vali Loss: 0.1187507 Test Loss: 0.1443793\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1020158\n",
      "\tspeed: 1.4266s/iter; left time: 108629.7933s\n",
      "\titers: 200, epoch: 4 | loss: 0.1003834\n",
      "\tspeed: 0.1065s/iter; left time: 8099.1186s\n",
      "\titers: 300, epoch: 4 | loss: 0.1127172\n",
      "\tspeed: 0.1054s/iter; left time: 8005.5924s\n",
      "\titers: 400, epoch: 4 | loss: 0.1082551\n",
      "\tspeed: 0.1117s/iter; left time: 8468.3907s\n",
      "\titers: 500, epoch: 4 | loss: 0.1041521\n",
      "\tspeed: 0.1102s/iter; left time: 8350.9373s\n",
      "\titers: 600, epoch: 4 | loss: 0.1070160\n",
      "\tspeed: 0.1089s/iter; left time: 8237.1190s\n",
      "\titers: 700, epoch: 4 | loss: 0.1449660\n",
      "\tspeed: 0.1088s/iter; left time: 8222.5166s\n",
      "\titers: 800, epoch: 4 | loss: 0.0877978\n",
      "\tspeed: 0.1106s/iter; left time: 8345.7626s\n",
      "\titers: 900, epoch: 4 | loss: 0.0985263\n",
      "\tspeed: 0.1102s/iter; left time: 8304.9653s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1049943\n",
      "\tspeed: 0.1072s/iter; left time: 8069.5046s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0958291\n",
      "\tspeed: 0.1103s/iter; left time: 8288.1820s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0853065\n",
      "\tspeed: 0.1109s/iter; left time: 8324.7872s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0855185\n",
      "\tspeed: 0.1059s/iter; left time: 7939.1575s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0994194\n",
      "\tspeed: 0.1074s/iter; left time: 8038.8326s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1068409\n",
      "\tspeed: 0.1097s/iter; left time: 8196.6038s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1120063\n",
      "\tspeed: 0.1123s/iter; left time: 8379.7792s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1221094\n",
      "\tspeed: 0.1110s/iter; left time: 8274.3631s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0878371\n",
      "\tspeed: 0.1078s/iter; left time: 8023.7988s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1065904\n",
      "\tspeed: 0.1085s/iter; left time: 8069.4740s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1217954\n",
      "\tspeed: 0.1044s/iter; left time: 7747.9174s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0763485\n",
      "\tspeed: 0.1073s/iter; left time: 7958.5286s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1082068\n",
      "\tspeed: 0.1090s/iter; left time: 8070.6059s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0984239\n",
      "\tspeed: 0.1061s/iter; left time: 7844.5492s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0943878\n",
      "\tspeed: 0.1040s/iter; left time: 7681.8681s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1143633\n",
      "\tspeed: 0.1093s/iter; left time: 8060.9938s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1017567\n",
      "\tspeed: 0.1068s/iter; left time: 7866.4857s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0997897\n",
      "\tspeed: 0.1085s/iter; left time: 7979.5815s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1115867\n",
      "\tspeed: 0.1097s/iter; left time: 8059.9033s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0922957\n",
      "\tspeed: 0.1011s/iter; left time: 7417.0598s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1038117\n",
      "\tspeed: 0.1058s/iter; left time: 7746.4603s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0926916\n",
      "\tspeed: 0.1085s/iter; left time: 7933.7269s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0785196\n",
      "\tspeed: 0.1055s/iter; left time: 7706.2941s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1024314\n",
      "\tspeed: 0.1066s/iter; left time: 7777.9395s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0930109\n",
      "\tspeed: 0.0964s/iter; left time: 7019.4791s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0849443\n",
      "\tspeed: 0.1076s/iter; left time: 7830.5603s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0900325\n",
      "\tspeed: 0.1052s/iter; left time: 7642.6536s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1102086\n",
      "\tspeed: 0.1036s/iter; left time: 7515.5689s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0800017\n",
      "\tspeed: 0.1056s/iter; left time: 7651.7566s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0829896\n",
      "\tspeed: 0.1077s/iter; left time: 7789.8891s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0955721\n",
      "\tspeed: 0.1121s/iter; left time: 8097.7927s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1014783\n",
      "\tspeed: 0.1107s/iter; left time: 7987.8152s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1079314\n",
      "\tspeed: 0.1064s/iter; left time: 7664.9586s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1017345\n",
      "\tspeed: 0.1061s/iter; left time: 7632.5592s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0957486\n",
      "\tspeed: 0.1083s/iter; left time: 7778.6908s\n",
      "Epoch: 4 cost time: 00h:08m:02.79s\n",
      "Epoch: 4 | Train Loss: 0.1004036 Vali Loss: 0.1205356 Test Loss: 0.1469130\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1013267\n",
      "\tspeed: 1.4324s/iter; left time: 102648.4436s\n",
      "\titers: 200, epoch: 5 | loss: 0.1063943\n",
      "\tspeed: 0.1077s/iter; left time: 7703.5968s\n",
      "\titers: 300, epoch: 5 | loss: 0.1020837\n",
      "\tspeed: 0.1118s/iter; left time: 7991.0566s\n",
      "\titers: 400, epoch: 5 | loss: 0.1110498\n",
      "\tspeed: 0.1102s/iter; left time: 7867.0923s\n",
      "\titers: 500, epoch: 5 | loss: 0.1179720\n",
      "\tspeed: 0.1061s/iter; left time: 7559.8261s\n",
      "\titers: 600, epoch: 5 | loss: 0.1031104\n",
      "\tspeed: 0.1075s/iter; left time: 7650.7286s\n",
      "\titers: 700, epoch: 5 | loss: 0.0947505\n",
      "\tspeed: 0.1100s/iter; left time: 7815.1928s\n",
      "\titers: 800, epoch: 5 | loss: 0.0988153\n",
      "\tspeed: 0.1023s/iter; left time: 7257.7675s\n",
      "\titers: 900, epoch: 5 | loss: 0.0980647\n",
      "\tspeed: 0.1092s/iter; left time: 7736.7493s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1027513\n",
      "\tspeed: 0.1090s/iter; left time: 7711.4596s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1019618\n",
      "\tspeed: 0.1066s/iter; left time: 7531.4308s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0872446\n",
      "\tspeed: 0.1048s/iter; left time: 7393.3369s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0995278\n",
      "\tspeed: 0.1102s/iter; left time: 7767.5143s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0856419\n",
      "\tspeed: 0.1063s/iter; left time: 7480.2797s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0984738\n",
      "\tspeed: 0.1074s/iter; left time: 7547.4283s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0897058\n",
      "\tspeed: 0.1081s/iter; left time: 7581.8695s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0837954\n",
      "\tspeed: 0.1081s/iter; left time: 7573.9114s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1022424\n",
      "\tspeed: 0.1108s/iter; left time: 7751.0991s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1100879\n",
      "\tspeed: 0.1067s/iter; left time: 7457.5476s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1003026\n",
      "\tspeed: 0.1099s/iter; left time: 7669.4448s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0989896\n",
      "\tspeed: 0.1127s/iter; left time: 7853.8883s\n",
      "\titers: 2200, epoch: 5 | loss: 0.1024873\n",
      "\tspeed: 0.1112s/iter; left time: 7738.3216s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1089459\n",
      "\tspeed: 0.1093s/iter; left time: 7593.1926s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1069664\n",
      "\tspeed: 0.1097s/iter; left time: 7606.0668s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0821716\n",
      "\tspeed: 0.1092s/iter; left time: 7565.3457s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0913218\n",
      "\tspeed: 0.1044s/iter; left time: 7219.4036s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0889648\n",
      "\tspeed: 0.1084s/iter; left time: 7482.9710s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0932212\n",
      "\tspeed: 0.1138s/iter; left time: 7850.3600s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0853349\n",
      "\tspeed: 0.1050s/iter; left time: 7233.3472s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0956874\n",
      "\tspeed: 0.1033s/iter; left time: 7104.3960s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0857884\n",
      "\tspeed: 0.1133s/iter; left time: 7782.6325s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0957507\n",
      "\tspeed: 0.1091s/iter; left time: 7478.0101s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0887952\n",
      "\tspeed: 0.1111s/iter; left time: 7607.7668s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0878523\n",
      "\tspeed: 0.1104s/iter; left time: 7544.4456s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0892572\n",
      "\tspeed: 0.1069s/iter; left time: 7295.6108s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1093634\n",
      "\tspeed: 0.1074s/iter; left time: 7320.9435s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0850211\n",
      "\tspeed: 0.1037s/iter; left time: 7057.2676s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0877998\n",
      "\tspeed: 0.1081s/iter; left time: 7348.3714s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1017472\n",
      "\tspeed: 0.1090s/iter; left time: 7399.1499s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0930923\n",
      "\tspeed: 0.1088s/iter; left time: 7375.1654s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0939313\n",
      "\tspeed: 0.1073s/iter; left time: 7258.2624s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0919951\n",
      "\tspeed: 0.1029s/iter; left time: 6950.2808s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0943117\n",
      "\tspeed: 0.1106s/iter; left time: 7464.2538s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0894200\n",
      "\tspeed: 0.1101s/iter; left time: 7414.6584s\n",
      "Epoch: 5 cost time: 00h:08m:06.31s\n",
      "Epoch: 5 | Train Loss: 0.0966544 Vali Loss: 0.1223298 Test Loss: 0.1498027\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0925073\n",
      "\tspeed: 1.4349s/iter; left time: 96391.4087s\n",
      "\titers: 200, epoch: 6 | loss: 0.1033233\n",
      "\tspeed: 0.1065s/iter; left time: 7143.5485s\n",
      "\titers: 300, epoch: 6 | loss: 0.1017852\n",
      "\tspeed: 0.1045s/iter; left time: 6996.4023s\n",
      "\titers: 400, epoch: 6 | loss: 0.0813918\n",
      "\tspeed: 0.1054s/iter; left time: 7051.0602s\n",
      "\titers: 500, epoch: 6 | loss: 0.1012578\n",
      "\tspeed: 0.1046s/iter; left time: 6986.6502s\n",
      "\titers: 600, epoch: 6 | loss: 0.0925734\n",
      "\tspeed: 0.1053s/iter; left time: 7019.1729s\n",
      "\titers: 700, epoch: 6 | loss: 0.0816648\n",
      "\tspeed: 0.1059s/iter; left time: 7051.2712s\n",
      "\titers: 800, epoch: 6 | loss: 0.0903690\n",
      "\tspeed: 0.1054s/iter; left time: 7008.7325s\n",
      "\titers: 900, epoch: 6 | loss: 0.1008372\n",
      "\tspeed: 0.1073s/iter; left time: 7119.6150s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0913643\n",
      "\tspeed: 0.1080s/iter; left time: 7158.1052s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1034070\n",
      "\tspeed: 0.1102s/iter; left time: 7291.8303s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1074490\n",
      "\tspeed: 0.1054s/iter; left time: 6966.9283s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0876377\n",
      "\tspeed: 0.1064s/iter; left time: 7017.0102s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1026839\n",
      "\tspeed: 0.1084s/iter; left time: 7141.4853s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0969250\n",
      "\tspeed: 0.1130s/iter; left time: 7435.6748s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0847653\n",
      "\tspeed: 0.1105s/iter; left time: 7254.9230s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0786244\n",
      "\tspeed: 0.1098s/iter; left time: 7202.3233s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0895295\n",
      "\tspeed: 0.1110s/iter; left time: 7266.2878s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0960533\n",
      "\tspeed: 0.1112s/iter; left time: 7269.8468s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1027374\n",
      "\tspeed: 0.1042s/iter; left time: 6802.7242s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0939934\n",
      "\tspeed: 0.1096s/iter; left time: 7140.3883s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0843199\n",
      "\tspeed: 0.1080s/iter; left time: 7025.6123s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0809122\n",
      "\tspeed: 0.1056s/iter; left time: 6864.2003s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1014044\n",
      "\tspeed: 0.1072s/iter; left time: 6957.7941s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0871974\n",
      "\tspeed: 0.1046s/iter; left time: 6775.4703s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0921036\n",
      "\tspeed: 0.1055s/iter; left time: 6823.3170s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1040257\n",
      "\tspeed: 0.1067s/iter; left time: 6893.3519s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1056846\n",
      "\tspeed: 0.1114s/iter; left time: 7185.6550s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0859896\n",
      "\tspeed: 0.1093s/iter; left time: 7033.2292s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0964024\n",
      "\tspeed: 0.1030s/iter; left time: 6619.1718s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0688204\n",
      "\tspeed: 0.1062s/iter; left time: 6816.5541s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0952048\n",
      "\tspeed: 0.1090s/iter; left time: 6987.1033s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1085152\n",
      "\tspeed: 0.1089s/iter; left time: 6967.9033s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0811777\n",
      "\tspeed: 0.1089s/iter; left time: 6958.1351s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1095598\n",
      "\tspeed: 0.1048s/iter; left time: 6682.1048s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1067103\n",
      "\tspeed: 0.1071s/iter; left time: 6818.2425s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0980428\n",
      "\tspeed: 0.1116s/iter; left time: 7095.8458s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0960030\n",
      "\tspeed: 0.1074s/iter; left time: 6816.9789s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0844862\n",
      "\tspeed: 0.1094s/iter; left time: 6931.5156s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0948572\n",
      "\tspeed: 0.1122s/iter; left time: 7098.3958s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0972997\n",
      "\tspeed: 0.1041s/iter; left time: 6578.0725s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0840513\n",
      "\tspeed: 0.1005s/iter; left time: 6342.2825s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0846837\n",
      "\tspeed: 0.1068s/iter; left time: 6726.8852s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0994278\n",
      "\tspeed: 0.1067s/iter; left time: 6710.9728s\n",
      "Epoch: 6 cost time: 00h:08m:02.05s\n",
      "Epoch: 6 | Train Loss: 0.0935319 Vali Loss: 0.1227152 Test Loss: 0.1494228\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0913173\n",
      "\tspeed: 1.4293s/iter; left time: 89602.9337s\n",
      "\titers: 200, epoch: 7 | loss: 0.1163411\n",
      "\tspeed: 0.1066s/iter; left time: 6672.4964s\n",
      "\titers: 300, epoch: 7 | loss: 0.0927998\n",
      "\tspeed: 0.1023s/iter; left time: 6393.4464s\n",
      "\titers: 400, epoch: 7 | loss: 0.0917988\n",
      "\tspeed: 0.1084s/iter; left time: 6766.0541s\n",
      "\titers: 500, epoch: 7 | loss: 0.0816753\n",
      "\tspeed: 0.1080s/iter; left time: 6724.9604s\n",
      "\titers: 600, epoch: 7 | loss: 0.1129898\n",
      "\tspeed: 0.1079s/iter; left time: 6710.5650s\n",
      "\titers: 700, epoch: 7 | loss: 0.0816540\n",
      "\tspeed: 0.1090s/iter; left time: 6768.0810s\n",
      "\titers: 800, epoch: 7 | loss: 0.1145246\n",
      "\tspeed: 0.1110s/iter; left time: 6878.3435s\n",
      "\titers: 900, epoch: 7 | loss: 0.0861063\n",
      "\tspeed: 0.1078s/iter; left time: 6671.9489s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0930915\n",
      "\tspeed: 0.1067s/iter; left time: 6595.2577s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0832640\n",
      "\tspeed: 0.1060s/iter; left time: 6540.8371s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1068092\n",
      "\tspeed: 0.1047s/iter; left time: 6445.8849s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1094583\n",
      "\tspeed: 0.1059s/iter; left time: 6509.7356s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0916336\n",
      "\tspeed: 0.1056s/iter; left time: 6482.0607s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0932818\n",
      "\tspeed: 0.1042s/iter; left time: 6384.5600s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0976237\n",
      "\tspeed: 0.1079s/iter; left time: 6604.1286s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1042555\n",
      "\tspeed: 0.1116s/iter; left time: 6817.9485s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0780962\n",
      "\tspeed: 0.1084s/iter; left time: 6609.7655s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0822116\n",
      "\tspeed: 0.1047s/iter; left time: 6373.6247s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0880490\n",
      "\tspeed: 0.1074s/iter; left time: 6528.3157s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0868164\n",
      "\tspeed: 0.1072s/iter; left time: 6507.6568s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0720907\n",
      "\tspeed: 0.1071s/iter; left time: 6491.6283s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0788777\n",
      "\tspeed: 0.1096s/iter; left time: 6631.8210s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0901415\n",
      "\tspeed: 0.1063s/iter; left time: 6419.7458s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0850017\n",
      "\tspeed: 0.1067s/iter; left time: 6433.9397s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0886241\n",
      "\tspeed: 0.1072s/iter; left time: 6449.7304s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0978564\n",
      "\tspeed: 0.1041s/iter; left time: 6255.0896s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0925564\n",
      "\tspeed: 0.1065s/iter; left time: 6389.7855s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1175411\n",
      "\tspeed: 0.1077s/iter; left time: 6451.2886s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0782495\n",
      "\tspeed: 0.1077s/iter; left time: 6441.0094s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0809937\n",
      "\tspeed: 0.1041s/iter; left time: 6211.7480s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0883481\n",
      "\tspeed: 0.1054s/iter; left time: 6280.1993s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0891999\n",
      "\tspeed: 0.1065s/iter; left time: 6332.8541s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0984669\n",
      "\tspeed: 0.1078s/iter; left time: 6401.0366s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0943845\n",
      "\tspeed: 0.1074s/iter; left time: 6369.6049s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0912553\n",
      "\tspeed: 0.1056s/iter; left time: 6248.8721s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0948698\n",
      "\tspeed: 0.1060s/iter; left time: 6261.7698s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0706965\n",
      "\tspeed: 0.1072s/iter; left time: 6323.6486s\n",
      "\titers: 3900, epoch: 7 | loss: 0.1006198\n",
      "\tspeed: 0.1067s/iter; left time: 6283.7804s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0881463\n",
      "\tspeed: 0.1084s/iter; left time: 6370.4621s\n",
      "\titers: 4100, epoch: 7 | loss: 0.1020899\n",
      "\tspeed: 0.1086s/iter; left time: 6371.3175s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0827523\n",
      "\tspeed: 0.1070s/iter; left time: 6269.6178s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1077781\n",
      "\tspeed: 0.1092s/iter; left time: 6388.1090s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0829007\n",
      "\tspeed: 0.1105s/iter; left time: 6453.8770s\n",
      "Epoch: 7 cost time: 00h:08m:01.26s\n",
      "Epoch: 7 | Train Loss: 0.0908818 Vali Loss: 0.1233640 Test Loss: 0.1499765\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0437089167535305, rmse:0.20906677842140198, mae:0.143436998128891, rse:0.7229486107826233\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 96: 01h:13m:33.46s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 143165\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-02 12:22:39,024] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 12:22:39,984] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 12:22:39,985] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 12:22:39,985] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 12:22:40,074] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 12:22:40,074] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 12:22:40,708] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 12:22:40,709] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 12:22:40,709] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 12:22:40,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 12:22:40,710] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 12:22:40,711] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 12:22:41,051] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 12:22:41,052] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.53 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,176] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 12:22:41,177] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,177] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.53 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,177] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 12:22:41,295] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 12:22:41,296] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.54 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 12:22:41,298] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0822259f10>\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1492039\n",
      "\tspeed: 0.1558s/iter; left time: 13920.1947s\n",
      "\titers: 200, epoch: 1 | loss: 0.1571750\n",
      "\tspeed: 0.1229s/iter; left time: 10972.2522s\n",
      "\titers: 300, epoch: 1 | loss: 0.1447653\n",
      "\tspeed: 0.1187s/iter; left time: 10586.4424s\n",
      "\titers: 400, epoch: 1 | loss: 0.1463560\n",
      "\tspeed: 0.1149s/iter; left time: 10234.8297s\n",
      "\titers: 500, epoch: 1 | loss: 0.1403078\n",
      "\tspeed: 0.1148s/iter; left time: 10213.2770s\n",
      "\titers: 600, epoch: 1 | loss: 0.1426290\n",
      "\tspeed: 0.1173s/iter; left time: 10421.6835s\n",
      "\titers: 700, epoch: 1 | loss: 0.1221828\n",
      "\tspeed: 0.1194s/iter; left time: 10599.4566s\n",
      "\titers: 800, epoch: 1 | loss: 0.1188438\n",
      "\tspeed: 0.1100s/iter; left time: 9748.9378s\n",
      "\titers: 900, epoch: 1 | loss: 0.1083227\n",
      "\tspeed: 0.1164s/iter; left time: 10306.3772s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1414768\n",
      "\tspeed: 0.1181s/iter; left time: 10450.8120s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1065454\n",
      "\tspeed: 0.1194s/iter; left time: 10553.2745s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1062137\n",
      "\tspeed: 0.1160s/iter; left time: 10237.9912s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1094389\n",
      "\tspeed: 0.1209s/iter; left time: 10656.9320s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1425477\n",
      "\tspeed: 0.1170s/iter; left time: 10306.5998s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1190569\n",
      "\tspeed: 0.1203s/iter; left time: 10578.8675s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1307243\n",
      "\tspeed: 0.1175s/iter; left time: 10327.7661s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1101217\n",
      "\tspeed: 0.1158s/iter; left time: 10165.8935s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1066481\n",
      "\tspeed: 0.1168s/iter; left time: 10235.8467s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1185276\n",
      "\tspeed: 0.1188s/iter; left time: 10399.7914s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1198424\n",
      "\tspeed: 0.1118s/iter; left time: 9778.2014s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1289180\n",
      "\tspeed: 0.1156s/iter; left time: 10096.3713s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1175074\n",
      "\tspeed: 0.1172s/iter; left time: 10222.9627s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1311432\n",
      "\tspeed: 0.1216s/iter; left time: 10596.7156s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1330000\n",
      "\tspeed: 0.1184s/iter; left time: 10305.9827s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1254928\n",
      "\tspeed: 0.1184s/iter; left time: 10299.5504s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1172541\n",
      "\tspeed: 0.1183s/iter; left time: 10272.4490s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1503642\n",
      "\tspeed: 0.1170s/iter; left time: 10152.3566s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1218206\n",
      "\tspeed: 0.1193s/iter; left time: 10341.2122s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1123841\n",
      "\tspeed: 0.1196s/iter; left time: 10353.4101s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1095140\n",
      "\tspeed: 0.1198s/iter; left time: 10360.1082s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1169514\n",
      "\tspeed: 0.1222s/iter; left time: 10555.3391s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1298743\n",
      "\tspeed: 0.1197s/iter; left time: 10325.1203s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1165393\n",
      "\tspeed: 0.1233s/iter; left time: 10622.9657s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0974565\n",
      "\tspeed: 0.1217s/iter; left time: 10474.5007s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1041744\n",
      "\tspeed: 0.1236s/iter; left time: 10620.6693s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0992299\n",
      "\tspeed: 0.1231s/iter; left time: 10572.3640s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1239099\n",
      "\tspeed: 0.1227s/iter; left time: 10524.0074s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0990040\n",
      "\tspeed: 0.1242s/iter; left time: 10642.4687s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1218022\n",
      "\tspeed: 0.1245s/iter; left time: 10655.7635s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0961539\n",
      "\tspeed: 0.1249s/iter; left time: 10676.5093s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1266004\n",
      "\tspeed: 0.1237s/iter; left time: 10561.9273s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1165152\n",
      "\tspeed: 0.1205s/iter; left time: 10275.6515s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1041826\n",
      "\tspeed: 0.1114s/iter; left time: 9484.2204s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1090055\n",
      "\tspeed: 0.1152s/iter; left time: 9801.7413s\n",
      "Epoch: 1 cost time: 00h:08m:52.60s\n",
      "Epoch: 1 | Train Loss: 0.1213396 Vali Loss: 0.1232667 Test Loss: 0.1479476\n",
      "Validation loss decreased (inf --> 0.123267).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1373354\n",
      "\tspeed: 1.6252s/iter; left time: 137957.5657s\n",
      "\titers: 200, epoch: 2 | loss: 0.1157282\n",
      "\tspeed: 0.1053s/iter; left time: 8929.5690s\n",
      "\titers: 300, epoch: 2 | loss: 0.1186823\n",
      "\tspeed: 0.1097s/iter; left time: 9286.5049s\n",
      "\titers: 400, epoch: 2 | loss: 0.1171655\n",
      "\tspeed: 0.0992s/iter; left time: 8390.7134s\n",
      "\titers: 500, epoch: 2 | loss: 0.1352277\n",
      "\tspeed: 0.1048s/iter; left time: 8850.3792s\n",
      "\titers: 600, epoch: 2 | loss: 0.1214827\n",
      "\tspeed: 0.1077s/iter; left time: 9087.7747s\n",
      "\titers: 700, epoch: 2 | loss: 0.1123141\n",
      "\tspeed: 0.1056s/iter; left time: 8898.3932s\n",
      "\titers: 800, epoch: 2 | loss: 0.1030472\n",
      "\tspeed: 0.1077s/iter; left time: 9069.4773s\n",
      "\titers: 900, epoch: 2 | loss: 0.1189815\n",
      "\tspeed: 0.1090s/iter; left time: 9169.2050s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1273633\n",
      "\tspeed: 0.1075s/iter; left time: 9030.3800s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1186239\n",
      "\tspeed: 0.1111s/iter; left time: 9321.8120s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1002740\n",
      "\tspeed: 0.1056s/iter; left time: 8848.6784s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1090776\n",
      "\tspeed: 0.1064s/iter; left time: 8904.2668s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1267764\n",
      "\tspeed: 0.1088s/iter; left time: 9095.7492s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1032072\n",
      "\tspeed: 0.1051s/iter; left time: 8772.7101s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1147069\n",
      "\tspeed: 0.1093s/iter; left time: 9112.9068s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1185646\n",
      "\tspeed: 0.1043s/iter; left time: 8686.1948s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0919052\n",
      "\tspeed: 0.1061s/iter; left time: 8827.8832s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1263514\n",
      "\tspeed: 0.1035s/iter; left time: 8597.2256s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1148299\n",
      "\tspeed: 0.1074s/iter; left time: 8916.4972s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1236556\n",
      "\tspeed: 0.1052s/iter; left time: 8716.8750s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1127978\n",
      "\tspeed: 0.1105s/iter; left time: 9150.8093s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0918464\n",
      "\tspeed: 0.1044s/iter; left time: 8632.4879s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1098604\n",
      "\tspeed: 0.1088s/iter; left time: 8988.8185s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1160145\n",
      "\tspeed: 0.1060s/iter; left time: 8745.7367s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1331852\n",
      "\tspeed: 0.1073s/iter; left time: 8842.2601s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0922735\n",
      "\tspeed: 0.1023s/iter; left time: 8418.5501s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1118323\n",
      "\tspeed: 0.1027s/iter; left time: 8438.4606s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1124189\n",
      "\tspeed: 0.1078s/iter; left time: 8851.2279s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1435691\n",
      "\tspeed: 0.1032s/iter; left time: 8459.3364s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1239352\n",
      "\tspeed: 0.1095s/iter; left time: 8969.4330s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1055086\n",
      "\tspeed: 0.1061s/iter; left time: 8675.8780s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1100568\n",
      "\tspeed: 0.1097s/iter; left time: 8965.1607s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1202246\n",
      "\tspeed: 0.1047s/iter; left time: 8541.5707s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1090674\n",
      "\tspeed: 0.1061s/iter; left time: 8642.5609s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1158136\n",
      "\tspeed: 0.1045s/iter; left time: 8508.9976s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1121945\n",
      "\tspeed: 0.1071s/iter; left time: 8705.8946s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0966658\n",
      "\tspeed: 0.1079s/iter; left time: 8758.5712s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0989787\n",
      "\tspeed: 0.1048s/iter; left time: 8495.3783s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1137499\n",
      "\tspeed: 0.1074s/iter; left time: 8700.3233s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1053024\n",
      "\tspeed: 0.1056s/iter; left time: 8543.3856s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1246223\n",
      "\tspeed: 0.1080s/iter; left time: 8722.7086s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1095846\n",
      "\tspeed: 0.1019s/iter; left time: 8221.0973s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1080796\n",
      "\tspeed: 0.1051s/iter; left time: 8465.9405s\n",
      "Epoch: 2 cost time: 00h:07m:55.95s\n",
      "Epoch: 2 | Train Loss: 0.1117745 Vali Loss: 0.1218290 Test Loss: 0.1485942\n",
      "Validation loss decreased (0.123267 --> 0.121829).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1073814\n",
      "\tspeed: 1.4114s/iter; left time: 113498.1080s\n",
      "\titers: 200, epoch: 3 | loss: 0.1077686\n",
      "\tspeed: 0.1066s/iter; left time: 8560.0081s\n",
      "\titers: 300, epoch: 3 | loss: 0.1152448\n",
      "\tspeed: 0.1063s/iter; left time: 8529.2180s\n",
      "\titers: 400, epoch: 3 | loss: 0.1090948\n",
      "\tspeed: 0.1083s/iter; left time: 8674.1726s\n",
      "\titers: 500, epoch: 3 | loss: 0.1247426\n",
      "\tspeed: 0.1057s/iter; left time: 8457.5316s\n",
      "\titers: 600, epoch: 3 | loss: 0.1079610\n",
      "\tspeed: 0.1063s/iter; left time: 8495.0244s\n",
      "\titers: 700, epoch: 3 | loss: 0.0978039\n",
      "\tspeed: 0.1015s/iter; left time: 8101.7318s\n",
      "\titers: 800, epoch: 3 | loss: 0.1046700\n",
      "\tspeed: 0.1065s/iter; left time: 8491.1226s\n",
      "\titers: 900, epoch: 3 | loss: 0.1096871\n",
      "\tspeed: 0.1093s/iter; left time: 8699.4375s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1161537\n",
      "\tspeed: 0.1058s/iter; left time: 8415.1731s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1075704\n",
      "\tspeed: 0.1060s/iter; left time: 8421.7693s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0993517\n",
      "\tspeed: 0.1085s/iter; left time: 8607.9020s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1047531\n",
      "\tspeed: 0.1059s/iter; left time: 8391.0421s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1217528\n",
      "\tspeed: 0.1048s/iter; left time: 8289.6904s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1148034\n",
      "\tspeed: 0.1097s/iter; left time: 8671.7472s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1048180\n",
      "\tspeed: 0.1082s/iter; left time: 8541.0903s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1097087\n",
      "\tspeed: 0.1065s/iter; left time: 8393.4404s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1012860\n",
      "\tspeed: 0.1089s/iter; left time: 8569.7534s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1285569\n",
      "\tspeed: 0.1107s/iter; left time: 8701.9393s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1124301\n",
      "\tspeed: 0.1103s/iter; left time: 8661.1579s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1148856\n",
      "\tspeed: 0.1060s/iter; left time: 8312.6979s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0905116\n",
      "\tspeed: 0.1039s/iter; left time: 8133.1436s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1224068\n",
      "\tspeed: 0.1103s/iter; left time: 8630.6263s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1213229\n",
      "\tspeed: 0.1031s/iter; left time: 8051.0014s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1161782\n",
      "\tspeed: 0.1136s/iter; left time: 8859.4862s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1238268\n",
      "\tspeed: 0.1084s/iter; left time: 8444.4547s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1218241\n",
      "\tspeed: 0.1141s/iter; left time: 8876.6183s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1076142\n",
      "\tspeed: 0.1111s/iter; left time: 8631.2765s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1070125\n",
      "\tspeed: 0.1054s/iter; left time: 8179.2377s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1054063\n",
      "\tspeed: 0.1093s/iter; left time: 8474.4103s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0906125\n",
      "\tspeed: 0.1092s/iter; left time: 8454.3057s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1038693\n",
      "\tspeed: 0.1100s/iter; left time: 8501.2242s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0932214\n",
      "\tspeed: 0.1023s/iter; left time: 7896.3574s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1064298\n",
      "\tspeed: 0.1070s/iter; left time: 8255.0368s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1178679\n",
      "\tspeed: 0.1087s/iter; left time: 8370.1033s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1058693\n",
      "\tspeed: 0.1068s/iter; left time: 8216.2819s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1053510\n",
      "\tspeed: 0.1035s/iter; left time: 7951.2590s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1111913\n",
      "\tspeed: 0.1086s/iter; left time: 8335.0260s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1219772\n",
      "\tspeed: 0.1066s/iter; left time: 8170.2211s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1169674\n",
      "\tspeed: 0.1051s/iter; left time: 8043.9718s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1220436\n",
      "\tspeed: 0.1082s/iter; left time: 8266.0103s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0994433\n",
      "\tspeed: 0.1064s/iter; left time: 8118.7653s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1174926\n",
      "\tspeed: 0.1058s/iter; left time: 8066.3382s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1029727\n",
      "\tspeed: 0.1032s/iter; left time: 7851.9695s\n",
      "Epoch: 3 cost time: 00h:08m:00.44s\n",
      "Epoch: 3 | Train Loss: 0.1101085 Vali Loss: 0.1225816 Test Loss: 0.1483277\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1163232\n",
      "\tspeed: 1.3902s/iter; left time: 105574.8514s\n",
      "\titers: 200, epoch: 4 | loss: 0.1172400\n",
      "\tspeed: 0.1052s/iter; left time: 7979.3333s\n",
      "\titers: 300, epoch: 4 | loss: 0.1053314\n",
      "\tspeed: 0.1014s/iter; left time: 7683.8419s\n",
      "\titers: 400, epoch: 4 | loss: 0.1159020\n",
      "\tspeed: 0.1082s/iter; left time: 8184.8829s\n",
      "\titers: 500, epoch: 4 | loss: 0.1131369\n",
      "\tspeed: 0.1073s/iter; left time: 8103.7347s\n",
      "\titers: 600, epoch: 4 | loss: 0.1101619\n",
      "\tspeed: 0.1095s/iter; left time: 8259.0579s\n",
      "\titers: 700, epoch: 4 | loss: 0.1124621\n",
      "\tspeed: 0.1051s/iter; left time: 7918.2770s\n",
      "\titers: 800, epoch: 4 | loss: 0.1200308\n",
      "\tspeed: 0.1080s/iter; left time: 8122.9645s\n",
      "\titers: 900, epoch: 4 | loss: 0.1239205\n",
      "\tspeed: 0.0996s/iter; left time: 7487.8467s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1212897\n",
      "\tspeed: 0.1060s/iter; left time: 7951.4048s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1169482\n",
      "\tspeed: 0.1068s/iter; left time: 8000.2019s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0989428\n",
      "\tspeed: 0.1103s/iter; left time: 8254.4950s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1034586\n",
      "\tspeed: 0.1093s/iter; left time: 8167.2503s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1046819\n",
      "\tspeed: 0.1049s/iter; left time: 7829.5851s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1052269\n",
      "\tspeed: 0.1072s/iter; left time: 7993.7522s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1120703\n",
      "\tspeed: 0.1058s/iter; left time: 7873.0954s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1059639\n",
      "\tspeed: 0.1120s/iter; left time: 8326.0777s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0905024\n",
      "\tspeed: 0.1119s/iter; left time: 8306.9460s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0942608\n",
      "\tspeed: 0.1082s/iter; left time: 8025.5263s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1152182\n",
      "\tspeed: 0.1061s/iter; left time: 7854.8691s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1152445\n",
      "\tspeed: 0.1064s/iter; left time: 7869.0397s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1241190\n",
      "\tspeed: 0.1049s/iter; left time: 7743.5570s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1150569\n",
      "\tspeed: 0.1040s/iter; left time: 7667.1770s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0937953\n",
      "\tspeed: 0.1090s/iter; left time: 8024.5135s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1057690\n",
      "\tspeed: 0.1082s/iter; left time: 7954.1924s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1197121\n",
      "\tspeed: 0.1095s/iter; left time: 8040.6101s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1218844\n",
      "\tspeed: 0.1095s/iter; left time: 8028.6505s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1118397\n",
      "\tspeed: 0.1071s/iter; left time: 7843.6559s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1214791\n",
      "\tspeed: 0.1087s/iter; left time: 7953.8045s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0951268\n",
      "\tspeed: 0.1071s/iter; left time: 7823.3377s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1038058\n",
      "\tspeed: 0.1046s/iter; left time: 7629.2405s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1209671\n",
      "\tspeed: 0.1118s/iter; left time: 8145.5158s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1161438\n",
      "\tspeed: 0.1112s/iter; left time: 8090.8911s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1275270\n",
      "\tspeed: 0.1061s/iter; left time: 7707.2413s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1004139\n",
      "\tspeed: 0.1078s/iter; left time: 7822.1911s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1117803\n",
      "\tspeed: 0.1035s/iter; left time: 7495.5683s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1009324\n",
      "\tspeed: 0.1039s/iter; left time: 7517.2787s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1207588\n",
      "\tspeed: 0.1094s/iter; left time: 7904.0515s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1000504\n",
      "\tspeed: 0.1033s/iter; left time: 7449.6913s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0920369\n",
      "\tspeed: 0.1054s/iter; left time: 7592.6974s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0991762\n",
      "\tspeed: 0.1084s/iter; left time: 7795.7800s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0991054\n",
      "\tspeed: 0.1024s/iter; left time: 7355.2127s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0927669\n",
      "\tspeed: 0.1055s/iter; left time: 7566.6614s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1017505\n",
      "\tspeed: 0.1043s/iter; left time: 7472.9461s\n",
      "Epoch: 4 cost time: 00h:07m:57.80s\n",
      "Epoch: 4 | Train Loss: 0.1083292 Vali Loss: 0.1215903 Test Loss: 0.1486534\n",
      "Validation loss decreased (0.121829 --> 0.121590).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1045664\n",
      "\tspeed: 1.4170s/iter; left time: 101270.5476s\n",
      "\titers: 200, epoch: 5 | loss: 0.1140283\n",
      "\tspeed: 0.1071s/iter; left time: 7641.6778s\n",
      "\titers: 300, epoch: 5 | loss: 0.1094048\n",
      "\tspeed: 0.1077s/iter; left time: 7674.9133s\n",
      "\titers: 400, epoch: 5 | loss: 0.1113927\n",
      "\tspeed: 0.1065s/iter; left time: 7581.8474s\n",
      "\titers: 500, epoch: 5 | loss: 0.1201597\n",
      "\tspeed: 0.1049s/iter; left time: 7452.3135s\n",
      "\titers: 600, epoch: 5 | loss: 0.0986795\n",
      "\tspeed: 0.1046s/iter; left time: 7420.5370s\n",
      "\titers: 700, epoch: 5 | loss: 0.1055745\n",
      "\tspeed: 0.1063s/iter; left time: 7535.6783s\n",
      "\titers: 800, epoch: 5 | loss: 0.1066505\n",
      "\tspeed: 0.1145s/iter; left time: 8106.1170s\n",
      "\titers: 900, epoch: 5 | loss: 0.1212705\n",
      "\tspeed: 0.1070s/iter; left time: 7560.2260s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1213971\n",
      "\tspeed: 0.1028s/iter; left time: 7254.0450s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1246218\n",
      "\tspeed: 0.1091s/iter; left time: 7685.0141s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0953918\n",
      "\tspeed: 0.1048s/iter; left time: 7374.1161s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1080353\n",
      "\tspeed: 0.1040s/iter; left time: 7305.7016s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1139053\n",
      "\tspeed: 0.1047s/iter; left time: 7347.1126s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1034893\n",
      "\tspeed: 0.1097s/iter; left time: 7683.2607s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1069212\n",
      "\tspeed: 0.1073s/iter; left time: 7506.5233s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1042794\n",
      "\tspeed: 0.1095s/iter; left time: 7650.6211s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0975064\n",
      "\tspeed: 0.1081s/iter; left time: 7545.0210s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0997893\n",
      "\tspeed: 0.1116s/iter; left time: 7772.7529s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1189045\n",
      "\tspeed: 0.1074s/iter; left time: 7475.0823s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0985344\n",
      "\tspeed: 0.1089s/iter; left time: 7566.8525s\n",
      "\titers: 2200, epoch: 5 | loss: 0.1044963\n",
      "\tspeed: 0.1090s/iter; left time: 7558.6321s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0955015\n",
      "\tspeed: 0.1061s/iter; left time: 7350.4122s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0906615\n",
      "\tspeed: 0.1091s/iter; left time: 7543.3621s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1142124\n",
      "\tspeed: 0.1090s/iter; left time: 7525.9499s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1056934\n",
      "\tspeed: 0.1079s/iter; left time: 7441.6523s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0991283\n",
      "\tspeed: 0.1083s/iter; left time: 7461.5399s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1261882\n",
      "\tspeed: 0.1089s/iter; left time: 7488.7920s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1248673\n",
      "\tspeed: 0.1096s/iter; left time: 7524.6232s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1106820\n",
      "\tspeed: 0.1042s/iter; left time: 7143.1180s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0868233\n",
      "\tspeed: 0.1076s/iter; left time: 7364.6034s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1209657\n",
      "\tspeed: 0.1048s/iter; left time: 7165.2671s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1171299\n",
      "\tspeed: 0.1077s/iter; left time: 7353.8246s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1117137\n",
      "\tspeed: 0.1045s/iter; left time: 7125.2499s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1193861\n",
      "\tspeed: 0.1083s/iter; left time: 7368.9425s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1158210\n",
      "\tspeed: 0.1083s/iter; left time: 7363.0574s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0986645\n",
      "\tspeed: 0.1099s/iter; left time: 7456.6559s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1102005\n",
      "\tspeed: 0.1085s/iter; left time: 7352.9382s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1302816\n",
      "\tspeed: 0.1057s/iter; left time: 7152.6720s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1082248\n",
      "\tspeed: 0.1079s/iter; left time: 7290.3610s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1047803\n",
      "\tspeed: 0.1089s/iter; left time: 7345.9116s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1059765\n",
      "\tspeed: 0.1053s/iter; left time: 7091.0210s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0873054\n",
      "\tspeed: 0.1079s/iter; left time: 7258.0964s\n",
      "\titers: 4400, epoch: 5 | loss: 0.1012158\n",
      "\tspeed: 0.1042s/iter; left time: 7001.3385s\n",
      "Epoch: 5 cost time: 00h:08m:01.40s\n",
      "Epoch: 5 | Train Loss: 0.1060327 Vali Loss: 0.1232200 Test Loss: 0.1516566\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1077035\n",
      "\tspeed: 1.4035s/iter; left time: 94030.2833s\n",
      "\titers: 200, epoch: 6 | loss: 0.1206247\n",
      "\tspeed: 0.1074s/iter; left time: 7184.5248s\n",
      "\titers: 300, epoch: 6 | loss: 0.1046793\n",
      "\tspeed: 0.1044s/iter; left time: 6971.7072s\n",
      "\titers: 400, epoch: 6 | loss: 0.0925871\n",
      "\tspeed: 0.1091s/iter; left time: 7278.2041s\n",
      "\titers: 500, epoch: 6 | loss: 0.0974372\n",
      "\tspeed: 0.1096s/iter; left time: 7299.0817s\n",
      "\titers: 600, epoch: 6 | loss: 0.1108949\n",
      "\tspeed: 0.1086s/iter; left time: 7223.8138s\n",
      "\titers: 700, epoch: 6 | loss: 0.1134460\n",
      "\tspeed: 0.1101s/iter; left time: 7309.6112s\n",
      "\titers: 800, epoch: 6 | loss: 0.0946199\n",
      "\tspeed: 0.1071s/iter; left time: 7103.3394s\n",
      "\titers: 900, epoch: 6 | loss: 0.0851431\n",
      "\tspeed: 0.1081s/iter; left time: 7154.9901s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1054931\n",
      "\tspeed: 0.1100s/iter; left time: 7269.8564s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1074604\n",
      "\tspeed: 0.1037s/iter; left time: 6845.8158s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0996368\n",
      "\tspeed: 0.1108s/iter; left time: 7303.6716s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1049811\n",
      "\tspeed: 0.1074s/iter; left time: 7064.6280s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1002386\n",
      "\tspeed: 0.1093s/iter; left time: 7181.5366s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1112094\n",
      "\tspeed: 0.1092s/iter; left time: 7161.1726s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0859496\n",
      "\tspeed: 0.1080s/iter; left time: 7075.3391s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1008478\n",
      "\tspeed: 0.1079s/iter; left time: 7057.9541s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1067413\n",
      "\tspeed: 0.1082s/iter; left time: 7067.3598s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1091929\n",
      "\tspeed: 0.1096s/iter; left time: 7148.6972s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1032816\n",
      "\tspeed: 0.1079s/iter; left time: 7026.5516s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0979405\n",
      "\tspeed: 0.1039s/iter; left time: 6751.1192s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1004898\n",
      "\tspeed: 0.1035s/iter; left time: 6717.4816s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1015077\n",
      "\tspeed: 0.1087s/iter; left time: 7045.5153s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1144938\n",
      "\tspeed: 0.1025s/iter; left time: 6633.0213s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1002379\n",
      "\tspeed: 0.1101s/iter; left time: 7114.1428s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1211278\n",
      "\tspeed: 0.1075s/iter; left time: 6931.0536s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0977566\n",
      "\tspeed: 0.1100s/iter; left time: 7086.3973s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1079447\n",
      "\tspeed: 0.1057s/iter; left time: 6793.6288s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1073356\n",
      "\tspeed: 0.1090s/iter; left time: 6994.3897s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1240855\n",
      "\tspeed: 0.1067s/iter; left time: 6838.9963s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1030167\n",
      "\tspeed: 0.1030s/iter; left time: 6590.1235s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1003815\n",
      "\tspeed: 0.1048s/iter; left time: 6693.7532s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0925418\n",
      "\tspeed: 0.1090s/iter; left time: 6953.9689s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1049529\n",
      "\tspeed: 0.1094s/iter; left time: 6969.3809s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1082636\n",
      "\tspeed: 0.1070s/iter; left time: 6802.9650s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0990641\n",
      "\tspeed: 0.1055s/iter; left time: 6695.8233s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1075102\n",
      "\tspeed: 0.1093s/iter; left time: 6930.3496s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0963298\n",
      "\tspeed: 0.1073s/iter; left time: 6792.3152s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0935764\n",
      "\tspeed: 0.1070s/iter; left time: 6760.9030s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1077523\n",
      "\tspeed: 0.1069s/iter; left time: 6743.8358s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1060187\n",
      "\tspeed: 0.1052s/iter; left time: 6627.6069s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0974801\n",
      "\tspeed: 0.1114s/iter; left time: 7008.6623s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1071582\n",
      "\tspeed: 0.1063s/iter; left time: 6674.0641s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1114140\n",
      "\tspeed: 0.1048s/iter; left time: 6568.6052s\n",
      "Epoch: 6 cost time: 00h:08m:01.53s\n",
      "Epoch: 6 | Train Loss: 0.1031456 Vali Loss: 0.1234408 Test Loss: 0.1535768\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1085103\n",
      "\tspeed: 1.3971s/iter; left time: 87349.4317s\n",
      "\titers: 200, epoch: 7 | loss: 0.0954523\n",
      "\tspeed: 0.1077s/iter; left time: 6721.8729s\n",
      "\titers: 300, epoch: 7 | loss: 0.1043126\n",
      "\tspeed: 0.1081s/iter; left time: 6734.5693s\n",
      "\titers: 400, epoch: 7 | loss: 0.0980591\n",
      "\tspeed: 0.1057s/iter; left time: 6576.6214s\n",
      "\titers: 500, epoch: 7 | loss: 0.1085198\n",
      "\tspeed: 0.1074s/iter; left time: 6671.3621s\n",
      "\titers: 600, epoch: 7 | loss: 0.1165193\n",
      "\tspeed: 0.1090s/iter; left time: 6758.4967s\n",
      "\titers: 700, epoch: 7 | loss: 0.0997535\n",
      "\tspeed: 0.1054s/iter; left time: 6526.4705s\n",
      "\titers: 800, epoch: 7 | loss: 0.1118336\n",
      "\tspeed: 0.1088s/iter; left time: 6725.1179s\n",
      "\titers: 900, epoch: 7 | loss: 0.1122482\n",
      "\tspeed: 0.1045s/iter; left time: 6452.3190s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0908243\n",
      "\tspeed: 0.1102s/iter; left time: 6790.8038s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1042221\n",
      "\tspeed: 0.1037s/iter; left time: 6378.0513s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0784654\n",
      "\tspeed: 0.1106s/iter; left time: 6791.1001s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1083403\n",
      "\tspeed: 0.1066s/iter; left time: 6534.6632s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0916019\n",
      "\tspeed: 0.1115s/iter; left time: 6827.2860s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1041972\n",
      "\tspeed: 0.1087s/iter; left time: 6643.6646s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1050012\n",
      "\tspeed: 0.1072s/iter; left time: 6542.1357s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0990551\n",
      "\tspeed: 0.1104s/iter; left time: 6725.7379s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1112182\n",
      "\tspeed: 0.1071s/iter; left time: 6513.3864s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0934280\n",
      "\tspeed: 0.1108s/iter; left time: 6729.4060s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0962716\n",
      "\tspeed: 0.1102s/iter; left time: 6680.9932s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0991424\n",
      "\tspeed: 0.1107s/iter; left time: 6702.2612s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0782585\n",
      "\tspeed: 0.1090s/iter; left time: 6583.4074s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0830829\n",
      "\tspeed: 0.1090s/iter; left time: 6574.1501s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1002160\n",
      "\tspeed: 0.1094s/iter; left time: 6590.2696s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0800429\n",
      "\tspeed: 0.1082s/iter; left time: 6507.8364s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1001138\n",
      "\tspeed: 0.1092s/iter; left time: 6554.4086s\n",
      "\titers: 2700, epoch: 7 | loss: 0.1066886\n",
      "\tspeed: 0.1124s/iter; left time: 6735.6474s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1104560\n",
      "\tspeed: 0.1086s/iter; left time: 6498.8932s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1018884\n",
      "\tspeed: 0.1014s/iter; left time: 6058.3551s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1037253\n",
      "\tspeed: 0.0963s/iter; left time: 5740.6386s\n",
      "\titers: 3100, epoch: 7 | loss: 0.1261247\n",
      "\tspeed: 0.0961s/iter; left time: 5722.7381s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1097564\n",
      "\tspeed: 0.1116s/iter; left time: 6630.2530s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0908472\n",
      "\tspeed: 0.1141s/iter; left time: 6771.3907s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0831330\n",
      "\tspeed: 0.1136s/iter; left time: 6726.1647s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0842484\n",
      "\tspeed: 0.1135s/iter; left time: 6708.4212s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1124989\n",
      "\tspeed: 0.1128s/iter; left time: 6655.2255s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1083518\n",
      "\tspeed: 0.1097s/iter; left time: 6463.4231s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1133735\n",
      "\tspeed: 0.1045s/iter; left time: 6144.8654s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0848982\n",
      "\tspeed: 0.1061s/iter; left time: 6228.8058s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1067260\n",
      "\tspeed: 0.1049s/iter; left time: 6148.3412s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0981014\n",
      "\tspeed: 0.1032s/iter; left time: 6036.7361s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0832176\n",
      "\tspeed: 0.1103s/iter; left time: 6443.1575s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1142252\n",
      "\tspeed: 0.1051s/iter; left time: 6131.8096s\n",
      "\titers: 4400, epoch: 7 | loss: 0.1292363\n",
      "\tspeed: 0.1079s/iter; left time: 6283.6865s\n",
      "Epoch: 7 cost time: 00h:08m:03.49s\n",
      "Epoch: 7 | Train Loss: 0.1004053 Vali Loss: 0.1252791 Test Loss: 0.1567259\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0986653\n",
      "\tspeed: 1.3887s/iter; left time: 80615.4066s\n",
      "\titers: 200, epoch: 8 | loss: 0.0983105\n",
      "\tspeed: 0.1070s/iter; left time: 6202.1602s\n",
      "\titers: 300, epoch: 8 | loss: 0.1024780\n",
      "\tspeed: 0.1065s/iter; left time: 6162.3177s\n",
      "\titers: 400, epoch: 8 | loss: 0.1091874\n",
      "\tspeed: 0.1058s/iter; left time: 6108.2424s\n",
      "\titers: 500, epoch: 8 | loss: 0.0796972\n",
      "\tspeed: 0.1044s/iter; left time: 6021.4761s\n",
      "\titers: 600, epoch: 8 | loss: 0.1151457\n",
      "\tspeed: 0.1078s/iter; left time: 6202.3343s\n",
      "\titers: 700, epoch: 8 | loss: 0.0923439\n",
      "\tspeed: 0.1044s/iter; left time: 5998.1825s\n",
      "\titers: 800, epoch: 8 | loss: 0.0942501\n",
      "\tspeed: 0.1056s/iter; left time: 6054.1411s\n",
      "\titers: 900, epoch: 8 | loss: 0.0994631\n",
      "\tspeed: 0.1035s/iter; left time: 5925.5695s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0976768\n",
      "\tspeed: 0.1049s/iter; left time: 5994.5175s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0949048\n",
      "\tspeed: 0.1066s/iter; left time: 6083.7003s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0971709\n",
      "\tspeed: 0.1062s/iter; left time: 6049.0040s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0919779\n",
      "\tspeed: 0.1087s/iter; left time: 6177.4734s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0921907\n",
      "\tspeed: 0.1005s/iter; left time: 5705.4225s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1062825\n",
      "\tspeed: 0.1044s/iter; left time: 5916.4223s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0986134\n",
      "\tspeed: 0.1040s/iter; left time: 5881.4220s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0987850\n",
      "\tspeed: 0.1087s/iter; left time: 6137.8484s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0880046\n",
      "\tspeed: 0.1025s/iter; left time: 5777.4243s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0998610\n",
      "\tspeed: 0.1077s/iter; left time: 6060.3033s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1191346\n",
      "\tspeed: 0.1032s/iter; left time: 5796.0598s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0997336\n",
      "\tspeed: 0.1071s/iter; left time: 6003.0262s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0977681\n",
      "\tspeed: 0.1060s/iter; left time: 5929.8658s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0942919\n",
      "\tspeed: 0.1015s/iter; left time: 5668.3825s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1181585\n",
      "\tspeed: 0.1077s/iter; left time: 6002.0095s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0900528\n",
      "\tspeed: 0.1019s/iter; left time: 5669.7670s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1048671\n",
      "\tspeed: 0.1085s/iter; left time: 6028.8179s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0932050\n",
      "\tspeed: 0.1105s/iter; left time: 6127.2632s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1049393\n",
      "\tspeed: 0.1059s/iter; left time: 5859.5793s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0715457\n",
      "\tspeed: 0.1100s/iter; left time: 6076.3097s\n",
      "\titers: 3000, epoch: 8 | loss: 0.1109473\n",
      "\tspeed: 0.1064s/iter; left time: 5870.6251s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0979033\n",
      "\tspeed: 0.1070s/iter; left time: 5892.5974s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1071441\n",
      "\tspeed: 0.1064s/iter; left time: 5847.0757s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0768796\n",
      "\tspeed: 0.1068s/iter; left time: 5857.9075s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0921377\n",
      "\tspeed: 0.1109s/iter; left time: 6069.4385s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0841669\n",
      "\tspeed: 0.1046s/iter; left time: 5718.1633s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0960754\n",
      "\tspeed: 0.1100s/iter; left time: 6002.2830s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0833702\n",
      "\tspeed: 0.1091s/iter; left time: 5942.8118s\n",
      "\titers: 3800, epoch: 8 | loss: 0.1056035\n",
      "\tspeed: 0.1073s/iter; left time: 5829.5628s\n",
      "\titers: 3900, epoch: 8 | loss: 0.1034919\n",
      "\tspeed: 0.1085s/iter; left time: 5884.0160s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0962752\n",
      "\tspeed: 0.1091s/iter; left time: 5909.0832s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0970299\n",
      "\tspeed: 0.1030s/iter; left time: 5567.9175s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0906406\n",
      "\tspeed: 0.1062s/iter; left time: 5728.8146s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0950922\n",
      "\tspeed: 0.1062s/iter; left time: 5717.0580s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1170644\n",
      "\tspeed: 0.1060s/iter; left time: 5696.4968s\n",
      "Epoch: 8 cost time: 00h:07m:55.42s\n",
      "Epoch: 8 | Train Loss: 0.0976592 Vali Loss: 0.1248156 Test Loss: 0.1552406\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.1081889\n",
      "\tspeed: 1.3984s/iter; left time: 74921.6007s\n",
      "\titers: 200, epoch: 9 | loss: 0.0912347\n",
      "\tspeed: 0.1082s/iter; left time: 5787.3827s\n",
      "\titers: 300, epoch: 9 | loss: 0.0898880\n",
      "\tspeed: 0.1094s/iter; left time: 5838.8974s\n",
      "\titers: 400, epoch: 9 | loss: 0.1006834\n",
      "\tspeed: 0.1102s/iter; left time: 5872.0914s\n",
      "\titers: 500, epoch: 9 | loss: 0.1055464\n",
      "\tspeed: 0.1077s/iter; left time: 5727.9411s\n",
      "\titers: 600, epoch: 9 | loss: 0.0820070\n",
      "\tspeed: 0.1102s/iter; left time: 5849.4555s\n",
      "\titers: 700, epoch: 9 | loss: 0.1049108\n",
      "\tspeed: 0.1066s/iter; left time: 5646.1689s\n",
      "\titers: 800, epoch: 9 | loss: 0.1005554\n",
      "\tspeed: 0.1050s/iter; left time: 5552.1492s\n",
      "\titers: 900, epoch: 9 | loss: 0.0876917\n",
      "\tspeed: 0.1079s/iter; left time: 5696.0949s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0906235\n",
      "\tspeed: 0.1055s/iter; left time: 5559.9999s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0937413\n",
      "\tspeed: 0.1011s/iter; left time: 5317.8502s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1121209\n",
      "\tspeed: 0.1073s/iter; left time: 5628.7097s\n",
      "\titers: 1300, epoch: 9 | loss: 0.1008148\n",
      "\tspeed: 0.1059s/iter; left time: 5546.6378s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0889356\n",
      "\tspeed: 0.1017s/iter; left time: 5317.7008s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0898405\n",
      "\tspeed: 0.1116s/iter; left time: 5822.3724s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0989260\n",
      "\tspeed: 0.1074s/iter; left time: 5591.4003s\n",
      "\titers: 1700, epoch: 9 | loss: 0.1049883\n",
      "\tspeed: 0.1090s/iter; left time: 5663.6288s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0921862\n",
      "\tspeed: 0.1057s/iter; left time: 5481.6351s\n",
      "\titers: 1900, epoch: 9 | loss: 0.1090681\n",
      "\tspeed: 0.1083s/iter; left time: 5608.5675s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0879408\n",
      "\tspeed: 0.1095s/iter; left time: 5658.3337s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0999407\n",
      "\tspeed: 0.1078s/iter; left time: 5562.2738s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0866363\n",
      "\tspeed: 0.1113s/iter; left time: 5727.4817s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1170187\n",
      "\tspeed: 0.1026s/iter; left time: 5271.4964s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0945719\n",
      "\tspeed: 0.1057s/iter; left time: 5419.3406s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0903585\n",
      "\tspeed: 0.1045s/iter; left time: 5349.5437s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0857023\n",
      "\tspeed: 0.1057s/iter; left time: 5400.0068s\n",
      "\titers: 2700, epoch: 9 | loss: 0.1100801\n",
      "\tspeed: 0.1023s/iter; left time: 5212.4255s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0914453\n",
      "\tspeed: 0.1075s/iter; left time: 5467.4288s\n",
      "\titers: 2900, epoch: 9 | loss: 0.1086308\n",
      "\tspeed: 0.1097s/iter; left time: 5567.7597s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1012945\n",
      "\tspeed: 0.1082s/iter; left time: 5481.6941s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0891736\n",
      "\tspeed: 0.1059s/iter; left time: 5353.9411s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0943430\n",
      "\tspeed: 0.1087s/iter; left time: 5485.3078s\n",
      "\titers: 3300, epoch: 9 | loss: 0.1029650\n",
      "\tspeed: 0.1107s/iter; left time: 5574.2274s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0784484\n",
      "\tspeed: 0.1102s/iter; left time: 5542.7671s\n",
      "\titers: 3500, epoch: 9 | loss: 0.1020526\n",
      "\tspeed: 0.1077s/iter; left time: 5406.4676s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0987637\n",
      "\tspeed: 0.1053s/iter; left time: 5273.0119s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0952342\n",
      "\tspeed: 0.1086s/iter; left time: 5426.0940s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1057804\n",
      "\tspeed: 0.1071s/iter; left time: 5340.4882s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0955375\n",
      "\tspeed: 0.1068s/iter; left time: 5316.8251s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0782716\n",
      "\tspeed: 0.1136s/iter; left time: 5640.9163s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0951779\n",
      "\tspeed: 0.1067s/iter; left time: 5290.7223s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0944427\n",
      "\tspeed: 0.0983s/iter; left time: 4865.4189s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0884099\n",
      "\tspeed: 0.1075s/iter; left time: 5310.1218s\n",
      "\titers: 4400, epoch: 9 | loss: 0.1025119\n",
      "\tspeed: 0.1111s/iter; left time: 5473.7040s\n",
      "Epoch: 9 cost time: 00h:08m:00.48s\n",
      "Epoch: 9 | Train Loss: 0.0950745 Vali Loss: 0.1270755 Test Loss: 0.1571813\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04692179709672928, rmse:0.21661439538002014, mae:0.14865338802337646, rse:0.7507471442222595\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 168: 01h:32m:49.29s\n",
      "\n",
      "Intermediate time for GB: 05h:13m:41.59s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 13:55:28,723] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 13:55:29,733] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 13:55:29,733] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 13:55:29,733] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 13:55:29,819] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 13:55:29,819] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 13:55:30,452] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 13:55:30,453] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 13:55:30,453] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 13:55:30,454] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 13:55:30,454] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 13:55:30,454] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 13:55:30,811] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 13:55:30,812] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:30,813] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:30,939] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 13:55:30,940] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:30,940] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:30,940] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 13:55:31,057] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 13:55:31,058] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:31,058] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd48878e3d0>\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 13:55:31,063] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1712448\n",
      "\tspeed: 0.1613s/iter; left time: 8684.6738s\n",
      "\titers: 200, epoch: 1 | loss: 0.2023432\n",
      "\tspeed: 0.1201s/iter; left time: 6453.8491s\n",
      "\titers: 300, epoch: 1 | loss: 0.1408175\n",
      "\tspeed: 0.1178s/iter; left time: 6317.8376s\n",
      "\titers: 400, epoch: 1 | loss: 0.1179628\n",
      "\tspeed: 0.1235s/iter; left time: 6610.1309s\n",
      "\titers: 500, epoch: 1 | loss: 0.0901266\n",
      "\tspeed: 0.1234s/iter; left time: 6593.8340s\n",
      "\titers: 600, epoch: 1 | loss: 0.0898690\n",
      "\tspeed: 0.1213s/iter; left time: 6472.8916s\n",
      "\titers: 700, epoch: 1 | loss: 0.0787271\n",
      "\tspeed: 0.1163s/iter; left time: 6190.2511s\n",
      "\titers: 800, epoch: 1 | loss: 0.1065296\n",
      "\tspeed: 0.1186s/iter; left time: 6304.2562s\n",
      "\titers: 900, epoch: 1 | loss: 0.0820746\n",
      "\tspeed: 0.1241s/iter; left time: 6579.9415s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0925007\n",
      "\tspeed: 0.1162s/iter; left time: 6153.6498s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0864945\n",
      "\tspeed: 0.1213s/iter; left time: 6410.4135s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0853928\n",
      "\tspeed: 0.1245s/iter; left time: 6563.8128s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0853296\n",
      "\tspeed: 0.1227s/iter; left time: 6456.6093s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0815412\n",
      "\tspeed: 0.1212s/iter; left time: 6365.9414s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0878881\n",
      "\tspeed: 0.1242s/iter; left time: 6513.1136s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0781828\n",
      "\tspeed: 0.1190s/iter; left time: 6227.6744s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0933578\n",
      "\tspeed: 0.1215s/iter; left time: 6345.8813s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0840652\n",
      "\tspeed: 0.1187s/iter; left time: 6191.7086s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0744049\n",
      "\tspeed: 0.1190s/iter; left time: 6195.1130s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0831344\n",
      "\tspeed: 0.1154s/iter; left time: 5995.7890s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0764061\n",
      "\tspeed: 0.1185s/iter; left time: 6145.6551s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0826099\n",
      "\tspeed: 0.1246s/iter; left time: 6444.6799s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0772082\n",
      "\tspeed: 0.1175s/iter; left time: 6069.2087s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0781461\n",
      "\tspeed: 0.1212s/iter; left time: 6245.1723s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0887459\n",
      "\tspeed: 0.1202s/iter; left time: 6184.5045s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0926115\n",
      "\tspeed: 0.1213s/iter; left time: 6225.8487s\n",
      "Epoch: 1 cost time: 00h:05m:25.87s\n",
      "Epoch: 1 | Train Loss: 0.1003548 Vali Loss: 0.0686999 Test Loss: 0.0770876\n",
      "Validation loss decreased (inf --> 0.068700).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0797757\n",
      "\tspeed: 1.1321s/iter; left time: 57899.3057s\n",
      "\titers: 200, epoch: 2 | loss: 0.0827035\n",
      "\tspeed: 0.1055s/iter; left time: 5382.8729s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977232\n",
      "\tspeed: 0.1085s/iter; left time: 5525.9815s\n",
      "\titers: 400, epoch: 2 | loss: 0.0749713\n",
      "\tspeed: 0.1031s/iter; left time: 5244.2386s\n",
      "\titers: 500, epoch: 2 | loss: 0.0862913\n",
      "\tspeed: 0.1044s/iter; left time: 5298.2570s\n",
      "\titers: 600, epoch: 2 | loss: 0.0929019\n",
      "\tspeed: 0.1039s/iter; left time: 5260.8256s\n",
      "\titers: 700, epoch: 2 | loss: 0.0713457\n",
      "\tspeed: 0.1070s/iter; left time: 5410.1494s\n",
      "\titers: 800, epoch: 2 | loss: 0.0867402\n",
      "\tspeed: 0.1100s/iter; left time: 5549.3582s\n",
      "\titers: 900, epoch: 2 | loss: 0.0903707\n",
      "\tspeed: 0.1077s/iter; left time: 5421.1584s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0810028\n",
      "\tspeed: 0.1094s/iter; left time: 5498.9982s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0841704\n",
      "\tspeed: 0.1074s/iter; left time: 5383.8395s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0849176\n",
      "\tspeed: 0.1061s/iter; left time: 5308.9948s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0906385\n",
      "\tspeed: 0.1059s/iter; left time: 5289.1828s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0782810\n",
      "\tspeed: 0.1071s/iter; left time: 5340.6999s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0824517\n",
      "\tspeed: 0.1119s/iter; left time: 5564.5417s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0916397\n",
      "\tspeed: 0.1071s/iter; left time: 5316.0470s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0733048\n",
      "\tspeed: 0.1027s/iter; left time: 5087.5426s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0718356\n",
      "\tspeed: 0.1049s/iter; left time: 5185.6725s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0834526\n",
      "\tspeed: 0.1045s/iter; left time: 5157.8781s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0808225\n",
      "\tspeed: 0.1083s/iter; left time: 5333.7927s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0826184\n",
      "\tspeed: 0.1001s/iter; left time: 4919.7559s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0727913\n",
      "\tspeed: 0.1052s/iter; left time: 5157.3315s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0784774\n",
      "\tspeed: 0.1051s/iter; left time: 5142.4298s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0701629\n",
      "\tspeed: 0.1095s/iter; left time: 5348.5788s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0810242\n",
      "\tspeed: 0.1046s/iter; left time: 5099.7382s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0680333\n",
      "\tspeed: 0.1039s/iter; left time: 5052.3724s\n",
      "Epoch: 2 cost time: 00h:04m:47.49s\n",
      "Epoch: 2 | Train Loss: 0.0786239 Vali Loss: 0.0661815 Test Loss: 0.0743510\n",
      "Validation loss decreased (0.068700 --> 0.066182).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0761278\n",
      "\tspeed: 0.9867s/iter; left time: 47803.7469s\n",
      "\titers: 200, epoch: 3 | loss: 0.0774389\n",
      "\tspeed: 0.1055s/iter; left time: 5100.0690s\n",
      "\titers: 300, epoch: 3 | loss: 0.0796090\n",
      "\tspeed: 0.1107s/iter; left time: 5339.9299s\n",
      "\titers: 400, epoch: 3 | loss: 0.0677147\n",
      "\tspeed: 0.1045s/iter; left time: 5033.4590s\n",
      "\titers: 500, epoch: 3 | loss: 0.0734856\n",
      "\tspeed: 0.1086s/iter; left time: 5219.7809s\n",
      "\titers: 600, epoch: 3 | loss: 0.0857239\n",
      "\tspeed: 0.1067s/iter; left time: 5115.9864s\n",
      "\titers: 700, epoch: 3 | loss: 0.0681393\n",
      "\tspeed: 0.1084s/iter; left time: 5184.6248s\n",
      "\titers: 800, epoch: 3 | loss: 0.0724246\n",
      "\tspeed: 0.1068s/iter; left time: 5097.0364s\n",
      "\titers: 900, epoch: 3 | loss: 0.0910780\n",
      "\tspeed: 0.1055s/iter; left time: 5026.2883s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0728555\n",
      "\tspeed: 0.1088s/iter; left time: 5175.3988s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0870226\n",
      "\tspeed: 0.1051s/iter; left time: 4985.4615s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0779801\n",
      "\tspeed: 0.1072s/iter; left time: 5077.1261s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0614597\n",
      "\tspeed: 0.1076s/iter; left time: 5083.3829s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0864894\n",
      "\tspeed: 0.1089s/iter; left time: 5134.2734s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0696164\n",
      "\tspeed: 0.1022s/iter; left time: 4809.6830s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0833256\n",
      "\tspeed: 0.1075s/iter; left time: 5047.4433s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0790411\n",
      "\tspeed: 0.1090s/iter; left time: 5106.4661s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0904982\n",
      "\tspeed: 0.1093s/iter; left time: 5111.1672s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0717343\n",
      "\tspeed: 0.1096s/iter; left time: 5113.7256s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0635259\n",
      "\tspeed: 0.1064s/iter; left time: 4951.5607s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0628298\n",
      "\tspeed: 0.1118s/iter; left time: 5192.2139s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0658889\n",
      "\tspeed: 0.1083s/iter; left time: 5019.2886s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0762297\n",
      "\tspeed: 0.1085s/iter; left time: 5019.5641s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0619692\n",
      "\tspeed: 0.1069s/iter; left time: 4933.1645s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0707970\n",
      "\tspeed: 0.1029s/iter; left time: 4736.3141s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0759995\n",
      "\tspeed: 0.1077s/iter; left time: 4947.2761s\n",
      "Epoch: 3 cost time: 00h:04m:50.10s\n",
      "Epoch: 3 | Train Loss: 0.0745886 Vali Loss: 0.0614387 Test Loss: 0.0695850\n",
      "Validation loss decreased (0.066182 --> 0.061439).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0740035\n",
      "\tspeed: 0.9679s/iter; left time: 44283.2179s\n",
      "\titers: 200, epoch: 4 | loss: 0.0772850\n",
      "\tspeed: 0.1074s/iter; left time: 4900.6231s\n",
      "\titers: 300, epoch: 4 | loss: 0.0706134\n",
      "\tspeed: 0.1053s/iter; left time: 4795.4921s\n",
      "\titers: 400, epoch: 4 | loss: 0.0690192\n",
      "\tspeed: 0.1057s/iter; left time: 4804.5128s\n",
      "\titers: 500, epoch: 4 | loss: 0.0804873\n",
      "\tspeed: 0.1013s/iter; left time: 4594.0107s\n",
      "\titers: 600, epoch: 4 | loss: 0.0721374\n",
      "\tspeed: 0.1108s/iter; left time: 5015.3915s\n",
      "\titers: 700, epoch: 4 | loss: 0.0607685\n",
      "\tspeed: 0.1113s/iter; left time: 5023.6644s\n",
      "\titers: 800, epoch: 4 | loss: 0.0716907\n",
      "\tspeed: 0.1018s/iter; left time: 4588.0548s\n",
      "\titers: 900, epoch: 4 | loss: 0.0583386\n",
      "\tspeed: 0.1101s/iter; left time: 4949.3779s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0691884\n",
      "\tspeed: 0.1091s/iter; left time: 4894.8875s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0729936\n",
      "\tspeed: 0.1058s/iter; left time: 4735.7368s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0830592\n",
      "\tspeed: 0.1080s/iter; left time: 4820.6197s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0928226\n",
      "\tspeed: 0.1067s/iter; left time: 4751.9578s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0729482\n",
      "\tspeed: 0.1092s/iter; left time: 4853.6016s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0764900\n",
      "\tspeed: 0.1073s/iter; left time: 4759.7122s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0752271\n",
      "\tspeed: 0.1103s/iter; left time: 4882.1301s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0638424\n",
      "\tspeed: 0.1099s/iter; left time: 4851.0980s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0620330\n",
      "\tspeed: 0.1105s/iter; left time: 4866.3266s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0701117\n",
      "\tspeed: 0.1043s/iter; left time: 4581.8071s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0681855\n",
      "\tspeed: 0.1073s/iter; left time: 4706.3663s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0762056\n",
      "\tspeed: 0.1052s/iter; left time: 4600.4788s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0690991\n",
      "\tspeed: 0.1053s/iter; left time: 4596.0362s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0679242\n",
      "\tspeed: 0.1066s/iter; left time: 4643.4121s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0739057\n",
      "\tspeed: 0.1058s/iter; left time: 4597.1895s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0679740\n",
      "\tspeed: 0.1057s/iter; left time: 4584.1439s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0779216\n",
      "\tspeed: 0.1067s/iter; left time: 4616.2239s\n",
      "Epoch: 4 cost time: 00h:04m:49.39s\n",
      "Epoch: 4 | Train Loss: 0.0718865 Vali Loss: 0.0601031 Test Loss: 0.0689823\n",
      "Validation loss decreased (0.061439 --> 0.060103).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0688475\n",
      "\tspeed: 0.9751s/iter; left time: 41981.7101s\n",
      "\titers: 200, epoch: 5 | loss: 0.0738413\n",
      "\tspeed: 0.1063s/iter; left time: 4565.6181s\n",
      "\titers: 300, epoch: 5 | loss: 0.0718522\n",
      "\tspeed: 0.1099s/iter; left time: 4707.6271s\n",
      "\titers: 400, epoch: 5 | loss: 0.0661217\n",
      "\tspeed: 0.1075s/iter; left time: 4598.0354s\n",
      "\titers: 500, epoch: 5 | loss: 0.0626277\n",
      "\tspeed: 0.1086s/iter; left time: 4632.2707s\n",
      "\titers: 600, epoch: 5 | loss: 0.0671054\n",
      "\tspeed: 0.1065s/iter; left time: 4532.0856s\n",
      "\titers: 700, epoch: 5 | loss: 0.0719781\n",
      "\tspeed: 0.1053s/iter; left time: 4472.0714s\n",
      "\titers: 800, epoch: 5 | loss: 0.0713939\n",
      "\tspeed: 0.1057s/iter; left time: 4474.6205s\n",
      "\titers: 900, epoch: 5 | loss: 0.0793560\n",
      "\tspeed: 0.1075s/iter; left time: 4543.0394s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0704821\n",
      "\tspeed: 0.1057s/iter; left time: 4457.0025s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0659771\n",
      "\tspeed: 0.1056s/iter; left time: 4442.2638s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0808677\n",
      "\tspeed: 0.1117s/iter; left time: 4684.9370s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0577165\n",
      "\tspeed: 0.1059s/iter; left time: 4433.9060s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0701355\n",
      "\tspeed: 0.1056s/iter; left time: 4407.8319s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0783938\n",
      "\tspeed: 0.1061s/iter; left time: 4420.4042s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0703886\n",
      "\tspeed: 0.1086s/iter; left time: 4513.3542s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0750756\n",
      "\tspeed: 0.1106s/iter; left time: 4585.5776s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0745274\n",
      "\tspeed: 0.1045s/iter; left time: 4321.2924s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0632909\n",
      "\tspeed: 0.1065s/iter; left time: 4392.0168s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0572714\n",
      "\tspeed: 0.1027s/iter; left time: 4225.5792s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0697682\n",
      "\tspeed: 0.1047s/iter; left time: 4296.4321s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0775003\n",
      "\tspeed: 0.1066s/iter; left time: 4364.6123s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0681202\n",
      "\tspeed: 0.1059s/iter; left time: 4325.2325s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0675049\n",
      "\tspeed: 0.1107s/iter; left time: 4509.7213s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0756652\n",
      "\tspeed: 0.1064s/iter; left time: 4325.3102s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0688920\n",
      "\tspeed: 0.1081s/iter; left time: 4385.1994s\n",
      "Epoch: 5 cost time: 00h:04m:48.80s\n",
      "Epoch: 5 | Train Loss: 0.0703978 Vali Loss: 0.0614755 Test Loss: 0.0700821\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0642571\n",
      "\tspeed: 0.9516s/iter; left time: 38401.7352s\n",
      "\titers: 200, epoch: 6 | loss: 0.0717407\n",
      "\tspeed: 0.1097s/iter; left time: 4415.2552s\n",
      "\titers: 300, epoch: 6 | loss: 0.0590067\n",
      "\tspeed: 0.1089s/iter; left time: 4373.4624s\n",
      "\titers: 400, epoch: 6 | loss: 0.0612367\n",
      "\tspeed: 0.1097s/iter; left time: 4394.4029s\n",
      "\titers: 500, epoch: 6 | loss: 0.0722813\n",
      "\tspeed: 0.1082s/iter; left time: 4321.3869s\n",
      "\titers: 600, epoch: 6 | loss: 0.0766656\n",
      "\tspeed: 0.1096s/iter; left time: 4366.7746s\n",
      "\titers: 700, epoch: 6 | loss: 0.0786778\n",
      "\tspeed: 0.1089s/iter; left time: 4328.6553s\n",
      "\titers: 800, epoch: 6 | loss: 0.0702733\n",
      "\tspeed: 0.1044s/iter; left time: 4141.1896s\n",
      "\titers: 900, epoch: 6 | loss: 0.0562902\n",
      "\tspeed: 0.1085s/iter; left time: 4291.9032s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0571464\n",
      "\tspeed: 0.1066s/iter; left time: 4205.6505s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0545283\n",
      "\tspeed: 0.1085s/iter; left time: 4269.9388s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0686143\n",
      "\tspeed: 0.1105s/iter; left time: 4337.5885s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0655344\n",
      "\tspeed: 0.1104s/iter; left time: 4324.0077s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0677483\n",
      "\tspeed: 0.1075s/iter; left time: 4198.1122s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0722037\n",
      "\tspeed: 0.1065s/iter; left time: 4146.9597s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0698562\n",
      "\tspeed: 0.1077s/iter; left time: 4183.5869s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0730506\n",
      "\tspeed: 0.1072s/iter; left time: 4154.0216s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0689412\n",
      "\tspeed: 0.1093s/iter; left time: 4225.8563s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0618468\n",
      "\tspeed: 0.1071s/iter; left time: 4131.1059s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0735297\n",
      "\tspeed: 0.1109s/iter; left time: 4265.9072s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0774064\n",
      "\tspeed: 0.1104s/iter; left time: 4235.9062s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0620106\n",
      "\tspeed: 0.1078s/iter; left time: 4123.6705s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0610147\n",
      "\tspeed: 0.1113s/iter; left time: 4245.3779s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0655296\n",
      "\tspeed: 0.1088s/iter; left time: 4139.2595s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0643461\n",
      "\tspeed: 0.1184s/iter; left time: 4495.1187s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0650501\n",
      "\tspeed: 0.1208s/iter; left time: 4572.8638s\n",
      "Epoch: 6 cost time: 00h:04m:56.77s\n",
      "Epoch: 6 | Train Loss: 0.0693189 Vali Loss: 0.0591683 Test Loss: 0.0672605\n",
      "Validation loss decreased (0.060103 --> 0.059168).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0697974\n",
      "\tspeed: 0.9952s/iter; left time: 37476.9033s\n",
      "\titers: 200, epoch: 7 | loss: 0.0755775\n",
      "\tspeed: 0.1059s/iter; left time: 3977.0852s\n",
      "\titers: 300, epoch: 7 | loss: 0.0618644\n",
      "\tspeed: 0.1065s/iter; left time: 3990.2135s\n",
      "\titers: 400, epoch: 7 | loss: 0.0668553\n",
      "\tspeed: 0.1030s/iter; left time: 3846.8323s\n",
      "\titers: 500, epoch: 7 | loss: 0.0719296\n",
      "\tspeed: 0.1103s/iter; left time: 4109.9221s\n",
      "\titers: 600, epoch: 7 | loss: 0.0714099\n",
      "\tspeed: 0.1059s/iter; left time: 3934.2465s\n",
      "\titers: 700, epoch: 7 | loss: 0.0720902\n",
      "\tspeed: 0.1040s/iter; left time: 3855.2449s\n",
      "\titers: 800, epoch: 7 | loss: 0.0795342\n",
      "\tspeed: 0.1064s/iter; left time: 3932.9802s\n",
      "\titers: 900, epoch: 7 | loss: 0.0655722\n",
      "\tspeed: 0.1070s/iter; left time: 3943.9740s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0711685\n",
      "\tspeed: 0.1080s/iter; left time: 3968.4331s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0755553\n",
      "\tspeed: 0.1060s/iter; left time: 3887.0493s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0766494\n",
      "\tspeed: 0.1044s/iter; left time: 3815.7570s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0651874\n",
      "\tspeed: 0.1074s/iter; left time: 3917.2365s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0650540\n",
      "\tspeed: 0.1055s/iter; left time: 3836.7274s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0855093\n",
      "\tspeed: 0.1028s/iter; left time: 3725.6440s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0753963\n",
      "\tspeed: 0.1103s/iter; left time: 3988.5567s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0613949\n",
      "\tspeed: 0.1048s/iter; left time: 3779.8557s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0692656\n",
      "\tspeed: 0.1026s/iter; left time: 3689.8381s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0711877\n",
      "\tspeed: 0.1039s/iter; left time: 3723.9673s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0700283\n",
      "\tspeed: 0.1074s/iter; left time: 3842.0613s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0562870\n",
      "\tspeed: 0.1044s/iter; left time: 3721.6851s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0848084\n",
      "\tspeed: 0.1066s/iter; left time: 3788.9110s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0658100\n",
      "\tspeed: 0.1067s/iter; left time: 3784.0007s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0565042\n",
      "\tspeed: 0.1054s/iter; left time: 3727.2828s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0774072\n",
      "\tspeed: 0.1059s/iter; left time: 3734.5611s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0680225\n",
      "\tspeed: 0.1102s/iter; left time: 3874.4911s\n",
      "Epoch: 7 cost time: 00h:04m:46.58s\n",
      "Epoch: 7 | Train Loss: 0.0684043 Vali Loss: 0.0598554 Test Loss: 0.0685234\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0863287\n",
      "\tspeed: 0.9523s/iter; left time: 33295.2367s\n",
      "\titers: 200, epoch: 8 | loss: 0.0786934\n",
      "\tspeed: 0.1030s/iter; left time: 3590.2538s\n",
      "\titers: 300, epoch: 8 | loss: 0.0739644\n",
      "\tspeed: 0.1023s/iter; left time: 3556.8383s\n",
      "\titers: 400, epoch: 8 | loss: 0.0589355\n",
      "\tspeed: 0.1054s/iter; left time: 3652.8755s\n",
      "\titers: 500, epoch: 8 | loss: 0.0686779\n",
      "\tspeed: 0.1068s/iter; left time: 3692.0885s\n",
      "\titers: 600, epoch: 8 | loss: 0.0584960\n",
      "\tspeed: 0.1051s/iter; left time: 3621.0724s\n",
      "\titers: 700, epoch: 8 | loss: 0.0871281\n",
      "\tspeed: 0.1059s/iter; left time: 3637.8838s\n",
      "\titers: 800, epoch: 8 | loss: 0.0599374\n",
      "\tspeed: 0.1125s/iter; left time: 3855.8626s\n",
      "\titers: 900, epoch: 8 | loss: 0.0789052\n",
      "\tspeed: 0.1069s/iter; left time: 3652.5201s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0873202\n",
      "\tspeed: 0.1076s/iter; left time: 3665.3601s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0671721\n",
      "\tspeed: 0.1031s/iter; left time: 3501.1413s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0676801\n",
      "\tspeed: 0.1005s/iter; left time: 3403.1546s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0818329\n",
      "\tspeed: 0.1073s/iter; left time: 3622.8143s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0655170\n",
      "\tspeed: 0.1114s/iter; left time: 3748.8925s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0690930\n",
      "\tspeed: 0.1100s/iter; left time: 3691.2117s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0599412\n",
      "\tspeed: 0.0992s/iter; left time: 3319.3962s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0730805\n",
      "\tspeed: 0.1004s/iter; left time: 3350.1140s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0648698\n",
      "\tspeed: 0.1098s/iter; left time: 3650.7728s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0638596\n",
      "\tspeed: 0.1068s/iter; left time: 3541.0975s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0609560\n",
      "\tspeed: 0.1096s/iter; left time: 3623.0832s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0601996\n",
      "\tspeed: 0.1109s/iter; left time: 3654.6756s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0709287\n",
      "\tspeed: 0.1060s/iter; left time: 3484.5731s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0781172\n",
      "\tspeed: 0.1091s/iter; left time: 3575.7529s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0663429\n",
      "\tspeed: 0.1096s/iter; left time: 3580.6048s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0782741\n",
      "\tspeed: 0.1062s/iter; left time: 3458.0091s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0815893\n",
      "\tspeed: 0.1087s/iter; left time: 3528.7187s\n",
      "Epoch: 8 cost time: 00h:04m:47.72s\n",
      "Epoch: 8 | Train Loss: 0.0677572 Vali Loss: 0.0597144 Test Loss: 0.0685009\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0733084\n",
      "\tspeed: 0.9591s/iter; left time: 30946.8273s\n",
      "\titers: 200, epoch: 9 | loss: 0.0649616\n",
      "\tspeed: 0.1070s/iter; left time: 3441.7858s\n",
      "\titers: 300, epoch: 9 | loss: 0.0522117\n",
      "\tspeed: 0.1106s/iter; left time: 3547.7316s\n",
      "\titers: 400, epoch: 9 | loss: 0.0659156\n",
      "\tspeed: 0.1058s/iter; left time: 3381.2549s\n",
      "\titers: 500, epoch: 9 | loss: 0.0653602\n",
      "\tspeed: 0.1022s/iter; left time: 3255.7305s\n",
      "\titers: 600, epoch: 9 | loss: 0.0684405\n",
      "\tspeed: 0.1054s/iter; left time: 3348.4559s\n",
      "\titers: 700, epoch: 9 | loss: 0.0707613\n",
      "\tspeed: 0.1071s/iter; left time: 3391.6700s\n",
      "\titers: 800, epoch: 9 | loss: 0.0594048\n",
      "\tspeed: 0.1084s/iter; left time: 3421.7803s\n",
      "\titers: 900, epoch: 9 | loss: 0.0666278\n",
      "\tspeed: 0.1108s/iter; left time: 3485.6078s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0586480\n",
      "\tspeed: 0.1058s/iter; left time: 3317.3673s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0657964\n",
      "\tspeed: 0.1075s/iter; left time: 3360.6361s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0660123\n",
      "\tspeed: 0.1072s/iter; left time: 3340.2201s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0626299\n",
      "\tspeed: 0.1068s/iter; left time: 3316.2178s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0734063\n",
      "\tspeed: 0.1075s/iter; left time: 3327.8793s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0596756\n",
      "\tspeed: 0.1052s/iter; left time: 3246.9703s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0683507\n",
      "\tspeed: 0.1067s/iter; left time: 3282.1504s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0777180\n",
      "\tspeed: 0.1058s/iter; left time: 3245.2439s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0691540\n",
      "\tspeed: 0.1076s/iter; left time: 3288.3782s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0617039\n",
      "\tspeed: 0.1008s/iter; left time: 3072.3402s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0588695\n",
      "\tspeed: 0.1043s/iter; left time: 3167.3205s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0621957\n",
      "\tspeed: 0.1058s/iter; left time: 3202.2180s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0547058\n",
      "\tspeed: 0.1109s/iter; left time: 3345.2274s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0607693\n",
      "\tspeed: 0.1073s/iter; left time: 3226.4864s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0594633\n",
      "\tspeed: 0.1104s/iter; left time: 3308.0158s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0635684\n",
      "\tspeed: 0.1045s/iter; left time: 3121.6698s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0643801\n",
      "\tspeed: 0.1124s/iter; left time: 3344.5355s\n",
      "Epoch: 9 cost time: 00h:04m:49.10s\n",
      "Epoch: 9 | Train Loss: 0.0671886 Vali Loss: 0.0583229 Test Loss: 0.0672066\n",
      "Validation loss decreased (0.059168 --> 0.058323).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0552282\n",
      "\tspeed: 0.9887s/iter; left time: 29233.8330s\n",
      "\titers: 200, epoch: 10 | loss: 0.0693061\n",
      "\tspeed: 0.1088s/iter; left time: 3205.9469s\n",
      "\titers: 300, epoch: 10 | loss: 0.0732012\n",
      "\tspeed: 0.1009s/iter; left time: 2963.2220s\n",
      "\titers: 400, epoch: 10 | loss: 0.0685882\n",
      "\tspeed: 0.1064s/iter; left time: 3115.1014s\n",
      "\titers: 500, epoch: 10 | loss: 0.0663253\n",
      "\tspeed: 0.1075s/iter; left time: 3136.1761s\n",
      "\titers: 600, epoch: 10 | loss: 0.0720553\n",
      "\tspeed: 0.1085s/iter; left time: 3153.3737s\n",
      "\titers: 700, epoch: 10 | loss: 0.0778783\n",
      "\tspeed: 0.1088s/iter; left time: 3151.5514s\n",
      "\titers: 800, epoch: 10 | loss: 0.0587212\n",
      "\tspeed: 0.1074s/iter; left time: 3100.8688s\n",
      "\titers: 900, epoch: 10 | loss: 0.0744800\n",
      "\tspeed: 0.1069s/iter; left time: 3074.3845s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0681581\n",
      "\tspeed: 0.1100s/iter; left time: 3153.2325s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0630505\n",
      "\tspeed: 0.1045s/iter; left time: 2985.0956s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0582443\n",
      "\tspeed: 0.1119s/iter; left time: 3184.8198s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0649366\n",
      "\tspeed: 0.1089s/iter; left time: 3089.5797s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0721512\n",
      "\tspeed: 0.1083s/iter; left time: 3060.3826s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0701118\n",
      "\tspeed: 0.1113s/iter; left time: 3135.2027s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0635339\n",
      "\tspeed: 0.1082s/iter; left time: 3035.7514s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0698001\n",
      "\tspeed: 0.1062s/iter; left time: 2969.6689s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0668549\n",
      "\tspeed: 0.1051s/iter; left time: 2928.5506s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0737289\n",
      "\tspeed: 0.1063s/iter; left time: 2952.5134s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0622393\n",
      "\tspeed: 0.1099s/iter; left time: 3039.8494s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0650669\n",
      "\tspeed: 0.1091s/iter; left time: 3006.4543s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0562495\n",
      "\tspeed: 0.1095s/iter; left time: 3007.6895s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0678977\n",
      "\tspeed: 0.1090s/iter; left time: 2982.1215s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0679341\n",
      "\tspeed: 0.1071s/iter; left time: 2920.9894s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0645824\n",
      "\tspeed: 0.1080s/iter; left time: 2935.3384s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0802735\n",
      "\tspeed: 0.1091s/iter; left time: 2953.8478s\n",
      "Epoch: 10 cost time: 00h:04m:51.61s\n",
      "Epoch: 10 | Train Loss: 0.0665142 Vali Loss: 0.0596833 Test Loss: 0.0681801\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0599668\n",
      "\tspeed: 0.9573s/iter; left time: 25722.9568s\n",
      "\titers: 200, epoch: 11 | loss: 0.0571347\n",
      "\tspeed: 0.1065s/iter; left time: 2850.5491s\n",
      "\titers: 300, epoch: 11 | loss: 0.0555977\n",
      "\tspeed: 0.1146s/iter; left time: 3056.2351s\n",
      "\titers: 400, epoch: 11 | loss: 0.0804140\n",
      "\tspeed: 0.1061s/iter; left time: 2820.0641s\n",
      "\titers: 500, epoch: 11 | loss: 0.0598191\n",
      "\tspeed: 0.1028s/iter; left time: 2720.1557s\n",
      "\titers: 600, epoch: 11 | loss: 0.0680211\n",
      "\tspeed: 0.1070s/iter; left time: 2821.8647s\n",
      "\titers: 700, epoch: 11 | loss: 0.0765506\n",
      "\tspeed: 0.1089s/iter; left time: 2859.6527s\n",
      "\titers: 800, epoch: 11 | loss: 0.0773061\n",
      "\tspeed: 0.1080s/iter; left time: 2826.4155s\n",
      "\titers: 900, epoch: 11 | loss: 0.0626874\n",
      "\tspeed: 0.1072s/iter; left time: 2796.0956s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0522671\n",
      "\tspeed: 0.1063s/iter; left time: 2760.4022s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0617847\n",
      "\tspeed: 0.1093s/iter; left time: 2827.6076s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0724408\n",
      "\tspeed: 0.1059s/iter; left time: 2730.3436s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0688577\n",
      "\tspeed: 0.1084s/iter; left time: 2783.6595s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0556950\n",
      "\tspeed: 0.1044s/iter; left time: 2669.4810s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0723047\n",
      "\tspeed: 0.1098s/iter; left time: 2797.0139s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0631053\n",
      "\tspeed: 0.1109s/iter; left time: 2812.8506s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0670555\n",
      "\tspeed: 0.1067s/iter; left time: 2696.8812s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0663534\n",
      "\tspeed: 0.1048s/iter; left time: 2637.7162s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0744134\n",
      "\tspeed: 0.1097s/iter; left time: 2751.1074s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0686474\n",
      "\tspeed: 0.1085s/iter; left time: 2708.1896s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0585510\n",
      "\tspeed: 0.1008s/iter; left time: 2508.0445s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0590663\n",
      "\tspeed: 0.1062s/iter; left time: 2629.5855s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0612476\n",
      "\tspeed: 0.1061s/iter; left time: 2618.7157s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0564206\n",
      "\tspeed: 0.1066s/iter; left time: 2620.3427s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0649748\n",
      "\tspeed: 0.1106s/iter; left time: 2705.8915s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0657220\n",
      "\tspeed: 0.1079s/iter; left time: 2630.7462s\n",
      "Epoch: 11 cost time: 00h:04m:50.00s\n",
      "Epoch: 11 | Train Loss: 0.0660851 Vali Loss: 0.0592204 Test Loss: 0.0686712\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0633860\n",
      "\tspeed: 0.9599s/iter; left time: 23204.3836s\n",
      "\titers: 200, epoch: 12 | loss: 0.0610290\n",
      "\tspeed: 0.1125s/iter; left time: 2707.5343s\n",
      "\titers: 300, epoch: 12 | loss: 0.0708786\n",
      "\tspeed: 0.1074s/iter; left time: 2574.5213s\n",
      "\titers: 400, epoch: 12 | loss: 0.0609895\n",
      "\tspeed: 0.1086s/iter; left time: 2593.8355s\n",
      "\titers: 500, epoch: 12 | loss: 0.0662539\n",
      "\tspeed: 0.1070s/iter; left time: 2544.1196s\n",
      "\titers: 600, epoch: 12 | loss: 0.0623685\n",
      "\tspeed: 0.1097s/iter; left time: 2596.2447s\n",
      "\titers: 700, epoch: 12 | loss: 0.0687687\n",
      "\tspeed: 0.1121s/iter; left time: 2643.3199s\n",
      "\titers: 800, epoch: 12 | loss: 0.0648808\n",
      "\tspeed: 0.1087s/iter; left time: 2551.1677s\n",
      "\titers: 900, epoch: 12 | loss: 0.0647268\n",
      "\tspeed: 0.1016s/iter; left time: 2373.7196s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0696728\n",
      "\tspeed: 0.1089s/iter; left time: 2535.6462s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0658773\n",
      "\tspeed: 0.1067s/iter; left time: 2471.9854s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0531521\n",
      "\tspeed: 0.1070s/iter; left time: 2468.5060s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0591090\n",
      "\tspeed: 0.1069s/iter; left time: 2455.4522s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0587741\n",
      "\tspeed: 0.1061s/iter; left time: 2426.6691s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0690005\n",
      "\tspeed: 0.1074s/iter; left time: 2446.9495s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0695689\n",
      "\tspeed: 0.1045s/iter; left time: 2369.4013s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0818760\n",
      "\tspeed: 0.1020s/iter; left time: 2302.3399s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0798704\n",
      "\tspeed: 0.1056s/iter; left time: 2374.0073s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0663811\n",
      "\tspeed: 0.1118s/iter; left time: 2502.4097s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0659007\n",
      "\tspeed: 0.1074s/iter; left time: 2392.9768s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0658267\n",
      "\tspeed: 0.1104s/iter; left time: 2447.0488s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0753273\n",
      "\tspeed: 0.1075s/iter; left time: 2373.4862s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0546868\n",
      "\tspeed: 0.1066s/iter; left time: 2343.3964s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0648501\n",
      "\tspeed: 0.1075s/iter; left time: 2351.3421s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0702483\n",
      "\tspeed: 0.1054s/iter; left time: 2294.0471s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0676616\n",
      "\tspeed: 0.1092s/iter; left time: 2367.6167s\n",
      "Epoch: 12 cost time: 00h:04m:50.84s\n",
      "Epoch: 12 | Train Loss: 0.0656386 Vali Loss: 0.0598788 Test Loss: 0.0692823\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0604794\n",
      "\tspeed: 0.9623s/iter; left time: 20667.4025s\n",
      "\titers: 200, epoch: 13 | loss: 0.0566729\n",
      "\tspeed: 0.1115s/iter; left time: 2383.8163s\n",
      "\titers: 300, epoch: 13 | loss: 0.0683386\n",
      "\tspeed: 0.1044s/iter; left time: 2221.7334s\n",
      "\titers: 400, epoch: 13 | loss: 0.0582213\n",
      "\tspeed: 0.1044s/iter; left time: 2211.1926s\n",
      "\titers: 500, epoch: 13 | loss: 0.0658862\n",
      "\tspeed: 0.1060s/iter; left time: 2234.2187s\n",
      "\titers: 600, epoch: 13 | loss: 0.0680031\n",
      "\tspeed: 0.1073s/iter; left time: 2250.3047s\n",
      "\titers: 700, epoch: 13 | loss: 0.0549427\n",
      "\tspeed: 0.1084s/iter; left time: 2263.1544s\n",
      "\titers: 800, epoch: 13 | loss: 0.0744731\n",
      "\tspeed: 0.1090s/iter; left time: 2265.1629s\n",
      "\titers: 900, epoch: 13 | loss: 0.0574103\n",
      "\tspeed: 0.1038s/iter; left time: 2146.3706s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0571434\n",
      "\tspeed: 0.1037s/iter; left time: 2133.7964s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0576256\n",
      "\tspeed: 0.1054s/iter; left time: 2157.5566s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0628352\n",
      "\tspeed: 0.1043s/iter; left time: 2124.8254s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0693550\n",
      "\tspeed: 0.1097s/iter; left time: 2224.0056s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0574602\n",
      "\tspeed: 0.1055s/iter; left time: 2128.5007s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0820113\n",
      "\tspeed: 0.1074s/iter; left time: 2156.9764s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0689111\n",
      "\tspeed: 0.1077s/iter; left time: 2152.1251s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0636011\n",
      "\tspeed: 0.1012s/iter; left time: 2012.1593s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0583539\n",
      "\tspeed: 0.1059s/iter; left time: 2094.1268s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0606554\n",
      "\tspeed: 0.1070s/iter; left time: 2105.1503s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0585214\n",
      "\tspeed: 0.1114s/iter; left time: 2181.7210s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0706814\n",
      "\tspeed: 0.1047s/iter; left time: 2038.4392s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0654261\n",
      "\tspeed: 0.1048s/iter; left time: 2031.2516s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0581603\n",
      "\tspeed: 0.1070s/iter; left time: 2062.0388s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0690250\n",
      "\tspeed: 0.1088s/iter; left time: 2086.9024s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0769652\n",
      "\tspeed: 0.1016s/iter; left time: 1938.6240s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0761575\n",
      "\tspeed: 0.1085s/iter; left time: 2059.0608s\n",
      "Epoch: 13 cost time: 00h:04m:47.77s\n",
      "Epoch: 13 | Train Loss: 0.0652557 Vali Loss: 0.0595088 Test Loss: 0.0683032\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0597761\n",
      "\tspeed: 0.9633s/iter; left time: 18091.5826s\n",
      "\titers: 200, epoch: 14 | loss: 0.0597405\n",
      "\tspeed: 0.1083s/iter; left time: 2023.3475s\n",
      "\titers: 300, epoch: 14 | loss: 0.0717362\n",
      "\tspeed: 0.1022s/iter; left time: 1899.2130s\n",
      "\titers: 400, epoch: 14 | loss: 0.0655360\n",
      "\tspeed: 0.1077s/iter; left time: 1991.1425s\n",
      "\titers: 500, epoch: 14 | loss: 0.0798717\n",
      "\tspeed: 0.1072s/iter; left time: 1969.8460s\n",
      "\titers: 600, epoch: 14 | loss: 0.0666957\n",
      "\tspeed: 0.1046s/iter; left time: 1911.4793s\n",
      "\titers: 700, epoch: 14 | loss: 0.0554276\n",
      "\tspeed: 0.1062s/iter; left time: 1930.6350s\n",
      "\titers: 800, epoch: 14 | loss: 0.0766484\n",
      "\tspeed: 0.1046s/iter; left time: 1890.7512s\n",
      "\titers: 900, epoch: 14 | loss: 0.0795836\n",
      "\tspeed: 0.1035s/iter; left time: 1861.6078s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0669616\n",
      "\tspeed: 0.1055s/iter; left time: 1885.5383s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0561617\n",
      "\tspeed: 0.1033s/iter; left time: 1837.4406s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0759818\n",
      "\tspeed: 0.1045s/iter; left time: 1846.9357s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0588643\n",
      "\tspeed: 0.1085s/iter; left time: 1907.6299s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0568695\n",
      "\tspeed: 0.1074s/iter; left time: 1876.9303s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0583049\n",
      "\tspeed: 0.1055s/iter; left time: 1834.1887s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0541657\n",
      "\tspeed: 0.1112s/iter; left time: 1922.0183s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0580471\n",
      "\tspeed: 0.1036s/iter; left time: 1780.6303s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0683933\n",
      "\tspeed: 0.1101s/iter; left time: 1879.6820s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0691114\n",
      "\tspeed: 0.1079s/iter; left time: 1831.3454s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0739959\n",
      "\tspeed: 0.1080s/iter; left time: 1823.0459s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0626454\n",
      "\tspeed: 0.1044s/iter; left time: 1751.9195s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0539643\n",
      "\tspeed: 0.1066s/iter; left time: 1777.7894s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0740233\n",
      "\tspeed: 0.1068s/iter; left time: 1770.6359s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0800757\n",
      "\tspeed: 0.1066s/iter; left time: 1756.5512s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0604383\n",
      "\tspeed: 0.1068s/iter; left time: 1748.9560s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0544207\n",
      "\tspeed: 0.1077s/iter; left time: 1753.0148s\n",
      "Epoch: 14 cost time: 00h:04m:47.33s\n",
      "Epoch: 14 | Train Loss: 0.0648617 Vali Loss: 0.0584747 Test Loss: 0.0677103\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0108054019510746, rmse:0.10394903272390366, mae:0.06720664352178574, rse:0.3051966428756714\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 24: 01h:26m:56.65s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 15:22:25,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 15:22:25,963] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 15:22:25,963] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 15:22:25,964] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 15:22:26,034] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 15:22:26,034] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 15:22:26,699] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 15:22:26,700] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 15:22:26,700] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 15:22:26,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 15:22:26,702] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 15:22:26,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 15:22:26,703] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 15:22:27,032] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 15:22:27,033] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,033] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.19 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,156] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 15:22:27,157] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,157] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.19 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,158] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 15:22:27,282] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 15:22:27,283] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,283] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.21 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,283] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 15:22:27,284] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8661b2a450>\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1883777\n",
      "\tspeed: 0.1633s/iter; left time: 8775.2950s\n",
      "\titers: 200, epoch: 1 | loss: 0.1642785\n",
      "\tspeed: 0.1161s/iter; left time: 6223.0266s\n",
      "\titers: 300, epoch: 1 | loss: 0.1586337\n",
      "\tspeed: 0.1227s/iter; left time: 6565.8886s\n",
      "\titers: 400, epoch: 1 | loss: 0.1215111\n",
      "\tspeed: 0.1174s/iter; left time: 6270.3139s\n",
      "\titers: 500, epoch: 1 | loss: 0.1161290\n",
      "\tspeed: 0.1187s/iter; left time: 6329.3336s\n",
      "\titers: 600, epoch: 1 | loss: 0.1045108\n",
      "\tspeed: 0.1210s/iter; left time: 6440.0003s\n",
      "\titers: 700, epoch: 1 | loss: 0.1008641\n",
      "\tspeed: 0.1206s/iter; left time: 6404.1045s\n",
      "\titers: 800, epoch: 1 | loss: 0.1015716\n",
      "\tspeed: 0.1204s/iter; left time: 6382.9067s\n",
      "\titers: 900, epoch: 1 | loss: 0.1061578\n",
      "\tspeed: 0.1167s/iter; left time: 6176.6558s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1075203\n",
      "\tspeed: 0.1218s/iter; left time: 6434.9860s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0915048\n",
      "\tspeed: 0.1173s/iter; left time: 6185.6594s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1113726\n",
      "\tspeed: 0.1205s/iter; left time: 6342.6255s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1193190\n",
      "\tspeed: 0.1181s/iter; left time: 6200.5595s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1057486\n",
      "\tspeed: 0.1203s/iter; left time: 6303.7762s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1058766\n",
      "\tspeed: 0.1208s/iter; left time: 6319.5797s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978397\n",
      "\tspeed: 0.1217s/iter; left time: 6355.8485s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0993557\n",
      "\tspeed: 0.1214s/iter; left time: 6328.9969s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1031770\n",
      "\tspeed: 0.1165s/iter; left time: 6058.6103s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0965251\n",
      "\tspeed: 0.1218s/iter; left time: 6325.7499s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0928079\n",
      "\tspeed: 0.1202s/iter; left time: 6226.6550s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0932634\n",
      "\tspeed: 0.1178s/iter; left time: 6093.9149s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1016541\n",
      "\tspeed: 0.1156s/iter; left time: 5964.9607s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0999433\n",
      "\tspeed: 0.1182s/iter; left time: 6089.1488s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0873916\n",
      "\tspeed: 0.1216s/iter; left time: 6252.8129s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0910878\n",
      "\tspeed: 0.1203s/iter; left time: 6176.1272s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1087127\n",
      "\tspeed: 0.1178s/iter; left time: 6033.2857s\n",
      "Epoch: 1 cost time: 00h:05m:23.31s\n",
      "Epoch: 1 | Train Loss: 0.1153284 Vali Loss: 0.0887686 Test Loss: 0.1010105\n",
      "Validation loss decreased (inf --> 0.088769).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0960205\n",
      "\tspeed: 1.1112s/iter; left time: 56705.4595s\n",
      "\titers: 200, epoch: 2 | loss: 0.0968005\n",
      "\tspeed: 0.1084s/iter; left time: 5522.1968s\n",
      "\titers: 300, epoch: 2 | loss: 0.1073778\n",
      "\tspeed: 0.1053s/iter; left time: 5351.6586s\n",
      "\titers: 400, epoch: 2 | loss: 0.0946367\n",
      "\tspeed: 0.1066s/iter; left time: 5407.8782s\n",
      "\titers: 500, epoch: 2 | loss: 0.1025144\n",
      "\tspeed: 0.1079s/iter; left time: 5462.6925s\n",
      "\titers: 600, epoch: 2 | loss: 0.0983518\n",
      "\tspeed: 0.1048s/iter; left time: 5297.9099s\n",
      "\titers: 700, epoch: 2 | loss: 0.0933921\n",
      "\tspeed: 0.1073s/iter; left time: 5408.6273s\n",
      "\titers: 800, epoch: 2 | loss: 0.0885800\n",
      "\tspeed: 0.1069s/iter; left time: 5378.0900s\n",
      "\titers: 900, epoch: 2 | loss: 0.0950760\n",
      "\tspeed: 0.1070s/iter; left time: 5375.5761s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0934220\n",
      "\tspeed: 0.1079s/iter; left time: 5410.7821s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1022061\n",
      "\tspeed: 0.1066s/iter; left time: 5333.7692s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0829705\n",
      "\tspeed: 0.1052s/iter; left time: 5252.5686s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1148255\n",
      "\tspeed: 0.1030s/iter; left time: 5133.3961s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0892722\n",
      "\tspeed: 0.1088s/iter; left time: 5409.7303s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0866018\n",
      "\tspeed: 0.1071s/iter; left time: 5314.3252s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0827379\n",
      "\tspeed: 0.1049s/iter; left time: 5194.5613s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1050955\n",
      "\tspeed: 0.1055s/iter; left time: 5217.1601s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1068194\n",
      "\tspeed: 0.1052s/iter; left time: 5190.2900s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0907224\n",
      "\tspeed: 0.1076s/iter; left time: 5299.5344s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0889307\n",
      "\tspeed: 0.1075s/iter; left time: 5279.8154s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0828655\n",
      "\tspeed: 0.1077s/iter; left time: 5278.4036s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1026875\n",
      "\tspeed: 0.1099s/iter; left time: 5375.0429s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0875362\n",
      "\tspeed: 0.1092s/iter; left time: 5333.6976s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0809421\n",
      "\tspeed: 0.1074s/iter; left time: 5234.9613s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0847859\n",
      "\tspeed: 0.1053s/iter; left time: 5118.4303s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0868387\n",
      "\tspeed: 0.1099s/iter; left time: 5333.3714s\n",
      "Epoch: 2 cost time: 00h:04m:48.57s\n",
      "Epoch: 2 | Train Loss: 0.0953358 Vali Loss: 0.0838980 Test Loss: 0.0960651\n",
      "Validation loss decreased (0.088769 --> 0.083898).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0928329\n",
      "\tspeed: 0.9862s/iter; left time: 47670.4618s\n",
      "\titers: 200, epoch: 3 | loss: 0.0811050\n",
      "\tspeed: 0.1161s/iter; left time: 5600.4357s\n",
      "\titers: 300, epoch: 3 | loss: 0.1074759\n",
      "\tspeed: 0.1070s/iter; left time: 5149.8389s\n",
      "\titers: 400, epoch: 3 | loss: 0.0964614\n",
      "\tspeed: 0.1077s/iter; left time: 5172.2576s\n",
      "\titers: 500, epoch: 3 | loss: 0.0867380\n",
      "\tspeed: 0.1053s/iter; left time: 5046.5864s\n",
      "\titers: 600, epoch: 3 | loss: 0.0928695\n",
      "\tspeed: 0.1092s/iter; left time: 5224.6830s\n",
      "\titers: 700, epoch: 3 | loss: 0.0858418\n",
      "\tspeed: 0.1073s/iter; left time: 5121.3753s\n",
      "\titers: 800, epoch: 3 | loss: 0.0918019\n",
      "\tspeed: 0.1065s/iter; left time: 5072.4651s\n",
      "\titers: 900, epoch: 3 | loss: 0.1130629\n",
      "\tspeed: 0.1070s/iter; left time: 5087.8674s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0996885\n",
      "\tspeed: 0.1049s/iter; left time: 4978.5365s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1002581\n",
      "\tspeed: 0.1097s/iter; left time: 5194.1672s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0801419\n",
      "\tspeed: 0.1075s/iter; left time: 5079.1085s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0852083\n",
      "\tspeed: 0.1083s/iter; left time: 5107.1702s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0802493\n",
      "\tspeed: 0.1044s/iter; left time: 4911.2287s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0922221\n",
      "\tspeed: 0.1100s/iter; left time: 5161.9898s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0829106\n",
      "\tspeed: 0.1074s/iter; left time: 5032.8094s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0909540\n",
      "\tspeed: 0.1058s/iter; left time: 4943.1901s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0992832\n",
      "\tspeed: 0.1073s/iter; left time: 5002.3535s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1036604\n",
      "\tspeed: 0.1048s/iter; left time: 4876.7098s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0947560\n",
      "\tspeed: 0.1055s/iter; left time: 4899.8111s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0935063\n",
      "\tspeed: 0.1090s/iter; left time: 5051.4298s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0912203\n",
      "\tspeed: 0.1015s/iter; left time: 4693.5850s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0946222\n",
      "\tspeed: 0.1068s/iter; left time: 4927.4407s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0792365\n",
      "\tspeed: 0.1075s/iter; left time: 4949.7346s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0930506\n",
      "\tspeed: 0.1021s/iter; left time: 4690.1926s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0854950\n",
      "\tspeed: 0.1063s/iter; left time: 4871.2306s\n",
      "Epoch: 3 cost time: 00h:04m:49.13s\n",
      "Epoch: 3 | Train Loss: 0.0915215 Vali Loss: 0.0821664 Test Loss: 0.0941079\n",
      "Validation loss decreased (0.083898 --> 0.082166).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0923687\n",
      "\tspeed: 0.9672s/iter; left time: 44149.4505s\n",
      "\titers: 200, epoch: 4 | loss: 0.0943383\n",
      "\tspeed: 0.1023s/iter; left time: 4660.4344s\n",
      "\titers: 300, epoch: 4 | loss: 0.0941553\n",
      "\tspeed: 0.1018s/iter; left time: 4625.8418s\n",
      "\titers: 400, epoch: 4 | loss: 0.0961315\n",
      "\tspeed: 0.1080s/iter; left time: 4896.0319s\n",
      "\titers: 500, epoch: 4 | loss: 0.1017793\n",
      "\tspeed: 0.1084s/iter; left time: 4904.2047s\n",
      "\titers: 600, epoch: 4 | loss: 0.0789471\n",
      "\tspeed: 0.1069s/iter; left time: 4826.5473s\n",
      "\titers: 700, epoch: 4 | loss: 0.0971971\n",
      "\tspeed: 0.1102s/iter; left time: 4962.9907s\n",
      "\titers: 800, epoch: 4 | loss: 0.0904444\n",
      "\tspeed: 0.1110s/iter; left time: 4987.5309s\n",
      "\titers: 900, epoch: 4 | loss: 0.0791268\n",
      "\tspeed: 0.1029s/iter; left time: 4615.9686s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0881244\n",
      "\tspeed: 0.1107s/iter; left time: 4953.3610s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0926313\n",
      "\tspeed: 0.0996s/iter; left time: 4444.7742s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0894173\n",
      "\tspeed: 0.1076s/iter; left time: 4791.3738s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0785045\n",
      "\tspeed: 0.1076s/iter; left time: 4781.0097s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0893424\n",
      "\tspeed: 0.1057s/iter; left time: 4688.4088s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0989641\n",
      "\tspeed: 0.1091s/iter; left time: 4828.1736s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0902890\n",
      "\tspeed: 0.1067s/iter; left time: 4709.9595s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0750174\n",
      "\tspeed: 0.1086s/iter; left time: 4785.6809s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0954725\n",
      "\tspeed: 0.1043s/iter; left time: 4582.1583s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0801812\n",
      "\tspeed: 0.1072s/iter; left time: 4700.9469s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0892521\n",
      "\tspeed: 0.1057s/iter; left time: 4624.6908s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0967448\n",
      "\tspeed: 0.1078s/iter; left time: 4704.9327s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0892473\n",
      "\tspeed: 0.1041s/iter; left time: 4531.9119s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1105002\n",
      "\tspeed: 0.1084s/iter; left time: 4709.2993s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0882045\n",
      "\tspeed: 0.1082s/iter; left time: 4689.7527s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0999998\n",
      "\tspeed: 0.1028s/iter; left time: 4443.7533s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0820032\n",
      "\tspeed: 0.1085s/iter; left time: 4680.1466s\n",
      "Epoch: 4 cost time: 00h:04m:47.46s\n",
      "Epoch: 4 | Train Loss: 0.0890123 Vali Loss: 0.0821997 Test Loss: 0.0937731\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0824345\n",
      "\tspeed: 0.9453s/iter; left time: 40605.9154s\n",
      "\titers: 200, epoch: 5 | loss: 0.0922259\n",
      "\tspeed: 0.1130s/iter; left time: 4844.8604s\n",
      "\titers: 300, epoch: 5 | loss: 0.0980015\n",
      "\tspeed: 0.1070s/iter; left time: 4576.3462s\n",
      "\titers: 400, epoch: 5 | loss: 0.0738367\n",
      "\tspeed: 0.1104s/iter; left time: 4711.3551s\n",
      "\titers: 500, epoch: 5 | loss: 0.0708809\n",
      "\tspeed: 0.1120s/iter; left time: 4767.7358s\n",
      "\titers: 600, epoch: 5 | loss: 0.0800962\n",
      "\tspeed: 0.1084s/iter; left time: 4602.6269s\n",
      "\titers: 700, epoch: 5 | loss: 0.0935463\n",
      "\tspeed: 0.1072s/iter; left time: 4542.5523s\n",
      "\titers: 800, epoch: 5 | loss: 0.0977683\n",
      "\tspeed: 0.1035s/iter; left time: 4374.0397s\n",
      "\titers: 900, epoch: 5 | loss: 0.0860935\n",
      "\tspeed: 0.1073s/iter; left time: 4523.5646s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0758314\n",
      "\tspeed: 0.1067s/iter; left time: 4489.0283s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0814082\n",
      "\tspeed: 0.1018s/iter; left time: 4270.5619s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0836338\n",
      "\tspeed: 0.1070s/iter; left time: 4479.7320s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0829336\n",
      "\tspeed: 0.1062s/iter; left time: 4435.0494s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0851112\n",
      "\tspeed: 0.1030s/iter; left time: 4289.4145s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0850937\n",
      "\tspeed: 0.1084s/iter; left time: 4506.2151s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0898389\n",
      "\tspeed: 0.1051s/iter; left time: 4357.1480s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0858165\n",
      "\tspeed: 0.1039s/iter; left time: 4298.8632s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0958296\n",
      "\tspeed: 0.1065s/iter; left time: 4392.8802s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0857323\n",
      "\tspeed: 0.1131s/iter; left time: 4655.0584s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0918692\n",
      "\tspeed: 0.1030s/iter; left time: 4228.9117s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0774475\n",
      "\tspeed: 0.1010s/iter; left time: 4136.6069s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0845494\n",
      "\tspeed: 0.1075s/iter; left time: 4392.7368s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0760283\n",
      "\tspeed: 0.1021s/iter; left time: 4161.3874s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0756378\n",
      "\tspeed: 0.1025s/iter; left time: 4167.8196s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0883363\n",
      "\tspeed: 0.1034s/iter; left time: 4195.5177s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0859664\n",
      "\tspeed: 0.1067s/iter; left time: 4316.4279s\n",
      "Epoch: 5 cost time: 00h:04m:46.37s\n",
      "Epoch: 5 | Train Loss: 0.0871692 Vali Loss: 0.0830350 Test Loss: 0.0942258\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0915383\n",
      "\tspeed: 0.9423s/iter; left time: 37943.1123s\n",
      "\titers: 200, epoch: 6 | loss: 0.0933986\n",
      "\tspeed: 0.1069s/iter; left time: 4292.3560s\n",
      "\titers: 300, epoch: 6 | loss: 0.0981537\n",
      "\tspeed: 0.1058s/iter; left time: 4237.9282s\n",
      "\titers: 400, epoch: 6 | loss: 0.0806353\n",
      "\tspeed: 0.1058s/iter; left time: 4226.9126s\n",
      "\titers: 500, epoch: 6 | loss: 0.0890182\n",
      "\tspeed: 0.1141s/iter; left time: 4546.9666s\n",
      "\titers: 600, epoch: 6 | loss: 0.0841566\n",
      "\tspeed: 0.1106s/iter; left time: 4397.1800s\n",
      "\titers: 700, epoch: 6 | loss: 0.0806254\n",
      "\tspeed: 0.1113s/iter; left time: 4416.3133s\n",
      "\titers: 800, epoch: 6 | loss: 0.0999852\n",
      "\tspeed: 0.1089s/iter; left time: 4308.5975s\n",
      "\titers: 900, epoch: 6 | loss: 0.0849152\n",
      "\tspeed: 0.1071s/iter; left time: 4227.5662s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0727589\n",
      "\tspeed: 0.1046s/iter; left time: 4117.9726s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0788582\n",
      "\tspeed: 0.1120s/iter; left time: 4399.3033s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0934843\n",
      "\tspeed: 0.1049s/iter; left time: 4108.5665s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0990512\n",
      "\tspeed: 0.1068s/iter; left time: 4174.1318s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0990836\n",
      "\tspeed: 0.1097s/iter; left time: 4276.2408s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0847571\n",
      "\tspeed: 0.1060s/iter; left time: 4120.4722s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0996485\n",
      "\tspeed: 0.1086s/iter; left time: 4211.1485s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0907493\n",
      "\tspeed: 0.1077s/iter; left time: 4162.6426s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0795749\n",
      "\tspeed: 0.1084s/iter; left time: 4182.1248s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0791156\n",
      "\tspeed: 0.1107s/iter; left time: 4257.6090s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0867164\n",
      "\tspeed: 0.1088s/iter; left time: 4174.8139s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0956665\n",
      "\tspeed: 0.1075s/iter; left time: 4115.4783s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0683716\n",
      "\tspeed: 0.1060s/iter; left time: 4043.8072s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0902782\n",
      "\tspeed: 0.1065s/iter; left time: 4052.8975s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0900548\n",
      "\tspeed: 0.1069s/iter; left time: 4057.7608s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0807459\n",
      "\tspeed: 0.1102s/iter; left time: 4172.5174s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0924551\n",
      "\tspeed: 0.1109s/iter; left time: 4187.1750s\n",
      "Epoch: 6 cost time: 00h:04m:51.55s\n",
      "Epoch: 6 | Train Loss: 0.0856582 Vali Loss: 0.0837385 Test Loss: 0.0947861\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0834660\n",
      "\tspeed: 0.9507s/iter; left time: 35723.7371s\n",
      "\titers: 200, epoch: 7 | loss: 0.0829694\n",
      "\tspeed: 0.1115s/iter; left time: 4179.7641s\n",
      "\titers: 300, epoch: 7 | loss: 0.0874605\n",
      "\tspeed: 0.1096s/iter; left time: 4096.6349s\n",
      "\titers: 400, epoch: 7 | loss: 0.0953893\n",
      "\tspeed: 0.1059s/iter; left time: 3945.9474s\n",
      "\titers: 500, epoch: 7 | loss: 0.0848485\n",
      "\tspeed: 0.1095s/iter; left time: 4069.8731s\n",
      "\titers: 600, epoch: 7 | loss: 0.0755942\n",
      "\tspeed: 0.1061s/iter; left time: 3935.3471s\n",
      "\titers: 700, epoch: 7 | loss: 0.0873404\n",
      "\tspeed: 0.1042s/iter; left time: 3852.8437s\n",
      "\titers: 800, epoch: 7 | loss: 0.0822200\n",
      "\tspeed: 0.1124s/iter; left time: 4143.6976s\n",
      "\titers: 900, epoch: 7 | loss: 0.0831545\n",
      "\tspeed: 0.1063s/iter; left time: 3908.0252s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0765932\n",
      "\tspeed: 0.1081s/iter; left time: 3962.8190s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0916426\n",
      "\tspeed: 0.1063s/iter; left time: 3889.7106s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0759756\n",
      "\tspeed: 0.1082s/iter; left time: 3945.4410s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0766117\n",
      "\tspeed: 0.1043s/iter; left time: 3794.8485s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0788070\n",
      "\tspeed: 0.1076s/iter; left time: 3904.0472s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1004317\n",
      "\tspeed: 0.1086s/iter; left time: 3930.3966s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0882952\n",
      "\tspeed: 0.1071s/iter; left time: 3864.3780s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0867677\n",
      "\tspeed: 0.1068s/iter; left time: 3842.4643s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0723154\n",
      "\tspeed: 0.1087s/iter; left time: 3900.5188s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0990128\n",
      "\tspeed: 0.1091s/iter; left time: 3902.1994s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0675883\n",
      "\tspeed: 0.1106s/iter; left time: 3945.9198s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0852043\n",
      "\tspeed: 0.1122s/iter; left time: 3990.1344s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0885752\n",
      "\tspeed: 0.1089s/iter; left time: 3864.1635s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0817467\n",
      "\tspeed: 0.1041s/iter; left time: 3681.1048s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0783509\n",
      "\tspeed: 0.1049s/iter; left time: 3701.9991s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0753475\n",
      "\tspeed: 0.1023s/iter; left time: 3597.0074s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0807764\n",
      "\tspeed: 0.1039s/iter; left time: 3642.7307s\n",
      "Epoch: 7 cost time: 00h:04m:49.85s\n",
      "Epoch: 7 | Train Loss: 0.0841171 Vali Loss: 0.0846731 Test Loss: 0.0937717\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0863863\n",
      "\tspeed: 0.9494s/iter; left time: 33117.9419s\n",
      "\titers: 200, epoch: 8 | loss: 0.0749087\n",
      "\tspeed: 0.1100s/iter; left time: 3825.2096s\n",
      "\titers: 300, epoch: 8 | loss: 0.0807150\n",
      "\tspeed: 0.1080s/iter; left time: 3745.5055s\n",
      "\titers: 400, epoch: 8 | loss: 0.0771998\n",
      "\tspeed: 0.1100s/iter; left time: 3803.8892s\n",
      "\titers: 500, epoch: 8 | loss: 0.0875563\n",
      "\tspeed: 0.1084s/iter; left time: 3738.6377s\n",
      "\titers: 600, epoch: 8 | loss: 0.0829368\n",
      "\tspeed: 0.1057s/iter; left time: 3634.9542s\n",
      "\titers: 700, epoch: 8 | loss: 0.0730132\n",
      "\tspeed: 0.1059s/iter; left time: 3631.4580s\n",
      "\titers: 800, epoch: 8 | loss: 0.0852739\n",
      "\tspeed: 0.1079s/iter; left time: 3689.0597s\n",
      "\titers: 900, epoch: 8 | loss: 0.0951376\n",
      "\tspeed: 0.1067s/iter; left time: 3637.2869s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0686005\n",
      "\tspeed: 0.1021s/iter; left time: 3468.3762s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0827262\n",
      "\tspeed: 0.1043s/iter; left time: 3535.0200s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0796252\n",
      "\tspeed: 0.1093s/iter; left time: 3691.5745s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0840250\n",
      "\tspeed: 0.1037s/iter; left time: 3491.9403s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0879034\n",
      "\tspeed: 0.1097s/iter; left time: 3684.5622s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0701827\n",
      "\tspeed: 0.1041s/iter; left time: 3485.9761s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0842006\n",
      "\tspeed: 0.1048s/iter; left time: 3497.2469s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0884516\n",
      "\tspeed: 0.1088s/iter; left time: 3619.6541s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1042492\n",
      "\tspeed: 0.1097s/iter; left time: 3640.4532s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0795427\n",
      "\tspeed: 0.1074s/iter; left time: 3552.2897s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0710123\n",
      "\tspeed: 0.1088s/iter; left time: 3589.3398s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0780427\n",
      "\tspeed: 0.1083s/iter; left time: 3562.3860s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0920559\n",
      "\tspeed: 0.1060s/iter; left time: 3473.6784s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0782730\n",
      "\tspeed: 0.1046s/iter; left time: 3418.4985s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0835265\n",
      "\tspeed: 0.1068s/iter; left time: 3478.3846s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0733754\n",
      "\tspeed: 0.1107s/iter; left time: 3596.9654s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0804105\n",
      "\tspeed: 0.1018s/iter; left time: 3295.7376s\n",
      "Epoch: 8 cost time: 00h:04m:48.48s\n",
      "Epoch: 8 | Train Loss: 0.0827061 Vali Loss: 0.0853918 Test Loss: 0.0946178\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019848739728331566, rmse:0.14088556170463562, mae:0.09410795569419861, rse:0.41384807229042053\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 96: 00h:50m:14.97s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 16:12:39,498] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 16:12:40,572] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 16:12:40,573] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 16:12:40,573] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 16:12:40,675] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 16:12:40,676] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 16:12:41,349] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 16:12:41,350] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 16:12:41,350] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 16:12:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 16:12:41,352] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 16:12:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 16:12:41,689] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 16:12:41,690] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,690] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,817] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 16:12:41,818] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,818] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,818] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 16:12:41,937] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 16:12:41,938] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 16:12:41,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 16:12:41,940] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc2486677d0>\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1778164\n",
      "\tspeed: 0.1568s/iter; left time: 8400.0508s\n",
      "\titers: 200, epoch: 1 | loss: 0.1971989\n",
      "\tspeed: 0.1192s/iter; left time: 6377.3523s\n",
      "\titers: 300, epoch: 1 | loss: 0.1594778\n",
      "\tspeed: 0.1232s/iter; left time: 6575.6436s\n",
      "\titers: 400, epoch: 1 | loss: 0.1233426\n",
      "\tspeed: 0.1147s/iter; left time: 6111.6815s\n",
      "\titers: 500, epoch: 1 | loss: 0.1213352\n",
      "\tspeed: 0.1189s/iter; left time: 6324.4946s\n",
      "\titers: 600, epoch: 1 | loss: 0.1119261\n",
      "\tspeed: 0.1218s/iter; left time: 6463.4824s\n",
      "\titers: 700, epoch: 1 | loss: 0.1127991\n",
      "\tspeed: 0.1219s/iter; left time: 6458.3757s\n",
      "\titers: 800, epoch: 1 | loss: 0.1025673\n",
      "\tspeed: 0.1211s/iter; left time: 6401.9189s\n",
      "\titers: 900, epoch: 1 | loss: 0.1033708\n",
      "\tspeed: 0.1208s/iter; left time: 6373.7260s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1163776\n",
      "\tspeed: 0.1170s/iter; left time: 6163.5149s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1146140\n",
      "\tspeed: 0.1225s/iter; left time: 6440.3435s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1062645\n",
      "\tspeed: 0.1206s/iter; left time: 6330.5963s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1152460\n",
      "\tspeed: 0.1215s/iter; left time: 6363.5154s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1100958\n",
      "\tspeed: 0.1152s/iter; left time: 6025.1233s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0964408\n",
      "\tspeed: 0.1175s/iter; left time: 6133.0263s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0965621\n",
      "\tspeed: 0.1209s/iter; left time: 6297.9019s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0997088\n",
      "\tspeed: 0.1155s/iter; left time: 6005.6949s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1003732\n",
      "\tspeed: 0.1206s/iter; left time: 6257.5801s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1042660\n",
      "\tspeed: 0.1207s/iter; left time: 6248.1295s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0969521\n",
      "\tspeed: 0.1202s/iter; left time: 6210.6491s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1015071\n",
      "\tspeed: 0.1194s/iter; left time: 6159.6435s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1033745\n",
      "\tspeed: 0.1223s/iter; left time: 6297.3941s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1043368\n",
      "\tspeed: 0.1198s/iter; left time: 6157.7247s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0983482\n",
      "\tspeed: 0.1177s/iter; left time: 6037.7931s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0977514\n",
      "\tspeed: 0.1148s/iter; left time: 5873.1216s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1156735\n",
      "\tspeed: 0.1235s/iter; left time: 6309.1311s\n",
      "Epoch: 1 cost time: 00h:05m:21.95s\n",
      "Epoch: 1 | Train Loss: 0.1179913 Vali Loss: 0.0912490 Test Loss: 0.1035000\n",
      "Validation loss decreased (inf --> 0.091249).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1087977\n",
      "\tspeed: 1.0831s/iter; left time: 55126.1953s\n",
      "\titers: 200, epoch: 2 | loss: 0.1095505\n",
      "\tspeed: 0.1064s/iter; left time: 5405.0116s\n",
      "\titers: 300, epoch: 2 | loss: 0.0956247\n",
      "\tspeed: 0.1052s/iter; left time: 5331.4448s\n",
      "\titers: 400, epoch: 2 | loss: 0.1070556\n",
      "\tspeed: 0.1094s/iter; left time: 5535.6977s\n",
      "\titers: 500, epoch: 2 | loss: 0.0973769\n",
      "\tspeed: 0.1070s/iter; left time: 5405.1235s\n",
      "\titers: 600, epoch: 2 | loss: 0.0893573\n",
      "\tspeed: 0.1065s/iter; left time: 5367.3892s\n",
      "\titers: 700, epoch: 2 | loss: 0.0881983\n",
      "\tspeed: 0.1108s/iter; left time: 5572.1252s\n",
      "\titers: 800, epoch: 2 | loss: 0.0978097\n",
      "\tspeed: 0.1110s/iter; left time: 5569.7477s\n",
      "\titers: 900, epoch: 2 | loss: 0.1160224\n",
      "\tspeed: 0.1074s/iter; left time: 5379.9568s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1127868\n",
      "\tspeed: 0.1097s/iter; left time: 5485.2445s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1066502\n",
      "\tspeed: 0.1080s/iter; left time: 5387.8599s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0881434\n",
      "\tspeed: 0.1071s/iter; left time: 5334.1044s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0893611\n",
      "\tspeed: 0.1098s/iter; left time: 5454.9817s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1034243\n",
      "\tspeed: 0.1085s/iter; left time: 5379.1939s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0995729\n",
      "\tspeed: 0.1069s/iter; left time: 5289.2479s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1037162\n",
      "\tspeed: 0.1134s/iter; left time: 5601.0162s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1003603\n",
      "\tspeed: 0.1106s/iter; left time: 5453.2157s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1068703\n",
      "\tspeed: 0.1114s/iter; left time: 5480.0954s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0950982\n",
      "\tspeed: 0.1109s/iter; left time: 5444.0518s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0966142\n",
      "\tspeed: 0.1096s/iter; left time: 5367.6743s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0983411\n",
      "\tspeed: 0.1115s/iter; left time: 5452.8035s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0992421\n",
      "\tspeed: 0.1160s/iter; left time: 5660.8030s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0934880\n",
      "\tspeed: 0.1209s/iter; left time: 5886.0998s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1001284\n",
      "\tspeed: 0.1206s/iter; left time: 5862.0561s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0928106\n",
      "\tspeed: 0.1182s/iter; left time: 5733.7461s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0870252\n",
      "\tspeed: 0.1209s/iter; left time: 5852.0493s\n",
      "Epoch: 2 cost time: 00h:04m:58.85s\n",
      "Epoch: 2 | Train Loss: 0.0990444 Vali Loss: 0.0869754 Test Loss: 0.0983193\n",
      "Validation loss decreased (0.091249 --> 0.086975).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1038128\n",
      "\tspeed: 0.9964s/iter; left time: 48039.4661s\n",
      "\titers: 200, epoch: 3 | loss: 0.0941214\n",
      "\tspeed: 0.1201s/iter; left time: 5779.3333s\n",
      "\titers: 300, epoch: 3 | loss: 0.0934169\n",
      "\tspeed: 0.1213s/iter; left time: 5824.3659s\n",
      "\titers: 400, epoch: 3 | loss: 0.0778983\n",
      "\tspeed: 0.1209s/iter; left time: 5790.6581s\n",
      "\titers: 500, epoch: 3 | loss: 0.0914758\n",
      "\tspeed: 0.1210s/iter; left time: 5786.5365s\n",
      "\titers: 600, epoch: 3 | loss: 0.0881995\n",
      "\tspeed: 0.1208s/iter; left time: 5763.0747s\n",
      "\titers: 700, epoch: 3 | loss: 0.0977619\n",
      "\tspeed: 0.1166s/iter; left time: 5551.1786s\n",
      "\titers: 800, epoch: 3 | loss: 0.1015535\n",
      "\tspeed: 0.1195s/iter; left time: 5676.9406s\n",
      "\titers: 900, epoch: 3 | loss: 0.1082432\n",
      "\tspeed: 0.1198s/iter; left time: 5681.4679s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0877309\n",
      "\tspeed: 0.1202s/iter; left time: 5687.6272s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1051150\n",
      "\tspeed: 0.1181s/iter; left time: 5577.2203s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0997433\n",
      "\tspeed: 0.1216s/iter; left time: 5729.0933s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1073216\n",
      "\tspeed: 0.1207s/iter; left time: 5673.5919s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1008831\n",
      "\tspeed: 0.1204s/iter; left time: 5647.8936s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1161132\n",
      "\tspeed: 0.1190s/iter; left time: 5572.2122s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0918901\n",
      "\tspeed: 0.1205s/iter; left time: 5629.8350s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0909352\n",
      "\tspeed: 0.1207s/iter; left time: 5625.9115s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1053216\n",
      "\tspeed: 0.1179s/iter; left time: 5486.0038s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0864966\n",
      "\tspeed: 0.1188s/iter; left time: 5515.0144s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0899872\n",
      "\tspeed: 0.1170s/iter; left time: 5416.6913s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0996342\n",
      "\tspeed: 0.1210s/iter; left time: 5589.6104s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0904091\n",
      "\tspeed: 0.1197s/iter; left time: 5521.2046s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0838375\n",
      "\tspeed: 0.1196s/iter; left time: 5503.2014s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0930159\n",
      "\tspeed: 0.1201s/iter; left time: 5513.1989s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0981316\n",
      "\tspeed: 0.1194s/iter; left time: 5470.3641s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0863672\n",
      "\tspeed: 0.1208s/iter; left time: 5523.6617s\n",
      "Epoch: 3 cost time: 00h:05m:22.30s\n",
      "Epoch: 3 | Train Loss: 0.0952172 Vali Loss: 0.0876163 Test Loss: 0.0995333\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0895635\n",
      "\tspeed: 0.9674s/iter; left time: 44045.3275s\n",
      "\titers: 200, epoch: 4 | loss: 0.0948796\n",
      "\tspeed: 0.1135s/iter; left time: 5154.2111s\n",
      "\titers: 300, epoch: 4 | loss: 0.1079016\n",
      "\tspeed: 0.1192s/iter; left time: 5402.4415s\n",
      "\titers: 400, epoch: 4 | loss: 0.0807232\n",
      "\tspeed: 0.1204s/iter; left time: 5447.4501s\n",
      "\titers: 500, epoch: 4 | loss: 0.0917320\n",
      "\tspeed: 0.1206s/iter; left time: 5443.9061s\n",
      "\titers: 600, epoch: 4 | loss: 0.1008527\n",
      "\tspeed: 0.1212s/iter; left time: 5457.4455s\n",
      "\titers: 700, epoch: 4 | loss: 0.0905929\n",
      "\tspeed: 0.1211s/iter; left time: 5441.5946s\n",
      "\titers: 800, epoch: 4 | loss: 0.0817054\n",
      "\tspeed: 0.1209s/iter; left time: 5418.5595s\n",
      "\titers: 900, epoch: 4 | loss: 0.1022704\n",
      "\tspeed: 0.1202s/iter; left time: 5376.9019s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0999512\n",
      "\tspeed: 0.1211s/iter; left time: 5404.3136s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0844137\n",
      "\tspeed: 0.1180s/iter; left time: 5256.2973s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0867865\n",
      "\tspeed: 0.1206s/iter; left time: 5359.1979s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1027440\n",
      "\tspeed: 0.1200s/iter; left time: 5320.0796s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0912939\n",
      "\tspeed: 0.1153s/iter; left time: 5100.1629s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0903338\n",
      "\tspeed: 0.1172s/iter; left time: 5172.8185s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0982658\n",
      "\tspeed: 0.1160s/iter; left time: 5108.6095s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1128476\n",
      "\tspeed: 0.1203s/iter; left time: 5285.2844s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0907316\n",
      "\tspeed: 0.1210s/iter; left time: 5305.2216s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0883882\n",
      "\tspeed: 0.1180s/iter; left time: 5161.3531s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0840461\n",
      "\tspeed: 0.1159s/iter; left time: 5057.2308s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0890422\n",
      "\tspeed: 0.1211s/iter; left time: 5272.9600s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0912848\n",
      "\tspeed: 0.1211s/iter; left time: 5258.3974s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0897235\n",
      "\tspeed: 0.1185s/iter; left time: 5134.2887s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1019709\n",
      "\tspeed: 0.1184s/iter; left time: 5116.5241s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1029302\n",
      "\tspeed: 0.1200s/iter; left time: 5173.7312s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0911055\n",
      "\tspeed: 0.1146s/iter; left time: 4929.7989s\n",
      "Epoch: 4 cost time: 00h:05m:19.24s\n",
      "Epoch: 4 | Train Loss: 0.0930185 Vali Loss: 0.0847382 Test Loss: 0.0970126\n",
      "Validation loss decreased (0.086975 --> 0.084738).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0929508\n",
      "\tspeed: 1.0025s/iter; left time: 42951.2239s\n",
      "\titers: 200, epoch: 5 | loss: 0.0826883\n",
      "\tspeed: 0.1199s/iter; left time: 5124.5079s\n",
      "\titers: 300, epoch: 5 | loss: 0.0946675\n",
      "\tspeed: 0.1210s/iter; left time: 5160.7740s\n",
      "\titers: 400, epoch: 5 | loss: 0.0974359\n",
      "\tspeed: 0.1195s/iter; left time: 5082.2881s\n",
      "\titers: 500, epoch: 5 | loss: 0.0892060\n",
      "\tspeed: 0.1167s/iter; left time: 4953.6776s\n",
      "\titers: 600, epoch: 5 | loss: 0.0965789\n",
      "\tspeed: 0.1204s/iter; left time: 5096.6301s\n",
      "\titers: 700, epoch: 5 | loss: 0.0803632\n",
      "\tspeed: 0.1200s/iter; left time: 5067.3871s\n",
      "\titers: 800, epoch: 5 | loss: 0.0816639\n",
      "\tspeed: 0.1213s/iter; left time: 5113.3156s\n",
      "\titers: 900, epoch: 5 | loss: 0.0796301\n",
      "\tspeed: 0.1208s/iter; left time: 5080.1200s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0947778\n",
      "\tspeed: 0.1208s/iter; left time: 5066.0018s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0807358\n",
      "\tspeed: 0.1191s/iter; left time: 4983.2578s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0978350\n",
      "\tspeed: 0.1177s/iter; left time: 4913.5078s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0848728\n",
      "\tspeed: 0.1177s/iter; left time: 4903.4044s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0797540\n",
      "\tspeed: 0.1152s/iter; left time: 4787.1344s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0875790\n",
      "\tspeed: 0.1201s/iter; left time: 4976.1642s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0836178\n",
      "\tspeed: 0.1178s/iter; left time: 4869.7515s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0997139\n",
      "\tspeed: 0.1199s/iter; left time: 4944.8162s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0831015\n",
      "\tspeed: 0.1210s/iter; left time: 4979.7305s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0892239\n",
      "\tspeed: 0.1206s/iter; left time: 4948.1102s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0964089\n",
      "\tspeed: 0.1180s/iter; left time: 4832.3976s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0864571\n",
      "\tspeed: 0.1157s/iter; left time: 4726.1310s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0878882\n",
      "\tspeed: 0.1009s/iter; left time: 4112.1107s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0879328\n",
      "\tspeed: 0.1120s/iter; left time: 4552.9007s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0824142\n",
      "\tspeed: 0.1168s/iter; left time: 4735.5517s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0919031\n",
      "\tspeed: 0.1178s/iter; left time: 4762.9217s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0861144\n",
      "\tspeed: 0.1173s/iter; left time: 4730.6570s\n",
      "Epoch: 5 cost time: 00h:05m:17.24s\n",
      "Epoch: 5 | Train Loss: 0.0909510 Vali Loss: 0.0893654 Test Loss: 0.1023573\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0915587\n",
      "\tspeed: 0.9724s/iter; left time: 39053.6271s\n",
      "\titers: 200, epoch: 6 | loss: 0.0839357\n",
      "\tspeed: 0.1172s/iter; left time: 4696.5596s\n",
      "\titers: 300, epoch: 6 | loss: 0.0889259\n",
      "\tspeed: 0.1207s/iter; left time: 4824.6661s\n",
      "\titers: 400, epoch: 6 | loss: 0.0936077\n",
      "\tspeed: 0.1174s/iter; left time: 4681.1153s\n",
      "\titers: 500, epoch: 6 | loss: 0.0880979\n",
      "\tspeed: 0.1211s/iter; left time: 4814.4650s\n",
      "\titers: 600, epoch: 6 | loss: 0.0841514\n",
      "\tspeed: 0.1163s/iter; left time: 4613.3570s\n",
      "\titers: 700, epoch: 6 | loss: 0.0755864\n",
      "\tspeed: 0.1119s/iter; left time: 4424.9740s\n",
      "\titers: 800, epoch: 6 | loss: 0.0940484\n",
      "\tspeed: 0.1193s/iter; left time: 4709.3520s\n",
      "\titers: 900, epoch: 6 | loss: 0.0851911\n",
      "\tspeed: 0.1204s/iter; left time: 4738.1928s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0822518\n",
      "\tspeed: 0.1171s/iter; left time: 4596.1306s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0868344\n",
      "\tspeed: 0.1186s/iter; left time: 4645.2629s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0963878\n",
      "\tspeed: 0.1201s/iter; left time: 4691.8284s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0748070\n",
      "\tspeed: 0.1208s/iter; left time: 4707.5500s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0844528\n",
      "\tspeed: 0.1209s/iter; left time: 4699.4556s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1027090\n",
      "\tspeed: 0.1202s/iter; left time: 4657.6838s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0973790\n",
      "\tspeed: 0.1175s/iter; left time: 4543.9550s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0843874\n",
      "\tspeed: 0.1152s/iter; left time: 4442.2165s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0826969\n",
      "\tspeed: 0.1178s/iter; left time: 4530.4196s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0803014\n",
      "\tspeed: 0.1199s/iter; left time: 4599.0814s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0895572\n",
      "\tspeed: 0.1163s/iter; left time: 4449.4302s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0838565\n",
      "\tspeed: 0.1150s/iter; left time: 4389.7193s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0904025\n",
      "\tspeed: 0.1150s/iter; left time: 4378.3628s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0983820\n",
      "\tspeed: 0.1139s/iter; left time: 4323.7095s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0976380\n",
      "\tspeed: 0.1158s/iter; left time: 4382.8892s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0838670\n",
      "\tspeed: 0.1193s/iter; left time: 4505.8964s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0699371\n",
      "\tspeed: 0.1031s/iter; left time: 3882.8722s\n",
      "Epoch: 6 cost time: 00h:05m:13.96s\n",
      "Epoch: 6 | Train Loss: 0.0890947 Vali Loss: 0.0903814 Test Loss: 0.1015913\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0818404\n",
      "\tspeed: 0.9498s/iter; left time: 35597.2597s\n",
      "\titers: 200, epoch: 7 | loss: 0.0816711\n",
      "\tspeed: 0.1205s/iter; left time: 4505.1611s\n",
      "\titers: 300, epoch: 7 | loss: 0.0799331\n",
      "\tspeed: 0.1183s/iter; left time: 4408.2543s\n",
      "\titers: 400, epoch: 7 | loss: 0.0868699\n",
      "\tspeed: 0.1196s/iter; left time: 4447.8012s\n",
      "\titers: 500, epoch: 7 | loss: 0.0727430\n",
      "\tspeed: 0.1205s/iter; left time: 4468.0822s\n",
      "\titers: 600, epoch: 7 | loss: 0.1011259\n",
      "\tspeed: 0.1199s/iter; left time: 4433.9748s\n",
      "\titers: 700, epoch: 7 | loss: 0.0941924\n",
      "\tspeed: 0.1167s/iter; left time: 4303.5286s\n",
      "\titers: 800, epoch: 7 | loss: 0.1013498\n",
      "\tspeed: 0.1180s/iter; left time: 4338.7645s\n",
      "\titers: 900, epoch: 7 | loss: 0.0759829\n",
      "\tspeed: 0.1145s/iter; left time: 4200.3665s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0930336\n",
      "\tspeed: 0.1187s/iter; left time: 4343.0069s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0847954\n",
      "\tspeed: 0.1173s/iter; left time: 4279.1998s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0941292\n",
      "\tspeed: 0.1117s/iter; left time: 4064.6430s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0802033\n",
      "\tspeed: 0.1201s/iter; left time: 4357.1399s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0939994\n",
      "\tspeed: 0.1140s/iter; left time: 4122.7113s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0865828\n",
      "\tspeed: 0.1168s/iter; left time: 4214.2135s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1023660\n",
      "\tspeed: 0.1166s/iter; left time: 4193.9149s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0873066\n",
      "\tspeed: 0.1160s/iter; left time: 4160.8034s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0956445\n",
      "\tspeed: 0.1195s/iter; left time: 4275.3586s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0968382\n",
      "\tspeed: 0.1204s/iter; left time: 4296.9154s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0982233\n",
      "\tspeed: 0.1196s/iter; left time: 4255.4905s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0968592\n",
      "\tspeed: 0.1206s/iter; left time: 4280.2815s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0934383\n",
      "\tspeed: 0.1163s/iter; left time: 4115.5740s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0847614\n",
      "\tspeed: 0.1069s/iter; left time: 3769.4850s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0907320\n",
      "\tspeed: 0.0988s/iter; left time: 3476.2804s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0855488\n",
      "\tspeed: 0.1049s/iter; left time: 3679.9229s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0829888\n",
      "\tspeed: 0.1192s/iter; left time: 4168.3115s\n",
      "Epoch: 7 cost time: 00h:05m:12.16s\n",
      "Epoch: 7 | Train Loss: 0.0873331 Vali Loss: 0.0918732 Test Loss: 0.1017229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0874928\n",
      "\tspeed: 0.9747s/iter; left time: 33914.2749s\n",
      "\titers: 200, epoch: 8 | loss: 0.0883391\n",
      "\tspeed: 0.1208s/iter; left time: 4192.1919s\n",
      "\titers: 300, epoch: 8 | loss: 0.0920282\n",
      "\tspeed: 0.1206s/iter; left time: 4172.7244s\n",
      "\titers: 400, epoch: 8 | loss: 0.0959568\n",
      "\tspeed: 0.1160s/iter; left time: 4002.3766s\n",
      "\titers: 500, epoch: 8 | loss: 0.0956558\n",
      "\tspeed: 0.1137s/iter; left time: 3911.7858s\n",
      "\titers: 600, epoch: 8 | loss: 0.0814130\n",
      "\tspeed: 0.1138s/iter; left time: 3903.7645s\n",
      "\titers: 700, epoch: 8 | loss: 0.0857233\n",
      "\tspeed: 0.1151s/iter; left time: 3936.0245s\n",
      "\titers: 800, epoch: 8 | loss: 0.0938325\n",
      "\tspeed: 0.1149s/iter; left time: 3917.1635s\n",
      "\titers: 900, epoch: 8 | loss: 0.0937250\n",
      "\tspeed: 0.1155s/iter; left time: 3927.7969s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0781743\n",
      "\tspeed: 0.1171s/iter; left time: 3970.0446s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0944517\n",
      "\tspeed: 0.1190s/iter; left time: 4020.8408s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0741252\n",
      "\tspeed: 0.1204s/iter; left time: 4056.7614s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0715947\n",
      "\tspeed: 0.1169s/iter; left time: 3927.1916s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0883247\n",
      "\tspeed: 0.1207s/iter; left time: 4041.0368s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0868704\n",
      "\tspeed: 0.1211s/iter; left time: 4044.6174s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0939969\n",
      "\tspeed: 0.1207s/iter; left time: 4018.8690s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0877550\n",
      "\tspeed: 0.1188s/iter; left time: 3943.0573s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0865582\n",
      "\tspeed: 0.1168s/iter; left time: 3866.4770s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0877632\n",
      "\tspeed: 0.1206s/iter; left time: 3980.0795s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0904972\n",
      "\tspeed: 0.1198s/iter; left time: 3942.0435s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0918995\n",
      "\tspeed: 0.1113s/iter; left time: 3650.8766s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0880162\n",
      "\tspeed: 0.0986s/iter; left time: 3224.0149s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0901393\n",
      "\tspeed: 0.0995s/iter; left time: 3243.9052s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0813730\n",
      "\tspeed: 0.1093s/iter; left time: 3550.2013s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0902627\n",
      "\tspeed: 0.1151s/iter; left time: 3727.5932s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0779674\n",
      "\tspeed: 0.1148s/iter; left time: 3707.7360s\n",
      "Epoch: 8 cost time: 00h:05m:09.77s\n",
      "Epoch: 8 | Train Loss: 0.0859112 Vali Loss: 0.0929700 Test Loss: 0.1019867\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0787471\n",
      "\tspeed: 0.9498s/iter; left time: 30497.4128s\n",
      "\titers: 200, epoch: 9 | loss: 0.0741966\n",
      "\tspeed: 0.1207s/iter; left time: 3863.2914s\n",
      "\titers: 300, epoch: 9 | loss: 0.0855073\n",
      "\tspeed: 0.1000s/iter; left time: 3191.7296s\n",
      "\titers: 400, epoch: 9 | loss: 0.0870367\n",
      "\tspeed: 0.1007s/iter; left time: 3204.3736s\n",
      "\titers: 500, epoch: 9 | loss: 0.0798489\n",
      "\tspeed: 0.1197s/iter; left time: 3795.4704s\n",
      "\titers: 600, epoch: 9 | loss: 0.0822773\n",
      "\tspeed: 0.1157s/iter; left time: 3658.4578s\n",
      "\titers: 700, epoch: 9 | loss: 0.0890888\n",
      "\tspeed: 0.1152s/iter; left time: 3631.3840s\n",
      "\titers: 800, epoch: 9 | loss: 0.0833702\n",
      "\tspeed: 0.1192s/iter; left time: 3743.8704s\n",
      "\titers: 900, epoch: 9 | loss: 0.0905662\n",
      "\tspeed: 0.1181s/iter; left time: 3698.1330s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0836187\n",
      "\tspeed: 0.1191s/iter; left time: 3717.9772s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0860944\n",
      "\tspeed: 0.1085s/iter; left time: 3375.6543s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0905798\n",
      "\tspeed: 0.1175s/iter; left time: 3644.6725s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0945324\n",
      "\tspeed: 0.1188s/iter; left time: 3672.6808s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0950186\n",
      "\tspeed: 0.1207s/iter; left time: 3719.6932s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0807866\n",
      "\tspeed: 0.1212s/iter; left time: 3722.9836s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0886903\n",
      "\tspeed: 0.1214s/iter; left time: 3716.6798s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0865668\n",
      "\tspeed: 0.1197s/iter; left time: 3651.5093s\n",
      "\titers: 1800, epoch: 9 | loss: 0.1007445\n",
      "\tspeed: 0.1214s/iter; left time: 3692.1223s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0763340\n",
      "\tspeed: 0.1209s/iter; left time: 3664.0420s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0765861\n",
      "\tspeed: 0.1181s/iter; left time: 3566.9978s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0786577\n",
      "\tspeed: 0.1166s/iter; left time: 3510.5500s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0867473\n",
      "\tspeed: 0.1143s/iter; left time: 3429.0570s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0803090\n",
      "\tspeed: 0.1174s/iter; left time: 3510.2132s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0835162\n",
      "\tspeed: 0.1194s/iter; left time: 3559.9645s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0848668\n",
      "\tspeed: 0.1213s/iter; left time: 3604.7663s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0791643\n",
      "\tspeed: 0.1211s/iter; left time: 3586.6447s\n",
      "Epoch: 9 cost time: 00h:05m:15.40s\n",
      "Epoch: 9 | Train Loss: 0.0844346 Vali Loss: 0.0939959 Test Loss: 0.1040973\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.021669914945960045, rmse:0.14720705151557922, mae:0.09701256453990936, rse:0.4322395920753479\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 168: 00h:59m:42.76s\n",
      "\n",
      "Intermediate time for ES: 03h:16m:54.38s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 17:12:24,451] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 17:12:25,602] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 17:12:25,602] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 17:12:25,602] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 17:12:25,704] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 17:12:25,704] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 17:12:26,388] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 17:12:26,390] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 17:12:26,390] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 17:12:26,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 17:12:26,391] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 17:12:26,391] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 17:12:26,729] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 17:12:26,730] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,730] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,855] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 17:12:26,856] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,856] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,857] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 17:12:26,981] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 17:12:26,982] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,982] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 17:12:26,983] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f73cce4d3d0>\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1235450\n",
      "\tspeed: 0.1709s/iter; left time: 9199.2743s\n",
      "\titers: 200, epoch: 1 | loss: 0.1423506\n",
      "\tspeed: 0.1264s/iter; left time: 6791.4893s\n",
      "\titers: 300, epoch: 1 | loss: 0.1272179\n",
      "\tspeed: 0.1274s/iter; left time: 6835.9856s\n",
      "\titers: 400, epoch: 1 | loss: 0.0875500\n",
      "\tspeed: 0.1272s/iter; left time: 6808.7591s\n",
      "\titers: 500, epoch: 1 | loss: 0.0672416\n",
      "\tspeed: 0.1263s/iter; left time: 6751.9674s\n",
      "\titers: 600, epoch: 1 | loss: 0.0630464\n",
      "\tspeed: 0.1064s/iter; left time: 5676.5741s\n",
      "\titers: 700, epoch: 1 | loss: 0.0556630\n",
      "\tspeed: 0.1132s/iter; left time: 6025.5661s\n",
      "\titers: 800, epoch: 1 | loss: 0.0871794\n",
      "\tspeed: 0.1206s/iter; left time: 6411.3034s\n",
      "\titers: 900, epoch: 1 | loss: 0.0597571\n",
      "\tspeed: 0.1234s/iter; left time: 6543.0970s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0630214\n",
      "\tspeed: 0.1252s/iter; left time: 6630.3009s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0680356\n",
      "\tspeed: 0.1159s/iter; left time: 6121.6533s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0580192\n",
      "\tspeed: 0.1251s/iter; left time: 6595.4756s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0591872\n",
      "\tspeed: 0.1258s/iter; left time: 6620.7701s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0653023\n",
      "\tspeed: 0.1249s/iter; left time: 6559.8268s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0712432\n",
      "\tspeed: 0.1207s/iter; left time: 6328.8275s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0695876\n",
      "\tspeed: 0.1167s/iter; left time: 6108.8087s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0758628\n",
      "\tspeed: 0.1251s/iter; left time: 6537.7578s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0618466\n",
      "\tspeed: 0.1250s/iter; left time: 6517.0575s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0605258\n",
      "\tspeed: 0.1239s/iter; left time: 6450.2507s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0676748\n",
      "\tspeed: 0.1252s/iter; left time: 6502.8349s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0548497\n",
      "\tspeed: 0.1245s/iter; left time: 6453.6853s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0646279\n",
      "\tspeed: 0.1233s/iter; left time: 6379.6147s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0551258\n",
      "\tspeed: 0.1231s/iter; left time: 6357.6462s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0547290\n",
      "\tspeed: 0.1202s/iter; left time: 6193.7285s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0605357\n",
      "\tspeed: 0.1090s/iter; left time: 5608.3225s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0547643\n",
      "\tspeed: 0.1256s/iter; left time: 6447.3371s\n",
      "Epoch: 1 cost time: 00h:05m:31.08s\n",
      "Epoch: 1 | Train Loss: 0.0753777 Vali Loss: 0.0618922 Test Loss: 0.0666550\n",
      "Validation loss decreased (inf --> 0.061892).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0666260\n",
      "\tspeed: 1.1350s/iter; left time: 58048.2317s\n",
      "\titers: 200, epoch: 2 | loss: 0.0643385\n",
      "\tspeed: 0.1153s/iter; left time: 5883.9379s\n",
      "\titers: 300, epoch: 2 | loss: 0.0632871\n",
      "\tspeed: 0.1162s/iter; left time: 5920.8166s\n",
      "\titers: 400, epoch: 2 | loss: 0.0495252\n",
      "\tspeed: 0.1149s/iter; left time: 5841.7038s\n",
      "\titers: 500, epoch: 2 | loss: 0.0526561\n",
      "\tspeed: 0.1160s/iter; left time: 5884.0225s\n",
      "\titers: 600, epoch: 2 | loss: 0.0668836\n",
      "\tspeed: 0.1148s/iter; left time: 5812.2386s\n",
      "\titers: 700, epoch: 2 | loss: 0.0494954\n",
      "\tspeed: 0.1163s/iter; left time: 5877.8236s\n",
      "\titers: 800, epoch: 2 | loss: 0.0575383\n",
      "\tspeed: 0.1176s/iter; left time: 5930.1979s\n",
      "\titers: 900, epoch: 2 | loss: 0.0667061\n",
      "\tspeed: 0.1162s/iter; left time: 5851.7246s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0703753\n",
      "\tspeed: 0.1163s/iter; left time: 5842.2256s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0538771\n",
      "\tspeed: 0.1175s/iter; left time: 5891.4125s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0611253\n",
      "\tspeed: 0.1161s/iter; left time: 5811.7703s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0747418\n",
      "\tspeed: 0.1162s/iter; left time: 5801.6492s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0687371\n",
      "\tspeed: 0.1164s/iter; left time: 5799.8883s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0663108\n",
      "\tspeed: 0.1131s/iter; left time: 5625.1032s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0538801\n",
      "\tspeed: 0.1160s/iter; left time: 5757.7508s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0519770\n",
      "\tspeed: 0.1161s/iter; left time: 5750.7496s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0473880\n",
      "\tspeed: 0.1157s/iter; left time: 5720.8980s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0611360\n",
      "\tspeed: 0.1091s/iter; left time: 5382.2693s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0615671\n",
      "\tspeed: 0.1157s/iter; left time: 5697.1500s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0669719\n",
      "\tspeed: 0.1144s/iter; left time: 5622.4151s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0486104\n",
      "\tspeed: 0.1115s/iter; left time: 5466.5169s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0629132\n",
      "\tspeed: 0.0996s/iter; left time: 4874.3981s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0576007\n",
      "\tspeed: 0.1159s/iter; left time: 5658.8639s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0599707\n",
      "\tspeed: 0.1151s/iter; left time: 5612.5594s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0471631\n",
      "\tspeed: 0.1138s/iter; left time: 5536.1960s\n",
      "Epoch: 2 cost time: 00h:05m:09.65s\n",
      "Epoch: 2 | Train Loss: 0.0591769 Vali Loss: 0.0600467 Test Loss: 0.0646332\n",
      "Validation loss decreased (0.061892 --> 0.060047).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0599396\n",
      "\tspeed: 1.0017s/iter; left time: 48531.2763s\n",
      "\titers: 200, epoch: 3 | loss: 0.0531570\n",
      "\tspeed: 0.1156s/iter; left time: 5589.2050s\n",
      "\titers: 300, epoch: 3 | loss: 0.0663147\n",
      "\tspeed: 0.1158s/iter; left time: 5586.6661s\n",
      "\titers: 400, epoch: 3 | loss: 0.0573017\n",
      "\tspeed: 0.1159s/iter; left time: 5580.3935s\n",
      "\titers: 500, epoch: 3 | loss: 0.0458791\n",
      "\tspeed: 0.1154s/iter; left time: 5546.2894s\n",
      "\titers: 600, epoch: 3 | loss: 0.0676508\n",
      "\tspeed: 0.1149s/iter; left time: 5509.2006s\n",
      "\titers: 700, epoch: 3 | loss: 0.0643910\n",
      "\tspeed: 0.1139s/iter; left time: 5447.9562s\n",
      "\titers: 800, epoch: 3 | loss: 0.0421067\n",
      "\tspeed: 0.1138s/iter; left time: 5435.4103s\n",
      "\titers: 900, epoch: 3 | loss: 0.0515741\n",
      "\tspeed: 0.1167s/iter; left time: 5561.4390s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0688308\n",
      "\tspeed: 0.1143s/iter; left time: 5432.7623s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0621392\n",
      "\tspeed: 0.1157s/iter; left time: 5488.6243s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0543004\n",
      "\tspeed: 0.1139s/iter; left time: 5393.9227s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0568760\n",
      "\tspeed: 0.1123s/iter; left time: 5308.0440s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0596535\n",
      "\tspeed: 0.1156s/iter; left time: 5449.7068s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0517118\n",
      "\tspeed: 0.1175s/iter; left time: 5526.7793s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0598669\n",
      "\tspeed: 0.1141s/iter; left time: 5355.7305s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0843264\n",
      "\tspeed: 0.1161s/iter; left time: 5440.6865s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0665351\n",
      "\tspeed: 0.1159s/iter; left time: 5417.3726s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0814799\n",
      "\tspeed: 0.1170s/iter; left time: 5457.2950s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0534720\n",
      "\tspeed: 0.1156s/iter; left time: 5381.8339s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0550123\n",
      "\tspeed: 0.1146s/iter; left time: 5323.4283s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0583318\n",
      "\tspeed: 0.1139s/iter; left time: 5279.1013s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0620737\n",
      "\tspeed: 0.1148s/iter; left time: 5309.9709s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0487021\n",
      "\tspeed: 0.1148s/iter; left time: 5295.7191s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0631454\n",
      "\tspeed: 0.1149s/iter; left time: 5290.4083s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0609962\n",
      "\tspeed: 0.1161s/iter; left time: 5334.7029s\n",
      "Epoch: 3 cost time: 00h:05m:11.05s\n",
      "Epoch: 3 | Train Loss: 0.0568341 Vali Loss: 0.0582712 Test Loss: 0.0628686\n",
      "Validation loss decreased (0.060047 --> 0.058271).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0608920\n",
      "\tspeed: 0.9958s/iter; left time: 45557.7186s\n",
      "\titers: 200, epoch: 4 | loss: 0.0551032\n",
      "\tspeed: 0.1162s/iter; left time: 5303.4218s\n",
      "\titers: 300, epoch: 4 | loss: 0.0613622\n",
      "\tspeed: 0.1152s/iter; left time: 5247.1252s\n",
      "\titers: 400, epoch: 4 | loss: 0.0487876\n",
      "\tspeed: 0.1171s/iter; left time: 5320.0904s\n",
      "\titers: 500, epoch: 4 | loss: 0.0552078\n",
      "\tspeed: 0.1154s/iter; left time: 5235.0554s\n",
      "\titers: 600, epoch: 4 | loss: 0.0562445\n",
      "\tspeed: 0.1146s/iter; left time: 5183.8728s\n",
      "\titers: 700, epoch: 4 | loss: 0.0495130\n",
      "\tspeed: 0.1151s/iter; left time: 5198.9802s\n",
      "\titers: 800, epoch: 4 | loss: 0.0537369\n",
      "\tspeed: 0.1157s/iter; left time: 5212.9689s\n",
      "\titers: 900, epoch: 4 | loss: 0.0510692\n",
      "\tspeed: 0.1087s/iter; left time: 4886.4472s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0544674\n",
      "\tspeed: 0.1074s/iter; left time: 4815.6019s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0422624\n",
      "\tspeed: 0.1168s/iter; left time: 5226.3263s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0601855\n",
      "\tspeed: 0.1161s/iter; left time: 5184.8261s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0548600\n",
      "\tspeed: 0.1165s/iter; left time: 5188.9580s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0726929\n",
      "\tspeed: 0.1159s/iter; left time: 5149.5405s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0452244\n",
      "\tspeed: 0.1088s/iter; left time: 4826.8430s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0690438\n",
      "\tspeed: 0.0972s/iter; left time: 4302.4621s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0575143\n",
      "\tspeed: 0.1068s/iter; left time: 4713.2015s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0486670\n",
      "\tspeed: 0.1148s/iter; left time: 5057.0657s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0498352\n",
      "\tspeed: 0.1158s/iter; left time: 5091.6035s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0595263\n",
      "\tspeed: 0.1151s/iter; left time: 5046.4383s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0519087\n",
      "\tspeed: 0.1161s/iter; left time: 5077.3423s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0471623\n",
      "\tspeed: 0.1157s/iter; left time: 5050.6021s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0451208\n",
      "\tspeed: 0.1147s/iter; left time: 4993.9801s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0595563\n",
      "\tspeed: 0.1163s/iter; left time: 5053.4402s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0549772\n",
      "\tspeed: 0.1171s/iter; left time: 5077.2092s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0421879\n",
      "\tspeed: 0.1182s/iter; left time: 5111.2165s\n",
      "Epoch: 4 cost time: 00h:05m:07.48s\n",
      "Epoch: 4 | Train Loss: 0.0554718 Vali Loss: 0.0564575 Test Loss: 0.0613898\n",
      "Validation loss decreased (0.058271 --> 0.056457).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0640360\n",
      "\tspeed: 1.0044s/iter; left time: 43243.4201s\n",
      "\titers: 200, epoch: 5 | loss: 0.0555768\n",
      "\tspeed: 0.1147s/iter; left time: 4925.9657s\n",
      "\titers: 300, epoch: 5 | loss: 0.0411871\n",
      "\tspeed: 0.1165s/iter; left time: 4991.0211s\n",
      "\titers: 400, epoch: 5 | loss: 0.0514965\n",
      "\tspeed: 0.1150s/iter; left time: 4918.4055s\n",
      "\titers: 500, epoch: 5 | loss: 0.0501315\n",
      "\tspeed: 0.1163s/iter; left time: 4960.9477s\n",
      "\titers: 600, epoch: 5 | loss: 0.0461096\n",
      "\tspeed: 0.1147s/iter; left time: 4880.6477s\n",
      "\titers: 700, epoch: 5 | loss: 0.0579179\n",
      "\tspeed: 0.1136s/iter; left time: 4820.8346s\n",
      "\titers: 800, epoch: 5 | loss: 0.0609003\n",
      "\tspeed: 0.1150s/iter; left time: 4869.3633s\n",
      "\titers: 900, epoch: 5 | loss: 0.0671605\n",
      "\tspeed: 0.1151s/iter; left time: 4861.7235s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0632332\n",
      "\tspeed: 0.1173s/iter; left time: 4944.0046s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0637036\n",
      "\tspeed: 0.1190s/iter; left time: 5004.7178s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0558049\n",
      "\tspeed: 0.1176s/iter; left time: 4932.2707s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0510187\n",
      "\tspeed: 0.1163s/iter; left time: 4866.3413s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0609071\n",
      "\tspeed: 0.1148s/iter; left time: 4792.6565s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0545633\n",
      "\tspeed: 0.1153s/iter; left time: 4803.1647s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0504324\n",
      "\tspeed: 0.1138s/iter; left time: 4729.1776s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0481021\n",
      "\tspeed: 0.1145s/iter; left time: 4745.4862s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0610766\n",
      "\tspeed: 0.1147s/iter; left time: 4741.8071s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0627400\n",
      "\tspeed: 0.1153s/iter; left time: 4756.0239s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0500407\n",
      "\tspeed: 0.1143s/iter; left time: 4703.0922s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0583279\n",
      "\tspeed: 0.1172s/iter; left time: 4811.2763s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0541765\n",
      "\tspeed: 0.1158s/iter; left time: 4743.4718s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0596457\n",
      "\tspeed: 0.1148s/iter; left time: 4689.5396s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0590647\n",
      "\tspeed: 0.1144s/iter; left time: 4663.7620s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0656443\n",
      "\tspeed: 0.1169s/iter; left time: 4751.0732s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0455275\n",
      "\tspeed: 0.1183s/iter; left time: 4797.2323s\n",
      "Epoch: 5 cost time: 00h:05m:12.21s\n",
      "Epoch: 5 | Train Loss: 0.0542485 Vali Loss: 0.0564108 Test Loss: 0.0613056\n",
      "Validation loss decreased (0.056457 --> 0.056411).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0637889\n",
      "\tspeed: 0.9923s/iter; left time: 40045.1374s\n",
      "\titers: 200, epoch: 6 | loss: 0.0451849\n",
      "\tspeed: 0.1147s/iter; left time: 4618.4988s\n",
      "\titers: 300, epoch: 6 | loss: 0.0558336\n",
      "\tspeed: 0.1178s/iter; left time: 4731.3767s\n",
      "\titers: 400, epoch: 6 | loss: 0.0449031\n",
      "\tspeed: 0.1143s/iter; left time: 4579.4036s\n",
      "\titers: 500, epoch: 6 | loss: 0.0486762\n",
      "\tspeed: 0.1138s/iter; left time: 4548.7897s\n",
      "\titers: 600, epoch: 6 | loss: 0.0463912\n",
      "\tspeed: 0.1045s/iter; left time: 4163.3650s\n",
      "\titers: 700, epoch: 6 | loss: 0.0641680\n",
      "\tspeed: 0.1140s/iter; left time: 4531.3681s\n",
      "\titers: 800, epoch: 6 | loss: 0.0503087\n",
      "\tspeed: 0.1131s/iter; left time: 4486.3651s\n",
      "\titers: 900, epoch: 6 | loss: 0.0475201\n",
      "\tspeed: 0.1131s/iter; left time: 4472.9158s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0430374\n",
      "\tspeed: 0.1087s/iter; left time: 4289.6954s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0464682\n",
      "\tspeed: 0.1172s/iter; left time: 4612.2652s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0538542\n",
      "\tspeed: 0.1124s/iter; left time: 4413.5484s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0497831\n",
      "\tspeed: 0.1154s/iter; left time: 4516.9435s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0475077\n",
      "\tspeed: 0.1203s/iter; left time: 4696.5876s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0516047\n",
      "\tspeed: 0.1202s/iter; left time: 4684.0462s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0589632\n",
      "\tspeed: 0.1198s/iter; left time: 4654.5062s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0611102\n",
      "\tspeed: 0.1087s/iter; left time: 4214.3618s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0457706\n",
      "\tspeed: 0.1011s/iter; left time: 3907.6123s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0506376\n",
      "\tspeed: 0.1035s/iter; left time: 3989.7789s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0497848\n",
      "\tspeed: 0.1010s/iter; left time: 3884.6604s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0544278\n",
      "\tspeed: 0.1170s/iter; left time: 4486.9950s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0567052\n",
      "\tspeed: 0.1170s/iter; left time: 4475.3916s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0539384\n",
      "\tspeed: 0.1164s/iter; left time: 4440.4460s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0443573\n",
      "\tspeed: 0.1149s/iter; left time: 4372.0053s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0481149\n",
      "\tspeed: 0.1163s/iter; left time: 4415.3780s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0425841\n",
      "\tspeed: 0.1157s/iter; left time: 4379.4275s\n",
      "Epoch: 6 cost time: 00h:05m:06.57s\n",
      "Epoch: 6 | Train Loss: 0.0533678 Vali Loss: 0.0547401 Test Loss: 0.0591254\n",
      "Validation loss decreased (0.056411 --> 0.054740).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0502016\n",
      "\tspeed: 1.0296s/iter; left time: 38773.3304s\n",
      "\titers: 200, epoch: 7 | loss: 0.0561645\n",
      "\tspeed: 0.1150s/iter; left time: 4319.0582s\n",
      "\titers: 300, epoch: 7 | loss: 0.0592544\n",
      "\tspeed: 0.1166s/iter; left time: 4368.5747s\n",
      "\titers: 400, epoch: 7 | loss: 0.0523131\n",
      "\tspeed: 0.1152s/iter; left time: 4302.4687s\n",
      "\titers: 500, epoch: 7 | loss: 0.0502636\n",
      "\tspeed: 0.1170s/iter; left time: 4358.5172s\n",
      "\titers: 600, epoch: 7 | loss: 0.0607144\n",
      "\tspeed: 0.1150s/iter; left time: 4271.4519s\n",
      "\titers: 700, epoch: 7 | loss: 0.0567622\n",
      "\tspeed: 0.1152s/iter; left time: 4267.8602s\n",
      "\titers: 800, epoch: 7 | loss: 0.0611483\n",
      "\tspeed: 0.1139s/iter; left time: 4207.9981s\n",
      "\titers: 900, epoch: 7 | loss: 0.0641236\n",
      "\tspeed: 0.1155s/iter; left time: 4257.5038s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0467553\n",
      "\tspeed: 0.1152s/iter; left time: 4235.1825s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0538213\n",
      "\tspeed: 0.1148s/iter; left time: 4209.1981s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0498144\n",
      "\tspeed: 0.1148s/iter; left time: 4198.4598s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0496599\n",
      "\tspeed: 0.1161s/iter; left time: 4231.8114s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0542373\n",
      "\tspeed: 0.1173s/iter; left time: 4263.3834s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0536195\n",
      "\tspeed: 0.1177s/iter; left time: 4266.0222s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0534565\n",
      "\tspeed: 0.1186s/iter; left time: 4288.0050s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0545159\n",
      "\tspeed: 0.1170s/iter; left time: 4218.4950s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0453232\n",
      "\tspeed: 0.1156s/iter; left time: 4158.2451s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0651416\n",
      "\tspeed: 0.1162s/iter; left time: 4166.1893s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0515023\n",
      "\tspeed: 0.1163s/iter; left time: 4159.4492s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0462779\n",
      "\tspeed: 0.1159s/iter; left time: 4134.0760s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0499391\n",
      "\tspeed: 0.1149s/iter; left time: 4086.8695s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0656782\n",
      "\tspeed: 0.1120s/iter; left time: 3973.0698s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0466200\n",
      "\tspeed: 0.1168s/iter; left time: 4128.5093s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0558347\n",
      "\tspeed: 0.1064s/iter; left time: 3751.7386s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0696534\n",
      "\tspeed: 0.1163s/iter; left time: 4087.9350s\n",
      "Epoch: 7 cost time: 00h:05m:11.90s\n",
      "Epoch: 7 | Train Loss: 0.0528111 Vali Loss: 0.0540425 Test Loss: 0.0587007\n",
      "Validation loss decreased (0.054740 --> 0.054043).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0460588\n",
      "\tspeed: 0.9988s/iter; left time: 34918.3940s\n",
      "\titers: 200, epoch: 8 | loss: 0.0622025\n",
      "\tspeed: 0.1157s/iter; left time: 4032.4403s\n",
      "\titers: 300, epoch: 8 | loss: 0.0519913\n",
      "\tspeed: 0.1162s/iter; left time: 4038.4653s\n",
      "\titers: 400, epoch: 8 | loss: 0.0406249\n",
      "\tspeed: 0.1150s/iter; left time: 3987.5934s\n",
      "\titers: 500, epoch: 8 | loss: 0.0526539\n",
      "\tspeed: 0.1142s/iter; left time: 3945.7237s\n",
      "\titers: 600, epoch: 8 | loss: 0.0560093\n",
      "\tspeed: 0.1175s/iter; left time: 4048.8991s\n",
      "\titers: 700, epoch: 8 | loss: 0.0679693\n",
      "\tspeed: 0.1141s/iter; left time: 3922.2742s\n",
      "\titers: 800, epoch: 8 | loss: 0.0448523\n",
      "\tspeed: 0.1151s/iter; left time: 3944.3258s\n",
      "\titers: 900, epoch: 8 | loss: 0.0458734\n",
      "\tspeed: 0.1150s/iter; left time: 3927.7100s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0651497\n",
      "\tspeed: 0.1132s/iter; left time: 3854.4789s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0496319\n",
      "\tspeed: 0.1132s/iter; left time: 3843.3400s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0648924\n",
      "\tspeed: 0.1132s/iter; left time: 3832.8576s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0607049\n",
      "\tspeed: 0.1129s/iter; left time: 3811.3001s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0580690\n",
      "\tspeed: 0.1145s/iter; left time: 3852.7940s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0442169\n",
      "\tspeed: 0.1133s/iter; left time: 3801.9133s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0520920\n",
      "\tspeed: 0.1161s/iter; left time: 3884.0944s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0542979\n",
      "\tspeed: 0.1164s/iter; left time: 3883.1795s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0425588\n",
      "\tspeed: 0.1149s/iter; left time: 3820.4780s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0418803\n",
      "\tspeed: 0.1177s/iter; left time: 3902.8595s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0463494\n",
      "\tspeed: 0.1135s/iter; left time: 3752.5452s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0457020\n",
      "\tspeed: 0.1149s/iter; left time: 3788.0254s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0524447\n",
      "\tspeed: 0.1151s/iter; left time: 3783.4499s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0519675\n",
      "\tspeed: 0.1137s/iter; left time: 3726.6258s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0533954\n",
      "\tspeed: 0.1140s/iter; left time: 3723.8943s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0505847\n",
      "\tspeed: 0.1151s/iter; left time: 3746.7302s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0587134\n",
      "\tspeed: 0.1112s/iter; left time: 3611.1521s\n",
      "Epoch: 8 cost time: 00h:05m:09.82s\n",
      "Epoch: 8 | Train Loss: 0.0522896 Vali Loss: 0.0555167 Test Loss: 0.0603411\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0553223\n",
      "\tspeed: 0.9867s/iter; left time: 31835.2533s\n",
      "\titers: 200, epoch: 9 | loss: 0.0466191\n",
      "\tspeed: 0.1148s/iter; left time: 3691.0295s\n",
      "\titers: 300, epoch: 9 | loss: 0.0613746\n",
      "\tspeed: 0.1182s/iter; left time: 3791.6792s\n",
      "\titers: 400, epoch: 9 | loss: 0.0450902\n",
      "\tspeed: 0.1144s/iter; left time: 3656.6553s\n",
      "\titers: 500, epoch: 9 | loss: 0.0554611\n",
      "\tspeed: 0.1145s/iter; left time: 3649.9308s\n",
      "\titers: 600, epoch: 9 | loss: 0.0576737\n",
      "\tspeed: 0.1145s/iter; left time: 3638.1291s\n",
      "\titers: 700, epoch: 9 | loss: 0.0529940\n",
      "\tspeed: 0.1136s/iter; left time: 3598.6982s\n",
      "\titers: 800, epoch: 9 | loss: 0.0483810\n",
      "\tspeed: 0.1137s/iter; left time: 3590.0342s\n",
      "\titers: 900, epoch: 9 | loss: 0.0450235\n",
      "\tspeed: 0.1146s/iter; left time: 3607.4244s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0433518\n",
      "\tspeed: 0.1136s/iter; left time: 3561.6420s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0480308\n",
      "\tspeed: 0.1146s/iter; left time: 3583.4718s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0439340\n",
      "\tspeed: 0.1164s/iter; left time: 3626.4656s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0469765\n",
      "\tspeed: 0.1178s/iter; left time: 3660.5851s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0589630\n",
      "\tspeed: 0.1170s/iter; left time: 3622.9203s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0431547\n",
      "\tspeed: 0.1166s/iter; left time: 3599.4052s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0477735\n",
      "\tspeed: 0.1174s/iter; left time: 3610.7391s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0572250\n",
      "\tspeed: 0.1168s/iter; left time: 3581.7098s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0460013\n",
      "\tspeed: 0.1160s/iter; left time: 3546.0751s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0480974\n",
      "\tspeed: 0.1157s/iter; left time: 3523.3599s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0449856\n",
      "\tspeed: 0.1150s/iter; left time: 3492.2872s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0463317\n",
      "\tspeed: 0.1146s/iter; left time: 3468.2989s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0446014\n",
      "\tspeed: 0.1154s/iter; left time: 3481.5565s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0504780\n",
      "\tspeed: 0.1150s/iter; left time: 3457.8265s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0436172\n",
      "\tspeed: 0.1044s/iter; left time: 3129.5063s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0481363\n",
      "\tspeed: 0.1109s/iter; left time: 3312.7002s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0430334\n",
      "\tspeed: 0.1163s/iter; left time: 3462.6061s\n",
      "Epoch: 9 cost time: 00h:05m:10.44s\n",
      "Epoch: 9 | Train Loss: 0.0518848 Vali Loss: 0.0543241 Test Loss: 0.0591126\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0361740\n",
      "\tspeed: 0.9902s/iter; left time: 29278.6451s\n",
      "\titers: 200, epoch: 10 | loss: 0.0424532\n",
      "\tspeed: 0.1171s/iter; left time: 3451.9981s\n",
      "\titers: 300, epoch: 10 | loss: 0.0439460\n",
      "\tspeed: 0.1168s/iter; left time: 3430.5602s\n",
      "\titers: 400, epoch: 10 | loss: 0.0425408\n",
      "\tspeed: 0.1149s/iter; left time: 3363.0818s\n",
      "\titers: 500, epoch: 10 | loss: 0.0446251\n",
      "\tspeed: 0.1083s/iter; left time: 3159.9032s\n",
      "\titers: 600, epoch: 10 | loss: 0.0730994\n",
      "\tspeed: 0.0978s/iter; left time: 2842.7083s\n",
      "\titers: 700, epoch: 10 | loss: 0.0452029\n",
      "\tspeed: 0.1071s/iter; left time: 3102.6407s\n",
      "\titers: 800, epoch: 10 | loss: 0.0518145\n",
      "\tspeed: 0.1168s/iter; left time: 3371.1007s\n",
      "\titers: 900, epoch: 10 | loss: 0.0441767\n",
      "\tspeed: 0.1157s/iter; left time: 3329.4667s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0625665\n",
      "\tspeed: 0.1147s/iter; left time: 3287.9282s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0505257\n",
      "\tspeed: 0.1168s/iter; left time: 3335.7106s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0455996\n",
      "\tspeed: 0.1160s/iter; left time: 3301.2877s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0378268\n",
      "\tspeed: 0.1171s/iter; left time: 3322.3388s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0600771\n",
      "\tspeed: 0.1090s/iter; left time: 3081.8802s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0465586\n",
      "\tspeed: 0.1049s/iter; left time: 2955.8424s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0524688\n",
      "\tspeed: 0.1152s/iter; left time: 3233.3545s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0475789\n",
      "\tspeed: 0.1150s/iter; left time: 3217.2197s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0450688\n",
      "\tspeed: 0.1152s/iter; left time: 3209.4761s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0655913\n",
      "\tspeed: 0.1140s/iter; left time: 3166.8476s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0443577\n",
      "\tspeed: 0.1162s/iter; left time: 3215.0263s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0562413\n",
      "\tspeed: 0.1150s/iter; left time: 3170.1847s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0390418\n",
      "\tspeed: 0.1138s/iter; left time: 3126.7674s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0545616\n",
      "\tspeed: 0.1150s/iter; left time: 3147.1205s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0571885\n",
      "\tspeed: 0.1142s/iter; left time: 3113.9495s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0536950\n",
      "\tspeed: 0.1161s/iter; left time: 3155.2774s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0660274\n",
      "\tspeed: 0.1142s/iter; left time: 3090.6736s\n",
      "Epoch: 10 cost time: 00h:05m:06.98s\n",
      "Epoch: 10 | Train Loss: 0.0514509 Vali Loss: 0.0540158 Test Loss: 0.0590907\n",
      "Validation loss decreased (0.054043 --> 0.054016).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0486004\n",
      "\tspeed: 1.0021s/iter; left time: 26928.6929s\n",
      "\titers: 200, epoch: 11 | loss: 0.0427696\n",
      "\tspeed: 0.1153s/iter; left time: 3085.9052s\n",
      "\titers: 300, epoch: 11 | loss: 0.0502599\n",
      "\tspeed: 0.1149s/iter; left time: 3065.5980s\n",
      "\titers: 400, epoch: 11 | loss: 0.0545642\n",
      "\tspeed: 0.1134s/iter; left time: 3012.5267s\n",
      "\titers: 500, epoch: 11 | loss: 0.0415499\n",
      "\tspeed: 0.1138s/iter; left time: 3011.8174s\n",
      "\titers: 600, epoch: 11 | loss: 0.0503596\n",
      "\tspeed: 0.1146s/iter; left time: 3023.1896s\n",
      "\titers: 700, epoch: 11 | loss: 0.0587240\n",
      "\tspeed: 0.1135s/iter; left time: 2982.7512s\n",
      "\titers: 800, epoch: 11 | loss: 0.0542230\n",
      "\tspeed: 0.1123s/iter; left time: 2938.1676s\n",
      "\titers: 900, epoch: 11 | loss: 0.0419671\n",
      "\tspeed: 0.1148s/iter; left time: 2992.4918s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0539247\n",
      "\tspeed: 0.1124s/iter; left time: 2920.3983s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0388036\n",
      "\tspeed: 0.1146s/iter; left time: 2964.1080s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0615964\n",
      "\tspeed: 0.1143s/iter; left time: 2944.4680s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0468018\n",
      "\tspeed: 0.1168s/iter; left time: 2997.5277s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0443031\n",
      "\tspeed: 0.1158s/iter; left time: 2960.0505s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0490965\n",
      "\tspeed: 0.1151s/iter; left time: 2931.7566s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0545864\n",
      "\tspeed: 0.1154s/iter; left time: 2927.6585s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0517874\n",
      "\tspeed: 0.1152s/iter; left time: 2910.0282s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0556483\n",
      "\tspeed: 0.1133s/iter; left time: 2852.6975s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0528931\n",
      "\tspeed: 0.1132s/iter; left time: 2836.9183s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0465289\n",
      "\tspeed: 0.1157s/iter; left time: 2890.0792s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0451324\n",
      "\tspeed: 0.1154s/iter; left time: 2869.9303s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0447210\n",
      "\tspeed: 0.1166s/iter; left time: 2887.1053s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0517201\n",
      "\tspeed: 0.1175s/iter; left time: 2899.2541s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0628082\n",
      "\tspeed: 0.1169s/iter; left time: 2871.3439s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0555514\n",
      "\tspeed: 0.1130s/iter; left time: 2765.3817s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0564730\n",
      "\tspeed: 0.1166s/iter; left time: 2842.0555s\n",
      "Epoch: 11 cost time: 00h:05m:10.13s\n",
      "Epoch: 11 | Train Loss: 0.0511123 Vali Loss: 0.0542110 Test Loss: 0.0589370\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0413404\n",
      "\tspeed: 0.9823s/iter; left time: 23745.4161s\n",
      "\titers: 200, epoch: 12 | loss: 0.0456423\n",
      "\tspeed: 0.1154s/iter; left time: 2779.3363s\n",
      "\titers: 300, epoch: 12 | loss: 0.0599975\n",
      "\tspeed: 0.1153s/iter; left time: 2763.6161s\n",
      "\titers: 400, epoch: 12 | loss: 0.0393989\n",
      "\tspeed: 0.1138s/iter; left time: 2715.9943s\n",
      "\titers: 500, epoch: 12 | loss: 0.0631208\n",
      "\tspeed: 0.1157s/iter; left time: 2751.6474s\n",
      "\titers: 600, epoch: 12 | loss: 0.0394312\n",
      "\tspeed: 0.1140s/iter; left time: 2698.6083s\n",
      "\titers: 700, epoch: 12 | loss: 0.0610043\n",
      "\tspeed: 0.1135s/iter; left time: 2675.2827s\n",
      "\titers: 800, epoch: 12 | loss: 0.0467539\n",
      "\tspeed: 0.1157s/iter; left time: 2715.7502s\n",
      "\titers: 900, epoch: 12 | loss: 0.0553005\n",
      "\tspeed: 0.1154s/iter; left time: 2697.5072s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0448402\n",
      "\tspeed: 0.1136s/iter; left time: 2644.6336s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0574175\n",
      "\tspeed: 0.1139s/iter; left time: 2638.3887s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0437614\n",
      "\tspeed: 0.1162s/iter; left time: 2680.1898s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0455631\n",
      "\tspeed: 0.1155s/iter; left time: 2653.7600s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0532934\n",
      "\tspeed: 0.1164s/iter; left time: 2663.1238s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0407963\n",
      "\tspeed: 0.1150s/iter; left time: 2618.6813s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0568854\n",
      "\tspeed: 0.1145s/iter; left time: 2596.8789s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0648938\n",
      "\tspeed: 0.1142s/iter; left time: 2577.4574s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0681437\n",
      "\tspeed: 0.1140s/iter; left time: 2562.1909s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0537705\n",
      "\tspeed: 0.1157s/iter; left time: 2588.1640s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0641221\n",
      "\tspeed: 0.1151s/iter; left time: 2564.6658s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0408601\n",
      "\tspeed: 0.1162s/iter; left time: 2576.4156s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0592678\n",
      "\tspeed: 0.1166s/iter; left time: 2573.9433s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0429511\n",
      "\tspeed: 0.1151s/iter; left time: 2528.9800s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0535271\n",
      "\tspeed: 0.1165s/iter; left time: 2548.6800s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0569462\n",
      "\tspeed: 0.1157s/iter; left time: 2518.9259s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0574420\n",
      "\tspeed: 0.1157s/iter; left time: 2508.4919s\n",
      "Epoch: 12 cost time: 00h:05m:11.04s\n",
      "Epoch: 12 | Train Loss: 0.0506596 Vali Loss: 0.0538890 Test Loss: 0.0585830\n",
      "Validation loss decreased (0.054016 --> 0.053889).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0584482\n",
      "\tspeed: 1.0053s/iter; left time: 21590.2078s\n",
      "\titers: 200, epoch: 13 | loss: 0.0413951\n",
      "\tspeed: 0.1161s/iter; left time: 2480.8565s\n",
      "\titers: 300, epoch: 13 | loss: 0.0452191\n",
      "\tspeed: 0.1165s/iter; left time: 2478.1191s\n",
      "\titers: 400, epoch: 13 | loss: 0.0583766\n",
      "\tspeed: 0.1098s/iter; left time: 2324.9653s\n",
      "\titers: 500, epoch: 13 | loss: 0.0591615\n",
      "\tspeed: 0.1100s/iter; left time: 2318.2569s\n",
      "\titers: 600, epoch: 13 | loss: 0.0405239\n",
      "\tspeed: 0.1142s/iter; left time: 2395.0054s\n",
      "\titers: 700, epoch: 13 | loss: 0.0362805\n",
      "\tspeed: 0.1175s/iter; left time: 2452.6224s\n",
      "\titers: 800, epoch: 13 | loss: 0.0517154\n",
      "\tspeed: 0.1177s/iter; left time: 2444.6048s\n",
      "\titers: 900, epoch: 13 | loss: 0.0501910\n",
      "\tspeed: 0.1155s/iter; left time: 2387.9347s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0493395\n",
      "\tspeed: 0.1146s/iter; left time: 2357.9690s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0423344\n",
      "\tspeed: 0.1135s/iter; left time: 2324.5108s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0497453\n",
      "\tspeed: 0.1068s/iter; left time: 2177.0310s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0577142\n",
      "\tspeed: 0.1165s/iter; left time: 2361.4740s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0529139\n",
      "\tspeed: 0.1156s/iter; left time: 2332.8182s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0540026\n",
      "\tspeed: 0.1148s/iter; left time: 2304.2520s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0617525\n",
      "\tspeed: 0.1158s/iter; left time: 2312.6715s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0441613\n",
      "\tspeed: 0.1173s/iter; left time: 2332.2387s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0492811\n",
      "\tspeed: 0.1146s/iter; left time: 2265.5691s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0474770\n",
      "\tspeed: 0.1157s/iter; left time: 2276.9391s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0442020\n",
      "\tspeed: 0.1147s/iter; left time: 2245.0456s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0425807\n",
      "\tspeed: 0.1154s/iter; left time: 2248.2726s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0565319\n",
      "\tspeed: 0.1149s/iter; left time: 2226.1965s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0483340\n",
      "\tspeed: 0.1142s/iter; left time: 2201.1308s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0448032\n",
      "\tspeed: 0.1157s/iter; left time: 2219.2562s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0564242\n",
      "\tspeed: 0.1155s/iter; left time: 2204.1146s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0571262\n",
      "\tspeed: 0.1146s/iter; left time: 2174.3821s\n",
      "Epoch: 13 cost time: 00h:05m:10.22s\n",
      "Epoch: 13 | Train Loss: 0.0503306 Vali Loss: 0.0541599 Test Loss: 0.0587250\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0493650\n",
      "\tspeed: 0.9792s/iter; left time: 18389.0411s\n",
      "\titers: 200, epoch: 14 | loss: 0.0601776\n",
      "\tspeed: 0.1034s/iter; left time: 1930.8380s\n",
      "\titers: 300, epoch: 14 | loss: 0.0522846\n",
      "\tspeed: 0.1021s/iter; left time: 1896.8407s\n",
      "\titers: 400, epoch: 14 | loss: 0.0502198\n",
      "\tspeed: 0.1145s/iter; left time: 2115.8220s\n",
      "\titers: 500, epoch: 14 | loss: 0.0539697\n",
      "\tspeed: 0.1164s/iter; left time: 2140.3343s\n",
      "\titers: 600, epoch: 14 | loss: 0.0492951\n",
      "\tspeed: 0.1176s/iter; left time: 2150.5131s\n",
      "\titers: 700, epoch: 14 | loss: 0.0499482\n",
      "\tspeed: 0.1150s/iter; left time: 2090.1754s\n",
      "\titers: 800, epoch: 14 | loss: 0.0521548\n",
      "\tspeed: 0.1161s/iter; left time: 2099.8732s\n",
      "\titers: 900, epoch: 14 | loss: 0.0697306\n",
      "\tspeed: 0.1148s/iter; left time: 2063.9074s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0585489\n",
      "\tspeed: 0.1154s/iter; left time: 2063.1400s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0513945\n",
      "\tspeed: 0.1099s/iter; left time: 1953.2543s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0772938\n",
      "\tspeed: 0.0962s/iter; left time: 1700.8666s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0545680\n",
      "\tspeed: 0.1087s/iter; left time: 1911.1805s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0644709\n",
      "\tspeed: 0.1148s/iter; left time: 2006.5881s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0620324\n",
      "\tspeed: 0.1151s/iter; left time: 2000.9906s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0397591\n",
      "\tspeed: 0.1161s/iter; left time: 2007.0436s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0408592\n",
      "\tspeed: 0.1163s/iter; left time: 1998.8850s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0460187\n",
      "\tspeed: 0.1155s/iter; left time: 1973.5367s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0569062\n",
      "\tspeed: 0.1158s/iter; left time: 1966.1250s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0487930\n",
      "\tspeed: 0.1182s/iter; left time: 1995.7385s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0453887\n",
      "\tspeed: 0.1157s/iter; left time: 1942.2739s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0463870\n",
      "\tspeed: 0.1146s/iter; left time: 1910.9998s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0511975\n",
      "\tspeed: 0.1136s/iter; left time: 1882.9192s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0511051\n",
      "\tspeed: 0.1133s/iter; left time: 1867.1957s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0467189\n",
      "\tspeed: 0.1142s/iter; left time: 1871.1775s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0436856\n",
      "\tspeed: 0.1135s/iter; left time: 1847.0458s\n",
      "Epoch: 14 cost time: 00h:05m:04.65s\n",
      "Epoch: 14 | Train Loss: 0.0499867 Vali Loss: 0.0545931 Test Loss: 0.0590969\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0640254\n",
      "\tspeed: 0.9585s/iter; left time: 15414.8101s\n",
      "\titers: 200, epoch: 15 | loss: 0.0394662\n",
      "\tspeed: 0.1147s/iter; left time: 1832.6257s\n",
      "\titers: 300, epoch: 15 | loss: 0.0387993\n",
      "\tspeed: 0.1114s/iter; left time: 1768.7440s\n",
      "\titers: 400, epoch: 15 | loss: 0.0437090\n",
      "\tspeed: 0.1139s/iter; left time: 1798.2244s\n",
      "\titers: 500, epoch: 15 | loss: 0.0550465\n",
      "\tspeed: 0.1150s/iter; left time: 1803.8693s\n",
      "\titers: 600, epoch: 15 | loss: 0.0468511\n",
      "\tspeed: 0.1146s/iter; left time: 1786.1690s\n",
      "\titers: 700, epoch: 15 | loss: 0.0450512\n",
      "\tspeed: 0.1049s/iter; left time: 1624.8732s\n",
      "\titers: 800, epoch: 15 | loss: 0.0656973\n",
      "\tspeed: 0.1028s/iter; left time: 1580.9335s\n",
      "\titers: 900, epoch: 15 | loss: 0.0515748\n",
      "\tspeed: 0.1146s/iter; left time: 1751.6202s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0481743\n",
      "\tspeed: 0.1146s/iter; left time: 1740.0851s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0463150\n",
      "\tspeed: 0.1153s/iter; left time: 1738.8196s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0436716\n",
      "\tspeed: 0.1163s/iter; left time: 1742.8213s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0492802\n",
      "\tspeed: 0.1154s/iter; left time: 1717.0990s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0472708\n",
      "\tspeed: 0.1164s/iter; left time: 1721.1618s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0623606\n",
      "\tspeed: 0.1149s/iter; left time: 1686.7182s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0426730\n",
      "\tspeed: 0.1020s/iter; left time: 1487.0946s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0558270\n",
      "\tspeed: 0.1052s/iter; left time: 1523.4790s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0486433\n",
      "\tspeed: 0.1157s/iter; left time: 1664.4138s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0478737\n",
      "\tspeed: 0.1145s/iter; left time: 1634.9584s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0472583\n",
      "\tspeed: 0.1154s/iter; left time: 1636.2746s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0523396\n",
      "\tspeed: 0.1153s/iter; left time: 1624.4740s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0442008\n",
      "\tspeed: 0.1147s/iter; left time: 1604.0077s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0422843\n",
      "\tspeed: 0.1151s/iter; left time: 1598.2178s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0520833\n",
      "\tspeed: 0.1162s/iter; left time: 1601.3281s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0416124\n",
      "\tspeed: 0.1169s/iter; left time: 1599.4313s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0408221\n",
      "\tspeed: 0.1160s/iter; left time: 1575.6306s\n",
      "Epoch: 15 cost time: 00h:05m:05.30s\n",
      "Epoch: 15 | Train Loss: 0.0497556 Vali Loss: 0.0541359 Test Loss: 0.0592553\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0512954\n",
      "\tspeed: 0.9859s/iter; left time: 13197.7336s\n",
      "\titers: 200, epoch: 16 | loss: 0.0593351\n",
      "\tspeed: 0.1170s/iter; left time: 1555.0710s\n",
      "\titers: 300, epoch: 16 | loss: 0.0391670\n",
      "\tspeed: 0.1167s/iter; left time: 1538.5807s\n",
      "\titers: 400, epoch: 16 | loss: 0.0442540\n",
      "\tspeed: 0.1148s/iter; left time: 1502.5334s\n",
      "\titers: 500, epoch: 16 | loss: 0.0314678\n",
      "\tspeed: 0.1151s/iter; left time: 1494.0879s\n",
      "\titers: 600, epoch: 16 | loss: 0.0491887\n",
      "\tspeed: 0.1163s/iter; left time: 1498.0406s\n",
      "\titers: 700, epoch: 16 | loss: 0.0406393\n",
      "\tspeed: 0.1175s/iter; left time: 1502.7032s\n",
      "\titers: 800, epoch: 16 | loss: 0.0444243\n",
      "\tspeed: 0.1161s/iter; left time: 1473.1850s\n",
      "\titers: 900, epoch: 16 | loss: 0.0567451\n",
      "\tspeed: 0.1157s/iter; left time: 1456.0354s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0386371\n",
      "\tspeed: 0.1164s/iter; left time: 1452.9967s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0542442\n",
      "\tspeed: 0.1157s/iter; left time: 1433.5309s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0485616\n",
      "\tspeed: 0.1151s/iter; left time: 1413.9901s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0415913\n",
      "\tspeed: 0.1157s/iter; left time: 1409.7789s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0443841\n",
      "\tspeed: 0.1154s/iter; left time: 1395.2161s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0549853\n",
      "\tspeed: 0.1153s/iter; left time: 1381.9400s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0475429\n",
      "\tspeed: 0.1159s/iter; left time: 1377.5505s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0519379\n",
      "\tspeed: 0.1164s/iter; left time: 1372.1601s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0507676\n",
      "\tspeed: 0.1171s/iter; left time: 1368.3624s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0471109\n",
      "\tspeed: 0.1154s/iter; left time: 1337.5959s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0422025\n",
      "\tspeed: 0.1085s/iter; left time: 1245.6917s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0510330\n",
      "\tspeed: 0.1127s/iter; left time: 1283.7618s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0449592\n",
      "\tspeed: 0.1148s/iter; left time: 1296.1556s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0611455\n",
      "\tspeed: 0.1136s/iter; left time: 1271.2093s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0596026\n",
      "\tspeed: 0.1161s/iter; left time: 1286.6216s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0501522\n",
      "\tspeed: 0.1142s/iter; left time: 1254.3166s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0497214\n",
      "\tspeed: 0.1132s/iter; left time: 1231.9641s\n",
      "Epoch: 16 cost time: 00h:05m:11.56s\n",
      "Epoch: 16 | Train Loss: 0.0493574 Vali Loss: 0.0541783 Test Loss: 0.0598210\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0487838\n",
      "\tspeed: 0.9838s/iter; left time: 10515.7423s\n",
      "\titers: 200, epoch: 17 | loss: 0.0512044\n",
      "\tspeed: 0.1157s/iter; left time: 1225.1472s\n",
      "\titers: 300, epoch: 17 | loss: 0.0408817\n",
      "\tspeed: 0.1164s/iter; left time: 1220.6455s\n",
      "\titers: 400, epoch: 17 | loss: 0.0356466\n",
      "\tspeed: 0.1136s/iter; left time: 1180.0588s\n",
      "\titers: 500, epoch: 17 | loss: 0.0510470\n",
      "\tspeed: 0.1157s/iter; left time: 1190.0262s\n",
      "\titers: 600, epoch: 17 | loss: 0.0428915\n",
      "\tspeed: 0.1165s/iter; left time: 1186.6402s\n",
      "\titers: 700, epoch: 17 | loss: 0.0436458\n",
      "\tspeed: 0.1149s/iter; left time: 1159.5412s\n",
      "\titers: 800, epoch: 17 | loss: 0.0422825\n",
      "\tspeed: 0.1146s/iter; left time: 1144.9722s\n",
      "\titers: 900, epoch: 17 | loss: 0.0605514\n",
      "\tspeed: 0.1144s/iter; left time: 1131.1799s\n",
      "\titers: 1000, epoch: 17 | loss: 0.0491973\n",
      "\tspeed: 0.1141s/iter; left time: 1117.0568s\n",
      "\titers: 1100, epoch: 17 | loss: 0.0428665\n",
      "\tspeed: 0.1137s/iter; left time: 1102.1115s\n",
      "\titers: 1200, epoch: 17 | loss: 0.0562080\n",
      "\tspeed: 0.1141s/iter; left time: 1093.6470s\n",
      "\titers: 1300, epoch: 17 | loss: 0.0395262\n",
      "\tspeed: 0.1141s/iter; left time: 1082.2386s\n",
      "\titers: 1400, epoch: 17 | loss: 0.0628368\n",
      "\tspeed: 0.1163s/iter; left time: 1091.7784s\n",
      "\titers: 1500, epoch: 17 | loss: 0.0415979\n",
      "\tspeed: 0.1177s/iter; left time: 1093.3501s\n",
      "\titers: 1600, epoch: 17 | loss: 0.0425477\n",
      "\tspeed: 0.1169s/iter; left time: 1073.9668s\n",
      "\titers: 1700, epoch: 17 | loss: 0.0511805\n",
      "\tspeed: 0.1170s/iter; left time: 1062.9802s\n",
      "\titers: 1800, epoch: 17 | loss: 0.0501039\n",
      "\tspeed: 0.1162s/iter; left time: 1044.8256s\n",
      "\titers: 1900, epoch: 17 | loss: 0.0420724\n",
      "\tspeed: 0.1165s/iter; left time: 1035.7642s\n",
      "\titers: 2000, epoch: 17 | loss: 0.0464478\n",
      "\tspeed: 0.1044s/iter; left time: 917.5155s\n",
      "\titers: 2100, epoch: 17 | loss: 0.0633934\n",
      "\tspeed: 0.1143s/iter; left time: 993.4731s\n",
      "\titers: 2200, epoch: 17 | loss: 0.0512717\n",
      "\tspeed: 0.1152s/iter; left time: 989.2179s\n",
      "\titers: 2300, epoch: 17 | loss: 0.0531858\n",
      "\tspeed: 0.1137s/iter; left time: 965.5545s\n",
      "\titers: 2400, epoch: 17 | loss: 0.0680438\n",
      "\tspeed: 0.1137s/iter; left time: 953.4365s\n",
      "\titers: 2500, epoch: 17 | loss: 0.0617753\n",
      "\tspeed: 0.1152s/iter; left time: 955.2259s\n",
      "\titers: 2600, epoch: 17 | loss: 0.0388164\n",
      "\tspeed: 0.1147s/iter; left time: 939.5783s\n",
      "Epoch: 17 cost time: 00h:05m:10.41s\n",
      "Epoch: 17 | Train Loss: 0.0491620 Vali Loss: 0.0546305 Test Loss: 0.0602432\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010352413170039654, rmse:0.1017468124628067, mae:0.058582987636327744, rse:0.392534464597702\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 24: 01h:50m:52.56s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 19:03:17,061] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 19:03:18,263] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 19:03:18,263] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 19:03:18,263] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 19:03:18,367] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 19:03:18,367] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 19:03:19,052] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 19:03:19,054] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 19:03:19,054] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 19:03:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 19:03:19,056] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 19:03:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 19:03:19,385] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 19:03:19,386] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,386] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,503] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 19:03:19,504] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,504] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,504] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 19:03:19,616] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 19:03:19,617] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 19:03:19,619] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 19:03:19,619] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdb688760d0>\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1399898\n",
      "\tspeed: 0.1710s/iter; left time: 9185.5004s\n",
      "\titers: 200, epoch: 1 | loss: 0.1168020\n",
      "\tspeed: 0.1273s/iter; left time: 6823.3374s\n",
      "\titers: 300, epoch: 1 | loss: 0.1196281\n",
      "\tspeed: 0.1278s/iter; left time: 6837.3337s\n",
      "\titers: 400, epoch: 1 | loss: 0.1010908\n",
      "\tspeed: 0.1129s/iter; left time: 6032.9864s\n",
      "\titers: 500, epoch: 1 | loss: 0.0785264\n",
      "\tspeed: 0.1178s/iter; left time: 6282.5744s\n",
      "\titers: 600, epoch: 1 | loss: 0.0839203\n",
      "\tspeed: 0.1142s/iter; left time: 6079.7280s\n",
      "\titers: 700, epoch: 1 | loss: 0.0833786\n",
      "\tspeed: 0.1114s/iter; left time: 5918.1044s\n",
      "\titers: 800, epoch: 1 | loss: 0.0887009\n",
      "\tspeed: 0.1250s/iter; left time: 6629.4213s\n",
      "\titers: 900, epoch: 1 | loss: 0.0852673\n",
      "\tspeed: 0.1264s/iter; left time: 6689.9738s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0693184\n",
      "\tspeed: 0.1111s/iter; left time: 5867.2965s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0696165\n",
      "\tspeed: 0.1177s/iter; left time: 6203.3647s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0693079\n",
      "\tspeed: 0.1249s/iter; left time: 6572.6196s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0922560\n",
      "\tspeed: 0.1254s/iter; left time: 6584.3957s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0846009\n",
      "\tspeed: 0.1255s/iter; left time: 6580.5273s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0679483\n",
      "\tspeed: 0.1245s/iter; left time: 6513.0298s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0670433\n",
      "\tspeed: 0.1239s/iter; left time: 6472.0220s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0842042\n",
      "\tspeed: 0.1255s/iter; left time: 6543.4722s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0703235\n",
      "\tspeed: 0.1258s/iter; left time: 6544.0364s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0694279\n",
      "\tspeed: 0.1259s/iter; left time: 6537.2713s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0827992\n",
      "\tspeed: 0.1147s/iter; left time: 5942.9665s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0705764\n",
      "\tspeed: 0.1218s/iter; left time: 6298.3915s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0862902\n",
      "\tspeed: 0.1244s/iter; left time: 6421.4284s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0764715\n",
      "\tspeed: 0.1241s/iter; left time: 6396.1412s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0649146\n",
      "\tspeed: 0.1239s/iter; left time: 6369.6821s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0675585\n",
      "\tspeed: 0.1242s/iter; left time: 6372.0066s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0922287\n",
      "\tspeed: 0.1273s/iter; left time: 6522.9023s\n",
      "Epoch: 1 cost time: 00h:05m:31.02s\n",
      "Epoch: 1 | Train Loss: 0.0871358 Vali Loss: 0.0761358 Test Loss: 0.0846921\n",
      "Validation loss decreased (inf --> 0.076136).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0748160\n",
      "\tspeed: 1.1148s/iter; left time: 56889.9772s\n",
      "\titers: 200, epoch: 2 | loss: 0.0666597\n",
      "\tspeed: 0.1153s/iter; left time: 5870.0649s\n",
      "\titers: 300, epoch: 2 | loss: 0.0733586\n",
      "\tspeed: 0.1155s/iter; left time: 5871.3775s\n",
      "\titers: 400, epoch: 2 | loss: 0.0713845\n",
      "\tspeed: 0.1182s/iter; left time: 5998.5678s\n",
      "\titers: 500, epoch: 2 | loss: 0.0738021\n",
      "\tspeed: 0.1163s/iter; left time: 5887.6479s\n",
      "\titers: 600, epoch: 2 | loss: 0.0705914\n",
      "\tspeed: 0.1166s/iter; left time: 5889.3509s\n",
      "\titers: 700, epoch: 2 | loss: 0.0696770\n",
      "\tspeed: 0.1146s/iter; left time: 5778.8295s\n",
      "\titers: 800, epoch: 2 | loss: 0.0733058\n",
      "\tspeed: 0.1152s/iter; left time: 5800.3344s\n",
      "\titers: 900, epoch: 2 | loss: 0.0828630\n",
      "\tspeed: 0.1149s/iter; left time: 5771.4858s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0755392\n",
      "\tspeed: 0.1149s/iter; left time: 5757.6629s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0760631\n",
      "\tspeed: 0.1146s/iter; left time: 5731.0691s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0672225\n",
      "\tspeed: 0.1143s/iter; left time: 5705.2586s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0847869\n",
      "\tspeed: 0.1149s/iter; left time: 5724.5213s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0584152\n",
      "\tspeed: 0.1133s/iter; left time: 5636.7114s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0725951\n",
      "\tspeed: 0.1136s/iter; left time: 5637.1066s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0695227\n",
      "\tspeed: 0.1145s/iter; left time: 5672.3644s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0701093\n",
      "\tspeed: 0.1135s/iter; left time: 5608.9226s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0907094\n",
      "\tspeed: 0.1151s/iter; left time: 5679.9633s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0744436\n",
      "\tspeed: 0.1156s/iter; left time: 5688.6276s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0658760\n",
      "\tspeed: 0.1141s/iter; left time: 5606.2591s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0895933\n",
      "\tspeed: 0.1143s/iter; left time: 5604.1308s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0795399\n",
      "\tspeed: 0.1144s/iter; left time: 5595.9551s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0616416\n",
      "\tspeed: 0.1144s/iter; left time: 5588.0029s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0696839\n",
      "\tspeed: 0.1149s/iter; left time: 5601.2856s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0602486\n",
      "\tspeed: 0.1139s/iter; left time: 5538.9357s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0719646\n",
      "\tspeed: 0.1147s/iter; left time: 5566.5009s\n",
      "Epoch: 2 cost time: 00h:05m:09.51s\n",
      "Epoch: 2 | Train Loss: 0.0721934 Vali Loss: 0.0745545 Test Loss: 0.0839525\n",
      "Validation loss decreased (0.076136 --> 0.074555).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0690167\n",
      "\tspeed: 0.9829s/iter; left time: 47513.1729s\n",
      "\titers: 200, epoch: 3 | loss: 0.0873170\n",
      "\tspeed: 0.1153s/iter; left time: 5560.8989s\n",
      "\titers: 300, epoch: 3 | loss: 0.0652578\n",
      "\tspeed: 0.1151s/iter; left time: 5542.9431s\n",
      "\titers: 400, epoch: 3 | loss: 0.0529526\n",
      "\tspeed: 0.1165s/iter; left time: 5597.0160s\n",
      "\titers: 500, epoch: 3 | loss: 0.0714227\n",
      "\tspeed: 0.1152s/iter; left time: 5524.0865s\n",
      "\titers: 600, epoch: 3 | loss: 0.0688608\n",
      "\tspeed: 0.1155s/iter; left time: 5524.1627s\n",
      "\titers: 700, epoch: 3 | loss: 0.0620311\n",
      "\tspeed: 0.1143s/iter; left time: 5456.6919s\n",
      "\titers: 800, epoch: 3 | loss: 0.0708930\n",
      "\tspeed: 0.1151s/iter; left time: 5482.7212s\n",
      "\titers: 900, epoch: 3 | loss: 0.0866079\n",
      "\tspeed: 0.1146s/iter; left time: 5446.4498s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0677101\n",
      "\tspeed: 0.1144s/iter; left time: 5429.1977s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0758670\n",
      "\tspeed: 0.1149s/iter; left time: 5437.1999s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0556624\n",
      "\tspeed: 0.1144s/iter; left time: 5402.1315s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0775621\n",
      "\tspeed: 0.1155s/iter; left time: 5442.7883s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0611256\n",
      "\tspeed: 0.1147s/iter; left time: 5394.3616s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0686625\n",
      "\tspeed: 0.1144s/iter; left time: 5368.4786s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0653413\n",
      "\tspeed: 0.1144s/iter; left time: 5358.8470s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0682989\n",
      "\tspeed: 0.1141s/iter; left time: 5335.2256s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0813247\n",
      "\tspeed: 0.1143s/iter; left time: 5329.0297s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0699451\n",
      "\tspeed: 0.1141s/iter; left time: 5310.2608s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0698264\n",
      "\tspeed: 0.1151s/iter; left time: 5347.3520s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0730097\n",
      "\tspeed: 0.1152s/iter; left time: 5338.2925s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0531443\n",
      "\tspeed: 0.1158s/iter; left time: 5355.9318s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0865950\n",
      "\tspeed: 0.1090s/iter; left time: 5029.5136s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0673362\n",
      "\tspeed: 0.1011s/iter; left time: 4654.7382s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0694377\n",
      "\tspeed: 0.1161s/iter; left time: 5335.0049s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0681323\n",
      "\tspeed: 0.1161s/iter; left time: 5321.2158s\n",
      "Epoch: 3 cost time: 00h:05m:07.51s\n",
      "Epoch: 3 | Train Loss: 0.0699143 Vali Loss: 0.0731580 Test Loss: 0.0823368\n",
      "Validation loss decreased (0.074555 --> 0.073158).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0732795\n",
      "\tspeed: 0.9948s/iter; left time: 45411.7549s\n",
      "\titers: 200, epoch: 4 | loss: 0.0683801\n",
      "\tspeed: 0.1161s/iter; left time: 5288.1880s\n",
      "\titers: 300, epoch: 4 | loss: 0.0746370\n",
      "\tspeed: 0.1159s/iter; left time: 5265.9576s\n",
      "\titers: 400, epoch: 4 | loss: 0.0614159\n",
      "\tspeed: 0.1162s/iter; left time: 5270.5503s\n",
      "\titers: 500, epoch: 4 | loss: 0.0871528\n",
      "\tspeed: 0.1066s/iter; left time: 4822.4420s\n",
      "\titers: 600, epoch: 4 | loss: 0.0667713\n",
      "\tspeed: 0.1041s/iter; left time: 4698.7239s\n",
      "\titers: 700, epoch: 4 | loss: 0.0765538\n",
      "\tspeed: 0.1160s/iter; left time: 5225.1456s\n",
      "\titers: 800, epoch: 4 | loss: 0.0732877\n",
      "\tspeed: 0.1163s/iter; left time: 5225.4239s\n",
      "\titers: 900, epoch: 4 | loss: 0.0660509\n",
      "\tspeed: 0.1156s/iter; left time: 5183.2518s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0636234\n",
      "\tspeed: 0.1159s/iter; left time: 5184.5016s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0860807\n",
      "\tspeed: 0.1158s/iter; left time: 5171.6883s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0955131\n",
      "\tspeed: 0.1154s/iter; left time: 5140.2352s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0609814\n",
      "\tspeed: 0.1156s/iter; left time: 5137.8842s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0679537\n",
      "\tspeed: 0.1161s/iter; left time: 5149.3415s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0625240\n",
      "\tspeed: 0.1164s/iter; left time: 5150.7455s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0702323\n",
      "\tspeed: 0.1138s/iter; left time: 5025.1369s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0583599\n",
      "\tspeed: 0.1120s/iter; left time: 4934.0505s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0770510\n",
      "\tspeed: 0.1162s/iter; left time: 5104.6844s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0586945\n",
      "\tspeed: 0.1156s/iter; left time: 5070.5662s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0661756\n",
      "\tspeed: 0.1150s/iter; left time: 5030.8945s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0605308\n",
      "\tspeed: 0.1143s/iter; left time: 4990.1155s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0575532\n",
      "\tspeed: 0.1147s/iter; left time: 4993.7352s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0731880\n",
      "\tspeed: 0.1102s/iter; left time: 4787.6022s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0779390\n",
      "\tspeed: 0.1149s/iter; left time: 4979.2589s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0875420\n",
      "\tspeed: 0.1159s/iter; left time: 5013.2793s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0610501\n",
      "\tspeed: 0.1139s/iter; left time: 4914.4886s\n",
      "Epoch: 4 cost time: 00h:05m:07.85s\n",
      "Epoch: 4 | Train Loss: 0.0683495 Vali Loss: 0.0731372 Test Loss: 0.0824272\n",
      "Validation loss decreased (0.073158 --> 0.073137).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0724338\n",
      "\tspeed: 0.9850s/iter; left time: 42312.7008s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606321\n",
      "\tspeed: 0.1163s/iter; left time: 4984.2519s\n",
      "\titers: 300, epoch: 5 | loss: 0.0696154\n",
      "\tspeed: 0.1152s/iter; left time: 4927.5901s\n",
      "\titers: 400, epoch: 5 | loss: 0.0509575\n",
      "\tspeed: 0.1163s/iter; left time: 4960.8790s\n",
      "\titers: 500, epoch: 5 | loss: 0.0617031\n",
      "\tspeed: 0.1151s/iter; left time: 4898.9553s\n",
      "\titers: 600, epoch: 5 | loss: 0.0621447\n",
      "\tspeed: 0.1153s/iter; left time: 4896.5531s\n",
      "\titers: 700, epoch: 5 | loss: 0.0803559\n",
      "\tspeed: 0.1160s/iter; left time: 4913.9575s\n",
      "\titers: 800, epoch: 5 | loss: 0.0754673\n",
      "\tspeed: 0.1135s/iter; left time: 4798.2497s\n",
      "\titers: 900, epoch: 5 | loss: 0.0662868\n",
      "\tspeed: 0.1113s/iter; left time: 4692.1219s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0548034\n",
      "\tspeed: 0.1160s/iter; left time: 4879.1030s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0607634\n",
      "\tspeed: 0.1158s/iter; left time: 4860.3527s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0584400\n",
      "\tspeed: 0.1155s/iter; left time: 4833.9256s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0783436\n",
      "\tspeed: 0.1142s/iter; left time: 4768.1298s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0662102\n",
      "\tspeed: 0.1146s/iter; left time: 4773.3554s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0863430\n",
      "\tspeed: 0.1157s/iter; left time: 4808.5477s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0737604\n",
      "\tspeed: 0.1166s/iter; left time: 4834.0150s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0550957\n",
      "\tspeed: 0.1155s/iter; left time: 4777.7990s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0731752\n",
      "\tspeed: 0.1155s/iter; left time: 4766.7627s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0629882\n",
      "\tspeed: 0.1151s/iter; left time: 4735.7323s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0697896\n",
      "\tspeed: 0.1153s/iter; left time: 4734.1007s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0609421\n",
      "\tspeed: 0.1154s/iter; left time: 4727.8986s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0741776\n",
      "\tspeed: 0.1156s/iter; left time: 4724.3142s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0553646\n",
      "\tspeed: 0.1154s/iter; left time: 4704.0377s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0486240\n",
      "\tspeed: 0.1153s/iter; left time: 4686.0936s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761278\n",
      "\tspeed: 0.1151s/iter; left time: 4668.5441s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0535641\n",
      "\tspeed: 0.1158s/iter; left time: 4684.5321s\n",
      "Epoch: 5 cost time: 00h:05m:10.77s\n",
      "Epoch: 5 | Train Loss: 0.0669839 Vali Loss: 0.0734747 Test Loss: 0.0828658\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0774265\n",
      "\tspeed: 0.9689s/iter; left time: 39014.1841s\n",
      "\titers: 200, epoch: 6 | loss: 0.0675554\n",
      "\tspeed: 0.1162s/iter; left time: 4667.5723s\n",
      "\titers: 300, epoch: 6 | loss: 0.0765251\n",
      "\tspeed: 0.1162s/iter; left time: 4654.4439s\n",
      "\titers: 400, epoch: 6 | loss: 0.0700064\n",
      "\tspeed: 0.1149s/iter; left time: 4592.5471s\n",
      "\titers: 500, epoch: 6 | loss: 0.0766230\n",
      "\tspeed: 0.1147s/iter; left time: 4572.6966s\n",
      "\titers: 600, epoch: 6 | loss: 0.0631887\n",
      "\tspeed: 0.1137s/iter; left time: 4522.0342s\n",
      "\titers: 700, epoch: 6 | loss: 0.0506507\n",
      "\tspeed: 0.1151s/iter; left time: 4565.3027s\n",
      "\titers: 800, epoch: 6 | loss: 0.0790317\n",
      "\tspeed: 0.0993s/iter; left time: 3929.9614s\n",
      "\titers: 900, epoch: 6 | loss: 0.0650835\n",
      "\tspeed: 0.1154s/iter; left time: 4556.2520s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0651186\n",
      "\tspeed: 0.1161s/iter; left time: 4568.5685s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0657033\n",
      "\tspeed: 0.1155s/iter; left time: 4534.0281s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0679954\n",
      "\tspeed: 0.1150s/iter; left time: 4504.6713s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0765673\n",
      "\tspeed: 0.1143s/iter; left time: 4464.3442s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0846173\n",
      "\tspeed: 0.1152s/iter; left time: 4488.0074s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0672733\n",
      "\tspeed: 0.1143s/iter; left time: 4444.2281s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0670121\n",
      "\tspeed: 0.1143s/iter; left time: 4430.9147s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0699374\n",
      "\tspeed: 0.0974s/iter; left time: 3766.6272s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0562305\n",
      "\tspeed: 0.1068s/iter; left time: 4118.4166s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0630337\n",
      "\tspeed: 0.1155s/iter; left time: 4443.1980s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0607302\n",
      "\tspeed: 0.1163s/iter; left time: 4460.5016s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0773381\n",
      "\tspeed: 0.1163s/iter; left time: 4448.4792s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0546207\n",
      "\tspeed: 0.1052s/iter; left time: 4016.9093s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0630324\n",
      "\tspeed: 0.1025s/iter; left time: 3902.9421s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0660211\n",
      "\tspeed: 0.1168s/iter; left time: 4432.5906s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0584088\n",
      "\tspeed: 0.1152s/iter; left time: 4361.7531s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0677779\n",
      "\tspeed: 0.1154s/iter; left time: 4356.9488s\n",
      "Epoch: 6 cost time: 00h:05m:04.28s\n",
      "Epoch: 6 | Train Loss: 0.0654514 Vali Loss: 0.0743005 Test Loss: 0.0836774\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0600294\n",
      "\tspeed: 0.9650s/iter; left time: 36260.1494s\n",
      "\titers: 200, epoch: 7 | loss: 0.0696560\n",
      "\tspeed: 0.1152s/iter; left time: 4316.2745s\n",
      "\titers: 300, epoch: 7 | loss: 0.0572565\n",
      "\tspeed: 0.1141s/iter; left time: 4266.3479s\n",
      "\titers: 400, epoch: 7 | loss: 0.0701152\n",
      "\tspeed: 0.1122s/iter; left time: 4180.6558s\n",
      "\titers: 500, epoch: 7 | loss: 0.0556676\n",
      "\tspeed: 0.1147s/iter; left time: 4263.4882s\n",
      "\titers: 600, epoch: 7 | loss: 0.0589390\n",
      "\tspeed: 0.1154s/iter; left time: 4276.9752s\n",
      "\titers: 700, epoch: 7 | loss: 0.0610996\n",
      "\tspeed: 0.1163s/iter; left time: 4301.8181s\n",
      "\titers: 800, epoch: 7 | loss: 0.0592947\n",
      "\tspeed: 0.1160s/iter; left time: 4277.1571s\n",
      "\titers: 900, epoch: 7 | loss: 0.0576262\n",
      "\tspeed: 0.1150s/iter; left time: 4227.6204s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0561066\n",
      "\tspeed: 0.1167s/iter; left time: 4279.2954s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0795155\n",
      "\tspeed: 0.1170s/iter; left time: 4278.0027s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0570814\n",
      "\tspeed: 0.1154s/iter; left time: 4209.8739s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0598396\n",
      "\tspeed: 0.1157s/iter; left time: 4207.7197s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0586431\n",
      "\tspeed: 0.1158s/iter; left time: 4199.2408s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0660124\n",
      "\tspeed: 0.1152s/iter; left time: 4167.1480s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0677008\n",
      "\tspeed: 0.1155s/iter; left time: 4165.6651s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0773932\n",
      "\tspeed: 0.1156s/iter; left time: 4157.4724s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0536487\n",
      "\tspeed: 0.1158s/iter; left time: 4153.7954s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0755399\n",
      "\tspeed: 0.1153s/iter; left time: 4123.6078s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0546124\n",
      "\tspeed: 0.1160s/iter; left time: 4136.9054s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0669152\n",
      "\tspeed: 0.1186s/iter; left time: 4218.3129s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0607164\n",
      "\tspeed: 0.1161s/iter; left time: 4117.7144s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0624044\n",
      "\tspeed: 0.1165s/iter; left time: 4119.8844s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0629992\n",
      "\tspeed: 0.1165s/iter; left time: 4108.9138s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0556533\n",
      "\tspeed: 0.1157s/iter; left time: 4069.1627s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0562715\n",
      "\tspeed: 0.1158s/iter; left time: 4061.0523s\n",
      "Epoch: 7 cost time: 00h:05m:11.57s\n",
      "Epoch: 7 | Train Loss: 0.0641364 Vali Loss: 0.0751676 Test Loss: 0.0842783\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0720103\n",
      "\tspeed: 0.9617s/iter; left time: 33548.6899s\n",
      "\titers: 200, epoch: 8 | loss: 0.0527454\n",
      "\tspeed: 0.1150s/iter; left time: 3998.8838s\n",
      "\titers: 300, epoch: 8 | loss: 0.0710557\n",
      "\tspeed: 0.1152s/iter; left time: 3995.5348s\n",
      "\titers: 400, epoch: 8 | loss: 0.0677130\n",
      "\tspeed: 0.1159s/iter; left time: 4009.8367s\n",
      "\titers: 500, epoch: 8 | loss: 0.0566254\n",
      "\tspeed: 0.1158s/iter; left time: 3992.6592s\n",
      "\titers: 600, epoch: 8 | loss: 0.0638889\n",
      "\tspeed: 0.1157s/iter; left time: 3977.3375s\n",
      "\titers: 700, epoch: 8 | loss: 0.0468303\n",
      "\tspeed: 0.1154s/iter; left time: 3956.4133s\n",
      "\titers: 800, epoch: 8 | loss: 0.0620303\n",
      "\tspeed: 0.1149s/iter; left time: 3926.0927s\n",
      "\titers: 900, epoch: 8 | loss: 0.0692302\n",
      "\tspeed: 0.1164s/iter; left time: 3968.8507s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0656562\n",
      "\tspeed: 0.1169s/iter; left time: 3971.7812s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0716746\n",
      "\tspeed: 0.1156s/iter; left time: 3915.3131s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0694502\n",
      "\tspeed: 0.1167s/iter; left time: 3943.7956s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0552431\n",
      "\tspeed: 0.0993s/iter; left time: 3346.0671s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0707242\n",
      "\tspeed: 0.1162s/iter; left time: 3903.1975s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0593503\n",
      "\tspeed: 0.1173s/iter; left time: 3928.6450s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0829420\n",
      "\tspeed: 0.1163s/iter; left time: 3882.3280s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0603914\n",
      "\tspeed: 0.1167s/iter; left time: 3883.2801s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0663493\n",
      "\tspeed: 0.1126s/iter; left time: 3736.0594s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0599436\n",
      "\tspeed: 0.1153s/iter; left time: 3815.3478s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0584302\n",
      "\tspeed: 0.1163s/iter; left time: 3836.6561s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0497975\n",
      "\tspeed: 0.1155s/iter; left time: 3797.8727s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0631689\n",
      "\tspeed: 0.1159s/iter; left time: 3798.3438s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0653137\n",
      "\tspeed: 0.1156s/iter; left time: 3778.1265s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0661211\n",
      "\tspeed: 0.1162s/iter; left time: 3787.1340s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0540105\n",
      "\tspeed: 0.1158s/iter; left time: 3761.9368s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0650571\n",
      "\tspeed: 0.1157s/iter; left time: 3747.3296s\n",
      "Epoch: 8 cost time: 00h:05m:10.26s\n",
      "Epoch: 8 | Train Loss: 0.0627744 Vali Loss: 0.0746843 Test Loss: 0.0836141\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0615777\n",
      "\tspeed: 0.9610s/iter; left time: 30936.0040s\n",
      "\titers: 200, epoch: 9 | loss: 0.0627441\n",
      "\tspeed: 0.1142s/iter; left time: 3665.9066s\n",
      "\titers: 300, epoch: 9 | loss: 0.0565763\n",
      "\tspeed: 0.1150s/iter; left time: 3679.9970s\n",
      "\titers: 400, epoch: 9 | loss: 0.0663073\n",
      "\tspeed: 0.1141s/iter; left time: 3639.1125s\n",
      "\titers: 500, epoch: 9 | loss: 0.0709065\n",
      "\tspeed: 0.1152s/iter; left time: 3662.8615s\n",
      "\titers: 600, epoch: 9 | loss: 0.0613810\n",
      "\tspeed: 0.1146s/iter; left time: 3631.0668s\n",
      "\titers: 700, epoch: 9 | loss: 0.0465641\n",
      "\tspeed: 0.1144s/iter; left time: 3613.6552s\n",
      "\titers: 800, epoch: 9 | loss: 0.0625643\n",
      "\tspeed: 0.1141s/iter; left time: 3592.9230s\n",
      "\titers: 900, epoch: 9 | loss: 0.0633669\n",
      "\tspeed: 0.1130s/iter; left time: 3547.3924s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0585374\n",
      "\tspeed: 0.1156s/iter; left time: 3617.3397s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0481584\n",
      "\tspeed: 0.0971s/iter; left time: 3029.7911s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0595765\n",
      "\tspeed: 0.0963s/iter; left time: 2992.9819s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0813546\n",
      "\tspeed: 0.0964s/iter; left time: 2986.8064s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0566317\n",
      "\tspeed: 0.1011s/iter; left time: 3124.6510s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0461254\n",
      "\tspeed: 0.1145s/iter; left time: 3526.9878s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0633289\n",
      "\tspeed: 0.1148s/iter; left time: 3524.0822s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0650627\n",
      "\tspeed: 0.1126s/iter; left time: 3445.7080s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0545909\n",
      "\tspeed: 0.0963s/iter; left time: 2935.1904s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0653074\n",
      "\tspeed: 0.0962s/iter; left time: 2923.3665s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0694104\n",
      "\tspeed: 0.1085s/iter; left time: 3288.0460s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0491853\n",
      "\tspeed: 0.1142s/iter; left time: 3447.3465s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0691575\n",
      "\tspeed: 0.1148s/iter; left time: 3453.3035s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0568749\n",
      "\tspeed: 0.1152s/iter; left time: 3456.2157s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0560714\n",
      "\tspeed: 0.1096s/iter; left time: 3277.5473s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0608205\n",
      "\tspeed: 0.1150s/iter; left time: 3425.9624s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0655743\n",
      "\tspeed: 0.1154s/iter; left time: 3427.3964s\n",
      "Epoch: 9 cost time: 00h:04m:57.27s\n",
      "Epoch: 9 | Train Loss: 0.0615349 Vali Loss: 0.0750233 Test Loss: 0.0850513\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018618784844875336, rmse:0.1364506632089615, mae:0.08242722600698471, rse:0.5278697609901428\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 96: 00h:59m:01.27s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 20:02:18,258] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 20:02:19,468] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 20:02:19,468] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 20:02:19,469] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 20:02:19,572] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 20:02:19,572] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 20:02:20,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 20:02:20,243] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 20:02:20,243] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 20:02:20,244] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 20:02:20,244] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 20:02:20,244] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 20:02:20,518] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 20:02:20,519] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,646] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 20:02:20,647] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,647] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,647] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 20:02:20,765] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 20:02:20,766] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,766] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 20:02:20,767] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa6999be090>\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1351442\n",
      "\tspeed: 0.1699s/iter; left time: 9105.9032s\n",
      "\titers: 200, epoch: 1 | loss: 0.1291015\n",
      "\tspeed: 0.1269s/iter; left time: 6784.2178s\n",
      "\titers: 300, epoch: 1 | loss: 0.1202403\n",
      "\tspeed: 0.1260s/iter; left time: 6727.0647s\n",
      "\titers: 400, epoch: 1 | loss: 0.0905234\n",
      "\tspeed: 0.1254s/iter; left time: 6679.5488s\n",
      "\titers: 500, epoch: 1 | loss: 0.0911944\n",
      "\tspeed: 0.1245s/iter; left time: 6618.7318s\n",
      "\titers: 600, epoch: 1 | loss: 0.0763717\n",
      "\tspeed: 0.1253s/iter; left time: 6651.2209s\n",
      "\titers: 700, epoch: 1 | loss: 0.0910461\n",
      "\tspeed: 0.1256s/iter; left time: 6653.2511s\n",
      "\titers: 800, epoch: 1 | loss: 0.0693435\n",
      "\tspeed: 0.1256s/iter; left time: 6639.6599s\n",
      "\titers: 900, epoch: 1 | loss: 0.0690839\n",
      "\tspeed: 0.1255s/iter; left time: 6622.4819s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0823524\n",
      "\tspeed: 0.1253s/iter; left time: 6602.2213s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0720417\n",
      "\tspeed: 0.1193s/iter; left time: 6273.7521s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0871387\n",
      "\tspeed: 0.1091s/iter; left time: 5725.1725s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0793843\n",
      "\tspeed: 0.1258s/iter; left time: 6587.3256s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0777027\n",
      "\tspeed: 0.1255s/iter; left time: 6560.8929s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0669637\n",
      "\tspeed: 0.1244s/iter; left time: 6493.7056s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0711201\n",
      "\tspeed: 0.1090s/iter; left time: 5679.2976s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0681182\n",
      "\tspeed: 0.1205s/iter; left time: 6266.1622s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0701449\n",
      "\tspeed: 0.1236s/iter; left time: 6414.7803s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0895861\n",
      "\tspeed: 0.1240s/iter; left time: 6422.2342s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0716423\n",
      "\tspeed: 0.1242s/iter; left time: 6420.4835s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0750876\n",
      "\tspeed: 0.1240s/iter; left time: 6396.4932s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0838144\n",
      "\tspeed: 0.1231s/iter; left time: 6339.3436s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0789797\n",
      "\tspeed: 0.1241s/iter; left time: 6376.1942s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0668762\n",
      "\tspeed: 0.1235s/iter; left time: 6334.2670s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0662106\n",
      "\tspeed: 0.1233s/iter; left time: 6311.9263s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0773352\n",
      "\tspeed: 0.1238s/iter; left time: 6326.0240s\n",
      "Epoch: 1 cost time: 00h:05m:32.23s\n",
      "Epoch: 1 | Train Loss: 0.0896614 Vali Loss: 0.0796900 Test Loss: 0.0892447\n",
      "Validation loss decreased (inf --> 0.079690).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0728463\n",
      "\tspeed: 1.0958s/iter; left time: 55774.3952s\n",
      "\titers: 200, epoch: 2 | loss: 0.0773688\n",
      "\tspeed: 0.1164s/iter; left time: 5910.8337s\n",
      "\titers: 300, epoch: 2 | loss: 0.0769546\n",
      "\tspeed: 0.1010s/iter; left time: 5121.3912s\n",
      "\titers: 400, epoch: 2 | loss: 0.0773306\n",
      "\tspeed: 0.1160s/iter; left time: 5867.6510s\n",
      "\titers: 500, epoch: 2 | loss: 0.0708913\n",
      "\tspeed: 0.1173s/iter; left time: 5923.1477s\n",
      "\titers: 600, epoch: 2 | loss: 0.0599826\n",
      "\tspeed: 0.1166s/iter; left time: 5874.3144s\n",
      "\titers: 700, epoch: 2 | loss: 0.0657055\n",
      "\tspeed: 0.1155s/iter; left time: 5808.7773s\n",
      "\titers: 800, epoch: 2 | loss: 0.0677629\n",
      "\tspeed: 0.1116s/iter; left time: 5600.0752s\n",
      "\titers: 900, epoch: 2 | loss: 0.0905147\n",
      "\tspeed: 0.1160s/iter; left time: 5809.5678s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0794925\n",
      "\tspeed: 0.1163s/iter; left time: 5816.0759s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0824175\n",
      "\tspeed: 0.1157s/iter; left time: 5771.1075s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0622338\n",
      "\tspeed: 0.1155s/iter; left time: 5750.7121s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0587609\n",
      "\tspeed: 0.1158s/iter; left time: 5756.7713s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0815516\n",
      "\tspeed: 0.1061s/iter; left time: 5264.1255s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0769047\n",
      "\tspeed: 0.1152s/iter; left time: 5702.1312s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0706145\n",
      "\tspeed: 0.1171s/iter; left time: 5784.1223s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0698010\n",
      "\tspeed: 0.1177s/iter; left time: 5803.1343s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0721849\n",
      "\tspeed: 0.1164s/iter; left time: 5724.9817s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0779953\n",
      "\tspeed: 0.1141s/iter; left time: 5599.8956s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0784639\n",
      "\tspeed: 0.1146s/iter; left time: 5613.2696s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0783538\n",
      "\tspeed: 0.1032s/iter; left time: 5047.3688s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0755614\n",
      "\tspeed: 0.0966s/iter; left time: 4714.5547s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0806916\n",
      "\tspeed: 0.0969s/iter; left time: 4716.6625s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0772925\n",
      "\tspeed: 0.0968s/iter; left time: 4703.1712s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0781110\n",
      "\tspeed: 0.1082s/iter; left time: 5249.4811s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0692590\n",
      "\tspeed: 0.1173s/iter; left time: 5676.5547s\n",
      "Epoch: 2 cost time: 00h:05m:01.36s\n",
      "Epoch: 2 | Train Loss: 0.0755227 Vali Loss: 0.0783544 Test Loss: 0.0887550\n",
      "Validation loss decreased (0.079690 --> 0.078354).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0933752\n",
      "\tspeed: 0.9840s/iter; left time: 47443.7926s\n",
      "\titers: 200, epoch: 3 | loss: 0.0752771\n",
      "\tspeed: 0.1178s/iter; left time: 5667.5518s\n",
      "\titers: 300, epoch: 3 | loss: 0.0674665\n",
      "\tspeed: 0.1180s/iter; left time: 5664.5428s\n",
      "\titers: 400, epoch: 3 | loss: 0.0666938\n",
      "\tspeed: 0.1172s/iter; left time: 5616.0198s\n",
      "\titers: 500, epoch: 3 | loss: 0.0829887\n",
      "\tspeed: 0.1172s/iter; left time: 5602.3442s\n",
      "\titers: 600, epoch: 3 | loss: 0.0702578\n",
      "\tspeed: 0.1176s/iter; left time: 5609.7557s\n",
      "\titers: 700, epoch: 3 | loss: 0.0772638\n",
      "\tspeed: 0.1186s/iter; left time: 5644.9005s\n",
      "\titers: 800, epoch: 3 | loss: 0.0794854\n",
      "\tspeed: 0.1171s/iter; left time: 5566.0472s\n",
      "\titers: 900, epoch: 3 | loss: 0.0809638\n",
      "\tspeed: 0.1166s/iter; left time: 5529.4685s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0592312\n",
      "\tspeed: 0.1179s/iter; left time: 5578.2232s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0774051\n",
      "\tspeed: 0.1166s/iter; left time: 5504.3563s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0845707\n",
      "\tspeed: 0.1157s/iter; left time: 5449.2912s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0741284\n",
      "\tspeed: 0.1033s/iter; left time: 4857.6912s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0733542\n",
      "\tspeed: 0.1050s/iter; left time: 4927.1836s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0812433\n",
      "\tspeed: 0.1170s/iter; left time: 5476.8757s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0667646\n",
      "\tspeed: 0.1167s/iter; left time: 5452.4898s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0619555\n",
      "\tspeed: 0.1165s/iter; left time: 5429.4795s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0823658\n",
      "\tspeed: 0.1170s/iter; left time: 5442.6550s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0658753\n",
      "\tspeed: 0.1163s/iter; left time: 5397.0997s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0627456\n",
      "\tspeed: 0.1153s/iter; left time: 5341.7067s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0761323\n",
      "\tspeed: 0.1153s/iter; left time: 5326.9113s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0735498\n",
      "\tspeed: 0.1153s/iter; left time: 5318.7964s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0608646\n",
      "\tspeed: 0.1022s/iter; left time: 4700.6453s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0716532\n",
      "\tspeed: 0.1103s/iter; left time: 5063.1770s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0780609\n",
      "\tspeed: 0.1170s/iter; left time: 5361.0799s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0768194\n",
      "\tspeed: 0.1170s/iter; left time: 5349.9745s\n",
      "Epoch: 3 cost time: 00h:05m:09.45s\n",
      "Epoch: 3 | Train Loss: 0.0732791 Vali Loss: 0.0775053 Test Loss: 0.0882827\n",
      "Validation loss decreased (0.078354 --> 0.077505).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0618128\n",
      "\tspeed: 0.9854s/iter; left time: 44863.6708s\n",
      "\titers: 200, epoch: 4 | loss: 0.0684583\n",
      "\tspeed: 0.1171s/iter; left time: 5319.3668s\n",
      "\titers: 300, epoch: 4 | loss: 0.0878761\n",
      "\tspeed: 0.1120s/iter; left time: 5077.5005s\n",
      "\titers: 400, epoch: 4 | loss: 0.0614962\n",
      "\tspeed: 0.1100s/iter; left time: 4973.6896s\n",
      "\titers: 500, epoch: 4 | loss: 0.0650298\n",
      "\tspeed: 0.1076s/iter; left time: 4854.9516s\n",
      "\titers: 600, epoch: 4 | loss: 0.0691537\n",
      "\tspeed: 0.1162s/iter; left time: 5231.9482s\n",
      "\titers: 700, epoch: 4 | loss: 0.0632137\n",
      "\tspeed: 0.1153s/iter; left time: 5182.1700s\n",
      "\titers: 800, epoch: 4 | loss: 0.0605262\n",
      "\tspeed: 0.1166s/iter; left time: 5227.5753s\n",
      "\titers: 900, epoch: 4 | loss: 0.0776869\n",
      "\tspeed: 0.1161s/iter; left time: 5192.7091s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0974346\n",
      "\tspeed: 0.1165s/iter; left time: 5198.8521s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0625406\n",
      "\tspeed: 0.1159s/iter; left time: 5160.0367s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0563061\n",
      "\tspeed: 0.1129s/iter; left time: 5015.2611s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0730062\n",
      "\tspeed: 0.1165s/iter; left time: 5162.6243s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0758484\n",
      "\tspeed: 0.1157s/iter; left time: 5118.4553s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0684197\n",
      "\tspeed: 0.1156s/iter; left time: 5100.2411s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0800384\n",
      "\tspeed: 0.1124s/iter; left time: 4948.6238s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1039231\n",
      "\tspeed: 0.1055s/iter; left time: 4634.5882s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0638697\n",
      "\tspeed: 0.0970s/iter; left time: 4253.5934s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0696328\n",
      "\tspeed: 0.0969s/iter; left time: 4236.3141s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0607886\n",
      "\tspeed: 0.1163s/iter; left time: 5074.3592s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0725573\n",
      "\tspeed: 0.1157s/iter; left time: 5034.8533s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0679766\n",
      "\tspeed: 0.1156s/iter; left time: 5018.6808s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0680952\n",
      "\tspeed: 0.1166s/iter; left time: 5053.9386s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0643962\n",
      "\tspeed: 0.1166s/iter; left time: 5041.6817s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0806731\n",
      "\tspeed: 0.1170s/iter; left time: 5047.9906s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0733256\n",
      "\tspeed: 0.1160s/iter; left time: 4991.8655s\n",
      "Epoch: 4 cost time: 00h:05m:05.10s\n",
      "Epoch: 4 | Train Loss: 0.0717433 Vali Loss: 0.0767714 Test Loss: 0.0877820\n",
      "Validation loss decreased (0.077505 --> 0.076771).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0772906\n",
      "\tspeed: 0.9804s/iter; left time: 42004.9112s\n",
      "\titers: 200, epoch: 5 | loss: 0.0602556\n",
      "\tspeed: 0.1152s/iter; left time: 4924.2128s\n",
      "\titers: 300, epoch: 5 | loss: 0.0761829\n",
      "\tspeed: 0.1156s/iter; left time: 4931.3701s\n",
      "\titers: 400, epoch: 5 | loss: 0.0785191\n",
      "\tspeed: 0.1158s/iter; left time: 4928.3237s\n",
      "\titers: 500, epoch: 5 | loss: 0.0726331\n",
      "\tspeed: 0.1176s/iter; left time: 4990.9619s\n",
      "\titers: 600, epoch: 5 | loss: 0.0628080\n",
      "\tspeed: 0.1190s/iter; left time: 5040.7194s\n",
      "\titers: 700, epoch: 5 | loss: 0.0705963\n",
      "\tspeed: 0.1208s/iter; left time: 5104.8287s\n",
      "\titers: 800, epoch: 5 | loss: 0.0640927\n",
      "\tspeed: 0.1170s/iter; left time: 4929.9057s\n",
      "\titers: 900, epoch: 5 | loss: 0.0642261\n",
      "\tspeed: 0.1168s/iter; left time: 4909.6580s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0796294\n",
      "\tspeed: 0.1165s/iter; left time: 4886.9596s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0674907\n",
      "\tspeed: 0.1161s/iter; left time: 4857.7713s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0745679\n",
      "\tspeed: 0.1176s/iter; left time: 4909.6260s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0665957\n",
      "\tspeed: 0.1170s/iter; left time: 4871.9910s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0686471\n",
      "\tspeed: 0.1155s/iter; left time: 4799.7413s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0801993\n",
      "\tspeed: 0.1171s/iter; left time: 4851.6019s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0743748\n",
      "\tspeed: 0.1161s/iter; left time: 4800.7026s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0830029\n",
      "\tspeed: 0.1153s/iter; left time: 4756.4299s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0718037\n",
      "\tspeed: 0.1164s/iter; left time: 4789.2782s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0723873\n",
      "\tspeed: 0.1169s/iter; left time: 4797.9684s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0700455\n",
      "\tspeed: 0.1187s/iter; left time: 4861.5283s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0692627\n",
      "\tspeed: 0.1159s/iter; left time: 4734.9627s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0740308\n",
      "\tspeed: 0.1189s/iter; left time: 4842.9737s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0640778\n",
      "\tspeed: 0.1175s/iter; left time: 4774.9139s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0611654\n",
      "\tspeed: 0.1158s/iter; left time: 4693.2176s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0829783\n",
      "\tspeed: 0.1152s/iter; left time: 4660.5101s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0644178\n",
      "\tspeed: 0.1072s/iter; left time: 4322.9743s\n",
      "Epoch: 5 cost time: 00h:05m:13.02s\n",
      "Epoch: 5 | Train Loss: 0.0704540 Vali Loss: 0.0779003 Test Loss: 0.0883271\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0766604\n",
      "\tspeed: 0.9490s/iter; left time: 38111.2026s\n",
      "\titers: 200, epoch: 6 | loss: 0.0602073\n",
      "\tspeed: 0.1150s/iter; left time: 4606.6883s\n",
      "\titers: 300, epoch: 6 | loss: 0.0693253\n",
      "\tspeed: 0.1157s/iter; left time: 4624.1599s\n",
      "\titers: 400, epoch: 6 | loss: 0.0750857\n",
      "\tspeed: 0.1155s/iter; left time: 4602.2455s\n",
      "\titers: 500, epoch: 6 | loss: 0.0740420\n",
      "\tspeed: 0.1162s/iter; left time: 4620.1758s\n",
      "\titers: 600, epoch: 6 | loss: 0.0870911\n",
      "\tspeed: 0.1158s/iter; left time: 4594.5795s\n",
      "\titers: 700, epoch: 6 | loss: 0.0626083\n",
      "\tspeed: 0.1151s/iter; left time: 4552.2886s\n",
      "\titers: 800, epoch: 6 | loss: 0.0737172\n",
      "\tspeed: 0.1159s/iter; left time: 4572.6820s\n",
      "\titers: 900, epoch: 6 | loss: 0.0666808\n",
      "\tspeed: 0.1152s/iter; left time: 4532.8060s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0632041\n",
      "\tspeed: 0.1152s/iter; left time: 4522.9705s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0679926\n",
      "\tspeed: 0.1149s/iter; left time: 4497.6465s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0704225\n",
      "\tspeed: 0.1138s/iter; left time: 4444.8616s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0641174\n",
      "\tspeed: 0.1144s/iter; left time: 4458.7806s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0752409\n",
      "\tspeed: 0.0969s/iter; left time: 3766.7865s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0732217\n",
      "\tspeed: 0.1159s/iter; left time: 4490.7261s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0692174\n",
      "\tspeed: 0.1169s/iter; left time: 4518.5567s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0633075\n",
      "\tspeed: 0.1160s/iter; left time: 4473.1074s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0659616\n",
      "\tspeed: 0.1154s/iter; left time: 4438.9299s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0701341\n",
      "\tspeed: 0.1159s/iter; left time: 4445.0044s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0799484\n",
      "\tspeed: 0.1162s/iter; left time: 4445.5652s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0665025\n",
      "\tspeed: 0.1153s/iter; left time: 4401.8152s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0605779\n",
      "\tspeed: 0.1154s/iter; left time: 4390.6827s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0754763\n",
      "\tspeed: 0.1155s/iter; left time: 4382.7074s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0764293\n",
      "\tspeed: 0.1146s/iter; left time: 4338.9996s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0653570\n",
      "\tspeed: 0.1148s/iter; left time: 4333.6815s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0497287\n",
      "\tspeed: 0.1147s/iter; left time: 4318.6174s\n",
      "Epoch: 6 cost time: 00h:05m:08.43s\n",
      "Epoch: 6 | Train Loss: 0.0689933 Vali Loss: 0.0801865 Test Loss: 0.0887019\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0567738\n",
      "\tspeed: 0.9551s/iter; left time: 35796.1148s\n",
      "\titers: 200, epoch: 7 | loss: 0.0698603\n",
      "\tspeed: 0.1164s/iter; left time: 4351.4329s\n",
      "\titers: 300, epoch: 7 | loss: 0.0627555\n",
      "\tspeed: 0.1164s/iter; left time: 4337.1812s\n",
      "\titers: 400, epoch: 7 | loss: 0.0711131\n",
      "\tspeed: 0.1148s/iter; left time: 4266.3924s\n",
      "\titers: 500, epoch: 7 | loss: 0.0619935\n",
      "\tspeed: 0.1056s/iter; left time: 3914.1867s\n",
      "\titers: 600, epoch: 7 | loss: 0.0684474\n",
      "\tspeed: 0.0993s/iter; left time: 3670.1069s\n",
      "\titers: 700, epoch: 7 | loss: 0.0584849\n",
      "\tspeed: 0.0964s/iter; left time: 3556.7805s\n",
      "\titers: 800, epoch: 7 | loss: 0.0825376\n",
      "\tspeed: 0.1175s/iter; left time: 4322.6650s\n",
      "\titers: 900, epoch: 7 | loss: 0.0589024\n",
      "\tspeed: 0.1162s/iter; left time: 4260.5552s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0760413\n",
      "\tspeed: 0.1181s/iter; left time: 4321.4916s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0752413\n",
      "\tspeed: 0.1166s/iter; left time: 4251.7001s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0719468\n",
      "\tspeed: 0.1162s/iter; left time: 4228.4449s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0687664\n",
      "\tspeed: 0.1178s/iter; left time: 4271.6289s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0635155\n",
      "\tspeed: 0.1170s/iter; left time: 4234.4860s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0563175\n",
      "\tspeed: 0.1154s/iter; left time: 4163.9422s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0767602\n",
      "\tspeed: 0.1161s/iter; left time: 4177.0578s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0655388\n",
      "\tspeed: 0.1168s/iter; left time: 4188.8851s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0644029\n",
      "\tspeed: 0.1167s/iter; left time: 4175.4584s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0621075\n",
      "\tspeed: 0.1163s/iter; left time: 4150.9808s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0698945\n",
      "\tspeed: 0.1183s/iter; left time: 4208.0165s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0594934\n",
      "\tspeed: 0.1169s/iter; left time: 4147.5437s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0764240\n",
      "\tspeed: 0.1169s/iter; left time: 4134.8430s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0701658\n",
      "\tspeed: 0.1163s/iter; left time: 4101.5739s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0640838\n",
      "\tspeed: 0.1173s/iter; left time: 4126.3804s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0787112\n",
      "\tspeed: 0.1165s/iter; left time: 4086.9096s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0690667\n",
      "\tspeed: 0.1156s/iter; left time: 4042.4559s\n",
      "Epoch: 7 cost time: 00h:05m:08.77s\n",
      "Epoch: 7 | Train Loss: 0.0675281 Vali Loss: 0.0804067 Test Loss: 0.0894281\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0717882\n",
      "\tspeed: 0.9574s/iter; left time: 33309.8724s\n",
      "\titers: 200, epoch: 8 | loss: 0.0605006\n",
      "\tspeed: 0.1160s/iter; left time: 4024.5968s\n",
      "\titers: 300, epoch: 8 | loss: 0.0679652\n",
      "\tspeed: 0.1162s/iter; left time: 4019.0646s\n",
      "\titers: 400, epoch: 8 | loss: 0.0670836\n",
      "\tspeed: 0.1138s/iter; left time: 3924.8729s\n",
      "\titers: 500, epoch: 8 | loss: 0.0609015\n",
      "\tspeed: 0.1095s/iter; left time: 3766.6694s\n",
      "\titers: 600, epoch: 8 | loss: 0.0691020\n",
      "\tspeed: 0.1154s/iter; left time: 3957.7316s\n",
      "\titers: 700, epoch: 8 | loss: 0.0870408\n",
      "\tspeed: 0.1170s/iter; left time: 4000.6373s\n",
      "\titers: 800, epoch: 8 | loss: 0.0670967\n",
      "\tspeed: 0.1153s/iter; left time: 3929.4964s\n",
      "\titers: 900, epoch: 8 | loss: 0.0613804\n",
      "\tspeed: 0.1153s/iter; left time: 3917.7232s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0556819\n",
      "\tspeed: 0.1169s/iter; left time: 3960.6988s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0724371\n",
      "\tspeed: 0.1152s/iter; left time: 3891.3479s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0520738\n",
      "\tspeed: 0.1176s/iter; left time: 3961.8690s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0637756\n",
      "\tspeed: 0.1162s/iter; left time: 3902.5430s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0791539\n",
      "\tspeed: 0.1144s/iter; left time: 3830.5374s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0764657\n",
      "\tspeed: 0.1085s/iter; left time: 3622.4277s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0693158\n",
      "\tspeed: 0.1170s/iter; left time: 3894.0243s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0667949\n",
      "\tspeed: 0.1159s/iter; left time: 3847.0470s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0755049\n",
      "\tspeed: 0.1168s/iter; left time: 3864.3251s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0684051\n",
      "\tspeed: 0.1168s/iter; left time: 3853.6416s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0655407\n",
      "\tspeed: 0.1169s/iter; left time: 3845.7445s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0763611\n",
      "\tspeed: 0.1165s/iter; left time: 3819.8486s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0691977\n",
      "\tspeed: 0.1162s/iter; left time: 3800.4033s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0659143\n",
      "\tspeed: 0.1161s/iter; left time: 3784.8566s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0583210\n",
      "\tspeed: 0.0995s/iter; left time: 3233.6551s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0685454\n",
      "\tspeed: 0.1171s/iter; left time: 3794.1018s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0590881\n",
      "\tspeed: 0.1169s/iter; left time: 3773.8764s\n",
      "Epoch: 8 cost time: 00h:05m:09.16s\n",
      "Epoch: 8 | Train Loss: 0.0660281 Vali Loss: 0.0795411 Test Loss: 0.0886591\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0612876\n",
      "\tspeed: 0.9596s/iter; left time: 30810.7018s\n",
      "\titers: 200, epoch: 9 | loss: 0.0573643\n",
      "\tspeed: 0.1176s/iter; left time: 3764.3037s\n",
      "\titers: 300, epoch: 9 | loss: 0.0823771\n",
      "\tspeed: 0.1178s/iter; left time: 3757.7956s\n",
      "\titers: 400, epoch: 9 | loss: 0.0628237\n",
      "\tspeed: 0.1158s/iter; left time: 3683.7614s\n",
      "\titers: 500, epoch: 9 | loss: 0.0592355\n",
      "\tspeed: 0.1159s/iter; left time: 3674.8911s\n",
      "\titers: 600, epoch: 9 | loss: 0.0575299\n",
      "\tspeed: 0.1155s/iter; left time: 3649.7265s\n",
      "\titers: 700, epoch: 9 | loss: 0.0713902\n",
      "\tspeed: 0.1150s/iter; left time: 3623.1765s\n",
      "\titers: 800, epoch: 9 | loss: 0.0673367\n",
      "\tspeed: 0.1168s/iter; left time: 3669.7089s\n",
      "\titers: 900, epoch: 9 | loss: 0.0654938\n",
      "\tspeed: 0.1144s/iter; left time: 3582.6519s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0654262\n",
      "\tspeed: 0.1163s/iter; left time: 3629.7195s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0678178\n",
      "\tspeed: 0.1165s/iter; left time: 3623.6421s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0703463\n",
      "\tspeed: 0.1161s/iter; left time: 3598.7283s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0616599\n",
      "\tspeed: 0.1175s/iter; left time: 3632.7587s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0652685\n",
      "\tspeed: 0.1133s/iter; left time: 3491.6282s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0672777\n",
      "\tspeed: 0.1177s/iter; left time: 3614.9463s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0607611\n",
      "\tspeed: 0.1159s/iter; left time: 3546.3244s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0589829\n",
      "\tspeed: 0.1155s/iter; left time: 3522.8006s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0738688\n",
      "\tspeed: 0.1164s/iter; left time: 3538.8598s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0663319\n",
      "\tspeed: 0.1154s/iter; left time: 3498.4936s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0749816\n",
      "\tspeed: 0.1156s/iter; left time: 3492.0085s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0585454\n",
      "\tspeed: 0.1163s/iter; left time: 3501.9838s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0625599\n",
      "\tspeed: 0.1175s/iter; left time: 3525.3204s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0571969\n",
      "\tspeed: 0.1177s/iter; left time: 3519.8619s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0624960\n",
      "\tspeed: 0.1170s/iter; left time: 3486.2738s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0616413\n",
      "\tspeed: 0.1165s/iter; left time: 3459.9627s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0632547\n",
      "\tspeed: 0.1166s/iter; left time: 3451.8509s\n",
      "Epoch: 9 cost time: 00h:05m:12.61s\n",
      "Epoch: 9 | Train Loss: 0.0648369 Vali Loss: 0.0807261 Test Loss: 0.0902847\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.020460350438952446, rmse:0.14303968846797943, mae:0.08778193593025208, rse:0.5541521310806274\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 168: 00h:59m:08.07s\n",
      "\n",
      "Intermediate time for FR: 03h:49m:01.90s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 21:01:26,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 21:01:27,921] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 21:01:27,921] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 21:01:27,921] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 21:01:28,030] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 21:01:28,030] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 21:01:28,698] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 21:01:28,700] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 21:01:28,700] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 21:01:28,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 21:01:28,702] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 21:01:28,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 21:01:29,038] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 21:01:29,039] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,039] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,160] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 21:01:29,161] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,161] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,161] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 21:01:29,274] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 21:01:29,275] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,276] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 21:01:29,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 21:01:29,277] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f888c84bc50>\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1616891\n",
      "\tspeed: 0.1710s/iter; left time: 9204.6952s\n",
      "\titers: 200, epoch: 1 | loss: 0.1988888\n",
      "\tspeed: 0.1278s/iter; left time: 6870.6330s\n",
      "\titers: 300, epoch: 1 | loss: 0.1653045\n",
      "\tspeed: 0.1282s/iter; left time: 6874.5522s\n",
      "\titers: 400, epoch: 1 | loss: 0.1196822\n",
      "\tspeed: 0.1257s/iter; left time: 6731.4162s\n",
      "\titers: 500, epoch: 1 | loss: 0.1014044\n",
      "\tspeed: 0.1252s/iter; left time: 6691.8765s\n",
      "\titers: 600, epoch: 1 | loss: 0.0925063\n",
      "\tspeed: 0.1268s/iter; left time: 6763.2208s\n",
      "\titers: 700, epoch: 1 | loss: 0.0790739\n",
      "\tspeed: 0.1249s/iter; left time: 6651.4008s\n",
      "\titers: 800, epoch: 1 | loss: 0.0929413\n",
      "\tspeed: 0.1247s/iter; left time: 6625.0595s\n",
      "\titers: 900, epoch: 1 | loss: 0.0834920\n",
      "\tspeed: 0.1238s/iter; left time: 6567.8658s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0941050\n",
      "\tspeed: 0.1239s/iter; left time: 6557.5897s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0919473\n",
      "\tspeed: 0.1250s/iter; left time: 6605.1167s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0990285\n",
      "\tspeed: 0.1270s/iter; left time: 6699.1351s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0970086\n",
      "\tspeed: 0.1124s/iter; left time: 5916.7375s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0915016\n",
      "\tspeed: 0.1241s/iter; left time: 6521.7872s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1036599\n",
      "\tspeed: 0.1248s/iter; left time: 6546.6892s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0895585\n",
      "\tspeed: 0.1241s/iter; left time: 6496.2484s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0857308\n",
      "\tspeed: 0.1225s/iter; left time: 6400.2457s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1034492\n",
      "\tspeed: 0.1259s/iter; left time: 6562.3418s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0879421\n",
      "\tspeed: 0.1243s/iter; left time: 6467.3579s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0744498\n",
      "\tspeed: 0.1255s/iter; left time: 6520.3883s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0770138\n",
      "\tspeed: 0.1273s/iter; left time: 6598.1901s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0825696\n",
      "\tspeed: 0.1246s/iter; left time: 6447.8373s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0842841\n",
      "\tspeed: 0.1255s/iter; left time: 6478.9438s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0767634\n",
      "\tspeed: 0.1241s/iter; left time: 6397.2387s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0836823\n",
      "\tspeed: 0.1252s/iter; left time: 6441.0390s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0827758\n",
      "\tspeed: 0.1258s/iter; left time: 6459.8688s\n",
      "Epoch: 1 cost time: 00h:05m:38.23s\n",
      "Epoch: 1 | Train Loss: 0.1062889 Vali Loss: 0.0663070 Test Loss: 0.0692037\n",
      "Validation loss decreased (inf --> 0.066307).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0920735\n",
      "\tspeed: 1.1417s/iter; left time: 58390.2894s\n",
      "\titers: 200, epoch: 2 | loss: 0.0877688\n",
      "\tspeed: 0.1157s/iter; left time: 5905.6337s\n",
      "\titers: 300, epoch: 2 | loss: 0.0842819\n",
      "\tspeed: 0.1162s/iter; left time: 5920.0065s\n",
      "\titers: 400, epoch: 2 | loss: 0.0832414\n",
      "\tspeed: 0.1161s/iter; left time: 5900.5147s\n",
      "\titers: 500, epoch: 2 | loss: 0.0781539\n",
      "\tspeed: 0.1150s/iter; left time: 5835.2356s\n",
      "\titers: 600, epoch: 2 | loss: 0.0809719\n",
      "\tspeed: 0.1158s/iter; left time: 5862.3119s\n",
      "\titers: 700, epoch: 2 | loss: 0.0726647\n",
      "\tspeed: 0.1163s/iter; left time: 5877.7406s\n",
      "\titers: 800, epoch: 2 | loss: 0.0951623\n",
      "\tspeed: 0.1123s/iter; left time: 5664.2424s\n",
      "\titers: 900, epoch: 2 | loss: 0.0855009\n",
      "\tspeed: 0.1155s/iter; left time: 5813.5665s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0684977\n",
      "\tspeed: 0.1161s/iter; left time: 5835.6063s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0749037\n",
      "\tspeed: 0.1147s/iter; left time: 5751.6981s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0930617\n",
      "\tspeed: 0.1146s/iter; left time: 5735.1883s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0960172\n",
      "\tspeed: 0.1165s/iter; left time: 5819.0670s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0739193\n",
      "\tspeed: 0.1167s/iter; left time: 5819.0180s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0809535\n",
      "\tspeed: 0.1167s/iter; left time: 5803.7011s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0809979\n",
      "\tspeed: 0.1146s/iter; left time: 5689.8242s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0809475\n",
      "\tspeed: 0.1144s/iter; left time: 5666.4364s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0921727\n",
      "\tspeed: 0.1093s/iter; left time: 5404.8987s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0837119\n",
      "\tspeed: 0.1000s/iter; left time: 4935.9752s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0755220\n",
      "\tspeed: 0.1160s/iter; left time: 5711.7831s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0859457\n",
      "\tspeed: 0.1161s/iter; left time: 5703.6259s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0785623\n",
      "\tspeed: 0.1155s/iter; left time: 5662.6971s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0885806\n",
      "\tspeed: 0.1166s/iter; left time: 5707.1235s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0720094\n",
      "\tspeed: 0.1148s/iter; left time: 5606.4342s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0886370\n",
      "\tspeed: 0.1121s/iter; left time: 5462.7781s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0621730\n",
      "\tspeed: 0.1173s/iter; left time: 5705.9170s\n",
      "Epoch: 2 cost time: 00h:05m:09.66s\n",
      "Epoch: 2 | Train Loss: 0.0804989 Vali Loss: 0.0635860 Test Loss: 0.0664676\n",
      "Validation loss decreased (0.066307 --> 0.063586).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0694548\n",
      "\tspeed: 0.9960s/iter; left time: 48251.9254s\n",
      "\titers: 200, epoch: 3 | loss: 0.0808334\n",
      "\tspeed: 0.1149s/iter; left time: 5554.1130s\n",
      "\titers: 300, epoch: 3 | loss: 0.0840988\n",
      "\tspeed: 0.1169s/iter; left time: 5641.2641s\n",
      "\titers: 400, epoch: 3 | loss: 0.0842470\n",
      "\tspeed: 0.1125s/iter; left time: 5417.6274s\n",
      "\titers: 500, epoch: 3 | loss: 0.0646141\n",
      "\tspeed: 0.1144s/iter; left time: 5495.3484s\n",
      "\titers: 600, epoch: 3 | loss: 0.0703670\n",
      "\tspeed: 0.1158s/iter; left time: 5551.9364s\n",
      "\titers: 700, epoch: 3 | loss: 0.1009109\n",
      "\tspeed: 0.1150s/iter; left time: 5502.4580s\n",
      "\titers: 800, epoch: 3 | loss: 0.0748487\n",
      "\tspeed: 0.1155s/iter; left time: 5516.2524s\n",
      "\titers: 900, epoch: 3 | loss: 0.0886928\n",
      "\tspeed: 0.1145s/iter; left time: 5457.6055s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0754743\n",
      "\tspeed: 0.1160s/iter; left time: 5516.3033s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0920176\n",
      "\tspeed: 0.1142s/iter; left time: 5417.5835s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0903100\n",
      "\tspeed: 0.1093s/iter; left time: 5174.3028s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0697832\n",
      "\tspeed: 0.1165s/iter; left time: 5504.6654s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0736806\n",
      "\tspeed: 0.1159s/iter; left time: 5462.9624s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0663067\n",
      "\tspeed: 0.1161s/iter; left time: 5463.4055s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0637103\n",
      "\tspeed: 0.1162s/iter; left time: 5455.8326s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0779963\n",
      "\tspeed: 0.1161s/iter; left time: 5441.1227s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0827028\n",
      "\tspeed: 0.1179s/iter; left time: 5512.7869s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1057388\n",
      "\tspeed: 0.1165s/iter; left time: 5433.2936s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0641804\n",
      "\tspeed: 0.1168s/iter; left time: 5436.3407s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0772232\n",
      "\tspeed: 0.1172s/iter; left time: 5444.7137s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0680376\n",
      "\tspeed: 0.1185s/iter; left time: 5490.3259s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0713496\n",
      "\tspeed: 0.1163s/iter; left time: 5376.3683s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0559037\n",
      "\tspeed: 0.1159s/iter; left time: 5349.9769s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0826620\n",
      "\tspeed: 0.1179s/iter; left time: 5427.1389s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0766963\n",
      "\tspeed: 0.1165s/iter; left time: 5351.9525s\n",
      "Epoch: 3 cost time: 00h:05m:12.54s\n",
      "Epoch: 3 | Train Loss: 0.0764710 Vali Loss: 0.0630326 Test Loss: 0.0662999\n",
      "Validation loss decreased (0.063586 --> 0.063033).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0755529\n",
      "\tspeed: 1.0027s/iter; left time: 45872.9982s\n",
      "\titers: 200, epoch: 4 | loss: 0.0685682\n",
      "\tspeed: 0.1149s/iter; left time: 5243.1481s\n",
      "\titers: 300, epoch: 4 | loss: 0.0985739\n",
      "\tspeed: 0.1156s/iter; left time: 5266.3793s\n",
      "\titers: 400, epoch: 4 | loss: 0.0769638\n",
      "\tspeed: 0.1143s/iter; left time: 5194.2122s\n",
      "\titers: 500, epoch: 4 | loss: 0.0797975\n",
      "\tspeed: 0.1152s/iter; left time: 5224.8052s\n",
      "\titers: 600, epoch: 4 | loss: 0.0556481\n",
      "\tspeed: 0.1143s/iter; left time: 5171.0541s\n",
      "\titers: 700, epoch: 4 | loss: 0.0617126\n",
      "\tspeed: 0.1145s/iter; left time: 5170.3960s\n",
      "\titers: 800, epoch: 4 | loss: 0.0742233\n",
      "\tspeed: 0.1145s/iter; left time: 5157.4712s\n",
      "\titers: 900, epoch: 4 | loss: 0.0639015\n",
      "\tspeed: 0.1143s/iter; left time: 5135.6826s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1003068\n",
      "\tspeed: 0.1155s/iter; left time: 5180.0391s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0659704\n",
      "\tspeed: 0.1157s/iter; left time: 5176.3203s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0864620\n",
      "\tspeed: 0.1156s/iter; left time: 5162.7542s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0829317\n",
      "\tspeed: 0.1178s/iter; left time: 5249.3917s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0683253\n",
      "\tspeed: 0.1154s/iter; left time: 5129.9348s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0755579\n",
      "\tspeed: 0.1159s/iter; left time: 5142.2098s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0791848\n",
      "\tspeed: 0.1155s/iter; left time: 5112.1593s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0764505\n",
      "\tspeed: 0.1153s/iter; left time: 5089.1324s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0608163\n",
      "\tspeed: 0.1147s/iter; left time: 5050.6747s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0749248\n",
      "\tspeed: 0.1144s/iter; left time: 5028.3983s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0778898\n",
      "\tspeed: 0.1146s/iter; left time: 5025.2626s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0662812\n",
      "\tspeed: 0.1173s/iter; left time: 5131.2864s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0748827\n",
      "\tspeed: 0.1107s/iter; left time: 4833.3044s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0675273\n",
      "\tspeed: 0.0966s/iter; left time: 4208.4745s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0791746\n",
      "\tspeed: 0.1056s/iter; left time: 4589.5804s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0636560\n",
      "\tspeed: 0.1145s/iter; left time: 4965.6911s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0793808\n",
      "\tspeed: 0.1169s/iter; left time: 5055.9000s\n",
      "Epoch: 4 cost time: 00h:05m:08.24s\n",
      "Epoch: 4 | Train Loss: 0.0742877 Vali Loss: 0.0603242 Test Loss: 0.0631236\n",
      "Validation loss decreased (0.063033 --> 0.060324).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0666447\n",
      "\tspeed: 1.0072s/iter; left time: 43364.4964s\n",
      "\titers: 200, epoch: 5 | loss: 0.0850932\n",
      "\tspeed: 0.1146s/iter; left time: 4921.1163s\n",
      "\titers: 300, epoch: 5 | loss: 0.0762963\n",
      "\tspeed: 0.1154s/iter; left time: 4945.3224s\n",
      "\titers: 400, epoch: 5 | loss: 0.0729807\n",
      "\tspeed: 0.1155s/iter; left time: 4938.5099s\n",
      "\titers: 500, epoch: 5 | loss: 0.0696062\n",
      "\tspeed: 0.1161s/iter; left time: 4951.8754s\n",
      "\titers: 600, epoch: 5 | loss: 0.0712013\n",
      "\tspeed: 0.1155s/iter; left time: 4915.4375s\n",
      "\titers: 700, epoch: 5 | loss: 0.0854386\n",
      "\tspeed: 0.1150s/iter; left time: 4882.3641s\n",
      "\titers: 800, epoch: 5 | loss: 0.0720558\n",
      "\tspeed: 0.1146s/iter; left time: 4853.2534s\n",
      "\titers: 900, epoch: 5 | loss: 0.0693585\n",
      "\tspeed: 0.1140s/iter; left time: 4815.9188s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0752329\n",
      "\tspeed: 0.1149s/iter; left time: 4842.3688s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0633822\n",
      "\tspeed: 0.1150s/iter; left time: 4834.9946s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0723745\n",
      "\tspeed: 0.1152s/iter; left time: 4834.8618s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0627828\n",
      "\tspeed: 0.1158s/iter; left time: 4847.3634s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0818627\n",
      "\tspeed: 0.1171s/iter; left time: 4887.8201s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0792903\n",
      "\tspeed: 0.1173s/iter; left time: 4885.9770s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0754565\n",
      "\tspeed: 0.1153s/iter; left time: 4792.2546s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0597070\n",
      "\tspeed: 0.1140s/iter; left time: 4723.8120s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0933210\n",
      "\tspeed: 0.1154s/iter; left time: 4770.6114s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0578079\n",
      "\tspeed: 0.1153s/iter; left time: 4757.5930s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0638854\n",
      "\tspeed: 0.1153s/iter; left time: 4746.6100s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0707066\n",
      "\tspeed: 0.1152s/iter; left time: 4729.5655s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0822564\n",
      "\tspeed: 0.1157s/iter; left time: 4737.3070s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0747746\n",
      "\tspeed: 0.1167s/iter; left time: 4767.0517s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0669004\n",
      "\tspeed: 0.1163s/iter; left time: 4738.6961s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0690131\n",
      "\tspeed: 0.1156s/iter; left time: 4698.4356s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0732940\n",
      "\tspeed: 0.1160s/iter; left time: 4702.6721s\n",
      "Epoch: 5 cost time: 00h:05m:11.60s\n",
      "Epoch: 5 | Train Loss: 0.0726861 Vali Loss: 0.0594335 Test Loss: 0.0628666\n",
      "Validation loss decreased (0.060324 --> 0.059433).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0712747\n",
      "\tspeed: 1.0098s/iter; left time: 40751.4645s\n",
      "\titers: 200, epoch: 6 | loss: 0.0677786\n",
      "\tspeed: 0.1160s/iter; left time: 4669.5459s\n",
      "\titers: 300, epoch: 6 | loss: 0.0659821\n",
      "\tspeed: 0.1129s/iter; left time: 4531.9930s\n",
      "\titers: 400, epoch: 6 | loss: 0.0641622\n",
      "\tspeed: 0.0966s/iter; left time: 3869.5682s\n",
      "\titers: 500, epoch: 6 | loss: 0.0645634\n",
      "\tspeed: 0.1138s/iter; left time: 4546.2729s\n",
      "\titers: 600, epoch: 6 | loss: 0.0953365\n",
      "\tspeed: 0.1147s/iter; left time: 4570.0293s\n",
      "\titers: 700, epoch: 6 | loss: 0.0762194\n",
      "\tspeed: 0.1156s/iter; left time: 4594.3468s\n",
      "\titers: 800, epoch: 6 | loss: 0.0763267\n",
      "\tspeed: 0.1150s/iter; left time: 4558.6683s\n",
      "\titers: 900, epoch: 6 | loss: 0.0695393\n",
      "\tspeed: 0.1166s/iter; left time: 4611.9340s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0557512\n",
      "\tspeed: 0.1150s/iter; left time: 4536.4202s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0601773\n",
      "\tspeed: 0.1157s/iter; left time: 4553.5366s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0580391\n",
      "\tspeed: 0.1148s/iter; left time: 4507.3691s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0789730\n",
      "\tspeed: 0.1146s/iter; left time: 4488.8459s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0700168\n",
      "\tspeed: 0.1151s/iter; left time: 4494.8010s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0772208\n",
      "\tspeed: 0.1163s/iter; left time: 4528.7595s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0704373\n",
      "\tspeed: 0.1179s/iter; left time: 4581.1736s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0689978\n",
      "\tspeed: 0.1154s/iter; left time: 4473.1068s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0662989\n",
      "\tspeed: 0.1172s/iter; left time: 4529.5006s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0793340\n",
      "\tspeed: 0.1157s/iter; left time: 4459.1564s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0628580\n",
      "\tspeed: 0.1159s/iter; left time: 4458.5505s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0749498\n",
      "\tspeed: 0.1148s/iter; left time: 4402.8768s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0798656\n",
      "\tspeed: 0.1170s/iter; left time: 4474.1263s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0600675\n",
      "\tspeed: 0.1153s/iter; left time: 4398.9489s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0627799\n",
      "\tspeed: 0.1141s/iter; left time: 4343.0951s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0818583\n",
      "\tspeed: 0.1154s/iter; left time: 4378.3304s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0600302\n",
      "\tspeed: 0.1018s/iter; left time: 3852.0382s\n",
      "Epoch: 6 cost time: 00h:05m:06.58s\n",
      "Epoch: 6 | Train Loss: 0.0715059 Vali Loss: 0.0587914 Test Loss: 0.0618361\n",
      "Validation loss decreased (0.059433 --> 0.058791).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0741726\n",
      "\tspeed: 0.9898s/iter; left time: 37275.9424s\n",
      "\titers: 200, epoch: 7 | loss: 0.0748587\n",
      "\tspeed: 0.1156s/iter; left time: 4340.0231s\n",
      "\titers: 300, epoch: 7 | loss: 0.0780735\n",
      "\tspeed: 0.1167s/iter; left time: 4372.8185s\n",
      "\titers: 400, epoch: 7 | loss: 0.0651705\n",
      "\tspeed: 0.1146s/iter; left time: 4280.4686s\n",
      "\titers: 500, epoch: 7 | loss: 0.0722456\n",
      "\tspeed: 0.1109s/iter; left time: 4132.7038s\n",
      "\titers: 600, epoch: 7 | loss: 0.0781499\n",
      "\tspeed: 0.1054s/iter; left time: 3915.9669s\n",
      "\titers: 700, epoch: 7 | loss: 0.0795180\n",
      "\tspeed: 0.0999s/iter; left time: 3702.2645s\n",
      "\titers: 800, epoch: 7 | loss: 0.0717324\n",
      "\tspeed: 0.1112s/iter; left time: 4108.1203s\n",
      "\titers: 900, epoch: 7 | loss: 0.0713158\n",
      "\tspeed: 0.1148s/iter; left time: 4230.6673s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0723086\n",
      "\tspeed: 0.1154s/iter; left time: 4243.1740s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0738894\n",
      "\tspeed: 0.1151s/iter; left time: 4219.7402s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0729173\n",
      "\tspeed: 0.1155s/iter; left time: 4222.6904s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0678999\n",
      "\tspeed: 0.1167s/iter; left time: 4256.3744s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0747445\n",
      "\tspeed: 0.1152s/iter; left time: 4187.8987s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0734894\n",
      "\tspeed: 0.1132s/iter; left time: 4106.2845s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0665492\n",
      "\tspeed: 0.1158s/iter; left time: 4188.5237s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0577601\n",
      "\tspeed: 0.1139s/iter; left time: 4106.2015s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0670582\n",
      "\tspeed: 0.1161s/iter; left time: 4174.8886s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0767235\n",
      "\tspeed: 0.1158s/iter; left time: 4152.4629s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0678485\n",
      "\tspeed: 0.1166s/iter; left time: 4168.5701s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0605740\n",
      "\tspeed: 0.1173s/iter; left time: 4181.9423s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0644402\n",
      "\tspeed: 0.1171s/iter; left time: 4162.1812s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0643342\n",
      "\tspeed: 0.1157s/iter; left time: 4101.2533s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0800190\n",
      "\tspeed: 0.1146s/iter; left time: 4051.4881s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0799797\n",
      "\tspeed: 0.1153s/iter; left time: 4064.3580s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0731578\n",
      "\tspeed: 0.1155s/iter; left time: 4060.5084s\n",
      "Epoch: 7 cost time: 00h:05m:08.92s\n",
      "Epoch: 7 | Train Loss: 0.0705104 Vali Loss: 0.0588274 Test Loss: 0.0623315\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0723313\n",
      "\tspeed: 0.9858s/iter; left time: 34464.8020s\n",
      "\titers: 200, epoch: 8 | loss: 0.0710911\n",
      "\tspeed: 0.1143s/iter; left time: 3983.3980s\n",
      "\titers: 300, epoch: 8 | loss: 0.0671540\n",
      "\tspeed: 0.1150s/iter; left time: 3998.1304s\n",
      "\titers: 400, epoch: 8 | loss: 0.0813148\n",
      "\tspeed: 0.1145s/iter; left time: 3969.5058s\n",
      "\titers: 500, epoch: 8 | loss: 0.0627014\n",
      "\tspeed: 0.1151s/iter; left time: 3978.6193s\n",
      "\titers: 600, epoch: 8 | loss: 0.0611403\n",
      "\tspeed: 0.1141s/iter; left time: 3931.4380s\n",
      "\titers: 700, epoch: 8 | loss: 0.0833048\n",
      "\tspeed: 0.1146s/iter; left time: 3939.2204s\n",
      "\titers: 800, epoch: 8 | loss: 0.0658636\n",
      "\tspeed: 0.1141s/iter; left time: 3908.3466s\n",
      "\titers: 900, epoch: 8 | loss: 0.0716917\n",
      "\tspeed: 0.1140s/iter; left time: 3895.7594s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0793457\n",
      "\tspeed: 0.1154s/iter; left time: 3929.5973s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0693654\n",
      "\tspeed: 0.1138s/iter; left time: 3863.4662s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0896589\n",
      "\tspeed: 0.1143s/iter; left time: 3869.8106s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0923285\n",
      "\tspeed: 0.1043s/iter; left time: 3520.1031s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0691433\n",
      "\tspeed: 0.1152s/iter; left time: 3876.5073s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0635347\n",
      "\tspeed: 0.1170s/iter; left time: 3926.9469s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0756910\n",
      "\tspeed: 0.1167s/iter; left time: 3903.9346s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0683165\n",
      "\tspeed: 0.1173s/iter; left time: 3912.8210s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0640305\n",
      "\tspeed: 0.1116s/iter; left time: 3713.2979s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0605875\n",
      "\tspeed: 0.1162s/iter; left time: 3853.8823s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0707027\n",
      "\tspeed: 0.1100s/iter; left time: 3637.5042s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0703436\n",
      "\tspeed: 0.1113s/iter; left time: 3668.6883s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0644438\n",
      "\tspeed: 0.1153s/iter; left time: 3788.0077s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0740883\n",
      "\tspeed: 0.1120s/iter; left time: 3668.1759s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0688714\n",
      "\tspeed: 0.1159s/iter; left time: 3786.6674s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0841499\n",
      "\tspeed: 0.1163s/iter; left time: 3787.9676s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0752901\n",
      "\tspeed: 0.1151s/iter; left time: 3736.7459s\n",
      "Epoch: 8 cost time: 00h:05m:08.57s\n",
      "Epoch: 8 | Train Loss: 0.0698472 Vali Loss: 0.0581357 Test Loss: 0.0614739\n",
      "Validation loss decreased (0.058791 --> 0.058136).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0787969\n",
      "\tspeed: 1.0198s/iter; left time: 32905.3798s\n",
      "\titers: 200, epoch: 9 | loss: 0.0585354\n",
      "\tspeed: 0.1158s/iter; left time: 3726.0159s\n",
      "\titers: 300, epoch: 9 | loss: 0.0858474\n",
      "\tspeed: 0.1160s/iter; left time: 3720.9209s\n",
      "\titers: 400, epoch: 9 | loss: 0.0597564\n",
      "\tspeed: 0.1168s/iter; left time: 3732.4743s\n",
      "\titers: 500, epoch: 9 | loss: 0.0880816\n",
      "\tspeed: 0.1147s/iter; left time: 3656.4618s\n",
      "\titers: 600, epoch: 9 | loss: 0.0697973\n",
      "\tspeed: 0.1143s/iter; left time: 3629.9317s\n",
      "\titers: 700, epoch: 9 | loss: 0.0531395\n",
      "\tspeed: 0.1137s/iter; left time: 3601.7995s\n",
      "\titers: 800, epoch: 9 | loss: 0.0829247\n",
      "\tspeed: 0.1131s/iter; left time: 3571.4335s\n",
      "\titers: 900, epoch: 9 | loss: 0.0627392\n",
      "\tspeed: 0.1139s/iter; left time: 3583.4731s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0607399\n",
      "\tspeed: 0.1139s/iter; left time: 3573.6254s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0579930\n",
      "\tspeed: 0.1143s/iter; left time: 3573.2211s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0707458\n",
      "\tspeed: 0.1140s/iter; left time: 3551.6134s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0769623\n",
      "\tspeed: 0.0971s/iter; left time: 3015.7709s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0854174\n",
      "\tspeed: 0.0980s/iter; left time: 3033.3488s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0642810\n",
      "\tspeed: 0.1155s/iter; left time: 3565.7086s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0589194\n",
      "\tspeed: 0.1082s/iter; left time: 3329.5130s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0693697\n",
      "\tspeed: 0.1096s/iter; left time: 3360.7651s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0666921\n",
      "\tspeed: 0.1132s/iter; left time: 3459.6487s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0715291\n",
      "\tspeed: 0.1159s/iter; left time: 3529.5364s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0702265\n",
      "\tspeed: 0.1149s/iter; left time: 3490.3398s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0707251\n",
      "\tspeed: 0.1154s/iter; left time: 3493.2598s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0606815\n",
      "\tspeed: 0.1157s/iter; left time: 3490.7647s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0693892\n",
      "\tspeed: 0.1145s/iter; left time: 3441.3629s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0803939\n",
      "\tspeed: 0.1162s/iter; left time: 3481.7337s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0663976\n",
      "\tspeed: 0.1151s/iter; left time: 3438.7551s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0611548\n",
      "\tspeed: 0.1135s/iter; left time: 3377.7964s\n",
      "Epoch: 9 cost time: 00h:05m:05.59s\n",
      "Epoch: 9 | Train Loss: 0.0691710 Vali Loss: 0.0575196 Test Loss: 0.0609406\n",
      "Validation loss decreased (0.058136 --> 0.057520).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0594188\n",
      "\tspeed: 0.9952s/iter; left time: 29425.7740s\n",
      "\titers: 200, epoch: 10 | loss: 0.0590801\n",
      "\tspeed: 0.1014s/iter; left time: 2989.4648s\n",
      "\titers: 300, epoch: 10 | loss: 0.0680371\n",
      "\tspeed: 0.1140s/iter; left time: 3347.5216s\n",
      "\titers: 400, epoch: 10 | loss: 0.0673472\n",
      "\tspeed: 0.1087s/iter; left time: 3181.0288s\n",
      "\titers: 500, epoch: 10 | loss: 0.0599458\n",
      "\tspeed: 0.1069s/iter; left time: 3117.6651s\n",
      "\titers: 600, epoch: 10 | loss: 0.0861257\n",
      "\tspeed: 0.1146s/iter; left time: 3330.4469s\n",
      "\titers: 700, epoch: 10 | loss: 0.0876099\n",
      "\tspeed: 0.1142s/iter; left time: 3309.2607s\n",
      "\titers: 800, epoch: 10 | loss: 0.0699282\n",
      "\tspeed: 0.1138s/iter; left time: 3285.3269s\n",
      "\titers: 900, epoch: 10 | loss: 0.0586489\n",
      "\tspeed: 0.1147s/iter; left time: 3300.8723s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0759529\n",
      "\tspeed: 0.1148s/iter; left time: 3290.2202s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0668376\n",
      "\tspeed: 0.1092s/iter; left time: 3119.7124s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0535884\n",
      "\tspeed: 0.1031s/iter; left time: 2934.3964s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0582658\n",
      "\tspeed: 0.1138s/iter; left time: 3227.0837s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0513382\n",
      "\tspeed: 0.1130s/iter; left time: 3193.3981s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0559071\n",
      "\tspeed: 0.1137s/iter; left time: 3202.5214s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0776514\n",
      "\tspeed: 0.1151s/iter; left time: 3229.5595s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0712357\n",
      "\tspeed: 0.1041s/iter; left time: 2912.5204s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0629981\n",
      "\tspeed: 0.1156s/iter; left time: 3220.4616s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0640645\n",
      "\tspeed: 0.1089s/iter; left time: 3024.4091s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0740883\n",
      "\tspeed: 0.1007s/iter; left time: 2785.6687s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0667572\n",
      "\tspeed: 0.1143s/iter; left time: 3150.6125s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0742129\n",
      "\tspeed: 0.1027s/iter; left time: 2820.5033s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0638777\n",
      "\tspeed: 0.1020s/iter; left time: 2791.8738s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0583167\n",
      "\tspeed: 0.1136s/iter; left time: 3098.3799s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0623477\n",
      "\tspeed: 0.1136s/iter; left time: 3085.3174s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0761587\n",
      "\tspeed: 0.1009s/iter; left time: 2731.3328s\n",
      "Epoch: 10 cost time: 00h:04m:55.09s\n",
      "Epoch: 10 | Train Loss: 0.0686641 Vali Loss: 0.0578670 Test Loss: 0.0614497\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0688605\n",
      "\tspeed: 0.9634s/iter; left time: 25886.5138s\n",
      "\titers: 200, epoch: 11 | loss: 0.0643643\n",
      "\tspeed: 0.1157s/iter; left time: 3096.3717s\n",
      "\titers: 300, epoch: 11 | loss: 0.0519321\n",
      "\tspeed: 0.1139s/iter; left time: 3038.8410s\n",
      "\titers: 400, epoch: 11 | loss: 0.0803871\n",
      "\tspeed: 0.1149s/iter; left time: 3052.1080s\n",
      "\titers: 500, epoch: 11 | loss: 0.0732333\n",
      "\tspeed: 0.1146s/iter; left time: 3033.9379s\n",
      "\titers: 600, epoch: 11 | loss: 0.0699096\n",
      "\tspeed: 0.1143s/iter; left time: 3015.4589s\n",
      "\titers: 700, epoch: 11 | loss: 0.0627109\n",
      "\tspeed: 0.1135s/iter; left time: 2981.7984s\n",
      "\titers: 800, epoch: 11 | loss: 0.0755273\n",
      "\tspeed: 0.1133s/iter; left time: 2965.6401s\n",
      "\titers: 900, epoch: 11 | loss: 0.0590502\n",
      "\tspeed: 0.1141s/iter; left time: 2974.0376s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0684834\n",
      "\tspeed: 0.0993s/iter; left time: 2578.6103s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0669859\n",
      "\tspeed: 0.1155s/iter; left time: 2988.3733s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0700008\n",
      "\tspeed: 0.1077s/iter; left time: 2775.5692s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0586604\n",
      "\tspeed: 0.1146s/iter; left time: 2941.7344s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0698778\n",
      "\tspeed: 0.1154s/iter; left time: 2951.8426s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0758755\n",
      "\tspeed: 0.1156s/iter; left time: 2944.7544s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0635368\n",
      "\tspeed: 0.1119s/iter; left time: 2837.8673s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0804984\n",
      "\tspeed: 0.1100s/iter; left time: 2778.9005s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0605070\n",
      "\tspeed: 0.1144s/iter; left time: 2878.6505s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0770305\n",
      "\tspeed: 0.1148s/iter; left time: 2877.3141s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0658592\n",
      "\tspeed: 0.1150s/iter; left time: 2870.8323s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0650982\n",
      "\tspeed: 0.1151s/iter; left time: 2863.3911s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0749165\n",
      "\tspeed: 0.1144s/iter; left time: 2832.8202s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0622882\n",
      "\tspeed: 0.1140s/iter; left time: 2812.4483s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0714804\n",
      "\tspeed: 0.1148s/iter; left time: 2820.8064s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0614139\n",
      "\tspeed: 0.1150s/iter; left time: 2814.2477s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0660085\n",
      "\tspeed: 0.1137s/iter; left time: 2769.7957s\n",
      "Epoch: 11 cost time: 00h:05m:06.71s\n",
      "Epoch: 11 | Train Loss: 0.0681724 Vali Loss: 0.0571862 Test Loss: 0.0607081\n",
      "Validation loss decreased (0.057520 --> 0.057186).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0519414\n",
      "\tspeed: 1.0039s/iter; left time: 24267.3512s\n",
      "\titers: 200, epoch: 12 | loss: 0.0603286\n",
      "\tspeed: 0.1136s/iter; left time: 2735.6495s\n",
      "\titers: 300, epoch: 12 | loss: 0.0825475\n",
      "\tspeed: 0.1147s/iter; left time: 2749.4585s\n",
      "\titers: 400, epoch: 12 | loss: 0.0733654\n",
      "\tspeed: 0.1163s/iter; left time: 2775.5488s\n",
      "\titers: 500, epoch: 12 | loss: 0.0657127\n",
      "\tspeed: 0.1162s/iter; left time: 2761.8917s\n",
      "\titers: 600, epoch: 12 | loss: 0.0646338\n",
      "\tspeed: 0.1151s/iter; left time: 2723.8446s\n",
      "\titers: 700, epoch: 12 | loss: 0.0897408\n",
      "\tspeed: 0.1142s/iter; left time: 2690.9921s\n",
      "\titers: 800, epoch: 12 | loss: 0.0609364\n",
      "\tspeed: 0.1145s/iter; left time: 2687.2109s\n",
      "\titers: 900, epoch: 12 | loss: 0.0622800\n",
      "\tspeed: 0.1146s/iter; left time: 2678.0021s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0894761\n",
      "\tspeed: 0.1137s/iter; left time: 2645.4986s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0589442\n",
      "\tspeed: 0.1055s/iter; left time: 2443.9838s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0551907\n",
      "\tspeed: 0.0961s/iter; left time: 2216.2643s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0603157\n",
      "\tspeed: 0.1038s/iter; left time: 2384.6036s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0810170\n",
      "\tspeed: 0.1144s/iter; left time: 2617.3443s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0754087\n",
      "\tspeed: 0.1142s/iter; left time: 2600.1290s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0668139\n",
      "\tspeed: 0.1132s/iter; left time: 2567.3034s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0780367\n",
      "\tspeed: 0.1115s/iter; left time: 2515.9899s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0738580\n",
      "\tspeed: 0.1070s/iter; left time: 2405.6110s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0685480\n",
      "\tspeed: 0.1146s/iter; left time: 2564.0579s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0631595\n",
      "\tspeed: 0.1140s/iter; left time: 2539.9188s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0612001\n",
      "\tspeed: 0.1143s/iter; left time: 2535.3791s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0731911\n",
      "\tspeed: 0.1143s/iter; left time: 2523.8004s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0563691\n",
      "\tspeed: 0.1154s/iter; left time: 2536.1860s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0569280\n",
      "\tspeed: 0.1144s/iter; left time: 2503.0689s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0736074\n",
      "\tspeed: 0.0999s/iter; left time: 2174.2873s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0686894\n",
      "\tspeed: 0.1084s/iter; left time: 2348.5141s\n",
      "Epoch: 12 cost time: 00h:05m:02.47s\n",
      "Epoch: 12 | Train Loss: 0.0676054 Vali Loss: 0.0583766 Test Loss: 0.0617521\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0610672\n",
      "\tspeed: 0.9819s/iter; left time: 21088.5091s\n",
      "\titers: 200, epoch: 13 | loss: 0.0567970\n",
      "\tspeed: 0.1160s/iter; left time: 2479.0005s\n",
      "\titers: 300, epoch: 13 | loss: 0.0503390\n",
      "\tspeed: 0.1150s/iter; left time: 2447.6480s\n",
      "\titers: 400, epoch: 13 | loss: 0.0588858\n",
      "\tspeed: 0.1144s/iter; left time: 2422.5092s\n",
      "\titers: 500, epoch: 13 | loss: 0.0866880\n",
      "\tspeed: 0.1142s/iter; left time: 2407.5235s\n",
      "\titers: 600, epoch: 13 | loss: 0.0590931\n",
      "\tspeed: 0.1146s/iter; left time: 2403.5746s\n",
      "\titers: 700, epoch: 13 | loss: 0.0699925\n",
      "\tspeed: 0.1131s/iter; left time: 2362.1300s\n",
      "\titers: 800, epoch: 13 | loss: 0.0752468\n",
      "\tspeed: 0.1143s/iter; left time: 2373.9407s\n",
      "\titers: 900, epoch: 13 | loss: 0.0647457\n",
      "\tspeed: 0.1146s/iter; left time: 2369.8364s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0609347\n",
      "\tspeed: 0.1144s/iter; left time: 2354.7522s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0517241\n",
      "\tspeed: 0.1137s/iter; left time: 2328.9166s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0599805\n",
      "\tspeed: 0.1142s/iter; left time: 2327.9382s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0654597\n",
      "\tspeed: 0.1142s/iter; left time: 2314.9592s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0604756\n",
      "\tspeed: 0.1142s/iter; left time: 2303.4778s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0713967\n",
      "\tspeed: 0.1155s/iter; left time: 2319.1291s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0787557\n",
      "\tspeed: 0.1158s/iter; left time: 2313.8345s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0548762\n",
      "\tspeed: 0.1150s/iter; left time: 2285.9566s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0690363\n",
      "\tspeed: 0.1153s/iter; left time: 2279.3040s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0558236\n",
      "\tspeed: 0.1152s/iter; left time: 2265.9576s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0533169\n",
      "\tspeed: 0.1135s/iter; left time: 2221.9672s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0763264\n",
      "\tspeed: 0.1144s/iter; left time: 2228.5516s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0767449\n",
      "\tspeed: 0.1153s/iter; left time: 2234.6715s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0773105\n",
      "\tspeed: 0.1139s/iter; left time: 2195.0481s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0802446\n",
      "\tspeed: 0.1137s/iter; left time: 2180.0878s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0897281\n",
      "\tspeed: 0.1136s/iter; left time: 2166.2115s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0587957\n",
      "\tspeed: 0.1148s/iter; left time: 2178.5897s\n",
      "Epoch: 13 cost time: 00h:05m:09.56s\n",
      "Epoch: 13 | Train Loss: 0.0671900 Vali Loss: 0.0578623 Test Loss: 0.0615834\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0538025\n",
      "\tspeed: 0.9785s/iter; left time: 18377.0368s\n",
      "\titers: 200, epoch: 14 | loss: 0.0614343\n",
      "\tspeed: 0.1136s/iter; left time: 2121.2640s\n",
      "\titers: 300, epoch: 14 | loss: 0.0657694\n",
      "\tspeed: 0.1144s/iter; left time: 2126.3909s\n",
      "\titers: 400, epoch: 14 | loss: 0.0752635\n",
      "\tspeed: 0.1155s/iter; left time: 2134.0448s\n",
      "\titers: 500, epoch: 14 | loss: 0.0564194\n",
      "\tspeed: 0.1159s/iter; left time: 2129.7759s\n",
      "\titers: 600, epoch: 14 | loss: 0.0707608\n",
      "\tspeed: 0.1155s/iter; left time: 2110.6360s\n",
      "\titers: 700, epoch: 14 | loss: 0.0544968\n",
      "\tspeed: 0.1148s/iter; left time: 2086.5129s\n",
      "\titers: 800, epoch: 14 | loss: 0.0735052\n",
      "\tspeed: 0.1150s/iter; left time: 2079.5721s\n",
      "\titers: 900, epoch: 14 | loss: 0.0844271\n",
      "\tspeed: 0.1140s/iter; left time: 2050.4539s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0708888\n",
      "\tspeed: 0.1129s/iter; left time: 2019.2231s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0629603\n",
      "\tspeed: 0.1145s/iter; left time: 2035.5142s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0756200\n",
      "\tspeed: 0.1146s/iter; left time: 2026.9095s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0599315\n",
      "\tspeed: 0.1149s/iter; left time: 2019.8470s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0612757\n",
      "\tspeed: 0.1146s/iter; left time: 2003.0199s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0702794\n",
      "\tspeed: 0.1139s/iter; left time: 1979.1056s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0578222\n",
      "\tspeed: 0.1139s/iter; left time: 1968.7100s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0617748\n",
      "\tspeed: 0.1166s/iter; left time: 2002.9887s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0585543\n",
      "\tspeed: 0.1154s/iter; left time: 1971.6422s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0675614\n",
      "\tspeed: 0.1146s/iter; left time: 1946.0450s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0662966\n",
      "\tspeed: 0.1155s/iter; left time: 1949.4416s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0603743\n",
      "\tspeed: 0.1146s/iter; left time: 1923.5369s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0601280\n",
      "\tspeed: 0.1140s/iter; left time: 1902.0830s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0591565\n",
      "\tspeed: 0.1167s/iter; left time: 1935.6409s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0908771\n",
      "\tspeed: 0.1146s/iter; left time: 1888.3587s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0688252\n",
      "\tspeed: 0.1142s/iter; left time: 1871.1532s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0583719\n",
      "\tspeed: 0.1147s/iter; left time: 1867.9632s\n",
      "Epoch: 14 cost time: 00h:05m:10.11s\n",
      "Epoch: 14 | Train Loss: 0.0668031 Vali Loss: 0.0573795 Test Loss: 0.0608568\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0688373\n",
      "\tspeed: 0.9843s/iter; left time: 15830.7326s\n",
      "\titers: 200, epoch: 15 | loss: 0.0569479\n",
      "\tspeed: 0.0966s/iter; left time: 1544.1532s\n",
      "\titers: 300, epoch: 15 | loss: 0.0754607\n",
      "\tspeed: 0.1126s/iter; left time: 1787.9768s\n",
      "\titers: 400, epoch: 15 | loss: 0.0596411\n",
      "\tspeed: 0.1138s/iter; left time: 1795.5628s\n",
      "\titers: 500, epoch: 15 | loss: 0.0809594\n",
      "\tspeed: 0.1149s/iter; left time: 1801.2614s\n",
      "\titers: 600, epoch: 15 | loss: 0.0602640\n",
      "\tspeed: 0.1147s/iter; left time: 1787.3242s\n",
      "\titers: 700, epoch: 15 | loss: 0.0620752\n",
      "\tspeed: 0.1161s/iter; left time: 1796.8484s\n",
      "\titers: 800, epoch: 15 | loss: 0.0750819\n",
      "\tspeed: 0.1136s/iter; left time: 1747.5055s\n",
      "\titers: 900, epoch: 15 | loss: 0.0663498\n",
      "\tspeed: 0.1142s/iter; left time: 1745.0218s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0724144\n",
      "\tspeed: 0.1118s/iter; left time: 1696.9032s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0621678\n",
      "\tspeed: 0.1136s/iter; left time: 1714.0208s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0722131\n",
      "\tspeed: 0.1131s/iter; left time: 1694.7276s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0759164\n",
      "\tspeed: 0.1176s/iter; left time: 1749.5838s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0824906\n",
      "\tspeed: 0.1154s/iter; left time: 1706.6226s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0806423\n",
      "\tspeed: 0.1166s/iter; left time: 1712.4875s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0617313\n",
      "\tspeed: 0.1157s/iter; left time: 1686.5711s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0703903\n",
      "\tspeed: 0.1147s/iter; left time: 1661.7245s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0691926\n",
      "\tspeed: 0.1157s/iter; left time: 1664.3588s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0609231\n",
      "\tspeed: 0.1138s/iter; left time: 1625.7338s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0674969\n",
      "\tspeed: 0.1137s/iter; left time: 1612.7425s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0743056\n",
      "\tspeed: 0.1155s/iter; left time: 1626.9866s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0678914\n",
      "\tspeed: 0.1144s/iter; left time: 1599.4736s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0633814\n",
      "\tspeed: 0.1143s/iter; left time: 1587.3052s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0677302\n",
      "\tspeed: 0.1159s/iter; left time: 1597.2985s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0633345\n",
      "\tspeed: 0.1145s/iter; left time: 1567.1010s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0611144\n",
      "\tspeed: 0.1171s/iter; left time: 1590.4667s\n",
      "Epoch: 15 cost time: 00h:05m:08.48s\n",
      "Epoch: 15 | Train Loss: 0.0663890 Vali Loss: 0.0575977 Test Loss: 0.0608814\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0731103\n",
      "\tspeed: 0.9861s/iter; left time: 13200.5695s\n",
      "\titers: 200, epoch: 16 | loss: 0.0706104\n",
      "\tspeed: 0.1138s/iter; left time: 1512.1604s\n",
      "\titers: 300, epoch: 16 | loss: 0.0581224\n",
      "\tspeed: 0.1156s/iter; left time: 1523.6973s\n",
      "\titers: 400, epoch: 16 | loss: 0.0628917\n",
      "\tspeed: 0.1159s/iter; left time: 1517.1239s\n",
      "\titers: 500, epoch: 16 | loss: 0.0536247\n",
      "\tspeed: 0.1051s/iter; left time: 1364.6688s\n",
      "\titers: 600, epoch: 16 | loss: 0.0669830\n",
      "\tspeed: 0.0966s/iter; left time: 1245.1209s\n",
      "\titers: 700, epoch: 16 | loss: 0.0593961\n",
      "\tspeed: 0.0966s/iter; left time: 1235.0858s\n",
      "\titers: 800, epoch: 16 | loss: 0.0518044\n",
      "\tspeed: 0.0966s/iter; left time: 1225.9406s\n",
      "\titers: 900, epoch: 16 | loss: 0.0667260\n",
      "\tspeed: 0.0963s/iter; left time: 1212.3648s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0675460\n",
      "\tspeed: 0.1111s/iter; left time: 1387.4215s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0571950\n",
      "\tspeed: 0.1155s/iter; left time: 1430.7804s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0801374\n",
      "\tspeed: 0.1140s/iter; left time: 1400.3314s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0643721\n",
      "\tspeed: 0.1145s/iter; left time: 1395.2823s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0662640\n",
      "\tspeed: 0.1159s/iter; left time: 1400.8208s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0630720\n",
      "\tspeed: 0.1149s/iter; left time: 1377.4275s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0607125\n",
      "\tspeed: 0.1125s/iter; left time: 1337.7461s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0707747\n",
      "\tspeed: 0.1155s/iter; left time: 1361.0946s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0875769\n",
      "\tspeed: 0.1150s/iter; left time: 1343.7932s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0685269\n",
      "\tspeed: 0.1181s/iter; left time: 1368.3147s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0532361\n",
      "\tspeed: 0.1156s/iter; left time: 1327.9820s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0648946\n",
      "\tspeed: 0.1152s/iter; left time: 1311.1761s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0554299\n",
      "\tspeed: 0.1138s/iter; left time: 1284.7339s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0634180\n",
      "\tspeed: 0.1142s/iter; left time: 1276.9366s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0690501\n",
      "\tspeed: 0.1145s/iter; left time: 1269.7447s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0580355\n",
      "\tspeed: 0.1141s/iter; left time: 1253.0987s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0707478\n",
      "\tspeed: 0.1141s/iter; left time: 1241.5975s\n",
      "Epoch: 16 cost time: 00h:05m:01.80s\n",
      "Epoch: 16 | Train Loss: 0.0658955 Vali Loss: 0.0577615 Test Loss: 0.0611815\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010438021272420883, rmse:0.10216663777828217, mae:0.060708094388246536, rse:0.3860151767730713\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 24: 01h:44m:02.92s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 22:45:29,624] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 22:45:30,848] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 22:45:30,848] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 22:45:30,849] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 22:45:30,950] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 22:45:30,950] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 22:45:31,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 22:45:31,619] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 22:45:31,619] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 22:45:31,621] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 22:45:31,621] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 22:45:31,621] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 22:45:31,622] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 22:45:31,951] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 22:45:31,952] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:31,952] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,068] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 22:45:32,069] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:32,069] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,069] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 22:45:32,182] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 22:45:32,183] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:32,183] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa370d9f1d0>\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1934633\n",
      "\tspeed: 0.1713s/iter; left time: 9200.5937s\n",
      "\titers: 200, epoch: 1 | loss: 0.1763859\n",
      "\tspeed: 0.1289s/iter; left time: 6911.9084s\n",
      "\titers: 300, epoch: 1 | loss: 0.1493364\n",
      "\tspeed: 0.1278s/iter; left time: 6839.0948s\n",
      "\titers: 400, epoch: 1 | loss: 0.1262816\n",
      "\tspeed: 0.1280s/iter; left time: 6839.0464s\n",
      "\titers: 500, epoch: 1 | loss: 0.1171123\n",
      "\tspeed: 0.1287s/iter; left time: 6863.6276s\n",
      "\titers: 600, epoch: 1 | loss: 0.1207857\n",
      "\tspeed: 0.1256s/iter; left time: 6686.9004s\n",
      "\titers: 700, epoch: 1 | loss: 0.1195831\n",
      "\tspeed: 0.1267s/iter; left time: 6732.0809s\n",
      "\titers: 800, epoch: 1 | loss: 0.1091291\n",
      "\tspeed: 0.1166s/iter; left time: 6181.7215s\n",
      "\titers: 900, epoch: 1 | loss: 0.1172049\n",
      "\tspeed: 0.1168s/iter; left time: 6182.9086s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0856876\n",
      "\tspeed: 0.1073s/iter; left time: 5668.7135s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1010989\n",
      "\tspeed: 0.1078s/iter; left time: 5682.7896s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1015837\n",
      "\tspeed: 0.1073s/iter; left time: 5645.8179s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1383912\n",
      "\tspeed: 0.1071s/iter; left time: 5625.1548s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1047541\n",
      "\tspeed: 0.1073s/iter; left time: 5626.4303s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1040020\n",
      "\tspeed: 0.1260s/iter; left time: 6590.7064s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0965157\n",
      "\tspeed: 0.1259s/iter; left time: 6576.2054s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1022582\n",
      "\tspeed: 0.1265s/iter; left time: 6592.7620s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1087027\n",
      "\tspeed: 0.1243s/iter; left time: 6467.8052s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1015980\n",
      "\tspeed: 0.1261s/iter; left time: 6546.8714s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0960999\n",
      "\tspeed: 0.1262s/iter; left time: 6537.8271s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0915355\n",
      "\tspeed: 0.1257s/iter; left time: 6502.1847s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1123403\n",
      "\tspeed: 0.1250s/iter; left time: 6453.1353s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1265393\n",
      "\tspeed: 0.1274s/iter; left time: 6563.0584s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0845973\n",
      "\tspeed: 0.1267s/iter; left time: 6514.9429s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0974592\n",
      "\tspeed: 0.1262s/iter; left time: 6478.6545s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1181518\n",
      "\tspeed: 0.1261s/iter; left time: 6457.0195s\n",
      "Epoch: 1 cost time: 00h:05m:30.32s\n",
      "Epoch: 1 | Train Loss: 0.1179917 Vali Loss: 0.0848198 Test Loss: 0.0898159\n",
      "Validation loss decreased (inf --> 0.084820).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1002837\n",
      "\tspeed: 1.0899s/iter; left time: 55615.3095s\n",
      "\titers: 200, epoch: 2 | loss: 0.0906165\n",
      "\tspeed: 0.0962s/iter; left time: 4900.1911s\n",
      "\titers: 300, epoch: 2 | loss: 0.0938583\n",
      "\tspeed: 0.1143s/iter; left time: 5811.7729s\n",
      "\titers: 400, epoch: 2 | loss: 0.0962128\n",
      "\tspeed: 0.1157s/iter; left time: 5871.3165s\n",
      "\titers: 500, epoch: 2 | loss: 0.0942962\n",
      "\tspeed: 0.1150s/iter; left time: 5821.1903s\n",
      "\titers: 600, epoch: 2 | loss: 0.1065316\n",
      "\tspeed: 0.1148s/iter; left time: 5802.8616s\n",
      "\titers: 700, epoch: 2 | loss: 0.0971361\n",
      "\tspeed: 0.1161s/iter; left time: 5856.9167s\n",
      "\titers: 800, epoch: 2 | loss: 0.1060096\n",
      "\tspeed: 0.1163s/iter; left time: 5852.4864s\n",
      "\titers: 900, epoch: 2 | loss: 0.1037547\n",
      "\tspeed: 0.1165s/iter; left time: 5853.7284s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1060831\n",
      "\tspeed: 0.1159s/iter; left time: 5812.3636s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1103433\n",
      "\tspeed: 0.1150s/iter; left time: 5753.1402s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1000298\n",
      "\tspeed: 0.1163s/iter; left time: 5809.2005s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1118626\n",
      "\tspeed: 0.1156s/iter; left time: 5758.9532s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0781817\n",
      "\tspeed: 0.1153s/iter; left time: 5734.5130s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0979162\n",
      "\tspeed: 0.1160s/iter; left time: 5758.2457s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0903831\n",
      "\tspeed: 0.1141s/iter; left time: 5649.7216s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0924969\n",
      "\tspeed: 0.1141s/iter; left time: 5639.3169s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1178317\n",
      "\tspeed: 0.1146s/iter; left time: 5653.3441s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0728766\n",
      "\tspeed: 0.1094s/iter; left time: 5385.8710s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0894050\n",
      "\tspeed: 0.1046s/iter; left time: 5139.9066s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1005967\n",
      "\tspeed: 0.1115s/iter; left time: 5467.9781s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0853283\n",
      "\tspeed: 0.1141s/iter; left time: 5581.4293s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0847278\n",
      "\tspeed: 0.1139s/iter; left time: 5559.7386s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1019449\n",
      "\tspeed: 0.1153s/iter; left time: 5620.5961s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0932351\n",
      "\tspeed: 0.1152s/iter; left time: 5600.7076s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0873921\n",
      "\tspeed: 0.1167s/iter; left time: 5663.7633s\n",
      "Epoch: 2 cost time: 00h:05m:04.86s\n",
      "Epoch: 2 | Train Loss: 0.0962975 Vali Loss: 0.0798545 Test Loss: 0.0858662\n",
      "Validation loss decreased (0.084820 --> 0.079855).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0884367\n",
      "\tspeed: 0.9828s/iter; left time: 47505.5496s\n",
      "\titers: 200, epoch: 3 | loss: 0.1001280\n",
      "\tspeed: 0.1150s/iter; left time: 5549.5756s\n",
      "\titers: 300, epoch: 3 | loss: 0.1046731\n",
      "\tspeed: 0.1074s/iter; left time: 5171.4595s\n",
      "\titers: 400, epoch: 3 | loss: 0.0806120\n",
      "\tspeed: 0.0970s/iter; left time: 4660.2030s\n",
      "\titers: 500, epoch: 3 | loss: 0.0833375\n",
      "\tspeed: 0.0965s/iter; left time: 4624.8956s\n",
      "\titers: 600, epoch: 3 | loss: 0.0915050\n",
      "\tspeed: 0.1107s/iter; left time: 5293.4542s\n",
      "\titers: 700, epoch: 3 | loss: 0.0814567\n",
      "\tspeed: 0.1144s/iter; left time: 5462.7055s\n",
      "\titers: 800, epoch: 3 | loss: 0.0916979\n",
      "\tspeed: 0.1150s/iter; left time: 5477.3659s\n",
      "\titers: 900, epoch: 3 | loss: 0.1094405\n",
      "\tspeed: 0.1170s/iter; left time: 5561.3051s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1011684\n",
      "\tspeed: 0.1181s/iter; left time: 5603.5740s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0865153\n",
      "\tspeed: 0.1163s/iter; left time: 5503.3759s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0769959\n",
      "\tspeed: 0.1185s/iter; left time: 5596.0877s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0918234\n",
      "\tspeed: 0.1152s/iter; left time: 5429.9668s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0899273\n",
      "\tspeed: 0.1158s/iter; left time: 5447.0141s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1062002\n",
      "\tspeed: 0.1168s/iter; left time: 5483.4192s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0708728\n",
      "\tspeed: 0.1159s/iter; left time: 5429.9981s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1091485\n",
      "\tspeed: 0.1166s/iter; left time: 5451.7992s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1023156\n",
      "\tspeed: 0.1149s/iter; left time: 5359.9744s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0731412\n",
      "\tspeed: 0.1085s/iter; left time: 5050.1919s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1124088\n",
      "\tspeed: 0.1142s/iter; left time: 5303.9769s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1252687\n",
      "\tspeed: 0.1169s/iter; left time: 5416.1146s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0836434\n",
      "\tspeed: 0.1153s/iter; left time: 5333.4217s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1059322\n",
      "\tspeed: 0.1165s/iter; left time: 5373.8691s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0889617\n",
      "\tspeed: 0.1159s/iter; left time: 5335.2100s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0884438\n",
      "\tspeed: 0.1146s/iter; left time: 5262.9297s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0789040\n",
      "\tspeed: 0.1152s/iter; left time: 5278.6567s\n",
      "Epoch: 3 cost time: 00h:05m:06.51s\n",
      "Epoch: 3 | Train Loss: 0.0925695 Vali Loss: 0.0794393 Test Loss: 0.0851859\n",
      "Validation loss decreased (0.079855 --> 0.079439).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0944193\n",
      "\tspeed: 0.9894s/iter; left time: 45165.9990s\n",
      "\titers: 200, epoch: 4 | loss: 0.1009860\n",
      "\tspeed: 0.1164s/iter; left time: 5300.7634s\n",
      "\titers: 300, epoch: 4 | loss: 0.1014991\n",
      "\tspeed: 0.1175s/iter; left time: 5338.4891s\n",
      "\titers: 400, epoch: 4 | loss: 0.0945554\n",
      "\tspeed: 0.1069s/iter; left time: 4848.2545s\n",
      "\titers: 500, epoch: 4 | loss: 0.0920349\n",
      "\tspeed: 0.1099s/iter; left time: 4973.2031s\n",
      "\titers: 600, epoch: 4 | loss: 0.0887014\n",
      "\tspeed: 0.1151s/iter; left time: 5194.5193s\n",
      "\titers: 700, epoch: 4 | loss: 0.0846027\n",
      "\tspeed: 0.1151s/iter; left time: 5183.7344s\n",
      "\titers: 800, epoch: 4 | loss: 0.0905010\n",
      "\tspeed: 0.1151s/iter; left time: 5174.5785s\n",
      "\titers: 900, epoch: 4 | loss: 0.0935379\n",
      "\tspeed: 0.1148s/iter; left time: 5148.6376s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0826019\n",
      "\tspeed: 0.1158s/iter; left time: 5181.6581s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1028748\n",
      "\tspeed: 0.1149s/iter; left time: 5131.9953s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1137985\n",
      "\tspeed: 0.1143s/iter; left time: 5090.8303s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0763816\n",
      "\tspeed: 0.1143s/iter; left time: 5078.2930s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0880500\n",
      "\tspeed: 0.1147s/iter; left time: 5085.6903s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0818828\n",
      "\tspeed: 0.1151s/iter; left time: 5093.7656s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0753789\n",
      "\tspeed: 0.1153s/iter; left time: 5089.6542s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0667379\n",
      "\tspeed: 0.1164s/iter; left time: 5126.1695s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0773144\n",
      "\tspeed: 0.1149s/iter; left time: 5048.0574s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0894531\n",
      "\tspeed: 0.1153s/iter; left time: 5057.1730s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0876464\n",
      "\tspeed: 0.1143s/iter; left time: 5000.3282s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0817939\n",
      "\tspeed: 0.1148s/iter; left time: 5010.6727s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0817806\n",
      "\tspeed: 0.1145s/iter; left time: 4988.0128s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1111523\n",
      "\tspeed: 0.1145s/iter; left time: 4974.3493s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1141342\n",
      "\tspeed: 0.1150s/iter; left time: 4986.0679s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1129447\n",
      "\tspeed: 0.1145s/iter; left time: 4952.6372s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0830951\n",
      "\tspeed: 0.1113s/iter; left time: 4803.8555s\n",
      "Epoch: 4 cost time: 00h:05m:08.73s\n",
      "Epoch: 4 | Train Loss: 0.0904820 Vali Loss: 0.0780338 Test Loss: 0.0831292\n",
      "Validation loss decreased (0.079439 --> 0.078034).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0972390\n",
      "\tspeed: 0.9804s/iter; left time: 42113.7149s\n",
      "\titers: 200, epoch: 5 | loss: 0.0866233\n",
      "\tspeed: 0.1155s/iter; left time: 4948.1146s\n",
      "\titers: 300, epoch: 5 | loss: 0.0958727\n",
      "\tspeed: 0.1177s/iter; left time: 5032.4006s\n",
      "\titers: 400, epoch: 5 | loss: 0.0693469\n",
      "\tspeed: 0.1165s/iter; left time: 4968.6235s\n",
      "\titers: 500, epoch: 5 | loss: 0.0849624\n",
      "\tspeed: 0.1164s/iter; left time: 4953.0272s\n",
      "\titers: 600, epoch: 5 | loss: 0.1001985\n",
      "\tspeed: 0.1186s/iter; left time: 5034.6916s\n",
      "\titers: 700, epoch: 5 | loss: 0.1063398\n",
      "\tspeed: 0.1176s/iter; left time: 4980.0291s\n",
      "\titers: 800, epoch: 5 | loss: 0.0999221\n",
      "\tspeed: 0.1158s/iter; left time: 4892.5268s\n",
      "\titers: 900, epoch: 5 | loss: 0.0816023\n",
      "\tspeed: 0.1173s/iter; left time: 4944.0625s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0799536\n",
      "\tspeed: 0.1156s/iter; left time: 4863.7731s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0782519\n",
      "\tspeed: 0.1180s/iter; left time: 4951.3473s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0764363\n",
      "\tspeed: 0.1154s/iter; left time: 4829.2510s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0967658\n",
      "\tspeed: 0.1170s/iter; left time: 4886.6172s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0868528\n",
      "\tspeed: 0.1162s/iter; left time: 4838.9733s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0770575\n",
      "\tspeed: 0.1179s/iter; left time: 4900.6489s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0936656\n",
      "\tspeed: 0.1191s/iter; left time: 4938.2399s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0781666\n",
      "\tspeed: 0.1202s/iter; left time: 4973.1603s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0932333\n",
      "\tspeed: 0.1178s/iter; left time: 4860.0693s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1001875\n",
      "\tspeed: 0.1168s/iter; left time: 4809.1613s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1147297\n",
      "\tspeed: 0.1168s/iter; left time: 4796.2472s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0869296\n",
      "\tspeed: 0.1206s/iter; left time: 4938.1741s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0745400\n",
      "\tspeed: 0.1186s/iter; left time: 4844.6170s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0715365\n",
      "\tspeed: 0.1161s/iter; left time: 4731.8313s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0643543\n",
      "\tspeed: 0.1185s/iter; left time: 4816.8680s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0852049\n",
      "\tspeed: 0.1155s/iter; left time: 4685.3851s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0833014\n",
      "\tspeed: 0.1183s/iter; left time: 4787.8768s\n",
      "Epoch: 5 cost time: 00h:05m:15.74s\n",
      "Epoch: 5 | Train Loss: 0.0887613 Vali Loss: 0.0775704 Test Loss: 0.0839218\n",
      "Validation loss decreased (0.078034 --> 0.077570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0947636\n",
      "\tspeed: 0.9904s/iter; left time: 39880.4076s\n",
      "\titers: 200, epoch: 6 | loss: 0.0871146\n",
      "\tspeed: 0.1148s/iter; left time: 4609.5723s\n",
      "\titers: 300, epoch: 6 | loss: 0.1186312\n",
      "\tspeed: 0.1145s/iter; left time: 4588.0703s\n",
      "\titers: 400, epoch: 6 | loss: 0.1023487\n",
      "\tspeed: 0.1146s/iter; left time: 4581.3191s\n",
      "\titers: 500, epoch: 6 | loss: 0.0941400\n",
      "\tspeed: 0.1160s/iter; left time: 4623.0951s\n",
      "\titers: 600, epoch: 6 | loss: 0.0893563\n",
      "\tspeed: 0.1142s/iter; left time: 4541.1525s\n",
      "\titers: 700, epoch: 6 | loss: 0.0804608\n",
      "\tspeed: 0.1157s/iter; left time: 4587.4114s\n",
      "\titers: 800, epoch: 6 | loss: 0.0967790\n",
      "\tspeed: 0.1140s/iter; left time: 4510.0502s\n",
      "\titers: 900, epoch: 6 | loss: 0.0773378\n",
      "\tspeed: 0.1147s/iter; left time: 4527.2170s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0813385\n",
      "\tspeed: 0.1153s/iter; left time: 4538.8424s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0976809\n",
      "\tspeed: 0.1153s/iter; left time: 4528.6515s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0809603\n",
      "\tspeed: 0.1159s/iter; left time: 4538.5710s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0903448\n",
      "\tspeed: 0.1150s/iter; left time: 4493.0150s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0978441\n",
      "\tspeed: 0.1152s/iter; left time: 4489.8987s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0846561\n",
      "\tspeed: 0.1174s/iter; left time: 4563.0950s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1038152\n",
      "\tspeed: 0.1135s/iter; left time: 4400.6125s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0767741\n",
      "\tspeed: 0.1166s/iter; left time: 4508.1000s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0908609\n",
      "\tspeed: 0.1169s/iter; left time: 4510.1077s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0820962\n",
      "\tspeed: 0.1176s/iter; left time: 4522.2339s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0735893\n",
      "\tspeed: 0.1173s/iter; left time: 4500.7639s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0998428\n",
      "\tspeed: 0.1150s/iter; left time: 4399.5530s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0660093\n",
      "\tspeed: 0.1152s/iter; left time: 4398.4991s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0850836\n",
      "\tspeed: 0.1113s/iter; left time: 4234.8741s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0883901\n",
      "\tspeed: 0.1006s/iter; left time: 3820.2006s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0745388\n",
      "\tspeed: 0.1022s/iter; left time: 3868.0306s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0925138\n",
      "\tspeed: 0.1153s/iter; left time: 4355.8140s\n",
      "Epoch: 6 cost time: 00h:05m:08.02s\n",
      "Epoch: 6 | Train Loss: 0.0873064 Vali Loss: 0.0791609 Test Loss: 0.0851259\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0738214\n",
      "\tspeed: 0.9711s/iter; left time: 36489.8400s\n",
      "\titers: 200, epoch: 7 | loss: 0.0840051\n",
      "\tspeed: 0.1161s/iter; left time: 4352.2370s\n",
      "\titers: 300, epoch: 7 | loss: 0.0792297\n",
      "\tspeed: 0.1152s/iter; left time: 4305.8477s\n",
      "\titers: 400, epoch: 7 | loss: 0.1042238\n",
      "\tspeed: 0.1162s/iter; left time: 4333.1863s\n",
      "\titers: 500, epoch: 7 | loss: 0.0820721\n",
      "\tspeed: 0.1171s/iter; left time: 4352.7926s\n",
      "\titers: 600, epoch: 7 | loss: 0.0845355\n",
      "\tspeed: 0.1178s/iter; left time: 4366.5657s\n",
      "\titers: 700, epoch: 7 | loss: 0.0905107\n",
      "\tspeed: 0.1159s/iter; left time: 4285.8246s\n",
      "\titers: 800, epoch: 7 | loss: 0.0772271\n",
      "\tspeed: 0.1149s/iter; left time: 4236.7052s\n",
      "\titers: 900, epoch: 7 | loss: 0.0770174\n",
      "\tspeed: 0.1168s/iter; left time: 4294.1460s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0694755\n",
      "\tspeed: 0.1169s/iter; left time: 4287.4194s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1038114\n",
      "\tspeed: 0.1172s/iter; left time: 4287.6980s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0788102\n",
      "\tspeed: 0.1171s/iter; left time: 4270.7940s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0758535\n",
      "\tspeed: 0.1167s/iter; left time: 4246.4545s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0813657\n",
      "\tspeed: 0.1152s/iter; left time: 4177.8630s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0944329\n",
      "\tspeed: 0.1164s/iter; left time: 4211.2387s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1017237\n",
      "\tspeed: 0.1156s/iter; left time: 4171.1375s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1062258\n",
      "\tspeed: 0.1157s/iter; left time: 4162.8680s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1014802\n",
      "\tspeed: 0.1167s/iter; left time: 4187.8144s\n",
      "\titers: 1900, epoch: 7 | loss: 0.1010558\n",
      "\tspeed: 0.1167s/iter; left time: 4174.3972s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0732227\n",
      "\tspeed: 0.1169s/iter; left time: 4169.7044s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0915565\n",
      "\tspeed: 0.1173s/iter; left time: 4174.4953s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0892187\n",
      "\tspeed: 0.1153s/iter; left time: 4090.8475s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0861450\n",
      "\tspeed: 0.1153s/iter; left time: 4077.3121s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0746982\n",
      "\tspeed: 0.1160s/iter; left time: 4093.0898s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0731142\n",
      "\tspeed: 0.1162s/iter; left time: 4088.0478s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0744261\n",
      "\tspeed: 0.1155s/iter; left time: 4049.5048s\n",
      "Epoch: 7 cost time: 00h:05m:13.39s\n",
      "Epoch: 7 | Train Loss: 0.0857488 Vali Loss: 0.0787276 Test Loss: 0.0848344\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0803231\n",
      "\tspeed: 0.9810s/iter; left time: 34221.1282s\n",
      "\titers: 200, epoch: 8 | loss: 0.0757215\n",
      "\tspeed: 0.1165s/iter; left time: 4050.9094s\n",
      "\titers: 300, epoch: 8 | loss: 0.0945297\n",
      "\tspeed: 0.1174s/iter; left time: 4071.7276s\n",
      "\titers: 400, epoch: 8 | loss: 0.0887359\n",
      "\tspeed: 0.1170s/iter; left time: 4046.3619s\n",
      "\titers: 500, epoch: 8 | loss: 0.0828975\n",
      "\tspeed: 0.1153s/iter; left time: 3975.1436s\n",
      "\titers: 600, epoch: 8 | loss: 0.0813282\n",
      "\tspeed: 0.1162s/iter; left time: 3995.7034s\n",
      "\titers: 700, epoch: 8 | loss: 0.0668938\n",
      "\tspeed: 0.1176s/iter; left time: 4032.2929s\n",
      "\titers: 800, epoch: 8 | loss: 0.0837017\n",
      "\tspeed: 0.1063s/iter; left time: 3633.6560s\n",
      "\titers: 900, epoch: 8 | loss: 0.0803642\n",
      "\tspeed: 0.1150s/iter; left time: 3918.1419s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0760200\n",
      "\tspeed: 0.1106s/iter; left time: 3759.4089s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0895748\n",
      "\tspeed: 0.1079s/iter; left time: 3655.4745s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0938020\n",
      "\tspeed: 0.1076s/iter; left time: 3636.5085s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0925988\n",
      "\tspeed: 0.1089s/iter; left time: 3669.5521s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0767561\n",
      "\tspeed: 0.1087s/iter; left time: 3651.4342s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0654879\n",
      "\tspeed: 0.1088s/iter; left time: 3642.3707s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0950684\n",
      "\tspeed: 0.1099s/iter; left time: 3669.1597s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0819798\n",
      "\tspeed: 0.1085s/iter; left time: 3610.1746s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1015619\n",
      "\tspeed: 0.1104s/iter; left time: 3662.4440s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0803827\n",
      "\tspeed: 0.1075s/iter; left time: 3555.6806s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0669943\n",
      "\tspeed: 0.1076s/iter; left time: 3548.2569s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0584329\n",
      "\tspeed: 0.1100s/iter; left time: 3617.0933s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0806590\n",
      "\tspeed: 0.1085s/iter; left time: 3556.5976s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0929671\n",
      "\tspeed: 0.1069s/iter; left time: 3493.5199s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0913270\n",
      "\tspeed: 0.1105s/iter; left time: 3600.6363s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0718101\n",
      "\tspeed: 0.1065s/iter; left time: 3459.0762s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0859066\n",
      "\tspeed: 0.1086s/iter; left time: 3518.3297s\n",
      "Epoch: 8 cost time: 00h:04m:58.79s\n",
      "Epoch: 8 | Train Loss: 0.0842752 Vali Loss: 0.0798619 Test Loss: 0.0841101\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0922106\n",
      "\tspeed: 0.9995s/iter; left time: 32176.1556s\n",
      "\titers: 200, epoch: 9 | loss: 0.0897571\n",
      "\tspeed: 0.1089s/iter; left time: 3495.7113s\n",
      "\titers: 300, epoch: 9 | loss: 0.0623380\n",
      "\tspeed: 0.1080s/iter; left time: 3456.0926s\n",
      "\titers: 400, epoch: 9 | loss: 0.0733511\n",
      "\tspeed: 0.1100s/iter; left time: 3509.7160s\n",
      "\titers: 500, epoch: 9 | loss: 0.0863153\n",
      "\tspeed: 0.1096s/iter; left time: 3485.3193s\n",
      "\titers: 600, epoch: 9 | loss: 0.0784250\n",
      "\tspeed: 0.1099s/iter; left time: 3482.5028s\n",
      "\titers: 700, epoch: 9 | loss: 0.0621964\n",
      "\tspeed: 0.1095s/iter; left time: 3460.7887s\n",
      "\titers: 800, epoch: 9 | loss: 0.0698314\n",
      "\tspeed: 0.1091s/iter; left time: 3436.6810s\n",
      "\titers: 900, epoch: 9 | loss: 0.0820047\n",
      "\tspeed: 0.1083s/iter; left time: 3401.3189s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0763479\n",
      "\tspeed: 0.1087s/iter; left time: 3400.6183s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0723443\n",
      "\tspeed: 0.1089s/iter; left time: 3398.3996s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0933339\n",
      "\tspeed: 0.1107s/iter; left time: 3440.4772s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0803285\n",
      "\tspeed: 0.1108s/iter; left time: 3433.4594s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0697617\n",
      "\tspeed: 0.1117s/iter; left time: 3451.2412s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0668261\n",
      "\tspeed: 0.1106s/iter; left time: 3404.3545s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0736176\n",
      "\tspeed: 0.1111s/iter; left time: 3409.7163s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0924216\n",
      "\tspeed: 0.1106s/iter; left time: 3384.7932s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0700373\n",
      "\tspeed: 0.1110s/iter; left time: 3386.1760s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0905273\n",
      "\tspeed: 0.1121s/iter; left time: 3407.0341s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0873711\n",
      "\tspeed: 0.1104s/iter; left time: 3343.4414s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0690359\n",
      "\tspeed: 0.1115s/iter; left time: 3365.0326s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0764138\n",
      "\tspeed: 0.1107s/iter; left time: 3330.2051s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0736902\n",
      "\tspeed: 0.1111s/iter; left time: 3331.5544s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0738030\n",
      "\tspeed: 0.1109s/iter; left time: 3314.5451s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0723926\n",
      "\tspeed: 0.1106s/iter; left time: 3295.7242s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0774040\n",
      "\tspeed: 0.1127s/iter; left time: 3345.2743s\n",
      "Epoch: 9 cost time: 00h:04m:57.58s\n",
      "Epoch: 9 | Train Loss: 0.0826723 Vali Loss: 0.0802053 Test Loss: 0.0843679\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0718719\n",
      "\tspeed: 1.0037s/iter; left time: 29611.6652s\n",
      "\titers: 200, epoch: 10 | loss: 0.0820134\n",
      "\tspeed: 0.1085s/iter; left time: 3191.3399s\n",
      "\titers: 300, epoch: 10 | loss: 0.0871429\n",
      "\tspeed: 0.1121s/iter; left time: 3285.7988s\n",
      "\titers: 400, epoch: 10 | loss: 0.0750499\n",
      "\tspeed: 0.1103s/iter; left time: 3221.5376s\n",
      "\titers: 500, epoch: 10 | loss: 0.0743526\n",
      "\tspeed: 0.1100s/iter; left time: 3202.5129s\n",
      "\titers: 600, epoch: 10 | loss: 0.0716704\n",
      "\tspeed: 0.1070s/iter; left time: 3103.4502s\n",
      "\titers: 700, epoch: 10 | loss: 0.0730481\n",
      "\tspeed: 0.1096s/iter; left time: 3166.3384s\n",
      "\titers: 800, epoch: 10 | loss: 0.0884776\n",
      "\tspeed: 0.1093s/iter; left time: 3148.1204s\n",
      "\titers: 900, epoch: 10 | loss: 0.0730145\n",
      "\tspeed: 0.1098s/iter; left time: 3150.3721s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0823452\n",
      "\tspeed: 0.1074s/iter; left time: 3072.0429s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0792662\n",
      "\tspeed: 0.1076s/iter; left time: 3067.9559s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0772781\n",
      "\tspeed: 0.1071s/iter; left time: 3041.7340s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0990755\n",
      "\tspeed: 0.1093s/iter; left time: 3093.3337s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0935434\n",
      "\tspeed: 0.1111s/iter; left time: 3132.4183s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0814993\n",
      "\tspeed: 0.1073s/iter; left time: 3016.1721s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0930371\n",
      "\tspeed: 0.1092s/iter; left time: 3057.1117s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0896463\n",
      "\tspeed: 0.1093s/iter; left time: 3049.7165s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0843857\n",
      "\tspeed: 0.1118s/iter; left time: 3109.5160s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0857551\n",
      "\tspeed: 0.1115s/iter; left time: 3088.6796s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0770558\n",
      "\tspeed: 0.1127s/iter; left time: 3110.4487s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0894686\n",
      "\tspeed: 0.1135s/iter; left time: 3121.6679s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0692895\n",
      "\tspeed: 0.1125s/iter; left time: 3082.0599s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0728806\n",
      "\tspeed: 0.1132s/iter; left time: 3090.6093s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0917956\n",
      "\tspeed: 0.1126s/iter; left time: 3064.0648s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0742273\n",
      "\tspeed: 0.1116s/iter; left time: 3025.0136s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0754556\n",
      "\tspeed: 0.1127s/iter; left time: 3044.0736s\n",
      "Epoch: 10 cost time: 00h:04m:57.45s\n",
      "Epoch: 10 | Train Loss: 0.0813775 Vali Loss: 0.0823561 Test Loss: 0.0861748\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01877460442483425, rmse:0.13702045381069183, mae:0.08392173796892166, rse:0.5181057453155518\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 96: 01h:05m:25.55s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 23:50:53,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 23:50:54,668] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 23:50:54,668] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 23:50:54,668] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 23:50:54,764] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 23:50:54,764] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 23:50:55,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 23:50:55,479] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 23:50:55,479] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 23:50:55,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 23:50:55,480] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 23:50:55,480] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 23:50:55,481] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 23:50:55,878] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 23:50:55,879] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:55,879] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.54 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,065] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 23:50:56,066] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:56,066] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.69 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,066] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 23:50:56,224] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 23:50:56,225] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:56,225] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.72 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,226] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efc80676d90>\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2029339\n",
      "\tspeed: 0.1679s/iter; left time: 8995.0314s\n",
      "\titers: 200, epoch: 1 | loss: 0.1965789\n",
      "\tspeed: 0.1201s/iter; left time: 6421.9761s\n",
      "\titers: 300, epoch: 1 | loss: 0.1898626\n",
      "\tspeed: 0.1198s/iter; left time: 6392.5897s\n",
      "\titers: 400, epoch: 1 | loss: 0.1325201\n",
      "\tspeed: 0.1219s/iter; left time: 6497.6080s\n",
      "\titers: 500, epoch: 1 | loss: 0.1311700\n",
      "\tspeed: 0.1221s/iter; left time: 6492.0127s\n",
      "\titers: 600, epoch: 1 | loss: 0.1017136\n",
      "\tspeed: 0.1197s/iter; left time: 6353.0440s\n",
      "\titers: 700, epoch: 1 | loss: 0.1076386\n",
      "\tspeed: 0.1220s/iter; left time: 6462.5694s\n",
      "\titers: 800, epoch: 1 | loss: 0.0973543\n",
      "\tspeed: 0.1208s/iter; left time: 6389.9462s\n",
      "\titers: 900, epoch: 1 | loss: 0.1088642\n",
      "\tspeed: 0.1220s/iter; left time: 6437.7781s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1006573\n",
      "\tspeed: 0.1170s/iter; left time: 6163.3873s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1109908\n",
      "\tspeed: 0.1200s/iter; left time: 6309.1371s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1184647\n",
      "\tspeed: 0.1189s/iter; left time: 6240.6206s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1054379\n",
      "\tspeed: 0.1211s/iter; left time: 6343.0842s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0932986\n",
      "\tspeed: 0.1217s/iter; left time: 6362.1241s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0875861\n",
      "\tspeed: 0.1198s/iter; left time: 6252.8626s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0992577\n",
      "\tspeed: 0.1198s/iter; left time: 6240.0593s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0822084\n",
      "\tspeed: 0.1217s/iter; left time: 6327.8348s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0944820\n",
      "\tspeed: 0.1208s/iter; left time: 6268.0620s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1164758\n",
      "\tspeed: 0.1205s/iter; left time: 6241.0843s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0924384\n",
      "\tspeed: 0.1191s/iter; left time: 6155.0111s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1107640\n",
      "\tspeed: 0.1185s/iter; left time: 6110.7540s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0971796\n",
      "\tspeed: 0.1189s/iter; left time: 6122.0563s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1212062\n",
      "\tspeed: 0.1190s/iter; left time: 6114.2571s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0958548\n",
      "\tspeed: 0.1187s/iter; left time: 6088.0092s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0936789\n",
      "\tspeed: 0.1194s/iter; left time: 6108.7054s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1049049\n",
      "\tspeed: 0.1187s/iter; left time: 6061.0823s\n",
      "Epoch: 1 cost time: 00h:05m:23.87s\n",
      "Epoch: 1 | Train Loss: 0.1210713 Vali Loss: 0.0890277 Test Loss: 0.0935509\n",
      "Validation loss decreased (inf --> 0.089028).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0989368\n",
      "\tspeed: 1.1391s/iter; left time: 57975.4121s\n",
      "\titers: 200, epoch: 2 | loss: 0.1063188\n",
      "\tspeed: 0.1097s/iter; left time: 5572.4976s\n",
      "\titers: 300, epoch: 2 | loss: 0.0887055\n",
      "\tspeed: 0.1095s/iter; left time: 5550.5036s\n",
      "\titers: 400, epoch: 2 | loss: 0.1021468\n",
      "\tspeed: 0.1079s/iter; left time: 5457.7022s\n",
      "\titers: 500, epoch: 2 | loss: 0.1028672\n",
      "\tspeed: 0.1096s/iter; left time: 5532.1899s\n",
      "\titers: 600, epoch: 2 | loss: 0.0918319\n",
      "\tspeed: 0.1079s/iter; left time: 5439.5561s\n",
      "\titers: 700, epoch: 2 | loss: 0.0951936\n",
      "\tspeed: 0.1054s/iter; left time: 5299.3630s\n",
      "\titers: 800, epoch: 2 | loss: 0.1038750\n",
      "\tspeed: 0.1104s/iter; left time: 5544.2517s\n",
      "\titers: 900, epoch: 2 | loss: 0.1151560\n",
      "\tspeed: 0.1108s/iter; left time: 5550.2593s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1127429\n",
      "\tspeed: 0.1094s/iter; left time: 5469.5303s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0969984\n",
      "\tspeed: 0.1083s/iter; left time: 5403.1301s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0921559\n",
      "\tspeed: 0.1101s/iter; left time: 5484.1706s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0809502\n",
      "\tspeed: 0.1082s/iter; left time: 5376.5853s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1063600\n",
      "\tspeed: 0.1123s/iter; left time: 5570.1085s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1049415\n",
      "\tspeed: 0.1081s/iter; left time: 5350.1810s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0913269\n",
      "\tspeed: 0.1085s/iter; left time: 5358.6075s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0902280\n",
      "\tspeed: 0.1107s/iter; left time: 5457.1329s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1115005\n",
      "\tspeed: 0.1093s/iter; left time: 5377.3792s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1071176\n",
      "\tspeed: 0.1084s/iter; left time: 5323.9618s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1019441\n",
      "\tspeed: 0.1092s/iter; left time: 5348.9450s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1077473\n",
      "\tspeed: 0.1128s/iter; left time: 5513.3037s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1180863\n",
      "\tspeed: 0.1093s/iter; left time: 5335.5151s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0979958\n",
      "\tspeed: 0.1077s/iter; left time: 5242.6888s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1100279\n",
      "\tspeed: 0.1086s/iter; left time: 5277.2700s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1068703\n",
      "\tspeed: 0.1106s/iter; left time: 5366.0161s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0836630\n",
      "\tspeed: 0.1099s/iter; left time: 5316.6257s\n",
      "Epoch: 2 cost time: 00h:04m:53.91s\n",
      "Epoch: 2 | Train Loss: 0.0996675 Vali Loss: 0.0841658 Test Loss: 0.0895244\n",
      "Validation loss decreased (0.089028 --> 0.084166).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1142319\n",
      "\tspeed: 1.0354s/iter; left time: 49920.7792s\n",
      "\titers: 200, epoch: 3 | loss: 0.0889279\n",
      "\tspeed: 0.1108s/iter; left time: 5332.4357s\n",
      "\titers: 300, epoch: 3 | loss: 0.0985157\n",
      "\tspeed: 0.1093s/iter; left time: 5245.7327s\n",
      "\titers: 400, epoch: 3 | loss: 0.0925059\n",
      "\tspeed: 0.1078s/iter; left time: 5166.7550s\n",
      "\titers: 500, epoch: 3 | loss: 0.0979997\n",
      "\tspeed: 0.1105s/iter; left time: 5284.2447s\n",
      "\titers: 600, epoch: 3 | loss: 0.1002590\n",
      "\tspeed: 0.1085s/iter; left time: 5176.8450s\n",
      "\titers: 700, epoch: 3 | loss: 0.0918859\n",
      "\tspeed: 0.1088s/iter; left time: 5181.8800s\n",
      "\titers: 800, epoch: 3 | loss: 0.0898818\n",
      "\tspeed: 0.1092s/iter; left time: 5186.8335s\n",
      "\titers: 900, epoch: 3 | loss: 0.1080294\n",
      "\tspeed: 0.1077s/iter; left time: 5107.2974s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0840585\n",
      "\tspeed: 0.1109s/iter; left time: 5248.6033s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1036685\n",
      "\tspeed: 0.1080s/iter; left time: 5099.2819s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0952610\n",
      "\tspeed: 0.1097s/iter; left time: 5170.3811s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0965679\n",
      "\tspeed: 0.1097s/iter; left time: 5157.9908s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0959029\n",
      "\tspeed: 0.1093s/iter; left time: 5128.9773s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1035789\n",
      "\tspeed: 0.1090s/iter; left time: 5103.8582s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1028642\n",
      "\tspeed: 0.1100s/iter; left time: 5137.9338s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0882429\n",
      "\tspeed: 0.1096s/iter; left time: 5110.8954s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0945800\n",
      "\tspeed: 0.1099s/iter; left time: 5112.2348s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0928347\n",
      "\tspeed: 0.1104s/iter; left time: 5124.0375s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1003603\n",
      "\tspeed: 0.1096s/iter; left time: 5077.2074s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0984533\n",
      "\tspeed: 0.1095s/iter; left time: 5062.3104s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1025412\n",
      "\tspeed: 0.1075s/iter; left time: 4955.1975s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0893745\n",
      "\tspeed: 0.1096s/iter; left time: 5041.8927s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0895675\n",
      "\tspeed: 0.1104s/iter; left time: 5068.4222s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1030405\n",
      "\tspeed: 0.1123s/iter; left time: 5143.2645s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0951473\n",
      "\tspeed: 0.1126s/iter; left time: 5145.1640s\n",
      "Epoch: 3 cost time: 00h:04m:54.67s\n",
      "Epoch: 3 | Train Loss: 0.0963206 Vali Loss: 0.0841027 Test Loss: 0.0897672\n",
      "Validation loss decreased (0.084166 --> 0.084103).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0876906\n",
      "\tspeed: 1.0156s/iter; left time: 46239.8347s\n",
      "\titers: 200, epoch: 4 | loss: 0.0991883\n",
      "\tspeed: 0.1097s/iter; left time: 4984.6479s\n",
      "\titers: 300, epoch: 4 | loss: 0.1039904\n",
      "\tspeed: 0.1111s/iter; left time: 5037.4476s\n",
      "\titers: 400, epoch: 4 | loss: 0.0816384\n",
      "\tspeed: 0.1112s/iter; left time: 5029.5138s\n",
      "\titers: 500, epoch: 4 | loss: 0.0884582\n",
      "\tspeed: 0.1091s/iter; left time: 4923.2094s\n",
      "\titers: 600, epoch: 4 | loss: 0.0939277\n",
      "\tspeed: 0.1102s/iter; left time: 4961.8169s\n",
      "\titers: 700, epoch: 4 | loss: 0.0897342\n",
      "\tspeed: 0.1086s/iter; left time: 4879.0680s\n",
      "\titers: 800, epoch: 4 | loss: 0.0904002\n",
      "\tspeed: 0.1136s/iter; left time: 5092.9051s\n",
      "\titers: 900, epoch: 4 | loss: 0.1019345\n",
      "\tspeed: 0.1094s/iter; left time: 4895.5678s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1152800\n",
      "\tspeed: 0.1103s/iter; left time: 4923.4616s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0801582\n",
      "\tspeed: 0.1103s/iter; left time: 4912.7238s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0705594\n",
      "\tspeed: 0.1108s/iter; left time: 4922.2872s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1035900\n",
      "\tspeed: 0.1114s/iter; left time: 4938.2054s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1084320\n",
      "\tspeed: 0.1099s/iter; left time: 4858.8270s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1027107\n",
      "\tspeed: 0.1103s/iter; left time: 4868.2863s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1098636\n",
      "\tspeed: 0.1125s/iter; left time: 4953.1752s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1155795\n",
      "\tspeed: 0.1101s/iter; left time: 4836.7290s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0852332\n",
      "\tspeed: 0.1088s/iter; left time: 4766.7043s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0975535\n",
      "\tspeed: 0.1080s/iter; left time: 4722.3323s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0907878\n",
      "\tspeed: 0.1094s/iter; left time: 4774.2535s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1058796\n",
      "\tspeed: 0.1083s/iter; left time: 4714.2260s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0916528\n",
      "\tspeed: 0.1085s/iter; left time: 4713.9516s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0722211\n",
      "\tspeed: 0.1091s/iter; left time: 4726.8356s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0976648\n",
      "\tspeed: 0.1091s/iter; left time: 4717.7498s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0973345\n",
      "\tspeed: 0.1098s/iter; left time: 4734.1440s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1003160\n",
      "\tspeed: 0.1075s/iter; left time: 4626.5454s\n",
      "Epoch: 4 cost time: 00h:04m:55.51s\n",
      "Epoch: 4 | Train Loss: 0.0942669 Vali Loss: 0.0841781 Test Loss: 0.0898632\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0956298\n",
      "\tspeed: 0.9791s/iter; left time: 41947.9687s\n",
      "\titers: 200, epoch: 5 | loss: 0.0926843\n",
      "\tspeed: 0.1115s/iter; left time: 4768.1909s\n",
      "\titers: 300, epoch: 5 | loss: 0.0960669\n",
      "\tspeed: 0.1119s/iter; left time: 4771.1583s\n",
      "\titers: 400, epoch: 5 | loss: 0.1028672\n",
      "\tspeed: 0.1133s/iter; left time: 4818.9602s\n",
      "\titers: 500, epoch: 5 | loss: 0.0889630\n",
      "\tspeed: 0.1136s/iter; left time: 4822.7283s\n",
      "\titers: 600, epoch: 5 | loss: 0.0886828\n",
      "\tspeed: 0.1118s/iter; left time: 4734.4450s\n",
      "\titers: 700, epoch: 5 | loss: 0.0865297\n",
      "\tspeed: 0.1123s/iter; left time: 4742.8606s\n",
      "\titers: 800, epoch: 5 | loss: 0.0793266\n",
      "\tspeed: 0.1123s/iter; left time: 4732.9254s\n",
      "\titers: 900, epoch: 5 | loss: 0.0875158\n",
      "\tspeed: 0.1100s/iter; left time: 4626.6067s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1002013\n",
      "\tspeed: 0.1128s/iter; left time: 4730.8102s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0797455\n",
      "\tspeed: 0.1109s/iter; left time: 4640.6352s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1008162\n",
      "\tspeed: 0.1114s/iter; left time: 4650.6303s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0942667\n",
      "\tspeed: 0.1117s/iter; left time: 4651.4745s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0891483\n",
      "\tspeed: 0.1108s/iter; left time: 4602.0999s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0945314\n",
      "\tspeed: 0.1104s/iter; left time: 4574.6913s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0918318\n",
      "\tspeed: 0.1082s/iter; left time: 4474.3307s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0960498\n",
      "\tspeed: 0.1101s/iter; left time: 4539.8121s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0857012\n",
      "\tspeed: 0.1114s/iter; left time: 4585.0055s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1000285\n",
      "\tspeed: 0.1093s/iter; left time: 4484.9406s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0954931\n",
      "\tspeed: 0.1111s/iter; left time: 4549.3282s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0967594\n",
      "\tspeed: 0.1115s/iter; left time: 4555.8473s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0867127\n",
      "\tspeed: 0.1100s/iter; left time: 4483.8720s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0886693\n",
      "\tspeed: 0.1081s/iter; left time: 4391.9438s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0934356\n",
      "\tspeed: 0.1099s/iter; left time: 4455.3731s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1050704\n",
      "\tspeed: 0.1047s/iter; left time: 4235.5352s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0817107\n",
      "\tspeed: 0.1089s/iter; left time: 4394.9630s\n",
      "Epoch: 5 cost time: 00h:04m:57.74s\n",
      "Epoch: 5 | Train Loss: 0.0925586 Vali Loss: 0.0840413 Test Loss: 0.0891893\n",
      "Validation loss decreased (0.084103 --> 0.084041).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1023156\n",
      "\tspeed: 0.9987s/iter; left time: 40109.5961s\n",
      "\titers: 200, epoch: 6 | loss: 0.0814353\n",
      "\tspeed: 0.1091s/iter; left time: 4372.3853s\n",
      "\titers: 300, epoch: 6 | loss: 0.0786190\n",
      "\tspeed: 0.1127s/iter; left time: 4505.0348s\n",
      "\titers: 400, epoch: 6 | loss: 0.0850952\n",
      "\tspeed: 0.1095s/iter; left time: 4364.9110s\n",
      "\titers: 500, epoch: 6 | loss: 0.0954753\n",
      "\tspeed: 0.1084s/iter; left time: 4310.2970s\n",
      "\titers: 600, epoch: 6 | loss: 0.0951714\n",
      "\tspeed: 0.1075s/iter; left time: 4263.4483s\n",
      "\titers: 700, epoch: 6 | loss: 0.0956363\n",
      "\tspeed: 0.1088s/iter; left time: 4304.6142s\n",
      "\titers: 800, epoch: 6 | loss: 0.0957980\n",
      "\tspeed: 0.1116s/iter; left time: 4403.2993s\n",
      "\titers: 900, epoch: 6 | loss: 0.0858036\n",
      "\tspeed: 0.1084s/iter; left time: 4266.2953s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0840625\n",
      "\tspeed: 0.1095s/iter; left time: 4299.1964s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1052077\n",
      "\tspeed: 0.1113s/iter; left time: 4360.4634s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0996516\n",
      "\tspeed: 0.1107s/iter; left time: 4323.7721s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0890602\n",
      "\tspeed: 0.1103s/iter; left time: 4299.3119s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0871324\n",
      "\tspeed: 0.1134s/iter; left time: 4405.4610s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1050522\n",
      "\tspeed: 0.1097s/iter; left time: 4251.4229s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0922716\n",
      "\tspeed: 0.1076s/iter; left time: 4160.7940s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0856184\n",
      "\tspeed: 0.1090s/iter; left time: 4201.4655s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0855285\n",
      "\tspeed: 0.1098s/iter; left time: 4223.7127s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0947269\n",
      "\tspeed: 0.1088s/iter; left time: 4174.5388s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0936098\n",
      "\tspeed: 0.1098s/iter; left time: 4200.9011s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0826341\n",
      "\tspeed: 0.1077s/iter; left time: 4111.5602s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0854143\n",
      "\tspeed: 0.1091s/iter; left time: 4153.8185s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1005903\n",
      "\tspeed: 0.1087s/iter; left time: 4125.2825s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1012520\n",
      "\tspeed: 0.1090s/iter; left time: 4125.6942s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0836399\n",
      "\tspeed: 0.1077s/iter; left time: 4068.0019s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0693969\n",
      "\tspeed: 0.1071s/iter; left time: 4033.4910s\n",
      "Epoch: 6 cost time: 00h:04m:54.35s\n",
      "Epoch: 6 | Train Loss: 0.0908017 Vali Loss: 0.0846587 Test Loss: 0.0896558\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0822827\n",
      "\tspeed: 0.9899s/iter; left time: 37100.0563s\n",
      "\titers: 200, epoch: 7 | loss: 0.0926162\n",
      "\tspeed: 0.1114s/iter; left time: 4164.7326s\n",
      "\titers: 300, epoch: 7 | loss: 0.0814312\n",
      "\tspeed: 0.1117s/iter; left time: 4164.3827s\n",
      "\titers: 400, epoch: 7 | loss: 0.0927256\n",
      "\tspeed: 0.1096s/iter; left time: 4074.4149s\n",
      "\titers: 500, epoch: 7 | loss: 0.0889228\n",
      "\tspeed: 0.1091s/iter; left time: 4045.6678s\n",
      "\titers: 600, epoch: 7 | loss: 0.1010134\n",
      "\tspeed: 0.1092s/iter; left time: 4039.2271s\n",
      "\titers: 700, epoch: 7 | loss: 0.0931574\n",
      "\tspeed: 0.1082s/iter; left time: 3988.9460s\n",
      "\titers: 800, epoch: 7 | loss: 0.1155873\n",
      "\tspeed: 0.1110s/iter; left time: 4080.5043s\n",
      "\titers: 900, epoch: 7 | loss: 0.0780460\n",
      "\tspeed: 0.1100s/iter; left time: 4035.3996s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0952020\n",
      "\tspeed: 0.1084s/iter; left time: 3963.3575s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0811136\n",
      "\tspeed: 0.1111s/iter; left time: 4052.5775s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0879815\n",
      "\tspeed: 0.1089s/iter; left time: 3961.4461s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0844414\n",
      "\tspeed: 0.1090s/iter; left time: 3956.0002s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0822187\n",
      "\tspeed: 0.1082s/iter; left time: 3915.6191s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0836276\n",
      "\tspeed: 0.1106s/iter; left time: 3991.4405s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0950310\n",
      "\tspeed: 0.1099s/iter; left time: 3952.8517s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0898525\n",
      "\tspeed: 0.1119s/iter; left time: 4015.9486s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1007908\n",
      "\tspeed: 0.1098s/iter; left time: 3928.4173s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0798731\n",
      "\tspeed: 0.1129s/iter; left time: 4028.6812s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1034588\n",
      "\tspeed: 0.1093s/iter; left time: 3887.3728s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0921248\n",
      "\tspeed: 0.1107s/iter; left time: 3926.8929s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1086720\n",
      "\tspeed: 0.1100s/iter; left time: 3890.3396s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0822674\n",
      "\tspeed: 0.1113s/iter; left time: 3926.1189s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0884877\n",
      "\tspeed: 0.1077s/iter; left time: 3790.1335s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1050238\n",
      "\tspeed: 0.1150s/iter; left time: 4034.0993s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0835463\n",
      "\tspeed: 0.1124s/iter; left time: 3930.4840s\n",
      "Epoch: 7 cost time: 00h:04m:56.70s\n",
      "Epoch: 7 | Train Loss: 0.0888387 Vali Loss: 0.0842900 Test Loss: 0.0893077\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0919155\n",
      "\tspeed: 0.9824s/iter; left time: 34179.8800s\n",
      "\titers: 200, epoch: 8 | loss: 0.0877724\n",
      "\tspeed: 0.1100s/iter; left time: 3816.5570s\n",
      "\titers: 300, epoch: 8 | loss: 0.0862236\n",
      "\tspeed: 0.1089s/iter; left time: 3768.4950s\n",
      "\titers: 400, epoch: 8 | loss: 0.0993591\n",
      "\tspeed: 0.1077s/iter; left time: 3716.4490s\n",
      "\titers: 500, epoch: 8 | loss: 0.1030686\n",
      "\tspeed: 0.1112s/iter; left time: 3823.2374s\n",
      "\titers: 600, epoch: 8 | loss: 0.0945140\n",
      "\tspeed: 0.1119s/iter; left time: 3838.9451s\n",
      "\titers: 700, epoch: 8 | loss: 0.1027590\n",
      "\tspeed: 0.1092s/iter; left time: 3732.5127s\n",
      "\titers: 800, epoch: 8 | loss: 0.1001980\n",
      "\tspeed: 0.1121s/iter; left time: 3822.6252s\n",
      "\titers: 900, epoch: 8 | loss: 0.0884102\n",
      "\tspeed: 0.1090s/iter; left time: 3705.9591s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0649632\n",
      "\tspeed: 0.1099s/iter; left time: 3723.3871s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0900450\n",
      "\tspeed: 0.1090s/iter; left time: 3683.1089s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0741200\n",
      "\tspeed: 0.1112s/iter; left time: 3746.8564s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0829209\n",
      "\tspeed: 0.1088s/iter; left time: 3654.2949s\n",
      "\titers: 1400, epoch: 8 | loss: 0.1002313\n",
      "\tspeed: 0.1093s/iter; left time: 3661.3948s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0962400\n",
      "\tspeed: 0.1087s/iter; left time: 3629.8484s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0859644\n",
      "\tspeed: 0.1107s/iter; left time: 3684.4940s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0893864\n",
      "\tspeed: 0.1133s/iter; left time: 3761.5151s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0875129\n",
      "\tspeed: 0.1101s/iter; left time: 3643.0258s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0895706\n",
      "\tspeed: 0.1103s/iter; left time: 3637.8111s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0810007\n",
      "\tspeed: 0.1099s/iter; left time: 3615.4657s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0934768\n",
      "\tspeed: 0.1105s/iter; left time: 3624.0294s\n",
      "\titers: 2200, epoch: 8 | loss: 0.1046799\n",
      "\tspeed: 0.1108s/iter; left time: 3623.2802s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0742580\n",
      "\tspeed: 0.1118s/iter; left time: 3643.2920s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0805892\n",
      "\tspeed: 0.1101s/iter; left time: 3575.9694s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0943531\n",
      "\tspeed: 0.1133s/iter; left time: 3669.6143s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0835769\n",
      "\tspeed: 0.1103s/iter; left time: 3563.4025s\n",
      "Epoch: 8 cost time: 00h:04m:56.76s\n",
      "Epoch: 8 | Train Loss: 0.0870042 Vali Loss: 0.0843130 Test Loss: 0.0891844\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0846436\n",
      "\tspeed: 0.9931s/iter; left time: 31886.2504s\n",
      "\titers: 200, epoch: 9 | loss: 0.0627204\n",
      "\tspeed: 0.1136s/iter; left time: 3635.0689s\n",
      "\titers: 300, epoch: 9 | loss: 0.0839949\n",
      "\tspeed: 0.1118s/iter; left time: 3566.4207s\n",
      "\titers: 400, epoch: 9 | loss: 0.0899176\n",
      "\tspeed: 0.1117s/iter; left time: 3554.0239s\n",
      "\titers: 500, epoch: 9 | loss: 0.0827703\n",
      "\tspeed: 0.1087s/iter; left time: 3448.3166s\n",
      "\titers: 600, epoch: 9 | loss: 0.0757234\n",
      "\tspeed: 0.1079s/iter; left time: 3410.3451s\n",
      "\titers: 700, epoch: 9 | loss: 0.0842483\n",
      "\tspeed: 0.1056s/iter; left time: 3328.5301s\n",
      "\titers: 800, epoch: 9 | loss: 0.0728695\n",
      "\tspeed: 0.1088s/iter; left time: 3417.5830s\n",
      "\titers: 900, epoch: 9 | loss: 0.0946383\n",
      "\tspeed: 0.1088s/iter; left time: 3407.5581s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0801099\n",
      "\tspeed: 0.1090s/iter; left time: 3403.3083s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0760204\n",
      "\tspeed: 0.1110s/iter; left time: 3452.1874s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0898418\n",
      "\tspeed: 0.1095s/iter; left time: 3395.2446s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0854249\n",
      "\tspeed: 0.1069s/iter; left time: 3304.6754s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0841468\n",
      "\tspeed: 0.1094s/iter; left time: 3371.7299s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0759359\n",
      "\tspeed: 0.1100s/iter; left time: 3376.7436s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0933973\n",
      "\tspeed: 0.1102s/iter; left time: 3374.0485s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0746514\n",
      "\tspeed: 0.1120s/iter; left time: 3416.6345s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0891489\n",
      "\tspeed: 0.1091s/iter; left time: 3316.1871s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0825078\n",
      "\tspeed: 0.1085s/iter; left time: 3289.5723s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0846198\n",
      "\tspeed: 0.1104s/iter; left time: 3335.3857s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0837791\n",
      "\tspeed: 0.1095s/iter; left time: 3296.0821s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0860811\n",
      "\tspeed: 0.1096s/iter; left time: 3287.7848s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0792212\n",
      "\tspeed: 0.1103s/iter; left time: 3298.4558s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0837938\n",
      "\tspeed: 0.1080s/iter; left time: 3219.4986s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0834742\n",
      "\tspeed: 0.1098s/iter; left time: 3263.2591s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0836421\n",
      "\tspeed: 0.1088s/iter; left time: 3220.5760s\n",
      "Epoch: 9 cost time: 00h:04m:54.56s\n",
      "Epoch: 9 | Train Loss: 0.0853277 Vali Loss: 0.0862787 Test Loss: 0.0900003\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0887046\n",
      "\tspeed: 0.9788s/iter; left time: 28801.2164s\n",
      "\titers: 200, epoch: 10 | loss: 0.0646053\n",
      "\tspeed: 0.1075s/iter; left time: 3152.2188s\n",
      "\titers: 300, epoch: 10 | loss: 0.0941833\n",
      "\tspeed: 0.1090s/iter; left time: 3186.2389s\n",
      "\titers: 400, epoch: 10 | loss: 0.0726987\n",
      "\tspeed: 0.1123s/iter; left time: 3271.5557s\n",
      "\titers: 500, epoch: 10 | loss: 0.0876720\n",
      "\tspeed: 0.1126s/iter; left time: 3269.1611s\n",
      "\titers: 600, epoch: 10 | loss: 0.0904420\n",
      "\tspeed: 0.1142s/iter; left time: 3303.6465s\n",
      "\titers: 700, epoch: 10 | loss: 0.0787993\n",
      "\tspeed: 0.1109s/iter; left time: 3197.0932s\n",
      "\titers: 800, epoch: 10 | loss: 0.0708453\n",
      "\tspeed: 0.1118s/iter; left time: 3211.4367s\n",
      "\titers: 900, epoch: 10 | loss: 0.0839387\n",
      "\tspeed: 0.1111s/iter; left time: 3180.7091s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0783891\n",
      "\tspeed: 0.1096s/iter; left time: 3127.5211s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0766438\n",
      "\tspeed: 0.1103s/iter; left time: 3134.5486s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0895397\n",
      "\tspeed: 0.1103s/iter; left time: 3123.7572s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0842266\n",
      "\tspeed: 0.1087s/iter; left time: 3068.8769s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0860861\n",
      "\tspeed: 0.1102s/iter; left time: 3098.9001s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0650968\n",
      "\tspeed: 0.1095s/iter; left time: 3068.1646s\n",
      "\titers: 1600, epoch: 10 | loss: 0.1023239\n",
      "\tspeed: 0.1092s/iter; left time: 3048.2008s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0784739\n",
      "\tspeed: 0.1103s/iter; left time: 3069.1363s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0877584\n",
      "\tspeed: 0.1096s/iter; left time: 3038.4388s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0746399\n",
      "\tspeed: 0.1106s/iter; left time: 3056.2888s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0881622\n",
      "\tspeed: 0.1076s/iter; left time: 2962.8424s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0905812\n",
      "\tspeed: 0.1096s/iter; left time: 3006.3810s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0936635\n",
      "\tspeed: 0.1130s/iter; left time: 3087.8383s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0850890\n",
      "\tspeed: 0.1102s/iter; left time: 2999.2200s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0925531\n",
      "\tspeed: 0.1115s/iter; left time: 3025.3495s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0950423\n",
      "\tspeed: 0.1093s/iter; left time: 2952.7029s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0860755\n",
      "\tspeed: 0.1111s/iter; left time: 2992.5685s\n",
      "Epoch: 10 cost time: 00h:04m:56.60s\n",
      "Epoch: 10 | Train Loss: 0.0836082 Vali Loss: 0.0852293 Test Loss: 0.0903601\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02039830945432186, rmse:0.14282265305519104, mae:0.0891893059015274, rse:0.5403580069541931\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 168: 01h:04m:11.86s\n",
      "\n",
      "Intermediate time for IT: 03h:53m:40.34s\n",
      "\n",
      "Total time: 20h:49m:25.28s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "timellm_results = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>0.0920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>0.1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0.1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>0.1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.0941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.0824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0217  0.1474  0.0920\n",
       "        96        0.0395  0.1989  0.1324\n",
       "        168       0.0395  0.1988  0.1348\n",
       "GB      24        0.0252  0.1588  0.1012\n",
       "        96        0.0437  0.2091  0.1434\n",
       "        168       0.0469  0.2166  0.1487\n",
       "ES      24        0.0108  0.1039  0.0672\n",
       "        96        0.0198  0.1409  0.0941\n",
       "        168       0.0217  0.1472  0.0970\n",
       "FR      24        0.0104  0.1017  0.0586\n",
       "        96        0.0186  0.1365  0.0824\n",
       "        168       0.0205  0.1430  0.0878\n",
       "IT      24        0.0104  0.1022  0.0607\n",
       "        96        0.0188  0.1370  0.0839\n",
       "        168       0.0204  0.1428  0.0892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM/336'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm_336.csv'))\n",
    "timellm_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
