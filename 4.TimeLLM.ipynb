{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. TimeLLM](#1-timellm)\n",
    "\n",
    "\n",
    "Results for TimeLLM. I run this code partitionally, but complete results are in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"2\"\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 512\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_512.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-03 01:12:05,716] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 01:12:06,757] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 01:12:06,757] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 01:12:06,757] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 01:12:06,848] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 01:12:06,848] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 01:12:07,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 01:12:07,584] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 01:12:07,584] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 01:12:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 01:12:07,586] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 01:12:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 01:12:07,586] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 01:12:07,942] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 01:12:07,944] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:07,944] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.49 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,122] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 01:12:08,123] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:08,123] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.53 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,123] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 01:12:08,286] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 01:12:08,287] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 01:12:08,287] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.42 GB, percent = 10.0%\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 01:12:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 01:12:08,289] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1cacb059d0>\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 01:12:08,290] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 01:12:08,291] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 01:12:08,292] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688982\n",
      "\tspeed: 0.1674s/iter; left time: 14941.3446s\n",
      "\titers: 200, epoch: 1 | loss: 0.1398762\n",
      "\tspeed: 0.1227s/iter; left time: 10938.8876s\n",
      "\titers: 300, epoch: 1 | loss: 0.1594033\n",
      "\tspeed: 0.1246s/iter; left time: 11095.0762s\n",
      "\titers: 400, epoch: 1 | loss: 0.1548022\n",
      "\tspeed: 0.1240s/iter; left time: 11033.2634s\n",
      "\titers: 500, epoch: 1 | loss: 0.1380969\n",
      "\tspeed: 0.1244s/iter; left time: 11056.1716s\n",
      "\titers: 600, epoch: 1 | loss: 0.1172558\n",
      "\tspeed: 0.1254s/iter; left time: 11131.3914s\n",
      "\titers: 700, epoch: 1 | loss: 0.1290968\n",
      "\tspeed: 0.1249s/iter; left time: 11072.9375s\n",
      "\titers: 800, epoch: 1 | loss: 0.1323915\n",
      "\tspeed: 0.1237s/iter; left time: 10953.4734s\n",
      "\titers: 900, epoch: 1 | loss: 0.0968730\n",
      "\tspeed: 0.1241s/iter; left time: 10982.1952s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1262417\n",
      "\tspeed: 0.1247s/iter; left time: 11018.8563s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1081160\n",
      "\tspeed: 0.1254s/iter; left time: 11067.6072s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1055638\n",
      "\tspeed: 0.1233s/iter; left time: 10866.4606s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0805435\n",
      "\tspeed: 0.1236s/iter; left time: 10880.7501s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1116071\n",
      "\tspeed: 0.1239s/iter; left time: 10895.0457s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0692826\n",
      "\tspeed: 0.1238s/iter; left time: 10879.3045s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0953681\n",
      "\tspeed: 0.1243s/iter; left time: 10905.0177s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1187560\n",
      "\tspeed: 0.1228s/iter; left time: 10768.5576s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0991687\n",
      "\tspeed: 0.1231s/iter; left time: 10777.3760s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1227836\n",
      "\tspeed: 0.1242s/iter; left time: 10861.5365s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0913499\n",
      "\tspeed: 0.1222s/iter; left time: 10676.2045s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0831885\n",
      "\tspeed: 0.1243s/iter; left time: 10845.1859s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1006144\n",
      "\tspeed: 0.1223s/iter; left time: 10662.3231s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1002881\n",
      "\tspeed: 0.1232s/iter; left time: 10727.6874s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0858188\n",
      "\tspeed: 0.1241s/iter; left time: 10787.8205s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0878481\n",
      "\tspeed: 0.1233s/iter; left time: 10707.2554s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0938092\n",
      "\tspeed: 0.1251s/iter; left time: 10856.3638s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1014624\n",
      "\tspeed: 0.1246s/iter; left time: 10801.6068s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0828544\n",
      "\tspeed: 0.1242s/iter; left time: 10747.8099s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0846184\n",
      "\tspeed: 0.1240s/iter; left time: 10719.4482s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0732674\n",
      "\tspeed: 0.1233s/iter; left time: 10648.1436s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0863261\n",
      "\tspeed: 0.1233s/iter; left time: 10635.2736s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1145399\n",
      "\tspeed: 0.1231s/iter; left time: 10606.0279s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0950736\n",
      "\tspeed: 0.1244s/iter; left time: 10705.7121s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1013337\n",
      "\tspeed: 0.1254s/iter; left time: 10783.4548s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0962714\n",
      "\tspeed: 0.1234s/iter; left time: 10599.2676s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045466\n",
      "\tspeed: 0.1249s/iter; left time: 10713.2908s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0921188\n",
      "\tspeed: 0.1228s/iter; left time: 10516.2821s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0915671\n",
      "\tspeed: 0.1235s/iter; left time: 10562.9555s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059835\n",
      "\tspeed: 0.1255s/iter; left time: 10726.9688s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0626834\n",
      "\tspeed: 0.1229s/iter; left time: 10488.5240s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0807726\n",
      "\tspeed: 0.1229s/iter; left time: 10482.4050s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1075011\n",
      "\tspeed: 0.1222s/iter; left time: 10402.4167s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0953812\n",
      "\tspeed: 0.1234s/iter; left time: 10499.2678s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0955328\n",
      "\tspeed: 0.1237s/iter; left time: 10509.4145s\n",
      "Epoch: 1 cost time: 00h:09m:14.48s\n",
      "Epoch: 1 | Train Loss: 0.1053388 Vali Loss: 0.0971123 Test Loss: 0.0994308\n",
      "Validation loss decreased (inf --> 0.097112).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0704475\n",
      "\tspeed: 1.7948s/iter; left time: 152182.9881s\n",
      "\titers: 200, epoch: 2 | loss: 0.0855533\n",
      "\tspeed: 0.1141s/iter; left time: 9659.3097s\n",
      "\titers: 300, epoch: 2 | loss: 0.0774946\n",
      "\tspeed: 0.1140s/iter; left time: 9640.4392s\n",
      "\titers: 400, epoch: 2 | loss: 0.0883429\n",
      "\tspeed: 0.1123s/iter; left time: 9491.7135s\n",
      "\titers: 500, epoch: 2 | loss: 0.0867326\n",
      "\tspeed: 0.1123s/iter; left time: 9475.7204s\n",
      "\titers: 600, epoch: 2 | loss: 0.0939545\n",
      "\tspeed: 0.1139s/iter; left time: 9597.7267s\n",
      "\titers: 700, epoch: 2 | loss: 0.0739971\n",
      "\tspeed: 0.1128s/iter; left time: 9497.3048s\n",
      "\titers: 800, epoch: 2 | loss: 0.0977335\n",
      "\tspeed: 0.1149s/iter; left time: 9664.4324s\n",
      "\titers: 900, epoch: 2 | loss: 0.0857365\n",
      "\tspeed: 0.1138s/iter; left time: 9555.0482s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0865258\n",
      "\tspeed: 0.1149s/iter; left time: 9639.7231s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0972862\n",
      "\tspeed: 0.1140s/iter; left time: 9554.6080s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0832878\n",
      "\tspeed: 0.1144s/iter; left time: 9574.7462s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0947991\n",
      "\tspeed: 0.1156s/iter; left time: 9662.8164s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0940965\n",
      "\tspeed: 0.1154s/iter; left time: 9639.0286s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0825809\n",
      "\tspeed: 0.1155s/iter; left time: 9628.6063s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0861147\n",
      "\tspeed: 0.1136s/iter; left time: 9465.3499s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1065482\n",
      "\tspeed: 0.1139s/iter; left time: 9479.3903s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0846911\n",
      "\tspeed: 0.1132s/iter; left time: 9403.8697s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1024258\n",
      "\tspeed: 0.1142s/iter; left time: 9480.0655s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0896808\n",
      "\tspeed: 0.1129s/iter; left time: 9358.1236s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1056611\n",
      "\tspeed: 0.1135s/iter; left time: 9397.8205s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0911830\n",
      "\tspeed: 0.1128s/iter; left time: 9331.4414s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0980409\n",
      "\tspeed: 0.1141s/iter; left time: 9421.8908s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0799288\n",
      "\tspeed: 0.1155s/iter; left time: 9529.0887s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0872002\n",
      "\tspeed: 0.1140s/iter; left time: 9391.3248s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0915525\n",
      "\tspeed: 0.1120s/iter; left time: 9218.2102s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1013657\n",
      "\tspeed: 0.1136s/iter; left time: 9339.5619s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0794303\n",
      "\tspeed: 0.1132s/iter; left time: 9294.8333s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0900755\n",
      "\tspeed: 0.1148s/iter; left time: 9411.2400s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0776787\n",
      "\tspeed: 0.1129s/iter; left time: 9244.6546s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1028968\n",
      "\tspeed: 0.1133s/iter; left time: 9268.9056s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0771540\n",
      "\tspeed: 0.1152s/iter; left time: 9411.0392s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0871339\n",
      "\tspeed: 0.1157s/iter; left time: 9437.3403s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1009780\n",
      "\tspeed: 0.1145s/iter; left time: 9332.5256s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0891745\n",
      "\tspeed: 0.1149s/iter; left time: 9348.6727s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0985909\n",
      "\tspeed: 0.1155s/iter; left time: 9389.6240s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1067271\n",
      "\tspeed: 0.1178s/iter; left time: 9560.6694s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0852604\n",
      "\tspeed: 0.1159s/iter; left time: 9395.5152s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0817025\n",
      "\tspeed: 0.1128s/iter; left time: 9132.8653s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0846763\n",
      "\tspeed: 0.1128s/iter; left time: 9121.4540s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0871173\n",
      "\tspeed: 0.1136s/iter; left time: 9175.4806s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0959664\n",
      "\tspeed: 0.1137s/iter; left time: 9174.5179s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0784686\n",
      "\tspeed: 0.1134s/iter; left time: 9141.7126s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0950680\n",
      "\tspeed: 0.1138s/iter; left time: 9156.8940s\n",
      "Epoch: 2 cost time: 00h:08m:30.16s\n",
      "Epoch: 2 | Train Loss: 0.0889246 Vali Loss: 0.0918921 Test Loss: 0.0945349\n",
      "Validation loss decreased (0.097112 --> 0.091892).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1014844\n",
      "\tspeed: 1.6040s/iter; left time: 128842.4170s\n",
      "\titers: 200, epoch: 3 | loss: 0.0968780\n",
      "\tspeed: 0.1137s/iter; left time: 9122.6059s\n",
      "\titers: 300, epoch: 3 | loss: 0.0694327\n",
      "\tspeed: 0.1162s/iter; left time: 9306.7944s\n",
      "\titers: 400, epoch: 3 | loss: 0.0818534\n",
      "\tspeed: 0.1131s/iter; left time: 9049.8084s\n",
      "\titers: 500, epoch: 3 | loss: 0.1179019\n",
      "\tspeed: 0.1105s/iter; left time: 8835.4020s\n",
      "\titers: 600, epoch: 3 | loss: 0.0902409\n",
      "\tspeed: 0.1127s/iter; left time: 8993.7143s\n",
      "\titers: 700, epoch: 3 | loss: 0.0945199\n",
      "\tspeed: 0.1126s/iter; left time: 8973.5594s\n",
      "\titers: 800, epoch: 3 | loss: 0.0943223\n",
      "\tspeed: 0.1126s/iter; left time: 8965.1199s\n",
      "\titers: 900, epoch: 3 | loss: 0.0608129\n",
      "\tspeed: 0.1143s/iter; left time: 9089.2406s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0815312\n",
      "\tspeed: 0.1142s/iter; left time: 9067.5614s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0903181\n",
      "\tspeed: 0.1137s/iter; left time: 9020.1890s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0806858\n",
      "\tspeed: 0.1142s/iter; left time: 9044.4942s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0762755\n",
      "\tspeed: 0.1140s/iter; left time: 9017.3213s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0923751\n",
      "\tspeed: 0.1116s/iter; left time: 8821.8508s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0904863\n",
      "\tspeed: 0.1127s/iter; left time: 8895.6489s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0592667\n",
      "\tspeed: 0.1135s/iter; left time: 8942.9849s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0708085\n",
      "\tspeed: 0.1157s/iter; left time: 9110.4364s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0865551\n",
      "\tspeed: 0.1119s/iter; left time: 8795.0520s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0666124\n",
      "\tspeed: 0.1124s/iter; left time: 8822.9030s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0717531\n",
      "\tspeed: 0.1142s/iter; left time: 8953.9096s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1037164\n",
      "\tspeed: 0.1134s/iter; left time: 8884.8049s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0819335\n",
      "\tspeed: 0.1118s/iter; left time: 8748.3673s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0837101\n",
      "\tspeed: 0.1114s/iter; left time: 8701.8710s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0792184\n",
      "\tspeed: 0.1116s/iter; left time: 8710.8425s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0968816\n",
      "\tspeed: 0.1135s/iter; left time: 8845.6248s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0933213\n",
      "\tspeed: 0.1132s/iter; left time: 8806.5863s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0743180\n",
      "\tspeed: 0.1126s/iter; left time: 8748.4160s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0937308\n",
      "\tspeed: 0.1126s/iter; left time: 8740.1547s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0838305\n",
      "\tspeed: 0.1137s/iter; left time: 8813.0296s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0647273\n",
      "\tspeed: 0.1146s/iter; left time: 8875.4268s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0913395\n",
      "\tspeed: 0.1137s/iter; left time: 8793.1195s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0819122\n",
      "\tspeed: 0.1131s/iter; left time: 8732.8662s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0788185\n",
      "\tspeed: 0.1123s/iter; left time: 8659.5411s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0784381\n",
      "\tspeed: 0.1121s/iter; left time: 8636.4983s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0678186\n",
      "\tspeed: 0.1117s/iter; left time: 8588.7881s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0698414\n",
      "\tspeed: 0.1137s/iter; left time: 8736.1703s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0900844\n",
      "\tspeed: 0.1122s/iter; left time: 8606.1957s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0756922\n",
      "\tspeed: 0.1149s/iter; left time: 8805.2898s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0685307\n",
      "\tspeed: 0.1125s/iter; left time: 8610.4216s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0735308\n",
      "\tspeed: 0.1119s/iter; left time: 8552.9052s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0800254\n",
      "\tspeed: 0.1119s/iter; left time: 8542.2113s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0855180\n",
      "\tspeed: 0.1154s/iter; left time: 8796.0084s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0667157\n",
      "\tspeed: 0.1144s/iter; left time: 8708.6890s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0843554\n",
      "\tspeed: 0.1124s/iter; left time: 8544.1850s\n",
      "Epoch: 3 cost time: 00h:08m:26.09s\n",
      "Epoch: 3 | Train Loss: 0.0853331 Vali Loss: 0.0911640 Test Loss: 0.0942841\n",
      "Validation loss decreased (0.091892 --> 0.091164).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0739329\n",
      "\tspeed: 1.5815s/iter; left time: 119968.3080s\n",
      "\titers: 200, epoch: 4 | loss: 0.1006263\n",
      "\tspeed: 0.1141s/iter; left time: 8647.2176s\n",
      "\titers: 300, epoch: 4 | loss: 0.0738416\n",
      "\tspeed: 0.1155s/iter; left time: 8739.9632s\n",
      "\titers: 400, epoch: 4 | loss: 0.0781351\n",
      "\tspeed: 0.1131s/iter; left time: 8548.7698s\n",
      "\titers: 500, epoch: 4 | loss: 0.0675929\n",
      "\tspeed: 0.1151s/iter; left time: 8687.4375s\n",
      "\titers: 600, epoch: 4 | loss: 0.0722866\n",
      "\tspeed: 0.1138s/iter; left time: 8578.3459s\n",
      "\titers: 700, epoch: 4 | loss: 0.0852794\n",
      "\tspeed: 0.1146s/iter; left time: 8627.3161s\n",
      "\titers: 800, epoch: 4 | loss: 0.0830615\n",
      "\tspeed: 0.1136s/iter; left time: 8539.2426s\n",
      "\titers: 900, epoch: 4 | loss: 0.0746369\n",
      "\tspeed: 0.1127s/iter; left time: 8458.1302s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0714534\n",
      "\tspeed: 0.1134s/iter; left time: 8499.4897s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0721987\n",
      "\tspeed: 0.1133s/iter; left time: 8483.5015s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0848372\n",
      "\tspeed: 0.1142s/iter; left time: 8540.1918s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0901184\n",
      "\tspeed: 0.1126s/iter; left time: 8406.3739s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0873635\n",
      "\tspeed: 0.1127s/iter; left time: 8402.5782s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0672005\n",
      "\tspeed: 0.1121s/iter; left time: 8348.9412s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0815371\n",
      "\tspeed: 0.1119s/iter; left time: 8321.4581s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0967667\n",
      "\tspeed: 0.1143s/iter; left time: 8487.8214s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1003437\n",
      "\tspeed: 0.1123s/iter; left time: 8328.0192s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0772678\n",
      "\tspeed: 0.1121s/iter; left time: 8303.8600s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1023371\n",
      "\tspeed: 0.1112s/iter; left time: 8226.3107s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0959407\n",
      "\tspeed: 0.1126s/iter; left time: 8313.9698s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0707141\n",
      "\tspeed: 0.1121s/iter; left time: 8266.0775s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1181110\n",
      "\tspeed: 0.1130s/iter; left time: 8323.1719s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0988108\n",
      "\tspeed: 0.1115s/iter; left time: 8201.0479s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1022194\n",
      "\tspeed: 0.1111s/iter; left time: 8160.0609s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0755848\n",
      "\tspeed: 0.1114s/iter; left time: 8168.9965s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0852136\n",
      "\tspeed: 0.1122s/iter; left time: 8220.4746s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1073938\n",
      "\tspeed: 0.1111s/iter; left time: 8125.4232s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0776819\n",
      "\tspeed: 0.1122s/iter; left time: 8200.1518s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0915424\n",
      "\tspeed: 0.1117s/iter; left time: 8146.3091s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0914509\n",
      "\tspeed: 0.1155s/iter; left time: 8412.5511s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0769955\n",
      "\tspeed: 0.1168s/iter; left time: 8498.2199s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0912195\n",
      "\tspeed: 0.1177s/iter; left time: 8550.1338s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0864609\n",
      "\tspeed: 0.1169s/iter; left time: 8485.3160s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0855569\n",
      "\tspeed: 0.1181s/iter; left time: 8557.3971s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0880700\n",
      "\tspeed: 0.1181s/iter; left time: 8544.6155s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0775939\n",
      "\tspeed: 0.1193s/iter; left time: 8618.5048s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0855137\n",
      "\tspeed: 0.1171s/iter; left time: 8451.2831s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0877626\n",
      "\tspeed: 0.1153s/iter; left time: 8308.4451s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0802846\n",
      "\tspeed: 0.1172s/iter; left time: 8436.5780s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0870235\n",
      "\tspeed: 0.1202s/iter; left time: 8634.0718s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0716099\n",
      "\tspeed: 0.1207s/iter; left time: 8660.1850s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0780072\n",
      "\tspeed: 0.1156s/iter; left time: 8281.9815s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0948033\n",
      "\tspeed: 0.1183s/iter; left time: 8463.4481s\n",
      "Epoch: 4 cost time: 00h:08m:31.84s\n",
      "Epoch: 4 | Train Loss: 0.0836256 Vali Loss: 0.0897763 Test Loss: 0.0933365\n",
      "Validation loss decreased (0.091164 --> 0.089776).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0797964\n",
      "\tspeed: 1.6464s/iter; left time: 117532.0136s\n",
      "\titers: 200, epoch: 5 | loss: 0.0838478\n",
      "\tspeed: 0.1194s/iter; left time: 8512.3885s\n",
      "\titers: 300, epoch: 5 | loss: 0.0742453\n",
      "\tspeed: 0.1179s/iter; left time: 8390.0387s\n",
      "\titers: 400, epoch: 5 | loss: 0.0888728\n",
      "\tspeed: 0.1203s/iter; left time: 8551.0436s\n",
      "\titers: 500, epoch: 5 | loss: 0.0898506\n",
      "\tspeed: 0.1186s/iter; left time: 8422.4576s\n",
      "\titers: 600, epoch: 5 | loss: 0.0773116\n",
      "\tspeed: 0.1140s/iter; left time: 8081.0151s\n",
      "\titers: 700, epoch: 5 | loss: 0.0626944\n",
      "\tspeed: 0.1191s/iter; left time: 8430.4481s\n",
      "\titers: 800, epoch: 5 | loss: 0.0781008\n",
      "\tspeed: 0.1205s/iter; left time: 8519.7659s\n",
      "\titers: 900, epoch: 5 | loss: 0.0825634\n",
      "\tspeed: 0.1205s/iter; left time: 8509.4591s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0848518\n",
      "\tspeed: 0.1189s/iter; left time: 8377.6728s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0829829\n",
      "\tspeed: 0.1204s/iter; left time: 8472.5536s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0997171\n",
      "\tspeed: 0.1201s/iter; left time: 8443.6639s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0798143\n",
      "\tspeed: 0.1197s/iter; left time: 8402.1515s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0764720\n",
      "\tspeed: 0.1176s/iter; left time: 8243.6340s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0732888\n",
      "\tspeed: 0.1193s/iter; left time: 8351.0889s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0836849\n",
      "\tspeed: 0.1157s/iter; left time: 8085.6975s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0724503\n",
      "\tspeed: 0.1156s/iter; left time: 8069.1478s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0894237\n",
      "\tspeed: 0.1198s/iter; left time: 8348.4672s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0685364\n",
      "\tspeed: 0.1178s/iter; left time: 8200.5653s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0697801\n",
      "\tspeed: 0.1175s/iter; left time: 8164.8331s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0814900\n",
      "\tspeed: 0.1169s/iter; left time: 8113.5792s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0934933\n",
      "\tspeed: 0.1142s/iter; left time: 7911.7020s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861936\n",
      "\tspeed: 0.1145s/iter; left time: 7919.7570s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0938417\n",
      "\tspeed: 0.1149s/iter; left time: 7940.1099s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0799977\n",
      "\tspeed: 0.1189s/iter; left time: 8205.5438s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0855416\n",
      "\tspeed: 0.1195s/iter; left time: 8229.9662s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0920220\n",
      "\tspeed: 0.1158s/iter; left time: 7968.0307s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0856894\n",
      "\tspeed: 0.1187s/iter; left time: 8154.6670s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0774287\n",
      "\tspeed: 0.1181s/iter; left time: 8098.6651s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0683412\n",
      "\tspeed: 0.1165s/iter; left time: 7976.2434s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0973268\n",
      "\tspeed: 0.1172s/iter; left time: 8015.3452s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0831058\n",
      "\tspeed: 0.1146s/iter; left time: 7827.2519s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0682414\n",
      "\tspeed: 0.1170s/iter; left time: 7980.1976s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0845700\n",
      "\tspeed: 0.1196s/iter; left time: 8140.1191s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0771565\n",
      "\tspeed: 0.1187s/iter; left time: 8072.2449s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0795133\n",
      "\tspeed: 0.1174s/iter; left time: 7969.1133s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0733312\n",
      "\tspeed: 0.1185s/iter; left time: 8031.3167s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0908493\n",
      "\tspeed: 0.1188s/iter; left time: 8044.7092s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0670330\n",
      "\tspeed: 0.1188s/iter; left time: 8032.0109s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0631396\n",
      "\tspeed: 0.1181s/iter; left time: 7969.3271s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0828409\n",
      "\tspeed: 0.1209s/iter; left time: 8148.0262s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0800139\n",
      "\tspeed: 0.1177s/iter; left time: 7917.2847s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0884054\n",
      "\tspeed: 0.1186s/iter; left time: 7967.7376s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0831661\n",
      "\tspeed: 0.1184s/iter; left time: 7943.2696s\n",
      "Epoch: 5 cost time: 00h:08m:48.26s\n",
      "Epoch: 5 | Train Loss: 0.0823728 Vali Loss: 0.0899557 Test Loss: 0.0942943\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1001324\n",
      "\tspeed: 1.6218s/iter; left time: 108534.5492s\n",
      "\titers: 200, epoch: 6 | loss: 0.0886922\n",
      "\tspeed: 0.1213s/iter; left time: 8105.9663s\n",
      "\titers: 300, epoch: 6 | loss: 0.0843214\n",
      "\tspeed: 0.1200s/iter; left time: 8005.3656s\n",
      "\titers: 400, epoch: 6 | loss: 0.0734667\n",
      "\tspeed: 0.1185s/iter; left time: 7896.9387s\n",
      "\titers: 500, epoch: 6 | loss: 0.0924280\n",
      "\tspeed: 0.1199s/iter; left time: 7979.1377s\n",
      "\titers: 600, epoch: 6 | loss: 0.0591974\n",
      "\tspeed: 0.1180s/iter; left time: 7835.7974s\n",
      "\titers: 700, epoch: 6 | loss: 0.0633274\n",
      "\tspeed: 0.1191s/iter; left time: 7899.5425s\n",
      "\titers: 800, epoch: 6 | loss: 0.0738303\n",
      "\tspeed: 0.1198s/iter; left time: 7930.7576s\n",
      "\titers: 900, epoch: 6 | loss: 0.1071520\n",
      "\tspeed: 0.1218s/iter; left time: 8053.4950s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0735545\n",
      "\tspeed: 0.1204s/iter; left time: 7946.3154s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0729964\n",
      "\tspeed: 0.1196s/iter; left time: 7881.9557s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0962333\n",
      "\tspeed: 0.1199s/iter; left time: 7892.1226s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0923874\n",
      "\tspeed: 0.1202s/iter; left time: 7901.9868s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0971264\n",
      "\tspeed: 0.1218s/iter; left time: 7992.7890s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0751228\n",
      "\tspeed: 0.1189s/iter; left time: 7790.8841s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0779485\n",
      "\tspeed: 0.1172s/iter; left time: 7668.9793s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0823796\n",
      "\tspeed: 0.1168s/iter; left time: 7627.4802s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0782527\n",
      "\tspeed: 0.1174s/iter; left time: 7659.5757s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0854593\n",
      "\tspeed: 0.1211s/iter; left time: 7887.2417s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0940582\n",
      "\tspeed: 0.1216s/iter; left time: 7906.6408s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0806722\n",
      "\tspeed: 0.1156s/iter; left time: 7503.7319s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0820966\n",
      "\tspeed: 0.1188s/iter; left time: 7698.6315s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0738427\n",
      "\tspeed: 0.1190s/iter; left time: 7704.3061s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0772219\n",
      "\tspeed: 0.1199s/iter; left time: 7749.0500s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0898360\n",
      "\tspeed: 0.1188s/iter; left time: 7662.1246s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0820411\n",
      "\tspeed: 0.1181s/iter; left time: 7605.2153s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0796860\n",
      "\tspeed: 0.1185s/iter; left time: 7624.4024s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989802\n",
      "\tspeed: 0.1191s/iter; left time: 7649.2150s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0941869\n",
      "\tspeed: 0.1154s/iter; left time: 7401.6936s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0823741\n",
      "\tspeed: 0.1177s/iter; left time: 7534.3386s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0726927\n",
      "\tspeed: 0.1189s/iter; left time: 7598.8294s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0867132\n",
      "\tspeed: 0.1197s/iter; left time: 7638.2616s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0877910\n",
      "\tspeed: 0.1177s/iter; left time: 7499.8711s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1008532\n",
      "\tspeed: 0.1194s/iter; left time: 7595.2404s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0882527\n",
      "\tspeed: 0.1193s/iter; left time: 7575.1135s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0798398\n",
      "\tspeed: 0.1182s/iter; left time: 7498.4859s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0708239\n",
      "\tspeed: 0.1203s/iter; left time: 7614.6941s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0720952\n",
      "\tspeed: 0.1156s/iter; left time: 7310.8871s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0832081\n",
      "\tspeed: 0.1173s/iter; left time: 7402.6487s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0772891\n",
      "\tspeed: 0.1173s/iter; left time: 7395.4665s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0615834\n",
      "\tspeed: 0.1181s/iter; left time: 7433.2315s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0647544\n",
      "\tspeed: 0.1197s/iter; left time: 7521.8139s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0650132\n",
      "\tspeed: 0.1182s/iter; left time: 7414.9879s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0766972\n",
      "\tspeed: 0.1205s/iter; left time: 7544.8569s\n",
      "Epoch: 6 cost time: 00h:08m:52.30s\n",
      "Epoch: 6 | Train Loss: 0.0813646 Vali Loss: 0.0895673 Test Loss: 0.0940461\n",
      "Validation loss decreased (0.089776 --> 0.089567).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0841343\n",
      "\tspeed: 1.6530s/iter; left time: 103235.3819s\n",
      "\titers: 200, epoch: 7 | loss: 0.0774401\n",
      "\tspeed: 0.1200s/iter; left time: 7484.1355s\n",
      "\titers: 300, epoch: 7 | loss: 0.0646475\n",
      "\tspeed: 0.1200s/iter; left time: 7471.3773s\n",
      "\titers: 400, epoch: 7 | loss: 0.0766103\n",
      "\tspeed: 0.1180s/iter; left time: 7334.7613s\n",
      "\titers: 500, epoch: 7 | loss: 0.0919630\n",
      "\tspeed: 0.1200s/iter; left time: 7444.7187s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806891\n",
      "\tspeed: 0.1211s/iter; left time: 7504.7698s\n",
      "\titers: 700, epoch: 7 | loss: 0.0947759\n",
      "\tspeed: 0.1203s/iter; left time: 7441.6268s\n",
      "\titers: 800, epoch: 7 | loss: 0.0886568\n",
      "\tspeed: 0.1175s/iter; left time: 7256.5548s\n",
      "\titers: 900, epoch: 7 | loss: 0.0786587\n",
      "\tspeed: 0.1190s/iter; left time: 7336.4875s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0800228\n",
      "\tspeed: 0.1187s/iter; left time: 7306.3839s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0899325\n",
      "\tspeed: 0.1206s/iter; left time: 7411.1211s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0851249\n",
      "\tspeed: 0.1214s/iter; left time: 7446.7356s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0694570\n",
      "\tspeed: 0.1176s/iter; left time: 7206.4126s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0889620\n",
      "\tspeed: 0.1177s/iter; left time: 7198.9649s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0885222\n",
      "\tspeed: 0.1168s/iter; left time: 7131.5664s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0703967\n",
      "\tspeed: 0.1198s/iter; left time: 7302.7558s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0967138\n",
      "\tspeed: 0.1184s/iter; left time: 7206.5879s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0742753\n",
      "\tspeed: 0.1198s/iter; left time: 7275.4256s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0714891\n",
      "\tspeed: 0.1200s/iter; left time: 7276.5838s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0690335\n",
      "\tspeed: 0.1203s/iter; left time: 7285.5115s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0778978\n",
      "\tspeed: 0.1159s/iter; left time: 7006.8084s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0716214\n",
      "\tspeed: 0.1175s/iter; left time: 7091.8659s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0747692\n",
      "\tspeed: 0.1168s/iter; left time: 7040.1868s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0834282\n",
      "\tspeed: 0.1175s/iter; left time: 7065.8316s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0855097\n",
      "\tspeed: 0.1192s/iter; left time: 7161.0485s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0906422\n",
      "\tspeed: 0.1192s/iter; left time: 7149.1376s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0708709\n",
      "\tspeed: 0.1180s/iter; left time: 7059.7972s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0859929\n",
      "\tspeed: 0.1210s/iter; left time: 7230.0027s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0687493\n",
      "\tspeed: 0.1198s/iter; left time: 7147.2599s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0796787\n",
      "\tspeed: 0.1212s/iter; left time: 7216.8474s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0671332\n",
      "\tspeed: 0.1196s/iter; left time: 7109.6244s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0683739\n",
      "\tspeed: 0.1167s/iter; left time: 6927.9818s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0691068\n",
      "\tspeed: 0.1165s/iter; left time: 6902.6622s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0798232\n",
      "\tspeed: 0.1187s/iter; left time: 7021.1931s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0687565\n",
      "\tspeed: 0.1172s/iter; left time: 6919.3256s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0730739\n",
      "\tspeed: 0.1182s/iter; left time: 6967.3658s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0768365\n",
      "\tspeed: 0.1170s/iter; left time: 6888.0754s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0917242\n",
      "\tspeed: 0.1202s/iter; left time: 7063.0170s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0721889\n",
      "\tspeed: 0.1199s/iter; left time: 7035.1511s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1057147\n",
      "\tspeed: 0.1143s/iter; left time: 6692.7616s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0877508\n",
      "\tspeed: 0.1129s/iter; left time: 6600.3418s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0842020\n",
      "\tspeed: 0.1130s/iter; left time: 6595.8754s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0788388\n",
      "\tspeed: 0.1125s/iter; left time: 6555.2348s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0780672\n",
      "\tspeed: 0.1129s/iter; left time: 6565.9990s\n",
      "Epoch: 7 cost time: 00h:08m:48.62s\n",
      "Epoch: 7 | Train Loss: 0.0802900 Vali Loss: 0.0892569 Test Loss: 0.0938624\n",
      "Validation loss decreased (0.089567 --> 0.089257).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0878868\n",
      "\tspeed: 1.5867s/iter; left time: 92005.2740s\n",
      "\titers: 200, epoch: 8 | loss: 0.0819466\n",
      "\tspeed: 0.1146s/iter; left time: 6632.6594s\n",
      "\titers: 300, epoch: 8 | loss: 0.0872314\n",
      "\tspeed: 0.1134s/iter; left time: 6552.0696s\n",
      "\titers: 400, epoch: 8 | loss: 0.0871727\n",
      "\tspeed: 0.1149s/iter; left time: 6629.7226s\n",
      "\titers: 500, epoch: 8 | loss: 0.0690386\n",
      "\tspeed: 0.1146s/iter; left time: 6600.8306s\n",
      "\titers: 600, epoch: 8 | loss: 0.0786107\n",
      "\tspeed: 0.1151s/iter; left time: 6617.8909s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654001\n",
      "\tspeed: 0.1152s/iter; left time: 6608.6635s\n",
      "\titers: 800, epoch: 8 | loss: 0.0706811\n",
      "\tspeed: 0.1148s/iter; left time: 6578.6997s\n",
      "\titers: 900, epoch: 8 | loss: 0.0823400\n",
      "\tspeed: 0.1177s/iter; left time: 6731.2845s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0656414\n",
      "\tspeed: 0.1133s/iter; left time: 6466.9469s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0717802\n",
      "\tspeed: 0.1147s/iter; left time: 6537.9668s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0914649\n",
      "\tspeed: 0.1137s/iter; left time: 6465.6763s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0788195\n",
      "\tspeed: 0.1150s/iter; left time: 6529.7991s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0938862\n",
      "\tspeed: 0.1143s/iter; left time: 6480.2686s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0883940\n",
      "\tspeed: 0.1134s/iter; left time: 6417.5553s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0923529\n",
      "\tspeed: 0.1169s/iter; left time: 6605.7957s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1006962\n",
      "\tspeed: 0.1165s/iter; left time: 6570.9238s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0933289\n",
      "\tspeed: 0.1140s/iter; left time: 6417.5089s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0580641\n",
      "\tspeed: 0.1138s/iter; left time: 6393.3432s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0778803\n",
      "\tspeed: 0.1146s/iter; left time: 6429.3977s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0813645\n",
      "\tspeed: 0.1118s/iter; left time: 6261.6275s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0837200\n",
      "\tspeed: 0.1152s/iter; left time: 6438.9706s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0852636\n",
      "\tspeed: 0.1148s/iter; left time: 6404.3086s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0721436\n",
      "\tspeed: 0.1121s/iter; left time: 6240.5479s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0740744\n",
      "\tspeed: 0.1138s/iter; left time: 6324.6577s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0824070\n",
      "\tspeed: 0.1133s/iter; left time: 6285.7820s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0793054\n",
      "\tspeed: 0.1134s/iter; left time: 6278.7559s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0864588\n",
      "\tspeed: 0.1141s/iter; left time: 6305.7187s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0741376\n",
      "\tspeed: 0.1149s/iter; left time: 6342.0597s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0899623\n",
      "\tspeed: 0.1143s/iter; left time: 6298.8285s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0805951\n",
      "\tspeed: 0.1152s/iter; left time: 6334.0774s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0612919\n",
      "\tspeed: 0.1161s/iter; left time: 6371.3833s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0753164\n",
      "\tspeed: 0.1166s/iter; left time: 6387.3990s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0797729\n",
      "\tspeed: 0.1152s/iter; left time: 6298.6305s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0591326\n",
      "\tspeed: 0.1120s/iter; left time: 6112.7558s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0769536\n",
      "\tspeed: 0.1158s/iter; left time: 6309.2520s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0768749\n",
      "\tspeed: 0.1139s/iter; left time: 6192.3924s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0756438\n",
      "\tspeed: 0.1140s/iter; left time: 6185.8640s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0782435\n",
      "\tspeed: 0.1146s/iter; left time: 6211.7814s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0735168\n",
      "\tspeed: 0.1134s/iter; left time: 6135.4879s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0852071\n",
      "\tspeed: 0.1144s/iter; left time: 6174.2145s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0820105\n",
      "\tspeed: 0.1162s/iter; left time: 6259.5710s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0806806\n",
      "\tspeed: 0.1163s/iter; left time: 6256.4244s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0883520\n",
      "\tspeed: 0.1131s/iter; left time: 6069.3030s\n",
      "Epoch: 8 cost time: 00h:08m:32.31s\n",
      "Epoch: 8 | Train Loss: 0.0793348 Vali Loss: 0.0892188 Test Loss: 0.0953779\n",
      "Validation loss decreased (0.089257 --> 0.089219).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0749750\n",
      "\tspeed: 1.5935s/iter; left time: 85278.7636s\n",
      "\titers: 200, epoch: 9 | loss: 0.0862554\n",
      "\tspeed: 0.1149s/iter; left time: 6139.0793s\n",
      "\titers: 300, epoch: 9 | loss: 0.0647535\n",
      "\tspeed: 0.1153s/iter; left time: 6147.8544s\n",
      "\titers: 400, epoch: 9 | loss: 0.0730983\n",
      "\tspeed: 0.1161s/iter; left time: 6177.1792s\n",
      "\titers: 500, epoch: 9 | loss: 0.0695380\n",
      "\tspeed: 0.1143s/iter; left time: 6073.9151s\n",
      "\titers: 600, epoch: 9 | loss: 0.0691377\n",
      "\tspeed: 0.1138s/iter; left time: 6031.9767s\n",
      "\titers: 700, epoch: 9 | loss: 0.0765528\n",
      "\tspeed: 0.1150s/iter; left time: 6086.4702s\n",
      "\titers: 800, epoch: 9 | loss: 0.0613169\n",
      "\tspeed: 0.1157s/iter; left time: 6112.7932s\n",
      "\titers: 900, epoch: 9 | loss: 0.0732772\n",
      "\tspeed: 0.1128s/iter; left time: 5946.8314s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0607491\n",
      "\tspeed: 0.1141s/iter; left time: 6005.3762s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0597287\n",
      "\tspeed: 0.1145s/iter; left time: 6014.2623s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0944503\n",
      "\tspeed: 0.1158s/iter; left time: 6068.9104s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0612135\n",
      "\tspeed: 0.1167s/iter; left time: 6104.9545s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0633327\n",
      "\tspeed: 0.1161s/iter; left time: 6061.6466s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0837415\n",
      "\tspeed: 0.1161s/iter; left time: 6052.5368s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0839318\n",
      "\tspeed: 0.1145s/iter; left time: 5957.2857s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0879491\n",
      "\tspeed: 0.1153s/iter; left time: 5987.8238s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0665905\n",
      "\tspeed: 0.1159s/iter; left time: 6007.1897s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0713467\n",
      "\tspeed: 0.1132s/iter; left time: 5856.2024s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0669027\n",
      "\tspeed: 0.1109s/iter; left time: 5722.6558s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0749533\n",
      "\tspeed: 0.1125s/iter; left time: 5795.3411s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0823501\n",
      "\tspeed: 0.1112s/iter; left time: 5718.2462s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0919251\n",
      "\tspeed: 0.1139s/iter; left time: 5844.6386s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0902282\n",
      "\tspeed: 0.1126s/iter; left time: 5768.5829s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0695416\n",
      "\tspeed: 0.1117s/iter; left time: 5711.8656s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0872029\n",
      "\tspeed: 0.1133s/iter; left time: 5778.4650s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0668999\n",
      "\tspeed: 0.1119s/iter; left time: 5698.7334s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0708064\n",
      "\tspeed: 0.1127s/iter; left time: 5725.9843s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0776473\n",
      "\tspeed: 0.1123s/iter; left time: 5694.7808s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0897343\n",
      "\tspeed: 0.1122s/iter; left time: 5681.0087s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0689395\n",
      "\tspeed: 0.1124s/iter; left time: 5679.9443s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0872946\n",
      "\tspeed: 0.1139s/iter; left time: 5744.6518s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0869129\n",
      "\tspeed: 0.1114s/iter; left time: 5604.8787s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0875106\n",
      "\tspeed: 0.1127s/iter; left time: 5660.6692s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0698793\n",
      "\tspeed: 0.1125s/iter; left time: 5636.6617s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0638036\n",
      "\tspeed: 0.1135s/iter; left time: 5676.5612s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0692865\n",
      "\tspeed: 0.1122s/iter; left time: 5601.1650s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0771837\n",
      "\tspeed: 0.1136s/iter; left time: 5659.3342s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0842480\n",
      "\tspeed: 0.1127s/iter; left time: 5605.2598s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0725595\n",
      "\tspeed: 0.1126s/iter; left time: 5588.1288s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0908936\n",
      "\tspeed: 0.1121s/iter; left time: 5549.9859s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0893085\n",
      "\tspeed: 0.1131s/iter; left time: 5589.3018s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0930462\n",
      "\tspeed: 0.1122s/iter; left time: 5534.2020s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0681519\n",
      "\tspeed: 0.1128s/iter; left time: 5549.7471s\n",
      "Epoch: 9 cost time: 00h:08m:28.34s\n",
      "Epoch: 9 | Train Loss: 0.0783692 Vali Loss: 0.0899408 Test Loss: 0.0950079\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0844053\n",
      "\tspeed: 1.5703s/iter; left time: 77023.2284s\n",
      "\titers: 200, epoch: 10 | loss: 0.1195817\n",
      "\tspeed: 0.1156s/iter; left time: 5658.7668s\n",
      "\titers: 300, epoch: 10 | loss: 0.0776654\n",
      "\tspeed: 0.1157s/iter; left time: 5653.6352s\n",
      "\titers: 400, epoch: 10 | loss: 0.0721945\n",
      "\tspeed: 0.1144s/iter; left time: 5577.8280s\n",
      "\titers: 500, epoch: 10 | loss: 0.0767422\n",
      "\tspeed: 0.1146s/iter; left time: 5576.6377s\n",
      "\titers: 600, epoch: 10 | loss: 0.0819118\n",
      "\tspeed: 0.1156s/iter; left time: 5610.0326s\n",
      "\titers: 700, epoch: 10 | loss: 0.0688704\n",
      "\tspeed: 0.1144s/iter; left time: 5540.6791s\n",
      "\titers: 800, epoch: 10 | loss: 0.1010805\n",
      "\tspeed: 0.1162s/iter; left time: 5619.3736s\n",
      "\titers: 900, epoch: 10 | loss: 0.0890266\n",
      "\tspeed: 0.1134s/iter; left time: 5473.4191s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0743395\n",
      "\tspeed: 0.1156s/iter; left time: 5567.6560s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0623439\n",
      "\tspeed: 0.1154s/iter; left time: 5542.6937s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0886193\n",
      "\tspeed: 0.1170s/iter; left time: 5610.6100s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0903579\n",
      "\tspeed: 0.1158s/iter; left time: 5541.5628s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0970001\n",
      "\tspeed: 0.1163s/iter; left time: 5550.9740s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0763897\n",
      "\tspeed: 0.1151s/iter; left time: 5483.7668s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0659897\n",
      "\tspeed: 0.1148s/iter; left time: 5460.3132s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0725043\n",
      "\tspeed: 0.1165s/iter; left time: 5526.4024s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0614413\n",
      "\tspeed: 0.1146s/iter; left time: 5425.1157s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0763310\n",
      "\tspeed: 0.1134s/iter; left time: 5356.3255s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0819842\n",
      "\tspeed: 0.1137s/iter; left time: 5362.4754s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0654081\n",
      "\tspeed: 0.1132s/iter; left time: 5325.6774s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0653410\n",
      "\tspeed: 0.1146s/iter; left time: 5380.6889s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0690481\n",
      "\tspeed: 0.1151s/iter; left time: 5390.4317s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0936227\n",
      "\tspeed: 0.1119s/iter; left time: 5232.1446s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0709767\n",
      "\tspeed: 0.1150s/iter; left time: 5366.5604s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0593837\n",
      "\tspeed: 0.1114s/iter; left time: 5186.2645s\n",
      "\titers: 2700, epoch: 10 | loss: 0.1029781\n",
      "\tspeed: 0.1120s/iter; left time: 5201.3187s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0660925\n",
      "\tspeed: 0.1124s/iter; left time: 5209.6535s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0810724\n",
      "\tspeed: 0.1154s/iter; left time: 5335.9264s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0861325\n",
      "\tspeed: 0.1147s/iter; left time: 5292.0603s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0749175\n",
      "\tspeed: 0.1137s/iter; left time: 5237.6337s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0772474\n",
      "\tspeed: 0.1164s/iter; left time: 5349.7385s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0663424\n",
      "\tspeed: 0.1137s/iter; left time: 5214.7671s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0681792\n",
      "\tspeed: 0.1141s/iter; left time: 5221.7219s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0826567\n",
      "\tspeed: 0.1140s/iter; left time: 5203.0824s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0750645\n",
      "\tspeed: 0.1131s/iter; left time: 5151.3906s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0935027\n",
      "\tspeed: 0.1151s/iter; left time: 5229.2264s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0815245\n",
      "\tspeed: 0.1150s/iter; left time: 5214.8541s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0724220\n",
      "\tspeed: 0.1144s/iter; left time: 5177.2966s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0900971\n",
      "\tspeed: 0.1154s/iter; left time: 5208.8186s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0711545\n",
      "\tspeed: 0.1136s/iter; left time: 5116.8096s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0712163\n",
      "\tspeed: 0.1162s/iter; left time: 5225.1819s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0879692\n",
      "\tspeed: 0.1138s/iter; left time: 5103.8100s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0614536\n",
      "\tspeed: 0.1149s/iter; left time: 5140.8918s\n",
      "Epoch: 10 cost time: 00h:08m:32.57s\n",
      "Epoch: 10 | Train Loss: 0.0775587 Vali Loss: 0.0896562 Test Loss: 0.0955003\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0843296\n",
      "\tspeed: 1.5739s/iter; left time: 70164.0240s\n",
      "\titers: 200, epoch: 11 | loss: 0.0733311\n",
      "\tspeed: 0.1145s/iter; left time: 5093.0879s\n",
      "\titers: 300, epoch: 11 | loss: 0.0601665\n",
      "\tspeed: 0.1165s/iter; left time: 5171.7973s\n",
      "\titers: 400, epoch: 11 | loss: 0.0716488\n",
      "\tspeed: 0.1158s/iter; left time: 5126.9430s\n",
      "\titers: 500, epoch: 11 | loss: 0.0750899\n",
      "\tspeed: 0.1165s/iter; left time: 5148.0669s\n",
      "\titers: 600, epoch: 11 | loss: 0.0896189\n",
      "\tspeed: 0.1153s/iter; left time: 5083.1043s\n",
      "\titers: 700, epoch: 11 | loss: 0.0716856\n",
      "\tspeed: 0.1139s/iter; left time: 5009.0067s\n",
      "\titers: 800, epoch: 11 | loss: 0.0796735\n",
      "\tspeed: 0.1153s/iter; left time: 5057.4234s\n",
      "\titers: 900, epoch: 11 | loss: 0.0532320\n",
      "\tspeed: 0.1148s/iter; left time: 5027.0354s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0739114\n",
      "\tspeed: 0.1165s/iter; left time: 5090.5324s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0947660\n",
      "\tspeed: 0.1156s/iter; left time: 5038.4828s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0835808\n",
      "\tspeed: 0.1145s/iter; left time: 4977.6359s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0754700\n",
      "\tspeed: 0.1149s/iter; left time: 4984.0475s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0658226\n",
      "\tspeed: 0.1150s/iter; left time: 4975.6781s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0543421\n",
      "\tspeed: 0.1149s/iter; left time: 4960.4601s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0710039\n",
      "\tspeed: 0.1151s/iter; left time: 4957.1768s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0833827\n",
      "\tspeed: 0.1159s/iter; left time: 4982.1454s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0674744\n",
      "\tspeed: 0.1155s/iter; left time: 4951.2929s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0692966\n",
      "\tspeed: 0.1154s/iter; left time: 4936.1148s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0941740\n",
      "\tspeed: 0.1148s/iter; left time: 4899.7874s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0860964\n",
      "\tspeed: 0.1159s/iter; left time: 4934.5398s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0810799\n",
      "\tspeed: 0.1154s/iter; left time: 4901.1828s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0745259\n",
      "\tspeed: 0.1157s/iter; left time: 4902.2137s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0968850\n",
      "\tspeed: 0.1147s/iter; left time: 4850.8245s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0774877\n",
      "\tspeed: 0.1128s/iter; left time: 4759.7731s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0719790\n",
      "\tspeed: 0.1129s/iter; left time: 4750.8537s\n",
      "\titers: 2700, epoch: 11 | loss: 0.0613416\n",
      "\tspeed: 0.1133s/iter; left time: 4755.9070s\n",
      "\titers: 2800, epoch: 11 | loss: 0.0705149\n",
      "\tspeed: 0.1153s/iter; left time: 4828.2350s\n",
      "\titers: 2900, epoch: 11 | loss: 0.0600997\n",
      "\tspeed: 0.1193s/iter; left time: 4982.8862s\n",
      "\titers: 3000, epoch: 11 | loss: 0.0838152\n",
      "\tspeed: 0.1120s/iter; left time: 4666.8976s\n",
      "\titers: 3100, epoch: 11 | loss: 0.0801776\n",
      "\tspeed: 0.1128s/iter; left time: 4691.7418s\n",
      "\titers: 3200, epoch: 11 | loss: 0.0818057\n",
      "\tspeed: 0.1120s/iter; left time: 4647.3603s\n",
      "\titers: 3300, epoch: 11 | loss: 0.0967920\n",
      "\tspeed: 0.1119s/iter; left time: 4631.3189s\n",
      "\titers: 3400, epoch: 11 | loss: 0.0851540\n",
      "\tspeed: 0.1118s/iter; left time: 4617.2067s\n",
      "\titers: 3500, epoch: 11 | loss: 0.0771672\n",
      "\tspeed: 0.1114s/iter; left time: 4589.0794s\n",
      "\titers: 3600, epoch: 11 | loss: 0.0722381\n",
      "\tspeed: 0.1124s/iter; left time: 4615.8247s\n",
      "\titers: 3700, epoch: 11 | loss: 0.0777316\n",
      "\tspeed: 0.1131s/iter; left time: 4634.9475s\n",
      "\titers: 3800, epoch: 11 | loss: 0.0811023\n",
      "\tspeed: 0.1122s/iter; left time: 4587.1586s\n",
      "\titers: 3900, epoch: 11 | loss: 0.0592589\n",
      "\tspeed: 0.1135s/iter; left time: 4629.6078s\n",
      "\titers: 4000, epoch: 11 | loss: 0.0921532\n",
      "\tspeed: 0.1125s/iter; left time: 4574.9062s\n",
      "\titers: 4100, epoch: 11 | loss: 0.0703947\n",
      "\tspeed: 0.1131s/iter; left time: 4591.6095s\n",
      "\titers: 4200, epoch: 11 | loss: 0.0643912\n",
      "\tspeed: 0.1144s/iter; left time: 4629.1487s\n",
      "\titers: 4300, epoch: 11 | loss: 0.0791592\n",
      "\tspeed: 0.1142s/iter; left time: 4612.0469s\n",
      "\titers: 4400, epoch: 11 | loss: 0.0683983\n",
      "\tspeed: 0.1123s/iter; left time: 4524.4935s\n",
      "Epoch: 11 cost time: 00h:08m:31.37s\n",
      "Epoch: 11 | Train Loss: 0.0766384 Vali Loss: 0.0910036 Test Loss: 0.0970744\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0710792\n",
      "\tspeed: 1.5636s/iter; left time: 62721.0444s\n",
      "\titers: 200, epoch: 12 | loss: 0.0842273\n",
      "\tspeed: 0.1133s/iter; left time: 4533.8282s\n",
      "\titers: 300, epoch: 12 | loss: 0.0629760\n",
      "\tspeed: 0.1142s/iter; left time: 4557.5035s\n",
      "\titers: 400, epoch: 12 | loss: 0.0810391\n",
      "\tspeed: 0.1137s/iter; left time: 4525.0781s\n",
      "\titers: 500, epoch: 12 | loss: 0.1013014\n",
      "\tspeed: 0.1144s/iter; left time: 4542.3369s\n",
      "\titers: 600, epoch: 12 | loss: 0.0584819\n",
      "\tspeed: 0.1137s/iter; left time: 4503.4232s\n",
      "\titers: 700, epoch: 12 | loss: 0.0732251\n",
      "\tspeed: 0.1132s/iter; left time: 4471.8834s\n",
      "\titers: 800, epoch: 12 | loss: 0.0754556\n",
      "\tspeed: 0.1120s/iter; left time: 4413.5894s\n",
      "\titers: 900, epoch: 12 | loss: 0.0640929\n",
      "\tspeed: 0.1135s/iter; left time: 4461.7356s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0760040\n",
      "\tspeed: 0.1133s/iter; left time: 4442.3115s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0927217\n",
      "\tspeed: 0.1133s/iter; left time: 4433.2293s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0876345\n",
      "\tspeed: 0.1150s/iter; left time: 4487.2739s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0746380\n",
      "\tspeed: 0.1136s/iter; left time: 4422.2023s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0757993\n",
      "\tspeed: 0.1150s/iter; left time: 4463.8057s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0810570\n",
      "\tspeed: 0.1153s/iter; left time: 4464.1727s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0619826\n",
      "\tspeed: 0.1155s/iter; left time: 4460.3334s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0746262\n",
      "\tspeed: 0.1122s/iter; left time: 4321.5653s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0823452\n",
      "\tspeed: 0.1131s/iter; left time: 4343.0094s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0711706\n",
      "\tspeed: 0.1119s/iter; left time: 4286.5702s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0654976\n",
      "\tspeed: 0.1114s/iter; left time: 4257.2980s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0801418\n",
      "\tspeed: 0.1130s/iter; left time: 4308.3075s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0796125\n",
      "\tspeed: 0.1143s/iter; left time: 4345.9646s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0880586\n",
      "\tspeed: 0.1140s/iter; left time: 4322.5583s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0664747\n",
      "\tspeed: 0.1128s/iter; left time: 4266.0571s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685178\n",
      "\tspeed: 0.1133s/iter; left time: 4272.3721s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0550742\n",
      "\tspeed: 0.1131s/iter; left time: 4252.6892s\n",
      "\titers: 2700, epoch: 12 | loss: 0.0827314\n",
      "\tspeed: 0.1116s/iter; left time: 4186.3833s\n",
      "\titers: 2800, epoch: 12 | loss: 0.0665130\n",
      "\tspeed: 0.1129s/iter; left time: 4225.7916s\n",
      "\titers: 2900, epoch: 12 | loss: 0.0725770\n",
      "\tspeed: 0.1153s/iter; left time: 4301.2409s\n",
      "\titers: 3000, epoch: 12 | loss: 0.0761487\n",
      "\tspeed: 0.1131s/iter; left time: 4207.9793s\n",
      "\titers: 3100, epoch: 12 | loss: 0.0791091\n",
      "\tspeed: 0.1113s/iter; left time: 4129.1158s\n",
      "\titers: 3200, epoch: 12 | loss: 0.0671570\n",
      "\tspeed: 0.1132s/iter; left time: 4188.6470s\n",
      "\titers: 3300, epoch: 12 | loss: 0.0749104\n",
      "\tspeed: 0.1108s/iter; left time: 4090.0929s\n",
      "\titers: 3400, epoch: 12 | loss: 0.0909807\n",
      "\tspeed: 0.1113s/iter; left time: 4097.7149s\n",
      "\titers: 3500, epoch: 12 | loss: 0.0923854\n",
      "\tspeed: 0.1126s/iter; left time: 4132.9393s\n",
      "\titers: 3600, epoch: 12 | loss: 0.0831650\n",
      "\tspeed: 0.1131s/iter; left time: 4139.3141s\n",
      "\titers: 3700, epoch: 12 | loss: 0.0813926\n",
      "\tspeed: 0.1130s/iter; left time: 4127.1017s\n",
      "\titers: 3800, epoch: 12 | loss: 0.0719712\n",
      "\tspeed: 0.1118s/iter; left time: 4070.7835s\n",
      "\titers: 3900, epoch: 12 | loss: 0.0697064\n",
      "\tspeed: 0.1118s/iter; left time: 4060.5863s\n",
      "\titers: 4000, epoch: 12 | loss: 0.0884542\n",
      "\tspeed: 0.1128s/iter; left time: 4084.2641s\n",
      "\titers: 4100, epoch: 12 | loss: 0.0673367\n",
      "\tspeed: 0.1129s/iter; left time: 4075.9061s\n",
      "\titers: 4200, epoch: 12 | loss: 0.0795303\n",
      "\tspeed: 0.1130s/iter; left time: 4069.6429s\n",
      "\titers: 4300, epoch: 12 | loss: 0.0724135\n",
      "\tspeed: 0.1119s/iter; left time: 4018.1664s\n",
      "\titers: 4400, epoch: 12 | loss: 0.0837259\n",
      "\tspeed: 0.1092s/iter; left time: 3912.5087s\n",
      "Epoch: 12 cost time: 00h:08m:26.23s\n",
      "Epoch: 12 | Train Loss: 0.0758073 Vali Loss: 0.0911360 Test Loss: 0.0977819\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0696547\n",
      "\tspeed: 1.5893s/iter; left time: 56649.7380s\n",
      "\titers: 200, epoch: 13 | loss: 0.0783792\n",
      "\tspeed: 0.1172s/iter; left time: 4164.4258s\n",
      "\titers: 300, epoch: 13 | loss: 0.0612777\n",
      "\tspeed: 0.1179s/iter; left time: 4178.9516s\n",
      "\titers: 400, epoch: 13 | loss: 0.0669738\n",
      "\tspeed: 0.1177s/iter; left time: 4158.8186s\n",
      "\titers: 500, epoch: 13 | loss: 0.0753981\n",
      "\tspeed: 0.1177s/iter; left time: 4147.4482s\n",
      "\titers: 600, epoch: 13 | loss: 0.0703675\n",
      "\tspeed: 0.1153s/iter; left time: 4053.7467s\n",
      "\titers: 700, epoch: 13 | loss: 0.0807081\n",
      "\tspeed: 0.1164s/iter; left time: 4077.7579s\n",
      "\titers: 800, epoch: 13 | loss: 0.0631655\n",
      "\tspeed: 0.1182s/iter; left time: 4130.4168s\n",
      "\titers: 900, epoch: 13 | loss: 0.0665464\n",
      "\tspeed: 0.1170s/iter; left time: 4075.9742s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0889466\n",
      "\tspeed: 0.1150s/iter; left time: 3996.6718s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0937114\n",
      "\tspeed: 0.1159s/iter; left time: 4013.9217s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0855564\n",
      "\tspeed: 0.1143s/iter; left time: 3949.4810s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0802188\n",
      "\tspeed: 0.1161s/iter; left time: 3999.4400s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0752418\n",
      "\tspeed: 0.1170s/iter; left time: 4019.6635s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0920139\n",
      "\tspeed: 0.1172s/iter; left time: 4013.8930s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0802863\n",
      "\tspeed: 0.1177s/iter; left time: 4019.6514s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0784608\n",
      "\tspeed: 0.1200s/iter; left time: 4086.0264s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0880799\n",
      "\tspeed: 0.1172s/iter; left time: 3979.4404s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0902761\n",
      "\tspeed: 0.1177s/iter; left time: 3984.6787s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0855317\n",
      "\tspeed: 0.1162s/iter; left time: 3920.8780s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0778534\n",
      "\tspeed: 0.1169s/iter; left time: 3934.1143s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0732491\n",
      "\tspeed: 0.1166s/iter; left time: 3910.2686s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0718696\n",
      "\tspeed: 0.1174s/iter; left time: 3927.0261s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643891\n",
      "\tspeed: 0.1167s/iter; left time: 3891.0356s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0634482\n",
      "\tspeed: 0.1176s/iter; left time: 3909.0249s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0696910\n",
      "\tspeed: 0.1181s/iter; left time: 3915.0711s\n",
      "\titers: 2700, epoch: 13 | loss: 0.0817821\n",
      "\tspeed: 0.1158s/iter; left time: 3825.6326s\n",
      "\titers: 2800, epoch: 13 | loss: 0.0640855\n",
      "\tspeed: 0.1157s/iter; left time: 3811.4909s\n",
      "\titers: 2900, epoch: 13 | loss: 0.0694307\n",
      "\tspeed: 0.1160s/iter; left time: 3810.3136s\n",
      "\titers: 3000, epoch: 13 | loss: 0.0864336\n",
      "\tspeed: 0.1171s/iter; left time: 3835.8686s\n",
      "\titers: 3100, epoch: 13 | loss: 0.0733568\n",
      "\tspeed: 0.1175s/iter; left time: 3835.4461s\n",
      "\titers: 3200, epoch: 13 | loss: 0.0625381\n",
      "\tspeed: 0.1159s/iter; left time: 3770.5600s\n",
      "\titers: 3300, epoch: 13 | loss: 0.0875661\n",
      "\tspeed: 0.1187s/iter; left time: 3852.2250s\n",
      "\titers: 3400, epoch: 13 | loss: 0.0675188\n",
      "\tspeed: 0.1180s/iter; left time: 3818.2630s\n",
      "\titers: 3500, epoch: 13 | loss: 0.0647626\n",
      "\tspeed: 0.1167s/iter; left time: 3763.5709s\n",
      "\titers: 3600, epoch: 13 | loss: 0.0939891\n",
      "\tspeed: 0.1169s/iter; left time: 3756.8278s\n",
      "\titers: 3700, epoch: 13 | loss: 0.0905394\n",
      "\tspeed: 0.1158s/iter; left time: 3709.2596s\n",
      "\titers: 3800, epoch: 13 | loss: 0.0685588\n",
      "\tspeed: 0.1173s/iter; left time: 3748.1331s\n",
      "\titers: 3900, epoch: 13 | loss: 0.0648391\n",
      "\tspeed: 0.1175s/iter; left time: 3740.8344s\n",
      "\titers: 4000, epoch: 13 | loss: 0.0833651\n",
      "\tspeed: 0.1160s/iter; left time: 3682.8446s\n",
      "\titers: 4100, epoch: 13 | loss: 0.0707878\n",
      "\tspeed: 0.1176s/iter; left time: 3722.4901s\n",
      "\titers: 4200, epoch: 13 | loss: 0.0837740\n",
      "\tspeed: 0.1175s/iter; left time: 3707.5046s\n",
      "\titers: 4300, epoch: 13 | loss: 0.0526903\n",
      "\tspeed: 0.1160s/iter; left time: 3649.0515s\n",
      "\titers: 4400, epoch: 13 | loss: 0.0681901\n",
      "\tspeed: 0.1168s/iter; left time: 3661.2333s\n",
      "Epoch: 13 cost time: 00h:08m:43.12s\n",
      "Epoch: 13 | Train Loss: 0.0749393 Vali Loss: 0.0928503 Test Loss: 0.1000453\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02273400127887726, rmse:0.15077798068523407, mae:0.09537788480520248, rse:0.5325193405151367\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 24: 02h:24m:39.95s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-03 03:36:47,323] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 03:36:48,374] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 03:36:48,374] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 03:36:48,374] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 03:36:48,464] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 03:36:48,464] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 03:36:49,208] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 03:36:49,210] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 03:36:49,210] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 03:36:49,211] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 03:36:49,211] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 03:36:49,212] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 03:36:49,212] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 03:36:49,660] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 03:36:49,661] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,661] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.86 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,845] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 03:36:49,846] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,846] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.8 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,846] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 03:36:49,986] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 03:36:49,987] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 03:36:49,987] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.83 GB, percent = 10.2%\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 03:36:49,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 03:36:49,989] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5e609dadd0>\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 03:36:49,990] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 03:36:49,991] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 03:36:49,992] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1857200\n",
      "\tspeed: 0.1749s/iter; left time: 15577.3743s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687170\n",
      "\tspeed: 0.1288s/iter; left time: 11457.6404s\n",
      "\titers: 300, epoch: 1 | loss: 0.1724844\n",
      "\tspeed: 0.1281s/iter; left time: 11376.5133s\n",
      "\titers: 400, epoch: 1 | loss: 0.1656757\n",
      "\tspeed: 0.1285s/iter; left time: 11406.8328s\n",
      "\titers: 500, epoch: 1 | loss: 0.1675965\n",
      "\tspeed: 0.1308s/iter; left time: 11595.5959s\n",
      "\titers: 600, epoch: 1 | loss: 0.1443687\n",
      "\tspeed: 0.1283s/iter; left time: 11360.4195s\n",
      "\titers: 700, epoch: 1 | loss: 0.1422241\n",
      "\tspeed: 0.1294s/iter; left time: 11444.9010s\n",
      "\titers: 800, epoch: 1 | loss: 0.1226619\n",
      "\tspeed: 0.1311s/iter; left time: 11583.9627s\n",
      "\titers: 900, epoch: 1 | loss: 0.1330729\n",
      "\tspeed: 0.1320s/iter; left time: 11645.3274s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1138731\n",
      "\tspeed: 0.1279s/iter; left time: 11277.5582s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1071478\n",
      "\tspeed: 0.1286s/iter; left time: 11321.9161s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1303404\n",
      "\tspeed: 0.1277s/iter; left time: 11226.3168s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1238685\n",
      "\tspeed: 0.1295s/iter; left time: 11371.7322s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1032463\n",
      "\tspeed: 0.1281s/iter; left time: 11238.4410s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1194469\n",
      "\tspeed: 0.1278s/iter; left time: 11204.7918s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1207578\n",
      "\tspeed: 0.1295s/iter; left time: 11336.0300s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1394709\n",
      "\tspeed: 0.1296s/iter; left time: 11329.0578s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1160002\n",
      "\tspeed: 0.1284s/iter; left time: 11216.9853s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1103131\n",
      "\tspeed: 0.1283s/iter; left time: 11196.2989s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1130958\n",
      "\tspeed: 0.1258s/iter; left time: 10963.6410s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1089309\n",
      "\tspeed: 0.1278s/iter; left time: 11119.7526s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1244033\n",
      "\tspeed: 0.1307s/iter; left time: 11360.0311s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1284142\n",
      "\tspeed: 0.1305s/iter; left time: 11330.0133s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1186614\n",
      "\tspeed: 0.1310s/iter; left time: 11359.6028s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1169964\n",
      "\tspeed: 0.1281s/iter; left time: 11094.6381s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1132365\n",
      "\tspeed: 0.1305s/iter; left time: 11294.7278s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1326390\n",
      "\tspeed: 0.1299s/iter; left time: 11227.3974s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1291778\n",
      "\tspeed: 0.1271s/iter; left time: 10971.6094s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1308049\n",
      "\tspeed: 0.1309s/iter; left time: 11287.2538s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1142576\n",
      "\tspeed: 0.1295s/iter; left time: 11153.1138s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0980470\n",
      "\tspeed: 0.1265s/iter; left time: 10886.4749s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1026124\n",
      "\tspeed: 0.1282s/iter; left time: 11013.8344s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1019026\n",
      "\tspeed: 0.1289s/iter; left time: 11065.1453s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1156720\n",
      "\tspeed: 0.1297s/iter; left time: 11123.8672s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1253485\n",
      "\tspeed: 0.1298s/iter; left time: 11116.5148s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1046039\n",
      "\tspeed: 0.1302s/iter; left time: 11137.7349s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1413063\n",
      "\tspeed: 0.1318s/iter; left time: 11263.1589s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1070143\n",
      "\tspeed: 0.1292s/iter; left time: 11023.9143s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1359391\n",
      "\tspeed: 0.1286s/iter; left time: 10961.5411s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1120188\n",
      "\tspeed: 0.1300s/iter; left time: 11066.3841s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1373022\n",
      "\tspeed: 0.1296s/iter; left time: 11017.7127s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1048007\n",
      "\tspeed: 0.1290s/iter; left time: 10959.8018s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1046956\n",
      "\tspeed: 0.1312s/iter; left time: 11129.0249s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1285643\n",
      "\tspeed: 0.1288s/iter; left time: 10912.1558s\n",
      "Epoch: 1 cost time: 00h:09m:37.13s\n",
      "Epoch: 1 | Train Loss: 0.1245267 Vali Loss: 0.1197564 Test Loss: 0.1277876\n",
      "Validation loss decreased (inf --> 0.119756).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1362824\n",
      "\tspeed: 1.8577s/iter; left time: 157131.2041s\n",
      "\titers: 200, epoch: 2 | loss: 0.1034659\n",
      "\tspeed: 0.1181s/iter; left time: 9979.1355s\n",
      "\titers: 300, epoch: 2 | loss: 0.1078680\n",
      "\tspeed: 0.1176s/iter; left time: 9924.3209s\n",
      "\titers: 400, epoch: 2 | loss: 0.1090891\n",
      "\tspeed: 0.1186s/iter; left time: 9997.2466s\n",
      "\titers: 500, epoch: 2 | loss: 0.0995298\n",
      "\tspeed: 0.1178s/iter; left time: 9912.9384s\n",
      "\titers: 600, epoch: 2 | loss: 0.1253273\n",
      "\tspeed: 0.1185s/iter; left time: 9967.8242s\n",
      "\titers: 700, epoch: 2 | loss: 0.1308996\n",
      "\tspeed: 0.1184s/iter; left time: 9945.0339s\n",
      "\titers: 800, epoch: 2 | loss: 0.1199580\n",
      "\tspeed: 0.1172s/iter; left time: 9827.3807s\n",
      "\titers: 900, epoch: 2 | loss: 0.1126412\n",
      "\tspeed: 0.1192s/iter; left time: 9990.0129s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1188750\n",
      "\tspeed: 0.1181s/iter; left time: 9884.5111s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1189725\n",
      "\tspeed: 0.1185s/iter; left time: 9905.0032s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1217855\n",
      "\tspeed: 0.1181s/iter; left time: 9861.4114s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1159035\n",
      "\tspeed: 0.1170s/iter; left time: 9752.8977s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0984060\n",
      "\tspeed: 0.1196s/iter; left time: 9960.2194s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0966178\n",
      "\tspeed: 0.1195s/iter; left time: 9938.6067s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1044325\n",
      "\tspeed: 0.1182s/iter; left time: 9816.7587s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1260282\n",
      "\tspeed: 0.1179s/iter; left time: 9784.6770s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1024257\n",
      "\tspeed: 0.1167s/iter; left time: 9671.8226s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0874435\n",
      "\tspeed: 0.1193s/iter; left time: 9873.2542s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1199540\n",
      "\tspeed: 0.1158s/iter; left time: 9576.7465s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1297425\n",
      "\tspeed: 0.1176s/iter; left time: 9711.1511s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1000140\n",
      "\tspeed: 0.1163s/iter; left time: 9589.7147s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1035635\n",
      "\tspeed: 0.1180s/iter; left time: 9723.6046s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1033574\n",
      "\tspeed: 0.1183s/iter; left time: 9735.6153s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1072888\n",
      "\tspeed: 0.1175s/iter; left time: 9658.7560s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1190963\n",
      "\tspeed: 0.1174s/iter; left time: 9636.4540s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1066877\n",
      "\tspeed: 0.1181s/iter; left time: 9686.3330s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1129730\n",
      "\tspeed: 0.1128s/iter; left time: 9237.4035s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1384397\n",
      "\tspeed: 0.1173s/iter; left time: 9596.9501s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1114533\n",
      "\tspeed: 0.1192s/iter; left time: 9732.7134s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1163946\n",
      "\tspeed: 0.1176s/iter; left time: 9597.6541s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1102893\n",
      "\tspeed: 0.1175s/iter; left time: 9575.9749s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0999768\n",
      "\tspeed: 0.1159s/iter; left time: 9432.5478s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0817148\n",
      "\tspeed: 0.1176s/iter; left time: 9559.8251s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1148444\n",
      "\tspeed: 0.1182s/iter; left time: 9593.3125s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1212692\n",
      "\tspeed: 0.1160s/iter; left time: 9403.9745s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1155532\n",
      "\tspeed: 0.1158s/iter; left time: 9379.7291s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1284737\n",
      "\tspeed: 0.1156s/iter; left time: 9351.2049s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1278360\n",
      "\tspeed: 0.1177s/iter; left time: 9505.1319s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092012\n",
      "\tspeed: 0.1165s/iter; left time: 9396.7610s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0933397\n",
      "\tspeed: 0.1175s/iter; left time: 9467.4941s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1042761\n",
      "\tspeed: 0.1169s/iter; left time: 9407.1105s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1036103\n",
      "\tspeed: 0.1155s/iter; left time: 9285.9710s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0886638\n",
      "\tspeed: 0.1162s/iter; left time: 9326.1864s\n",
      "Epoch: 2 cost time: 00h:08m:44.44s\n",
      "Epoch: 2 | Train Loss: 0.1098625 Vali Loss: 0.1185164 Test Loss: 0.1282422\n",
      "Validation loss decreased (0.119756 --> 0.118516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1121855\n",
      "\tspeed: 1.6093s/iter; left time: 128949.3745s\n",
      "\titers: 200, epoch: 3 | loss: 0.0993601\n",
      "\tspeed: 0.1182s/iter; left time: 9455.5526s\n",
      "\titers: 300, epoch: 3 | loss: 0.0954742\n",
      "\tspeed: 0.1185s/iter; left time: 9471.8841s\n",
      "\titers: 400, epoch: 3 | loss: 0.1022189\n",
      "\tspeed: 0.1167s/iter; left time: 9318.9562s\n",
      "\titers: 500, epoch: 3 | loss: 0.1028623\n",
      "\tspeed: 0.1179s/iter; left time: 9400.1779s\n",
      "\titers: 600, epoch: 3 | loss: 0.1073780\n",
      "\tspeed: 0.1188s/iter; left time: 9455.7653s\n",
      "\titers: 700, epoch: 3 | loss: 0.1252172\n",
      "\tspeed: 0.1184s/iter; left time: 9415.0870s\n",
      "\titers: 800, epoch: 3 | loss: 0.0985473\n",
      "\tspeed: 0.1175s/iter; left time: 9332.2909s\n",
      "\titers: 900, epoch: 3 | loss: 0.1068274\n",
      "\tspeed: 0.1197s/iter; left time: 9496.0076s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1010967\n",
      "\tspeed: 0.1183s/iter; left time: 9374.0746s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1209155\n",
      "\tspeed: 0.1169s/iter; left time: 9252.1070s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1471111\n",
      "\tspeed: 0.1176s/iter; left time: 9295.8203s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0935464\n",
      "\tspeed: 0.1189s/iter; left time: 9381.0565s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1135274\n",
      "\tspeed: 0.1186s/iter; left time: 9352.5755s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0995740\n",
      "\tspeed: 0.1168s/iter; left time: 9194.3072s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0949781\n",
      "\tspeed: 0.1172s/iter; left time: 9212.4557s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1161713\n",
      "\tspeed: 0.1180s/iter; left time: 9264.1771s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1313455\n",
      "\tspeed: 0.1162s/iter; left time: 9111.7358s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1055434\n",
      "\tspeed: 0.1190s/iter; left time: 9317.3203s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1189179\n",
      "\tspeed: 0.1176s/iter; left time: 9198.3426s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0911239\n",
      "\tspeed: 0.1181s/iter; left time: 9227.0430s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1390544\n",
      "\tspeed: 0.1160s/iter; left time: 9050.4505s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976654\n",
      "\tspeed: 0.1182s/iter; left time: 9211.7403s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1079981\n",
      "\tspeed: 0.1198s/iter; left time: 9321.4916s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1111981\n",
      "\tspeed: 0.1177s/iter; left time: 9145.8877s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1110975\n",
      "\tspeed: 0.1151s/iter; left time: 8936.2508s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1392076\n",
      "\tspeed: 0.1162s/iter; left time: 9005.2980s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1271442\n",
      "\tspeed: 0.1160s/iter; left time: 8980.1116s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1141754\n",
      "\tspeed: 0.1152s/iter; left time: 8905.0857s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1144832\n",
      "\tspeed: 0.1152s/iter; left time: 8899.4490s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0868490\n",
      "\tspeed: 0.1183s/iter; left time: 9124.7607s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1013458\n",
      "\tspeed: 0.1181s/iter; left time: 9098.0172s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0867222\n",
      "\tspeed: 0.1157s/iter; left time: 8899.0869s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0821788\n",
      "\tspeed: 0.1162s/iter; left time: 8927.4794s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1050519\n",
      "\tspeed: 0.1156s/iter; left time: 8873.3044s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1225686\n",
      "\tspeed: 0.1185s/iter; left time: 9080.4483s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1083653\n",
      "\tspeed: 0.1174s/iter; left time: 8983.1473s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0852168\n",
      "\tspeed: 0.1169s/iter; left time: 8936.1334s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1460541\n",
      "\tspeed: 0.1177s/iter; left time: 8986.7995s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0892489\n",
      "\tspeed: 0.1159s/iter; left time: 8836.6297s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1043740\n",
      "\tspeed: 0.1151s/iter; left time: 8763.5466s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1233985\n",
      "\tspeed: 0.1171s/iter; left time: 8903.0900s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1201573\n",
      "\tspeed: 0.1143s/iter; left time: 8682.1961s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0884113\n",
      "\tspeed: 0.1163s/iter; left time: 8815.0683s\n",
      "Epoch: 3 cost time: 00h:08m:43.20s\n",
      "Epoch: 3 | Train Loss: 0.1059200 Vali Loss: 0.1199043 Test Loss: 0.1320190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1017862\n",
      "\tspeed: 1.5936s/iter; left time: 120585.6451s\n",
      "\titers: 200, epoch: 4 | loss: 0.1054816\n",
      "\tspeed: 0.1159s/iter; left time: 8755.5299s\n",
      "\titers: 300, epoch: 4 | loss: 0.1008089\n",
      "\tspeed: 0.1170s/iter; left time: 8831.5805s\n",
      "\titers: 400, epoch: 4 | loss: 0.1103006\n",
      "\tspeed: 0.1153s/iter; left time: 8688.9430s\n",
      "\titers: 500, epoch: 4 | loss: 0.1008416\n",
      "\tspeed: 0.1145s/iter; left time: 8619.2124s\n",
      "\titers: 600, epoch: 4 | loss: 0.0872107\n",
      "\tspeed: 0.1160s/iter; left time: 8722.1032s\n",
      "\titers: 700, epoch: 4 | loss: 0.1059361\n",
      "\tspeed: 0.1195s/iter; left time: 8968.0293s\n",
      "\titers: 800, epoch: 4 | loss: 0.1094390\n",
      "\tspeed: 0.1181s/iter; left time: 8851.8553s\n",
      "\titers: 900, epoch: 4 | loss: 0.1180137\n",
      "\tspeed: 0.1181s/iter; left time: 8844.5917s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1055246\n",
      "\tspeed: 0.1197s/iter; left time: 8947.2815s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1000077\n",
      "\tspeed: 0.1173s/iter; left time: 8759.0721s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0994589\n",
      "\tspeed: 0.1172s/iter; left time: 8735.9921s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1067580\n",
      "\tspeed: 0.1171s/iter; left time: 8721.4622s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0998731\n",
      "\tspeed: 0.1158s/iter; left time: 8613.1813s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1033912\n",
      "\tspeed: 0.1119s/iter; left time: 8308.1780s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0937374\n",
      "\tspeed: 0.1107s/iter; left time: 8208.9945s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0970818\n",
      "\tspeed: 0.1124s/iter; left time: 8327.5368s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1050920\n",
      "\tspeed: 0.1120s/iter; left time: 8285.6897s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1291669\n",
      "\tspeed: 0.1121s/iter; left time: 8279.0598s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0771152\n",
      "\tspeed: 0.1133s/iter; left time: 8356.6795s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0940207\n",
      "\tspeed: 0.1136s/iter; left time: 8366.1464s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0884763\n",
      "\tspeed: 0.1108s/iter; left time: 8149.7752s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0902145\n",
      "\tspeed: 0.1114s/iter; left time: 8187.5958s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1145035\n",
      "\tspeed: 0.1129s/iter; left time: 8281.5547s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1050488\n",
      "\tspeed: 0.1144s/iter; left time: 8378.4388s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0954266\n",
      "\tspeed: 0.1140s/iter; left time: 8343.4925s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0995934\n",
      "\tspeed: 0.1122s/iter; left time: 8201.3794s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1045068\n",
      "\tspeed: 0.1108s/iter; left time: 8082.1594s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0896091\n",
      "\tspeed: 0.1110s/iter; left time: 8087.5701s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1133813\n",
      "\tspeed: 0.1100s/iter; left time: 8006.8918s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1004615\n",
      "\tspeed: 0.1144s/iter; left time: 8311.1747s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1033023\n",
      "\tspeed: 0.1111s/iter; left time: 8059.7486s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1076910\n",
      "\tspeed: 0.1126s/iter; left time: 8158.9434s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0782520\n",
      "\tspeed: 0.1114s/iter; left time: 8063.5273s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0946225\n",
      "\tspeed: 0.1116s/iter; left time: 8068.2659s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1047806\n",
      "\tspeed: 0.1135s/iter; left time: 8194.1758s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0901383\n",
      "\tspeed: 0.1132s/iter; left time: 8157.3959s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0860445\n",
      "\tspeed: 0.1126s/iter; left time: 8101.8086s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0885248\n",
      "\tspeed: 0.1116s/iter; left time: 8023.3105s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1189710\n",
      "\tspeed: 0.1109s/iter; left time: 7958.3027s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0907215\n",
      "\tspeed: 0.1112s/iter; left time: 7967.6398s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1236514\n",
      "\tspeed: 0.1111s/iter; left time: 7950.3721s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1064944\n",
      "\tspeed: 0.1120s/iter; left time: 8004.7838s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0968458\n",
      "\tspeed: 0.1119s/iter; left time: 7984.6857s\n",
      "Epoch: 4 cost time: 00h:08m:27.07s\n",
      "Epoch: 4 | Train Loss: 0.1021763 Vali Loss: 0.1203913 Test Loss: 0.1309304\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0960487\n",
      "\tspeed: 1.5315s/iter; left time: 109064.0538s\n",
      "\titers: 200, epoch: 5 | loss: 0.0997009\n",
      "\tspeed: 0.1119s/iter; left time: 7958.2026s\n",
      "\titers: 300, epoch: 5 | loss: 0.1327493\n",
      "\tspeed: 0.1136s/iter; left time: 8067.9951s\n",
      "\titers: 400, epoch: 5 | loss: 0.1110552\n",
      "\tspeed: 0.1112s/iter; left time: 7884.0786s\n",
      "\titers: 500, epoch: 5 | loss: 0.0997856\n",
      "\tspeed: 0.1119s/iter; left time: 7927.2165s\n",
      "\titers: 600, epoch: 5 | loss: 0.1085910\n",
      "\tspeed: 0.1141s/iter; left time: 8068.7005s\n",
      "\titers: 700, epoch: 5 | loss: 0.0906752\n",
      "\tspeed: 0.1124s/iter; left time: 7937.0686s\n",
      "\titers: 800, epoch: 5 | loss: 0.0881957\n",
      "\tspeed: 0.1132s/iter; left time: 7983.2926s\n",
      "\titers: 900, epoch: 5 | loss: 0.1219619\n",
      "\tspeed: 0.1158s/iter; left time: 8154.0782s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1071277\n",
      "\tspeed: 0.1137s/iter; left time: 7997.3908s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1084025\n",
      "\tspeed: 0.1156s/iter; left time: 8117.4470s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0964981\n",
      "\tspeed: 0.1152s/iter; left time: 8076.2562s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0842596\n",
      "\tspeed: 0.1136s/iter; left time: 7952.2301s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0926249\n",
      "\tspeed: 0.1134s/iter; left time: 7929.3991s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0907346\n",
      "\tspeed: 0.1130s/iter; left time: 7888.9182s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1106110\n",
      "\tspeed: 0.1145s/iter; left time: 7981.8965s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0988417\n",
      "\tspeed: 0.1146s/iter; left time: 7978.9288s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0920804\n",
      "\tspeed: 0.1143s/iter; left time: 7944.8159s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0919630\n",
      "\tspeed: 0.1155s/iter; left time: 8015.5406s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1004907\n",
      "\tspeed: 0.1155s/iter; left time: 8005.2387s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1181042\n",
      "\tspeed: 0.1144s/iter; left time: 7914.6309s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0876759\n",
      "\tspeed: 0.1135s/iter; left time: 7840.9967s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1073024\n",
      "\tspeed: 0.1145s/iter; left time: 7900.6209s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0821408\n",
      "\tspeed: 0.1135s/iter; left time: 7818.9375s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0953626\n",
      "\tspeed: 0.1153s/iter; left time: 7932.7214s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1104295\n",
      "\tspeed: 0.1141s/iter; left time: 7843.2803s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1122409\n",
      "\tspeed: 0.1150s/iter; left time: 7888.2967s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0743636\n",
      "\tspeed: 0.1139s/iter; left time: 7800.6765s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0947080\n",
      "\tspeed: 0.1125s/iter; left time: 7699.2210s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1082617\n",
      "\tspeed: 0.1138s/iter; left time: 7774.5218s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0972924\n",
      "\tspeed: 0.1158s/iter; left time: 7897.4971s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1161426\n",
      "\tspeed: 0.1147s/iter; left time: 7810.0341s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0934898\n",
      "\tspeed: 0.1141s/iter; left time: 7761.1548s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0890894\n",
      "\tspeed: 0.1152s/iter; left time: 7820.6728s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0774576\n",
      "\tspeed: 0.1154s/iter; left time: 7827.0229s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1041150\n",
      "\tspeed: 0.1168s/iter; left time: 7905.8663s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0764355\n",
      "\tspeed: 0.1134s/iter; left time: 7667.8056s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0967276\n",
      "\tspeed: 0.1140s/iter; left time: 7694.3834s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1057422\n",
      "\tspeed: 0.1142s/iter; left time: 7697.6299s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1008978\n",
      "\tspeed: 0.1144s/iter; left time: 7701.1571s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1179423\n",
      "\tspeed: 0.1149s/iter; left time: 7720.0400s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1178501\n",
      "\tspeed: 0.1099s/iter; left time: 7376.8081s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0911569\n",
      "\tspeed: 0.1127s/iter; left time: 7551.5480s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0975429\n",
      "\tspeed: 0.1136s/iter; left time: 7599.4730s\n",
      "Epoch: 5 cost time: 00h:08m:29.17s\n",
      "Epoch: 5 | Train Loss: 0.0987026 Vali Loss: 0.1216048 Test Loss: 0.1345417\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1011339\n",
      "\tspeed: 1.5360s/iter; left time: 102536.0272s\n",
      "\titers: 200, epoch: 6 | loss: 0.0883259\n",
      "\tspeed: 0.1171s/iter; left time: 7804.2875s\n",
      "\titers: 300, epoch: 6 | loss: 0.1075289\n",
      "\tspeed: 0.1141s/iter; left time: 7594.8624s\n",
      "\titers: 400, epoch: 6 | loss: 0.0989359\n",
      "\tspeed: 0.1163s/iter; left time: 7728.1429s\n",
      "\titers: 500, epoch: 6 | loss: 0.0862649\n",
      "\tspeed: 0.1143s/iter; left time: 7586.5485s\n",
      "\titers: 600, epoch: 6 | loss: 0.1002253\n",
      "\tspeed: 0.1150s/iter; left time: 7621.8291s\n",
      "\titers: 700, epoch: 6 | loss: 0.1083656\n",
      "\tspeed: 0.1156s/iter; left time: 7648.8588s\n",
      "\titers: 800, epoch: 6 | loss: 0.0928279\n",
      "\tspeed: 0.1160s/iter; left time: 7662.9690s\n",
      "\titers: 900, epoch: 6 | loss: 0.0807987\n",
      "\tspeed: 0.1151s/iter; left time: 7593.2415s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0948872\n",
      "\tspeed: 0.1148s/iter; left time: 7563.3529s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0827360\n",
      "\tspeed: 0.1164s/iter; left time: 7655.3289s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1174165\n",
      "\tspeed: 0.1145s/iter; left time: 7518.5707s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0992087\n",
      "\tspeed: 0.1147s/iter; left time: 7520.0064s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0848289\n",
      "\tspeed: 0.1166s/iter; left time: 7629.1240s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0922957\n",
      "\tspeed: 0.1152s/iter; left time: 7528.3518s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1061229\n",
      "\tspeed: 0.1166s/iter; left time: 7606.1201s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0937682\n",
      "\tspeed: 0.1160s/iter; left time: 7561.1492s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1026462\n",
      "\tspeed: 0.1133s/iter; left time: 7371.2713s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0818370\n",
      "\tspeed: 0.1157s/iter; left time: 7513.2731s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1049387\n",
      "\tspeed: 0.1159s/iter; left time: 7519.7116s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0856962\n",
      "\tspeed: 0.1147s/iter; left time: 7428.7141s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1132535\n",
      "\tspeed: 0.1156s/iter; left time: 7473.8852s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0780297\n",
      "\tspeed: 0.1136s/iter; left time: 7334.1105s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0942708\n",
      "\tspeed: 0.1139s/iter; left time: 7340.2087s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0917941\n",
      "\tspeed: 0.1145s/iter; left time: 7366.6319s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0872721\n",
      "\tspeed: 0.1154s/iter; left time: 7417.8992s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0825186\n",
      "\tspeed: 0.1150s/iter; left time: 7377.3480s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0905587\n",
      "\tspeed: 0.1162s/iter; left time: 7444.0894s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0887696\n",
      "\tspeed: 0.1138s/iter; left time: 7277.6593s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1022261\n",
      "\tspeed: 0.1136s/iter; left time: 7256.6604s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1016905\n",
      "\tspeed: 0.1158s/iter; left time: 7382.1130s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0889834\n",
      "\tspeed: 0.1147s/iter; left time: 7304.1106s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0868730\n",
      "\tspeed: 0.1146s/iter; left time: 7285.4443s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0905508\n",
      "\tspeed: 0.1136s/iter; left time: 7207.7473s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0885148\n",
      "\tspeed: 0.1152s/iter; left time: 7298.9761s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0922447\n",
      "\tspeed: 0.1151s/iter; left time: 7280.8357s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0886584\n",
      "\tspeed: 0.1145s/iter; left time: 7228.4714s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0922629\n",
      "\tspeed: 0.1146s/iter; left time: 7225.4081s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0939470\n",
      "\tspeed: 0.1130s/iter; left time: 7113.7216s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0905015\n",
      "\tspeed: 0.1145s/iter; left time: 7194.3585s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0758674\n",
      "\tspeed: 0.1167s/iter; left time: 7321.7619s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0957208\n",
      "\tspeed: 0.1136s/iter; left time: 7117.3013s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1053216\n",
      "\tspeed: 0.1134s/iter; left time: 7095.8461s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0906008\n",
      "\tspeed: 0.1147s/iter; left time: 7163.6950s\n",
      "Epoch: 6 cost time: 00h:08m:32.77s\n",
      "Epoch: 6 | Train Loss: 0.0954798 Vali Loss: 0.1227273 Test Loss: 0.1344287\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1033562\n",
      "\tspeed: 1.5449s/iter; left time: 96242.8774s\n",
      "\titers: 200, epoch: 7 | loss: 0.0896851\n",
      "\tspeed: 0.1128s/iter; left time: 7014.1229s\n",
      "\titers: 300, epoch: 7 | loss: 0.0913225\n",
      "\tspeed: 0.1154s/iter; left time: 7166.6403s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853823\n",
      "\tspeed: 0.1144s/iter; left time: 7095.4587s\n",
      "\titers: 500, epoch: 7 | loss: 0.0857644\n",
      "\tspeed: 0.1135s/iter; left time: 7026.9296s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806916\n",
      "\tspeed: 0.1141s/iter; left time: 7053.9584s\n",
      "\titers: 700, epoch: 7 | loss: 0.1021493\n",
      "\tspeed: 0.1169s/iter; left time: 7212.0236s\n",
      "\titers: 800, epoch: 7 | loss: 0.0732420\n",
      "\tspeed: 0.1147s/iter; left time: 7066.7822s\n",
      "\titers: 900, epoch: 7 | loss: 0.0850529\n",
      "\tspeed: 0.1133s/iter; left time: 6969.6772s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0993610\n",
      "\tspeed: 0.1148s/iter; left time: 7050.6510s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1097831\n",
      "\tspeed: 0.1150s/iter; left time: 7049.1661s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1054772\n",
      "\tspeed: 0.1153s/iter; left time: 7054.5038s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0870716\n",
      "\tspeed: 0.1149s/iter; left time: 7022.7027s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0907186\n",
      "\tspeed: 0.1148s/iter; left time: 7003.8567s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0836305\n",
      "\tspeed: 0.1153s/iter; left time: 7019.3724s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0796124\n",
      "\tspeed: 0.1154s/iter; left time: 7013.8048s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0783959\n",
      "\tspeed: 0.1152s/iter; left time: 6994.2668s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1006658\n",
      "\tspeed: 0.1179s/iter; left time: 7146.0686s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0884392\n",
      "\tspeed: 0.1131s/iter; left time: 6844.4431s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0906714\n",
      "\tspeed: 0.1152s/iter; left time: 6955.5823s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0930545\n",
      "\tspeed: 0.1124s/iter; left time: 6774.6318s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0999339\n",
      "\tspeed: 0.1150s/iter; left time: 6923.0216s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0763448\n",
      "\tspeed: 0.1141s/iter; left time: 6858.2620s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0873501\n",
      "\tspeed: 0.1148s/iter; left time: 6890.8708s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0835626\n",
      "\tspeed: 0.1134s/iter; left time: 6789.8264s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0899396\n",
      "\tspeed: 0.1144s/iter; left time: 6843.9955s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0973071\n",
      "\tspeed: 0.1155s/iter; left time: 6893.7186s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0889941\n",
      "\tspeed: 0.1119s/iter; left time: 6669.7559s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0885513\n",
      "\tspeed: 0.1140s/iter; left time: 6784.7081s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0899405\n",
      "\tspeed: 0.1142s/iter; left time: 6782.1823s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0913069\n",
      "\tspeed: 0.1141s/iter; left time: 6766.5576s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0929181\n",
      "\tspeed: 0.1131s/iter; left time: 6693.3679s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0748527\n",
      "\tspeed: 0.1134s/iter; left time: 6702.6012s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0883430\n",
      "\tspeed: 0.1127s/iter; left time: 6649.7692s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0995779\n",
      "\tspeed: 0.1146s/iter; left time: 6752.0857s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0879578\n",
      "\tspeed: 0.1146s/iter; left time: 6737.0533s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0936343\n",
      "\tspeed: 0.1164s/iter; left time: 6831.2676s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0912004\n",
      "\tspeed: 0.1137s/iter; left time: 6662.9285s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0808091\n",
      "\tspeed: 0.1143s/iter; left time: 6686.6893s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1068037\n",
      "\tspeed: 0.1160s/iter; left time: 6773.0787s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0853559\n",
      "\tspeed: 0.1127s/iter; left time: 6573.1499s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1038158\n",
      "\tspeed: 0.1145s/iter; left time: 6666.0709s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1103765\n",
      "\tspeed: 0.1153s/iter; left time: 6698.0690s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0968011\n",
      "\tspeed: 0.1133s/iter; left time: 6573.2121s\n",
      "Epoch: 7 cost time: 00h:08m:30.89s\n",
      "Epoch: 7 | Train Loss: 0.0925967 Vali Loss: 0.1245577 Test Loss: 0.1345884\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03578183427453041, rmse:0.18916086852550507, mae:0.12824216485023499, rse:0.6698496341705322\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 96: 01h:19m:09.94s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-03 04:55:56,825] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 04:55:57,946] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 04:55:57,946] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 04:55:57,946] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 04:55:58,038] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 04:55:58,038] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 04:55:58,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 04:55:58,774] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 04:55:58,774] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 04:55:58,775] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 04:55:58,775] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 04:55:58,775] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 04:55:58,775] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 04:55:59,162] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 04:55:59,163] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,194] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 88.96 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,359] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 04:55:59,360] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 88.97 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,360] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 04:55:59,504] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 04:55:59,505] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 04:55:59,505] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 89.03 GB, percent = 11.8%\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 04:55:59,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 04:55:59,507] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01f9fd6f50>\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 04:55:59,508] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 04:55:59,509] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 04:55:59,510] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1492427\n",
      "\tspeed: 0.1689s/iter; left time: 14998.9946s\n",
      "\titers: 200, epoch: 1 | loss: 0.1605359\n",
      "\tspeed: 0.1261s/iter; left time: 11190.3225s\n",
      "\titers: 300, epoch: 1 | loss: 0.1671229\n",
      "\tspeed: 0.1253s/iter; left time: 11106.0189s\n",
      "\titers: 400, epoch: 1 | loss: 0.1654047\n",
      "\tspeed: 0.1255s/iter; left time: 11113.6227s\n",
      "\titers: 500, epoch: 1 | loss: 0.1528231\n",
      "\tspeed: 0.1259s/iter; left time: 11130.4096s\n",
      "\titers: 600, epoch: 1 | loss: 0.1641308\n",
      "\tspeed: 0.1259s/iter; left time: 11120.4534s\n",
      "\titers: 700, epoch: 1 | loss: 0.1460070\n",
      "\tspeed: 0.1265s/iter; left time: 11163.3958s\n",
      "\titers: 800, epoch: 1 | loss: 0.1177307\n",
      "\tspeed: 0.1276s/iter; left time: 11242.2914s\n",
      "\titers: 900, epoch: 1 | loss: 0.1349947\n",
      "\tspeed: 0.1269s/iter; left time: 11173.5874s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1090521\n",
      "\tspeed: 0.1261s/iter; left time: 11086.5921s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1334336\n",
      "\tspeed: 0.1265s/iter; left time: 11109.3633s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1300411\n",
      "\tspeed: 0.1276s/iter; left time: 11197.1827s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1205211\n",
      "\tspeed: 0.1266s/iter; left time: 11093.1295s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1304281\n",
      "\tspeed: 0.1244s/iter; left time: 10886.0260s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1271410\n",
      "\tspeed: 0.1264s/iter; left time: 11049.2675s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1058479\n",
      "\tspeed: 0.1275s/iter; left time: 11129.7255s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1209870\n",
      "\tspeed: 0.1265s/iter; left time: 11029.6078s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1023959\n",
      "\tspeed: 0.1272s/iter; left time: 11078.7808s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1413429\n",
      "\tspeed: 0.1274s/iter; left time: 11089.8278s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1427599\n",
      "\tspeed: 0.1268s/iter; left time: 11019.0196s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1329999\n",
      "\tspeed: 0.1256s/iter; left time: 10905.1808s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1120558\n",
      "\tspeed: 0.1267s/iter; left time: 10986.1723s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0989436\n",
      "\tspeed: 0.1277s/iter; left time: 11060.9135s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1160462\n",
      "\tspeed: 0.1281s/iter; left time: 11079.1538s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1234167\n",
      "\tspeed: 0.1267s/iter; left time: 10953.3932s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1299602\n",
      "\tspeed: 0.1275s/iter; left time: 11009.6709s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1306166\n",
      "\tspeed: 0.1254s/iter; left time: 10812.0762s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1211876\n",
      "\tspeed: 0.1246s/iter; left time: 10732.5673s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1157711\n",
      "\tspeed: 0.1257s/iter; left time: 10811.2061s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0999164\n",
      "\tspeed: 0.1260s/iter; left time: 10828.2032s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1516199\n",
      "\tspeed: 0.1257s/iter; left time: 10790.8558s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1097987\n",
      "\tspeed: 0.1227s/iter; left time: 10516.8003s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1195762\n",
      "\tspeed: 0.1243s/iter; left time: 10638.8691s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1289778\n",
      "\tspeed: 0.1240s/iter; left time: 10602.2915s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1021916\n",
      "\tspeed: 0.1258s/iter; left time: 10744.1492s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1183287\n",
      "\tspeed: 0.1243s/iter; left time: 10606.4077s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1355938\n",
      "\tspeed: 0.1235s/iter; left time: 10522.3467s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1252127\n",
      "\tspeed: 0.1223s/iter; left time: 10407.7603s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1068365\n",
      "\tspeed: 0.1246s/iter; left time: 10597.5062s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1084060\n",
      "\tspeed: 0.1245s/iter; left time: 10571.2230s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1262050\n",
      "\tspeed: 0.1235s/iter; left time: 10477.3395s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1099667\n",
      "\tspeed: 0.1241s/iter; left time: 10516.0039s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1208501\n",
      "\tspeed: 0.1237s/iter; left time: 10468.0181s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1325888\n",
      "\tspeed: 0.1255s/iter; left time: 10609.7889s\n",
      "Epoch: 1 cost time: 00h:09m:20.32s\n",
      "Epoch: 1 | Train Loss: 0.1270849 Vali Loss: 0.1233267 Test Loss: 0.1336193\n",
      "Validation loss decreased (inf --> 0.123327).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1183373\n",
      "\tspeed: 1.8223s/iter; left time: 153754.1186s\n",
      "\titers: 200, epoch: 2 | loss: 0.1098894\n",
      "\tspeed: 0.1182s/iter; left time: 9964.4478s\n",
      "\titers: 300, epoch: 2 | loss: 0.1131674\n",
      "\tspeed: 0.1166s/iter; left time: 9818.2258s\n",
      "\titers: 400, epoch: 2 | loss: 0.1160400\n",
      "\tspeed: 0.1172s/iter; left time: 9850.7146s\n",
      "\titers: 500, epoch: 2 | loss: 0.0886714\n",
      "\tspeed: 0.1199s/iter; left time: 10065.0167s\n",
      "\titers: 600, epoch: 2 | loss: 0.1193221\n",
      "\tspeed: 0.1171s/iter; left time: 9825.4766s\n",
      "\titers: 700, epoch: 2 | loss: 0.1303138\n",
      "\tspeed: 0.1174s/iter; left time: 9832.2253s\n",
      "\titers: 800, epoch: 2 | loss: 0.1092902\n",
      "\tspeed: 0.1191s/iter; left time: 9966.0618s\n",
      "\titers: 900, epoch: 2 | loss: 0.0996466\n",
      "\tspeed: 0.1227s/iter; left time: 10250.5498s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1157148\n",
      "\tspeed: 0.1179s/iter; left time: 9845.2584s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1112141\n",
      "\tspeed: 0.1194s/iter; left time: 9953.4943s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1180607\n",
      "\tspeed: 0.1182s/iter; left time: 9846.0720s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0878751\n",
      "\tspeed: 0.1193s/iter; left time: 9920.3713s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1204318\n",
      "\tspeed: 0.1198s/iter; left time: 9951.3580s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1055659\n",
      "\tspeed: 0.1207s/iter; left time: 10015.5060s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1180415\n",
      "\tspeed: 0.1220s/iter; left time: 10108.0111s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1256569\n",
      "\tspeed: 0.1165s/iter; left time: 9646.1022s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1117147\n",
      "\tspeed: 0.1159s/iter; left time: 9583.7666s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1046314\n",
      "\tspeed: 0.1190s/iter; left time: 9824.2144s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1071824\n",
      "\tspeed: 0.1218s/iter; left time: 10045.4236s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1128184\n",
      "\tspeed: 0.1198s/iter; left time: 9872.1925s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1227209\n",
      "\tspeed: 0.1191s/iter; left time: 9795.1126s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1197821\n",
      "\tspeed: 0.1197s/iter; left time: 9834.7361s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1249823\n",
      "\tspeed: 0.1185s/iter; left time: 9729.3528s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1261758\n",
      "\tspeed: 0.1177s/iter; left time: 9645.1390s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1112454\n",
      "\tspeed: 0.1169s/iter; left time: 9568.3370s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1138095\n",
      "\tspeed: 0.1158s/iter; left time: 9466.3586s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0994604\n",
      "\tspeed: 0.1175s/iter; left time: 9597.1147s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1096554\n",
      "\tspeed: 0.1142s/iter; left time: 9316.7782s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1068463\n",
      "\tspeed: 0.1170s/iter; left time: 9529.2329s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1206979\n",
      "\tspeed: 0.1152s/iter; left time: 9372.1133s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1085434\n",
      "\tspeed: 0.1208s/iter; left time: 9814.7812s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1045549\n",
      "\tspeed: 0.1196s/iter; left time: 9709.0210s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1207838\n",
      "\tspeed: 0.1189s/iter; left time: 9640.1564s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1154747\n",
      "\tspeed: 0.1164s/iter; left time: 9424.9487s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1074588\n",
      "\tspeed: 0.1167s/iter; left time: 9435.2399s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1014723\n",
      "\tspeed: 0.1171s/iter; left time: 9457.4684s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0964471\n",
      "\tspeed: 0.1169s/iter; left time: 9432.4725s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1228269\n",
      "\tspeed: 0.1173s/iter; left time: 9447.5816s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1099236\n",
      "\tspeed: 0.1169s/iter; left time: 9405.8898s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171050\n",
      "\tspeed: 0.1216s/iter; left time: 9775.9550s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1104717\n",
      "\tspeed: 0.1183s/iter; left time: 9499.1964s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1126500\n",
      "\tspeed: 0.1213s/iter; left time: 9726.5423s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1323302\n",
      "\tspeed: 0.1198s/iter; left time: 9595.5465s\n",
      "Epoch: 2 cost time: 00h:08m:47.56s\n",
      "Epoch: 2 | Train Loss: 0.1139773 Vali Loss: 0.1234052 Test Loss: 0.1342462\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1082153\n",
      "\tspeed: 1.5796s/iter; left time: 126255.4090s\n",
      "\titers: 200, epoch: 3 | loss: 0.1202841\n",
      "\tspeed: 0.1189s/iter; left time: 9493.6879s\n",
      "\titers: 300, epoch: 3 | loss: 0.0996917\n",
      "\tspeed: 0.1184s/iter; left time: 9435.9582s\n",
      "\titers: 400, epoch: 3 | loss: 0.1064174\n",
      "\tspeed: 0.1157s/iter; left time: 9214.8919s\n",
      "\titers: 500, epoch: 3 | loss: 0.1139071\n",
      "\tspeed: 0.1180s/iter; left time: 9386.0109s\n",
      "\titers: 600, epoch: 3 | loss: 0.1215968\n",
      "\tspeed: 0.1142s/iter; left time: 9071.2138s\n",
      "\titers: 700, epoch: 3 | loss: 0.1096018\n",
      "\tspeed: 0.1197s/iter; left time: 9495.2993s\n",
      "\titers: 800, epoch: 3 | loss: 0.1310975\n",
      "\tspeed: 0.1203s/iter; left time: 9530.2566s\n",
      "\titers: 900, epoch: 3 | loss: 0.1056969\n",
      "\tspeed: 0.1183s/iter; left time: 9362.9065s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1081555\n",
      "\tspeed: 0.1158s/iter; left time: 9150.0619s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0982659\n",
      "\tspeed: 0.1183s/iter; left time: 9336.1627s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1056586\n",
      "\tspeed: 0.1197s/iter; left time: 9436.5511s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1162706\n",
      "\tspeed: 0.1196s/iter; left time: 9419.4681s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1264145\n",
      "\tspeed: 0.1210s/iter; left time: 9512.3302s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1110645\n",
      "\tspeed: 0.1198s/iter; left time: 9411.0207s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1157964\n",
      "\tspeed: 0.1202s/iter; left time: 9428.6409s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1245955\n",
      "\tspeed: 0.1179s/iter; left time: 9236.0999s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1219978\n",
      "\tspeed: 0.1211s/iter; left time: 9474.4907s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1215639\n",
      "\tspeed: 0.1194s/iter; left time: 9329.4055s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1222039\n",
      "\tspeed: 0.1194s/iter; left time: 9318.2211s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0978573\n",
      "\tspeed: 0.1180s/iter; left time: 9193.5410s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1149430\n",
      "\tspeed: 0.1199s/iter; left time: 9330.3611s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1145440\n",
      "\tspeed: 0.1201s/iter; left time: 9334.5904s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0993142\n",
      "\tspeed: 0.1208s/iter; left time: 9376.5978s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0932033\n",
      "\tspeed: 0.1171s/iter; left time: 9076.9066s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1083821\n",
      "\tspeed: 0.1178s/iter; left time: 9120.5453s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1158013\n",
      "\tspeed: 0.1144s/iter; left time: 8849.0927s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1378757\n",
      "\tspeed: 0.1162s/iter; left time: 8974.1123s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0924771\n",
      "\tspeed: 0.1162s/iter; left time: 8961.4239s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1077057\n",
      "\tspeed: 0.1180s/iter; left time: 9090.4405s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1156989\n",
      "\tspeed: 0.1167s/iter; left time: 8979.6128s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1000350\n",
      "\tspeed: 0.1160s/iter; left time: 8909.7129s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1146565\n",
      "\tspeed: 0.1172s/iter; left time: 8990.0227s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1231436\n",
      "\tspeed: 0.1170s/iter; left time: 8968.6194s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1224817\n",
      "\tspeed: 0.1151s/iter; left time: 8805.5514s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1114092\n",
      "\tspeed: 0.1154s/iter; left time: 8821.8669s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1154894\n",
      "\tspeed: 0.1148s/iter; left time: 8760.2005s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0983998\n",
      "\tspeed: 0.1184s/iter; left time: 9027.7861s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1058075\n",
      "\tspeed: 0.1176s/iter; left time: 8949.7973s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1094842\n",
      "\tspeed: 0.1186s/iter; left time: 9016.7895s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1029801\n",
      "\tspeed: 0.1215s/iter; left time: 9222.0609s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1099910\n",
      "\tspeed: 0.1177s/iter; left time: 8921.4090s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1054428\n",
      "\tspeed: 0.1200s/iter; left time: 9088.1881s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0913317\n",
      "\tspeed: 0.1180s/iter; left time: 8925.4389s\n",
      "Epoch: 3 cost time: 00h:08m:45.78s\n",
      "Epoch: 3 | Train Loss: 0.1093807 Vali Loss: 0.1236754 Test Loss: 0.1367024\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0959466\n",
      "\tspeed: 1.5738s/iter; left time: 118798.5643s\n",
      "\titers: 200, epoch: 4 | loss: 0.0858854\n",
      "\tspeed: 0.1189s/iter; left time: 8961.9629s\n",
      "\titers: 300, epoch: 4 | loss: 0.0893636\n",
      "\tspeed: 0.1198s/iter; left time: 9019.7581s\n",
      "\titers: 400, epoch: 4 | loss: 0.1088450\n",
      "\tspeed: 0.1180s/iter; left time: 8870.8322s\n",
      "\titers: 500, epoch: 4 | loss: 0.1087373\n",
      "\tspeed: 0.1165s/iter; left time: 8747.7988s\n",
      "\titers: 600, epoch: 4 | loss: 0.1005550\n",
      "\tspeed: 0.1151s/iter; left time: 8630.8889s\n",
      "\titers: 700, epoch: 4 | loss: 0.0969683\n",
      "\tspeed: 0.1181s/iter; left time: 8846.1387s\n",
      "\titers: 800, epoch: 4 | loss: 0.1045174\n",
      "\tspeed: 0.1171s/iter; left time: 8754.9369s\n",
      "\titers: 900, epoch: 4 | loss: 0.1086594\n",
      "\tspeed: 0.1166s/iter; left time: 8704.5463s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1303164\n",
      "\tspeed: 0.1176s/iter; left time: 8774.3390s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1121879\n",
      "\tspeed: 0.1182s/iter; left time: 8802.4399s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1189122\n",
      "\tspeed: 0.1178s/iter; left time: 8761.6614s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1055549\n",
      "\tspeed: 0.1178s/iter; left time: 8754.0308s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1178071\n",
      "\tspeed: 0.1213s/iter; left time: 8998.8108s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1012621\n",
      "\tspeed: 0.1200s/iter; left time: 8890.9761s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1049342\n",
      "\tspeed: 0.1199s/iter; left time: 8871.7970s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1118138\n",
      "\tspeed: 0.1209s/iter; left time: 8934.2080s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1249317\n",
      "\tspeed: 0.1209s/iter; left time: 8916.8846s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1314284\n",
      "\tspeed: 0.1186s/iter; left time: 8740.8187s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1265800\n",
      "\tspeed: 0.1188s/iter; left time: 8738.2363s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1018833\n",
      "\tspeed: 0.1207s/iter; left time: 8866.4914s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1211214\n",
      "\tspeed: 0.1200s/iter; left time: 8803.3129s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0848068\n",
      "\tspeed: 0.1199s/iter; left time: 8789.0981s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1038890\n",
      "\tspeed: 0.1198s/iter; left time: 8769.5086s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1123954\n",
      "\tspeed: 0.1190s/iter; left time: 8694.3077s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0989040\n",
      "\tspeed: 0.1209s/iter; left time: 8822.2843s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1005402\n",
      "\tspeed: 0.1177s/iter; left time: 8575.7761s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1054526\n",
      "\tspeed: 0.1200s/iter; left time: 8734.2448s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1114677\n",
      "\tspeed: 0.1160s/iter; left time: 8431.7920s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1075196\n",
      "\tspeed: 0.1189s/iter; left time: 8628.7340s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0856683\n",
      "\tspeed: 0.1195s/iter; left time: 8658.9518s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1058800\n",
      "\tspeed: 0.1180s/iter; left time: 8543.6849s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1083319\n",
      "\tspeed: 0.1208s/iter; left time: 8732.2973s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0981609\n",
      "\tspeed: 0.1202s/iter; left time: 8678.0036s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0921983\n",
      "\tspeed: 0.1207s/iter; left time: 8697.5703s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1049358\n",
      "\tspeed: 0.1205s/iter; left time: 8675.4825s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0929390\n",
      "\tspeed: 0.1195s/iter; left time: 8590.2117s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0982588\n",
      "\tspeed: 0.1194s/iter; left time: 8569.5723s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918985\n",
      "\tspeed: 0.1207s/iter; left time: 8649.6845s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1195650\n",
      "\tspeed: 0.1191s/iter; left time: 8526.0042s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1090801\n",
      "\tspeed: 0.1204s/iter; left time: 8606.8944s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1110307\n",
      "\tspeed: 0.1192s/iter; left time: 8505.5317s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1110436\n",
      "\tspeed: 0.1168s/iter; left time: 8327.9308s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1014118\n",
      "\tspeed: 0.1167s/iter; left time: 8308.2562s\n",
      "Epoch: 4 cost time: 00h:08m:50.08s\n",
      "Epoch: 4 | Train Loss: 0.1046495 Vali Loss: 0.1247817 Test Loss: 0.1472365\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1008341\n",
      "\tspeed: 1.5943s/iter; left time: 113251.2470s\n",
      "\titers: 200, epoch: 5 | loss: 0.1144764\n",
      "\tspeed: 0.1212s/iter; left time: 8596.8570s\n",
      "\titers: 300, epoch: 5 | loss: 0.1179324\n",
      "\tspeed: 0.1224s/iter; left time: 8667.5363s\n",
      "\titers: 400, epoch: 5 | loss: 0.1061344\n",
      "\tspeed: 0.1203s/iter; left time: 8507.9813s\n",
      "\titers: 500, epoch: 5 | loss: 0.0869319\n",
      "\tspeed: 0.1168s/iter; left time: 8251.2865s\n",
      "\titers: 600, epoch: 5 | loss: 0.0929676\n",
      "\tspeed: 0.1206s/iter; left time: 8505.0312s\n",
      "\titers: 700, epoch: 5 | loss: 0.1068786\n",
      "\tspeed: 0.1206s/iter; left time: 8494.8491s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935982\n",
      "\tspeed: 0.1185s/iter; left time: 8337.9691s\n",
      "\titers: 900, epoch: 5 | loss: 0.1054375\n",
      "\tspeed: 0.1175s/iter; left time: 8254.9225s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1086309\n",
      "\tspeed: 0.1202s/iter; left time: 8432.1821s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1122420\n",
      "\tspeed: 0.1192s/iter; left time: 8346.6720s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1004656\n",
      "\tspeed: 0.1199s/iter; left time: 8382.4266s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0959615\n",
      "\tspeed: 0.1190s/iter; left time: 8312.2799s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1126448\n",
      "\tspeed: 0.1158s/iter; left time: 8076.4031s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0902287\n",
      "\tspeed: 0.1192s/iter; left time: 8298.4404s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0997650\n",
      "\tspeed: 0.1182s/iter; left time: 8220.6378s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1175994\n",
      "\tspeed: 0.1196s/iter; left time: 8304.8478s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1059145\n",
      "\tspeed: 0.1186s/iter; left time: 8226.3011s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0982435\n",
      "\tspeed: 0.1187s/iter; left time: 8218.5163s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1148793\n",
      "\tspeed: 0.1204s/iter; left time: 8326.4242s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1237828\n",
      "\tspeed: 0.1166s/iter; left time: 8049.6795s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0931523\n",
      "\tspeed: 0.1141s/iter; left time: 7862.8554s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861305\n",
      "\tspeed: 0.1160s/iter; left time: 7987.8383s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0990851\n",
      "\tspeed: 0.1203s/iter; left time: 8266.9898s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0847341\n",
      "\tspeed: 0.1185s/iter; left time: 8131.6616s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1076903\n",
      "\tspeed: 0.1178s/iter; left time: 8076.7580s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1186636\n",
      "\tspeed: 0.1187s/iter; left time: 8124.0679s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0813073\n",
      "\tspeed: 0.1194s/iter; left time: 8161.1644s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0954584\n",
      "\tspeed: 0.1186s/iter; left time: 8091.0418s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0905743\n",
      "\tspeed: 0.1183s/iter; left time: 8063.9963s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1002901\n",
      "\tspeed: 0.1187s/iter; left time: 8075.8405s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0847461\n",
      "\tspeed: 0.1158s/iter; left time: 7868.1699s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1024117\n",
      "\tspeed: 0.1141s/iter; left time: 7742.7515s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0998646\n",
      "\tspeed: 0.1148s/iter; left time: 7772.8822s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0978041\n",
      "\tspeed: 0.1169s/iter; left time: 7909.1294s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1110223\n",
      "\tspeed: 0.1153s/iter; left time: 7785.4547s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0857197\n",
      "\tspeed: 0.1136s/iter; left time: 7663.1865s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0900995\n",
      "\tspeed: 0.1147s/iter; left time: 7720.9080s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0950421\n",
      "\tspeed: 0.1149s/iter; left time: 7724.4726s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1002941\n",
      "\tspeed: 0.1176s/iter; left time: 7894.1738s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1072492\n",
      "\tspeed: 0.1189s/iter; left time: 7973.0274s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0923533\n",
      "\tspeed: 0.1184s/iter; left time: 7922.1517s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0978186\n",
      "\tspeed: 0.1173s/iter; left time: 7837.3911s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0912306\n",
      "\tspeed: 0.1186s/iter; left time: 7914.4866s\n",
      "Epoch: 5 cost time: 00h:08m:46.01s\n",
      "Epoch: 5 | Train Loss: 0.1001263 Vali Loss: 0.1304869 Test Loss: 0.1511040\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1040925\n",
      "\tspeed: 1.5616s/iter; left time: 103988.1173s\n",
      "\titers: 200, epoch: 6 | loss: 0.1047199\n",
      "\tspeed: 0.1154s/iter; left time: 7674.7197s\n",
      "\titers: 300, epoch: 6 | loss: 0.0804931\n",
      "\tspeed: 0.1161s/iter; left time: 7705.3513s\n",
      "\titers: 400, epoch: 6 | loss: 0.0875467\n",
      "\tspeed: 0.1160s/iter; left time: 7688.9225s\n",
      "\titers: 500, epoch: 6 | loss: 0.0984672\n",
      "\tspeed: 0.1171s/iter; left time: 7747.7005s\n",
      "\titers: 600, epoch: 6 | loss: 0.1009006\n",
      "\tspeed: 0.1151s/iter; left time: 7604.0128s\n",
      "\titers: 700, epoch: 6 | loss: 0.0918718\n",
      "\tspeed: 0.1153s/iter; left time: 7606.5829s\n",
      "\titers: 800, epoch: 6 | loss: 0.1056673\n",
      "\tspeed: 0.1163s/iter; left time: 7662.1389s\n",
      "\titers: 900, epoch: 6 | loss: 0.0862563\n",
      "\tspeed: 0.1173s/iter; left time: 7717.6531s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0999702\n",
      "\tspeed: 0.1183s/iter; left time: 7772.3439s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0961189\n",
      "\tspeed: 0.1175s/iter; left time: 7709.2899s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1147054\n",
      "\tspeed: 0.1196s/iter; left time: 7832.8036s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0912160\n",
      "\tspeed: 0.1159s/iter; left time: 7580.3703s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0981800\n",
      "\tspeed: 0.1178s/iter; left time: 7691.0740s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0838766\n",
      "\tspeed: 0.1163s/iter; left time: 7580.5629s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0883116\n",
      "\tspeed: 0.1173s/iter; left time: 7636.6964s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0896513\n",
      "\tspeed: 0.1184s/iter; left time: 7691.9413s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0974333\n",
      "\tspeed: 0.1184s/iter; left time: 7682.3544s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0899982\n",
      "\tspeed: 0.1191s/iter; left time: 7717.4354s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0925732\n",
      "\tspeed: 0.1177s/iter; left time: 7611.6916s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0867949\n",
      "\tspeed: 0.1157s/iter; left time: 7475.8082s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1011756\n",
      "\tspeed: 0.1154s/iter; left time: 7445.3419s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0931246\n",
      "\tspeed: 0.1148s/iter; left time: 7393.9080s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0912003\n",
      "\tspeed: 0.1158s/iter; left time: 7443.0243s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0813842\n",
      "\tspeed: 0.1183s/iter; left time: 7595.5862s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0974265\n",
      "\tspeed: 0.1168s/iter; left time: 7488.7034s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1037935\n",
      "\tspeed: 0.1178s/iter; left time: 7539.9889s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0945832\n",
      "\tspeed: 0.1156s/iter; left time: 7388.0370s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0993364\n",
      "\tspeed: 0.1164s/iter; left time: 7422.9639s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0983878\n",
      "\tspeed: 0.1167s/iter; left time: 7434.3003s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0967573\n",
      "\tspeed: 0.1179s/iter; left time: 7497.4195s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0809352\n",
      "\tspeed: 0.1174s/iter; left time: 7454.3270s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0907727\n",
      "\tspeed: 0.1185s/iter; left time: 7510.3753s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1019389\n",
      "\tspeed: 0.1167s/iter; left time: 7387.1968s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1059106\n",
      "\tspeed: 0.1177s/iter; left time: 7434.7280s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0917989\n",
      "\tspeed: 0.1153s/iter; left time: 7276.6796s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0842525\n",
      "\tspeed: 0.1166s/iter; left time: 7343.0432s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0876396\n",
      "\tspeed: 0.1160s/iter; left time: 7297.2793s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1118588\n",
      "\tspeed: 0.1165s/iter; left time: 7313.6204s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0991682\n",
      "\tspeed: 0.1169s/iter; left time: 7331.3833s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0930842\n",
      "\tspeed: 0.1147s/iter; left time: 7177.1510s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0981919\n",
      "\tspeed: 0.1146s/iter; left time: 7161.5941s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0870969\n",
      "\tspeed: 0.1152s/iter; left time: 7187.5492s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0964762\n",
      "\tspeed: 0.1156s/iter; left time: 7198.3402s\n",
      "Epoch: 6 cost time: 00h:08m:39.51s\n",
      "Epoch: 6 | Train Loss: 0.0960726 Vali Loss: 0.1291619 Test Loss: 0.1511165\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03767301142215729, rmse:0.19409537315368652, mae:0.1336192935705185, rse:0.6876434087753296\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 168: 01h:09m:06.59s\n",
      "\n",
      "Intermediate time for DE: 04h:52m:56.47s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-03 06:05:07,960] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 06:05:09,402] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 06:05:09,402] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 06:05:09,402] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 06:05:09,558] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 06:05:09,558] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 06:05:10,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 06:05:10,277] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 06:05:10,277] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 06:05:10,278] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 06:05:10,279] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 06:05:10,279] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 06:05:10,279] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 06:05:10,610] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 06:05:10,611] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,611] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.89 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,736] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 06:05:10,737] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,737] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.89 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,737] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 06:05:10,860] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 06:05:10,861] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 06:05:10,861] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.87 GB, percent = 13.2%\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 06:05:10,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 06:05:10,863] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f31c2cc07d0>\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 06:05:10,864] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 06:05:10,865] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 06:05:10,866] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1466742\n",
      "\tspeed: 0.1748s/iter; left time: 15598.4857s\n",
      "\titers: 200, epoch: 1 | loss: 0.1377795\n",
      "\tspeed: 0.1298s/iter; left time: 11573.9787s\n",
      "\titers: 300, epoch: 1 | loss: 0.1637115\n",
      "\tspeed: 0.1289s/iter; left time: 11478.2904s\n",
      "\titers: 400, epoch: 1 | loss: 0.1077301\n",
      "\tspeed: 0.1275s/iter; left time: 11342.6149s\n",
      "\titers: 500, epoch: 1 | loss: 0.1226833\n",
      "\tspeed: 0.1276s/iter; left time: 11334.4573s\n",
      "\titers: 600, epoch: 1 | loss: 0.1199201\n",
      "\tspeed: 0.1312s/iter; left time: 11642.2717s\n",
      "\titers: 700, epoch: 1 | loss: 0.1068131\n",
      "\tspeed: 0.1287s/iter; left time: 11414.0931s\n",
      "\titers: 800, epoch: 1 | loss: 0.1092960\n",
      "\tspeed: 0.1320s/iter; left time: 11692.7568s\n",
      "\titers: 900, epoch: 1 | loss: 0.0919815\n",
      "\tspeed: 0.1286s/iter; left time: 11373.6150s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1038064\n",
      "\tspeed: 0.1298s/iter; left time: 11468.7619s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0846845\n",
      "\tspeed: 0.1312s/iter; left time: 11578.5412s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0949691\n",
      "\tspeed: 0.1288s/iter; left time: 11353.3324s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0737247\n",
      "\tspeed: 0.1311s/iter; left time: 11545.0504s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1031198\n",
      "\tspeed: 0.1302s/iter; left time: 11452.3215s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0780963\n",
      "\tspeed: 0.1307s/iter; left time: 11482.6928s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978673\n",
      "\tspeed: 0.1296s/iter; left time: 11371.6500s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1247796\n",
      "\tspeed: 0.1308s/iter; left time: 11464.9652s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062685\n",
      "\tspeed: 0.1303s/iter; left time: 11404.8978s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1049088\n",
      "\tspeed: 0.1276s/iter; left time: 11160.2830s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0870789\n",
      "\tspeed: 0.1304s/iter; left time: 11394.5054s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0814199\n",
      "\tspeed: 0.1311s/iter; left time: 11437.0125s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0885388\n",
      "\tspeed: 0.1297s/iter; left time: 11307.0506s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0876981\n",
      "\tspeed: 0.1284s/iter; left time: 11179.8422s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0750865\n",
      "\tspeed: 0.1304s/iter; left time: 11339.8303s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0770076\n",
      "\tspeed: 0.1299s/iter; left time: 11285.7984s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0930856\n",
      "\tspeed: 0.1271s/iter; left time: 11031.3798s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0914229\n",
      "\tspeed: 0.1238s/iter; left time: 10727.8290s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0742800\n",
      "\tspeed: 0.1251s/iter; left time: 10831.5647s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0699159\n",
      "\tspeed: 0.1299s/iter; left time: 11227.7913s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0888919\n",
      "\tspeed: 0.1286s/iter; left time: 11104.4819s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0732918\n",
      "\tspeed: 0.1276s/iter; left time: 11008.2922s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0881244\n",
      "\tspeed: 0.1289s/iter; left time: 11106.4858s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0822907\n",
      "\tspeed: 0.1264s/iter; left time: 10876.1296s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0753193\n",
      "\tspeed: 0.1298s/iter; left time: 11160.7985s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0816539\n",
      "\tspeed: 0.1282s/iter; left time: 11005.9994s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0759263\n",
      "\tspeed: 0.1307s/iter; left time: 11205.8377s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0969475\n",
      "\tspeed: 0.1297s/iter; left time: 11113.0399s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0810152\n",
      "\tspeed: 0.1271s/iter; left time: 10871.4882s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0943139\n",
      "\tspeed: 0.1299s/iter; left time: 11097.8455s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0698772\n",
      "\tspeed: 0.1268s/iter; left time: 10825.6037s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0704658\n",
      "\tspeed: 0.1269s/iter; left time: 10819.6445s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1035011\n",
      "\tspeed: 0.1271s/iter; left time: 10823.0052s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0949263\n",
      "\tspeed: 0.1275s/iter; left time: 10844.8182s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0885897\n",
      "\tspeed: 0.1288s/iter; left time: 10939.3516s\n",
      "Epoch: 1 cost time: 00h:09m:37.08s\n",
      "Epoch: 1 | Train Loss: 0.0973537 Vali Loss: 0.0928246 Test Loss: 0.1050737\n",
      "Validation loss decreased (inf --> 0.092825).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0701588\n",
      "\tspeed: 1.8726s/iter; left time: 158786.1147s\n",
      "\titers: 200, epoch: 2 | loss: 0.1044648\n",
      "\tspeed: 0.1163s/iter; left time: 9848.0479s\n",
      "\titers: 300, epoch: 2 | loss: 0.0884274\n",
      "\tspeed: 0.1176s/iter; left time: 9950.3132s\n",
      "\titers: 400, epoch: 2 | loss: 0.1147017\n",
      "\tspeed: 0.1188s/iter; left time: 10040.6889s\n",
      "\titers: 500, epoch: 2 | loss: 0.0937675\n",
      "\tspeed: 0.1192s/iter; left time: 10056.2784s\n",
      "\titers: 600, epoch: 2 | loss: 0.0921706\n",
      "\tspeed: 0.1180s/iter; left time: 9950.1293s\n",
      "\titers: 700, epoch: 2 | loss: 0.0930738\n",
      "\tspeed: 0.1198s/iter; left time: 10087.4863s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000150\n",
      "\tspeed: 0.1200s/iter; left time: 10094.0531s\n",
      "\titers: 900, epoch: 2 | loss: 0.0975661\n",
      "\tspeed: 0.1182s/iter; left time: 9925.3564s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0700671\n",
      "\tspeed: 0.1170s/iter; left time: 9816.9412s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0929756\n",
      "\tspeed: 0.1192s/iter; left time: 9988.0118s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0745666\n",
      "\tspeed: 0.1180s/iter; left time: 9879.2296s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0924439\n",
      "\tspeed: 0.1171s/iter; left time: 9789.0249s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1111560\n",
      "\tspeed: 0.1153s/iter; left time: 9628.5784s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0793124\n",
      "\tspeed: 0.1199s/iter; left time: 10001.1410s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0794686\n",
      "\tspeed: 0.1185s/iter; left time: 9867.7083s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0875069\n",
      "\tspeed: 0.1171s/iter; left time: 9738.6008s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0863800\n",
      "\tspeed: 0.1146s/iter; left time: 9522.1638s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0864655\n",
      "\tspeed: 0.1185s/iter; left time: 9833.4448s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0874661\n",
      "\tspeed: 0.1182s/iter; left time: 9800.8815s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0986785\n",
      "\tspeed: 0.1171s/iter; left time: 9692.0347s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0935980\n",
      "\tspeed: 0.1163s/iter; left time: 9617.8344s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0926060\n",
      "\tspeed: 0.1199s/iter; left time: 9902.1111s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731041\n",
      "\tspeed: 0.1162s/iter; left time: 9588.0168s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0878800\n",
      "\tspeed: 0.1154s/iter; left time: 9504.3980s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1000494\n",
      "\tspeed: 0.1144s/iter; left time: 9412.6264s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0909730\n",
      "\tspeed: 0.1134s/iter; left time: 9324.6489s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1120826\n",
      "\tspeed: 0.1131s/iter; left time: 9284.1912s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0753991\n",
      "\tspeed: 0.1170s/iter; left time: 9593.3664s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0918223\n",
      "\tspeed: 0.1166s/iter; left time: 9550.6203s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1169008\n",
      "\tspeed: 0.1160s/iter; left time: 9491.9086s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0799625\n",
      "\tspeed: 0.1160s/iter; left time: 9477.0554s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0924724\n",
      "\tspeed: 0.1156s/iter; left time: 9434.9133s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0893005\n",
      "\tspeed: 0.1156s/iter; left time: 9423.6612s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0870048\n",
      "\tspeed: 0.1142s/iter; left time: 9291.7373s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0749805\n",
      "\tspeed: 0.1153s/iter; left time: 9374.1188s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0773853\n",
      "\tspeed: 0.1178s/iter; left time: 9567.4743s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0819646\n",
      "\tspeed: 0.1175s/iter; left time: 9526.6254s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0823601\n",
      "\tspeed: 0.1144s/iter; left time: 9265.6871s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0686158\n",
      "\tspeed: 0.1143s/iter; left time: 9248.0084s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0982631\n",
      "\tspeed: 0.1139s/iter; left time: 9199.3269s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0787107\n",
      "\tspeed: 0.1149s/iter; left time: 9272.7571s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0879422\n",
      "\tspeed: 0.1146s/iter; left time: 9238.3802s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0896147\n",
      "\tspeed: 0.1152s/iter; left time: 9276.6456s\n",
      "Epoch: 2 cost time: 00h:08m:41.97s\n",
      "Epoch: 2 | Train Loss: 0.0860910 Vali Loss: 0.0922121 Test Loss: 0.1065872\n",
      "Validation loss decreased (0.092825 --> 0.092212).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0907432\n",
      "\tspeed: 1.6204s/iter; left time: 130160.2971s\n",
      "\titers: 200, epoch: 3 | loss: 0.0779589\n",
      "\tspeed: 0.1152s/iter; left time: 9241.5971s\n",
      "\titers: 300, epoch: 3 | loss: 0.0885364\n",
      "\tspeed: 0.1163s/iter; left time: 9316.8426s\n",
      "\titers: 400, epoch: 3 | loss: 0.0848934\n",
      "\tspeed: 0.1171s/iter; left time: 9367.1094s\n",
      "\titers: 500, epoch: 3 | loss: 0.0842507\n",
      "\tspeed: 0.1172s/iter; left time: 9364.6484s\n",
      "\titers: 600, epoch: 3 | loss: 0.0788545\n",
      "\tspeed: 0.1169s/iter; left time: 9328.3574s\n",
      "\titers: 700, epoch: 3 | loss: 0.0803074\n",
      "\tspeed: 0.1174s/iter; left time: 9359.3329s\n",
      "\titers: 800, epoch: 3 | loss: 0.0902780\n",
      "\tspeed: 0.1153s/iter; left time: 9179.2263s\n",
      "\titers: 900, epoch: 3 | loss: 0.0700542\n",
      "\tspeed: 0.1165s/iter; left time: 9260.9572s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0909660\n",
      "\tspeed: 0.1148s/iter; left time: 9119.6276s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0937846\n",
      "\tspeed: 0.1147s/iter; left time: 9097.6639s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0607558\n",
      "\tspeed: 0.1154s/iter; left time: 9145.7810s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0803862\n",
      "\tspeed: 0.1159s/iter; left time: 9167.5892s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0932646\n",
      "\tspeed: 0.1168s/iter; left time: 9233.6737s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0733327\n",
      "\tspeed: 0.1160s/iter; left time: 9157.2614s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0595024\n",
      "\tspeed: 0.1156s/iter; left time: 9109.5161s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0652814\n",
      "\tspeed: 0.1152s/iter; left time: 9072.5026s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0982525\n",
      "\tspeed: 0.1133s/iter; left time: 8909.1675s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0716447\n",
      "\tspeed: 0.1150s/iter; left time: 9034.0922s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0835190\n",
      "\tspeed: 0.1154s/iter; left time: 9052.4635s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0834547\n",
      "\tspeed: 0.1173s/iter; left time: 9187.7496s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0948817\n",
      "\tspeed: 0.1146s/iter; left time: 8965.3189s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0844729\n",
      "\tspeed: 0.1131s/iter; left time: 8832.9053s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0765860\n",
      "\tspeed: 0.1153s/iter; left time: 8993.3148s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0919627\n",
      "\tspeed: 0.1158s/iter; left time: 9022.1140s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0797675\n",
      "\tspeed: 0.1170s/iter; left time: 9102.4720s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0741416\n",
      "\tspeed: 0.1155s/iter; left time: 8973.8882s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0829531\n",
      "\tspeed: 0.1155s/iter; left time: 8968.6057s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0889192\n",
      "\tspeed: 0.1147s/iter; left time: 8889.3441s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0789738\n",
      "\tspeed: 0.1158s/iter; left time: 8968.2427s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0949245\n",
      "\tspeed: 0.1167s/iter; left time: 9026.3304s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0981605\n",
      "\tspeed: 0.1178s/iter; left time: 9098.8648s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0789458\n",
      "\tspeed: 0.1169s/iter; left time: 9016.7586s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0741549\n",
      "\tspeed: 0.1187s/iter; left time: 9139.5161s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0773597\n",
      "\tspeed: 0.1182s/iter; left time: 9091.3377s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0935745\n",
      "\tspeed: 0.1173s/iter; left time: 9012.3308s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0769829\n",
      "\tspeed: 0.1185s/iter; left time: 9089.8660s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0949859\n",
      "\tspeed: 0.1183s/iter; left time: 9068.1966s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0808080\n",
      "\tspeed: 0.1185s/iter; left time: 9066.9092s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0665414\n",
      "\tspeed: 0.1160s/iter; left time: 8861.8437s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0754559\n",
      "\tspeed: 0.1176s/iter; left time: 8973.6887s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0699142\n",
      "\tspeed: 0.1166s/iter; left time: 8885.9167s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0632910\n",
      "\tspeed: 0.1188s/iter; left time: 9041.2206s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0808305\n",
      "\tspeed: 0.1158s/iter; left time: 8802.9235s\n",
      "Epoch: 3 cost time: 00h:08m:40.38s\n",
      "Epoch: 3 | Train Loss: 0.0838997 Vali Loss: 0.0905142 Test Loss: 0.1035439\n",
      "Validation loss decreased (0.092212 --> 0.090514).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0914321\n",
      "\tspeed: 1.6342s/iter; left time: 123966.1902s\n",
      "\titers: 200, epoch: 4 | loss: 0.0923484\n",
      "\tspeed: 0.1178s/iter; left time: 8923.0128s\n",
      "\titers: 300, epoch: 4 | loss: 0.0886503\n",
      "\tspeed: 0.1152s/iter; left time: 8712.2391s\n",
      "\titers: 400, epoch: 4 | loss: 0.0890385\n",
      "\tspeed: 0.1155s/iter; left time: 8726.3773s\n",
      "\titers: 500, epoch: 4 | loss: 0.0889764\n",
      "\tspeed: 0.1156s/iter; left time: 8719.1197s\n",
      "\titers: 600, epoch: 4 | loss: 0.0682504\n",
      "\tspeed: 0.1178s/iter; left time: 8874.6318s\n",
      "\titers: 700, epoch: 4 | loss: 0.0858520\n",
      "\tspeed: 0.1163s/iter; left time: 8755.5159s\n",
      "\titers: 800, epoch: 4 | loss: 0.0807579\n",
      "\tspeed: 0.1139s/iter; left time: 8558.9584s\n",
      "\titers: 900, epoch: 4 | loss: 0.0771627\n",
      "\tspeed: 0.1157s/iter; left time: 8685.2748s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0891313\n",
      "\tspeed: 0.1158s/iter; left time: 8678.3248s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0808600\n",
      "\tspeed: 0.1133s/iter; left time: 8481.0700s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1001335\n",
      "\tspeed: 0.1148s/iter; left time: 8579.0647s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1131133\n",
      "\tspeed: 0.1136s/iter; left time: 8482.4180s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0778967\n",
      "\tspeed: 0.1156s/iter; left time: 8617.9859s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0659208\n",
      "\tspeed: 0.1143s/iter; left time: 8512.1282s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0821595\n",
      "\tspeed: 0.1144s/iter; left time: 8504.2124s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0919877\n",
      "\tspeed: 0.1148s/iter; left time: 8525.1535s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0923949\n",
      "\tspeed: 0.1144s/iter; left time: 8486.7110s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0738678\n",
      "\tspeed: 0.1155s/iter; left time: 8557.1487s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0819451\n",
      "\tspeed: 0.1135s/iter; left time: 8393.9204s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0814312\n",
      "\tspeed: 0.1126s/iter; left time: 8317.2582s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0888639\n",
      "\tspeed: 0.1129s/iter; left time: 8323.5396s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1003453\n",
      "\tspeed: 0.1129s/iter; left time: 8318.3946s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0999414\n",
      "\tspeed: 0.1145s/iter; left time: 8425.5708s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0742211\n",
      "\tspeed: 0.1128s/iter; left time: 8283.9718s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0718067\n",
      "\tspeed: 0.1142s/iter; left time: 8377.3046s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0911036\n",
      "\tspeed: 0.1134s/iter; left time: 8308.4250s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877281\n",
      "\tspeed: 0.1117s/iter; left time: 8169.8015s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0797919\n",
      "\tspeed: 0.1138s/iter; left time: 8314.7661s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0775673\n",
      "\tspeed: 0.1138s/iter; left time: 8305.5354s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0879129\n",
      "\tspeed: 0.1121s/iter; left time: 8167.1228s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0780287\n",
      "\tspeed: 0.1125s/iter; left time: 8183.4866s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0876071\n",
      "\tspeed: 0.1155s/iter; left time: 8394.3315s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0757971\n",
      "\tspeed: 0.1134s/iter; left time: 8226.7189s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0894305\n",
      "\tspeed: 0.1156s/iter; left time: 8377.4186s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0851934\n",
      "\tspeed: 0.1136s/iter; left time: 8218.1792s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0877569\n",
      "\tspeed: 0.1131s/iter; left time: 8175.3755s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0781913\n",
      "\tspeed: 0.1133s/iter; left time: 8172.6620s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0874085\n",
      "\tspeed: 0.1160s/iter; left time: 8359.8425s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0781264\n",
      "\tspeed: 0.1144s/iter; left time: 8235.0631s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0745134\n",
      "\tspeed: 0.1133s/iter; left time: 8138.9936s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0732169\n",
      "\tspeed: 0.1151s/iter; left time: 8258.0444s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0699919\n",
      "\tspeed: 0.1143s/iter; left time: 8192.2432s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0951408\n",
      "\tspeed: 0.1139s/iter; left time: 8151.1855s\n",
      "Epoch: 4 cost time: 00h:08m:31.76s\n",
      "Epoch: 4 | Train Loss: 0.0825824 Vali Loss: 0.0908698 Test Loss: 0.1047861\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0933834\n",
      "\tspeed: 1.5999s/iter; left time: 114213.8981s\n",
      "\titers: 200, epoch: 5 | loss: 0.0743399\n",
      "\tspeed: 0.1159s/iter; left time: 8264.3638s\n",
      "\titers: 300, epoch: 5 | loss: 0.0725531\n",
      "\tspeed: 0.1166s/iter; left time: 8303.3212s\n",
      "\titers: 400, epoch: 5 | loss: 0.0881155\n",
      "\tspeed: 0.1159s/iter; left time: 8242.0165s\n",
      "\titers: 500, epoch: 5 | loss: 0.0945252\n",
      "\tspeed: 0.1146s/iter; left time: 8136.5189s\n",
      "\titers: 600, epoch: 5 | loss: 0.0865966\n",
      "\tspeed: 0.1161s/iter; left time: 8226.7902s\n",
      "\titers: 700, epoch: 5 | loss: 0.0829508\n",
      "\tspeed: 0.1164s/iter; left time: 8240.0986s\n",
      "\titers: 800, epoch: 5 | loss: 0.0887444\n",
      "\tspeed: 0.1174s/iter; left time: 8298.8268s\n",
      "\titers: 900, epoch: 5 | loss: 0.1051521\n",
      "\tspeed: 0.1152s/iter; left time: 8132.9371s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0809733\n",
      "\tspeed: 0.1146s/iter; left time: 8075.1920s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0754441\n",
      "\tspeed: 0.1171s/iter; left time: 8239.9571s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0971770\n",
      "\tspeed: 0.1164s/iter; left time: 8179.7599s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0774984\n",
      "\tspeed: 0.1165s/iter; left time: 8179.5923s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0693193\n",
      "\tspeed: 0.1139s/iter; left time: 7981.7273s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0622493\n",
      "\tspeed: 0.1144s/iter; left time: 8004.3232s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0728563\n",
      "\tspeed: 0.1140s/iter; left time: 7964.1308s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0667490\n",
      "\tspeed: 0.1165s/iter; left time: 8131.8763s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0948240\n",
      "\tspeed: 0.1157s/iter; left time: 8066.1557s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0625855\n",
      "\tspeed: 0.1140s/iter; left time: 7935.2409s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0731169\n",
      "\tspeed: 0.1153s/iter; left time: 8012.9702s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0691220\n",
      "\tspeed: 0.1131s/iter; left time: 7850.1405s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0799258\n",
      "\tspeed: 0.1139s/iter; left time: 7891.7664s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0892581\n",
      "\tspeed: 0.1147s/iter; left time: 7938.9441s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0849659\n",
      "\tspeed: 0.1138s/iter; left time: 7859.0180s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0802421\n",
      "\tspeed: 0.1131s/iter; left time: 7804.0505s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0774631\n",
      "\tspeed: 0.1140s/iter; left time: 7852.8952s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0812496\n",
      "\tspeed: 0.1143s/iter; left time: 7862.3026s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0752958\n",
      "\tspeed: 0.1138s/iter; left time: 7819.8376s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0816401\n",
      "\tspeed: 0.1172s/iter; left time: 8041.8530s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0708815\n",
      "\tspeed: 0.1164s/iter; left time: 7969.1932s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1038611\n",
      "\tspeed: 0.1152s/iter; left time: 7878.3293s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0952071\n",
      "\tspeed: 0.1161s/iter; left time: 7926.1036s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0748858\n",
      "\tspeed: 0.1175s/iter; left time: 8012.6982s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0783696\n",
      "\tspeed: 0.1139s/iter; left time: 7758.4646s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0848109\n",
      "\tspeed: 0.1161s/iter; left time: 7894.9146s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0840811\n",
      "\tspeed: 0.1160s/iter; left time: 7871.9308s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0691066\n",
      "\tspeed: 0.1172s/iter; left time: 7944.3060s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0793731\n",
      "\tspeed: 0.1158s/iter; left time: 7835.1559s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0673810\n",
      "\tspeed: 0.1163s/iter; left time: 7860.3347s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0856006\n",
      "\tspeed: 0.1140s/iter; left time: 7696.4913s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0628996\n",
      "\tspeed: 0.1153s/iter; left time: 7768.3951s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0903348\n",
      "\tspeed: 0.1154s/iter; left time: 7765.9766s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0866672\n",
      "\tspeed: 0.1165s/iter; left time: 7825.4286s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0803752\n",
      "\tspeed: 0.1151s/iter; left time: 7723.7072s\n",
      "Epoch: 5 cost time: 00h:08m:36.25s\n",
      "Epoch: 5 | Train Loss: 0.0813836 Vali Loss: 0.0900973 Test Loss: 0.1040331\n",
      "Validation loss decreased (0.090514 --> 0.090097).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0863184\n",
      "\tspeed: 1.6079s/iter; left time: 107602.2371s\n",
      "\titers: 200, epoch: 6 | loss: 0.0890369\n",
      "\tspeed: 0.1166s/iter; left time: 7792.9815s\n",
      "\titers: 300, epoch: 6 | loss: 0.0835939\n",
      "\tspeed: 0.1167s/iter; left time: 7788.6726s\n",
      "\titers: 400, epoch: 6 | loss: 0.0703793\n",
      "\tspeed: 0.1153s/iter; left time: 7681.3484s\n",
      "\titers: 500, epoch: 6 | loss: 0.0827197\n",
      "\tspeed: 0.1163s/iter; left time: 7737.2977s\n",
      "\titers: 600, epoch: 6 | loss: 0.0714470\n",
      "\tspeed: 0.1183s/iter; left time: 7858.6330s\n",
      "\titers: 700, epoch: 6 | loss: 0.0747727\n",
      "\tspeed: 0.1166s/iter; left time: 7734.6799s\n",
      "\titers: 800, epoch: 6 | loss: 0.0732784\n",
      "\tspeed: 0.1169s/iter; left time: 7739.5273s\n",
      "\titers: 900, epoch: 6 | loss: 0.0771534\n",
      "\tspeed: 0.1158s/iter; left time: 7658.9947s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0741465\n",
      "\tspeed: 0.1157s/iter; left time: 7640.8549s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0746656\n",
      "\tspeed: 0.1148s/iter; left time: 7567.3418s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0931855\n",
      "\tspeed: 0.1191s/iter; left time: 7837.7050s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0977927\n",
      "\tspeed: 0.1163s/iter; left time: 7644.3140s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0876032\n",
      "\tspeed: 0.1166s/iter; left time: 7651.1682s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0801408\n",
      "\tspeed: 0.1177s/iter; left time: 7712.7444s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721369\n",
      "\tspeed: 0.1177s/iter; left time: 7696.8397s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0922342\n",
      "\tspeed: 0.1171s/iter; left time: 7648.3469s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0814926\n",
      "\tspeed: 0.1143s/iter; left time: 7452.2997s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0799462\n",
      "\tspeed: 0.1161s/iter; left time: 7563.4943s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0895512\n",
      "\tspeed: 0.1159s/iter; left time: 7538.6946s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0853165\n",
      "\tspeed: 0.1170s/iter; left time: 7596.2929s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1018588\n",
      "\tspeed: 0.1164s/iter; left time: 7545.7194s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0753619\n",
      "\tspeed: 0.1164s/iter; left time: 7536.7268s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0675035\n",
      "\tspeed: 0.1160s/iter; left time: 7493.0730s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0995846\n",
      "\tspeed: 0.1147s/iter; left time: 7398.4242s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0759167\n",
      "\tspeed: 0.1147s/iter; left time: 7387.5417s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0709326\n",
      "\tspeed: 0.1167s/iter; left time: 7503.3345s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1127540\n",
      "\tspeed: 0.1168s/iter; left time: 7503.3535s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0813409\n",
      "\tspeed: 0.1158s/iter; left time: 7424.7262s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0786561\n",
      "\tspeed: 0.1145s/iter; left time: 7330.6350s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0826245\n",
      "\tspeed: 0.1170s/iter; left time: 7480.9854s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0986693\n",
      "\tspeed: 0.1163s/iter; left time: 7423.9302s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0849674\n",
      "\tspeed: 0.1173s/iter; left time: 7471.3527s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0744465\n",
      "\tspeed: 0.1141s/iter; left time: 7260.0666s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0913548\n",
      "\tspeed: 0.1164s/iter; left time: 7393.4716s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0656657\n",
      "\tspeed: 0.1173s/iter; left time: 7438.1420s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0794955\n",
      "\tspeed: 0.1132s/iter; left time: 7168.1313s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0760554\n",
      "\tspeed: 0.1145s/iter; left time: 7239.6268s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0779494\n",
      "\tspeed: 0.1147s/iter; left time: 7242.1353s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0917149\n",
      "\tspeed: 0.1142s/iter; left time: 7199.8447s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0724742\n",
      "\tspeed: 0.1148s/iter; left time: 7224.6386s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0769409\n",
      "\tspeed: 0.1161s/iter; left time: 7291.8852s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0667945\n",
      "\tspeed: 0.1167s/iter; left time: 7317.3923s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0704586\n",
      "\tspeed: 0.1129s/iter; left time: 7072.5778s\n",
      "Epoch: 6 cost time: 00h:08m:38.83s\n",
      "Epoch: 6 | Train Loss: 0.0803718 Vali Loss: 0.0905898 Test Loss: 0.1044702\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0694771\n",
      "\tspeed: 1.5992s/iter; left time: 99877.1591s\n",
      "\titers: 200, epoch: 7 | loss: 0.0701124\n",
      "\tspeed: 0.1165s/iter; left time: 7262.2913s\n",
      "\titers: 300, epoch: 7 | loss: 0.0611065\n",
      "\tspeed: 0.1148s/iter; left time: 7147.6222s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853083\n",
      "\tspeed: 0.1183s/iter; left time: 7353.4378s\n",
      "\titers: 500, epoch: 7 | loss: 0.0865817\n",
      "\tspeed: 0.1156s/iter; left time: 7176.3244s\n",
      "\titers: 600, epoch: 7 | loss: 0.0731895\n",
      "\tspeed: 0.1170s/iter; left time: 7250.7958s\n",
      "\titers: 700, epoch: 7 | loss: 0.1010686\n",
      "\tspeed: 0.1172s/iter; left time: 7250.9657s\n",
      "\titers: 800, epoch: 7 | loss: 0.0807473\n",
      "\tspeed: 0.1166s/iter; left time: 7200.2405s\n",
      "\titers: 900, epoch: 7 | loss: 0.0729549\n",
      "\tspeed: 0.1158s/iter; left time: 7137.6264s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0784136\n",
      "\tspeed: 0.1160s/iter; left time: 7142.9028s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0915089\n",
      "\tspeed: 0.1162s/iter; left time: 7138.6109s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0813137\n",
      "\tspeed: 0.1176s/iter; left time: 7214.8011s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0710811\n",
      "\tspeed: 0.1147s/iter; left time: 7023.3959s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674738\n",
      "\tspeed: 0.1169s/iter; left time: 7146.4507s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0731219\n",
      "\tspeed: 0.1159s/iter; left time: 7076.6751s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0795863\n",
      "\tspeed: 0.1175s/iter; left time: 7159.0093s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0768837\n",
      "\tspeed: 0.1178s/iter; left time: 7169.3604s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0973881\n",
      "\tspeed: 0.1162s/iter; left time: 7058.1413s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0784142\n",
      "\tspeed: 0.1173s/iter; left time: 7111.9250s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0736109\n",
      "\tspeed: 0.1175s/iter; left time: 7113.4931s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0847631\n",
      "\tspeed: 0.1163s/iter; left time: 7030.1128s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0700621\n",
      "\tspeed: 0.1185s/iter; left time: 7152.6374s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0826743\n",
      "\tspeed: 0.1178s/iter; left time: 7096.9864s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0855349\n",
      "\tspeed: 0.1179s/iter; left time: 7089.6478s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0894471\n",
      "\tspeed: 0.1159s/iter; left time: 6961.7498s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0709336\n",
      "\tspeed: 0.1170s/iter; left time: 7014.0372s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0635979\n",
      "\tspeed: 0.1142s/iter; left time: 6838.1543s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0727365\n",
      "\tspeed: 0.1165s/iter; left time: 6960.3037s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0891747\n",
      "\tspeed: 0.1137s/iter; left time: 6781.6957s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0914883\n",
      "\tspeed: 0.1150s/iter; left time: 6848.7837s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0926042\n",
      "\tspeed: 0.1167s/iter; left time: 6940.1020s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0622337\n",
      "\tspeed: 0.1151s/iter; left time: 6832.7122s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0735094\n",
      "\tspeed: 0.1146s/iter; left time: 6793.1364s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0951522\n",
      "\tspeed: 0.1152s/iter; left time: 6813.5322s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0720976\n",
      "\tspeed: 0.1153s/iter; left time: 6807.6841s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0690883\n",
      "\tspeed: 0.1146s/iter; left time: 6753.2053s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0841284\n",
      "\tspeed: 0.1187s/iter; left time: 6985.1423s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0866462\n",
      "\tspeed: 0.1156s/iter; left time: 6790.0288s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0677450\n",
      "\tspeed: 0.1156s/iter; left time: 6780.3658s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0808895\n",
      "\tspeed: 0.1148s/iter; left time: 6723.6332s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0839261\n",
      "\tspeed: 0.1135s/iter; left time: 6633.9557s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0723493\n",
      "\tspeed: 0.1171s/iter; left time: 6835.8888s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0775747\n",
      "\tspeed: 0.1183s/iter; left time: 6893.1300s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0767770\n",
      "\tspeed: 0.1166s/iter; left time: 6778.6296s\n",
      "Epoch: 7 cost time: 00h:08m:40.24s\n",
      "Epoch: 7 | Train Loss: 0.0794193 Vali Loss: 0.0901961 Test Loss: 0.1045509\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0895511\n",
      "\tspeed: 1.6001s/iter; left time: 92783.0760s\n",
      "\titers: 200, epoch: 8 | loss: 0.0866904\n",
      "\tspeed: 0.1176s/iter; left time: 6804.9920s\n",
      "\titers: 300, epoch: 8 | loss: 0.0797954\n",
      "\tspeed: 0.1173s/iter; left time: 6778.7685s\n",
      "\titers: 400, epoch: 8 | loss: 0.0820298\n",
      "\tspeed: 0.1157s/iter; left time: 6671.7858s\n",
      "\titers: 500, epoch: 8 | loss: 0.0783670\n",
      "\tspeed: 0.1153s/iter; left time: 6637.8132s\n",
      "\titers: 600, epoch: 8 | loss: 0.0902877\n",
      "\tspeed: 0.1160s/iter; left time: 6667.4776s\n",
      "\titers: 700, epoch: 8 | loss: 0.0648628\n",
      "\tspeed: 0.1193s/iter; left time: 6846.3671s\n",
      "\titers: 800, epoch: 8 | loss: 0.0814682\n",
      "\tspeed: 0.1170s/iter; left time: 6705.1745s\n",
      "\titers: 900, epoch: 8 | loss: 0.0808761\n",
      "\tspeed: 0.1179s/iter; left time: 6740.2376s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0752130\n",
      "\tspeed: 0.1156s/iter; left time: 6597.0368s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0748815\n",
      "\tspeed: 0.1141s/iter; left time: 6500.1897s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0795547\n",
      "\tspeed: 0.1146s/iter; left time: 6521.2980s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0994396\n",
      "\tspeed: 0.1153s/iter; left time: 6548.6293s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0792517\n",
      "\tspeed: 0.1165s/iter; left time: 6602.8411s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0784374\n",
      "\tspeed: 0.1173s/iter; left time: 6636.2352s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0859944\n",
      "\tspeed: 0.1158s/iter; left time: 6538.8571s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0819602\n",
      "\tspeed: 0.1172s/iter; left time: 6609.0135s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0977695\n",
      "\tspeed: 0.1156s/iter; left time: 6505.3635s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0645730\n",
      "\tspeed: 0.1171s/iter; left time: 6578.6172s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0558155\n",
      "\tspeed: 0.1168s/iter; left time: 6549.4902s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0748824\n",
      "\tspeed: 0.1157s/iter; left time: 6478.2495s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0751968\n",
      "\tspeed: 0.1162s/iter; left time: 6494.5979s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0775747\n",
      "\tspeed: 0.1148s/iter; left time: 6404.6842s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0743494\n",
      "\tspeed: 0.1139s/iter; left time: 6342.7048s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0847968\n",
      "\tspeed: 0.1157s/iter; left time: 6433.2662s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0757934\n",
      "\tspeed: 0.1164s/iter; left time: 6456.4094s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0962444\n",
      "\tspeed: 0.1137s/iter; left time: 6297.2839s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0798034\n",
      "\tspeed: 0.1155s/iter; left time: 6386.9256s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0616579\n",
      "\tspeed: 0.1153s/iter; left time: 6360.0942s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0749053\n",
      "\tspeed: 0.1160s/iter; left time: 6388.2991s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0779971\n",
      "\tspeed: 0.1161s/iter; left time: 6384.3218s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0703546\n",
      "\tspeed: 0.1156s/iter; left time: 6344.9051s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0924814\n",
      "\tspeed: 0.1184s/iter; left time: 6485.7043s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0760914\n",
      "\tspeed: 0.1151s/iter; left time: 6293.5055s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0540513\n",
      "\tspeed: 0.1156s/iter; left time: 6309.0888s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0791183\n",
      "\tspeed: 0.1166s/iter; left time: 6350.9989s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0715009\n",
      "\tspeed: 0.1170s/iter; left time: 6365.5592s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0779355\n",
      "\tspeed: 0.1161s/iter; left time: 6304.4201s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0706417\n",
      "\tspeed: 0.1178s/iter; left time: 6382.4899s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0706924\n",
      "\tspeed: 0.1169s/iter; left time: 6324.4682s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0704471\n",
      "\tspeed: 0.1175s/iter; left time: 6342.1722s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0729290\n",
      "\tspeed: 0.1144s/iter; left time: 6166.8255s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0735274\n",
      "\tspeed: 0.1167s/iter; left time: 6279.3577s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0819775\n",
      "\tspeed: 0.1163s/iter; left time: 6245.3451s\n",
      "Epoch: 8 cost time: 00h:08m:39.75s\n",
      "Epoch: 8 | Train Loss: 0.0784829 Vali Loss: 0.0906655 Test Loss: 0.1073768\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0807247\n",
      "\tspeed: 1.5920s/iter; left time: 85200.9096s\n",
      "\titers: 200, epoch: 9 | loss: 0.0876306\n",
      "\tspeed: 0.1159s/iter; left time: 6188.4256s\n",
      "\titers: 300, epoch: 9 | loss: 0.0690702\n",
      "\tspeed: 0.1141s/iter; left time: 6083.2244s\n",
      "\titers: 400, epoch: 9 | loss: 0.0685977\n",
      "\tspeed: 0.1155s/iter; left time: 6144.9507s\n",
      "\titers: 500, epoch: 9 | loss: 0.1049898\n",
      "\tspeed: 0.1168s/iter; left time: 6202.2266s\n",
      "\titers: 600, epoch: 9 | loss: 0.0862811\n",
      "\tspeed: 0.1132s/iter; left time: 5999.1441s\n",
      "\titers: 700, epoch: 9 | loss: 0.0700331\n",
      "\tspeed: 0.1152s/iter; left time: 6093.7406s\n",
      "\titers: 800, epoch: 9 | loss: 0.0694490\n",
      "\tspeed: 0.1144s/iter; left time: 6041.9345s\n",
      "\titers: 900, epoch: 9 | loss: 0.0769776\n",
      "\tspeed: 0.1146s/iter; left time: 6043.7254s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0690558\n",
      "\tspeed: 0.1141s/iter; left time: 6005.1500s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0718805\n",
      "\tspeed: 0.1166s/iter; left time: 6125.6806s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0790950\n",
      "\tspeed: 0.1166s/iter; left time: 6113.3885s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0878323\n",
      "\tspeed: 0.1139s/iter; left time: 5957.3476s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0601362\n",
      "\tspeed: 0.1162s/iter; left time: 6066.5406s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0813463\n",
      "\tspeed: 0.1156s/iter; left time: 6026.6967s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0808004\n",
      "\tspeed: 0.1173s/iter; left time: 6101.3994s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0846742\n",
      "\tspeed: 0.1160s/iter; left time: 6021.5311s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0694577\n",
      "\tspeed: 0.1165s/iter; left time: 6037.4243s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0717922\n",
      "\tspeed: 0.1162s/iter; left time: 6010.5074s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0688320\n",
      "\tspeed: 0.1155s/iter; left time: 5960.9380s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0924541\n",
      "\tspeed: 0.1158s/iter; left time: 5965.4880s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0821416\n",
      "\tspeed: 0.1134s/iter; left time: 5831.2938s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0760271\n",
      "\tspeed: 0.1136s/iter; left time: 5831.3681s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0767931\n",
      "\tspeed: 0.1146s/iter; left time: 5867.6329s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0697705\n",
      "\tspeed: 0.1166s/iter; left time: 5959.7000s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0924100\n",
      "\tspeed: 0.1174s/iter; left time: 5988.3751s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0637605\n",
      "\tspeed: 0.1151s/iter; left time: 5862.7900s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0797227\n",
      "\tspeed: 0.1144s/iter; left time: 5814.0759s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0813500\n",
      "\tspeed: 0.1136s/iter; left time: 5762.7865s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0780612\n",
      "\tspeed: 0.1135s/iter; left time: 5746.4798s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0720700\n",
      "\tspeed: 0.1135s/iter; left time: 5735.5167s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0765664\n",
      "\tspeed: 0.1161s/iter; left time: 5855.5431s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0852109\n",
      "\tspeed: 0.1160s/iter; left time: 5839.0464s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0872862\n",
      "\tspeed: 0.1130s/iter; left time: 5674.4628s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0673751\n",
      "\tspeed: 0.1135s/iter; left time: 5689.5537s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0731839\n",
      "\tspeed: 0.1136s/iter; left time: 5680.6715s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0864574\n",
      "\tspeed: 0.1145s/iter; left time: 5713.9607s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0817628\n",
      "\tspeed: 0.1132s/iter; left time: 5638.7590s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0986147\n",
      "\tspeed: 0.1148s/iter; left time: 5706.3174s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0703890\n",
      "\tspeed: 0.1135s/iter; left time: 5632.6065s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0744173\n",
      "\tspeed: 0.1171s/iter; left time: 5796.8049s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0785395\n",
      "\tspeed: 0.1150s/iter; left time: 5681.7597s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0744364\n",
      "\tspeed: 0.1147s/iter; left time: 5655.6415s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0595877\n",
      "\tspeed: 0.1140s/iter; left time: 5610.7776s\n",
      "Epoch: 9 cost time: 00h:08m:34.31s\n",
      "Epoch: 9 | Train Loss: 0.0773867 Vali Loss: 0.0910278 Test Loss: 0.1069496\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0671151\n",
      "\tspeed: 1.5785s/iter; left time: 77423.5353s\n",
      "\titers: 200, epoch: 10 | loss: 0.0828441\n",
      "\tspeed: 0.1145s/iter; left time: 5605.6839s\n",
      "\titers: 300, epoch: 10 | loss: 0.0783897\n",
      "\tspeed: 0.1136s/iter; left time: 5546.8106s\n",
      "\titers: 400, epoch: 10 | loss: 0.0834453\n",
      "\tspeed: 0.1153s/iter; left time: 5620.6085s\n",
      "\titers: 500, epoch: 10 | loss: 0.0739451\n",
      "\tspeed: 0.1143s/iter; left time: 5561.1659s\n",
      "\titers: 600, epoch: 10 | loss: 0.0635547\n",
      "\tspeed: 0.1166s/iter; left time: 5660.8866s\n",
      "\titers: 700, epoch: 10 | loss: 0.0789368\n",
      "\tspeed: 0.1144s/iter; left time: 5543.8684s\n",
      "\titers: 800, epoch: 10 | loss: 0.0980061\n",
      "\tspeed: 0.1155s/iter; left time: 5582.6527s\n",
      "\titers: 900, epoch: 10 | loss: 0.0827744\n",
      "\tspeed: 0.1135s/iter; left time: 5478.4677s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0858516\n",
      "\tspeed: 0.1160s/iter; left time: 5586.9409s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0609591\n",
      "\tspeed: 0.1159s/iter; left time: 5570.3575s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0810569\n",
      "\tspeed: 0.1169s/iter; left time: 5603.0406s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0937729\n",
      "\tspeed: 0.1171s/iter; left time: 5602.3511s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0835219\n",
      "\tspeed: 0.1161s/iter; left time: 5545.6390s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0743610\n",
      "\tspeed: 0.1172s/iter; left time: 5584.3104s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0712053\n",
      "\tspeed: 0.1170s/iter; left time: 5562.3177s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0691880\n",
      "\tspeed: 0.1129s/iter; left time: 5357.6994s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0734778\n",
      "\tspeed: 0.1141s/iter; left time: 5404.1841s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0740059\n",
      "\tspeed: 0.1151s/iter; left time: 5439.5805s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0803476\n",
      "\tspeed: 0.1189s/iter; left time: 5607.9472s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0573778\n",
      "\tspeed: 0.1133s/iter; left time: 5331.1378s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633182\n",
      "\tspeed: 0.1135s/iter; left time: 5330.4922s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0607398\n",
      "\tspeed: 0.1139s/iter; left time: 5334.9009s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0995448\n",
      "\tspeed: 0.1147s/iter; left time: 5363.4710s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0664814\n",
      "\tspeed: 0.1141s/iter; left time: 5324.1224s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0725956\n",
      "\tspeed: 0.1151s/iter; left time: 5357.9144s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0891252\n",
      "\tspeed: 0.1147s/iter; left time: 5329.2590s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0708456\n",
      "\tspeed: 0.1129s/iter; left time: 5231.2750s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0679720\n",
      "\tspeed: 0.1145s/iter; left time: 5294.0154s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0656617\n",
      "\tspeed: 0.1147s/iter; left time: 5293.4144s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0661887\n",
      "\tspeed: 0.1137s/iter; left time: 5235.6263s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0758295\n",
      "\tspeed: 0.1158s/iter; left time: 5322.2497s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0698776\n",
      "\tspeed: 0.1148s/iter; left time: 5263.9330s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0789215\n",
      "\tspeed: 0.1148s/iter; left time: 5251.9707s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0749588\n",
      "\tspeed: 0.1140s/iter; left time: 5202.1299s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1019792\n",
      "\tspeed: 0.1160s/iter; left time: 5285.0743s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0910538\n",
      "\tspeed: 0.1161s/iter; left time: 5275.2146s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0882741\n",
      "\tspeed: 0.1137s/iter; left time: 5154.9439s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0684295\n",
      "\tspeed: 0.1169s/iter; left time: 5291.0654s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0834578\n",
      "\tspeed: 0.1151s/iter; left time: 5197.8577s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0795520\n",
      "\tspeed: 0.1142s/iter; left time: 5146.1165s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0775932\n",
      "\tspeed: 0.1176s/iter; left time: 5286.7117s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0749520\n",
      "\tspeed: 0.1152s/iter; left time: 5168.5459s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0773880\n",
      "\tspeed: 0.1140s/iter; left time: 5103.3444s\n",
      "Epoch: 10 cost time: 00h:08m:34.73s\n",
      "Epoch: 10 | Train Loss: 0.0764753 Vali Loss: 0.0915059 Test Loss: 0.1088750\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.025583267211914062, rmse:0.1599477082490921, mae:0.10403311252593994, rse:0.5514940619468689\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 24: 01h:52m:46.62s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-03 07:57:52,023] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 07:57:53,303] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 07:57:53,303] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 07:57:53,303] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 07:57:53,403] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 07:57:53,403] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 07:57:54,256] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 07:57:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 07:57:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 07:57:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 07:57:54,259] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 07:57:54,259] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 07:57:54,259] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 07:57:54,707] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 07:57:54,708] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:54,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.75 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:54,878] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 07:57:54,879] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:54,879] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.77 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:54,879] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 07:57:55,059] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 07:57:55,060] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 07:57:55,060] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 152.72 GB, percent = 20.2%\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 07:57:55,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 07:57:55,062] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a74e9add0>\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 07:57:55,063] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 07:57:55,064] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 07:57:55,065] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688266\n",
      "\tspeed: 0.1785s/iter; left time: 15893.5365s\n",
      "\titers: 200, epoch: 1 | loss: 0.1466758\n",
      "\tspeed: 0.1255s/iter; left time: 11163.3362s\n",
      "\titers: 300, epoch: 1 | loss: 0.1657802\n",
      "\tspeed: 0.1288s/iter; left time: 11443.9814s\n",
      "\titers: 400, epoch: 1 | loss: 0.1365485\n",
      "\tspeed: 0.1305s/iter; left time: 11583.9117s\n",
      "\titers: 500, epoch: 1 | loss: 0.1343239\n",
      "\tspeed: 0.1286s/iter; left time: 11396.9528s\n",
      "\titers: 600, epoch: 1 | loss: 0.1273262\n",
      "\tspeed: 0.1305s/iter; left time: 11555.1903s\n",
      "\titers: 700, epoch: 1 | loss: 0.1142251\n",
      "\tspeed: 0.1300s/iter; left time: 11495.5934s\n",
      "\titers: 800, epoch: 1 | loss: 0.1129106\n",
      "\tspeed: 0.1282s/iter; left time: 11327.4622s\n",
      "\titers: 900, epoch: 1 | loss: 0.1081580\n",
      "\tspeed: 0.1301s/iter; left time: 11483.9961s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1082976\n",
      "\tspeed: 0.1288s/iter; left time: 11355.2133s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1104180\n",
      "\tspeed: 0.1282s/iter; left time: 11289.5287s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1293156\n",
      "\tspeed: 0.1285s/iter; left time: 11304.2771s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1199676\n",
      "\tspeed: 0.1286s/iter; left time: 11298.3167s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0853492\n",
      "\tspeed: 0.1277s/iter; left time: 11204.6884s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1185155\n",
      "\tspeed: 0.1289s/iter; left time: 11294.9506s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1091379\n",
      "\tspeed: 0.1280s/iter; left time: 11201.8787s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1387339\n",
      "\tspeed: 0.1293s/iter; left time: 11306.8202s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1115096\n",
      "\tspeed: 0.1266s/iter; left time: 11058.3400s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1182949\n",
      "\tspeed: 0.1303s/iter; left time: 11364.1194s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1110286\n",
      "\tspeed: 0.1287s/iter; left time: 11218.9095s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1118804\n",
      "\tspeed: 0.1277s/iter; left time: 11113.0769s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1165919\n",
      "\tspeed: 0.1274s/iter; left time: 11074.5511s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1117706\n",
      "\tspeed: 0.1301s/iter; left time: 11299.2092s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1129438\n",
      "\tspeed: 0.1294s/iter; left time: 11222.3812s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1135126\n",
      "\tspeed: 0.1281s/iter; left time: 11096.9533s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1112166\n",
      "\tspeed: 0.1286s/iter; left time: 11128.1423s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1203404\n",
      "\tspeed: 0.1266s/iter; left time: 10943.4566s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1182374\n",
      "\tspeed: 0.1295s/iter; left time: 11184.3534s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1069434\n",
      "\tspeed: 0.1288s/iter; left time: 11103.8088s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1128676\n",
      "\tspeed: 0.1270s/iter; left time: 10942.3124s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1112418\n",
      "\tspeed: 0.1259s/iter; left time: 10836.4119s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1109535\n",
      "\tspeed: 0.1280s/iter; left time: 11004.6363s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1022410\n",
      "\tspeed: 0.1280s/iter; left time: 10991.1581s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1067109\n",
      "\tspeed: 0.1288s/iter; left time: 11039.7652s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1093592\n",
      "\tspeed: 0.1301s/iter; left time: 11142.3125s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1050644\n",
      "\tspeed: 0.1300s/iter; left time: 11120.9045s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1453399\n",
      "\tspeed: 0.1292s/iter; left time: 11037.9253s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1064153\n",
      "\tspeed: 0.1306s/iter; left time: 11142.7100s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1275871\n",
      "\tspeed: 0.1284s/iter; left time: 10940.9372s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1056856\n",
      "\tspeed: 0.1266s/iter; left time: 10782.8795s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1163018\n",
      "\tspeed: 0.1286s/iter; left time: 10939.7400s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1195716\n",
      "\tspeed: 0.1284s/iter; left time: 10907.8794s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1011549\n",
      "\tspeed: 0.1282s/iter; left time: 10878.4997s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1301714\n",
      "\tspeed: 0.1273s/iter; left time: 10789.5248s\n",
      "Epoch: 1 cost time: 00h:09m:34.14s\n",
      "Epoch: 1 | Train Loss: 0.1159376 Vali Loss: 0.1171800 Test Loss: 0.1386980\n",
      "Validation loss decreased (inf --> 0.117180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1323460\n",
      "\tspeed: 1.8177s/iter; left time: 153750.1873s\n",
      "\titers: 200, epoch: 2 | loss: 0.0998212\n",
      "\tspeed: 0.1151s/iter; left time: 9726.5930s\n",
      "\titers: 300, epoch: 2 | loss: 0.1072860\n",
      "\tspeed: 0.1144s/iter; left time: 9650.6818s\n",
      "\titers: 400, epoch: 2 | loss: 0.1047579\n",
      "\tspeed: 0.1165s/iter; left time: 9817.5607s\n",
      "\titers: 500, epoch: 2 | loss: 0.0988569\n",
      "\tspeed: 0.1131s/iter; left time: 9518.1567s\n",
      "\titers: 600, epoch: 2 | loss: 0.1400726\n",
      "\tspeed: 0.1153s/iter; left time: 9696.2221s\n",
      "\titers: 700, epoch: 2 | loss: 0.1279359\n",
      "\tspeed: 0.1146s/iter; left time: 9621.8230s\n",
      "\titers: 800, epoch: 2 | loss: 0.1120330\n",
      "\tspeed: 0.1140s/iter; left time: 9561.5629s\n",
      "\titers: 900, epoch: 2 | loss: 0.1051929\n",
      "\tspeed: 0.1129s/iter; left time: 9458.3001s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1209462\n",
      "\tspeed: 0.1157s/iter; left time: 9681.0412s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1275426\n",
      "\tspeed: 0.1137s/iter; left time: 9505.8381s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1159983\n",
      "\tspeed: 0.1157s/iter; left time: 9656.7284s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1053278\n",
      "\tspeed: 0.1147s/iter; left time: 9564.5253s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0974504\n",
      "\tspeed: 0.1146s/iter; left time: 9545.8152s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0961491\n",
      "\tspeed: 0.1162s/iter; left time: 9668.4279s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1127011\n",
      "\tspeed: 0.1150s/iter; left time: 9551.3628s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1136763\n",
      "\tspeed: 0.1160s/iter; left time: 9629.7080s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1032368\n",
      "\tspeed: 0.1138s/iter; left time: 9430.6147s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0907084\n",
      "\tspeed: 0.1128s/iter; left time: 9339.1341s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1084257\n",
      "\tspeed: 0.1156s/iter; left time: 9560.5969s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1318485\n",
      "\tspeed: 0.1146s/iter; left time: 9465.4027s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1092268\n",
      "\tspeed: 0.1150s/iter; left time: 9482.2905s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0971450\n",
      "\tspeed: 0.1159s/iter; left time: 9546.3702s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1082254\n",
      "\tspeed: 0.1141s/iter; left time: 9386.9575s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1057774\n",
      "\tspeed: 0.1130s/iter; left time: 9288.2027s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1233867\n",
      "\tspeed: 0.1132s/iter; left time: 9294.7662s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1121541\n",
      "\tspeed: 0.1137s/iter; left time: 9322.9370s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1023491\n",
      "\tspeed: 0.1140s/iter; left time: 9334.0303s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1325897\n",
      "\tspeed: 0.1140s/iter; left time: 9320.0574s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0932151\n",
      "\tspeed: 0.1154s/iter; left time: 9425.6305s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1183982\n",
      "\tspeed: 0.1155s/iter; left time: 9426.6047s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1071068\n",
      "\tspeed: 0.1148s/iter; left time: 9355.3898s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0916324\n",
      "\tspeed: 0.1159s/iter; left time: 9432.0369s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0934265\n",
      "\tspeed: 0.1107s/iter; left time: 8998.8114s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1121869\n",
      "\tspeed: 0.1131s/iter; left time: 9182.5247s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1178536\n",
      "\tspeed: 0.1135s/iter; left time: 9204.1838s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1048463\n",
      "\tspeed: 0.1135s/iter; left time: 9189.4847s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1119690\n",
      "\tspeed: 0.1135s/iter; left time: 9180.8183s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1302823\n",
      "\tspeed: 0.1146s/iter; left time: 9255.9061s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1027673\n",
      "\tspeed: 0.1133s/iter; left time: 9141.1086s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1058483\n",
      "\tspeed: 0.1124s/iter; left time: 9057.8689s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1003960\n",
      "\tspeed: 0.1139s/iter; left time: 9169.8420s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0997679\n",
      "\tspeed: 0.1143s/iter; left time: 9190.6403s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1080339\n",
      "\tspeed: 0.1127s/iter; left time: 9048.9508s\n",
      "Epoch: 2 cost time: 00h:08m:30.12s\n",
      "Epoch: 2 | Train Loss: 0.1063434 Vali Loss: 0.1159233 Test Loss: 0.1404574\n",
      "Validation loss decreased (0.117180 --> 0.115923).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0953313\n",
      "\tspeed: 1.5298s/iter; left time: 122579.7741s\n",
      "\titers: 200, epoch: 3 | loss: 0.0960641\n",
      "\tspeed: 0.1129s/iter; left time: 9037.5760s\n",
      "\titers: 300, epoch: 3 | loss: 0.1168496\n",
      "\tspeed: 0.1128s/iter; left time: 9018.4111s\n",
      "\titers: 400, epoch: 3 | loss: 0.1128009\n",
      "\tspeed: 0.1139s/iter; left time: 9091.4012s\n",
      "\titers: 500, epoch: 3 | loss: 0.1003084\n",
      "\tspeed: 0.1134s/iter; left time: 9037.9726s\n",
      "\titers: 600, epoch: 3 | loss: 0.1153115\n",
      "\tspeed: 0.1150s/iter; left time: 9153.9593s\n",
      "\titers: 700, epoch: 3 | loss: 0.1167073\n",
      "\tspeed: 0.1128s/iter; left time: 8969.6655s\n",
      "\titers: 800, epoch: 3 | loss: 0.1098146\n",
      "\tspeed: 0.1131s/iter; left time: 8983.1531s\n",
      "\titers: 900, epoch: 3 | loss: 0.0888241\n",
      "\tspeed: 0.1131s/iter; left time: 8970.7544s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963814\n",
      "\tspeed: 0.1139s/iter; left time: 9022.4195s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1247230\n",
      "\tspeed: 0.1133s/iter; left time: 8968.2897s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1112344\n",
      "\tspeed: 0.1125s/iter; left time: 8891.8323s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1090816\n",
      "\tspeed: 0.1131s/iter; left time: 8923.5137s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1014220\n",
      "\tspeed: 0.1129s/iter; left time: 8897.4957s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1205137\n",
      "\tspeed: 0.1142s/iter; left time: 8989.3578s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0969793\n",
      "\tspeed: 0.1143s/iter; left time: 8985.5839s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1139035\n",
      "\tspeed: 0.1153s/iter; left time: 9057.3622s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1128580\n",
      "\tspeed: 0.1135s/iter; left time: 8903.3702s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1043471\n",
      "\tspeed: 0.1139s/iter; left time: 8917.9254s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0987270\n",
      "\tspeed: 0.1139s/iter; left time: 8912.4455s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0881278\n",
      "\tspeed: 0.1126s/iter; left time: 8796.8159s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1081288\n",
      "\tspeed: 0.1120s/iter; left time: 8738.4418s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976076\n",
      "\tspeed: 0.1126s/iter; left time: 8772.7511s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0961220\n",
      "\tspeed: 0.1128s/iter; left time: 8775.3532s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0965293\n",
      "\tspeed: 0.1127s/iter; left time: 8760.7689s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1095369\n",
      "\tspeed: 0.1120s/iter; left time: 8697.7938s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1212904\n",
      "\tspeed: 0.1144s/iter; left time: 8865.4812s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1003948\n",
      "\tspeed: 0.1133s/iter; left time: 8775.0465s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1081474\n",
      "\tspeed: 0.1133s/iter; left time: 8761.8701s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1015081\n",
      "\tspeed: 0.1129s/iter; left time: 8715.9515s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0989254\n",
      "\tspeed: 0.1135s/iter; left time: 8754.3035s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1039556\n",
      "\tspeed: 0.1128s/iter; left time: 8692.0616s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0903572\n",
      "\tspeed: 0.1132s/iter; left time: 8709.2199s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0759418\n",
      "\tspeed: 0.1130s/iter; left time: 8682.8255s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1176762\n",
      "\tspeed: 0.1120s/iter; left time: 8595.1412s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0941180\n",
      "\tspeed: 0.1109s/iter; left time: 8494.2782s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1044800\n",
      "\tspeed: 0.1118s/iter; left time: 8553.7069s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1119280\n",
      "\tspeed: 0.1118s/iter; left time: 8543.0616s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1174034\n",
      "\tspeed: 0.1113s/iter; left time: 8495.1171s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0906112\n",
      "\tspeed: 0.1126s/iter; left time: 8584.3571s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1110326\n",
      "\tspeed: 0.1132s/iter; left time: 8617.3934s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1211934\n",
      "\tspeed: 0.1132s/iter; left time: 8603.3143s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1134209\n",
      "\tspeed: 0.1138s/iter; left time: 8643.2430s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0881434\n",
      "\tspeed: 0.1132s/iter; left time: 8582.5215s\n",
      "Epoch: 3 cost time: 00h:08m:24.63s\n",
      "Epoch: 3 | Train Loss: 0.1022726 Vali Loss: 0.1180037 Test Loss: 0.1440704\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0873546\n",
      "\tspeed: 1.5100s/iter; left time: 114263.3071s\n",
      "\titers: 200, epoch: 4 | loss: 0.1036950\n",
      "\tspeed: 0.1139s/iter; left time: 8609.2430s\n",
      "\titers: 300, epoch: 4 | loss: 0.0934119\n",
      "\tspeed: 0.1139s/iter; left time: 8593.5786s\n",
      "\titers: 400, epoch: 4 | loss: 0.1056632\n",
      "\tspeed: 0.1150s/iter; left time: 8664.7593s\n",
      "\titers: 500, epoch: 4 | loss: 0.0883508\n",
      "\tspeed: 0.1135s/iter; left time: 8540.5182s\n",
      "\titers: 600, epoch: 4 | loss: 0.0899846\n",
      "\tspeed: 0.1128s/iter; left time: 8478.9874s\n",
      "\titers: 700, epoch: 4 | loss: 0.0998617\n",
      "\tspeed: 0.1132s/iter; left time: 8494.6099s\n",
      "\titers: 800, epoch: 4 | loss: 0.0895698\n",
      "\tspeed: 0.1122s/iter; left time: 8408.1319s\n",
      "\titers: 900, epoch: 4 | loss: 0.1108057\n",
      "\tspeed: 0.1133s/iter; left time: 8481.2006s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1005503\n",
      "\tspeed: 0.1139s/iter; left time: 8514.6839s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0851609\n",
      "\tspeed: 0.1138s/iter; left time: 8497.9852s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0922965\n",
      "\tspeed: 0.1133s/iter; left time: 8452.3653s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0940839\n",
      "\tspeed: 0.1130s/iter; left time: 8413.9763s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0969528\n",
      "\tspeed: 0.1134s/iter; left time: 8436.9304s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0909017\n",
      "\tspeed: 0.1125s/iter; left time: 8358.9278s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0886021\n",
      "\tspeed: 0.1141s/iter; left time: 8463.9709s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0961353\n",
      "\tspeed: 0.1132s/iter; left time: 8384.6226s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0882971\n",
      "\tspeed: 0.1139s/iter; left time: 8427.3077s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0977799\n",
      "\tspeed: 0.1143s/iter; left time: 8446.6530s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0880991\n",
      "\tspeed: 0.1126s/iter; left time: 8309.3337s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0910579\n",
      "\tspeed: 0.1132s/iter; left time: 8340.1650s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0977751\n",
      "\tspeed: 0.1129s/iter; left time: 8305.7125s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0931557\n",
      "\tspeed: 0.1141s/iter; left time: 8380.7850s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1089414\n",
      "\tspeed: 0.1128s/iter; left time: 8275.7434s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1060289\n",
      "\tspeed: 0.1136s/iter; left time: 8326.6336s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0834869\n",
      "\tspeed: 0.1109s/iter; left time: 8113.2451s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1122872\n",
      "\tspeed: 0.1139s/iter; left time: 8325.9781s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0878354\n",
      "\tspeed: 0.1136s/iter; left time: 8292.4274s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0907886\n",
      "\tspeed: 0.1146s/iter; left time: 8353.3154s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0977618\n",
      "\tspeed: 0.1131s/iter; left time: 8230.6683s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0923466\n",
      "\tspeed: 0.1129s/iter; left time: 8204.9562s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0991563\n",
      "\tspeed: 0.1173s/iter; left time: 8508.9227s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1049987\n",
      "\tspeed: 0.1162s/iter; left time: 8419.4591s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0708893\n",
      "\tspeed: 0.1140s/iter; left time: 8252.0366s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0792143\n",
      "\tspeed: 0.1137s/iter; left time: 8215.6924s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0895646\n",
      "\tspeed: 0.1126s/iter; left time: 8129.8788s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1049187\n",
      "\tspeed: 0.1127s/iter; left time: 8124.3975s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0842904\n",
      "\tspeed: 0.1129s/iter; left time: 8126.8559s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0941178\n",
      "\tspeed: 0.1108s/iter; left time: 7961.7936s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1109897\n",
      "\tspeed: 0.1139s/iter; left time: 8177.1761s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1001586\n",
      "\tspeed: 0.1125s/iter; left time: 8065.4055s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1093832\n",
      "\tspeed: 0.1127s/iter; left time: 8068.1410s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0876529\n",
      "\tspeed: 0.1123s/iter; left time: 8028.4879s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952323\n",
      "\tspeed: 0.1132s/iter; left time: 8081.8479s\n",
      "Epoch: 4 cost time: 00h:08m:25.73s\n",
      "Epoch: 4 | Train Loss: 0.0973941 Vali Loss: 0.1181420 Test Loss: 0.1481285\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0899206\n",
      "\tspeed: 1.5235s/iter; left time: 108491.2324s\n",
      "\titers: 200, epoch: 5 | loss: 0.1038604\n",
      "\tspeed: 0.1154s/iter; left time: 8205.3218s\n",
      "\titers: 300, epoch: 5 | loss: 0.1103195\n",
      "\tspeed: 0.1156s/iter; left time: 8207.7192s\n",
      "\titers: 400, epoch: 5 | loss: 0.0988987\n",
      "\tspeed: 0.1153s/iter; left time: 8173.8791s\n",
      "\titers: 500, epoch: 5 | loss: 0.1051342\n",
      "\tspeed: 0.1136s/iter; left time: 8046.3911s\n",
      "\titers: 600, epoch: 5 | loss: 0.0994484\n",
      "\tspeed: 0.1174s/iter; left time: 8298.3677s\n",
      "\titers: 700, epoch: 5 | loss: 0.0807362\n",
      "\tspeed: 0.1152s/iter; left time: 8132.8384s\n",
      "\titers: 800, epoch: 5 | loss: 0.1006454\n",
      "\tspeed: 0.1152s/iter; left time: 8125.9078s\n",
      "\titers: 900, epoch: 5 | loss: 0.1117228\n",
      "\tspeed: 0.1140s/iter; left time: 8025.7115s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0958334\n",
      "\tspeed: 0.1154s/iter; left time: 8114.8124s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0945488\n",
      "\tspeed: 0.1149s/iter; left time: 8068.0753s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0999299\n",
      "\tspeed: 0.1168s/iter; left time: 8190.4201s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0833874\n",
      "\tspeed: 0.1151s/iter; left time: 8061.9571s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0800731\n",
      "\tspeed: 0.1166s/iter; left time: 8150.3149s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0873000\n",
      "\tspeed: 0.1170s/iter; left time: 8169.8289s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0974982\n",
      "\tspeed: 0.1148s/iter; left time: 8005.9197s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0832424\n",
      "\tspeed: 0.1148s/iter; left time: 7990.4724s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0964874\n",
      "\tspeed: 0.1142s/iter; left time: 7938.4620s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1015488\n",
      "\tspeed: 0.1148s/iter; left time: 7971.9532s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0962706\n",
      "\tspeed: 0.1139s/iter; left time: 7893.5208s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0944651\n",
      "\tspeed: 0.1158s/iter; left time: 8014.9157s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0883529\n",
      "\tspeed: 0.1142s/iter; left time: 7892.4674s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0954471\n",
      "\tspeed: 0.1152s/iter; left time: 7952.3328s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0827175\n",
      "\tspeed: 0.1146s/iter; left time: 7899.6119s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1006347\n",
      "\tspeed: 0.1144s/iter; left time: 7870.0949s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0951255\n",
      "\tspeed: 0.1165s/iter; left time: 8005.8141s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1033527\n",
      "\tspeed: 0.1175s/iter; left time: 8060.1569s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0756043\n",
      "\tspeed: 0.1147s/iter; left time: 7858.7596s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0850169\n",
      "\tspeed: 0.1152s/iter; left time: 7882.4580s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1090293\n",
      "\tspeed: 0.1144s/iter; left time: 7814.8763s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0975026\n",
      "\tspeed: 0.1145s/iter; left time: 7813.4333s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0954872\n",
      "\tspeed: 0.1149s/iter; left time: 7823.0406s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0834685\n",
      "\tspeed: 0.1166s/iter; left time: 7931.9783s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0841382\n",
      "\tspeed: 0.1164s/iter; left time: 7902.3602s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0745072\n",
      "\tspeed: 0.1160s/iter; left time: 7863.0053s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1053615\n",
      "\tspeed: 0.1157s/iter; left time: 7837.6776s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0791975\n",
      "\tspeed: 0.1165s/iter; left time: 7879.8089s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0998375\n",
      "\tspeed: 0.1157s/iter; left time: 7814.3196s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0949914\n",
      "\tspeed: 0.1157s/iter; left time: 7801.0721s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0968361\n",
      "\tspeed: 0.1149s/iter; left time: 7735.8557s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0942416\n",
      "\tspeed: 0.1143s/iter; left time: 7685.6907s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1008289\n",
      "\tspeed: 0.1138s/iter; left time: 7634.2870s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0779671\n",
      "\tspeed: 0.1151s/iter; left time: 7711.2709s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0973403\n",
      "\tspeed: 0.1155s/iter; left time: 7725.3286s\n",
      "Epoch: 5 cost time: 00h:08m:34.07s\n",
      "Epoch: 5 | Train Loss: 0.0930669 Vali Loss: 0.1177695 Test Loss: 0.1494393\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0950565\n",
      "\tspeed: 1.5478s/iter; left time: 103326.1281s\n",
      "\titers: 200, epoch: 6 | loss: 0.0880194\n",
      "\tspeed: 0.1140s/iter; left time: 7598.5819s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845609\n",
      "\tspeed: 0.1146s/iter; left time: 7628.5804s\n",
      "\titers: 400, epoch: 6 | loss: 0.0856797\n",
      "\tspeed: 0.1171s/iter; left time: 7784.0110s\n",
      "\titers: 500, epoch: 6 | loss: 0.0811084\n",
      "\tspeed: 0.1152s/iter; left time: 7641.6682s\n",
      "\titers: 600, epoch: 6 | loss: 0.1066922\n",
      "\tspeed: 0.1153s/iter; left time: 7640.4823s\n",
      "\titers: 700, epoch: 6 | loss: 0.0946200\n",
      "\tspeed: 0.1154s/iter; left time: 7632.8492s\n",
      "\titers: 800, epoch: 6 | loss: 0.1000760\n",
      "\tspeed: 0.1148s/iter; left time: 7581.1448s\n",
      "\titers: 900, epoch: 6 | loss: 0.0695545\n",
      "\tspeed: 0.1170s/iter; left time: 7717.6166s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1009072\n",
      "\tspeed: 0.1155s/iter; left time: 7606.8553s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0820811\n",
      "\tspeed: 0.1148s/iter; left time: 7547.9258s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1012075\n",
      "\tspeed: 0.1142s/iter; left time: 7495.7848s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0863402\n",
      "\tspeed: 0.1158s/iter; left time: 7588.2879s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0826545\n",
      "\tspeed: 0.1159s/iter; left time: 7587.4863s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0777465\n",
      "\tspeed: 0.1156s/iter; left time: 7557.8843s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1057802\n",
      "\tspeed: 0.1166s/iter; left time: 7611.9507s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0942861\n",
      "\tspeed: 0.1162s/iter; left time: 7569.9625s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0920153\n",
      "\tspeed: 0.1164s/iter; left time: 7574.8956s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0782236\n",
      "\tspeed: 0.1161s/iter; left time: 7542.9847s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0907045\n",
      "\tspeed: 0.1116s/iter; left time: 7237.2041s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0759282\n",
      "\tspeed: 0.1137s/iter; left time: 7359.8177s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1017066\n",
      "\tspeed: 0.1148s/iter; left time: 7422.9566s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0679502\n",
      "\tspeed: 0.1181s/iter; left time: 7621.3176s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0776927\n",
      "\tspeed: 0.1167s/iter; left time: 7525.2174s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0888472\n",
      "\tspeed: 0.1147s/iter; left time: 7383.1680s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0768875\n",
      "\tspeed: 0.1166s/iter; left time: 7491.8912s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0782935\n",
      "\tspeed: 0.1167s/iter; left time: 7489.6762s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0903737\n",
      "\tspeed: 0.1143s/iter; left time: 7319.0551s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1004810\n",
      "\tspeed: 0.1151s/iter; left time: 7362.3392s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0948790\n",
      "\tspeed: 0.1163s/iter; left time: 7428.4829s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0886455\n",
      "\tspeed: 0.1128s/iter; left time: 7194.6490s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0854608\n",
      "\tspeed: 0.1158s/iter; left time: 7370.2266s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0769016\n",
      "\tspeed: 0.1162s/iter; left time: 7384.1257s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0817375\n",
      "\tspeed: 0.1164s/iter; left time: 7384.1548s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0888851\n",
      "\tspeed: 0.1152s/iter; left time: 7295.7853s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0727457\n",
      "\tspeed: 0.1156s/iter; left time: 7311.4228s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0903441\n",
      "\tspeed: 0.1161s/iter; left time: 7334.4639s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0950212\n",
      "\tspeed: 0.1164s/iter; left time: 7338.4941s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0870087\n",
      "\tspeed: 0.1164s/iter; left time: 7330.3570s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0871792\n",
      "\tspeed: 0.1141s/iter; left time: 7170.3075s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0932686\n",
      "\tspeed: 0.1145s/iter; left time: 7187.2149s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0874465\n",
      "\tspeed: 0.1179s/iter; left time: 7389.2674s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0889675\n",
      "\tspeed: 0.1146s/iter; left time: 7171.8204s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0926063\n",
      "\tspeed: 0.1141s/iter; left time: 7129.0473s\n",
      "Epoch: 6 cost time: 00h:08m:35.30s\n",
      "Epoch: 6 | Train Loss: 0.0893757 Vali Loss: 0.1196648 Test Loss: 0.1541289\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1048594\n",
      "\tspeed: 1.5670s/iter; left time: 97624.5618s\n",
      "\titers: 200, epoch: 7 | loss: 0.0843566\n",
      "\tspeed: 0.1158s/iter; left time: 7202.4942s\n",
      "\titers: 300, epoch: 7 | loss: 0.0917804\n",
      "\tspeed: 0.1159s/iter; left time: 7197.7076s\n",
      "\titers: 400, epoch: 7 | loss: 0.0795149\n",
      "\tspeed: 0.1163s/iter; left time: 7208.8362s\n",
      "\titers: 500, epoch: 7 | loss: 0.0842911\n",
      "\tspeed: 0.1164s/iter; left time: 7204.4799s\n",
      "\titers: 600, epoch: 7 | loss: 0.0841490\n",
      "\tspeed: 0.1165s/iter; left time: 7197.0453s\n",
      "\titers: 700, epoch: 7 | loss: 0.1009502\n",
      "\tspeed: 0.1176s/iter; left time: 7256.1310s\n",
      "\titers: 800, epoch: 7 | loss: 0.0739675\n",
      "\tspeed: 0.1168s/iter; left time: 7195.5046s\n",
      "\titers: 900, epoch: 7 | loss: 0.0888571\n",
      "\tspeed: 0.1167s/iter; left time: 7176.0577s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0870971\n",
      "\tspeed: 0.1167s/iter; left time: 7162.7047s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0928913\n",
      "\tspeed: 0.1164s/iter; left time: 7136.7999s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0886939\n",
      "\tspeed: 0.1136s/iter; left time: 6953.2413s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0893422\n",
      "\tspeed: 0.1159s/iter; left time: 7082.8468s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0847866\n",
      "\tspeed: 0.1163s/iter; left time: 7092.0799s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0769473\n",
      "\tspeed: 0.1159s/iter; left time: 7060.6891s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0924601\n",
      "\tspeed: 0.1148s/iter; left time: 6981.6211s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0808676\n",
      "\tspeed: 0.1129s/iter; left time: 6850.2870s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0922723\n",
      "\tspeed: 0.1167s/iter; left time: 7071.4805s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0814242\n",
      "\tspeed: 0.1156s/iter; left time: 6992.7185s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0870774\n",
      "\tspeed: 0.1151s/iter; left time: 6950.2598s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0811431\n",
      "\tspeed: 0.1156s/iter; left time: 6970.0987s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0951526\n",
      "\tspeed: 0.1132s/iter; left time: 6812.4758s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0774082\n",
      "\tspeed: 0.1148s/iter; left time: 6898.2062s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0838267\n",
      "\tspeed: 0.1152s/iter; left time: 6909.5983s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0840728\n",
      "\tspeed: 0.1140s/iter; left time: 6825.7495s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0857305\n",
      "\tspeed: 0.1154s/iter; left time: 6900.0571s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0911212\n",
      "\tspeed: 0.1161s/iter; left time: 6931.8133s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0788338\n",
      "\tspeed: 0.1136s/iter; left time: 6770.5995s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0812203\n",
      "\tspeed: 0.1129s/iter; left time: 6717.0057s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0968771\n",
      "\tspeed: 0.1153s/iter; left time: 6846.5521s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0815360\n",
      "\tspeed: 0.1158s/iter; left time: 6866.4748s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0820606\n",
      "\tspeed: 0.1159s/iter; left time: 6862.3245s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0775566\n",
      "\tspeed: 0.1134s/iter; left time: 6703.6955s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1014951\n",
      "\tspeed: 0.1149s/iter; left time: 6777.4195s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0893218\n",
      "\tspeed: 0.1148s/iter; left time: 6759.9584s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0940796\n",
      "\tspeed: 0.1172s/iter; left time: 6888.9346s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0836638\n",
      "\tspeed: 0.1131s/iter; left time: 6636.1693s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1027704\n",
      "\tspeed: 0.1150s/iter; left time: 6736.3065s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0680492\n",
      "\tspeed: 0.1139s/iter; left time: 6663.9134s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1034615\n",
      "\tspeed: 0.1145s/iter; left time: 6683.8426s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0949323\n",
      "\tspeed: 0.1138s/iter; left time: 6632.1384s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0865043\n",
      "\tspeed: 0.1150s/iter; left time: 6693.9101s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0960433\n",
      "\tspeed: 0.1149s/iter; left time: 6677.9859s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0965981\n",
      "\tspeed: 0.1142s/iter; left time: 6624.5261s\n",
      "Epoch: 7 cost time: 00h:08m:34.14s\n",
      "Epoch: 7 | Train Loss: 0.0862290 Vali Loss: 0.1190629 Test Loss: 0.1528910\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04200141876935959, rmse:0.2049424797296524, mae:0.14045733213424683, rse:0.7086868286132812\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 96: 01h:18m:33.66s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-03 09:16:27,227] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 09:16:28,647] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 09:16:28,647] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 09:16:28,648] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 09:16:28,773] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 09:16:28,773] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 09:16:29,576] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 09:16:29,577] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 09:16:29,577] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 09:16:29,579] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 09:16:29,579] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 09:16:29,579] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 09:16:29,579] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 09:16:29,986] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 09:16:29,987] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:29,988] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.5 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,171] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 09:16:30,172] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:30,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.51 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,172] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 09:16:30,348] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 09:16:30,349] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 09:16:30,349] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 161.51 GB, percent = 21.4%\n",
      "[2024-11-03 09:16:30,350] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 09:16:30,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 09:16:30,352] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f80738ae1d0>\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 09:16:30,353] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 09:16:30,354] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 09:16:30,355] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 09:16:30,356] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1482348\n",
      "\tspeed: 0.1806s/iter; left time: 16043.0744s\n",
      "\titers: 200, epoch: 1 | loss: 0.1453139\n",
      "\tspeed: 0.1298s/iter; left time: 11516.6845s\n",
      "\titers: 300, epoch: 1 | loss: 0.1553313\n",
      "\tspeed: 0.1296s/iter; left time: 11486.2147s\n",
      "\titers: 400, epoch: 1 | loss: 0.1554058\n",
      "\tspeed: 0.1298s/iter; left time: 11493.2830s\n",
      "\titers: 500, epoch: 1 | loss: 0.1471658\n",
      "\tspeed: 0.1276s/iter; left time: 11279.4548s\n",
      "\titers: 600, epoch: 1 | loss: 0.1542638\n",
      "\tspeed: 0.1299s/iter; left time: 11473.4496s\n",
      "\titers: 700, epoch: 1 | loss: 0.1336013\n",
      "\tspeed: 0.1302s/iter; left time: 11485.4159s\n",
      "\titers: 800, epoch: 1 | loss: 0.1162374\n",
      "\tspeed: 0.1307s/iter; left time: 11521.5088s\n",
      "\titers: 900, epoch: 1 | loss: 0.1247154\n",
      "\tspeed: 0.1290s/iter; left time: 11358.7186s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1020275\n",
      "\tspeed: 0.1299s/iter; left time: 11422.7040s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1375454\n",
      "\tspeed: 0.1282s/iter; left time: 11259.6940s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1161896\n",
      "\tspeed: 0.1314s/iter; left time: 11523.9214s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1136029\n",
      "\tspeed: 0.1286s/iter; left time: 11267.5807s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1167578\n",
      "\tspeed: 0.1286s/iter; left time: 11253.9748s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1177376\n",
      "\tspeed: 0.1304s/iter; left time: 11401.8118s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1085610\n",
      "\tspeed: 0.1304s/iter; left time: 11385.5133s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1154755\n",
      "\tspeed: 0.1287s/iter; left time: 11223.4444s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0951475\n",
      "\tspeed: 0.1291s/iter; left time: 11248.3587s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1343727\n",
      "\tspeed: 0.1315s/iter; left time: 11440.2881s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1305830\n",
      "\tspeed: 0.1288s/iter; left time: 11194.6836s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1169467\n",
      "\tspeed: 0.1273s/iter; left time: 11050.9776s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1000786\n",
      "\tspeed: 0.1265s/iter; left time: 10966.5075s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0901878\n",
      "\tspeed: 0.1276s/iter; left time: 11055.7600s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1336366\n",
      "\tspeed: 0.1273s/iter; left time: 11016.1030s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1131255\n",
      "\tspeed: 0.1289s/iter; left time: 11142.5916s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1157848\n",
      "\tspeed: 0.1275s/iter; left time: 11003.2673s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1087803\n",
      "\tspeed: 0.1296s/iter; left time: 11171.4330s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1142708\n",
      "\tspeed: 0.1293s/iter; left time: 11135.9459s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1243213\n",
      "\tspeed: 0.1301s/iter; left time: 11193.6051s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0919846\n",
      "\tspeed: 0.1320s/iter; left time: 11342.2288s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1445014\n",
      "\tspeed: 0.1291s/iter; left time: 11081.8479s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0993669\n",
      "\tspeed: 0.1289s/iter; left time: 11046.5269s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1163438\n",
      "\tspeed: 0.1293s/iter; left time: 11069.4295s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1209128\n",
      "\tspeed: 0.1283s/iter; left time: 10975.8670s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0993587\n",
      "\tspeed: 0.1268s/iter; left time: 10831.2122s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1136707\n",
      "\tspeed: 0.1281s/iter; left time: 10929.0588s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1139015\n",
      "\tspeed: 0.1290s/iter; left time: 10996.0999s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1168825\n",
      "\tspeed: 0.1306s/iter; left time: 11112.7088s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059878\n",
      "\tspeed: 0.1280s/iter; left time: 10882.4153s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1127900\n",
      "\tspeed: 0.1280s/iter; left time: 10865.7985s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1130453\n",
      "\tspeed: 0.1272s/iter; left time: 10787.6265s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1120209\n",
      "\tspeed: 0.1293s/iter; left time: 10953.2413s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1165368\n",
      "\tspeed: 0.1280s/iter; left time: 10831.6520s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1169618\n",
      "\tspeed: 0.1292s/iter; left time: 10917.5637s\n",
      "Epoch: 1 cost time: 00h:09m:35.04s\n",
      "Epoch: 1 | Train Loss: 0.1205897 Vali Loss: 0.1209952 Test Loss: 0.1438364\n",
      "Validation loss decreased (inf --> 0.120995).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1197857\n",
      "\tspeed: 1.8114s/iter; left time: 152836.4726s\n",
      "\titers: 200, epoch: 2 | loss: 0.1099122\n",
      "\tspeed: 0.1168s/iter; left time: 9845.0516s\n",
      "\titers: 300, epoch: 2 | loss: 0.1050249\n",
      "\tspeed: 0.1172s/iter; left time: 9866.8842s\n",
      "\titers: 400, epoch: 2 | loss: 0.1127598\n",
      "\tspeed: 0.1170s/iter; left time: 9837.4411s\n",
      "\titers: 500, epoch: 2 | loss: 0.0847004\n",
      "\tspeed: 0.1180s/iter; left time: 9909.6726s\n",
      "\titers: 600, epoch: 2 | loss: 0.1266375\n",
      "\tspeed: 0.1166s/iter; left time: 9776.1746s\n",
      "\titers: 700, epoch: 2 | loss: 0.1209411\n",
      "\tspeed: 0.1175s/iter; left time: 9841.5933s\n",
      "\titers: 800, epoch: 2 | loss: 0.1068922\n",
      "\tspeed: 0.1167s/iter; left time: 9763.5481s\n",
      "\titers: 900, epoch: 2 | loss: 0.0993572\n",
      "\tspeed: 0.1166s/iter; left time: 9743.6605s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1088792\n",
      "\tspeed: 0.1193s/iter; left time: 9954.9116s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1103762\n",
      "\tspeed: 0.1184s/iter; left time: 9871.9279s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1110987\n",
      "\tspeed: 0.1178s/iter; left time: 9809.1376s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0816215\n",
      "\tspeed: 0.1163s/iter; left time: 9669.8693s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1149543\n",
      "\tspeed: 0.1170s/iter; left time: 9723.7857s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1127167\n",
      "\tspeed: 0.1155s/iter; left time: 9583.8694s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1143629\n",
      "\tspeed: 0.1171s/iter; left time: 9706.3140s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1134126\n",
      "\tspeed: 0.1158s/iter; left time: 9581.8043s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1078830\n",
      "\tspeed: 0.1166s/iter; left time: 9640.7901s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0992961\n",
      "\tspeed: 0.1160s/iter; left time: 9576.2030s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1067948\n",
      "\tspeed: 0.1166s/iter; left time: 9617.4982s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1038578\n",
      "\tspeed: 0.1145s/iter; left time: 9432.7129s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177262\n",
      "\tspeed: 0.1165s/iter; left time: 9584.8832s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1152344\n",
      "\tspeed: 0.1140s/iter; left time: 9365.2021s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1077954\n",
      "\tspeed: 0.1157s/iter; left time: 9498.0317s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1127032\n",
      "\tspeed: 0.1171s/iter; left time: 9602.6803s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1118686\n",
      "\tspeed: 0.1122s/iter; left time: 9184.2086s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1096208\n",
      "\tspeed: 0.1129s/iter; left time: 9234.7673s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0965174\n",
      "\tspeed: 0.1125s/iter; left time: 9190.5343s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1007073\n",
      "\tspeed: 0.1129s/iter; left time: 9208.9160s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0923950\n",
      "\tspeed: 0.1153s/iter; left time: 9395.5281s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1115430\n",
      "\tspeed: 0.1138s/iter; left time: 9260.0025s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1126868\n",
      "\tspeed: 0.1146s/iter; left time: 9311.4516s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1034839\n",
      "\tspeed: 0.1118s/iter; left time: 9072.6644s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1078405\n",
      "\tspeed: 0.1113s/iter; left time: 9022.3053s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1037599\n",
      "\tspeed: 0.1137s/iter; left time: 9208.3424s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0952058\n",
      "\tspeed: 0.1122s/iter; left time: 9076.4295s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1051804\n",
      "\tspeed: 0.1120s/iter; left time: 9045.5504s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1044087\n",
      "\tspeed: 0.1130s/iter; left time: 9113.2990s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1066130\n",
      "\tspeed: 0.1147s/iter; left time: 9241.7066s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092677\n",
      "\tspeed: 0.1133s/iter; left time: 9121.1625s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1083696\n",
      "\tspeed: 0.1133s/iter; left time: 9110.3547s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1197105\n",
      "\tspeed: 0.1132s/iter; left time: 9089.6904s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1068081\n",
      "\tspeed: 0.1117s/iter; left time: 8954.9869s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1261329\n",
      "\tspeed: 0.1128s/iter; left time: 9030.0277s\n",
      "Epoch: 2 cost time: 00h:08m:32.27s\n",
      "Epoch: 2 | Train Loss: 0.1098766 Vali Loss: 0.1225332 Test Loss: 0.1477604\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0998430\n",
      "\tspeed: 1.5405s/iter; left time: 123129.9321s\n",
      "\titers: 200, epoch: 3 | loss: 0.1029584\n",
      "\tspeed: 0.1128s/iter; left time: 9008.1590s\n",
      "\titers: 300, epoch: 3 | loss: 0.0965041\n",
      "\tspeed: 0.1161s/iter; left time: 9253.1586s\n",
      "\titers: 400, epoch: 3 | loss: 0.1077358\n",
      "\tspeed: 0.1167s/iter; left time: 9291.2182s\n",
      "\titers: 500, epoch: 3 | loss: 0.1056079\n",
      "\tspeed: 0.1153s/iter; left time: 9165.7626s\n",
      "\titers: 600, epoch: 3 | loss: 0.1190159\n",
      "\tspeed: 0.1167s/iter; left time: 9271.6581s\n",
      "\titers: 700, epoch: 3 | loss: 0.1028953\n",
      "\tspeed: 0.1166s/iter; left time: 9248.6437s\n",
      "\titers: 800, epoch: 3 | loss: 0.1252995\n",
      "\tspeed: 0.1160s/iter; left time: 9193.5876s\n",
      "\titers: 900, epoch: 3 | loss: 0.0910322\n",
      "\tspeed: 0.1143s/iter; left time: 9045.1347s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1096593\n",
      "\tspeed: 0.1166s/iter; left time: 9214.4819s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0958329\n",
      "\tspeed: 0.1161s/iter; left time: 9163.4822s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1051666\n",
      "\tspeed: 0.1139s/iter; left time: 8978.8459s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1145009\n",
      "\tspeed: 0.1157s/iter; left time: 9111.8886s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1089370\n",
      "\tspeed: 0.1166s/iter; left time: 9170.4829s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0982486\n",
      "\tspeed: 0.1153s/iter; left time: 9055.8982s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1116064\n",
      "\tspeed: 0.1162s/iter; left time: 9112.0542s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1152646\n",
      "\tspeed: 0.1147s/iter; left time: 8981.9711s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1033108\n",
      "\tspeed: 0.1157s/iter; left time: 9047.4058s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1160483\n",
      "\tspeed: 0.1145s/iter; left time: 8942.6018s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1133725\n",
      "\tspeed: 0.1152s/iter; left time: 8988.5962s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0946887\n",
      "\tspeed: 0.1162s/iter; left time: 9056.8138s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1068842\n",
      "\tspeed: 0.1158s/iter; left time: 9008.9040s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1091909\n",
      "\tspeed: 0.1141s/iter; left time: 8869.8343s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0988755\n",
      "\tspeed: 0.1144s/iter; left time: 8880.5515s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0938124\n",
      "\tspeed: 0.1152s/iter; left time: 8929.7448s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1091160\n",
      "\tspeed: 0.1153s/iter; left time: 8929.1321s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1095193\n",
      "\tspeed: 0.1152s/iter; left time: 8908.6028s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1060785\n",
      "\tspeed: 0.1142s/iter; left time: 8823.2657s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0896944\n",
      "\tspeed: 0.1151s/iter; left time: 8875.4527s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0934717\n",
      "\tspeed: 0.1164s/iter; left time: 8969.1079s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1221983\n",
      "\tspeed: 0.1152s/iter; left time: 8861.5937s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0963738\n",
      "\tspeed: 0.1147s/iter; left time: 8815.4265s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1190489\n",
      "\tspeed: 0.1161s/iter; left time: 8908.4538s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1036496\n",
      "\tspeed: 0.1165s/iter; left time: 8928.5374s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1124387\n",
      "\tspeed: 0.1168s/iter; left time: 8939.4267s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1170041\n",
      "\tspeed: 0.1155s/iter; left time: 8827.6652s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1089111\n",
      "\tspeed: 0.1159s/iter; left time: 8850.0768s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0951907\n",
      "\tspeed: 0.1147s/iter; left time: 8746.3940s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0979321\n",
      "\tspeed: 0.1144s/iter; left time: 8706.1174s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0999806\n",
      "\tspeed: 0.1163s/iter; left time: 8842.1246s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0940368\n",
      "\tspeed: 0.1148s/iter; left time: 8719.9476s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0999659\n",
      "\tspeed: 0.1162s/iter; left time: 8810.3419s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0951753\n",
      "\tspeed: 0.1153s/iter; left time: 8732.1293s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0947775\n",
      "\tspeed: 0.1145s/iter; left time: 8662.4110s\n",
      "Epoch: 3 cost time: 00h:08m:33.85s\n",
      "Epoch: 3 | Train Loss: 0.1043304 Vali Loss: 0.1245715 Test Loss: 0.1501074\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0944092\n",
      "\tspeed: 1.5497s/iter; left time: 116975.3279s\n",
      "\titers: 200, epoch: 4 | loss: 0.0867483\n",
      "\tspeed: 0.1149s/iter; left time: 8658.0705s\n",
      "\titers: 300, epoch: 4 | loss: 0.1064334\n",
      "\tspeed: 0.1155s/iter; left time: 8698.3918s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956620\n",
      "\tspeed: 0.1158s/iter; left time: 8705.6986s\n",
      "\titers: 500, epoch: 4 | loss: 0.1117885\n",
      "\tspeed: 0.1166s/iter; left time: 8758.2921s\n",
      "\titers: 600, epoch: 4 | loss: 0.1063067\n",
      "\tspeed: 0.1150s/iter; left time: 8626.7746s\n",
      "\titers: 700, epoch: 4 | loss: 0.0915639\n",
      "\tspeed: 0.1156s/iter; left time: 8655.7974s\n",
      "\titers: 800, epoch: 4 | loss: 0.1026455\n",
      "\tspeed: 0.1153s/iter; left time: 8624.9928s\n",
      "\titers: 900, epoch: 4 | loss: 0.1035407\n",
      "\tspeed: 0.1159s/iter; left time: 8652.7572s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1075997\n",
      "\tspeed: 0.1163s/iter; left time: 8675.9252s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0891981\n",
      "\tspeed: 0.1140s/iter; left time: 8491.5195s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1041374\n",
      "\tspeed: 0.1134s/iter; left time: 8438.0519s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1022310\n",
      "\tspeed: 0.1133s/iter; left time: 8418.8444s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1072065\n",
      "\tspeed: 0.1122s/iter; left time: 8324.7086s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0838472\n",
      "\tspeed: 0.1134s/iter; left time: 8403.0457s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0923450\n",
      "\tspeed: 0.1137s/iter; left time: 8410.4056s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1015804\n",
      "\tspeed: 0.1152s/iter; left time: 8511.1904s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1218250\n",
      "\tspeed: 0.1150s/iter; left time: 8486.1731s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1150391\n",
      "\tspeed: 0.1155s/iter; left time: 8510.3849s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1164713\n",
      "\tspeed: 0.1148s/iter; left time: 8449.4671s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1034276\n",
      "\tspeed: 0.1131s/iter; left time: 8312.4625s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0898664\n",
      "\tspeed: 0.1181s/iter; left time: 8665.1293s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0815171\n",
      "\tspeed: 0.1187s/iter; left time: 8698.5061s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1059342\n",
      "\tspeed: 0.1173s/iter; left time: 8586.7144s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1042058\n",
      "\tspeed: 0.1165s/iter; left time: 8515.5548s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0878983\n",
      "\tspeed: 0.1162s/iter; left time: 8481.3466s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0832504\n",
      "\tspeed: 0.1187s/iter; left time: 8649.1016s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0833858\n",
      "\tspeed: 0.1179s/iter; left time: 8580.0004s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1033833\n",
      "\tspeed: 0.1149s/iter; left time: 8354.4193s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0952015\n",
      "\tspeed: 0.1168s/iter; left time: 8477.1869s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0781978\n",
      "\tspeed: 0.1149s/iter; left time: 8326.1763s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1017636\n",
      "\tspeed: 0.1138s/iter; left time: 8238.4425s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1010068\n",
      "\tspeed: 0.1156s/iter; left time: 8357.3458s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1017999\n",
      "\tspeed: 0.1184s/iter; left time: 8548.1644s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0878085\n",
      "\tspeed: 0.1168s/iter; left time: 8419.6193s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0985364\n",
      "\tspeed: 0.1170s/iter; left time: 8422.9713s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0910970\n",
      "\tspeed: 0.1139s/iter; left time: 8190.7131s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0913839\n",
      "\tspeed: 0.1141s/iter; left time: 8192.0792s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918390\n",
      "\tspeed: 0.1143s/iter; left time: 8189.9435s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1008365\n",
      "\tspeed: 0.1174s/iter; left time: 8401.5474s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0926459\n",
      "\tspeed: 0.1177s/iter; left time: 8410.5593s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0991397\n",
      "\tspeed: 0.1154s/iter; left time: 8239.6806s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0954166\n",
      "\tspeed: 0.1142s/iter; left time: 8141.7228s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0878920\n",
      "\tspeed: 0.1161s/iter; left time: 8266.2998s\n",
      "Epoch: 4 cost time: 00h:08m:34.28s\n",
      "Epoch: 4 | Train Loss: 0.0984264 Vali Loss: 0.1281532 Test Loss: 0.1525455\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1027972\n",
      "\tspeed: 1.5516s/iter; left time: 110220.2287s\n",
      "\titers: 200, epoch: 5 | loss: 0.1047456\n",
      "\tspeed: 0.1157s/iter; left time: 8208.4461s\n",
      "\titers: 300, epoch: 5 | loss: 0.0907510\n",
      "\tspeed: 0.1167s/iter; left time: 8265.7405s\n",
      "\titers: 400, epoch: 5 | loss: 0.0943936\n",
      "\tspeed: 0.1172s/iter; left time: 8292.2366s\n",
      "\titers: 500, epoch: 5 | loss: 0.0821496\n",
      "\tspeed: 0.1161s/iter; left time: 8200.0644s\n",
      "\titers: 600, epoch: 5 | loss: 0.0936060\n",
      "\tspeed: 0.1160s/iter; left time: 8185.4850s\n",
      "\titers: 700, epoch: 5 | loss: 0.0996595\n",
      "\tspeed: 0.1171s/iter; left time: 8250.5821s\n",
      "\titers: 800, epoch: 5 | loss: 0.0979459\n",
      "\tspeed: 0.1154s/iter; left time: 8119.3812s\n",
      "\titers: 900, epoch: 5 | loss: 0.1001264\n",
      "\tspeed: 0.1133s/iter; left time: 7958.2826s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1090219\n",
      "\tspeed: 0.1141s/iter; left time: 8005.5584s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1011967\n",
      "\tspeed: 0.1139s/iter; left time: 7978.4549s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0915010\n",
      "\tspeed: 0.1158s/iter; left time: 8096.2294s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1061302\n",
      "\tspeed: 0.1162s/iter; left time: 8116.5520s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1118270\n",
      "\tspeed: 0.1164s/iter; left time: 8118.4551s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0901711\n",
      "\tspeed: 0.1157s/iter; left time: 8055.1593s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0990031\n",
      "\tspeed: 0.1164s/iter; left time: 8095.2933s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1073154\n",
      "\tspeed: 0.1151s/iter; left time: 7990.1939s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0957215\n",
      "\tspeed: 0.1146s/iter; left time: 7948.0382s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0969568\n",
      "\tspeed: 0.1147s/iter; left time: 7938.2746s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1063399\n",
      "\tspeed: 0.1152s/iter; left time: 7965.5585s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1092876\n",
      "\tspeed: 0.1184s/iter; left time: 8170.8366s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0891375\n",
      "\tspeed: 0.1147s/iter; left time: 7910.2817s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0779103\n",
      "\tspeed: 0.1175s/iter; left time: 8087.8896s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0807228\n",
      "\tspeed: 0.1168s/iter; left time: 8025.9800s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0742790\n",
      "\tspeed: 0.1168s/iter; left time: 8016.6586s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0908970\n",
      "\tspeed: 0.1147s/iter; left time: 7858.3526s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1129368\n",
      "\tspeed: 0.1143s/iter; left time: 7820.8059s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0733277\n",
      "\tspeed: 0.1184s/iter; left time: 8090.8699s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1080936\n",
      "\tspeed: 0.1183s/iter; left time: 8070.8115s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0774377\n",
      "\tspeed: 0.1167s/iter; left time: 7949.2214s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0847868\n",
      "\tspeed: 0.1155s/iter; left time: 7861.0918s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0902236\n",
      "\tspeed: 0.1149s/iter; left time: 7805.2258s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0947069\n",
      "\tspeed: 0.1163s/iter; left time: 7886.0970s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0958610\n",
      "\tspeed: 0.1161s/iter; left time: 7863.7970s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0908997\n",
      "\tspeed: 0.1148s/iter; left time: 7767.2626s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0939663\n",
      "\tspeed: 0.1141s/iter; left time: 7708.7375s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0827342\n",
      "\tspeed: 0.1151s/iter; left time: 7761.5226s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1009660\n",
      "\tspeed: 0.1139s/iter; left time: 7667.6180s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0889867\n",
      "\tspeed: 0.1142s/iter; left time: 7681.4587s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0949150\n",
      "\tspeed: 0.1134s/iter; left time: 7613.2535s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1014487\n",
      "\tspeed: 0.1142s/iter; left time: 7656.9560s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0968869\n",
      "\tspeed: 0.1153s/iter; left time: 7716.1146s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0901529\n",
      "\tspeed: 0.1157s/iter; left time: 7732.6608s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0915154\n",
      "\tspeed: 0.1180s/iter; left time: 7871.9444s\n",
      "Epoch: 5 cost time: 00h:08m:34.78s\n",
      "Epoch: 5 | Train Loss: 0.0934047 Vali Loss: 0.1274233 Test Loss: 0.1514446\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0924059\n",
      "\tspeed: 1.5476s/iter; left time: 103055.2063s\n",
      "\titers: 200, epoch: 6 | loss: 0.0953507\n",
      "\tspeed: 0.1191s/iter; left time: 7915.8442s\n",
      "\titers: 300, epoch: 6 | loss: 0.0743414\n",
      "\tspeed: 0.1163s/iter; left time: 7718.9871s\n",
      "\titers: 400, epoch: 6 | loss: 0.0888031\n",
      "\tspeed: 0.1185s/iter; left time: 7852.1903s\n",
      "\titers: 500, epoch: 6 | loss: 0.1043609\n",
      "\tspeed: 0.1168s/iter; left time: 7733.2412s\n",
      "\titers: 600, epoch: 6 | loss: 0.0847825\n",
      "\tspeed: 0.1188s/iter; left time: 7854.5862s\n",
      "\titers: 700, epoch: 6 | loss: 0.1001649\n",
      "\tspeed: 0.1176s/iter; left time: 7758.1293s\n",
      "\titers: 800, epoch: 6 | loss: 0.1008426\n",
      "\tspeed: 0.1162s/iter; left time: 7657.0937s\n",
      "\titers: 900, epoch: 6 | loss: 0.0914951\n",
      "\tspeed: 0.1168s/iter; left time: 7683.7625s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0961552\n",
      "\tspeed: 0.1188s/iter; left time: 7801.7333s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0863231\n",
      "\tspeed: 0.1186s/iter; left time: 7781.1236s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0925186\n",
      "\tspeed: 0.1202s/iter; left time: 7871.2631s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0857735\n",
      "\tspeed: 0.1174s/iter; left time: 7675.7391s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0983386\n",
      "\tspeed: 0.1202s/iter; left time: 7851.0174s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0876124\n",
      "\tspeed: 0.1204s/iter; left time: 7846.9987s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0781590\n",
      "\tspeed: 0.1156s/iter; left time: 7526.7923s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0933763\n",
      "\tspeed: 0.1205s/iter; left time: 7832.0395s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0982791\n",
      "\tspeed: 0.1149s/iter; left time: 7454.8488s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0728418\n",
      "\tspeed: 0.1143s/iter; left time: 7402.4854s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0914764\n",
      "\tspeed: 0.1159s/iter; left time: 7497.4194s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0815687\n",
      "\tspeed: 0.1173s/iter; left time: 7575.4451s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0897236\n",
      "\tspeed: 0.1172s/iter; left time: 7557.0015s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0844050\n",
      "\tspeed: 0.1166s/iter; left time: 7508.0284s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0881767\n",
      "\tspeed: 0.1161s/iter; left time: 7461.0308s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0834182\n",
      "\tspeed: 0.1222s/iter; left time: 7841.6178s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0942059\n",
      "\tspeed: 0.1146s/iter; left time: 7342.8139s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0970327\n",
      "\tspeed: 0.1168s/iter; left time: 7472.0707s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989593\n",
      "\tspeed: 0.1172s/iter; left time: 7486.8818s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0968164\n",
      "\tspeed: 0.1192s/iter; left time: 7603.9717s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0916561\n",
      "\tspeed: 0.1167s/iter; left time: 7429.6563s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0938281\n",
      "\tspeed: 0.1172s/iter; left time: 7455.4710s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0751842\n",
      "\tspeed: 0.1199s/iter; left time: 7613.1808s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0846767\n",
      "\tspeed: 0.1195s/iter; left time: 7572.9081s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0979014\n",
      "\tspeed: 0.1223s/iter; left time: 7737.9127s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0967091\n",
      "\tspeed: 0.1144s/iter; left time: 7230.6336s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0888710\n",
      "\tspeed: 0.1178s/iter; left time: 7430.2501s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0812877\n",
      "\tspeed: 0.1143s/iter; left time: 7198.5841s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0757051\n",
      "\tspeed: 0.1140s/iter; left time: 7168.3648s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0898106\n",
      "\tspeed: 0.1154s/iter; left time: 7244.5771s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1019688\n",
      "\tspeed: 0.1157s/iter; left time: 7252.7306s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0886026\n",
      "\tspeed: 0.1212s/iter; left time: 7585.3348s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0841476\n",
      "\tspeed: 0.1170s/iter; left time: 7309.4672s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0868416\n",
      "\tspeed: 0.1157s/iter; left time: 7217.7561s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0976313\n",
      "\tspeed: 0.1141s/iter; left time: 7107.8399s\n",
      "Epoch: 6 cost time: 00h:08m:42.56s\n",
      "Epoch: 6 | Train Loss: 0.0896105 Vali Loss: 0.1277131 Test Loss: 0.1531950\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04277150332927704, rmse:0.20681272447109222, mae:0.14383640885353088, rse:0.7167763113975525\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 168: 01h:08m:23.04s\n",
      "\n",
      "Intermediate time for GB: 04h:19m:43.32s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 10:24:52,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 10:24:54,700] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 10:24:54,700] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 10:24:54,700] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 10:24:54,895] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 10:24:54,895] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 10:24:55,670] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 10:24:55,671] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 10:24:55,672] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 10:24:55,673] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 10:24:55,673] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 10:24:55,673] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 10:24:55,673] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 10:24:56,136] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 10:24:56,138] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.22 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,274] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 10:24:56,275] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,275] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.16 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,275] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 10:24:56,398] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 10:24:56,399] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 10:24:56,399] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.13 GB, percent = 21.5%\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 10:24:56,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f755575a050>\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 10:24:56,401] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 10:24:56,402] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 10:24:56,403] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1790063\n",
      "\tspeed: 0.1804s/iter; left time: 9654.8354s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687252\n",
      "\tspeed: 0.1288s/iter; left time: 6882.6025s\n",
      "\titers: 300, epoch: 1 | loss: 0.1309537\n",
      "\tspeed: 0.1273s/iter; left time: 6785.8478s\n",
      "\titers: 400, epoch: 1 | loss: 0.1245550\n",
      "\tspeed: 0.1274s/iter; left time: 6779.2307s\n",
      "\titers: 500, epoch: 1 | loss: 0.1051500\n",
      "\tspeed: 0.1296s/iter; left time: 6883.5540s\n",
      "\titers: 600, epoch: 1 | loss: 0.0886984\n",
      "\tspeed: 0.1291s/iter; left time: 6847.2720s\n",
      "\titers: 700, epoch: 1 | loss: 0.0941334\n",
      "\tspeed: 0.1298s/iter; left time: 6866.7539s\n",
      "\titers: 800, epoch: 1 | loss: 0.0780938\n",
      "\tspeed: 0.1254s/iter; left time: 6625.6681s\n",
      "\titers: 900, epoch: 1 | loss: 0.0840373\n",
      "\tspeed: 0.1287s/iter; left time: 6784.1158s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0893998\n",
      "\tspeed: 0.1293s/iter; left time: 6803.2337s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0767727\n",
      "\tspeed: 0.1283s/iter; left time: 6739.6440s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0874630\n",
      "\tspeed: 0.1275s/iter; left time: 6685.8349s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0802690\n",
      "\tspeed: 0.1263s/iter; left time: 6608.3775s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0984690\n",
      "\tspeed: 0.1274s/iter; left time: 6652.6463s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0814931\n",
      "\tspeed: 0.1302s/iter; left time: 6785.3352s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0775438\n",
      "\tspeed: 0.1344s/iter; left time: 6989.4507s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801652\n",
      "\tspeed: 0.1321s/iter; left time: 6860.4821s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0743842\n",
      "\tspeed: 0.1277s/iter; left time: 6618.4710s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0854426\n",
      "\tspeed: 0.1287s/iter; left time: 6654.7200s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0780391\n",
      "\tspeed: 0.1286s/iter; left time: 6638.0561s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0724551\n",
      "\tspeed: 0.1272s/iter; left time: 6552.3398s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0770094\n",
      "\tspeed: 0.1267s/iter; left time: 6515.2400s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0864393\n",
      "\tspeed: 0.1267s/iter; left time: 6501.0342s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0777969\n",
      "\tspeed: 0.1283s/iter; left time: 6572.1778s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0745067\n",
      "\tspeed: 0.1275s/iter; left time: 6518.0973s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0927263\n",
      "\tspeed: 0.1279s/iter; left time: 6526.9623s\n",
      "Epoch: 1 cost time: 00h:05m:45.68s\n",
      "Epoch: 1 | Train Loss: 0.0993755 Vali Loss: 0.0662276 Test Loss: 0.0754743\n",
      "Validation loss decreased (inf --> 0.066228).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0936529\n",
      "\tspeed: 1.2303s/iter; left time: 62549.7002s\n",
      "\titers: 200, epoch: 2 | loss: 0.0912237\n",
      "\tspeed: 0.1184s/iter; left time: 6010.0761s\n",
      "\titers: 300, epoch: 2 | loss: 0.0807760\n",
      "\tspeed: 0.1178s/iter; left time: 5966.7274s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864775\n",
      "\tspeed: 0.1187s/iter; left time: 5997.6103s\n",
      "\titers: 500, epoch: 2 | loss: 0.0780736\n",
      "\tspeed: 0.1146s/iter; left time: 5781.9483s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745789\n",
      "\tspeed: 0.1154s/iter; left time: 5809.9201s\n",
      "\titers: 700, epoch: 2 | loss: 0.0697930\n",
      "\tspeed: 0.1180s/iter; left time: 5926.9884s\n",
      "\titers: 800, epoch: 2 | loss: 0.0780450\n",
      "\tspeed: 0.1161s/iter; left time: 5821.2941s\n",
      "\titers: 900, epoch: 2 | loss: 0.0729557\n",
      "\tspeed: 0.1163s/iter; left time: 5819.5825s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0800805\n",
      "\tspeed: 0.1147s/iter; left time: 5727.8370s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0736994\n",
      "\tspeed: 0.1143s/iter; left time: 5696.9979s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0718818\n",
      "\tspeed: 0.1132s/iter; left time: 5631.1617s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0728408\n",
      "\tspeed: 0.1143s/iter; left time: 5672.7104s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0732936\n",
      "\tspeed: 0.1155s/iter; left time: 5721.5886s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0862661\n",
      "\tspeed: 0.1155s/iter; left time: 5708.7566s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0885015\n",
      "\tspeed: 0.1165s/iter; left time: 5746.3741s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0699046\n",
      "\tspeed: 0.1194s/iter; left time: 5877.9799s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0673015\n",
      "\tspeed: 0.1178s/iter; left time: 5786.2836s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0852279\n",
      "\tspeed: 0.1161s/iter; left time: 5693.6767s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0779036\n",
      "\tspeed: 0.1150s/iter; left time: 5628.4731s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0709876\n",
      "\tspeed: 0.1149s/iter; left time: 5610.1158s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0774782\n",
      "\tspeed: 0.1137s/iter; left time: 5539.6278s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0751852\n",
      "\tspeed: 0.1162s/iter; left time: 5654.1786s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0819853\n",
      "\tspeed: 0.1158s/iter; left time: 5620.3485s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0652728\n",
      "\tspeed: 0.1170s/iter; left time: 5668.2419s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0770506\n",
      "\tspeed: 0.1167s/iter; left time: 5641.0626s\n",
      "Epoch: 2 cost time: 00h:05m:11.76s\n",
      "Epoch: 2 | Train Loss: 0.0773266 Vali Loss: 0.0630829 Test Loss: 0.0720199\n",
      "Validation loss decreased (0.066228 --> 0.063083).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0664506\n",
      "\tspeed: 1.0860s/iter; left time: 52298.8217s\n",
      "\titers: 200, epoch: 3 | loss: 0.0780226\n",
      "\tspeed: 0.1148s/iter; left time: 5515.8168s\n",
      "\titers: 300, epoch: 3 | loss: 0.0809838\n",
      "\tspeed: 0.1146s/iter; left time: 5496.9904s\n",
      "\titers: 400, epoch: 3 | loss: 0.0751593\n",
      "\tspeed: 0.1146s/iter; left time: 5482.8314s\n",
      "\titers: 500, epoch: 3 | loss: 0.0690148\n",
      "\tspeed: 0.1186s/iter; left time: 5664.2265s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651948\n",
      "\tspeed: 0.1200s/iter; left time: 5717.9900s\n",
      "\titers: 700, epoch: 3 | loss: 0.0696828\n",
      "\tspeed: 0.1192s/iter; left time: 5669.5340s\n",
      "\titers: 800, epoch: 3 | loss: 0.0909723\n",
      "\tspeed: 0.1180s/iter; left time: 5601.0442s\n",
      "\titers: 900, epoch: 3 | loss: 0.0693156\n",
      "\tspeed: 0.1179s/iter; left time: 5583.0343s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0763046\n",
      "\tspeed: 0.1202s/iter; left time: 5681.1260s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0654815\n",
      "\tspeed: 0.1174s/iter; left time: 5538.4323s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0816431\n",
      "\tspeed: 0.1205s/iter; left time: 5671.2245s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0731011\n",
      "\tspeed: 0.1176s/iter; left time: 5522.5890s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0798532\n",
      "\tspeed: 0.1202s/iter; left time: 5634.7652s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0781588\n",
      "\tspeed: 0.1197s/iter; left time: 5594.8528s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0688082\n",
      "\tspeed: 0.1186s/iter; left time: 5535.3495s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0722890\n",
      "\tspeed: 0.1157s/iter; left time: 5385.5811s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0578647\n",
      "\tspeed: 0.1150s/iter; left time: 5343.9319s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0831098\n",
      "\tspeed: 0.1192s/iter; left time: 5524.3157s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0657302\n",
      "\tspeed: 0.1168s/iter; left time: 5401.8942s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0686098\n",
      "\tspeed: 0.1184s/iter; left time: 5463.8973s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0793400\n",
      "\tspeed: 0.1222s/iter; left time: 5627.4213s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0739723\n",
      "\tspeed: 0.1189s/iter; left time: 5464.7096s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0883555\n",
      "\tspeed: 0.1152s/iter; left time: 5282.4396s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0636316\n",
      "\tspeed: 0.1158s/iter; left time: 5297.6672s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0704639\n",
      "\tspeed: 0.1139s/iter; left time: 5199.1854s\n",
      "Epoch: 3 cost time: 00h:05m:15.64s\n",
      "Epoch: 3 | Train Loss: 0.0731855 Vali Loss: 0.0623539 Test Loss: 0.0720562\n",
      "Validation loss decreased (0.063083 --> 0.062354).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0695748\n",
      "\tspeed: 1.0455s/iter; left time: 47545.2099s\n",
      "\titers: 200, epoch: 4 | loss: 0.0786053\n",
      "\tspeed: 0.1144s/iter; left time: 5191.6691s\n",
      "\titers: 300, epoch: 4 | loss: 0.0584018\n",
      "\tspeed: 0.1150s/iter; left time: 5205.4502s\n",
      "\titers: 400, epoch: 4 | loss: 0.0929034\n",
      "\tspeed: 0.1155s/iter; left time: 5216.2159s\n",
      "\titers: 500, epoch: 4 | loss: 0.0779471\n",
      "\tspeed: 0.1114s/iter; left time: 5020.0233s\n",
      "\titers: 600, epoch: 4 | loss: 0.0695291\n",
      "\tspeed: 0.1130s/iter; left time: 5082.2797s\n",
      "\titers: 700, epoch: 4 | loss: 0.0666811\n",
      "\tspeed: 0.1151s/iter; left time: 5166.2757s\n",
      "\titers: 800, epoch: 4 | loss: 0.0734229\n",
      "\tspeed: 0.1154s/iter; left time: 5167.0127s\n",
      "\titers: 900, epoch: 4 | loss: 0.0612417\n",
      "\tspeed: 0.1138s/iter; left time: 5084.3318s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0748140\n",
      "\tspeed: 0.1142s/iter; left time: 5088.6718s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0748632\n",
      "\tspeed: 0.1153s/iter; left time: 5127.5359s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0688734\n",
      "\tspeed: 0.1159s/iter; left time: 5144.1363s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0741981\n",
      "\tspeed: 0.1160s/iter; left time: 5137.8647s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0658868\n",
      "\tspeed: 0.1148s/iter; left time: 5072.0988s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0664924\n",
      "\tspeed: 0.1150s/iter; left time: 5068.6878s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0765673\n",
      "\tspeed: 0.1144s/iter; left time: 5031.6526s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0755662\n",
      "\tspeed: 0.1157s/iter; left time: 5074.7618s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0777128\n",
      "\tspeed: 0.1148s/iter; left time: 5026.6644s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0851511\n",
      "\tspeed: 0.1154s/iter; left time: 5040.0799s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0781193\n",
      "\tspeed: 0.1161s/iter; left time: 5057.9787s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0825148\n",
      "\tspeed: 0.1156s/iter; left time: 5025.7944s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0718397\n",
      "\tspeed: 0.1155s/iter; left time: 5009.7955s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0678933\n",
      "\tspeed: 0.1169s/iter; left time: 5058.4917s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0727424\n",
      "\tspeed: 0.1153s/iter; left time: 4979.7414s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0754367\n",
      "\tspeed: 0.1147s/iter; left time: 4941.4695s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0694987\n",
      "\tspeed: 0.1147s/iter; left time: 4927.9194s\n",
      "Epoch: 4 cost time: 00h:05m:08.58s\n",
      "Epoch: 4 | Train Loss: 0.0712121 Vali Loss: 0.0611709 Test Loss: 0.0710917\n",
      "Validation loss decreased (0.062354 --> 0.061171).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0630679\n",
      "\tspeed: 1.0569s/iter; left time: 45230.5762s\n",
      "\titers: 200, epoch: 5 | loss: 0.0689823\n",
      "\tspeed: 0.1152s/iter; left time: 4917.2996s\n",
      "\titers: 300, epoch: 5 | loss: 0.0810203\n",
      "\tspeed: 0.1162s/iter; left time: 4949.1596s\n",
      "\titers: 400, epoch: 5 | loss: 0.0599910\n",
      "\tspeed: 0.1150s/iter; left time: 4886.2999s\n",
      "\titers: 500, epoch: 5 | loss: 0.0720650\n",
      "\tspeed: 0.1143s/iter; left time: 4845.0679s\n",
      "\titers: 600, epoch: 5 | loss: 0.0723064\n",
      "\tspeed: 0.1147s/iter; left time: 4852.3544s\n",
      "\titers: 700, epoch: 5 | loss: 0.0714625\n",
      "\tspeed: 0.1139s/iter; left time: 4807.6518s\n",
      "\titers: 800, epoch: 5 | loss: 0.0736922\n",
      "\tspeed: 0.1128s/iter; left time: 4748.6478s\n",
      "\titers: 900, epoch: 5 | loss: 0.0730744\n",
      "\tspeed: 0.1134s/iter; left time: 4760.4105s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0762590\n",
      "\tspeed: 0.1141s/iter; left time: 4781.9349s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0776123\n",
      "\tspeed: 0.1133s/iter; left time: 4735.9502s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0766415\n",
      "\tspeed: 0.1127s/iter; left time: 4697.1781s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0700914\n",
      "\tspeed: 0.1137s/iter; left time: 4729.9397s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0647124\n",
      "\tspeed: 0.1127s/iter; left time: 4676.9359s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0750828\n",
      "\tspeed: 0.1134s/iter; left time: 4694.3295s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0726999\n",
      "\tspeed: 0.1137s/iter; left time: 4695.2950s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0699233\n",
      "\tspeed: 0.1123s/iter; left time: 4626.5963s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0737871\n",
      "\tspeed: 0.1139s/iter; left time: 4682.4217s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0672794\n",
      "\tspeed: 0.1147s/iter; left time: 4701.4137s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0721373\n",
      "\tspeed: 0.1153s/iter; left time: 4716.3622s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0577321\n",
      "\tspeed: 0.1148s/iter; left time: 4682.7861s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0599433\n",
      "\tspeed: 0.1149s/iter; left time: 4677.9945s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0708436\n",
      "\tspeed: 0.1131s/iter; left time: 4590.2444s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0647120\n",
      "\tspeed: 0.1143s/iter; left time: 4627.6918s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0707248\n",
      "\tspeed: 0.1142s/iter; left time: 4613.0119s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0699116\n",
      "\tspeed: 0.1138s/iter; left time: 4585.3660s\n",
      "Epoch: 5 cost time: 00h:05m:06.02s\n",
      "Epoch: 5 | Train Loss: 0.0699275 Vali Loss: 0.0589704 Test Loss: 0.0681652\n",
      "Validation loss decreased (0.061171 --> 0.058970).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0585817\n",
      "\tspeed: 1.0444s/iter; left time: 41895.8387s\n",
      "\titers: 200, epoch: 6 | loss: 0.0743628\n",
      "\tspeed: 0.1150s/iter; left time: 4601.7528s\n",
      "\titers: 300, epoch: 6 | loss: 0.0816830\n",
      "\tspeed: 0.1144s/iter; left time: 4568.0115s\n",
      "\titers: 400, epoch: 6 | loss: 0.0686249\n",
      "\tspeed: 0.1157s/iter; left time: 4606.7715s\n",
      "\titers: 500, epoch: 6 | loss: 0.0658081\n",
      "\tspeed: 0.1160s/iter; left time: 4607.8605s\n",
      "\titers: 600, epoch: 6 | loss: 0.0699370\n",
      "\tspeed: 0.1123s/iter; left time: 4448.1751s\n",
      "\titers: 700, epoch: 6 | loss: 0.0579729\n",
      "\tspeed: 0.1141s/iter; left time: 4509.3703s\n",
      "\titers: 800, epoch: 6 | loss: 0.0594627\n",
      "\tspeed: 0.1135s/iter; left time: 4473.9968s\n",
      "\titers: 900, epoch: 6 | loss: 0.0765333\n",
      "\tspeed: 0.1126s/iter; left time: 4425.4338s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0679004\n",
      "\tspeed: 0.1127s/iter; left time: 4419.3486s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0628908\n",
      "\tspeed: 0.1142s/iter; left time: 4465.3444s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0643912\n",
      "\tspeed: 0.1134s/iter; left time: 4422.9509s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0718672\n",
      "\tspeed: 0.1122s/iter; left time: 4366.1406s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0650968\n",
      "\tspeed: 0.1133s/iter; left time: 4397.1992s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0803147\n",
      "\tspeed: 0.1126s/iter; left time: 4357.9873s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721307\n",
      "\tspeed: 0.1128s/iter; left time: 4355.1406s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0664947\n",
      "\tspeed: 0.1121s/iter; left time: 4318.4247s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0713947\n",
      "\tspeed: 0.1131s/iter; left time: 4345.8420s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0666180\n",
      "\tspeed: 0.1140s/iter; left time: 4367.4006s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0742579\n",
      "\tspeed: 0.1155s/iter; left time: 4415.1730s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0744588\n",
      "\tspeed: 0.1138s/iter; left time: 4336.3663s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0583553\n",
      "\tspeed: 0.1137s/iter; left time: 4324.2063s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0706729\n",
      "\tspeed: 0.1135s/iter; left time: 4304.5950s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0550006\n",
      "\tspeed: 0.1142s/iter; left time: 4317.8107s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0605696\n",
      "\tspeed: 0.1146s/iter; left time: 4324.0003s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0701861\n",
      "\tspeed: 0.1142s/iter; left time: 4296.4502s\n",
      "Epoch: 6 cost time: 00h:05m:05.76s\n",
      "Epoch: 6 | Train Loss: 0.0689073 Vali Loss: 0.0598308 Test Loss: 0.0697114\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0657653\n",
      "\tspeed: 1.0371s/iter; left time: 38823.0358s\n",
      "\titers: 200, epoch: 7 | loss: 0.0665484\n",
      "\tspeed: 0.1143s/iter; left time: 4265.6868s\n",
      "\titers: 300, epoch: 7 | loss: 0.0624240\n",
      "\tspeed: 0.1151s/iter; left time: 4287.0299s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643224\n",
      "\tspeed: 0.1143s/iter; left time: 4245.1658s\n",
      "\titers: 500, epoch: 7 | loss: 0.0836057\n",
      "\tspeed: 0.1148s/iter; left time: 4250.6829s\n",
      "\titers: 600, epoch: 7 | loss: 0.0660850\n",
      "\tspeed: 0.1122s/iter; left time: 4144.5433s\n",
      "\titers: 700, epoch: 7 | loss: 0.0869880\n",
      "\tspeed: 0.1148s/iter; left time: 4227.2054s\n",
      "\titers: 800, epoch: 7 | loss: 0.0704906\n",
      "\tspeed: 0.1132s/iter; left time: 4157.9783s\n",
      "\titers: 900, epoch: 7 | loss: 0.0647015\n",
      "\tspeed: 0.1136s/iter; left time: 4162.2106s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0806929\n",
      "\tspeed: 0.1146s/iter; left time: 4187.5710s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0759010\n",
      "\tspeed: 0.1140s/iter; left time: 4153.3982s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0615634\n",
      "\tspeed: 0.1137s/iter; left time: 4131.3824s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0764042\n",
      "\tspeed: 0.1122s/iter; left time: 4064.2429s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674644\n",
      "\tspeed: 0.1125s/iter; left time: 4065.6722s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0740616\n",
      "\tspeed: 0.1134s/iter; left time: 4084.7985s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0792665\n",
      "\tspeed: 0.1136s/iter; left time: 4082.3793s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0690767\n",
      "\tspeed: 0.1127s/iter; left time: 4039.7644s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0598226\n",
      "\tspeed: 0.1149s/iter; left time: 4104.9907s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0678398\n",
      "\tspeed: 0.1154s/iter; left time: 4113.3813s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0747984\n",
      "\tspeed: 0.1150s/iter; left time: 4087.7855s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0712984\n",
      "\tspeed: 0.1151s/iter; left time: 4077.3569s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0657289\n",
      "\tspeed: 0.1137s/iter; left time: 4016.5526s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0730863\n",
      "\tspeed: 0.1156s/iter; left time: 4073.4537s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0687535\n",
      "\tspeed: 0.1143s/iter; left time: 4015.7164s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0626324\n",
      "\tspeed: 0.1146s/iter; left time: 4016.0510s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0694073\n",
      "\tspeed: 0.1139s/iter; left time: 3980.2275s\n",
      "Epoch: 7 cost time: 00h:05m:06.51s\n",
      "Epoch: 7 | Train Loss: 0.0681056 Vali Loss: 0.0597853 Test Loss: 0.0696563\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0955697\n",
      "\tspeed: 1.0430s/iter; left time: 36246.7374s\n",
      "\titers: 200, epoch: 8 | loss: 0.0681316\n",
      "\tspeed: 0.1134s/iter; left time: 3929.1527s\n",
      "\titers: 300, epoch: 8 | loss: 0.0796526\n",
      "\tspeed: 0.1125s/iter; left time: 3887.7026s\n",
      "\titers: 400, epoch: 8 | loss: 0.0672699\n",
      "\tspeed: 0.1137s/iter; left time: 3916.2965s\n",
      "\titers: 500, epoch: 8 | loss: 0.0579451\n",
      "\tspeed: 0.1137s/iter; left time: 3905.9708s\n",
      "\titers: 600, epoch: 8 | loss: 0.0730144\n",
      "\tspeed: 0.1146s/iter; left time: 3924.0622s\n",
      "\titers: 700, epoch: 8 | loss: 0.0561597\n",
      "\tspeed: 0.1131s/iter; left time: 3861.1952s\n",
      "\titers: 800, epoch: 8 | loss: 0.0630030\n",
      "\tspeed: 0.1123s/iter; left time: 3823.9748s\n",
      "\titers: 900, epoch: 8 | loss: 0.0696586\n",
      "\tspeed: 0.1131s/iter; left time: 3839.3908s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0760989\n",
      "\tspeed: 0.1129s/iter; left time: 3823.1975s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0601159\n",
      "\tspeed: 0.1125s/iter; left time: 3798.2438s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0697419\n",
      "\tspeed: 0.1127s/iter; left time: 3792.1555s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0581929\n",
      "\tspeed: 0.1156s/iter; left time: 3878.0686s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0630530\n",
      "\tspeed: 0.1134s/iter; left time: 3792.1820s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0653922\n",
      "\tspeed: 0.1125s/iter; left time: 3752.6265s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0719701\n",
      "\tspeed: 0.1122s/iter; left time: 3729.8680s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0626506\n",
      "\tspeed: 0.1124s/iter; left time: 3727.5302s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0601200\n",
      "\tspeed: 0.1121s/iter; left time: 3706.5486s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0739461\n",
      "\tspeed: 0.1125s/iter; left time: 3706.8998s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0698656\n",
      "\tspeed: 0.1148s/iter; left time: 3772.4910s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0620731\n",
      "\tspeed: 0.1124s/iter; left time: 3681.6874s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0645657\n",
      "\tspeed: 0.1139s/iter; left time: 3719.4823s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0703310\n",
      "\tspeed: 0.1129s/iter; left time: 3676.4973s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0673083\n",
      "\tspeed: 0.1124s/iter; left time: 3646.4198s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0710015\n",
      "\tspeed: 0.1135s/iter; left time: 3673.5000s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0631191\n",
      "\tspeed: 0.1134s/iter; left time: 3656.2917s\n",
      "Epoch: 8 cost time: 00h:05m:04.16s\n",
      "Epoch: 8 | Train Loss: 0.0675166 Vali Loss: 0.0581114 Test Loss: 0.0668070\n",
      "Validation loss decreased (0.058970 --> 0.058111).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0710958\n",
      "\tspeed: 1.0400s/iter; left time: 33355.3824s\n",
      "\titers: 200, epoch: 9 | loss: 0.0710134\n",
      "\tspeed: 0.1117s/iter; left time: 3570.8230s\n",
      "\titers: 300, epoch: 9 | loss: 0.0564387\n",
      "\tspeed: 0.1126s/iter; left time: 3588.6726s\n",
      "\titers: 400, epoch: 9 | loss: 0.0593314\n",
      "\tspeed: 0.1127s/iter; left time: 3581.2274s\n",
      "\titers: 500, epoch: 9 | loss: 0.0549250\n",
      "\tspeed: 0.1135s/iter; left time: 3594.9364s\n",
      "\titers: 600, epoch: 9 | loss: 0.0573312\n",
      "\tspeed: 0.1121s/iter; left time: 3538.6037s\n",
      "\titers: 700, epoch: 9 | loss: 0.0749223\n",
      "\tspeed: 0.1143s/iter; left time: 3597.2976s\n",
      "\titers: 800, epoch: 9 | loss: 0.0616827\n",
      "\tspeed: 0.1156s/iter; left time: 3627.1548s\n",
      "\titers: 900, epoch: 9 | loss: 0.0696181\n",
      "\tspeed: 0.1130s/iter; left time: 3533.0747s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0527223\n",
      "\tspeed: 0.1131s/iter; left time: 3524.5021s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0648112\n",
      "\tspeed: 0.1121s/iter; left time: 3484.1837s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0737804\n",
      "\tspeed: 0.1123s/iter; left time: 3478.6223s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0596698\n",
      "\tspeed: 0.1130s/iter; left time: 3488.1215s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0660387\n",
      "\tspeed: 0.1121s/iter; left time: 3449.5615s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0742668\n",
      "\tspeed: 0.1149s/iter; left time: 3523.9911s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0563254\n",
      "\tspeed: 0.1146s/iter; left time: 3504.2331s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0555071\n",
      "\tspeed: 0.1133s/iter; left time: 3453.3819s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0757507\n",
      "\tspeed: 0.1136s/iter; left time: 3451.1903s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0711249\n",
      "\tspeed: 0.1131s/iter; left time: 3423.7884s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0721804\n",
      "\tspeed: 0.1134s/iter; left time: 3420.5568s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0665950\n",
      "\tspeed: 0.1139s/iter; left time: 3425.0314s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0581865\n",
      "\tspeed: 0.1138s/iter; left time: 3411.8899s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0679084\n",
      "\tspeed: 0.1130s/iter; left time: 3376.8615s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0549888\n",
      "\tspeed: 0.1133s/iter; left time: 3372.3990s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0570390\n",
      "\tspeed: 0.1135s/iter; left time: 3368.7344s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0600057\n",
      "\tspeed: 0.1133s/iter; left time: 3350.1827s\n",
      "Epoch: 9 cost time: 00h:05m:04.05s\n",
      "Epoch: 9 | Train Loss: 0.0669224 Vali Loss: 0.0591406 Test Loss: 0.0683875\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0737776\n",
      "\tspeed: 1.0245s/iter; left time: 30110.7750s\n",
      "\titers: 200, epoch: 10 | loss: 0.0724111\n",
      "\tspeed: 0.1137s/iter; left time: 3329.1865s\n",
      "\titers: 300, epoch: 10 | loss: 0.0616018\n",
      "\tspeed: 0.1138s/iter; left time: 3321.2091s\n",
      "\titers: 400, epoch: 10 | loss: 0.0517321\n",
      "\tspeed: 0.1130s/iter; left time: 3287.4474s\n",
      "\titers: 500, epoch: 10 | loss: 0.0636202\n",
      "\tspeed: 0.1132s/iter; left time: 3281.2723s\n",
      "\titers: 600, epoch: 10 | loss: 0.0686904\n",
      "\tspeed: 0.1132s/iter; left time: 3270.4862s\n",
      "\titers: 700, epoch: 10 | loss: 0.0717656\n",
      "\tspeed: 0.1130s/iter; left time: 3254.5680s\n",
      "\titers: 800, epoch: 10 | loss: 0.0680537\n",
      "\tspeed: 0.1145s/iter; left time: 3284.0216s\n",
      "\titers: 900, epoch: 10 | loss: 0.0653778\n",
      "\tspeed: 0.1123s/iter; left time: 3211.9863s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0478307\n",
      "\tspeed: 0.1131s/iter; left time: 3222.6684s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0693082\n",
      "\tspeed: 0.1129s/iter; left time: 3204.8677s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0719121\n",
      "\tspeed: 0.1142s/iter; left time: 3231.4140s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0745866\n",
      "\tspeed: 0.1141s/iter; left time: 3215.3624s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0660974\n",
      "\tspeed: 0.1120s/iter; left time: 3146.3648s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0666839\n",
      "\tspeed: 0.1148s/iter; left time: 3213.9156s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0883639\n",
      "\tspeed: 0.1116s/iter; left time: 3113.4350s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0611924\n",
      "\tspeed: 0.1139s/iter; left time: 3166.0606s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0584188\n",
      "\tspeed: 0.1134s/iter; left time: 3139.2218s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0601275\n",
      "\tspeed: 0.1138s/iter; left time: 3138.8746s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0625480\n",
      "\tspeed: 0.1141s/iter; left time: 3136.6568s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0705743\n",
      "\tspeed: 0.1130s/iter; left time: 3095.4025s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633123\n",
      "\tspeed: 0.1131s/iter; left time: 3086.8010s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0581706\n",
      "\tspeed: 0.1119s/iter; left time: 3042.6933s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0554645\n",
      "\tspeed: 0.1136s/iter; left time: 3076.7753s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0550835\n",
      "\tspeed: 0.1134s/iter; left time: 3060.9786s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0590457\n",
      "\tspeed: 0.1126s/iter; left time: 3028.8592s\n",
      "Epoch: 10 cost time: 00h:05m:03.96s\n",
      "Epoch: 10 | Train Loss: 0.0663446 Vali Loss: 0.0583170 Test Loss: 0.0684556\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0767739\n",
      "\tspeed: 1.0372s/iter; left time: 27704.6867s\n",
      "\titers: 200, epoch: 11 | loss: 0.0565607\n",
      "\tspeed: 0.1145s/iter; left time: 3047.2512s\n",
      "\titers: 300, epoch: 11 | loss: 0.0709955\n",
      "\tspeed: 0.1149s/iter; left time: 3046.1056s\n",
      "\titers: 400, epoch: 11 | loss: 0.0625823\n",
      "\tspeed: 0.1138s/iter; left time: 3004.5073s\n",
      "\titers: 500, epoch: 11 | loss: 0.0661458\n",
      "\tspeed: 0.1146s/iter; left time: 3014.9425s\n",
      "\titers: 600, epoch: 11 | loss: 0.0606733\n",
      "\tspeed: 0.1151s/iter; left time: 3016.2975s\n",
      "\titers: 700, epoch: 11 | loss: 0.0683432\n",
      "\tspeed: 0.1140s/iter; left time: 2975.9419s\n",
      "\titers: 800, epoch: 11 | loss: 0.0626959\n",
      "\tspeed: 0.1138s/iter; left time: 2959.4348s\n",
      "\titers: 900, epoch: 11 | loss: 0.0729644\n",
      "\tspeed: 0.1129s/iter; left time: 2924.6392s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0603989\n",
      "\tspeed: 0.1131s/iter; left time: 2919.0391s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0806362\n",
      "\tspeed: 0.1139s/iter; left time: 2928.9323s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0656467\n",
      "\tspeed: 0.1131s/iter; left time: 2896.3354s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0590451\n",
      "\tspeed: 0.1133s/iter; left time: 2890.3598s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0639197\n",
      "\tspeed: 0.1121s/iter; left time: 2848.7497s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0649276\n",
      "\tspeed: 0.1138s/iter; left time: 2881.1500s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0672397\n",
      "\tspeed: 0.1133s/iter; left time: 2856.4783s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0771958\n",
      "\tspeed: 0.1128s/iter; left time: 2832.8237s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0633096\n",
      "\tspeed: 0.1133s/iter; left time: 2833.8208s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0575457\n",
      "\tspeed: 0.1132s/iter; left time: 2819.3834s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0709095\n",
      "\tspeed: 0.1137s/iter; left time: 2821.8423s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0625228\n",
      "\tspeed: 0.1124s/iter; left time: 2776.7867s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0722681\n",
      "\tspeed: 0.1124s/iter; left time: 2765.1727s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0579663\n",
      "\tspeed: 0.1139s/iter; left time: 2790.6840s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0612664\n",
      "\tspeed: 0.1118s/iter; left time: 2728.5146s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0591755\n",
      "\tspeed: 0.1124s/iter; left time: 2732.7433s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0699752\n",
      "\tspeed: 0.1148s/iter; left time: 2778.2495s\n",
      "Epoch: 11 cost time: 00h:05m:04.77s\n",
      "Epoch: 11 | Train Loss: 0.0658757 Vali Loss: 0.0578032 Test Loss: 0.0670076\n",
      "Validation loss decreased (0.058111 --> 0.057803).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0724038\n",
      "\tspeed: 1.0561s/iter; left time: 25377.5917s\n",
      "\titers: 200, epoch: 12 | loss: 0.0599244\n",
      "\tspeed: 0.1167s/iter; left time: 2791.5539s\n",
      "\titers: 300, epoch: 12 | loss: 0.0652725\n",
      "\tspeed: 0.1146s/iter; left time: 2730.2562s\n",
      "\titers: 400, epoch: 12 | loss: 0.0576787\n",
      "\tspeed: 0.1151s/iter; left time: 2731.3073s\n",
      "\titers: 500, epoch: 12 | loss: 0.0657673\n",
      "\tspeed: 0.1162s/iter; left time: 2745.4736s\n",
      "\titers: 600, epoch: 12 | loss: 0.0662366\n",
      "\tspeed: 0.1159s/iter; left time: 2726.3024s\n",
      "\titers: 700, epoch: 12 | loss: 0.0807744\n",
      "\tspeed: 0.1156s/iter; left time: 2708.6135s\n",
      "\titers: 800, epoch: 12 | loss: 0.0683301\n",
      "\tspeed: 0.1149s/iter; left time: 2681.5224s\n",
      "\titers: 900, epoch: 12 | loss: 0.0653859\n",
      "\tspeed: 0.1139s/iter; left time: 2645.7059s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0721641\n",
      "\tspeed: 0.1158s/iter; left time: 2678.6462s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0629323\n",
      "\tspeed: 0.1133s/iter; left time: 2609.9367s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0567239\n",
      "\tspeed: 0.1160s/iter; left time: 2660.6701s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0589142\n",
      "\tspeed: 0.1145s/iter; left time: 2614.6360s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0682594\n",
      "\tspeed: 0.1172s/iter; left time: 2664.7349s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0660052\n",
      "\tspeed: 0.1145s/iter; left time: 2591.8247s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0597542\n",
      "\tspeed: 0.1149s/iter; left time: 2588.6791s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0569995\n",
      "\tspeed: 0.1162s/iter; left time: 2605.3605s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0621652\n",
      "\tspeed: 0.1139s/iter; left time: 2543.5189s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0574851\n",
      "\tspeed: 0.1154s/iter; left time: 2566.3783s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0754092\n",
      "\tspeed: 0.1168s/iter; left time: 2583.9628s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0705758\n",
      "\tspeed: 0.1139s/iter; left time: 2508.5002s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0638501\n",
      "\tspeed: 0.1144s/iter; left time: 2509.4028s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0607917\n",
      "\tspeed: 0.1150s/iter; left time: 2509.3821s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0704667\n",
      "\tspeed: 0.1134s/iter; left time: 2465.0269s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0649680\n",
      "\tspeed: 0.1152s/iter; left time: 2492.0219s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0622617\n",
      "\tspeed: 0.1135s/iter; left time: 2443.2286s\n",
      "Epoch: 12 cost time: 00h:05m:09.35s\n",
      "Epoch: 12 | Train Loss: 0.0654141 Vali Loss: 0.0592872 Test Loss: 0.0696159\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0571517\n",
      "\tspeed: 1.0461s/iter; left time: 22332.6199s\n",
      "\titers: 200, epoch: 13 | loss: 0.0563359\n",
      "\tspeed: 0.1160s/iter; left time: 2465.5303s\n",
      "\titers: 300, epoch: 13 | loss: 0.0752222\n",
      "\tspeed: 0.1144s/iter; left time: 2420.4436s\n",
      "\titers: 400, epoch: 13 | loss: 0.0552250\n",
      "\tspeed: 0.1159s/iter; left time: 2440.2397s\n",
      "\titers: 500, epoch: 13 | loss: 0.0716464\n",
      "\tspeed: 0.1161s/iter; left time: 2431.9619s\n",
      "\titers: 600, epoch: 13 | loss: 0.0713721\n",
      "\tspeed: 0.1143s/iter; left time: 2383.0557s\n",
      "\titers: 700, epoch: 13 | loss: 0.0675928\n",
      "\tspeed: 0.1141s/iter; left time: 2367.1671s\n",
      "\titers: 800, epoch: 13 | loss: 0.0716297\n",
      "\tspeed: 0.1131s/iter; left time: 2335.4889s\n",
      "\titers: 900, epoch: 13 | loss: 0.0656308\n",
      "\tspeed: 0.1131s/iter; left time: 2324.4347s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0822458\n",
      "\tspeed: 0.1156s/iter; left time: 2364.7438s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0752804\n",
      "\tspeed: 0.1129s/iter; left time: 2298.0280s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0652262\n",
      "\tspeed: 0.1172s/iter; left time: 2373.8849s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0598756\n",
      "\tspeed: 0.1151s/iter; left time: 2318.4410s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0691182\n",
      "\tspeed: 0.1149s/iter; left time: 2303.1170s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0635491\n",
      "\tspeed: 0.1145s/iter; left time: 2283.3282s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0895910\n",
      "\tspeed: 0.1124s/iter; left time: 2231.5296s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0710886\n",
      "\tspeed: 0.1151s/iter; left time: 2272.3943s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0558507\n",
      "\tspeed: 0.1159s/iter; left time: 2276.5376s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0647579\n",
      "\tspeed: 0.1128s/iter; left time: 2204.6798s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0736879\n",
      "\tspeed: 0.1139s/iter; left time: 2214.5192s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0558424\n",
      "\tspeed: 0.1143s/iter; left time: 2211.3625s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0630306\n",
      "\tspeed: 0.1142s/iter; left time: 2197.4649s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0580184\n",
      "\tspeed: 0.1155s/iter; left time: 2211.6236s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643669\n",
      "\tspeed: 0.1134s/iter; left time: 2160.5518s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0530160\n",
      "\tspeed: 0.1140s/iter; left time: 2160.1064s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0617090\n",
      "\tspeed: 0.1137s/iter; left time: 2142.7455s\n",
      "Epoch: 13 cost time: 00h:05m:07.74s\n",
      "Epoch: 13 | Train Loss: 0.0649905 Vali Loss: 0.0579648 Test Loss: 0.0673596\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0698010\n",
      "\tspeed: 1.0505s/iter; left time: 19609.8385s\n",
      "\titers: 200, epoch: 14 | loss: 0.0725076\n",
      "\tspeed: 0.1178s/iter; left time: 2187.9124s\n",
      "\titers: 300, epoch: 14 | loss: 0.0715118\n",
      "\tspeed: 0.1160s/iter; left time: 2143.0169s\n",
      "\titers: 400, epoch: 14 | loss: 0.0609961\n",
      "\tspeed: 0.1183s/iter; left time: 2172.8697s\n",
      "\titers: 500, epoch: 14 | loss: 0.0552948\n",
      "\tspeed: 0.1142s/iter; left time: 2087.0287s\n",
      "\titers: 600, epoch: 14 | loss: 0.0565410\n",
      "\tspeed: 0.1163s/iter; left time: 2113.0871s\n",
      "\titers: 700, epoch: 14 | loss: 0.0706945\n",
      "\tspeed: 0.1147s/iter; left time: 2072.3260s\n",
      "\titers: 800, epoch: 14 | loss: 0.0687059\n",
      "\tspeed: 0.1151s/iter; left time: 2067.9418s\n",
      "\titers: 900, epoch: 14 | loss: 0.0700648\n",
      "\tspeed: 0.1139s/iter; left time: 2035.3535s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0679078\n",
      "\tspeed: 0.1145s/iter; left time: 2034.5733s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0623200\n",
      "\tspeed: 0.1180s/iter; left time: 2084.1258s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0606130\n",
      "\tspeed: 0.1154s/iter; left time: 2026.6224s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0608021\n",
      "\tspeed: 0.1154s/iter; left time: 2015.2278s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0654804\n",
      "\tspeed: 0.1158s/iter; left time: 2011.5886s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0613914\n",
      "\tspeed: 0.1146s/iter; left time: 1978.1197s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0539721\n",
      "\tspeed: 0.1188s/iter; left time: 2040.0403s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0673941\n",
      "\tspeed: 0.1140s/iter; left time: 1945.7779s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0644400\n",
      "\tspeed: 0.1151s/iter; left time: 1952.6388s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0617564\n",
      "\tspeed: 0.1189s/iter; left time: 2004.8755s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0573019\n",
      "\tspeed: 0.1138s/iter; left time: 1907.5353s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0627567\n",
      "\tspeed: 0.1157s/iter; left time: 1927.8315s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0622240\n",
      "\tspeed: 0.1143s/iter; left time: 1894.0622s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0786616\n",
      "\tspeed: 0.1130s/iter; left time: 1860.1168s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0765208\n",
      "\tspeed: 0.1167s/iter; left time: 1909.5444s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0689429\n",
      "\tspeed: 0.1148s/iter; left time: 1867.9114s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0559283\n",
      "\tspeed: 0.1152s/iter; left time: 1862.1300s\n",
      "Epoch: 14 cost time: 00h:05m:10.55s\n",
      "Epoch: 14 | Train Loss: 0.0645770 Vali Loss: 0.0574409 Test Loss: 0.0664566\n",
      "Validation loss decreased (0.057803 --> 0.057441).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0564922\n",
      "\tspeed: 1.0504s/iter; left time: 16792.2142s\n",
      "\titers: 200, epoch: 15 | loss: 0.0621809\n",
      "\tspeed: 0.1152s/iter; left time: 1830.9058s\n",
      "\titers: 300, epoch: 15 | loss: 0.0641433\n",
      "\tspeed: 0.1150s/iter; left time: 1814.7649s\n",
      "\titers: 400, epoch: 15 | loss: 0.0578732\n",
      "\tspeed: 0.1162s/iter; left time: 1823.4217s\n",
      "\titers: 500, epoch: 15 | loss: 0.0655350\n",
      "\tspeed: 0.1155s/iter; left time: 1800.4331s\n",
      "\titers: 600, epoch: 15 | loss: 0.0686196\n",
      "\tspeed: 0.1140s/iter; left time: 1765.6390s\n",
      "\titers: 700, epoch: 15 | loss: 0.0824154\n",
      "\tspeed: 0.1150s/iter; left time: 1769.5573s\n",
      "\titers: 800, epoch: 15 | loss: 0.0650468\n",
      "\tspeed: 0.1172s/iter; left time: 1790.8980s\n",
      "\titers: 900, epoch: 15 | loss: 0.0688387\n",
      "\tspeed: 0.1168s/iter; left time: 1773.9111s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0771232\n",
      "\tspeed: 0.1136s/iter; left time: 1714.3128s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0594005\n",
      "\tspeed: 0.1151s/iter; left time: 1724.9768s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0690505\n",
      "\tspeed: 0.1168s/iter; left time: 1738.1331s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0685179\n",
      "\tspeed: 0.1155s/iter; left time: 1707.5917s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0714361\n",
      "\tspeed: 0.1157s/iter; left time: 1698.7921s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0690282\n",
      "\tspeed: 0.1158s/iter; left time: 1689.5013s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0619706\n",
      "\tspeed: 0.1154s/iter; left time: 1672.0780s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0708041\n",
      "\tspeed: 0.1165s/iter; left time: 1676.4341s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0704809\n",
      "\tspeed: 0.1164s/iter; left time: 1662.5459s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0758016\n",
      "\tspeed: 0.1157s/iter; left time: 1641.6784s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0559394\n",
      "\tspeed: 0.1133s/iter; left time: 1596.5836s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0584624\n",
      "\tspeed: 0.1185s/iter; left time: 1657.0702s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0783027\n",
      "\tspeed: 0.1177s/iter; left time: 1634.3422s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0668146\n",
      "\tspeed: 0.1158s/iter; left time: 1596.5994s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0637044\n",
      "\tspeed: 0.1169s/iter; left time: 1600.2406s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0603980\n",
      "\tspeed: 0.1133s/iter; left time: 1539.4111s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0710332\n",
      "\tspeed: 0.1172s/iter; left time: 1580.7251s\n",
      "Epoch: 15 cost time: 00h:05m:11.05s\n",
      "Epoch: 15 | Train Loss: 0.0641804 Vali Loss: 0.0605943 Test Loss: 0.0706357\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0574767\n",
      "\tspeed: 1.0832s/iter; left time: 14413.1043s\n",
      "\titers: 200, epoch: 16 | loss: 0.0573915\n",
      "\tspeed: 0.1138s/iter; left time: 1503.4549s\n",
      "\titers: 300, epoch: 16 | loss: 0.0506497\n",
      "\tspeed: 0.1197s/iter; left time: 1568.4148s\n",
      "\titers: 400, epoch: 16 | loss: 0.0601425\n",
      "\tspeed: 0.1245s/iter; left time: 1619.3223s\n",
      "\titers: 500, epoch: 16 | loss: 0.0669079\n",
      "\tspeed: 0.1170s/iter; left time: 1510.5657s\n",
      "\titers: 600, epoch: 16 | loss: 0.0705684\n",
      "\tspeed: 0.1265s/iter; left time: 1619.5410s\n",
      "\titers: 700, epoch: 16 | loss: 0.0603802\n",
      "\tspeed: 0.1172s/iter; left time: 1489.1899s\n",
      "\titers: 800, epoch: 16 | loss: 0.0707469\n",
      "\tspeed: 0.1186s/iter; left time: 1494.8500s\n",
      "\titers: 900, epoch: 16 | loss: 0.0820457\n",
      "\tspeed: 0.1133s/iter; left time: 1417.4639s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0660811\n",
      "\tspeed: 0.1144s/iter; left time: 1418.7206s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0602575\n",
      "\tspeed: 0.1211s/iter; left time: 1490.2298s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0644291\n",
      "\tspeed: 0.1163s/iter; left time: 1419.6789s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0740635\n",
      "\tspeed: 0.1166s/iter; left time: 1411.3760s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0634106\n",
      "\tspeed: 0.1186s/iter; left time: 1423.7740s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0686077\n",
      "\tspeed: 0.1129s/iter; left time: 1344.6050s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0558413\n",
      "\tspeed: 0.1153s/iter; left time: 1361.7066s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0561398\n",
      "\tspeed: 0.1176s/iter; left time: 1376.6157s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0640715\n",
      "\tspeed: 0.1212s/iter; left time: 1407.1287s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0615180\n",
      "\tspeed: 0.1137s/iter; left time: 1308.5983s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0579631\n",
      "\tspeed: 0.1218s/iter; left time: 1388.9642s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0742498\n",
      "\tspeed: 0.1371s/iter; left time: 1550.2575s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0673364\n",
      "\tspeed: 0.1217s/iter; left time: 1364.0987s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0641308\n",
      "\tspeed: 0.1156s/iter; left time: 1283.5332s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0701997\n",
      "\tspeed: 0.1180s/iter; left time: 1298.6027s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0703389\n",
      "\tspeed: 0.1198s/iter; left time: 1307.0178s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0659333\n",
      "\tspeed: 0.1143s/iter; left time: 1234.6238s\n",
      "Epoch: 16 cost time: 00h:05m:18.40s\n",
      "Epoch: 16 | Train Loss: 0.0638276 Vali Loss: 0.0593083 Test Loss: 0.0686335\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0632415\n",
      "\tspeed: 1.0655s/iter; left time: 11321.3357s\n",
      "\titers: 200, epoch: 17 | loss: 0.0688461\n",
      "\tspeed: 0.1274s/iter; left time: 1340.6059s\n",
      "\titers: 300, epoch: 17 | loss: 0.0631828\n",
      "\tspeed: 0.1190s/iter; left time: 1240.7238s\n",
      "\titers: 400, epoch: 17 | loss: 0.0736408\n",
      "\tspeed: 0.1165s/iter; left time: 1203.3470s\n",
      "\titers: 500, epoch: 17 | loss: 0.0770576\n",
      "\tspeed: 0.1157s/iter; left time: 1182.5244s\n",
      "\titers: 600, epoch: 17 | loss: 0.0678533\n",
      "\tspeed: 0.1210s/iter; left time: 1224.7393s\n",
      "\titers: 700, epoch: 17 | loss: 0.0549429\n",
      "\tspeed: 0.1198s/iter; left time: 1200.9915s\n",
      "\titers: 800, epoch: 17 | loss: 0.0515392\n",
      "\tspeed: 0.1164s/iter; left time: 1155.6964s\n",
      "\titers: 900, epoch: 17 | loss: 0.0676008\n",
      "\tspeed: 0.1183s/iter; left time: 1162.2364s\n",
      "\titers: 1000, epoch: 17 | loss: 0.0647659\n",
      "\tspeed: 0.1275s/iter; left time: 1239.9174s\n",
      "\titers: 1100, epoch: 17 | loss: 0.0694209\n",
      "\tspeed: 0.1152s/iter; left time: 1108.8508s\n",
      "\titers: 1200, epoch: 17 | loss: 0.0576050\n",
      "\tspeed: 0.1232s/iter; left time: 1173.7085s\n",
      "\titers: 1300, epoch: 17 | loss: 0.0682291\n",
      "\tspeed: 0.1285s/iter; left time: 1211.3266s\n",
      "\titers: 1400, epoch: 17 | loss: 0.0641747\n",
      "\tspeed: 0.1159s/iter; left time: 1080.6953s\n",
      "\titers: 1500, epoch: 17 | loss: 0.0657903\n",
      "\tspeed: 0.1203s/iter; left time: 1109.7518s\n",
      "\titers: 1600, epoch: 17 | loss: 0.0568783\n",
      "\tspeed: 0.1165s/iter; left time: 1063.3112s\n",
      "\titers: 1700, epoch: 17 | loss: 0.0658871\n",
      "\tspeed: 0.1160s/iter; left time: 1047.2816s\n",
      "\titers: 1800, epoch: 17 | loss: 0.0624544\n",
      "\tspeed: 0.1171s/iter; left time: 1044.9825s\n",
      "\titers: 1900, epoch: 17 | loss: 0.0611300\n",
      "\tspeed: 0.1184s/iter; left time: 1044.9180s\n",
      "\titers: 2000, epoch: 17 | loss: 0.0487223\n",
      "\tspeed: 0.1155s/iter; left time: 1007.8602s\n",
      "\titers: 2100, epoch: 17 | loss: 0.0584179\n",
      "\tspeed: 0.1158s/iter; left time: 998.5470s\n",
      "\titers: 2200, epoch: 17 | loss: 0.0644555\n",
      "\tspeed: 0.1157s/iter; left time: 986.0106s\n",
      "\titers: 2300, epoch: 17 | loss: 0.0529996\n",
      "\tspeed: 0.1187s/iter; left time: 1000.3853s\n",
      "\titers: 2400, epoch: 17 | loss: 0.0567144\n",
      "\tspeed: 0.1177s/iter; left time: 980.1682s\n",
      "\titers: 2500, epoch: 17 | loss: 0.0710186\n",
      "\tspeed: 0.1150s/iter; left time: 946.1899s\n",
      "\titers: 2600, epoch: 17 | loss: 0.0712463\n",
      "\tspeed: 0.1362s/iter; left time: 1106.6550s\n",
      "Epoch: 17 cost time: 00h:05m:20.52s\n",
      "Epoch: 17 | Train Loss: 0.0634753 Vali Loss: 0.0594251 Test Loss: 0.0680345\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0608606\n",
      "\tspeed: 1.0592s/iter; left time: 8414.4599s\n",
      "\titers: 200, epoch: 18 | loss: 0.0630744\n",
      "\tspeed: 0.1173s/iter; left time: 920.3750s\n",
      "\titers: 300, epoch: 18 | loss: 0.0573201\n",
      "\tspeed: 0.1164s/iter; left time: 901.6809s\n",
      "\titers: 400, epoch: 18 | loss: 0.0669892\n",
      "\tspeed: 0.1149s/iter; left time: 878.1572s\n",
      "\titers: 500, epoch: 18 | loss: 0.0642769\n",
      "\tspeed: 0.1175s/iter; left time: 886.4048s\n",
      "\titers: 600, epoch: 18 | loss: 0.0601702\n",
      "\tspeed: 0.1171s/iter; left time: 871.5879s\n",
      "\titers: 700, epoch: 18 | loss: 0.0813551\n",
      "\tspeed: 0.1213s/iter; left time: 891.1741s\n",
      "\titers: 800, epoch: 18 | loss: 0.0769780\n",
      "\tspeed: 0.1153s/iter; left time: 835.3674s\n",
      "\titers: 900, epoch: 18 | loss: 0.0527711\n",
      "\tspeed: 0.1147s/iter; left time: 819.4615s\n",
      "\titers: 1000, epoch: 18 | loss: 0.0642087\n",
      "\tspeed: 0.1179s/iter; left time: 830.5596s\n",
      "\titers: 1100, epoch: 18 | loss: 0.0753492\n",
      "\tspeed: 0.1156s/iter; left time: 802.4335s\n",
      "\titers: 1200, epoch: 18 | loss: 0.0503302\n",
      "\tspeed: 0.1153s/iter; left time: 789.4083s\n",
      "\titers: 1300, epoch: 18 | loss: 0.0638088\n",
      "\tspeed: 0.1176s/iter; left time: 793.3942s\n",
      "\titers: 1400, epoch: 18 | loss: 0.0659324\n",
      "\tspeed: 0.1136s/iter; left time: 755.0098s\n",
      "\titers: 1500, epoch: 18 | loss: 0.0653730\n",
      "\tspeed: 0.1465s/iter; left time: 958.7802s\n",
      "\titers: 1600, epoch: 18 | loss: 0.0739145\n",
      "\tspeed: 0.1208s/iter; left time: 778.5894s\n",
      "\titers: 1700, epoch: 18 | loss: 0.0535310\n",
      "\tspeed: 0.1176s/iter; left time: 746.3211s\n",
      "\titers: 1800, epoch: 18 | loss: 0.0655848\n",
      "\tspeed: 0.1177s/iter; left time: 735.2285s\n",
      "\titers: 1900, epoch: 18 | loss: 0.0589777\n",
      "\tspeed: 0.1218s/iter; left time: 748.3657s\n",
      "\titers: 2000, epoch: 18 | loss: 0.0685786\n",
      "\tspeed: 0.1147s/iter; left time: 693.5202s\n",
      "\titers: 2100, epoch: 18 | loss: 0.0555254\n",
      "\tspeed: 0.1135s/iter; left time: 674.5372s\n",
      "\titers: 2200, epoch: 18 | loss: 0.0631094\n",
      "\tspeed: 0.1178s/iter; left time: 688.4124s\n",
      "\titers: 2300, epoch: 18 | loss: 0.0528505\n",
      "\tspeed: 0.1173s/iter; left time: 673.8333s\n",
      "\titers: 2400, epoch: 18 | loss: 0.0643650\n",
      "\tspeed: 0.1150s/iter; left time: 648.8403s\n",
      "\titers: 2500, epoch: 18 | loss: 0.0647579\n",
      "\tspeed: 0.1146s/iter; left time: 635.5729s\n",
      "\titers: 2600, epoch: 18 | loss: 0.0699769\n",
      "\tspeed: 0.1192s/iter; left time: 649.0398s\n",
      "Epoch: 18 cost time: 00h:05m:16.65s\n",
      "Epoch: 18 | Train Loss: 0.0629919 Vali Loss: 0.0601041 Test Loss: 0.0698167\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0543833\n",
      "\tspeed: 1.0633s/iter; left time: 5595.9750s\n",
      "\titers: 200, epoch: 19 | loss: 0.0554405\n",
      "\tspeed: 0.1168s/iter; left time: 602.8683s\n",
      "\titers: 300, epoch: 19 | loss: 0.0701018\n",
      "\tspeed: 0.1165s/iter; left time: 589.9046s\n",
      "\titers: 400, epoch: 19 | loss: 0.0615413\n",
      "\tspeed: 0.1210s/iter; left time: 600.4576s\n",
      "\titers: 500, epoch: 19 | loss: 0.0614020\n",
      "\tspeed: 0.1151s/iter; left time: 559.5217s\n",
      "\titers: 600, epoch: 19 | loss: 0.0573202\n",
      "\tspeed: 0.1146s/iter; left time: 545.8222s\n",
      "\titers: 700, epoch: 19 | loss: 0.0611640\n",
      "\tspeed: 0.1167s/iter; left time: 544.2196s\n",
      "\titers: 800, epoch: 19 | loss: 0.0607653\n",
      "\tspeed: 0.1140s/iter; left time: 520.1647s\n",
      "\titers: 900, epoch: 19 | loss: 0.0566523\n",
      "\tspeed: 0.1147s/iter; left time: 512.0958s\n",
      "\titers: 1000, epoch: 19 | loss: 0.0622958\n",
      "\tspeed: 0.1178s/iter; left time: 514.1242s\n",
      "\titers: 1100, epoch: 19 | loss: 0.0550250\n",
      "\tspeed: 0.1133s/iter; left time: 483.1861s\n",
      "\titers: 1200, epoch: 19 | loss: 0.0663746\n",
      "\tspeed: 0.1155s/iter; left time: 480.6642s\n",
      "\titers: 1300, epoch: 19 | loss: 0.0650760\n",
      "\tspeed: 0.1151s/iter; left time: 467.5133s\n",
      "\titers: 1400, epoch: 19 | loss: 0.0588802\n",
      "\tspeed: 0.1159s/iter; left time: 459.2294s\n",
      "\titers: 1500, epoch: 19 | loss: 0.0603579\n",
      "\tspeed: 0.1202s/iter; left time: 464.4885s\n",
      "\titers: 1600, epoch: 19 | loss: 0.0569299\n",
      "\tspeed: 0.1162s/iter; left time: 437.3346s\n",
      "\titers: 1700, epoch: 19 | loss: 0.0491759\n",
      "\tspeed: 0.1141s/iter; left time: 417.7748s\n",
      "\titers: 1800, epoch: 19 | loss: 0.0864070\n",
      "\tspeed: 0.1154s/iter; left time: 411.2592s\n",
      "\titers: 1900, epoch: 19 | loss: 0.0639838\n",
      "\tspeed: 0.1170s/iter; left time: 405.0338s\n",
      "\titers: 2000, epoch: 19 | loss: 0.0655531\n",
      "\tspeed: 0.1135s/iter; left time: 381.5999s\n",
      "\titers: 2100, epoch: 19 | loss: 0.0550640\n",
      "\tspeed: 0.1187s/iter; left time: 387.3000s\n",
      "\titers: 2200, epoch: 19 | loss: 0.0624830\n",
      "\tspeed: 0.1146s/iter; left time: 362.3764s\n",
      "\titers: 2300, epoch: 19 | loss: 0.0577416\n",
      "\tspeed: 0.1153s/iter; left time: 353.3156s\n",
      "\titers: 2400, epoch: 19 | loss: 0.0651264\n",
      "\tspeed: 0.1176s/iter; left time: 348.4014s\n",
      "\titers: 2500, epoch: 19 | loss: 0.0614737\n",
      "\tspeed: 0.1146s/iter; left time: 328.0820s\n",
      "\titers: 2600, epoch: 19 | loss: 0.0525950\n",
      "\tspeed: 0.1160s/iter; left time: 320.5934s\n",
      "Epoch: 19 cost time: 00h:05m:11.75s\n",
      "Epoch: 19 | Train Loss: 0.0626410 Vali Loss: 0.0593832 Test Loss: 0.0687746\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010672738775610924, rmse:0.10330894589424133, mae:0.0664566159248352, rse:0.3033173382282257\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 24: 02h:06m:51.14s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 12:31:39,413] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 12:31:40,501] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 12:31:40,502] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 12:31:40,502] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 12:31:40,610] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 12:31:40,611] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 12:31:41,354] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 12:31:41,355] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 12:31:41,355] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 12:31:41,357] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 12:31:41,357] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 12:31:41,357] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 12:31:41,357] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 12:31:41,749] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 12:31:41,750] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:41,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.95 GB, percent = 54.9%\n",
      "[2024-11-03 12:31:42,004] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 12:31:42,006] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:42,006] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 415.18 GB, percent = 55.0%\n",
      "[2024-11-03 12:31:42,006] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 12:31:42,208] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 12:31:42,209] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 12:31:42,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 416.07 GB, percent = 55.1%\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 12:31:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 12:31:42,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 12:31:42,211] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 12:31:42,212] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f347c58d590>\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 12:31:42,213] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 12:31:42,214] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 12:31:42,215] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1911721\n",
      "\tspeed: 0.2250s/iter; left time: 12010.3608s\n",
      "\titers: 200, epoch: 1 | loss: 0.1927420\n",
      "\tspeed: 0.1312s/iter; left time: 6991.0572s\n",
      "\titers: 300, epoch: 1 | loss: 0.1694221\n",
      "\tspeed: 0.1279s/iter; left time: 6803.3074s\n",
      "\titers: 400, epoch: 1 | loss: 0.1720468\n",
      "\tspeed: 0.1280s/iter; left time: 6792.1212s\n",
      "\titers: 500, epoch: 1 | loss: 0.1218105\n",
      "\tspeed: 0.1259s/iter; left time: 6671.8709s\n",
      "\titers: 600, epoch: 1 | loss: 0.1114848\n",
      "\tspeed: 0.1327s/iter; left time: 7017.3328s\n",
      "\titers: 700, epoch: 1 | loss: 0.1140701\n",
      "\tspeed: 0.1261s/iter; left time: 6657.5580s\n",
      "\titers: 800, epoch: 1 | loss: 0.1041137\n",
      "\tspeed: 0.1255s/iter; left time: 6613.8594s\n",
      "\titers: 900, epoch: 1 | loss: 0.1242962\n",
      "\tspeed: 0.1287s/iter; left time: 6765.5096s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1122826\n",
      "\tspeed: 0.1289s/iter; left time: 6764.8339s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1078091\n",
      "\tspeed: 0.1265s/iter; left time: 6627.9105s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0886153\n",
      "\tspeed: 0.1313s/iter; left time: 6863.7768s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1067653\n",
      "\tspeed: 0.1269s/iter; left time: 6623.9458s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1063584\n",
      "\tspeed: 0.1258s/iter; left time: 6553.2690s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1006202\n",
      "\tspeed: 0.1279s/iter; left time: 6649.1573s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0992378\n",
      "\tspeed: 0.1268s/iter; left time: 6578.8237s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1159680\n",
      "\tspeed: 0.1262s/iter; left time: 6533.5496s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0989420\n",
      "\tspeed: 0.1287s/iter; left time: 6648.8436s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1210326\n",
      "\tspeed: 0.1272s/iter; left time: 6559.0411s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0897738\n",
      "\tspeed: 0.1263s/iter; left time: 6502.1211s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1054735\n",
      "\tspeed: 0.1254s/iter; left time: 6444.7193s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0856309\n",
      "\tspeed: 0.1277s/iter; left time: 6549.6717s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1068027\n",
      "\tspeed: 0.1290s/iter; left time: 6600.0312s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0811701\n",
      "\tspeed: 0.1258s/iter; left time: 6424.3765s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0951465\n",
      "\tspeed: 0.1272s/iter; left time: 6486.0592s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1027023\n",
      "\tspeed: 0.1281s/iter; left time: 6518.3456s\n",
      "Epoch: 1 cost time: 00h:05m:47.64s\n",
      "Epoch: 1 | Train Loss: 0.1174485 Vali Loss: 0.0865509 Test Loss: 0.1004628\n",
      "Validation loss decreased (inf --> 0.086551).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1076370\n",
      "\tspeed: 1.2004s/iter; left time: 60869.2661s\n",
      "\titers: 200, epoch: 2 | loss: 0.1094375\n",
      "\tspeed: 0.1190s/iter; left time: 6020.3157s\n",
      "\titers: 300, epoch: 2 | loss: 0.0885852\n",
      "\tspeed: 0.1138s/iter; left time: 5749.9829s\n",
      "\titers: 400, epoch: 2 | loss: 0.0921508\n",
      "\tspeed: 0.1151s/iter; left time: 5802.3581s\n",
      "\titers: 500, epoch: 2 | loss: 0.0874121\n",
      "\tspeed: 0.1181s/iter; left time: 5942.7607s\n",
      "\titers: 600, epoch: 2 | loss: 0.0942486\n",
      "\tspeed: 0.1156s/iter; left time: 5804.2839s\n",
      "\titers: 700, epoch: 2 | loss: 0.0875211\n",
      "\tspeed: 0.1157s/iter; left time: 5798.5184s\n",
      "\titers: 800, epoch: 2 | loss: 0.0975673\n",
      "\tspeed: 0.1169s/iter; left time: 5844.5824s\n",
      "\titers: 900, epoch: 2 | loss: 0.0873782\n",
      "\tspeed: 0.1145s/iter; left time: 5715.6672s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0910329\n",
      "\tspeed: 0.1148s/iter; left time: 5717.4644s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0909847\n",
      "\tspeed: 0.1147s/iter; left time: 5702.5887s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0977218\n",
      "\tspeed: 0.1141s/iter; left time: 5662.2298s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0851496\n",
      "\tspeed: 0.1118s/iter; left time: 5535.0687s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0878843\n",
      "\tspeed: 0.1116s/iter; left time: 5515.6216s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0895761\n",
      "\tspeed: 0.1254s/iter; left time: 6182.0052s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0866336\n",
      "\tspeed: 0.1183s/iter; left time: 5819.2809s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1094462\n",
      "\tspeed: 0.1163s/iter; left time: 5712.2202s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1022051\n",
      "\tspeed: 0.1145s/iter; left time: 5609.4801s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0886707\n",
      "\tspeed: 0.1180s/iter; left time: 5770.6991s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0902492\n",
      "\tspeed: 0.1182s/iter; left time: 5768.7588s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0888425\n",
      "\tspeed: 0.1190s/iter; left time: 5794.1530s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0878865\n",
      "\tspeed: 0.1177s/iter; left time: 5722.5241s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0795095\n",
      "\tspeed: 0.1194s/iter; left time: 5789.5118s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0914220\n",
      "\tspeed: 0.1162s/iter; left time: 5622.8384s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0818894\n",
      "\tspeed: 0.1161s/iter; left time: 5607.8173s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0820524\n",
      "\tspeed: 0.1184s/iter; left time: 5705.3554s\n",
      "Epoch: 2 cost time: 00h:05m:12.15s\n",
      "Epoch: 2 | Train Loss: 0.0935248 Vali Loss: 0.0811140 Test Loss: 0.0944012\n",
      "Validation loss decreased (0.086551 --> 0.081114).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0952333\n",
      "\tspeed: 1.0778s/iter; left time: 51768.2510s\n",
      "\titers: 200, epoch: 3 | loss: 0.0934332\n",
      "\tspeed: 0.1083s/iter; left time: 5191.1988s\n",
      "\titers: 300, epoch: 3 | loss: 0.0905903\n",
      "\tspeed: 0.1008s/iter; left time: 4819.9429s\n",
      "\titers: 400, epoch: 3 | loss: 0.0940271\n",
      "\tspeed: 0.1110s/iter; left time: 5296.5709s\n",
      "\titers: 500, epoch: 3 | loss: 0.0887500\n",
      "\tspeed: 0.0976s/iter; left time: 4648.3151s\n",
      "\titers: 600, epoch: 3 | loss: 0.0991548\n",
      "\tspeed: 0.0925s/iter; left time: 4397.1942s\n",
      "\titers: 700, epoch: 3 | loss: 0.0992021\n",
      "\tspeed: 0.0981s/iter; left time: 4651.3183s\n",
      "\titers: 800, epoch: 3 | loss: 0.0928352\n",
      "\tspeed: 0.0910s/iter; left time: 4306.5169s\n",
      "\titers: 900, epoch: 3 | loss: 0.0962132\n",
      "\tspeed: 0.0898s/iter; left time: 4241.9536s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963722\n",
      "\tspeed: 0.0940s/iter; left time: 4430.7202s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0940291\n",
      "\tspeed: 0.0908s/iter; left time: 4269.6858s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0946871\n",
      "\tspeed: 0.0876s/iter; left time: 4112.2190s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0897035\n",
      "\tspeed: 0.0935s/iter; left time: 4379.2322s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0817999\n",
      "\tspeed: 0.1109s/iter; left time: 5182.1395s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0934589\n",
      "\tspeed: 0.1107s/iter; left time: 5164.5937s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1075991\n",
      "\tspeed: 0.1093s/iter; left time: 5087.5857s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029885\n",
      "\tspeed: 0.1141s/iter; left time: 5297.1691s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0796154\n",
      "\tspeed: 0.1162s/iter; left time: 5385.4164s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1007038\n",
      "\tspeed: 0.1152s/iter; left time: 5328.2288s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0810669\n",
      "\tspeed: 0.1161s/iter; left time: 5353.9627s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1120119\n",
      "\tspeed: 0.1162s/iter; left time: 5349.2830s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1014793\n",
      "\tspeed: 0.1159s/iter; left time: 5322.9236s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0782545\n",
      "\tspeed: 0.1156s/iter; left time: 5298.5233s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0805253\n",
      "\tspeed: 0.1144s/iter; left time: 5229.9599s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0871867\n",
      "\tspeed: 0.1147s/iter; left time: 5233.2774s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0826894\n",
      "\tspeed: 0.1139s/iter; left time: 5186.5886s\n",
      "Epoch: 3 cost time: 00h:04m:44.91s\n",
      "Epoch: 3 | Train Loss: 0.0897070 Vali Loss: 0.0801801 Test Loss: 0.0940483\n",
      "Validation loss decreased (0.081114 --> 0.080180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0769089\n",
      "\tspeed: 1.0491s/iter; left time: 47586.7382s\n",
      "\titers: 200, epoch: 4 | loss: 0.0909401\n",
      "\tspeed: 0.1141s/iter; left time: 5162.4431s\n",
      "\titers: 300, epoch: 4 | loss: 0.0769373\n",
      "\tspeed: 0.1169s/iter; left time: 5280.5800s\n",
      "\titers: 400, epoch: 4 | loss: 0.0873026\n",
      "\tspeed: 0.1158s/iter; left time: 5219.3674s\n",
      "\titers: 500, epoch: 4 | loss: 0.0947193\n",
      "\tspeed: 0.1174s/iter; left time: 5280.2416s\n",
      "\titers: 600, epoch: 4 | loss: 0.0831833\n",
      "\tspeed: 0.1155s/iter; left time: 5180.0694s\n",
      "\titers: 700, epoch: 4 | loss: 0.0980527\n",
      "\tspeed: 0.1161s/iter; left time: 5194.4162s\n",
      "\titers: 800, epoch: 4 | loss: 0.0867212\n",
      "\tspeed: 0.1164s/iter; left time: 5197.0489s\n",
      "\titers: 900, epoch: 4 | loss: 0.0815352\n",
      "\tspeed: 0.1140s/iter; left time: 5078.6952s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0801502\n",
      "\tspeed: 0.1139s/iter; left time: 5064.2236s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0883447\n",
      "\tspeed: 0.1138s/iter; left time: 5048.9823s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0877123\n",
      "\tspeed: 0.1137s/iter; left time: 5032.9758s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0836246\n",
      "\tspeed: 0.1149s/iter; left time: 5076.0069s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0986883\n",
      "\tspeed: 0.1153s/iter; left time: 5082.1712s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0836472\n",
      "\tspeed: 0.1137s/iter; left time: 4999.1454s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0853868\n",
      "\tspeed: 0.1142s/iter; left time: 5007.7051s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0907262\n",
      "\tspeed: 0.1127s/iter; left time: 4932.0601s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0784356\n",
      "\tspeed: 0.1122s/iter; left time: 4899.6599s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0888463\n",
      "\tspeed: 0.1158s/iter; left time: 5043.7710s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0986566\n",
      "\tspeed: 0.1152s/iter; left time: 5008.2468s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0922876\n",
      "\tspeed: 0.1123s/iter; left time: 4870.9778s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0821516\n",
      "\tspeed: 0.1154s/iter; left time: 4991.4455s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0900341\n",
      "\tspeed: 0.1147s/iter; left time: 4952.2516s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0951754\n",
      "\tspeed: 0.1142s/iter; left time: 4916.3450s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0837031\n",
      "\tspeed: 0.1139s/iter; left time: 4891.1125s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0842158\n",
      "\tspeed: 0.1163s/iter; left time: 4986.0906s\n",
      "Epoch: 4 cost time: 00h:05m:07.49s\n",
      "Epoch: 4 | Train Loss: 0.0869057 Vali Loss: 0.0800106 Test Loss: 0.0955628\n",
      "Validation loss decreased (0.080180 --> 0.080011).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0871619\n",
      "\tspeed: 1.0465s/iter; left time: 44668.5079s\n",
      "\titers: 200, epoch: 5 | loss: 0.1035479\n",
      "\tspeed: 0.1166s/iter; left time: 4966.4781s\n",
      "\titers: 300, epoch: 5 | loss: 0.0778850\n",
      "\tspeed: 0.1167s/iter; left time: 4960.0059s\n",
      "\titers: 400, epoch: 5 | loss: 0.0712645\n",
      "\tspeed: 0.1169s/iter; left time: 4953.1657s\n",
      "\titers: 500, epoch: 5 | loss: 0.0783551\n",
      "\tspeed: 0.1165s/iter; left time: 4925.0533s\n",
      "\titers: 600, epoch: 5 | loss: 0.0905214\n",
      "\tspeed: 0.1162s/iter; left time: 4901.8905s\n",
      "\titers: 700, epoch: 5 | loss: 0.0817809\n",
      "\tspeed: 0.1151s/iter; left time: 4844.6803s\n",
      "\titers: 800, epoch: 5 | loss: 0.0813011\n",
      "\tspeed: 0.1165s/iter; left time: 4890.1612s\n",
      "\titers: 900, epoch: 5 | loss: 0.0899853\n",
      "\tspeed: 0.1141s/iter; left time: 4779.5406s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0864681\n",
      "\tspeed: 0.1146s/iter; left time: 4789.0321s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0875253\n",
      "\tspeed: 0.1164s/iter; left time: 4852.1821s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0901085\n",
      "\tspeed: 0.1122s/iter; left time: 4666.6779s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0639921\n",
      "\tspeed: 0.1127s/iter; left time: 4674.1220s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0969867\n",
      "\tspeed: 0.1155s/iter; left time: 4780.2230s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0761939\n",
      "\tspeed: 0.1140s/iter; left time: 4707.7693s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0842133\n",
      "\tspeed: 0.1135s/iter; left time: 4674.0158s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0735740\n",
      "\tspeed: 0.1144s/iter; left time: 4701.2770s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0814190\n",
      "\tspeed: 0.1134s/iter; left time: 4647.7697s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0850187\n",
      "\tspeed: 0.1138s/iter; left time: 4651.2324s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0746921\n",
      "\tspeed: 0.1143s/iter; left time: 4661.2029s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0881317\n",
      "\tspeed: 0.1139s/iter; left time: 4633.6248s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0845793\n",
      "\tspeed: 0.1158s/iter; left time: 4699.8150s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0848546\n",
      "\tspeed: 0.1141s/iter; left time: 4618.5652s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0750976\n",
      "\tspeed: 0.1138s/iter; left time: 4594.6637s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0834830\n",
      "\tspeed: 0.1144s/iter; left time: 4609.4049s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0875506\n",
      "\tspeed: 0.1140s/iter; left time: 4579.0886s\n",
      "Epoch: 5 cost time: 00h:05m:07.39s\n",
      "Epoch: 5 | Train Loss: 0.0848302 Vali Loss: 0.0812559 Test Loss: 0.0953243\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1068747\n",
      "\tspeed: 1.0150s/iter; left time: 40609.2939s\n",
      "\titers: 200, epoch: 6 | loss: 0.0783328\n",
      "\tspeed: 0.1153s/iter; left time: 4602.4415s\n",
      "\titers: 300, epoch: 6 | loss: 0.0739404\n",
      "\tspeed: 0.1158s/iter; left time: 4608.7733s\n",
      "\titers: 400, epoch: 6 | loss: 0.0745426\n",
      "\tspeed: 0.1151s/iter; left time: 4570.9422s\n",
      "\titers: 500, epoch: 6 | loss: 0.0807710\n",
      "\tspeed: 0.1141s/iter; left time: 4521.2147s\n",
      "\titers: 600, epoch: 6 | loss: 0.0781820\n",
      "\tspeed: 0.1145s/iter; left time: 4522.0859s\n",
      "\titers: 700, epoch: 6 | loss: 0.0852243\n",
      "\tspeed: 0.1138s/iter; left time: 4486.1345s\n",
      "\titers: 800, epoch: 6 | loss: 0.0808892\n",
      "\tspeed: 0.1159s/iter; left time: 4556.8919s\n",
      "\titers: 900, epoch: 6 | loss: 0.0863012\n",
      "\tspeed: 0.1144s/iter; left time: 4487.2771s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0845646\n",
      "\tspeed: 0.1148s/iter; left time: 4490.8618s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0786446\n",
      "\tspeed: 0.1156s/iter; left time: 4508.3062s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0738035\n",
      "\tspeed: 0.1137s/iter; left time: 4424.7178s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0962725\n",
      "\tspeed: 0.1139s/iter; left time: 4419.6932s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0873117\n",
      "\tspeed: 0.1157s/iter; left time: 4477.5946s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0892730\n",
      "\tspeed: 0.1148s/iter; left time: 4430.8446s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0773154\n",
      "\tspeed: 0.1143s/iter; left time: 4400.2482s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0777241\n",
      "\tspeed: 0.1152s/iter; left time: 4426.3397s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0818046\n",
      "\tspeed: 0.1160s/iter; left time: 4445.0211s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0856868\n",
      "\tspeed: 0.1147s/iter; left time: 4384.3465s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849679\n",
      "\tspeed: 0.1143s/iter; left time: 4357.2855s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0735170\n",
      "\tspeed: 0.1158s/iter; left time: 4401.3630s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0838584\n",
      "\tspeed: 0.1133s/iter; left time: 4295.5436s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0831294\n",
      "\tspeed: 0.1176s/iter; left time: 4447.5343s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0829203\n",
      "\tspeed: 0.1143s/iter; left time: 4309.3404s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0855648\n",
      "\tspeed: 0.1149s/iter; left time: 4320.9223s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0735175\n",
      "\tspeed: 0.1140s/iter; left time: 4276.1456s\n",
      "Epoch: 6 cost time: 00h:05m:07.62s\n",
      "Epoch: 6 | Train Loss: 0.0828304 Vali Loss: 0.0819640 Test Loss: 0.0980078\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0902265\n",
      "\tspeed: 1.0371s/iter; left time: 38722.3254s\n",
      "\titers: 200, epoch: 7 | loss: 0.0986805\n",
      "\tspeed: 0.1142s/iter; left time: 4250.9448s\n",
      "\titers: 300, epoch: 7 | loss: 0.0778703\n",
      "\tspeed: 0.1127s/iter; left time: 4186.3723s\n",
      "\titers: 400, epoch: 7 | loss: 0.0782931\n",
      "\tspeed: 0.1160s/iter; left time: 4295.4712s\n",
      "\titers: 500, epoch: 7 | loss: 0.0746187\n",
      "\tspeed: 0.1161s/iter; left time: 4289.4106s\n",
      "\titers: 600, epoch: 7 | loss: 0.0827935\n",
      "\tspeed: 0.1153s/iter; left time: 4245.6709s\n",
      "\titers: 700, epoch: 7 | loss: 0.0739331\n",
      "\tspeed: 0.1138s/iter; left time: 4180.5216s\n",
      "\titers: 800, epoch: 7 | loss: 0.0809667\n",
      "\tspeed: 0.1159s/iter; left time: 4245.3415s\n",
      "\titers: 900, epoch: 7 | loss: 0.0907432\n",
      "\tspeed: 0.1135s/iter; left time: 4148.0988s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0945175\n",
      "\tspeed: 0.1139s/iter; left time: 4151.0506s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0788761\n",
      "\tspeed: 0.1161s/iter; left time: 4218.0290s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0823496\n",
      "\tspeed: 0.1136s/iter; left time: 4115.4184s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0750478\n",
      "\tspeed: 0.1144s/iter; left time: 4134.4843s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0663611\n",
      "\tspeed: 0.1139s/iter; left time: 4105.3062s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0791697\n",
      "\tspeed: 0.1154s/iter; left time: 4146.3280s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0843642\n",
      "\tspeed: 0.1140s/iter; left time: 4085.1494s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0820326\n",
      "\tspeed: 0.1147s/iter; left time: 4097.9003s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0798094\n",
      "\tspeed: 0.1149s/iter; left time: 4093.0102s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0762156\n",
      "\tspeed: 0.1138s/iter; left time: 4045.6408s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0743921\n",
      "\tspeed: 0.1145s/iter; left time: 4058.3544s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0891102\n",
      "\tspeed: 0.1150s/iter; left time: 4062.2655s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0900708\n",
      "\tspeed: 0.1123s/iter; left time: 3958.3714s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0966768\n",
      "\tspeed: 0.1148s/iter; left time: 4035.3253s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0648704\n",
      "\tspeed: 0.1094s/iter; left time: 3832.9021s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0843473\n",
      "\tspeed: 0.1263s/iter; left time: 4411.6532s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0859990\n",
      "\tspeed: 0.1143s/iter; left time: 3981.3376s\n",
      "Epoch: 7 cost time: 00h:05m:07.73s\n",
      "Epoch: 7 | Train Loss: 0.0812575 Vali Loss: 0.0815965 Test Loss: 0.0964520\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0794488\n",
      "\tspeed: 1.0163s/iter; left time: 35228.0566s\n",
      "\titers: 200, epoch: 8 | loss: 0.0848540\n",
      "\tspeed: 0.1132s/iter; left time: 3911.2718s\n",
      "\titers: 300, epoch: 8 | loss: 0.0769302\n",
      "\tspeed: 0.1125s/iter; left time: 3876.9898s\n",
      "\titers: 400, epoch: 8 | loss: 0.0856332\n",
      "\tspeed: 0.1128s/iter; left time: 3877.8341s\n",
      "\titers: 500, epoch: 8 | loss: 0.0824820\n",
      "\tspeed: 0.1137s/iter; left time: 3895.0860s\n",
      "\titers: 600, epoch: 8 | loss: 0.0854290\n",
      "\tspeed: 0.1154s/iter; left time: 3943.4592s\n",
      "\titers: 700, epoch: 8 | loss: 0.0854288\n",
      "\tspeed: 0.1132s/iter; left time: 3857.4683s\n",
      "\titers: 800, epoch: 8 | loss: 0.0680083\n",
      "\tspeed: 0.1152s/iter; left time: 3914.0443s\n",
      "\titers: 900, epoch: 8 | loss: 0.0768383\n",
      "\tspeed: 0.1128s/iter; left time: 3819.4727s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0693779\n",
      "\tspeed: 0.1141s/iter; left time: 3852.3100s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0754587\n",
      "\tspeed: 0.1131s/iter; left time: 3805.9977s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0802685\n",
      "\tspeed: 0.1123s/iter; left time: 3770.7072s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0774171\n",
      "\tspeed: 0.1106s/iter; left time: 3700.4081s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754229\n",
      "\tspeed: 0.1129s/iter; left time: 3767.4139s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0888831\n",
      "\tspeed: 0.1134s/iter; left time: 3771.3276s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0705786\n",
      "\tspeed: 0.1153s/iter; left time: 3823.0475s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0788224\n",
      "\tspeed: 0.1161s/iter; left time: 3838.7923s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0689727\n",
      "\tspeed: 0.1123s/iter; left time: 3702.2267s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0841972\n",
      "\tspeed: 0.1137s/iter; left time: 3736.1949s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0773610\n",
      "\tspeed: 0.1131s/iter; left time: 3705.2475s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0929009\n",
      "\tspeed: 0.1137s/iter; left time: 3715.3777s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0882238\n",
      "\tspeed: 0.1135s/iter; left time: 3697.1907s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0731173\n",
      "\tspeed: 0.1146s/iter; left time: 3718.7553s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0878904\n",
      "\tspeed: 0.1116s/iter; left time: 3611.7746s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0880044\n",
      "\tspeed: 0.1123s/iter; left time: 3624.7379s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0914146\n",
      "\tspeed: 0.1131s/iter; left time: 3638.1944s\n",
      "Epoch: 8 cost time: 00h:05m:03.87s\n",
      "Epoch: 8 | Train Loss: 0.0798089 Vali Loss: 0.0830694 Test Loss: 0.0950187\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0826529\n",
      "\tspeed: 1.0051s/iter; left time: 32153.6951s\n",
      "\titers: 200, epoch: 9 | loss: 0.0691043\n",
      "\tspeed: 0.1136s/iter; left time: 3623.4213s\n",
      "\titers: 300, epoch: 9 | loss: 0.0827234\n",
      "\tspeed: 0.1129s/iter; left time: 3589.9077s\n",
      "\titers: 400, epoch: 9 | loss: 0.0850475\n",
      "\tspeed: 0.1135s/iter; left time: 3597.6932s\n",
      "\titers: 500, epoch: 9 | loss: 0.0775021\n",
      "\tspeed: 0.1146s/iter; left time: 3619.6377s\n",
      "\titers: 600, epoch: 9 | loss: 0.0795878\n",
      "\tspeed: 0.1145s/iter; left time: 3605.7645s\n",
      "\titers: 700, epoch: 9 | loss: 0.0837808\n",
      "\tspeed: 0.1141s/iter; left time: 3581.4114s\n",
      "\titers: 800, epoch: 9 | loss: 0.0798738\n",
      "\tspeed: 0.1141s/iter; left time: 3568.5339s\n",
      "\titers: 900, epoch: 9 | loss: 0.0779057\n",
      "\tspeed: 0.1147s/iter; left time: 3576.2283s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0727953\n",
      "\tspeed: 0.1138s/iter; left time: 3537.9186s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0807575\n",
      "\tspeed: 0.1118s/iter; left time: 3463.9758s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0787443\n",
      "\tspeed: 0.1131s/iter; left time: 3492.1398s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0788000\n",
      "\tspeed: 0.1126s/iter; left time: 3466.7881s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0729088\n",
      "\tspeed: 0.1134s/iter; left time: 3480.7516s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0709907\n",
      "\tspeed: 0.1127s/iter; left time: 3447.6636s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0749613\n",
      "\tspeed: 0.1132s/iter; left time: 3450.1363s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0788948\n",
      "\tspeed: 0.1122s/iter; left time: 3410.4812s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0830306\n",
      "\tspeed: 0.1127s/iter; left time: 3412.3783s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0778918\n",
      "\tspeed: 0.1129s/iter; left time: 3409.4986s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0789471\n",
      "\tspeed: 0.1115s/iter; left time: 3355.3329s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0754702\n",
      "\tspeed: 0.1164s/iter; left time: 3489.8922s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0705925\n",
      "\tspeed: 0.1147s/iter; left time: 3428.4263s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0731009\n",
      "\tspeed: 0.1136s/iter; left time: 3383.9702s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0785734\n",
      "\tspeed: 0.1141s/iter; left time: 3388.0698s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0922928\n",
      "\tspeed: 0.1156s/iter; left time: 3419.5757s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0796332\n",
      "\tspeed: 0.1155s/iter; left time: 3406.9806s\n",
      "Epoch: 9 cost time: 00h:05m:04.53s\n",
      "Epoch: 9 | Train Loss: 0.0784978 Vali Loss: 0.0850671 Test Loss: 0.0985102\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02088882215321064, rmse:0.14452965557575226, mae:0.09556279331445694, rse:0.4245525300502777\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 96: 01h:00m:14.20s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 13:31:51,060] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 13:31:51,984] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 13:31:51,984] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 13:31:51,984] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 13:31:52,087] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 13:31:52,087] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 13:31:52,844] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 13:31:52,845] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 13:31:52,845] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 13:31:52,846] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 13:31:52,846] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 13:31:52,846] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 13:31:52,846] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 13:31:52,847] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 13:31:53,247] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 13:31:53,248] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,248] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.46 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,434] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 13:31:53,435] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,435] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.48 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,435] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 13:31:53,587] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 13:31:53,588] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 13:31:53,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.46 GB, percent = 10.1%\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 13:31:53,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 13:31:53,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 13:31:53,590] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efcb907f850>\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 13:31:53,591] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 13:31:53,592] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 13:31:53,593] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2009432\n",
      "\tspeed: 0.2035s/iter; left time: 10833.1145s\n",
      "\titers: 200, epoch: 1 | loss: 0.2018673\n",
      "\tspeed: 0.1293s/iter; left time: 6868.6140s\n",
      "\titers: 300, epoch: 1 | loss: 0.1664647\n",
      "\tspeed: 0.1290s/iter; left time: 6843.2504s\n",
      "\titers: 400, epoch: 1 | loss: 0.1907074\n",
      "\tspeed: 0.1283s/iter; left time: 6792.1821s\n",
      "\titers: 500, epoch: 1 | loss: 0.1524435\n",
      "\tspeed: 0.1282s/iter; left time: 6775.9449s\n",
      "\titers: 600, epoch: 1 | loss: 0.1277965\n",
      "\tspeed: 0.1273s/iter; left time: 6713.1010s\n",
      "\titers: 700, epoch: 1 | loss: 0.1040149\n",
      "\tspeed: 0.1279s/iter; left time: 6735.2929s\n",
      "\titers: 800, epoch: 1 | loss: 0.1075614\n",
      "\tspeed: 0.1278s/iter; left time: 6714.1902s\n",
      "\titers: 900, epoch: 1 | loss: 0.1123559\n",
      "\tspeed: 0.1285s/iter; left time: 6738.3145s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1132720\n",
      "\tspeed: 0.1271s/iter; left time: 6650.2271s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0960071\n",
      "\tspeed: 0.1284s/iter; left time: 6706.3182s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1164301\n",
      "\tspeed: 0.1278s/iter; left time: 6661.5176s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1128735\n",
      "\tspeed: 0.1285s/iter; left time: 6687.8148s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1040649\n",
      "\tspeed: 0.1297s/iter; left time: 6737.2592s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1173225\n",
      "\tspeed: 0.1280s/iter; left time: 6633.6710s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1031440\n",
      "\tspeed: 0.1295s/iter; left time: 6699.5237s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0966299\n",
      "\tspeed: 0.1276s/iter; left time: 6591.9157s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1064342\n",
      "\tspeed: 0.1291s/iter; left time: 6654.9832s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0987400\n",
      "\tspeed: 0.1275s/iter; left time: 6559.7447s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1042845\n",
      "\tspeed: 0.1294s/iter; left time: 6643.9934s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0977351\n",
      "\tspeed: 0.1279s/iter; left time: 6552.6752s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0965085\n",
      "\tspeed: 0.1264s/iter; left time: 6463.5276s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0990303\n",
      "\tspeed: 0.1272s/iter; left time: 6491.7162s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1100298\n",
      "\tspeed: 0.1277s/iter; left time: 6505.7362s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0987217\n",
      "\tspeed: 0.1287s/iter; left time: 6541.4817s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1110397\n",
      "\tspeed: 0.1283s/iter; left time: 6507.8580s\n",
      "Epoch: 1 cost time: 00h:05m:46.31s\n",
      "Epoch: 1 | Train Loss: 0.1213860 Vali Loss: 0.0894020 Test Loss: 0.1034134\n",
      "Validation loss decreased (inf --> 0.089402).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0940944\n",
      "\tspeed: 1.1672s/iter; left time: 59029.4659s\n",
      "\titers: 200, epoch: 2 | loss: 0.0937534\n",
      "\tspeed: 0.1127s/iter; left time: 5687.0277s\n",
      "\titers: 300, epoch: 2 | loss: 0.1080840\n",
      "\tspeed: 0.1129s/iter; left time: 5685.3026s\n",
      "\titers: 400, epoch: 2 | loss: 0.0966061\n",
      "\tspeed: 0.1141s/iter; left time: 5735.0344s\n",
      "\titers: 500, epoch: 2 | loss: 0.1093938\n",
      "\tspeed: 0.1147s/iter; left time: 5756.6175s\n",
      "\titers: 600, epoch: 2 | loss: 0.0923856\n",
      "\tspeed: 0.1165s/iter; left time: 5835.2127s\n",
      "\titers: 700, epoch: 2 | loss: 0.0916145\n",
      "\tspeed: 0.1149s/iter; left time: 5742.0539s\n",
      "\titers: 800, epoch: 2 | loss: 0.0939326\n",
      "\tspeed: 0.1145s/iter; left time: 5708.3946s\n",
      "\titers: 900, epoch: 2 | loss: 0.1001902\n",
      "\tspeed: 0.1131s/iter; left time: 5629.7220s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0946725\n",
      "\tspeed: 0.1164s/iter; left time: 5780.2450s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0976080\n",
      "\tspeed: 0.1134s/iter; left time: 5622.9334s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017308\n",
      "\tspeed: 0.1144s/iter; left time: 5662.1506s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1020761\n",
      "\tspeed: 0.1132s/iter; left time: 5588.9233s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0914178\n",
      "\tspeed: 0.1127s/iter; left time: 5554.0966s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1076127\n",
      "\tspeed: 0.1107s/iter; left time: 5443.4879s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0961113\n",
      "\tspeed: 0.1147s/iter; left time: 5627.7270s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1049188\n",
      "\tspeed: 0.1157s/iter; left time: 5664.6585s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0905886\n",
      "\tspeed: 0.1133s/iter; left time: 5536.0354s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0959672\n",
      "\tspeed: 0.1155s/iter; left time: 5631.1644s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0967659\n",
      "\tspeed: 0.1152s/iter; left time: 5605.1883s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1091169\n",
      "\tspeed: 0.1165s/iter; left time: 5656.5862s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0912857\n",
      "\tspeed: 0.1129s/iter; left time: 5471.1685s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0814347\n",
      "\tspeed: 0.1148s/iter; left time: 5555.4270s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1000843\n",
      "\tspeed: 0.1147s/iter; left time: 5537.4035s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0908363\n",
      "\tspeed: 0.1124s/iter; left time: 5415.2498s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0960613\n",
      "\tspeed: 0.1152s/iter; left time: 5539.6381s\n",
      "Epoch: 2 cost time: 00h:05m:05.02s\n",
      "Epoch: 2 | Train Loss: 0.0975315 Vali Loss: 0.0867351 Test Loss: 0.0988254\n",
      "Validation loss decreased (0.089402 --> 0.086735).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0956383\n",
      "\tspeed: 1.0260s/iter; left time: 49153.2021s\n",
      "\titers: 200, epoch: 3 | loss: 0.0700900\n",
      "\tspeed: 0.1136s/iter; left time: 5432.9049s\n",
      "\titers: 300, epoch: 3 | loss: 0.0907799\n",
      "\tspeed: 0.1157s/iter; left time: 5521.0043s\n",
      "\titers: 400, epoch: 3 | loss: 0.1059120\n",
      "\tspeed: 0.1128s/iter; left time: 5371.1919s\n",
      "\titers: 500, epoch: 3 | loss: 0.0847025\n",
      "\tspeed: 0.1155s/iter; left time: 5489.1741s\n",
      "\titers: 600, epoch: 3 | loss: 0.1060325\n",
      "\tspeed: 0.1141s/iter; left time: 5408.1395s\n",
      "\titers: 700, epoch: 3 | loss: 0.0842078\n",
      "\tspeed: 0.1145s/iter; left time: 5418.1948s\n",
      "\titers: 800, epoch: 3 | loss: 0.1065284\n",
      "\tspeed: 0.1155s/iter; left time: 5451.8367s\n",
      "\titers: 900, epoch: 3 | loss: 0.0883381\n",
      "\tspeed: 0.1152s/iter; left time: 5427.6164s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0973774\n",
      "\tspeed: 0.1147s/iter; left time: 5392.8294s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1029167\n",
      "\tspeed: 0.1151s/iter; left time: 5398.3411s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1063046\n",
      "\tspeed: 0.1145s/iter; left time: 5361.3742s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0910941\n",
      "\tspeed: 0.1125s/iter; left time: 5255.2026s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0989402\n",
      "\tspeed: 0.1167s/iter; left time: 5436.8592s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0897562\n",
      "\tspeed: 0.1155s/iter; left time: 5372.7001s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1038726\n",
      "\tspeed: 0.1128s/iter; left time: 5234.9290s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029803\n",
      "\tspeed: 0.1167s/iter; left time: 5402.0971s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0976344\n",
      "\tspeed: 0.1143s/iter; left time: 5281.2633s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0954050\n",
      "\tspeed: 0.1146s/iter; left time: 5285.8710s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0907561\n",
      "\tspeed: 0.1151s/iter; left time: 5297.6428s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1184174\n",
      "\tspeed: 0.1146s/iter; left time: 5259.1909s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0903510\n",
      "\tspeed: 0.1169s/iter; left time: 5356.7739s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0935952\n",
      "\tspeed: 0.1136s/iter; left time: 5190.0479s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0925637\n",
      "\tspeed: 0.1134s/iter; left time: 5174.0763s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0878491\n",
      "\tspeed: 0.1141s/iter; left time: 5191.0910s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1013690\n",
      "\tspeed: 0.1146s/iter; left time: 5203.8776s\n",
      "Epoch: 3 cost time: 00h:05m:06.35s\n",
      "Epoch: 3 | Train Loss: 0.0937625 Vali Loss: 0.0846537 Test Loss: 0.0967681\n",
      "Validation loss decreased (0.086735 --> 0.084654).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0943716\n",
      "\tspeed: 1.0024s/iter; left time: 45349.0421s\n",
      "\titers: 200, epoch: 4 | loss: 0.0859998\n",
      "\tspeed: 0.1120s/iter; left time: 5056.7647s\n",
      "\titers: 300, epoch: 4 | loss: 0.0874903\n",
      "\tspeed: 0.1157s/iter; left time: 5209.7013s\n",
      "\titers: 400, epoch: 4 | loss: 0.1020916\n",
      "\tspeed: 0.1141s/iter; left time: 5128.7587s\n",
      "\titers: 500, epoch: 4 | loss: 0.0874067\n",
      "\tspeed: 0.1157s/iter; left time: 5186.5889s\n",
      "\titers: 600, epoch: 4 | loss: 0.1072563\n",
      "\tspeed: 0.1163s/iter; left time: 5201.6022s\n",
      "\titers: 700, epoch: 4 | loss: 0.0964151\n",
      "\tspeed: 0.1148s/iter; left time: 5122.9166s\n",
      "\titers: 800, epoch: 4 | loss: 0.0861687\n",
      "\tspeed: 0.1151s/iter; left time: 5128.6994s\n",
      "\titers: 900, epoch: 4 | loss: 0.0826063\n",
      "\tspeed: 0.1158s/iter; left time: 5148.3643s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0798693\n",
      "\tspeed: 0.1149s/iter; left time: 5095.4728s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0928330\n",
      "\tspeed: 0.1145s/iter; left time: 5063.9366s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0862791\n",
      "\tspeed: 0.1143s/iter; left time: 5045.4097s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0830672\n",
      "\tspeed: 0.1141s/iter; left time: 5023.8441s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0836192\n",
      "\tspeed: 0.1138s/iter; left time: 4998.9174s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0898090\n",
      "\tspeed: 0.1153s/iter; left time: 5053.3962s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0897531\n",
      "\tspeed: 0.1150s/iter; left time: 5031.5050s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0794996\n",
      "\tspeed: 0.1137s/iter; left time: 4962.9002s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0978874\n",
      "\tspeed: 0.1142s/iter; left time: 4973.5847s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0965451\n",
      "\tspeed: 0.1134s/iter; left time: 4927.6185s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0926374\n",
      "\tspeed: 0.1135s/iter; left time: 4919.1444s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1069342\n",
      "\tspeed: 0.1117s/iter; left time: 4830.5466s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0779019\n",
      "\tspeed: 0.1142s/iter; left time: 4927.8059s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0973944\n",
      "\tspeed: 0.1117s/iter; left time: 4808.6873s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0905181\n",
      "\tspeed: 0.1153s/iter; left time: 4952.0385s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0938979\n",
      "\tspeed: 0.1141s/iter; left time: 4887.8665s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0907974\n",
      "\tspeed: 0.1140s/iter; left time: 4873.3340s\n",
      "Epoch: 4 cost time: 00h:05m:05.20s\n",
      "Epoch: 4 | Train Loss: 0.0911554 Vali Loss: 0.0860957 Test Loss: 0.0986955\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0987655\n",
      "\tspeed: 0.9952s/iter; left time: 42367.6231s\n",
      "\titers: 200, epoch: 5 | loss: 0.0867120\n",
      "\tspeed: 0.1171s/iter; left time: 4973.4730s\n",
      "\titers: 300, epoch: 5 | loss: 0.0962949\n",
      "\tspeed: 0.1150s/iter; left time: 4872.4313s\n",
      "\titers: 400, epoch: 5 | loss: 0.0800745\n",
      "\tspeed: 0.1159s/iter; left time: 4899.5939s\n",
      "\titers: 500, epoch: 5 | loss: 0.0932057\n",
      "\tspeed: 0.1154s/iter; left time: 4867.5819s\n",
      "\titers: 600, epoch: 5 | loss: 0.0953641\n",
      "\tspeed: 0.1147s/iter; left time: 4824.5822s\n",
      "\titers: 700, epoch: 5 | loss: 0.0878022\n",
      "\tspeed: 0.1167s/iter; left time: 4897.0108s\n",
      "\titers: 800, epoch: 5 | loss: 0.0875066\n",
      "\tspeed: 0.1152s/iter; left time: 4824.5467s\n",
      "\titers: 900, epoch: 5 | loss: 0.0935306\n",
      "\tspeed: 0.1156s/iter; left time: 4827.2523s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0829855\n",
      "\tspeed: 0.1151s/iter; left time: 4795.5210s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0839508\n",
      "\tspeed: 0.1156s/iter; left time: 4807.3895s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0897008\n",
      "\tspeed: 0.1156s/iter; left time: 4792.3140s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0881073\n",
      "\tspeed: 0.1146s/iter; left time: 4742.8585s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0853253\n",
      "\tspeed: 0.1140s/iter; left time: 4704.8949s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0889603\n",
      "\tspeed: 0.1154s/iter; left time: 4752.0199s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1002251\n",
      "\tspeed: 0.1152s/iter; left time: 4732.4010s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1032137\n",
      "\tspeed: 0.1144s/iter; left time: 4687.8406s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0937006\n",
      "\tspeed: 0.1135s/iter; left time: 4638.2335s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0855721\n",
      "\tspeed: 0.1146s/iter; left time: 4671.3233s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0733401\n",
      "\tspeed: 0.1138s/iter; left time: 4628.5156s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0945749\n",
      "\tspeed: 0.1134s/iter; left time: 4600.9319s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0890472\n",
      "\tspeed: 0.1146s/iter; left time: 4636.7185s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0819968\n",
      "\tspeed: 0.1150s/iter; left time: 4642.4987s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0929638\n",
      "\tspeed: 0.1133s/iter; left time: 4561.2040s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0771377\n",
      "\tspeed: 0.1118s/iter; left time: 4492.8096s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0927820\n",
      "\tspeed: 0.1145s/iter; left time: 4588.9597s\n",
      "Epoch: 5 cost time: 00h:05m:06.40s\n",
      "Epoch: 5 | Train Loss: 0.0889107 Vali Loss: 0.0877582 Test Loss: 0.1025786\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0778098\n",
      "\tspeed: 0.9945s/iter; left time: 39685.6441s\n",
      "\titers: 200, epoch: 6 | loss: 0.0997249\n",
      "\tspeed: 0.1172s/iter; left time: 4664.5360s\n",
      "\titers: 300, epoch: 6 | loss: 0.0860444\n",
      "\tspeed: 0.1130s/iter; left time: 4487.8382s\n",
      "\titers: 400, epoch: 6 | loss: 0.0840693\n",
      "\tspeed: 0.1132s/iter; left time: 4483.2478s\n",
      "\titers: 500, epoch: 6 | loss: 0.0925132\n",
      "\tspeed: 0.1152s/iter; left time: 4551.5005s\n",
      "\titers: 600, epoch: 6 | loss: 0.0898937\n",
      "\tspeed: 0.1136s/iter; left time: 4477.7601s\n",
      "\titers: 700, epoch: 6 | loss: 0.0814919\n",
      "\tspeed: 0.1136s/iter; left time: 4463.2690s\n",
      "\titers: 800, epoch: 6 | loss: 0.1018386\n",
      "\tspeed: 0.1140s/iter; left time: 4470.2427s\n",
      "\titers: 900, epoch: 6 | loss: 0.0892932\n",
      "\tspeed: 0.1119s/iter; left time: 4376.0153s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1022972\n",
      "\tspeed: 0.1144s/iter; left time: 4463.6517s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0904634\n",
      "\tspeed: 0.1123s/iter; left time: 4368.4022s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0951061\n",
      "\tspeed: 0.1129s/iter; left time: 4379.7294s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0853480\n",
      "\tspeed: 0.1134s/iter; left time: 4387.5211s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0741242\n",
      "\tspeed: 0.1127s/iter; left time: 4350.3364s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0868103\n",
      "\tspeed: 0.1131s/iter; left time: 4355.5334s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0908482\n",
      "\tspeed: 0.1155s/iter; left time: 4437.4801s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0931770\n",
      "\tspeed: 0.1144s/iter; left time: 4381.3060s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0878102\n",
      "\tspeed: 0.1139s/iter; left time: 4351.3232s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0864693\n",
      "\tspeed: 0.1136s/iter; left time: 4328.0597s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849704\n",
      "\tspeed: 0.1143s/iter; left time: 4343.3339s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0827954\n",
      "\tspeed: 0.1146s/iter; left time: 4342.5904s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0875449\n",
      "\tspeed: 0.1159s/iter; left time: 4380.5153s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0922521\n",
      "\tspeed: 0.1128s/iter; left time: 4252.8108s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0752860\n",
      "\tspeed: 0.1127s/iter; left time: 4238.0111s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0840485\n",
      "\tspeed: 0.1136s/iter; left time: 4262.2877s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0845857\n",
      "\tspeed: 0.1137s/iter; left time: 4253.1654s\n",
      "Epoch: 6 cost time: 00h:05m:04.20s\n",
      "Epoch: 6 | Train Loss: 0.0867293 Vali Loss: 0.0884304 Test Loss: 0.1025518\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0863808\n",
      "\tspeed: 0.9864s/iter; left time: 36732.8987s\n",
      "\titers: 200, epoch: 7 | loss: 0.0894201\n",
      "\tspeed: 0.1121s/iter; left time: 4164.6086s\n",
      "\titers: 300, epoch: 7 | loss: 0.0827672\n",
      "\tspeed: 0.1130s/iter; left time: 4184.5254s\n",
      "\titers: 400, epoch: 7 | loss: 0.0769152\n",
      "\tspeed: 0.1119s/iter; left time: 4134.6707s\n",
      "\titers: 500, epoch: 7 | loss: 0.0987583\n",
      "\tspeed: 0.1143s/iter; left time: 4212.1974s\n",
      "\titers: 600, epoch: 7 | loss: 0.0983056\n",
      "\tspeed: 0.1149s/iter; left time: 4219.9864s\n",
      "\titers: 700, epoch: 7 | loss: 0.0806711\n",
      "\tspeed: 0.1140s/iter; left time: 4177.7662s\n",
      "\titers: 800, epoch: 7 | loss: 0.0801147\n",
      "\tspeed: 0.1122s/iter; left time: 4100.5414s\n",
      "\titers: 900, epoch: 7 | loss: 0.0864820\n",
      "\tspeed: 0.1124s/iter; left time: 4094.9165s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0967652\n",
      "\tspeed: 0.1134s/iter; left time: 4119.4826s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0854905\n",
      "\tspeed: 0.1120s/iter; left time: 4058.1003s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0898021\n",
      "\tspeed: 0.1120s/iter; left time: 4047.9327s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0775288\n",
      "\tspeed: 0.1125s/iter; left time: 4052.7830s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0811367\n",
      "\tspeed: 0.1133s/iter; left time: 4073.1715s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0821564\n",
      "\tspeed: 0.1130s/iter; left time: 4048.8283s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0834867\n",
      "\tspeed: 0.1149s/iter; left time: 4105.0264s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0906567\n",
      "\tspeed: 0.1133s/iter; left time: 4037.1594s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0855301\n",
      "\tspeed: 0.1132s/iter; left time: 4024.6562s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0763849\n",
      "\tspeed: 0.1151s/iter; left time: 4080.5070s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0703013\n",
      "\tspeed: 0.1135s/iter; left time: 4012.6210s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0814764\n",
      "\tspeed: 0.1135s/iter; left time: 4000.1259s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0879330\n",
      "\tspeed: 0.1136s/iter; left time: 3992.9013s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0835790\n",
      "\tspeed: 0.1154s/iter; left time: 4043.7016s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0896361\n",
      "\tspeed: 0.1151s/iter; left time: 4023.0234s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0880383\n",
      "\tspeed: 0.1147s/iter; left time: 3994.3573s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0784809\n",
      "\tspeed: 0.1152s/iter; left time: 4000.8033s\n",
      "Epoch: 7 cost time: 00h:05m:03.25s\n",
      "Epoch: 7 | Train Loss: 0.0847558 Vali Loss: 0.0862495 Test Loss: 0.0994056\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0741981\n",
      "\tspeed: 0.9933s/iter; left time: 34339.2805s\n",
      "\titers: 200, epoch: 8 | loss: 0.0925967\n",
      "\tspeed: 0.1136s/iter; left time: 3917.0845s\n",
      "\titers: 300, epoch: 8 | loss: 0.0885595\n",
      "\tspeed: 0.1148s/iter; left time: 3946.5618s\n",
      "\titers: 400, epoch: 8 | loss: 0.0859595\n",
      "\tspeed: 0.1137s/iter; left time: 3896.4052s\n",
      "\titers: 500, epoch: 8 | loss: 0.0826866\n",
      "\tspeed: 0.1135s/iter; left time: 3878.4150s\n",
      "\titers: 600, epoch: 8 | loss: 0.0911786\n",
      "\tspeed: 0.1126s/iter; left time: 3838.0058s\n",
      "\titers: 700, epoch: 8 | loss: 0.0777338\n",
      "\tspeed: 0.1148s/iter; left time: 3901.0649s\n",
      "\titers: 800, epoch: 8 | loss: 0.0907550\n",
      "\tspeed: 0.1159s/iter; left time: 3924.6959s\n",
      "\titers: 900, epoch: 8 | loss: 0.0851398\n",
      "\tspeed: 0.1143s/iter; left time: 3860.4160s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0833690\n",
      "\tspeed: 0.1141s/iter; left time: 3841.0356s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0865234\n",
      "\tspeed: 0.1131s/iter; left time: 3797.9051s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0920289\n",
      "\tspeed: 0.1137s/iter; left time: 3805.3657s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0871832\n",
      "\tspeed: 0.1141s/iter; left time: 3807.8263s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754475\n",
      "\tspeed: 0.1139s/iter; left time: 3791.0663s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0724243\n",
      "\tspeed: 0.1130s/iter; left time: 3748.3517s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0741268\n",
      "\tspeed: 0.1149s/iter; left time: 3801.4922s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0789550\n",
      "\tspeed: 0.1151s/iter; left time: 3795.2618s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0848181\n",
      "\tspeed: 0.1131s/iter; left time: 3719.2967s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0913343\n",
      "\tspeed: 0.1155s/iter; left time: 3786.3674s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0802559\n",
      "\tspeed: 0.1130s/iter; left time: 3693.1555s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0726332\n",
      "\tspeed: 0.1144s/iter; left time: 3725.4667s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0801809\n",
      "\tspeed: 0.1176s/iter; left time: 3818.9217s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0922091\n",
      "\tspeed: 0.1157s/iter; left time: 3746.9757s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0758863\n",
      "\tspeed: 0.1144s/iter; left time: 3692.1940s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0947716\n",
      "\tspeed: 0.1168s/iter; left time: 3757.6151s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0881028\n",
      "\tspeed: 0.1149s/iter; left time: 3685.7139s\n",
      "Epoch: 8 cost time: 00h:05m:05.67s\n",
      "Epoch: 8 | Train Loss: 0.0829597 Vali Loss: 0.0875380 Test Loss: 0.1024083\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02114104852080345, rmse:0.14539961516857147, mae:0.09676817059516907, rse:0.4269324839115143\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 168: 00h:53m:27.00s\n",
      "\n",
      "Intermediate time for ES: 04h:00m:32.34s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 14:25:18,527] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 14:25:19,550] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 14:25:19,551] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 14:25:19,551] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 14:25:19,613] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 14:25:19,613] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 14:25:20,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 14:25:20,312] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 14:25:20,312] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 14:25:20,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 14:25:20,313] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 14:25:20,313] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 14:25:20,313] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 14:25:20,667] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 14:25:20,669] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:20,702] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:20,874] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 14:25:20,875] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:20,876] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.52 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:20,876] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 14:25:21,034] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 14:25:21,035] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 14:25:21,035] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.42 GB, percent = 10.1%\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 14:25:21,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 14:25:21,037] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a91d56f50>\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 14:25:21,038] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 14:25:21,039] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 14:25:21,040] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1535002\n",
      "\tspeed: 0.1670s/iter; left time: 8937.9413s\n",
      "\titers: 200, epoch: 1 | loss: 0.1159472\n",
      "\tspeed: 0.1275s/iter; left time: 6809.3841s\n",
      "\titers: 300, epoch: 1 | loss: 0.1058322\n",
      "\tspeed: 0.1286s/iter; left time: 6857.4477s\n",
      "\titers: 400, epoch: 1 | loss: 0.0828711\n",
      "\tspeed: 0.1281s/iter; left time: 6815.4143s\n",
      "\titers: 500, epoch: 1 | loss: 0.0717254\n",
      "\tspeed: 0.1262s/iter; left time: 6705.7068s\n",
      "\titers: 600, epoch: 1 | loss: 0.0627306\n",
      "\tspeed: 0.1277s/iter; left time: 6771.5508s\n",
      "\titers: 700, epoch: 1 | loss: 0.0745939\n",
      "\tspeed: 0.1279s/iter; left time: 6769.5948s\n",
      "\titers: 800, epoch: 1 | loss: 0.0592833\n",
      "\tspeed: 0.1284s/iter; left time: 6781.5981s\n",
      "\titers: 900, epoch: 1 | loss: 0.0581057\n",
      "\tspeed: 0.1259s/iter; left time: 6636.9253s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0899070\n",
      "\tspeed: 0.1285s/iter; left time: 6763.9433s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0486953\n",
      "\tspeed: 0.1277s/iter; left time: 6706.1418s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0688207\n",
      "\tspeed: 0.1277s/iter; left time: 6694.8470s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0650532\n",
      "\tspeed: 0.1284s/iter; left time: 6719.3383s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0587849\n",
      "\tspeed: 0.1262s/iter; left time: 6591.1690s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0618791\n",
      "\tspeed: 0.1259s/iter; left time: 6559.9799s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0509475\n",
      "\tspeed: 0.1254s/iter; left time: 6524.6139s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0735843\n",
      "\tspeed: 0.1276s/iter; left time: 6624.1152s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0482276\n",
      "\tspeed: 0.1256s/iter; left time: 6506.3246s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0562918\n",
      "\tspeed: 0.1279s/iter; left time: 6613.9771s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0636575\n",
      "\tspeed: 0.1298s/iter; left time: 6700.3561s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0499355\n",
      "\tspeed: 0.1273s/iter; left time: 6560.9881s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0701026\n",
      "\tspeed: 0.1284s/iter; left time: 6600.1536s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0674836\n",
      "\tspeed: 0.1258s/iter; left time: 6457.8720s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0581415\n",
      "\tspeed: 0.1270s/iter; left time: 6507.2497s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0673574\n",
      "\tspeed: 0.1290s/iter; left time: 6595.6348s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0543020\n",
      "\tspeed: 0.1304s/iter; left time: 6651.2315s\n",
      "Epoch: 1 cost time: 00h:05m:42.77s\n",
      "Epoch: 1 | Train Loss: 0.0739179 Vali Loss: 0.0605835 Test Loss: 0.0654644\n",
      "Validation loss decreased (inf --> 0.060583).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0619027\n",
      "\tspeed: 1.1911s/iter; left time: 60556.0340s\n",
      "\titers: 200, epoch: 2 | loss: 0.0812935\n",
      "\tspeed: 0.1142s/iter; left time: 5796.6521s\n",
      "\titers: 300, epoch: 2 | loss: 0.0682675\n",
      "\tspeed: 0.1137s/iter; left time: 5756.8308s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864105\n",
      "\tspeed: 0.1148s/iter; left time: 5803.4670s\n",
      "\titers: 500, epoch: 2 | loss: 0.0529756\n",
      "\tspeed: 0.1137s/iter; left time: 5734.8439s\n",
      "\titers: 600, epoch: 2 | loss: 0.0543025\n",
      "\tspeed: 0.1122s/iter; left time: 5647.7195s\n",
      "\titers: 700, epoch: 2 | loss: 0.0502084\n",
      "\tspeed: 0.1135s/iter; left time: 5704.4916s\n",
      "\titers: 800, epoch: 2 | loss: 0.0579205\n",
      "\tspeed: 0.1151s/iter; left time: 5773.4711s\n",
      "\titers: 900, epoch: 2 | loss: 0.0560098\n",
      "\tspeed: 0.1139s/iter; left time: 5697.6910s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0432923\n",
      "\tspeed: 0.1140s/iter; left time: 5695.6111s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0533719\n",
      "\tspeed: 0.1131s/iter; left time: 5635.8386s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0699260\n",
      "\tspeed: 0.1132s/iter; left time: 5630.1798s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0667628\n",
      "\tspeed: 0.1142s/iter; left time: 5667.3855s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0512179\n",
      "\tspeed: 0.1156s/iter; left time: 5729.1609s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0579801\n",
      "\tspeed: 0.1151s/iter; left time: 5689.2001s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0674562\n",
      "\tspeed: 0.1125s/iter; left time: 5552.5770s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0445822\n",
      "\tspeed: 0.1139s/iter; left time: 5607.3974s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0552087\n",
      "\tspeed: 0.1140s/iter; left time: 5602.9960s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0708642\n",
      "\tspeed: 0.1177s/iter; left time: 5773.8578s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0525380\n",
      "\tspeed: 0.1167s/iter; left time: 5713.1574s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0523798\n",
      "\tspeed: 0.1136s/iter; left time: 5549.7491s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0721295\n",
      "\tspeed: 0.1166s/iter; left time: 5685.0079s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0610736\n",
      "\tspeed: 0.1142s/iter; left time: 5555.7468s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0564372\n",
      "\tspeed: 0.1164s/iter; left time: 5648.3881s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0445440\n",
      "\tspeed: 0.1162s/iter; left time: 5627.6453s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0545508\n",
      "\tspeed: 0.1149s/iter; left time: 5555.9755s\n",
      "Epoch: 2 cost time: 00h:05m:07.46s\n",
      "Epoch: 2 | Train Loss: 0.0589849 Vali Loss: 0.0589729 Test Loss: 0.0641147\n",
      "Validation loss decreased (0.060583 --> 0.058973).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0636515\n",
      "\tspeed: 1.0464s/iter; left time: 50393.2997s\n",
      "\titers: 200, epoch: 3 | loss: 0.0587107\n",
      "\tspeed: 0.1175s/iter; left time: 5646.8515s\n",
      "\titers: 300, epoch: 3 | loss: 0.0566106\n",
      "\tspeed: 0.1164s/iter; left time: 5582.5851s\n",
      "\titers: 400, epoch: 3 | loss: 0.0612787\n",
      "\tspeed: 0.1159s/iter; left time: 5546.6321s\n",
      "\titers: 500, epoch: 3 | loss: 0.0468024\n",
      "\tspeed: 0.1167s/iter; left time: 5574.9883s\n",
      "\titers: 600, epoch: 3 | loss: 0.0613940\n",
      "\tspeed: 0.1163s/iter; left time: 5541.0090s\n",
      "\titers: 700, epoch: 3 | loss: 0.0506664\n",
      "\tspeed: 0.1168s/iter; left time: 5554.4904s\n",
      "\titers: 800, epoch: 3 | loss: 0.0535050\n",
      "\tspeed: 0.1146s/iter; left time: 5439.7039s\n",
      "\titers: 900, epoch: 3 | loss: 0.0552201\n",
      "\tspeed: 0.1144s/iter; left time: 5418.4324s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0510626\n",
      "\tspeed: 0.1163s/iter; left time: 5494.0462s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0414352\n",
      "\tspeed: 0.1163s/iter; left time: 5485.1237s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0635938\n",
      "\tspeed: 0.1138s/iter; left time: 5357.0654s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0423877\n",
      "\tspeed: 0.1161s/iter; left time: 5452.5070s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0537974\n",
      "\tspeed: 0.1151s/iter; left time: 5392.1644s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0664770\n",
      "\tspeed: 0.1158s/iter; left time: 5414.7966s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0489237\n",
      "\tspeed: 0.1161s/iter; left time: 5418.3764s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0435316\n",
      "\tspeed: 0.1177s/iter; left time: 5479.9241s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0485576\n",
      "\tspeed: 0.1181s/iter; left time: 5487.7930s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0613631\n",
      "\tspeed: 0.1161s/iter; left time: 5382.2814s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0441004\n",
      "\tspeed: 0.1166s/iter; left time: 5391.9324s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0450289\n",
      "\tspeed: 0.1163s/iter; left time: 5370.0735s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0547356\n",
      "\tspeed: 0.1152s/iter; left time: 5304.7146s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0599108\n",
      "\tspeed: 0.1152s/iter; left time: 5295.8818s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0434626\n",
      "\tspeed: 0.1159s/iter; left time: 5313.3749s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0669618\n",
      "\tspeed: 0.1147s/iter; left time: 5246.7893s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0644993\n",
      "\tspeed: 0.1144s/iter; left time: 5222.2977s\n",
      "Epoch: 3 cost time: 00h:05m:11.03s\n",
      "Epoch: 3 | Train Loss: 0.0563925 Vali Loss: 0.0578105 Test Loss: 0.0628996\n",
      "Validation loss decreased (0.058973 --> 0.057811).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0500212\n",
      "\tspeed: 1.0506s/iter; left time: 47778.1646s\n",
      "\titers: 200, epoch: 4 | loss: 0.0662548\n",
      "\tspeed: 0.1175s/iter; left time: 5330.5472s\n",
      "\titers: 300, epoch: 4 | loss: 0.0432065\n",
      "\tspeed: 0.1171s/iter; left time: 5300.3254s\n",
      "\titers: 400, epoch: 4 | loss: 0.0570253\n",
      "\tspeed: 0.1166s/iter; left time: 5266.3735s\n",
      "\titers: 500, epoch: 4 | loss: 0.0514172\n",
      "\tspeed: 0.1160s/iter; left time: 5229.6123s\n",
      "\titers: 600, epoch: 4 | loss: 0.0459452\n",
      "\tspeed: 0.1163s/iter; left time: 5231.5868s\n",
      "\titers: 700, epoch: 4 | loss: 0.0489160\n",
      "\tspeed: 0.1175s/iter; left time: 5272.4026s\n",
      "\titers: 800, epoch: 4 | loss: 0.0543107\n",
      "\tspeed: 0.1173s/iter; left time: 5251.5969s\n",
      "\titers: 900, epoch: 4 | loss: 0.0444514\n",
      "\tspeed: 0.1172s/iter; left time: 5235.3134s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0546588\n",
      "\tspeed: 0.1172s/iter; left time: 5224.0163s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0588563\n",
      "\tspeed: 0.1181s/iter; left time: 5253.8665s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0534603\n",
      "\tspeed: 0.1166s/iter; left time: 5174.8999s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0535597\n",
      "\tspeed: 0.1171s/iter; left time: 5183.7953s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0503390\n",
      "\tspeed: 0.1161s/iter; left time: 5130.1821s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0542765\n",
      "\tspeed: 0.1151s/iter; left time: 5072.2718s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0546105\n",
      "\tspeed: 0.1151s/iter; left time: 5059.6824s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0538735\n",
      "\tspeed: 0.1161s/iter; left time: 5095.0027s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0518390\n",
      "\tspeed: 0.1178s/iter; left time: 5156.7007s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0519650\n",
      "\tspeed: 0.1165s/iter; left time: 5089.3941s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0477860\n",
      "\tspeed: 0.1168s/iter; left time: 5092.0510s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0585879\n",
      "\tspeed: 0.1162s/iter; left time: 5052.5469s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0599166\n",
      "\tspeed: 0.1159s/iter; left time: 5026.3131s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0504315\n",
      "\tspeed: 0.1145s/iter; left time: 4956.8512s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0512709\n",
      "\tspeed: 0.1140s/iter; left time: 4922.5263s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0432640\n",
      "\tspeed: 0.1139s/iter; left time: 4905.8741s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0526977\n",
      "\tspeed: 0.1152s/iter; left time: 4953.0588s\n",
      "Epoch: 4 cost time: 00h:05m:11.89s\n",
      "Epoch: 4 | Train Loss: 0.0547071 Vali Loss: 0.0565705 Test Loss: 0.0614690\n",
      "Validation loss decreased (0.057811 --> 0.056570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0500493\n",
      "\tspeed: 1.0484s/iter; left time: 44866.7391s\n",
      "\titers: 200, epoch: 5 | loss: 0.0452871\n",
      "\tspeed: 0.1145s/iter; left time: 4888.1574s\n",
      "\titers: 300, epoch: 5 | loss: 0.0708607\n",
      "\tspeed: 0.1138s/iter; left time: 4846.1708s\n",
      "\titers: 400, epoch: 5 | loss: 0.0393255\n",
      "\tspeed: 0.1172s/iter; left time: 4982.5589s\n",
      "\titers: 500, epoch: 5 | loss: 0.0584477\n",
      "\tspeed: 0.1150s/iter; left time: 4875.3948s\n",
      "\titers: 600, epoch: 5 | loss: 0.0544610\n",
      "\tspeed: 0.1140s/iter; left time: 4821.9702s\n",
      "\titers: 700, epoch: 5 | loss: 0.0538923\n",
      "\tspeed: 0.1139s/iter; left time: 4805.8009s\n",
      "\titers: 800, epoch: 5 | loss: 0.0542743\n",
      "\tspeed: 0.1144s/iter; left time: 4815.9957s\n",
      "\titers: 900, epoch: 5 | loss: 0.0646305\n",
      "\tspeed: 0.1151s/iter; left time: 4835.4620s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0515460\n",
      "\tspeed: 0.1158s/iter; left time: 4850.9438s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0574071\n",
      "\tspeed: 0.1162s/iter; left time: 4856.9851s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0493728\n",
      "\tspeed: 0.1156s/iter; left time: 4819.2445s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0618911\n",
      "\tspeed: 0.1169s/iter; left time: 4862.4262s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0536328\n",
      "\tspeed: 0.1164s/iter; left time: 4829.5067s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0613508\n",
      "\tspeed: 0.1165s/iter; left time: 4823.5586s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0503679\n",
      "\tspeed: 0.1169s/iter; left time: 4825.6196s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0521905\n",
      "\tspeed: 0.1185s/iter; left time: 4882.8882s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0556836\n",
      "\tspeed: 0.1160s/iter; left time: 4767.8640s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0641983\n",
      "\tspeed: 0.1143s/iter; left time: 4683.9989s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0585940\n",
      "\tspeed: 0.1147s/iter; left time: 4690.2370s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0435131\n",
      "\tspeed: 0.1145s/iter; left time: 4672.2849s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0479925\n",
      "\tspeed: 0.1142s/iter; left time: 4646.0430s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0534774\n",
      "\tspeed: 0.1155s/iter; left time: 4689.6705s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0596892\n",
      "\tspeed: 0.1144s/iter; left time: 4633.2802s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0679414\n",
      "\tspeed: 0.1129s/iter; left time: 4560.5640s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0447534\n",
      "\tspeed: 0.1147s/iter; left time: 4622.3254s\n",
      "Epoch: 5 cost time: 00h:05m:09.46s\n",
      "Epoch: 5 | Train Loss: 0.0534883 Vali Loss: 0.0565160 Test Loss: 0.0616658\n",
      "Validation loss decreased (0.056570 --> 0.056516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0517764\n",
      "\tspeed: 1.0445s/iter; left time: 41900.6783s\n",
      "\titers: 200, epoch: 6 | loss: 0.0638383\n",
      "\tspeed: 0.1134s/iter; left time: 4537.3457s\n",
      "\titers: 300, epoch: 6 | loss: 0.0642103\n",
      "\tspeed: 0.1128s/iter; left time: 4502.1621s\n",
      "\titers: 400, epoch: 6 | loss: 0.0472873\n",
      "\tspeed: 0.1133s/iter; left time: 4512.5762s\n",
      "\titers: 500, epoch: 6 | loss: 0.0434228\n",
      "\tspeed: 0.1128s/iter; left time: 4479.2753s\n",
      "\titers: 600, epoch: 6 | loss: 0.0501320\n",
      "\tspeed: 0.1136s/iter; left time: 4501.4273s\n",
      "\titers: 700, epoch: 6 | loss: 0.0372537\n",
      "\tspeed: 0.1124s/iter; left time: 4441.1197s\n",
      "\titers: 800, epoch: 6 | loss: 0.0626084\n",
      "\tspeed: 0.1113s/iter; left time: 4388.8442s\n",
      "\titers: 900, epoch: 6 | loss: 0.0585214\n",
      "\tspeed: 0.1117s/iter; left time: 4392.3550s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0676461\n",
      "\tspeed: 0.1126s/iter; left time: 4416.9388s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0532079\n",
      "\tspeed: 0.1122s/iter; left time: 4388.2556s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0608200\n",
      "\tspeed: 0.1120s/iter; left time: 4369.7022s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0481969\n",
      "\tspeed: 0.1127s/iter; left time: 4384.0525s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0451223\n",
      "\tspeed: 0.1121s/iter; left time: 4351.5105s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0623270\n",
      "\tspeed: 0.1142s/iter; left time: 4422.8118s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0533873\n",
      "\tspeed: 0.1136s/iter; left time: 4385.8150s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0445180\n",
      "\tspeed: 0.1126s/iter; left time: 4336.7563s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0662053\n",
      "\tspeed: 0.1132s/iter; left time: 4348.3974s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0493251\n",
      "\tspeed: 0.1128s/iter; left time: 4320.9003s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0448093\n",
      "\tspeed: 0.1136s/iter; left time: 4340.1308s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0606518\n",
      "\tspeed: 0.1118s/iter; left time: 4261.2007s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0587236\n",
      "\tspeed: 0.1131s/iter; left time: 4299.3428s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0420482\n",
      "\tspeed: 0.1136s/iter; left time: 4305.8230s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0426998\n",
      "\tspeed: 0.1142s/iter; left time: 4319.9757s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0466667\n",
      "\tspeed: 0.1155s/iter; left time: 4355.6813s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0537168\n",
      "\tspeed: 0.1127s/iter; left time: 4240.1491s\n",
      "Epoch: 6 cost time: 00h:05m:03.61s\n",
      "Epoch: 6 | Train Loss: 0.0525643 Vali Loss: 0.0556040 Test Loss: 0.0610864\n",
      "Validation loss decreased (0.056516 --> 0.055604).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0668757\n",
      "\tspeed: 1.0519s/iter; left time: 39376.3420s\n",
      "\titers: 200, epoch: 7 | loss: 0.0545903\n",
      "\tspeed: 0.1115s/iter; left time: 4163.8687s\n",
      "\titers: 300, epoch: 7 | loss: 0.0470054\n",
      "\tspeed: 0.1123s/iter; left time: 4181.6989s\n",
      "\titers: 400, epoch: 7 | loss: 0.0559453\n",
      "\tspeed: 0.1150s/iter; left time: 4270.1900s\n",
      "\titers: 500, epoch: 7 | loss: 0.0481483\n",
      "\tspeed: 0.1164s/iter; left time: 4309.3933s\n",
      "\titers: 600, epoch: 7 | loss: 0.0419734\n",
      "\tspeed: 0.1134s/iter; left time: 4187.1318s\n",
      "\titers: 700, epoch: 7 | loss: 0.0507655\n",
      "\tspeed: 0.1132s/iter; left time: 4171.0630s\n",
      "\titers: 800, epoch: 7 | loss: 0.0553637\n",
      "\tspeed: 0.1127s/iter; left time: 4138.8237s\n",
      "\titers: 900, epoch: 7 | loss: 0.0417836\n",
      "\tspeed: 0.1129s/iter; left time: 4136.5673s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0594274\n",
      "\tspeed: 0.1124s/iter; left time: 4107.9578s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0467496\n",
      "\tspeed: 0.1130s/iter; left time: 4116.9479s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0645316\n",
      "\tspeed: 0.1121s/iter; left time: 4074.7185s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0443441\n",
      "\tspeed: 0.1142s/iter; left time: 4136.5834s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0480593\n",
      "\tspeed: 0.1133s/iter; left time: 4093.1993s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0565358\n",
      "\tspeed: 0.1144s/iter; left time: 4122.1659s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0584448\n",
      "\tspeed: 0.1122s/iter; left time: 4030.6157s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0491271\n",
      "\tspeed: 0.1134s/iter; left time: 4063.1363s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0501366\n",
      "\tspeed: 0.1122s/iter; left time: 4011.0066s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0490542\n",
      "\tspeed: 0.1133s/iter; left time: 4035.9678s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0508287\n",
      "\tspeed: 0.1151s/iter; left time: 4088.5964s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0515859\n",
      "\tspeed: 0.1139s/iter; left time: 4036.4593s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0453970\n",
      "\tspeed: 0.1135s/iter; left time: 4009.3773s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0537278\n",
      "\tspeed: 0.1134s/iter; left time: 3996.6944s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0542764\n",
      "\tspeed: 0.1138s/iter; left time: 3998.1799s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0465567\n",
      "\tspeed: 0.1138s/iter; left time: 3985.5715s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0497594\n",
      "\tspeed: 0.1147s/iter; left time: 4006.3793s\n",
      "Epoch: 7 cost time: 00h:05m:04.75s\n",
      "Epoch: 7 | Train Loss: 0.0518482 Vali Loss: 0.0550725 Test Loss: 0.0603381\n",
      "Validation loss decreased (0.055604 --> 0.055073).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0724908\n",
      "\tspeed: 1.0599s/iter; left time: 36834.1324s\n",
      "\titers: 200, epoch: 8 | loss: 0.0371657\n",
      "\tspeed: 0.1160s/iter; left time: 4019.2294s\n",
      "\titers: 300, epoch: 8 | loss: 0.0593067\n",
      "\tspeed: 0.1154s/iter; left time: 3985.9655s\n",
      "\titers: 400, epoch: 8 | loss: 0.0512719\n",
      "\tspeed: 0.1166s/iter; left time: 4017.0193s\n",
      "\titers: 500, epoch: 8 | loss: 0.0472560\n",
      "\tspeed: 0.1165s/iter; left time: 4003.6044s\n",
      "\titers: 600, epoch: 8 | loss: 0.0475125\n",
      "\tspeed: 0.1151s/iter; left time: 3942.3386s\n",
      "\titers: 700, epoch: 8 | loss: 0.0528827\n",
      "\tspeed: 0.1138s/iter; left time: 3886.4899s\n",
      "\titers: 800, epoch: 8 | loss: 0.0542871\n",
      "\tspeed: 0.1135s/iter; left time: 3865.0685s\n",
      "\titers: 900, epoch: 8 | loss: 0.0525813\n",
      "\tspeed: 0.1143s/iter; left time: 3880.3574s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0576860\n",
      "\tspeed: 0.1137s/iter; left time: 3849.6669s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0429319\n",
      "\tspeed: 0.1154s/iter; left time: 3896.3183s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0528561\n",
      "\tspeed: 0.1133s/iter; left time: 3813.9504s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0534785\n",
      "\tspeed: 0.1126s/iter; left time: 3778.5499s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0459838\n",
      "\tspeed: 0.1152s/iter; left time: 3855.0111s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0495800\n",
      "\tspeed: 0.1134s/iter; left time: 3780.7558s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0621961\n",
      "\tspeed: 0.1122s/iter; left time: 3730.8639s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0502770\n",
      "\tspeed: 0.1138s/iter; left time: 3771.8794s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0546408\n",
      "\tspeed: 0.1121s/iter; left time: 3704.1273s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0553437\n",
      "\tspeed: 0.1153s/iter; left time: 3800.1403s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0498420\n",
      "\tspeed: 0.1125s/iter; left time: 3694.8584s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0456712\n",
      "\tspeed: 0.1123s/iter; left time: 3676.7870s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0570142\n",
      "\tspeed: 0.1142s/iter; left time: 3729.3105s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0586175\n",
      "\tspeed: 0.1147s/iter; left time: 3734.0547s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0472034\n",
      "\tspeed: 0.1149s/iter; left time: 3727.6649s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0537372\n",
      "\tspeed: 0.1137s/iter; left time: 3677.3987s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0509919\n",
      "\tspeed: 0.1121s/iter; left time: 3615.2861s\n",
      "Epoch: 8 cost time: 00h:05m:06.39s\n",
      "Epoch: 8 | Train Loss: 0.0511598 Vali Loss: 0.0561661 Test Loss: 0.0617564\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0486406\n",
      "\tspeed: 1.0277s/iter; left time: 32960.2609s\n",
      "\titers: 200, epoch: 9 | loss: 0.0556457\n",
      "\tspeed: 0.1133s/iter; left time: 3623.7339s\n",
      "\titers: 300, epoch: 9 | loss: 0.0395175\n",
      "\tspeed: 0.1130s/iter; left time: 3601.6127s\n",
      "\titers: 400, epoch: 9 | loss: 0.0557750\n",
      "\tspeed: 0.1131s/iter; left time: 3594.7021s\n",
      "\titers: 500, epoch: 9 | loss: 0.0424190\n",
      "\tspeed: 0.1129s/iter; left time: 3575.5721s\n",
      "\titers: 600, epoch: 9 | loss: 0.0351209\n",
      "\tspeed: 0.1124s/iter; left time: 3547.6705s\n",
      "\titers: 700, epoch: 9 | loss: 0.0446073\n",
      "\tspeed: 0.1135s/iter; left time: 3572.3584s\n",
      "\titers: 800, epoch: 9 | loss: 0.0384792\n",
      "\tspeed: 0.1122s/iter; left time: 3520.8111s\n",
      "\titers: 900, epoch: 9 | loss: 0.0720906\n",
      "\tspeed: 0.1121s/iter; left time: 3505.5232s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0432853\n",
      "\tspeed: 0.1126s/iter; left time: 3511.5431s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0520263\n",
      "\tspeed: 0.1140s/iter; left time: 3542.9423s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0406994\n",
      "\tspeed: 0.1136s/iter; left time: 3518.8090s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0368552\n",
      "\tspeed: 0.1128s/iter; left time: 3483.4962s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0537488\n",
      "\tspeed: 0.1140s/iter; left time: 3509.3689s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0565366\n",
      "\tspeed: 0.1129s/iter; left time: 3462.6149s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0400267\n",
      "\tspeed: 0.1144s/iter; left time: 3497.2432s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0363783\n",
      "\tspeed: 0.1132s/iter; left time: 3450.2868s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0432666\n",
      "\tspeed: 0.1121s/iter; left time: 3405.0745s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0467523\n",
      "\tspeed: 0.1126s/iter; left time: 3408.0894s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0596244\n",
      "\tspeed: 0.1132s/iter; left time: 3416.5046s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0576644\n",
      "\tspeed: 0.1129s/iter; left time: 3394.4302s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0496884\n",
      "\tspeed: 0.1102s/iter; left time: 3301.5614s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0494345\n",
      "\tspeed: 0.1091s/iter; left time: 3259.4055s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0386619\n",
      "\tspeed: 0.1149s/iter; left time: 3420.6504s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0411887\n",
      "\tspeed: 0.1115s/iter; left time: 3308.3598s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0443211\n",
      "\tspeed: 0.1138s/iter; left time: 3365.7200s\n",
      "Epoch: 9 cost time: 00h:05m:03.14s\n",
      "Epoch: 9 | Train Loss: 0.0506318 Vali Loss: 0.0546863 Test Loss: 0.0599749\n",
      "Validation loss decreased (0.055073 --> 0.054686).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0549749\n",
      "\tspeed: 1.0393s/iter; left time: 30545.8957s\n",
      "\titers: 200, epoch: 10 | loss: 0.0383406\n",
      "\tspeed: 0.1121s/iter; left time: 3282.3379s\n",
      "\titers: 300, epoch: 10 | loss: 0.0472804\n",
      "\tspeed: 0.1125s/iter; left time: 3283.6315s\n",
      "\titers: 400, epoch: 10 | loss: 0.0422506\n",
      "\tspeed: 0.1118s/iter; left time: 3253.6303s\n",
      "\titers: 500, epoch: 10 | loss: 0.0550114\n",
      "\tspeed: 0.1137s/iter; left time: 3296.0525s\n",
      "\titers: 600, epoch: 10 | loss: 0.0441338\n",
      "\tspeed: 0.1127s/iter; left time: 3255.9567s\n",
      "\titers: 700, epoch: 10 | loss: 0.0540485\n",
      "\tspeed: 0.1117s/iter; left time: 3215.9544s\n",
      "\titers: 800, epoch: 10 | loss: 0.0546766\n",
      "\tspeed: 0.1133s/iter; left time: 3252.0772s\n",
      "\titers: 900, epoch: 10 | loss: 0.0499431\n",
      "\tspeed: 0.1119s/iter; left time: 3198.8956s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0570361\n",
      "\tspeed: 0.1122s/iter; left time: 3195.4844s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0438565\n",
      "\tspeed: 0.1123s/iter; left time: 3188.3446s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0453070\n",
      "\tspeed: 0.1106s/iter; left time: 3127.9442s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0541585\n",
      "\tspeed: 0.1130s/iter; left time: 3186.5914s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0488800\n",
      "\tspeed: 0.1114s/iter; left time: 3129.6275s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0653748\n",
      "\tspeed: 0.1134s/iter; left time: 3174.8546s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0513718\n",
      "\tspeed: 0.1116s/iter; left time: 3113.5575s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0519211\n",
      "\tspeed: 0.1117s/iter; left time: 3103.9028s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0510913\n",
      "\tspeed: 0.1127s/iter; left time: 3121.0756s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0787238\n",
      "\tspeed: 0.1153s/iter; left time: 3181.8783s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0443497\n",
      "\tspeed: 0.1139s/iter; left time: 3130.7918s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0386278\n",
      "\tspeed: 0.1122s/iter; left time: 3073.5192s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0553838\n",
      "\tspeed: 0.1118s/iter; left time: 3051.9252s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0513475\n",
      "\tspeed: 0.1139s/iter; left time: 3097.5671s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0396648\n",
      "\tspeed: 0.1116s/iter; left time: 3024.2607s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0409211\n",
      "\tspeed: 0.1113s/iter; left time: 3004.2385s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0425374\n",
      "\tspeed: 0.1118s/iter; left time: 3007.5790s\n",
      "Epoch: 10 cost time: 00h:05m:02.03s\n",
      "Epoch: 10 | Train Loss: 0.0501928 Vali Loss: 0.0552204 Test Loss: 0.0600623\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0620002\n",
      "\tspeed: 1.0251s/iter; left time: 27382.6088s\n",
      "\titers: 200, epoch: 11 | loss: 0.0374523\n",
      "\tspeed: 0.1119s/iter; left time: 2977.7641s\n",
      "\titers: 300, epoch: 11 | loss: 0.0529428\n",
      "\tspeed: 0.1136s/iter; left time: 3011.8567s\n",
      "\titers: 400, epoch: 11 | loss: 0.0386790\n",
      "\tspeed: 0.1130s/iter; left time: 2983.6250s\n",
      "\titers: 500, epoch: 11 | loss: 0.0355124\n",
      "\tspeed: 0.1119s/iter; left time: 2943.1978s\n",
      "\titers: 600, epoch: 11 | loss: 0.0515568\n",
      "\tspeed: 0.1151s/iter; left time: 3017.2496s\n",
      "\titers: 700, epoch: 11 | loss: 0.0656706\n",
      "\tspeed: 0.1176s/iter; left time: 3071.1696s\n",
      "\titers: 800, epoch: 11 | loss: 0.0524726\n",
      "\tspeed: 0.1126s/iter; left time: 2929.5851s\n",
      "\titers: 900, epoch: 11 | loss: 0.0677181\n",
      "\tspeed: 0.1138s/iter; left time: 2949.0683s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0548737\n",
      "\tspeed: 0.1144s/iter; left time: 2951.5983s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0569288\n",
      "\tspeed: 0.1127s/iter; left time: 2898.6880s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0609900\n",
      "\tspeed: 0.1115s/iter; left time: 2856.0894s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0399404\n",
      "\tspeed: 0.1144s/iter; left time: 2918.1807s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0586750\n",
      "\tspeed: 0.1136s/iter; left time: 2886.1428s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0501058\n",
      "\tspeed: 0.1107s/iter; left time: 2800.8886s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0411009\n",
      "\tspeed: 0.1105s/iter; left time: 2785.1531s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0643648\n",
      "\tspeed: 0.1137s/iter; left time: 2854.6794s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0525917\n",
      "\tspeed: 0.1141s/iter; left time: 2854.7075s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0451737\n",
      "\tspeed: 0.1122s/iter; left time: 2794.4115s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0579281\n",
      "\tspeed: 0.1119s/iter; left time: 2777.5283s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0540055\n",
      "\tspeed: 0.1122s/iter; left time: 2771.3486s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0435182\n",
      "\tspeed: 0.1105s/iter; left time: 2718.7439s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0541128\n",
      "\tspeed: 0.1131s/iter; left time: 2771.3440s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0472108\n",
      "\tspeed: 0.1135s/iter; left time: 2771.5390s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0526714\n",
      "\tspeed: 0.1147s/iter; left time: 2789.4014s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0492443\n",
      "\tspeed: 0.1118s/iter; left time: 2706.8650s\n",
      "Epoch: 11 cost time: 00h:05m:03.58s\n",
      "Epoch: 11 | Train Loss: 0.0496507 Vali Loss: 0.0557215 Test Loss: 0.0603047\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0461969\n",
      "\tspeed: 1.0286s/iter; left time: 24718.1822s\n",
      "\titers: 200, epoch: 12 | loss: 0.0482488\n",
      "\tspeed: 0.1127s/iter; left time: 2697.5579s\n",
      "\titers: 300, epoch: 12 | loss: 0.0463038\n",
      "\tspeed: 0.1140s/iter; left time: 2716.9514s\n",
      "\titers: 400, epoch: 12 | loss: 0.0601207\n",
      "\tspeed: 0.1123s/iter; left time: 2663.8539s\n",
      "\titers: 500, epoch: 12 | loss: 0.0524425\n",
      "\tspeed: 0.1156s/iter; left time: 2731.6391s\n",
      "\titers: 600, epoch: 12 | loss: 0.0494985\n",
      "\tspeed: 0.1158s/iter; left time: 2724.5077s\n",
      "\titers: 700, epoch: 12 | loss: 0.0530444\n",
      "\tspeed: 0.1140s/iter; left time: 2671.1297s\n",
      "\titers: 800, epoch: 12 | loss: 0.0502007\n",
      "\tspeed: 0.1148s/iter; left time: 2677.7831s\n",
      "\titers: 900, epoch: 12 | loss: 0.0386652\n",
      "\tspeed: 0.1154s/iter; left time: 2679.8984s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0524950\n",
      "\tspeed: 0.1140s/iter; left time: 2637.0591s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0360246\n",
      "\tspeed: 0.1154s/iter; left time: 2657.0871s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0573246\n",
      "\tspeed: 0.1153s/iter; left time: 2643.9142s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0448311\n",
      "\tspeed: 0.1121s/iter; left time: 2558.4108s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0468765\n",
      "\tspeed: 0.1130s/iter; left time: 2568.5121s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0427486\n",
      "\tspeed: 0.1146s/iter; left time: 2592.7664s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0422873\n",
      "\tspeed: 0.1144s/iter; left time: 2576.6648s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0424657\n",
      "\tspeed: 0.1082s/iter; left time: 2427.6676s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0512158\n",
      "\tspeed: 0.1091s/iter; left time: 2436.9783s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0482876\n",
      "\tspeed: 0.1118s/iter; left time: 2485.1582s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0403065\n",
      "\tspeed: 0.1119s/iter; left time: 2476.3739s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0512401\n",
      "\tspeed: 0.1121s/iter; left time: 2470.0594s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0528152\n",
      "\tspeed: 0.1136s/iter; left time: 2490.4916s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0479994\n",
      "\tspeed: 0.1122s/iter; left time: 2448.9373s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0547172\n",
      "\tspeed: 0.1116s/iter; left time: 2426.1544s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0471118\n",
      "\tspeed: 0.1123s/iter; left time: 2429.9498s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0501922\n",
      "\tspeed: 0.1138s/iter; left time: 2449.2207s\n",
      "Epoch: 12 cost time: 00h:05m:03.90s\n",
      "Epoch: 12 | Train Loss: 0.0491253 Vali Loss: 0.0558346 Test Loss: 0.0616492\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0454289\n",
      "\tspeed: 1.0255s/iter; left time: 21893.8210s\n",
      "\titers: 200, epoch: 13 | loss: 0.0566784\n",
      "\tspeed: 0.1133s/iter; left time: 2406.8231s\n",
      "\titers: 300, epoch: 13 | loss: 0.0492385\n",
      "\tspeed: 0.1141s/iter; left time: 2413.8053s\n",
      "\titers: 400, epoch: 13 | loss: 0.0413138\n",
      "\tspeed: 0.1131s/iter; left time: 2380.9215s\n",
      "\titers: 500, epoch: 13 | loss: 0.0430361\n",
      "\tspeed: 0.1147s/iter; left time: 2401.8599s\n",
      "\titers: 600, epoch: 13 | loss: 0.0513702\n",
      "\tspeed: 0.1137s/iter; left time: 2370.2280s\n",
      "\titers: 700, epoch: 13 | loss: 0.0502950\n",
      "\tspeed: 0.1152s/iter; left time: 2391.1957s\n",
      "\titers: 800, epoch: 13 | loss: 0.0534758\n",
      "\tspeed: 0.1164s/iter; left time: 2402.5526s\n",
      "\titers: 900, epoch: 13 | loss: 0.0518013\n",
      "\tspeed: 0.1142s/iter; left time: 2346.3902s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0583335\n",
      "\tspeed: 0.1140s/iter; left time: 2331.8613s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0650049\n",
      "\tspeed: 0.1123s/iter; left time: 2285.9362s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0528312\n",
      "\tspeed: 0.1121s/iter; left time: 2270.7913s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0473740\n",
      "\tspeed: 0.1127s/iter; left time: 2270.3789s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0452519\n",
      "\tspeed: 0.1119s/iter; left time: 2243.0886s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0424276\n",
      "\tspeed: 0.1122s/iter; left time: 2237.7048s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0521983\n",
      "\tspeed: 0.1140s/iter; left time: 2262.3137s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0471075\n",
      "\tspeed: 0.1110s/iter; left time: 2192.8490s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0435151\n",
      "\tspeed: 0.1114s/iter; left time: 2188.5153s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0494255\n",
      "\tspeed: 0.1151s/iter; left time: 2249.1457s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0469670\n",
      "\tspeed: 0.1146s/iter; left time: 2228.1475s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0472251\n",
      "\tspeed: 0.1128s/iter; left time: 2183.4210s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0486188\n",
      "\tspeed: 0.1151s/iter; left time: 2215.8062s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0509845\n",
      "\tspeed: 0.1136s/iter; left time: 2175.1362s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0457661\n",
      "\tspeed: 0.1142s/iter; left time: 2174.7222s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0508229\n",
      "\tspeed: 0.1117s/iter; left time: 2116.8378s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0558406\n",
      "\tspeed: 0.1130s/iter; left time: 2129.6985s\n",
      "Epoch: 13 cost time: 00h:05m:04.57s\n",
      "Epoch: 13 | Train Loss: 0.0487104 Vali Loss: 0.0579919 Test Loss: 0.0634963\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0413323\n",
      "\tspeed: 1.0277s/iter; left time: 19185.7892s\n",
      "\titers: 200, epoch: 14 | loss: 0.0421818\n",
      "\tspeed: 0.1115s/iter; left time: 2070.8073s\n",
      "\titers: 300, epoch: 14 | loss: 0.0630368\n",
      "\tspeed: 0.1124s/iter; left time: 2076.6077s\n",
      "\titers: 400, epoch: 14 | loss: 0.0455646\n",
      "\tspeed: 0.1125s/iter; left time: 2067.1773s\n",
      "\titers: 500, epoch: 14 | loss: 0.0373397\n",
      "\tspeed: 0.1103s/iter; left time: 2014.1091s\n",
      "\titers: 600, epoch: 14 | loss: 0.0462459\n",
      "\tspeed: 0.1112s/iter; left time: 2020.9447s\n",
      "\titers: 700, epoch: 14 | loss: 0.0535143\n",
      "\tspeed: 0.1137s/iter; left time: 2054.4473s\n",
      "\titers: 800, epoch: 14 | loss: 0.0559032\n",
      "\tspeed: 0.1129s/iter; left time: 2028.6877s\n",
      "\titers: 900, epoch: 14 | loss: 0.0622907\n",
      "\tspeed: 0.1120s/iter; left time: 2000.8022s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0430358\n",
      "\tspeed: 0.1112s/iter; left time: 1975.0108s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0412004\n",
      "\tspeed: 0.1117s/iter; left time: 1974.1785s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0447963\n",
      "\tspeed: 0.1132s/iter; left time: 1988.1058s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0473538\n",
      "\tspeed: 0.1146s/iter; left time: 2002.2731s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0410066\n",
      "\tspeed: 0.1133s/iter; left time: 1967.6612s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0531902\n",
      "\tspeed: 0.1126s/iter; left time: 1945.0240s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0443050\n",
      "\tspeed: 0.1142s/iter; left time: 1960.8417s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0529488\n",
      "\tspeed: 0.1144s/iter; left time: 1953.0810s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0568460\n",
      "\tspeed: 0.1163s/iter; left time: 1973.1694s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0442665\n",
      "\tspeed: 0.1132s/iter; left time: 1908.8723s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0497327\n",
      "\tspeed: 0.1127s/iter; left time: 1889.8253s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0499501\n",
      "\tspeed: 0.1154s/iter; left time: 1922.7709s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0507595\n",
      "\tspeed: 0.1127s/iter; left time: 1867.6123s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0538237\n",
      "\tspeed: 0.1122s/iter; left time: 1848.2104s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0460961\n",
      "\tspeed: 0.1136s/iter; left time: 1859.4444s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0627327\n",
      "\tspeed: 0.1098s/iter; left time: 1786.1661s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0460155\n",
      "\tspeed: 0.1111s/iter; left time: 1796.8610s\n",
      "Epoch: 14 cost time: 00h:05m:02.79s\n",
      "Epoch: 14 | Train Loss: 0.0482707 Vali Loss: 0.0567447 Test Loss: 0.0618993\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.011070186272263527, rmse:0.10521495342254639, mae:0.059974875301122665, rse:0.40591442584991455\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 24: 01h:32m:37.52s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 15:57:56,806] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 15:57:57,848] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 15:57:57,848] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 15:57:57,848] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 15:57:57,978] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 15:57:57,979] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 15:57:58,762] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 15:57:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 15:57:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 15:57:58,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 15:57:58,766] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 15:57:58,766] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 15:57:58,766] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 15:57:59,171] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 15:57:59,172] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.58 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,319] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 15:57:59,320] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,320] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.63 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,320] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 15:57:59,489] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 15:57:59,490] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 15:57:59,490] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.67 GB, percent = 41.3%\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 15:57:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 15:57:59,492] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5a92bc61d0>\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 15:57:59,493] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 15:57:59,494] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 15:57:59,495] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 15:57:59,496] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1506957\n",
      "\tspeed: 0.1707s/iter; left time: 9113.0167s\n",
      "\titers: 200, epoch: 1 | loss: 0.1479850\n",
      "\tspeed: 0.1272s/iter; left time: 6776.0640s\n",
      "\titers: 300, epoch: 1 | loss: 0.1137763\n",
      "\tspeed: 0.1266s/iter; left time: 6732.0131s\n",
      "\titers: 400, epoch: 1 | loss: 0.1076533\n",
      "\tspeed: 0.1272s/iter; left time: 6750.7454s\n",
      "\titers: 500, epoch: 1 | loss: 0.0889011\n",
      "\tspeed: 0.1260s/iter; left time: 6677.0404s\n",
      "\titers: 600, epoch: 1 | loss: 0.0769084\n",
      "\tspeed: 0.1259s/iter; left time: 6658.3469s\n",
      "\titers: 700, epoch: 1 | loss: 0.0896481\n",
      "\tspeed: 0.1268s/iter; left time: 6693.0274s\n",
      "\titers: 800, epoch: 1 | loss: 0.0794844\n",
      "\tspeed: 0.1264s/iter; left time: 6658.0271s\n",
      "\titers: 900, epoch: 1 | loss: 0.0815863\n",
      "\tspeed: 0.1259s/iter; left time: 6619.0603s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0779093\n",
      "\tspeed: 0.1268s/iter; left time: 6653.2277s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0663487\n",
      "\tspeed: 0.1268s/iter; left time: 6639.5392s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0627032\n",
      "\tspeed: 0.1270s/iter; left time: 6641.1303s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0696527\n",
      "\tspeed: 0.1250s/iter; left time: 6525.2010s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0784859\n",
      "\tspeed: 0.1281s/iter; left time: 6671.6439s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0762833\n",
      "\tspeed: 0.1253s/iter; left time: 6512.1729s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0648971\n",
      "\tspeed: 0.1261s/iter; left time: 6542.8828s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801568\n",
      "\tspeed: 0.1264s/iter; left time: 6544.9091s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0775711\n",
      "\tspeed: 0.1235s/iter; left time: 6383.1070s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0935053\n",
      "\tspeed: 0.1275s/iter; left time: 6577.2110s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0565456\n",
      "\tspeed: 0.1257s/iter; left time: 6472.9286s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0903167\n",
      "\tspeed: 0.1253s/iter; left time: 6438.8915s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0752582\n",
      "\tspeed: 0.1283s/iter; left time: 6581.6070s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0761704\n",
      "\tspeed: 0.1280s/iter; left time: 6552.6275s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0646984\n",
      "\tspeed: 0.1251s/iter; left time: 6390.5272s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0703335\n",
      "\tspeed: 0.1260s/iter; left time: 6425.0778s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0783555\n",
      "\tspeed: 0.1272s/iter; left time: 6472.8357s\n",
      "Epoch: 1 cost time: 00h:05m:39.18s\n",
      "Epoch: 1 | Train Loss: 0.0856850 Vali Loss: 0.0756439 Test Loss: 0.0843909\n",
      "Validation loss decreased (inf --> 0.075644).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0820017\n",
      "\tspeed: 1.1856s/iter; left time: 60116.7690s\n",
      "\titers: 200, epoch: 2 | loss: 0.0687233\n",
      "\tspeed: 0.1123s/iter; left time: 5684.0124s\n",
      "\titers: 300, epoch: 2 | loss: 0.0772514\n",
      "\tspeed: 0.1136s/iter; left time: 5735.1321s\n",
      "\titers: 400, epoch: 2 | loss: 0.0848139\n",
      "\tspeed: 0.1172s/iter; left time: 5908.0864s\n",
      "\titers: 500, epoch: 2 | loss: 0.0686562\n",
      "\tspeed: 0.1156s/iter; left time: 5817.7403s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745008\n",
      "\tspeed: 0.1163s/iter; left time: 5841.3102s\n",
      "\titers: 700, epoch: 2 | loss: 0.0605222\n",
      "\tspeed: 0.1161s/iter; left time: 5817.0332s\n",
      "\titers: 800, epoch: 2 | loss: 0.0737291\n",
      "\tspeed: 0.1160s/iter; left time: 5800.6933s\n",
      "\titers: 900, epoch: 2 | loss: 0.0731786\n",
      "\tspeed: 0.1150s/iter; left time: 5737.7970s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0705629\n",
      "\tspeed: 0.1147s/iter; left time: 5714.8002s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0684758\n",
      "\tspeed: 0.1153s/iter; left time: 5733.4341s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017207\n",
      "\tspeed: 0.1130s/iter; left time: 5603.5418s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0608158\n",
      "\tspeed: 0.1151s/iter; left time: 5699.1413s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0733602\n",
      "\tspeed: 0.1129s/iter; left time: 5576.6890s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0823769\n",
      "\tspeed: 0.1142s/iter; left time: 5632.0723s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0670033\n",
      "\tspeed: 0.1133s/iter; left time: 5574.8166s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0635963\n",
      "\tspeed: 0.1143s/iter; left time: 5613.5456s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0702989\n",
      "\tspeed: 0.1162s/iter; left time: 5693.8001s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0682929\n",
      "\tspeed: 0.1155s/iter; left time: 5650.0819s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0600829\n",
      "\tspeed: 0.1143s/iter; left time: 5577.4614s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0654664\n",
      "\tspeed: 0.1160s/iter; left time: 5650.9377s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0597576\n",
      "\tspeed: 0.1135s/iter; left time: 5518.1442s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0667420\n",
      "\tspeed: 0.1160s/iter; left time: 5629.2045s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0765030\n",
      "\tspeed: 0.1169s/iter; left time: 5656.5137s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0623692\n",
      "\tspeed: 0.1144s/iter; left time: 5527.0673s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0587819\n",
      "\tspeed: 0.1134s/iter; left time: 5467.1744s\n",
      "Epoch: 2 cost time: 00h:05m:07.74s\n",
      "Epoch: 2 | Train Loss: 0.0710048 Vali Loss: 0.0745253 Test Loss: 0.0838230\n",
      "Validation loss decreased (0.075644 --> 0.074525).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0658879\n",
      "\tspeed: 1.0551s/iter; left time: 50679.4805s\n",
      "\titers: 200, epoch: 3 | loss: 0.0615703\n",
      "\tspeed: 0.1147s/iter; left time: 5498.9434s\n",
      "\titers: 300, epoch: 3 | loss: 0.0726208\n",
      "\tspeed: 0.1124s/iter; left time: 5374.3410s\n",
      "\titers: 400, epoch: 3 | loss: 0.0762616\n",
      "\tspeed: 0.1152s/iter; left time: 5497.1977s\n",
      "\titers: 500, epoch: 3 | loss: 0.0660659\n",
      "\tspeed: 0.1135s/iter; left time: 5404.0749s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651958\n",
      "\tspeed: 0.1157s/iter; left time: 5500.9620s\n",
      "\titers: 700, epoch: 3 | loss: 0.0897581\n",
      "\tspeed: 0.1144s/iter; left time: 5425.0195s\n",
      "\titers: 800, epoch: 3 | loss: 0.0652855\n",
      "\tspeed: 0.1165s/iter; left time: 5513.4883s\n",
      "\titers: 900, epoch: 3 | loss: 0.0668169\n",
      "\tspeed: 0.1150s/iter; left time: 5429.7491s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0626059\n",
      "\tspeed: 0.1127s/iter; left time: 5313.1327s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0549380\n",
      "\tspeed: 0.1151s/iter; left time: 5414.6272s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0706437\n",
      "\tspeed: 0.1127s/iter; left time: 5287.4627s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0740227\n",
      "\tspeed: 0.1168s/iter; left time: 5471.6214s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0664061\n",
      "\tspeed: 0.1158s/iter; left time: 5410.8156s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0757374\n",
      "\tspeed: 0.1141s/iter; left time: 5320.4736s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0855467\n",
      "\tspeed: 0.1139s/iter; left time: 5301.0619s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0756636\n",
      "\tspeed: 0.1169s/iter; left time: 5428.4163s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0538661\n",
      "\tspeed: 0.1141s/iter; left time: 5286.8755s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0783621\n",
      "\tspeed: 0.1174s/iter; left time: 5426.1842s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0645575\n",
      "\tspeed: 0.1167s/iter; left time: 5384.5331s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0941539\n",
      "\tspeed: 0.1144s/iter; left time: 5266.8994s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0742499\n",
      "\tspeed: 0.1152s/iter; left time: 5292.9174s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0604884\n",
      "\tspeed: 0.1164s/iter; left time: 5336.3567s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0683079\n",
      "\tspeed: 0.1133s/iter; left time: 5182.0659s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0758141\n",
      "\tspeed: 0.1135s/iter; left time: 5177.5742s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0615954\n",
      "\tspeed: 0.1145s/iter; left time: 5213.3850s\n",
      "Epoch: 3 cost time: 00h:05m:07.65s\n",
      "Epoch: 3 | Train Loss: 0.0687785 Vali Loss: 0.0725997 Test Loss: 0.0817405\n",
      "Validation loss decreased (0.074525 --> 0.072600).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0635042\n",
      "\tspeed: 1.0342s/iter; left time: 46908.5323s\n",
      "\titers: 200, epoch: 4 | loss: 0.0766480\n",
      "\tspeed: 0.1169s/iter; left time: 5290.6569s\n",
      "\titers: 300, epoch: 4 | loss: 0.0603194\n",
      "\tspeed: 0.1149s/iter; left time: 5190.9310s\n",
      "\titers: 400, epoch: 4 | loss: 0.0879604\n",
      "\tspeed: 0.1153s/iter; left time: 5196.5941s\n",
      "\titers: 500, epoch: 4 | loss: 0.0716096\n",
      "\tspeed: 0.1173s/iter; left time: 5274.9817s\n",
      "\titers: 600, epoch: 4 | loss: 0.0661810\n",
      "\tspeed: 0.1123s/iter; left time: 5036.5761s\n",
      "\titers: 700, epoch: 4 | loss: 0.0630168\n",
      "\tspeed: 0.1146s/iter; left time: 5130.1413s\n",
      "\titers: 800, epoch: 4 | loss: 0.0750177\n",
      "\tspeed: 0.1139s/iter; left time: 5085.6809s\n",
      "\titers: 900, epoch: 4 | loss: 0.0560872\n",
      "\tspeed: 0.1120s/iter; left time: 4991.7414s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0561997\n",
      "\tspeed: 0.1138s/iter; left time: 5061.1343s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0634589\n",
      "\tspeed: 0.1102s/iter; left time: 4889.6764s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0602584\n",
      "\tspeed: 0.1117s/iter; left time: 4944.1367s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0809753\n",
      "\tspeed: 0.1122s/iter; left time: 4952.7185s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0696282\n",
      "\tspeed: 0.1121s/iter; left time: 4940.2120s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0582918\n",
      "\tspeed: 0.1145s/iter; left time: 5032.7009s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0587403\n",
      "\tspeed: 0.1132s/iter; left time: 4966.3179s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0575944\n",
      "\tspeed: 0.1112s/iter; left time: 4867.8195s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0667402\n",
      "\tspeed: 0.1137s/iter; left time: 4962.0644s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0664581\n",
      "\tspeed: 0.1148s/iter; left time: 5002.5965s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0745811\n",
      "\tspeed: 0.1129s/iter; left time: 4907.8576s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0695823\n",
      "\tspeed: 0.1111s/iter; left time: 4817.2379s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0542657\n",
      "\tspeed: 0.1129s/iter; left time: 4882.6145s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0670997\n",
      "\tspeed: 0.1105s/iter; left time: 4770.8912s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0595331\n",
      "\tspeed: 0.1125s/iter; left time: 4843.8712s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0590853\n",
      "\tspeed: 0.1134s/iter; left time: 4869.7242s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0622148\n",
      "\tspeed: 0.1115s/iter; left time: 4778.2491s\n",
      "Epoch: 4 cost time: 00h:05m:03.21s\n",
      "Epoch: 4 | Train Loss: 0.0669450 Vali Loss: 0.0741470 Test Loss: 0.0824094\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0655506\n",
      "\tspeed: 1.0279s/iter; left time: 43876.3073s\n",
      "\titers: 200, epoch: 5 | loss: 0.0827420\n",
      "\tspeed: 0.1139s/iter; left time: 4849.6463s\n",
      "\titers: 300, epoch: 5 | loss: 0.0619815\n",
      "\tspeed: 0.1149s/iter; left time: 4883.1241s\n",
      "\titers: 400, epoch: 5 | loss: 0.0519550\n",
      "\tspeed: 0.1118s/iter; left time: 4738.8475s\n",
      "\titers: 500, epoch: 5 | loss: 0.0678731\n",
      "\tspeed: 0.1125s/iter; left time: 4758.4126s\n",
      "\titers: 600, epoch: 5 | loss: 0.0686385\n",
      "\tspeed: 0.1133s/iter; left time: 4778.2636s\n",
      "\titers: 700, epoch: 5 | loss: 0.0715512\n",
      "\tspeed: 0.1117s/iter; left time: 4700.8591s\n",
      "\titers: 800, epoch: 5 | loss: 0.0702961\n",
      "\tspeed: 0.1136s/iter; left time: 4770.0126s\n",
      "\titers: 900, epoch: 5 | loss: 0.0748347\n",
      "\tspeed: 0.1141s/iter; left time: 4778.6170s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0546004\n",
      "\tspeed: 0.1121s/iter; left time: 4682.2282s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0699323\n",
      "\tspeed: 0.1169s/iter; left time: 4871.2229s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0556661\n",
      "\tspeed: 0.1123s/iter; left time: 4670.5481s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0645581\n",
      "\tspeed: 0.1148s/iter; left time: 4764.2947s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0688608\n",
      "\tspeed: 0.1150s/iter; left time: 4758.5611s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0485171\n",
      "\tspeed: 0.1118s/iter; left time: 4615.0795s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0664087\n",
      "\tspeed: 0.1126s/iter; left time: 4638.3682s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0548556\n",
      "\tspeed: 0.1134s/iter; left time: 4659.2041s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0654618\n",
      "\tspeed: 0.1146s/iter; left time: 4695.4469s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0673945\n",
      "\tspeed: 0.1149s/iter; left time: 4697.6498s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0469344\n",
      "\tspeed: 0.1129s/iter; left time: 4602.5989s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0672865\n",
      "\tspeed: 0.1125s/iter; left time: 4578.5228s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0703256\n",
      "\tspeed: 0.1153s/iter; left time: 4679.2620s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0843584\n",
      "\tspeed: 0.1133s/iter; left time: 4586.4834s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0603486\n",
      "\tspeed: 0.1109s/iter; left time: 4477.8991s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761832\n",
      "\tspeed: 0.1141s/iter; left time: 4597.2399s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0514573\n",
      "\tspeed: 0.1127s/iter; left time: 4528.0888s\n",
      "Epoch: 5 cost time: 00h:05m:03.59s\n",
      "Epoch: 5 | Train Loss: 0.0649131 Vali Loss: 0.0754854 Test Loss: 0.0844793\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0773710\n",
      "\tspeed: 1.0069s/iter; left time: 40285.6555s\n",
      "\titers: 200, epoch: 6 | loss: 0.0613844\n",
      "\tspeed: 0.1138s/iter; left time: 4542.1712s\n",
      "\titers: 300, epoch: 6 | loss: 0.0576228\n",
      "\tspeed: 0.1128s/iter; left time: 4491.8380s\n",
      "\titers: 400, epoch: 6 | loss: 0.0596324\n",
      "\tspeed: 0.1127s/iter; left time: 4476.8733s\n",
      "\titers: 500, epoch: 6 | loss: 0.0579083\n",
      "\tspeed: 0.1122s/iter; left time: 4442.8573s\n",
      "\titers: 600, epoch: 6 | loss: 0.0726682\n",
      "\tspeed: 0.1135s/iter; left time: 4482.6344s\n",
      "\titers: 700, epoch: 6 | loss: 0.0726198\n",
      "\tspeed: 0.1146s/iter; left time: 4516.1612s\n",
      "\titers: 800, epoch: 6 | loss: 0.0645957\n",
      "\tspeed: 0.1146s/iter; left time: 4504.1012s\n",
      "\titers: 900, epoch: 6 | loss: 0.0725977\n",
      "\tspeed: 0.1156s/iter; left time: 4531.2633s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0638610\n",
      "\tspeed: 0.1151s/iter; left time: 4501.1788s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0556347\n",
      "\tspeed: 0.1140s/iter; left time: 4446.7623s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0472695\n",
      "\tspeed: 0.1152s/iter; left time: 4481.4860s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0792933\n",
      "\tspeed: 0.1131s/iter; left time: 4389.8681s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0675426\n",
      "\tspeed: 0.1117s/iter; left time: 4322.6630s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0533914\n",
      "\tspeed: 0.1132s/iter; left time: 4371.7503s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0623400\n",
      "\tspeed: 0.1111s/iter; left time: 4278.8181s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0586607\n",
      "\tspeed: 0.1123s/iter; left time: 4311.9919s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0620784\n",
      "\tspeed: 0.1118s/iter; left time: 4283.5127s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0596254\n",
      "\tspeed: 0.1134s/iter; left time: 4334.5241s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0587260\n",
      "\tspeed: 0.1135s/iter; left time: 4326.0426s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0641739\n",
      "\tspeed: 0.1125s/iter; left time: 4277.7994s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0735683\n",
      "\tspeed: 0.1132s/iter; left time: 4293.3688s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0535805\n",
      "\tspeed: 0.1139s/iter; left time: 4307.2052s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0671832\n",
      "\tspeed: 0.1129s/iter; left time: 4257.8680s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0640754\n",
      "\tspeed: 0.1123s/iter; left time: 4224.7128s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0475231\n",
      "\tspeed: 0.1124s/iter; left time: 4216.1819s\n",
      "Epoch: 6 cost time: 00h:05m:03.11s\n",
      "Epoch: 6 | Train Loss: 0.0630431 Vali Loss: 0.0751659 Test Loss: 0.0838745\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0737136\n",
      "\tspeed: 1.0189s/iter; left time: 38040.9426s\n",
      "\titers: 200, epoch: 7 | loss: 0.0773198\n",
      "\tspeed: 0.1144s/iter; left time: 4260.6138s\n",
      "\titers: 300, epoch: 7 | loss: 0.0562561\n",
      "\tspeed: 0.1137s/iter; left time: 4224.2310s\n",
      "\titers: 400, epoch: 7 | loss: 0.0674500\n",
      "\tspeed: 0.1141s/iter; left time: 4224.7118s\n",
      "\titers: 500, epoch: 7 | loss: 0.0538294\n",
      "\tspeed: 0.1153s/iter; left time: 4259.2230s\n",
      "\titers: 600, epoch: 7 | loss: 0.0683504\n",
      "\tspeed: 0.1155s/iter; left time: 4254.6560s\n",
      "\titers: 700, epoch: 7 | loss: 0.0571279\n",
      "\tspeed: 0.1155s/iter; left time: 4244.5785s\n",
      "\titers: 800, epoch: 7 | loss: 0.0532890\n",
      "\tspeed: 0.1145s/iter; left time: 4193.3060s\n",
      "\titers: 900, epoch: 7 | loss: 0.0591477\n",
      "\tspeed: 0.1146s/iter; left time: 4187.5830s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0772009\n",
      "\tspeed: 0.1166s/iter; left time: 4248.5963s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0556193\n",
      "\tspeed: 0.1131s/iter; left time: 4110.9963s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0560169\n",
      "\tspeed: 0.1131s/iter; left time: 4097.1813s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0597722\n",
      "\tspeed: 0.1163s/iter; left time: 4204.4354s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0604149\n",
      "\tspeed: 0.1141s/iter; left time: 4111.2249s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0788255\n",
      "\tspeed: 0.1133s/iter; left time: 4070.2651s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0704609\n",
      "\tspeed: 0.1124s/iter; left time: 4028.6042s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0629530\n",
      "\tspeed: 0.1124s/iter; left time: 4015.4502s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0610598\n",
      "\tspeed: 0.1121s/iter; left time: 3994.9430s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0446289\n",
      "\tspeed: 0.1151s/iter; left time: 4089.8119s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0594098\n",
      "\tspeed: 0.1126s/iter; left time: 3991.0313s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0708012\n",
      "\tspeed: 0.1122s/iter; left time: 3965.9981s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0720370\n",
      "\tspeed: 0.1143s/iter; left time: 4026.7334s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0789985\n",
      "\tspeed: 0.1111s/iter; left time: 3903.4207s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0545131\n",
      "\tspeed: 0.1114s/iter; left time: 3902.6692s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0678101\n",
      "\tspeed: 0.1142s/iter; left time: 3990.9587s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0619310\n",
      "\tspeed: 0.1112s/iter; left time: 3873.8694s\n",
      "Epoch: 7 cost time: 00h:05m:04.62s\n",
      "Epoch: 7 | Train Loss: 0.0613680 Vali Loss: 0.0761664 Test Loss: 0.0845503\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0591804\n",
      "\tspeed: 1.0174s/iter; left time: 35265.2915s\n",
      "\titers: 200, epoch: 8 | loss: 0.0601895\n",
      "\tspeed: 0.1139s/iter; left time: 3936.2679s\n",
      "\titers: 300, epoch: 8 | loss: 0.0537779\n",
      "\tspeed: 0.1147s/iter; left time: 3951.2009s\n",
      "\titers: 400, epoch: 8 | loss: 0.0596471\n",
      "\tspeed: 0.1150s/iter; left time: 3952.8149s\n",
      "\titers: 500, epoch: 8 | loss: 0.0748389\n",
      "\tspeed: 0.1152s/iter; left time: 3946.7594s\n",
      "\titers: 600, epoch: 8 | loss: 0.0619526\n",
      "\tspeed: 0.1142s/iter; left time: 3900.6351s\n",
      "\titers: 700, epoch: 8 | loss: 0.0492099\n",
      "\tspeed: 0.1158s/iter; left time: 3943.2424s\n",
      "\titers: 800, epoch: 8 | loss: 0.0553380\n",
      "\tspeed: 0.1146s/iter; left time: 3893.2388s\n",
      "\titers: 900, epoch: 8 | loss: 0.0660264\n",
      "\tspeed: 0.1124s/iter; left time: 3804.8857s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0512587\n",
      "\tspeed: 0.1160s/iter; left time: 3916.6093s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0606477\n",
      "\tspeed: 0.1154s/iter; left time: 3885.3929s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0488810\n",
      "\tspeed: 0.1150s/iter; left time: 3860.4352s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0636803\n",
      "\tspeed: 0.1152s/iter; left time: 3854.7517s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0603122\n",
      "\tspeed: 0.1152s/iter; left time: 3843.0386s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0753196\n",
      "\tspeed: 0.1143s/iter; left time: 3802.7447s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0476781\n",
      "\tspeed: 0.1152s/iter; left time: 3820.4807s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0543786\n",
      "\tspeed: 0.1175s/iter; left time: 3885.4586s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0479689\n",
      "\tspeed: 0.1144s/iter; left time: 3772.0123s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0567113\n",
      "\tspeed: 0.1152s/iter; left time: 3786.4991s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0584764\n",
      "\tspeed: 0.1148s/iter; left time: 3762.3402s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0674268\n",
      "\tspeed: 0.1149s/iter; left time: 3753.6997s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0617867\n",
      "\tspeed: 0.1140s/iter; left time: 3711.0942s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0588714\n",
      "\tspeed: 0.1135s/iter; left time: 3684.6010s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0716897\n",
      "\tspeed: 0.1161s/iter; left time: 3756.1475s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0649153\n",
      "\tspeed: 0.1134s/iter; left time: 3658.8856s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0538609\n",
      "\tspeed: 0.1148s/iter; left time: 3691.1308s\n",
      "Epoch: 8 cost time: 00h:05m:07.49s\n",
      "Epoch: 8 | Train Loss: 0.0601105 Vali Loss: 0.0785164 Test Loss: 0.0872316\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018459701910614967, rmse:0.13586649298667908, mae:0.08174050599336624, rse:0.5256097912788391\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 96: 00h:53m:37.48s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 16:51:34,589] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 16:51:35,763] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 16:51:35,764] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 16:51:35,764] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 16:51:35,879] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 16:51:35,879] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 16:51:36,608] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 16:51:36,609] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 16:51:36,609] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 16:51:36,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 16:51:36,611] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 16:51:36,611] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 16:51:36,611] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 16:51:37,007] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 16:51:37,008] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 312.97 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,202] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 16:51:37,203] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,203] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 313.01 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,203] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 16:51:37,377] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 16:51:37,378] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 16:51:37,379] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 313.01 GB, percent = 41.5%\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 16:51:37,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 16:51:37,381] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd374ba9ed0>\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 16:51:37,382] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 16:51:37,383] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 16:51:37,384] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1461806\n",
      "\tspeed: 0.1728s/iter; left time: 9201.6091s\n",
      "\titers: 200, epoch: 1 | loss: 0.1501896\n",
      "\tspeed: 0.1258s/iter; left time: 6685.5717s\n",
      "\titers: 300, epoch: 1 | loss: 0.1255645\n",
      "\tspeed: 0.1265s/iter; left time: 6707.1471s\n",
      "\titers: 400, epoch: 1 | loss: 0.0948358\n",
      "\tspeed: 0.1274s/iter; left time: 6742.0429s\n",
      "\titers: 500, epoch: 1 | loss: 0.0860671\n",
      "\tspeed: 0.1254s/iter; left time: 6627.6459s\n",
      "\titers: 600, epoch: 1 | loss: 0.0797759\n",
      "\tspeed: 0.1268s/iter; left time: 6686.3868s\n",
      "\titers: 700, epoch: 1 | loss: 0.0779733\n",
      "\tspeed: 0.1257s/iter; left time: 6618.0314s\n",
      "\titers: 800, epoch: 1 | loss: 0.0854992\n",
      "\tspeed: 0.1223s/iter; left time: 6426.5638s\n",
      "\titers: 900, epoch: 1 | loss: 0.0824629\n",
      "\tspeed: 0.1239s/iter; left time: 6498.8014s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0905522\n",
      "\tspeed: 0.1266s/iter; left time: 6625.9316s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0848468\n",
      "\tspeed: 0.1262s/iter; left time: 6591.7380s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0831938\n",
      "\tspeed: 0.1253s/iter; left time: 6531.3858s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0840276\n",
      "\tspeed: 0.1238s/iter; left time: 6443.5033s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0715065\n",
      "\tspeed: 0.1244s/iter; left time: 6459.2688s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0868700\n",
      "\tspeed: 0.1237s/iter; left time: 6412.8650s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0816183\n",
      "\tspeed: 0.1259s/iter; left time: 6516.6899s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0746674\n",
      "\tspeed: 0.1258s/iter; left time: 6496.2027s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0842547\n",
      "\tspeed: 0.1277s/iter; left time: 6581.8055s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0750883\n",
      "\tspeed: 0.1244s/iter; left time: 6396.8053s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0767707\n",
      "\tspeed: 0.1264s/iter; left time: 6489.2698s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0823143\n",
      "\tspeed: 0.1248s/iter; left time: 6394.4898s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0773541\n",
      "\tspeed: 0.1260s/iter; left time: 6441.5222s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0822798\n",
      "\tspeed: 0.1244s/iter; left time: 6351.4653s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0697680\n",
      "\tspeed: 0.1264s/iter; left time: 6440.4329s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0781708\n",
      "\tspeed: 0.1239s/iter; left time: 6299.2218s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0923201\n",
      "\tspeed: 0.1256s/iter; left time: 6372.9236s\n",
      "Epoch: 1 cost time: 00h:05m:35.64s\n",
      "Epoch: 1 | Train Loss: 0.0882067 Vali Loss: 0.0789939 Test Loss: 0.0885378\n",
      "Validation loss decreased (inf --> 0.078994).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0682366\n",
      "\tspeed: 1.1571s/iter; left time: 58517.2971s\n",
      "\titers: 200, epoch: 2 | loss: 0.0756340\n",
      "\tspeed: 0.1126s/iter; left time: 5682.0940s\n",
      "\titers: 300, epoch: 2 | loss: 0.0757687\n",
      "\tspeed: 0.1163s/iter; left time: 5856.8647s\n",
      "\titers: 400, epoch: 2 | loss: 0.0809062\n",
      "\tspeed: 0.1147s/iter; left time: 5765.0768s\n",
      "\titers: 500, epoch: 2 | loss: 0.0785947\n",
      "\tspeed: 0.1158s/iter; left time: 5810.3476s\n",
      "\titers: 600, epoch: 2 | loss: 0.0788290\n",
      "\tspeed: 0.1148s/iter; left time: 5748.4740s\n",
      "\titers: 700, epoch: 2 | loss: 0.0643081\n",
      "\tspeed: 0.1140s/iter; left time: 5699.4764s\n",
      "\titers: 800, epoch: 2 | loss: 0.0724190\n",
      "\tspeed: 0.1153s/iter; left time: 5749.9291s\n",
      "\titers: 900, epoch: 2 | loss: 0.0765032\n",
      "\tspeed: 0.1134s/iter; left time: 5645.3463s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0680958\n",
      "\tspeed: 0.1127s/iter; left time: 5596.1018s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0636572\n",
      "\tspeed: 0.1140s/iter; left time: 5650.9959s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0647527\n",
      "\tspeed: 0.1138s/iter; left time: 5628.4425s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0695712\n",
      "\tspeed: 0.1125s/iter; left time: 5556.3360s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0781222\n",
      "\tspeed: 0.1108s/iter; left time: 5459.3519s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0884119\n",
      "\tspeed: 0.1148s/iter; left time: 5646.0649s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0732782\n",
      "\tspeed: 0.1128s/iter; left time: 5537.6323s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0834294\n",
      "\tspeed: 0.1125s/iter; left time: 5510.1069s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0762573\n",
      "\tspeed: 0.1135s/iter; left time: 5545.8452s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0638730\n",
      "\tspeed: 0.1122s/iter; left time: 5473.9790s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0719824\n",
      "\tspeed: 0.1118s/iter; left time: 5440.0921s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0811448\n",
      "\tspeed: 0.1130s/iter; left time: 5487.5219s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0702735\n",
      "\tspeed: 0.1139s/iter; left time: 5522.9308s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0598425\n",
      "\tspeed: 0.1125s/iter; left time: 5441.5109s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0801101\n",
      "\tspeed: 0.1135s/iter; left time: 5481.4252s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0596308\n",
      "\tspeed: 0.1117s/iter; left time: 5383.4181s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0712510\n",
      "\tspeed: 0.1133s/iter; left time: 5448.0658s\n",
      "Epoch: 2 cost time: 00h:05m:03.32s\n",
      "Epoch: 2 | Train Loss: 0.0743254 Vali Loss: 0.0777038 Test Loss: 0.0871744\n",
      "Validation loss decreased (0.078994 --> 0.077704).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0675267\n",
      "\tspeed: 1.0175s/iter; left time: 48745.3355s\n",
      "\titers: 200, epoch: 3 | loss: 0.0488273\n",
      "\tspeed: 0.1142s/iter; left time: 5457.8495s\n",
      "\titers: 300, epoch: 3 | loss: 0.0692624\n",
      "\tspeed: 0.1129s/iter; left time: 5385.2999s\n",
      "\titers: 400, epoch: 3 | loss: 0.0781940\n",
      "\tspeed: 0.1140s/iter; left time: 5425.0041s\n",
      "\titers: 500, epoch: 3 | loss: 0.0633163\n",
      "\tspeed: 0.1146s/iter; left time: 5446.1456s\n",
      "\titers: 600, epoch: 3 | loss: 0.0786292\n",
      "\tspeed: 0.1144s/iter; left time: 5422.7847s\n",
      "\titers: 700, epoch: 3 | loss: 0.0660528\n",
      "\tspeed: 0.1128s/iter; left time: 5334.5753s\n",
      "\titers: 800, epoch: 3 | loss: 0.0873496\n",
      "\tspeed: 0.1138s/iter; left time: 5373.4053s\n",
      "\titers: 900, epoch: 3 | loss: 0.0665047\n",
      "\tspeed: 0.1121s/iter; left time: 5280.7909s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0717470\n",
      "\tspeed: 0.1128s/iter; left time: 5303.9321s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0673361\n",
      "\tspeed: 0.1147s/iter; left time: 5382.1805s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0878783\n",
      "\tspeed: 0.1125s/iter; left time: 5267.7362s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0672138\n",
      "\tspeed: 0.1147s/iter; left time: 5357.9900s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0739039\n",
      "\tspeed: 0.1151s/iter; left time: 5365.0421s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0644442\n",
      "\tspeed: 0.1117s/iter; left time: 5195.0276s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0913087\n",
      "\tspeed: 0.1122s/iter; left time: 5206.8303s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0859381\n",
      "\tspeed: 0.1127s/iter; left time: 5219.3783s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0756321\n",
      "\tspeed: 0.1105s/iter; left time: 5107.9640s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0888247\n",
      "\tspeed: 0.1094s/iter; left time: 5043.7622s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0595378\n",
      "\tspeed: 0.1115s/iter; left time: 5127.8967s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0935414\n",
      "\tspeed: 0.1130s/iter; left time: 5188.4238s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0669725\n",
      "\tspeed: 0.1131s/iter; left time: 5178.5974s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0605008\n",
      "\tspeed: 0.1144s/iter; left time: 5227.8167s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0831451\n",
      "\tspeed: 0.1111s/iter; left time: 5066.6465s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0697700\n",
      "\tspeed: 0.1123s/iter; left time: 5112.1039s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0661712\n",
      "\tspeed: 0.1122s/iter; left time: 5095.2971s\n",
      "Epoch: 3 cost time: 00h:05m:01.88s\n",
      "Epoch: 3 | Train Loss: 0.0718358 Vali Loss: 0.0789278 Test Loss: 0.0876987\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0648829\n",
      "\tspeed: 0.9919s/iter; left time: 44872.7863s\n",
      "\titers: 200, epoch: 4 | loss: 0.0765537\n",
      "\tspeed: 0.1145s/iter; left time: 5167.3367s\n",
      "\titers: 300, epoch: 4 | loss: 0.0551690\n",
      "\tspeed: 0.1123s/iter; left time: 5059.4413s\n",
      "\titers: 400, epoch: 4 | loss: 0.0743216\n",
      "\tspeed: 0.1145s/iter; left time: 5144.3220s\n",
      "\titers: 500, epoch: 4 | loss: 0.0566139\n",
      "\tspeed: 0.1147s/iter; left time: 5143.2865s\n",
      "\titers: 600, epoch: 4 | loss: 0.0790194\n",
      "\tspeed: 0.1133s/iter; left time: 5069.6998s\n",
      "\titers: 700, epoch: 4 | loss: 0.0843454\n",
      "\tspeed: 0.1173s/iter; left time: 5234.2535s\n",
      "\titers: 800, epoch: 4 | loss: 0.0670473\n",
      "\tspeed: 0.1154s/iter; left time: 5138.4053s\n",
      "\titers: 900, epoch: 4 | loss: 0.0606182\n",
      "\tspeed: 0.1156s/iter; left time: 5136.3097s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0734311\n",
      "\tspeed: 0.1161s/iter; left time: 5148.5885s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0760170\n",
      "\tspeed: 0.1164s/iter; left time: 5150.3318s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0643949\n",
      "\tspeed: 0.1147s/iter; left time: 5062.8959s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0647174\n",
      "\tspeed: 0.1145s/iter; left time: 5043.0254s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0552365\n",
      "\tspeed: 0.1156s/iter; left time: 5077.9105s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0723340\n",
      "\tspeed: 0.1150s/iter; left time: 5040.2806s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0671190\n",
      "\tspeed: 0.1131s/iter; left time: 4948.4789s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0635535\n",
      "\tspeed: 0.1122s/iter; left time: 4895.0363s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0668017\n",
      "\tspeed: 0.1162s/iter; left time: 5058.7632s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0685256\n",
      "\tspeed: 0.1150s/iter; left time: 4994.0688s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0721530\n",
      "\tspeed: 0.1153s/iter; left time: 4996.1174s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0882780\n",
      "\tspeed: 0.1124s/iter; left time: 4859.0463s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0493336\n",
      "\tspeed: 0.1148s/iter; left time: 4954.4873s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0750259\n",
      "\tspeed: 0.1153s/iter; left time: 4964.6474s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0622560\n",
      "\tspeed: 0.1117s/iter; left time: 4798.4092s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0759433\n",
      "\tspeed: 0.1141s/iter; left time: 4887.5694s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0722388\n",
      "\tspeed: 0.1123s/iter; left time: 4800.1817s\n",
      "Epoch: 4 cost time: 00h:05m:05.82s\n",
      "Epoch: 4 | Train Loss: 0.0692130 Vali Loss: 0.0806310 Test Loss: 0.0891718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0739205\n",
      "\tspeed: 1.0042s/iter; left time: 42753.4276s\n",
      "\titers: 200, epoch: 5 | loss: 0.0671281\n",
      "\tspeed: 0.1143s/iter; left time: 4853.5783s\n",
      "\titers: 300, epoch: 5 | loss: 0.0831495\n",
      "\tspeed: 0.1165s/iter; left time: 4937.0348s\n",
      "\titers: 400, epoch: 5 | loss: 0.0562803\n",
      "\tspeed: 0.1164s/iter; left time: 4918.9635s\n",
      "\titers: 500, epoch: 5 | loss: 0.0612629\n",
      "\tspeed: 0.1151s/iter; left time: 4855.1824s\n",
      "\titers: 600, epoch: 5 | loss: 0.0725645\n",
      "\tspeed: 0.1160s/iter; left time: 4880.8346s\n",
      "\titers: 700, epoch: 5 | loss: 0.0736510\n",
      "\tspeed: 0.1155s/iter; left time: 4846.2801s\n",
      "\titers: 800, epoch: 5 | loss: 0.0621689\n",
      "\tspeed: 0.1132s/iter; left time: 4738.0259s\n",
      "\titers: 900, epoch: 5 | loss: 0.0703253\n",
      "\tspeed: 0.1148s/iter; left time: 4794.1539s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0583030\n",
      "\tspeed: 0.1140s/iter; left time: 4751.3104s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0703888\n",
      "\tspeed: 0.1148s/iter; left time: 4771.1987s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0585797\n",
      "\tspeed: 0.1134s/iter; left time: 4704.2690s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0742435\n",
      "\tspeed: 0.1126s/iter; left time: 4659.8960s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0719956\n",
      "\tspeed: 0.1149s/iter; left time: 4742.0219s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0606002\n",
      "\tspeed: 0.1151s/iter; left time: 4737.2574s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0692182\n",
      "\tspeed: 0.1123s/iter; left time: 4611.1768s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0654579\n",
      "\tspeed: 0.1119s/iter; left time: 4585.1680s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0808130\n",
      "\tspeed: 0.1136s/iter; left time: 4642.1484s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0635292\n",
      "\tspeed: 0.1130s/iter; left time: 4606.2225s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0533435\n",
      "\tspeed: 0.1117s/iter; left time: 4544.1873s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0679128\n",
      "\tspeed: 0.1130s/iter; left time: 4586.4621s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0734313\n",
      "\tspeed: 0.1126s/iter; left time: 4556.3801s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0613091\n",
      "\tspeed: 0.1141s/iter; left time: 4607.2712s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0723060\n",
      "\tspeed: 0.1129s/iter; left time: 4547.2335s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0571138\n",
      "\tspeed: 0.1128s/iter; left time: 4530.7935s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0643532\n",
      "\tspeed: 0.1128s/iter; left time: 4520.1549s\n",
      "Epoch: 5 cost time: 00h:05m:04.35s\n",
      "Epoch: 5 | Train Loss: 0.0666898 Vali Loss: 0.0816376 Test Loss: 0.0911526\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0592945\n",
      "\tspeed: 0.9876s/iter; left time: 39411.1732s\n",
      "\titers: 200, epoch: 6 | loss: 0.0701825\n",
      "\tspeed: 0.1144s/iter; left time: 4555.2524s\n",
      "\titers: 300, epoch: 6 | loss: 0.0733728\n",
      "\tspeed: 0.1133s/iter; left time: 4498.4615s\n",
      "\titers: 400, epoch: 6 | loss: 0.0500985\n",
      "\tspeed: 0.1121s/iter; left time: 4440.6552s\n",
      "\titers: 500, epoch: 6 | loss: 0.0708840\n",
      "\tspeed: 0.1141s/iter; left time: 4508.6746s\n",
      "\titers: 600, epoch: 6 | loss: 0.0746798\n",
      "\tspeed: 0.1151s/iter; left time: 4534.9810s\n",
      "\titers: 700, epoch: 6 | loss: 0.0672090\n",
      "\tspeed: 0.1126s/iter; left time: 4426.7414s\n",
      "\titers: 800, epoch: 6 | loss: 0.0675267\n",
      "\tspeed: 0.1126s/iter; left time: 4416.2322s\n",
      "\titers: 900, epoch: 6 | loss: 0.0650174\n",
      "\tspeed: 0.1131s/iter; left time: 4422.3891s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0685931\n",
      "\tspeed: 0.1135s/iter; left time: 4427.0639s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0609383\n",
      "\tspeed: 0.1119s/iter; left time: 4351.8524s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0727255\n",
      "\tspeed: 0.1125s/iter; left time: 4365.8285s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0623817\n",
      "\tspeed: 0.1120s/iter; left time: 4335.5399s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603095\n",
      "\tspeed: 0.1135s/iter; left time: 4380.0029s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0660724\n",
      "\tspeed: 0.1131s/iter; left time: 4353.1481s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0712819\n",
      "\tspeed: 0.1116s/iter; left time: 4285.8897s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0682647\n",
      "\tspeed: 0.1127s/iter; left time: 4318.5119s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0655491\n",
      "\tspeed: 0.1132s/iter; left time: 4325.5446s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0615094\n",
      "\tspeed: 0.1136s/iter; left time: 4329.7846s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0686113\n",
      "\tspeed: 0.1129s/iter; left time: 4292.6018s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0684695\n",
      "\tspeed: 0.1131s/iter; left time: 4286.8611s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0678513\n",
      "\tspeed: 0.1119s/iter; left time: 4230.2983s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0722934\n",
      "\tspeed: 0.1124s/iter; left time: 4237.9501s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0518117\n",
      "\tspeed: 0.1115s/iter; left time: 4194.3563s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0615523\n",
      "\tspeed: 0.1132s/iter; left time: 4244.6177s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0523729\n",
      "\tspeed: 0.1137s/iter; left time: 4252.3758s\n",
      "Epoch: 6 cost time: 00h:05m:01.92s\n",
      "Epoch: 6 | Train Loss: 0.0646321 Vali Loss: 0.0833404 Test Loss: 0.0923264\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0626126\n",
      "\tspeed: 0.9871s/iter; left time: 36759.9347s\n",
      "\titers: 200, epoch: 7 | loss: 0.0635089\n",
      "\tspeed: 0.1141s/iter; left time: 4239.2426s\n",
      "\titers: 300, epoch: 7 | loss: 0.0645225\n",
      "\tspeed: 0.1116s/iter; left time: 4132.2544s\n",
      "\titers: 400, epoch: 7 | loss: 0.0587169\n",
      "\tspeed: 0.1136s/iter; left time: 4194.6714s\n",
      "\titers: 500, epoch: 7 | loss: 0.0679099\n",
      "\tspeed: 0.1126s/iter; left time: 4147.4079s\n",
      "\titers: 600, epoch: 7 | loss: 0.0761093\n",
      "\tspeed: 0.1128s/iter; left time: 4143.3336s\n",
      "\titers: 700, epoch: 7 | loss: 0.0614073\n",
      "\tspeed: 0.1146s/iter; left time: 4200.4443s\n",
      "\titers: 800, epoch: 7 | loss: 0.0606054\n",
      "\tspeed: 0.1138s/iter; left time: 4156.8729s\n",
      "\titers: 900, epoch: 7 | loss: 0.0640666\n",
      "\tspeed: 0.1133s/iter; left time: 4128.3271s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0652940\n",
      "\tspeed: 0.1120s/iter; left time: 4068.4261s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0591778\n",
      "\tspeed: 0.1137s/iter; left time: 4122.0008s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0599352\n",
      "\tspeed: 0.1132s/iter; left time: 4091.2681s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0674185\n",
      "\tspeed: 0.1136s/iter; left time: 4094.0902s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0645608\n",
      "\tspeed: 0.1127s/iter; left time: 4048.6741s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0626351\n",
      "\tspeed: 0.1126s/iter; left time: 4036.2563s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0564335\n",
      "\tspeed: 0.1140s/iter; left time: 4075.1212s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0628764\n",
      "\tspeed: 0.1123s/iter; left time: 4002.8423s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0579112\n",
      "\tspeed: 0.1133s/iter; left time: 4025.9566s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0576657\n",
      "\tspeed: 0.1140s/iter; left time: 4039.2995s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0645774\n",
      "\tspeed: 0.1120s/iter; left time: 3958.6573s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0611573\n",
      "\tspeed: 0.1133s/iter; left time: 3991.7090s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0668616\n",
      "\tspeed: 0.1108s/iter; left time: 3891.7385s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0692711\n",
      "\tspeed: 0.1121s/iter; left time: 3927.3236s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0704450\n",
      "\tspeed: 0.1122s/iter; left time: 3920.1121s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0594643\n",
      "\tspeed: 0.1145s/iter; left time: 3987.4141s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0567257\n",
      "\tspeed: 0.1143s/iter; left time: 3971.3102s\n",
      "Epoch: 7 cost time: 00h:05m:01.84s\n",
      "Epoch: 7 | Train Loss: 0.0629600 Vali Loss: 0.0832727 Test Loss: 0.0938695\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02040581777691841, rmse:0.142848938703537, mae:0.08717440813779831, rse:0.5534131526947021\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 168: 00h:46m:39.29s\n",
      "\n",
      "Intermediate time for FR: 03h:12m:54.28s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-03 17:38:13,775] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 17:38:14,961] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 17:38:14,962] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 17:38:14,962] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 17:38:15,055] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 17:38:15,055] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 17:38:15,742] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 17:38:15,744] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 17:38:15,744] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 17:38:15,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 17:38:15,745] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 17:38:15,745] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 17:38:15,745] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 17:38:16,138] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 17:38:16,141] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,141] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.71 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,321] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 17:38:16,322] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,322] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.71 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,322] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 17:38:16,520] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 17:38:16,521] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 17:38:16,521] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.6 GB, percent = 41.3%\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 17:38:16,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 17:38:16,523] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7e6cfcd050>\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 17:38:16,524] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 17:38:16,525] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 17:38:16,526] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1961632\n",
      "\tspeed: 0.1721s/iter; left time: 9212.2875s\n",
      "\titers: 200, epoch: 1 | loss: 0.1897899\n",
      "\tspeed: 0.1240s/iter; left time: 6625.8756s\n",
      "\titers: 300, epoch: 1 | loss: 0.1400119\n",
      "\tspeed: 0.1264s/iter; left time: 6740.8285s\n",
      "\titers: 400, epoch: 1 | loss: 0.1414553\n",
      "\tspeed: 0.1249s/iter; left time: 6646.2304s\n",
      "\titers: 500, epoch: 1 | loss: 0.1245781\n",
      "\tspeed: 0.1244s/iter; left time: 6608.3626s\n",
      "\titers: 600, epoch: 1 | loss: 0.0947767\n",
      "\tspeed: 0.1283s/iter; left time: 6802.8842s\n",
      "\titers: 700, epoch: 1 | loss: 0.0984841\n",
      "\tspeed: 0.1260s/iter; left time: 6670.1009s\n",
      "\titers: 800, epoch: 1 | loss: 0.0828382\n",
      "\tspeed: 0.1247s/iter; left time: 6585.5102s\n",
      "\titers: 900, epoch: 1 | loss: 0.0719742\n",
      "\tspeed: 0.1284s/iter; left time: 6768.0768s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1195307\n",
      "\tspeed: 0.1332s/iter; left time: 7008.2822s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0817368\n",
      "\tspeed: 0.1278s/iter; left time: 6709.7699s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0921901\n",
      "\tspeed: 0.1293s/iter; left time: 6778.1289s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0989637\n",
      "\tspeed: 0.1268s/iter; left time: 6632.0273s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0795155\n",
      "\tspeed: 0.1257s/iter; left time: 6565.8398s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0898348\n",
      "\tspeed: 0.1275s/iter; left time: 6645.7793s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0705952\n",
      "\tspeed: 0.1250s/iter; left time: 6502.1500s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0890884\n",
      "\tspeed: 0.1274s/iter; left time: 6616.2715s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0708546\n",
      "\tspeed: 0.1278s/iter; left time: 6621.3609s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0823215\n",
      "\tspeed: 0.1279s/iter; left time: 6614.7988s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0903587\n",
      "\tspeed: 0.1271s/iter; left time: 6558.6651s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0667403\n",
      "\tspeed: 0.1263s/iter; left time: 6506.3228s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0840566\n",
      "\tspeed: 0.1270s/iter; left time: 6530.3758s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1007551\n",
      "\tspeed: 0.1270s/iter; left time: 6517.5440s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0791763\n",
      "\tspeed: 0.1250s/iter; left time: 6404.0763s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0936417\n",
      "\tspeed: 0.1246s/iter; left time: 6368.7051s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0681735\n",
      "\tspeed: 0.1263s/iter; left time: 6441.6549s\n",
      "Epoch: 1 cost time: 00h:05m:40.50s\n",
      "Epoch: 1 | Train Loss: 0.1042742 Vali Loss: 0.0697548 Test Loss: 0.0724680\n",
      "Validation loss decreased (inf --> 0.069755).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0968200\n",
      "\tspeed: 1.1964s/iter; left time: 60823.4643s\n",
      "\titers: 200, epoch: 2 | loss: 0.0908526\n",
      "\tspeed: 0.1135s/iter; left time: 5759.7286s\n",
      "\titers: 300, epoch: 2 | loss: 0.0880430\n",
      "\tspeed: 0.1132s/iter; left time: 5733.4311s\n",
      "\titers: 400, epoch: 2 | loss: 0.0909087\n",
      "\tspeed: 0.1148s/iter; left time: 5800.0278s\n",
      "\titers: 500, epoch: 2 | loss: 0.0755832\n",
      "\tspeed: 0.1150s/iter; left time: 5800.8973s\n",
      "\titers: 600, epoch: 2 | loss: 0.0719677\n",
      "\tspeed: 0.1130s/iter; left time: 5690.1754s\n",
      "\titers: 700, epoch: 2 | loss: 0.0632761\n",
      "\tspeed: 0.1161s/iter; left time: 5833.7360s\n",
      "\titers: 800, epoch: 2 | loss: 0.0799158\n",
      "\tspeed: 0.1135s/iter; left time: 5689.4282s\n",
      "\titers: 900, epoch: 2 | loss: 0.0778257\n",
      "\tspeed: 0.1120s/iter; left time: 5604.9770s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0686750\n",
      "\tspeed: 0.1139s/iter; left time: 5689.7352s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0759580\n",
      "\tspeed: 0.1121s/iter; left time: 5589.1426s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0850438\n",
      "\tspeed: 0.1132s/iter; left time: 5628.8998s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0768398\n",
      "\tspeed: 0.1126s/iter; left time: 5591.4223s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0734043\n",
      "\tspeed: 0.1126s/iter; left time: 5576.2087s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0702756\n",
      "\tspeed: 0.1134s/iter; left time: 5608.4505s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0901470\n",
      "\tspeed: 0.1134s/iter; left time: 5595.7804s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0736003\n",
      "\tspeed: 0.1132s/iter; left time: 5574.8807s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0812662\n",
      "\tspeed: 0.1141s/iter; left time: 5607.1686s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0856911\n",
      "\tspeed: 0.1117s/iter; left time: 5479.8548s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0710515\n",
      "\tspeed: 0.1123s/iter; left time: 5494.9948s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0731606\n",
      "\tspeed: 0.1108s/iter; left time: 5413.7384s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0908238\n",
      "\tspeed: 0.1137s/iter; left time: 5541.7092s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0716156\n",
      "\tspeed: 0.1145s/iter; left time: 5569.4548s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0662761\n",
      "\tspeed: 0.1124s/iter; left time: 5454.4864s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0705364\n",
      "\tspeed: 0.1146s/iter; left time: 5552.5707s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0787798\n",
      "\tspeed: 0.1157s/iter; left time: 5591.1339s\n",
      "Epoch: 2 cost time: 00h:05m:04.83s\n",
      "Epoch: 2 | Train Loss: 0.0792366 Vali Loss: 0.0630193 Test Loss: 0.0664905\n",
      "Validation loss decreased (0.069755 --> 0.063019).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0837417\n",
      "\tspeed: 1.0566s/iter; left time: 50887.1933s\n",
      "\titers: 200, epoch: 3 | loss: 0.0720257\n",
      "\tspeed: 0.1153s/iter; left time: 5539.1675s\n",
      "\titers: 300, epoch: 3 | loss: 0.0743371\n",
      "\tspeed: 0.1143s/iter; left time: 5483.9913s\n",
      "\titers: 400, epoch: 3 | loss: 0.0783527\n",
      "\tspeed: 0.1124s/iter; left time: 5378.7126s\n",
      "\titers: 500, epoch: 3 | loss: 0.0714736\n",
      "\tspeed: 0.1124s/iter; left time: 5367.3301s\n",
      "\titers: 600, epoch: 3 | loss: 0.0732643\n",
      "\tspeed: 0.1158s/iter; left time: 5517.5317s\n",
      "\titers: 700, epoch: 3 | loss: 0.0646055\n",
      "\tspeed: 0.1127s/iter; left time: 5357.9279s\n",
      "\titers: 800, epoch: 3 | loss: 0.0739734\n",
      "\tspeed: 0.1117s/iter; left time: 5300.4545s\n",
      "\titers: 900, epoch: 3 | loss: 0.0694846\n",
      "\tspeed: 0.1164s/iter; left time: 5513.7908s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0750011\n",
      "\tspeed: 0.1145s/iter; left time: 5409.1384s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0742698\n",
      "\tspeed: 0.1131s/iter; left time: 5331.3288s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0718982\n",
      "\tspeed: 0.1133s/iter; left time: 5332.9434s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0686082\n",
      "\tspeed: 0.1150s/iter; left time: 5398.8493s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0775685\n",
      "\tspeed: 0.1141s/iter; left time: 5346.2458s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0676080\n",
      "\tspeed: 0.1129s/iter; left time: 5281.1198s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0703222\n",
      "\tspeed: 0.1147s/iter; left time: 5351.9548s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0706295\n",
      "\tspeed: 0.1164s/iter; left time: 5419.2978s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0611054\n",
      "\tspeed: 0.1123s/iter; left time: 5216.7241s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0822397\n",
      "\tspeed: 0.1122s/iter; left time: 5203.7787s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0630606\n",
      "\tspeed: 0.1133s/iter; left time: 5243.2936s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0664413\n",
      "\tspeed: 0.1144s/iter; left time: 5280.1509s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0754516\n",
      "\tspeed: 0.1126s/iter; left time: 5183.9776s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0628932\n",
      "\tspeed: 0.1117s/iter; left time: 5135.4868s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0689445\n",
      "\tspeed: 0.1132s/iter; left time: 5193.1520s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0753874\n",
      "\tspeed: 0.1143s/iter; left time: 5231.9445s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0803154\n",
      "\tspeed: 0.1117s/iter; left time: 5100.1547s\n",
      "Epoch: 3 cost time: 00h:05m:05.16s\n",
      "Epoch: 3 | Train Loss: 0.0754979 Vali Loss: 0.0626933 Test Loss: 0.0662443\n",
      "Validation loss decreased (0.063019 --> 0.062693).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0701278\n",
      "\tspeed: 1.0424s/iter; left time: 47406.5237s\n",
      "\titers: 200, epoch: 4 | loss: 0.0728249\n",
      "\tspeed: 0.1145s/iter; left time: 5197.6080s\n",
      "\titers: 300, epoch: 4 | loss: 0.0778461\n",
      "\tspeed: 0.1130s/iter; left time: 5117.2262s\n",
      "\titers: 400, epoch: 4 | loss: 0.0785829\n",
      "\tspeed: 0.1132s/iter; left time: 5116.0930s\n",
      "\titers: 500, epoch: 4 | loss: 0.0790459\n",
      "\tspeed: 0.1153s/iter; left time: 5197.8971s\n",
      "\titers: 600, epoch: 4 | loss: 0.0710924\n",
      "\tspeed: 0.1143s/iter; left time: 5139.3973s\n",
      "\titers: 700, epoch: 4 | loss: 0.0721118\n",
      "\tspeed: 0.1152s/iter; left time: 5171.2332s\n",
      "\titers: 800, epoch: 4 | loss: 0.0836879\n",
      "\tspeed: 0.1165s/iter; left time: 5217.0912s\n",
      "\titers: 900, epoch: 4 | loss: 0.0670347\n",
      "\tspeed: 0.1127s/iter; left time: 5037.3830s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0732893\n",
      "\tspeed: 0.1135s/iter; left time: 5058.3337s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0734157\n",
      "\tspeed: 0.1130s/iter; left time: 5026.3469s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0836176\n",
      "\tspeed: 0.1125s/iter; left time: 4993.3019s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0914069\n",
      "\tspeed: 0.1140s/iter; left time: 5049.6116s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0554709\n",
      "\tspeed: 0.1130s/iter; left time: 4994.0839s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0638680\n",
      "\tspeed: 0.1121s/iter; left time: 4940.7801s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0843564\n",
      "\tspeed: 0.1153s/iter; left time: 5072.0975s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0867873\n",
      "\tspeed: 0.1154s/iter; left time: 5062.8955s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0825667\n",
      "\tspeed: 0.1119s/iter; left time: 4899.0202s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0808909\n",
      "\tspeed: 0.1123s/iter; left time: 4903.3557s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0671010\n",
      "\tspeed: 0.1135s/iter; left time: 4947.8210s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0800280\n",
      "\tspeed: 0.1121s/iter; left time: 4873.7499s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0652930\n",
      "\tspeed: 0.1110s/iter; left time: 4816.3385s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0597176\n",
      "\tspeed: 0.1110s/iter; left time: 4805.4423s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0868424\n",
      "\tspeed: 0.1124s/iter; left time: 4852.0543s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0782121\n",
      "\tspeed: 0.1127s/iter; left time: 4856.6246s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0802435\n",
      "\tspeed: 0.1120s/iter; left time: 4812.7082s\n",
      "Epoch: 4 cost time: 00h:05m:04.23s\n",
      "Epoch: 4 | Train Loss: 0.0732161 Vali Loss: 0.0602605 Test Loss: 0.0639531\n",
      "Validation loss decreased (0.062693 --> 0.060260).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0794265\n",
      "\tspeed: 1.0445s/iter; left time: 44703.2173s\n",
      "\titers: 200, epoch: 5 | loss: 0.0705508\n",
      "\tspeed: 0.1145s/iter; left time: 4889.2588s\n",
      "\titers: 300, epoch: 5 | loss: 0.0829213\n",
      "\tspeed: 0.1137s/iter; left time: 4841.8229s\n",
      "\titers: 400, epoch: 5 | loss: 0.0772983\n",
      "\tspeed: 0.1103s/iter; left time: 4687.0188s\n",
      "\titers: 500, epoch: 5 | loss: 0.0697670\n",
      "\tspeed: 0.1129s/iter; left time: 4788.1117s\n",
      "\titers: 600, epoch: 5 | loss: 0.0615391\n",
      "\tspeed: 0.1130s/iter; left time: 4779.3203s\n",
      "\titers: 700, epoch: 5 | loss: 0.0567918\n",
      "\tspeed: 0.1124s/iter; left time: 4742.9584s\n",
      "\titers: 800, epoch: 5 | loss: 0.0847099\n",
      "\tspeed: 0.1118s/iter; left time: 4707.3078s\n",
      "\titers: 900, epoch: 5 | loss: 0.0704188\n",
      "\tspeed: 0.1122s/iter; left time: 4713.4398s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0620770\n",
      "\tspeed: 0.1124s/iter; left time: 4708.3377s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0741380\n",
      "\tspeed: 0.1131s/iter; left time: 4725.7189s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0661858\n",
      "\tspeed: 0.1152s/iter; left time: 4802.3252s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0621480\n",
      "\tspeed: 0.1118s/iter; left time: 4652.4158s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0629700\n",
      "\tspeed: 0.1152s/iter; left time: 4779.4661s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0820492\n",
      "\tspeed: 0.1127s/iter; left time: 4665.4697s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0610860\n",
      "\tspeed: 0.1122s/iter; left time: 4633.4398s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0861114\n",
      "\tspeed: 0.1111s/iter; left time: 4575.4531s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0606930\n",
      "\tspeed: 0.1111s/iter; left time: 4567.4049s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0612833\n",
      "\tspeed: 0.1121s/iter; left time: 4597.4820s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0786318\n",
      "\tspeed: 0.1110s/iter; left time: 4539.6443s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0779789\n",
      "\tspeed: 0.1123s/iter; left time: 4580.0710s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0667817\n",
      "\tspeed: 0.1111s/iter; left time: 4523.4657s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0773585\n",
      "\tspeed: 0.1128s/iter; left time: 4581.3611s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0830474\n",
      "\tspeed: 0.1119s/iter; left time: 4533.0059s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0766403\n",
      "\tspeed: 0.1110s/iter; left time: 4485.0244s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0594169\n",
      "\tspeed: 0.1122s/iter; left time: 4521.0120s\n",
      "Epoch: 5 cost time: 00h:05m:01.95s\n",
      "Epoch: 5 | Train Loss: 0.0718611 Vali Loss: 0.0608224 Test Loss: 0.0644853\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0779639\n",
      "\tspeed: 1.0313s/iter; left time: 41373.1934s\n",
      "\titers: 200, epoch: 6 | loss: 0.0661459\n",
      "\tspeed: 0.1129s/iter; left time: 4519.0567s\n",
      "\titers: 300, epoch: 6 | loss: 0.0925239\n",
      "\tspeed: 0.1145s/iter; left time: 4568.8113s\n",
      "\titers: 400, epoch: 6 | loss: 0.0681258\n",
      "\tspeed: 0.1124s/iter; left time: 4473.6384s\n",
      "\titers: 500, epoch: 6 | loss: 0.0705238\n",
      "\tspeed: 0.1128s/iter; left time: 4479.2027s\n",
      "\titers: 600, epoch: 6 | loss: 0.0755477\n",
      "\tspeed: 0.1134s/iter; left time: 4494.4193s\n",
      "\titers: 700, epoch: 6 | loss: 0.0716280\n",
      "\tspeed: 0.1112s/iter; left time: 4395.6017s\n",
      "\titers: 800, epoch: 6 | loss: 0.0619959\n",
      "\tspeed: 0.1134s/iter; left time: 4469.7612s\n",
      "\titers: 900, epoch: 6 | loss: 0.0714130\n",
      "\tspeed: 0.1125s/iter; left time: 4421.3718s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0737989\n",
      "\tspeed: 0.1145s/iter; left time: 4490.0219s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0621136\n",
      "\tspeed: 0.1151s/iter; left time: 4501.3607s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0685738\n",
      "\tspeed: 0.1155s/iter; left time: 4504.9048s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0633010\n",
      "\tspeed: 0.1145s/iter; left time: 4454.9581s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603377\n",
      "\tspeed: 0.1131s/iter; left time: 4388.6346s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0727599\n",
      "\tspeed: 0.1139s/iter; left time: 4409.1309s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0715380\n",
      "\tspeed: 0.1111s/iter; left time: 4289.6488s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0616430\n",
      "\tspeed: 0.1125s/iter; left time: 4333.5642s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0798777\n",
      "\tspeed: 0.1129s/iter; left time: 4335.9876s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0765420\n",
      "\tspeed: 0.1119s/iter; left time: 4286.7227s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0671012\n",
      "\tspeed: 0.1099s/iter; left time: 4200.8745s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0616646\n",
      "\tspeed: 0.1135s/iter; left time: 4325.0934s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0681724\n",
      "\tspeed: 0.1132s/iter; left time: 4302.4291s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0847921\n",
      "\tspeed: 0.1123s/iter; left time: 4259.4616s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0787449\n",
      "\tspeed: 0.1131s/iter; left time: 4275.7450s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0638925\n",
      "\tspeed: 0.1127s/iter; left time: 4251.6605s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0637040\n",
      "\tspeed: 0.1121s/iter; left time: 4218.4933s\n",
      "Epoch: 6 cost time: 00h:05m:03.44s\n",
      "Epoch: 6 | Train Loss: 0.0710204 Vali Loss: 0.0602071 Test Loss: 0.0637255\n",
      "Validation loss decreased (0.060260 --> 0.060207).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0805032\n",
      "\tspeed: 1.0514s/iter; left time: 39357.4660s\n",
      "\titers: 200, epoch: 7 | loss: 0.0928424\n",
      "\tspeed: 0.1134s/iter; left time: 4234.1236s\n",
      "\titers: 300, epoch: 7 | loss: 0.0699223\n",
      "\tspeed: 0.1140s/iter; left time: 4245.8493s\n",
      "\titers: 400, epoch: 7 | loss: 0.0772188\n",
      "\tspeed: 0.1138s/iter; left time: 4225.0050s\n",
      "\titers: 500, epoch: 7 | loss: 0.0806367\n",
      "\tspeed: 0.1140s/iter; left time: 4222.1278s\n",
      "\titers: 600, epoch: 7 | loss: 0.0634284\n",
      "\tspeed: 0.1119s/iter; left time: 4131.5210s\n",
      "\titers: 700, epoch: 7 | loss: 0.0699541\n",
      "\tspeed: 0.1147s/iter; left time: 4223.7646s\n",
      "\titers: 800, epoch: 7 | loss: 0.0892019\n",
      "\tspeed: 0.1146s/iter; left time: 4210.1537s\n",
      "\titers: 900, epoch: 7 | loss: 0.0712903\n",
      "\tspeed: 0.1136s/iter; left time: 4163.4128s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0747735\n",
      "\tspeed: 0.1142s/iter; left time: 4170.9985s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0782191\n",
      "\tspeed: 0.1127s/iter; left time: 4106.8679s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0647047\n",
      "\tspeed: 0.1116s/iter; left time: 4055.0160s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0666225\n",
      "\tspeed: 0.1144s/iter; left time: 4146.6489s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843146\n",
      "\tspeed: 0.1130s/iter; left time: 4081.5626s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0681136\n",
      "\tspeed: 0.1137s/iter; left time: 4097.0857s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0701010\n",
      "\tspeed: 0.1120s/iter; left time: 4026.2285s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0843430\n",
      "\tspeed: 0.1126s/iter; left time: 4034.4264s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0538953\n",
      "\tspeed: 0.1131s/iter; left time: 4042.4488s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0655326\n",
      "\tspeed: 0.1124s/iter; left time: 4006.9932s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0805343\n",
      "\tspeed: 0.1128s/iter; left time: 4006.6221s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0631403\n",
      "\tspeed: 0.1123s/iter; left time: 3980.9580s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0604495\n",
      "\tspeed: 0.1122s/iter; left time: 3964.3230s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0642110\n",
      "\tspeed: 0.1123s/iter; left time: 3958.0783s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0664487\n",
      "\tspeed: 0.1137s/iter; left time: 3995.3425s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0816911\n",
      "\tspeed: 0.1134s/iter; left time: 3972.5020s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0661209\n",
      "\tspeed: 0.1129s/iter; left time: 3944.3765s\n",
      "Epoch: 7 cost time: 00h:05m:03.94s\n",
      "Epoch: 7 | Train Loss: 0.0702127 Vali Loss: 0.0597712 Test Loss: 0.0634713\n",
      "Validation loss decreased (0.060207 --> 0.059771).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0825887\n",
      "\tspeed: 1.0613s/iter; left time: 36883.3912s\n",
      "\titers: 200, epoch: 8 | loss: 0.0750446\n",
      "\tspeed: 0.1157s/iter; left time: 4008.4034s\n",
      "\titers: 300, epoch: 8 | loss: 0.0714472\n",
      "\tspeed: 0.1147s/iter; left time: 3964.0194s\n",
      "\titers: 400, epoch: 8 | loss: 0.0683220\n",
      "\tspeed: 0.1138s/iter; left time: 3919.4924s\n",
      "\titers: 500, epoch: 8 | loss: 0.0658177\n",
      "\tspeed: 0.1144s/iter; left time: 3930.1622s\n",
      "\titers: 600, epoch: 8 | loss: 0.0671118\n",
      "\tspeed: 0.1134s/iter; left time: 3886.0113s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654151\n",
      "\tspeed: 0.1139s/iter; left time: 3888.8765s\n",
      "\titers: 800, epoch: 8 | loss: 0.0633116\n",
      "\tspeed: 0.1150s/iter; left time: 3916.3384s\n",
      "\titers: 900, epoch: 8 | loss: 0.0762280\n",
      "\tspeed: 0.1129s/iter; left time: 3834.4540s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0904688\n",
      "\tspeed: 0.1132s/iter; left time: 3833.6557s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0685867\n",
      "\tspeed: 0.1157s/iter; left time: 3904.7728s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0753753\n",
      "\tspeed: 0.1157s/iter; left time: 3892.1284s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0869639\n",
      "\tspeed: 0.1128s/iter; left time: 3783.3242s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0656767\n",
      "\tspeed: 0.1127s/iter; left time: 3769.6066s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0751337\n",
      "\tspeed: 0.1129s/iter; left time: 3767.1325s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0769074\n",
      "\tspeed: 0.1114s/iter; left time: 3705.3295s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0628735\n",
      "\tspeed: 0.1138s/iter; left time: 3774.2191s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0630130\n",
      "\tspeed: 0.1123s/iter; left time: 3712.3849s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0680705\n",
      "\tspeed: 0.1126s/iter; left time: 3710.9236s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0652012\n",
      "\tspeed: 0.1123s/iter; left time: 3690.3335s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0650971\n",
      "\tspeed: 0.1104s/iter; left time: 3616.4226s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0778571\n",
      "\tspeed: 0.1124s/iter; left time: 3669.9809s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0902112\n",
      "\tspeed: 0.1117s/iter; left time: 3637.1336s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0791112\n",
      "\tspeed: 0.1114s/iter; left time: 3616.3970s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0677406\n",
      "\tspeed: 0.1130s/iter; left time: 3655.9542s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0605854\n",
      "\tspeed: 0.1125s/iter; left time: 3628.7290s\n",
      "Epoch: 8 cost time: 00h:05m:04.33s\n",
      "Epoch: 8 | Train Loss: 0.0696060 Vali Loss: 0.0590203 Test Loss: 0.0629085\n",
      "Validation loss decreased (0.059771 --> 0.059020).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0732988\n",
      "\tspeed: 1.0469s/iter; left time: 33575.7486s\n",
      "\titers: 200, epoch: 9 | loss: 0.0632006\n",
      "\tspeed: 0.1134s/iter; left time: 3625.6311s\n",
      "\titers: 300, epoch: 9 | loss: 0.0675089\n",
      "\tspeed: 0.1133s/iter; left time: 3610.2588s\n",
      "\titers: 400, epoch: 9 | loss: 0.0753807\n",
      "\tspeed: 0.1126s/iter; left time: 3576.9251s\n",
      "\titers: 500, epoch: 9 | loss: 0.0688046\n",
      "\tspeed: 0.1139s/iter; left time: 3606.3103s\n",
      "\titers: 600, epoch: 9 | loss: 0.0563322\n",
      "\tspeed: 0.1114s/iter; left time: 3518.7991s\n",
      "\titers: 700, epoch: 9 | loss: 0.0637789\n",
      "\tspeed: 0.1147s/iter; left time: 3609.5353s\n",
      "\titers: 800, epoch: 9 | loss: 0.0552997\n",
      "\tspeed: 0.1143s/iter; left time: 3585.2895s\n",
      "\titers: 900, epoch: 9 | loss: 0.0730415\n",
      "\tspeed: 0.1131s/iter; left time: 3536.3882s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0624509\n",
      "\tspeed: 0.1131s/iter; left time: 3525.1544s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0611722\n",
      "\tspeed: 0.1142s/iter; left time: 3548.2124s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0624793\n",
      "\tspeed: 0.1136s/iter; left time: 3518.4102s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0554433\n",
      "\tspeed: 0.1126s/iter; left time: 3474.9651s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0854482\n",
      "\tspeed: 0.1130s/iter; left time: 3478.8142s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1009781\n",
      "\tspeed: 0.1139s/iter; left time: 3494.9430s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0588896\n",
      "\tspeed: 0.1126s/iter; left time: 3443.7265s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0664771\n",
      "\tspeed: 0.1125s/iter; left time: 3427.2529s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0875199\n",
      "\tspeed: 0.1135s/iter; left time: 3447.8911s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0652644\n",
      "\tspeed: 0.1130s/iter; left time: 3421.9041s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0729210\n",
      "\tspeed: 0.1112s/iter; left time: 3355.2659s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0595015\n",
      "\tspeed: 0.1128s/iter; left time: 3391.7246s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0645168\n",
      "\tspeed: 0.1130s/iter; left time: 3388.3325s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0664069\n",
      "\tspeed: 0.1108s/iter; left time: 3310.9877s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0560363\n",
      "\tspeed: 0.1127s/iter; left time: 3354.7876s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0640769\n",
      "\tspeed: 0.1137s/iter; left time: 3373.7109s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0609662\n",
      "\tspeed: 0.1146s/iter; left time: 3390.4395s\n",
      "Epoch: 9 cost time: 00h:05m:04.13s\n",
      "Epoch: 9 | Train Loss: 0.0688502 Vali Loss: 0.0579161 Test Loss: 0.0620261\n",
      "Validation loss decreased (0.059020 --> 0.057916).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0692350\n",
      "\tspeed: 1.0613s/iter; left time: 31192.3696s\n",
      "\titers: 200, epoch: 10 | loss: 0.0792829\n",
      "\tspeed: 0.1143s/iter; left time: 3349.2427s\n",
      "\titers: 300, epoch: 10 | loss: 0.0651196\n",
      "\tspeed: 0.1137s/iter; left time: 3317.6927s\n",
      "\titers: 400, epoch: 10 | loss: 0.0529624\n",
      "\tspeed: 0.1138s/iter; left time: 3309.5114s\n",
      "\titers: 500, epoch: 10 | loss: 0.0596995\n",
      "\tspeed: 0.1145s/iter; left time: 3320.1693s\n",
      "\titers: 600, epoch: 10 | loss: 0.0596051\n",
      "\tspeed: 0.1158s/iter; left time: 3346.7888s\n",
      "\titers: 700, epoch: 10 | loss: 0.0729908\n",
      "\tspeed: 0.1113s/iter; left time: 3205.6438s\n",
      "\titers: 800, epoch: 10 | loss: 0.0748923\n",
      "\tspeed: 0.1140s/iter; left time: 3270.4554s\n",
      "\titers: 900, epoch: 10 | loss: 0.0764702\n",
      "\tspeed: 0.1167s/iter; left time: 3335.9231s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0590842\n",
      "\tspeed: 0.1168s/iter; left time: 3327.6925s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0709747\n",
      "\tspeed: 0.1160s/iter; left time: 3292.8934s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0812026\n",
      "\tspeed: 0.1153s/iter; left time: 3260.7871s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0673435\n",
      "\tspeed: 0.1149s/iter; left time: 3238.6523s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0555878\n",
      "\tspeed: 0.1158s/iter; left time: 3251.9341s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0766573\n",
      "\tspeed: 0.1145s/iter; left time: 3205.1303s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0764899\n",
      "\tspeed: 0.1143s/iter; left time: 3187.6596s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0618597\n",
      "\tspeed: 0.1160s/iter; left time: 3223.2066s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0638845\n",
      "\tspeed: 0.1111s/iter; left time: 3077.8686s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0900016\n",
      "\tspeed: 0.1143s/iter; left time: 3152.6443s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0711123\n",
      "\tspeed: 0.1149s/iter; left time: 3158.2682s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0715640\n",
      "\tspeed: 0.1122s/iter; left time: 3072.3091s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0749826\n",
      "\tspeed: 0.1123s/iter; left time: 3064.6542s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0696980\n",
      "\tspeed: 0.1130s/iter; left time: 3071.7559s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0574644\n",
      "\tspeed: 0.1118s/iter; left time: 3029.9863s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0694450\n",
      "\tspeed: 0.1117s/iter; left time: 3014.1432s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0577533\n",
      "\tspeed: 0.1135s/iter; left time: 3051.3840s\n",
      "Epoch: 10 cost time: 00h:05m:06.70s\n",
      "Epoch: 10 | Train Loss: 0.0683724 Vali Loss: 0.0582843 Test Loss: 0.0620225\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0733247\n",
      "\tspeed: 1.0370s/iter; left time: 27698.2191s\n",
      "\titers: 200, epoch: 11 | loss: 0.0593850\n",
      "\tspeed: 0.1133s/iter; left time: 3016.3245s\n",
      "\titers: 300, epoch: 11 | loss: 0.0672786\n",
      "\tspeed: 0.1143s/iter; left time: 3028.9575s\n",
      "\titers: 400, epoch: 11 | loss: 0.0505884\n",
      "\tspeed: 0.1162s/iter; left time: 3067.8889s\n",
      "\titers: 500, epoch: 11 | loss: 0.0644879\n",
      "\tspeed: 0.1131s/iter; left time: 2976.2848s\n",
      "\titers: 600, epoch: 11 | loss: 0.0813994\n",
      "\tspeed: 0.1134s/iter; left time: 2972.4012s\n",
      "\titers: 700, epoch: 11 | loss: 0.0774051\n",
      "\tspeed: 0.1157s/iter; left time: 3021.8245s\n",
      "\titers: 800, epoch: 11 | loss: 0.0700469\n",
      "\tspeed: 0.1136s/iter; left time: 2954.5585s\n",
      "\titers: 900, epoch: 11 | loss: 0.0562205\n",
      "\tspeed: 0.1144s/iter; left time: 2965.5106s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0744258\n",
      "\tspeed: 0.1142s/iter; left time: 2947.7942s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0834967\n",
      "\tspeed: 0.1150s/iter; left time: 2957.1412s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0731772\n",
      "\tspeed: 0.1142s/iter; left time: 2924.9462s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0543189\n",
      "\tspeed: 0.1162s/iter; left time: 2964.6617s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0754633\n",
      "\tspeed: 0.1160s/iter; left time: 2947.0532s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0645236\n",
      "\tspeed: 0.1141s/iter; left time: 2888.1280s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0708787\n",
      "\tspeed: 0.1155s/iter; left time: 2911.1337s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0754559\n",
      "\tspeed: 0.1132s/iter; left time: 2842.4902s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0719775\n",
      "\tspeed: 0.1122s/iter; left time: 2804.9962s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0709032\n",
      "\tspeed: 0.1151s/iter; left time: 2867.6776s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0738048\n",
      "\tspeed: 0.1125s/iter; left time: 2791.8765s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0666654\n",
      "\tspeed: 0.1133s/iter; left time: 2800.5792s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0716899\n",
      "\tspeed: 0.1127s/iter; left time: 2774.4368s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0751313\n",
      "\tspeed: 0.1113s/iter; left time: 2728.3758s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0650967\n",
      "\tspeed: 0.1115s/iter; left time: 2722.6908s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0566285\n",
      "\tspeed: 0.1131s/iter; left time: 2748.4264s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0661598\n",
      "\tspeed: 0.1119s/iter; left time: 2709.0795s\n",
      "Epoch: 11 cost time: 00h:05m:06.02s\n",
      "Epoch: 11 | Train Loss: 0.0677539 Vali Loss: 0.0579457 Test Loss: 0.0615977\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0564539\n",
      "\tspeed: 1.0320s/iter; left time: 24800.0710s\n",
      "\titers: 200, epoch: 12 | loss: 0.0552143\n",
      "\tspeed: 0.1127s/iter; left time: 2696.2229s\n",
      "\titers: 300, epoch: 12 | loss: 0.0666377\n",
      "\tspeed: 0.1130s/iter; left time: 2693.7457s\n",
      "\titers: 400, epoch: 12 | loss: 0.0854366\n",
      "\tspeed: 0.1118s/iter; left time: 2653.1796s\n",
      "\titers: 500, epoch: 12 | loss: 0.0836129\n",
      "\tspeed: 0.1138s/iter; left time: 2688.2405s\n",
      "\titers: 600, epoch: 12 | loss: 0.0733599\n",
      "\tspeed: 0.1144s/iter; left time: 2692.4083s\n",
      "\titers: 700, epoch: 12 | loss: 0.0519824\n",
      "\tspeed: 0.1141s/iter; left time: 2674.1239s\n",
      "\titers: 800, epoch: 12 | loss: 0.0707659\n",
      "\tspeed: 0.1120s/iter; left time: 2612.7351s\n",
      "\titers: 900, epoch: 12 | loss: 0.0599417\n",
      "\tspeed: 0.1124s/iter; left time: 2610.4100s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0641551\n",
      "\tspeed: 0.1119s/iter; left time: 2587.2098s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0645704\n",
      "\tspeed: 0.1141s/iter; left time: 2627.2422s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0506950\n",
      "\tspeed: 0.1148s/iter; left time: 2631.2853s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0640931\n",
      "\tspeed: 0.1143s/iter; left time: 2610.5703s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0689833\n",
      "\tspeed: 0.1145s/iter; left time: 2601.8537s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0576060\n",
      "\tspeed: 0.1140s/iter; left time: 2580.7300s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0531667\n",
      "\tspeed: 0.1152s/iter; left time: 2595.6259s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0634687\n",
      "\tspeed: 0.1156s/iter; left time: 2592.7872s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0635579\n",
      "\tspeed: 0.1152s/iter; left time: 2571.7915s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0655776\n",
      "\tspeed: 0.1167s/iter; left time: 2594.8326s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0597161\n",
      "\tspeed: 0.1159s/iter; left time: 2565.2309s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0718565\n",
      "\tspeed: 0.1146s/iter; left time: 2525.0952s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0863532\n",
      "\tspeed: 0.1146s/iter; left time: 2512.7847s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0655009\n",
      "\tspeed: 0.1138s/iter; left time: 2483.6130s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0780334\n",
      "\tspeed: 0.1132s/iter; left time: 2459.7022s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685403\n",
      "\tspeed: 0.1140s/iter; left time: 2465.9565s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0669672\n",
      "\tspeed: 0.1128s/iter; left time: 2428.6642s\n",
      "Epoch: 12 cost time: 00h:05m:06.14s\n",
      "Epoch: 12 | Train Loss: 0.0672516 Vali Loss: 0.0583924 Test Loss: 0.0623229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0646751\n",
      "\tspeed: 1.0364s/iter; left time: 22126.2339s\n",
      "\titers: 200, epoch: 13 | loss: 0.0718063\n",
      "\tspeed: 0.1127s/iter; left time: 2394.7919s\n",
      "\titers: 300, epoch: 13 | loss: 0.0704412\n",
      "\tspeed: 0.1128s/iter; left time: 2385.4888s\n",
      "\titers: 400, epoch: 13 | loss: 0.0566958\n",
      "\tspeed: 0.1143s/iter; left time: 2406.1945s\n",
      "\titers: 500, epoch: 13 | loss: 0.0677867\n",
      "\tspeed: 0.1114s/iter; left time: 2334.0660s\n",
      "\titers: 600, epoch: 13 | loss: 0.0662406\n",
      "\tspeed: 0.1131s/iter; left time: 2358.9155s\n",
      "\titers: 700, epoch: 13 | loss: 0.0692414\n",
      "\tspeed: 0.1158s/iter; left time: 2403.1566s\n",
      "\titers: 800, epoch: 13 | loss: 0.0744722\n",
      "\tspeed: 0.1136s/iter; left time: 2345.5722s\n",
      "\titers: 900, epoch: 13 | loss: 0.0682855\n",
      "\tspeed: 0.1139s/iter; left time: 2339.8397s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0692520\n",
      "\tspeed: 0.1138s/iter; left time: 2327.3567s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0754888\n",
      "\tspeed: 0.1127s/iter; left time: 2293.1628s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0644453\n",
      "\tspeed: 0.1117s/iter; left time: 2262.0843s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0570012\n",
      "\tspeed: 0.1152s/iter; left time: 2321.7263s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0658361\n",
      "\tspeed: 0.1138s/iter; left time: 2282.3398s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0681140\n",
      "\tspeed: 0.1120s/iter; left time: 2234.3692s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0861393\n",
      "\tspeed: 0.1131s/iter; left time: 2244.6265s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0685929\n",
      "\tspeed: 0.1121s/iter; left time: 2213.1584s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0834594\n",
      "\tspeed: 0.1121s/iter; left time: 2201.6722s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0550227\n",
      "\tspeed: 0.1143s/iter; left time: 2234.3334s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0614033\n",
      "\tspeed: 0.1144s/iter; left time: 2225.0598s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0588906\n",
      "\tspeed: 0.1141s/iter; left time: 2207.2866s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0599210\n",
      "\tspeed: 0.1156s/iter; left time: 2224.4833s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0557182\n",
      "\tspeed: 0.1158s/iter; left time: 2216.8033s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0523902\n",
      "\tspeed: 0.1137s/iter; left time: 2165.3673s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0614735\n",
      "\tspeed: 0.1128s/iter; left time: 2138.0992s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0746370\n",
      "\tspeed: 0.1149s/iter; left time: 2166.1827s\n",
      "Epoch: 13 cost time: 00h:05m:05.24s\n",
      "Epoch: 13 | Train Loss: 0.0666866 Vali Loss: 0.0598697 Test Loss: 0.0634292\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0550120\n",
      "\tspeed: 1.0248s/iter; left time: 19130.7921s\n",
      "\titers: 200, epoch: 14 | loss: 0.0765724\n",
      "\tspeed: 0.1136s/iter; left time: 2109.4309s\n",
      "\titers: 300, epoch: 14 | loss: 0.0594586\n",
      "\tspeed: 0.1121s/iter; left time: 2069.9351s\n",
      "\titers: 400, epoch: 14 | loss: 0.0718415\n",
      "\tspeed: 0.1145s/iter; left time: 2102.6518s\n",
      "\titers: 500, epoch: 14 | loss: 0.0611447\n",
      "\tspeed: 0.1153s/iter; left time: 2106.0231s\n",
      "\titers: 600, epoch: 14 | loss: 0.0523001\n",
      "\tspeed: 0.1114s/iter; left time: 2023.0540s\n",
      "\titers: 700, epoch: 14 | loss: 0.0722972\n",
      "\tspeed: 0.1141s/iter; left time: 2060.6737s\n",
      "\titers: 800, epoch: 14 | loss: 0.0649306\n",
      "\tspeed: 0.1160s/iter; left time: 2083.8280s\n",
      "\titers: 900, epoch: 14 | loss: 0.0694602\n",
      "\tspeed: 0.1121s/iter; left time: 2003.6791s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0615104\n",
      "\tspeed: 0.1132s/iter; left time: 2011.5344s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0770956\n",
      "\tspeed: 0.1140s/iter; left time: 2014.7615s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0567148\n",
      "\tspeed: 0.1149s/iter; left time: 2019.0759s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0728452\n",
      "\tspeed: 0.1126s/iter; left time: 1967.7282s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0593382\n",
      "\tspeed: 0.1136s/iter; left time: 1972.6366s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0824114\n",
      "\tspeed: 0.1135s/iter; left time: 1960.3904s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0644274\n",
      "\tspeed: 0.1138s/iter; left time: 1954.0745s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0727649\n",
      "\tspeed: 0.1132s/iter; left time: 1932.2008s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0557669\n",
      "\tspeed: 0.1159s/iter; left time: 1966.0186s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0580711\n",
      "\tspeed: 0.1141s/iter; left time: 1925.0778s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0541300\n",
      "\tspeed: 0.1131s/iter; left time: 1896.9430s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0559602\n",
      "\tspeed: 0.1127s/iter; left time: 1878.8913s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0641657\n",
      "\tspeed: 0.1133s/iter; left time: 1876.6334s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0765522\n",
      "\tspeed: 0.1120s/iter; left time: 1843.8689s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0656291\n",
      "\tspeed: 0.1125s/iter; left time: 1841.0284s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0698191\n",
      "\tspeed: 0.1118s/iter; left time: 1818.2485s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0704088\n",
      "\tspeed: 0.1125s/iter; left time: 1819.4298s\n",
      "Epoch: 14 cost time: 00h:05m:04.54s\n",
      "Epoch: 14 | Train Loss: 0.0661358 Vali Loss: 0.0595082 Test Loss: 0.0636313\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010770452208817005, rmse:0.10378079116344452, mae:0.06202606484293938, rse:0.39211392402648926\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 24: 01h:32m:28.32s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-03 19:10:42,525] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 19:10:43,613] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 19:10:43,613] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 19:10:43,613] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 19:10:43,698] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 19:10:43,698] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 19:10:44,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 19:10:44,423] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 19:10:44,423] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 19:10:44,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 19:10:44,425] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 19:10:44,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 19:10:44,425] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 19:10:44,816] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 19:10:44,817] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:44,817] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.89 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:44,985] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 19:10:44,986] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:44,986] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.96 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:44,986] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 19:10:45,157] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 19:10:45,158] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-03 19:10:45,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 303.98 GB, percent = 40.3%\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 19:10:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 19:10:45,160] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f2cac6550>\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 19:10:45,161] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 19:10:45,162] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 19:10:45,163] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2074100\n",
      "\tspeed: 0.1698s/iter; left time: 9065.7885s\n",
      "\titers: 200, epoch: 1 | loss: 0.1933736\n",
      "\tspeed: 0.1209s/iter; left time: 6441.2101s\n",
      "\titers: 300, epoch: 1 | loss: 0.1680937\n",
      "\tspeed: 0.1243s/iter; left time: 6612.0969s\n",
      "\titers: 400, epoch: 1 | loss: 0.1478600\n",
      "\tspeed: 0.1233s/iter; left time: 6543.0074s\n",
      "\titers: 500, epoch: 1 | loss: 0.1329422\n",
      "\tspeed: 0.1241s/iter; left time: 6574.7097s\n",
      "\titers: 600, epoch: 1 | loss: 0.1080209\n",
      "\tspeed: 0.1237s/iter; left time: 6541.7857s\n",
      "\titers: 700, epoch: 1 | loss: 0.1268893\n",
      "\tspeed: 0.1274s/iter; left time: 6724.4386s\n",
      "\titers: 800, epoch: 1 | loss: 0.1043758\n",
      "\tspeed: 0.1244s/iter; left time: 6552.9860s\n",
      "\titers: 900, epoch: 1 | loss: 0.1108791\n",
      "\tspeed: 0.1236s/iter; left time: 6499.0257s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1154346\n",
      "\tspeed: 0.1242s/iter; left time: 6520.1496s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0978668\n",
      "\tspeed: 0.1230s/iter; left time: 6444.5262s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0894702\n",
      "\tspeed: 0.1251s/iter; left time: 6540.3517s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1008194\n",
      "\tspeed: 0.1253s/iter; left time: 6537.7925s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1065734\n",
      "\tspeed: 0.1244s/iter; left time: 6480.2460s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1051371\n",
      "\tspeed: 0.1249s/iter; left time: 6494.8085s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0991306\n",
      "\tspeed: 0.1247s/iter; left time: 6467.0995s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1286744\n",
      "\tspeed: 0.1231s/iter; left time: 6375.7148s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1012800\n",
      "\tspeed: 0.1206s/iter; left time: 6234.7451s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1123592\n",
      "\tspeed: 0.1227s/iter; left time: 6330.6444s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0761735\n",
      "\tspeed: 0.1225s/iter; left time: 6308.5215s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1169404\n",
      "\tspeed: 0.1229s/iter; left time: 6316.9931s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1165787\n",
      "\tspeed: 0.1224s/iter; left time: 6277.8256s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1044903\n",
      "\tspeed: 0.1213s/iter; left time: 6206.7813s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0961953\n",
      "\tspeed: 0.1252s/iter; left time: 6397.0164s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1058684\n",
      "\tspeed: 0.1251s/iter; left time: 6377.4777s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0999430\n",
      "\tspeed: 0.1232s/iter; left time: 6268.4567s\n",
      "Epoch: 1 cost time: 00h:05m:32.20s\n",
      "Epoch: 1 | Train Loss: 0.1198931 Vali Loss: 0.0841650 Test Loss: 0.0884351\n",
      "Validation loss decreased (inf --> 0.084165).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1127564\n",
      "\tspeed: 1.1665s/iter; left time: 59149.8786s\n",
      "\titers: 200, epoch: 2 | loss: 0.0858049\n",
      "\tspeed: 0.1124s/iter; left time: 5687.4056s\n",
      "\titers: 300, epoch: 2 | loss: 0.0845444\n",
      "\tspeed: 0.1103s/iter; left time: 5568.9101s\n",
      "\titers: 400, epoch: 2 | loss: 0.0956632\n",
      "\tspeed: 0.1131s/iter; left time: 5699.8288s\n",
      "\titers: 500, epoch: 2 | loss: 0.0922268\n",
      "\tspeed: 0.1148s/iter; left time: 5775.3422s\n",
      "\titers: 600, epoch: 2 | loss: 0.0877490\n",
      "\tspeed: 0.1141s/iter; left time: 5730.1084s\n",
      "\titers: 700, epoch: 2 | loss: 0.0789305\n",
      "\tspeed: 0.1129s/iter; left time: 5658.8953s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000123\n",
      "\tspeed: 0.1131s/iter; left time: 5655.9864s\n",
      "\titers: 900, epoch: 2 | loss: 0.1008030\n",
      "\tspeed: 0.1136s/iter; left time: 5670.5816s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0849907\n",
      "\tspeed: 0.1112s/iter; left time: 5537.6149s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0847871\n",
      "\tspeed: 0.1123s/iter; left time: 5580.6413s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1110906\n",
      "\tspeed: 0.1117s/iter; left time: 5543.3750s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0902140\n",
      "\tspeed: 0.1116s/iter; left time: 5526.9080s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1075663\n",
      "\tspeed: 0.1122s/iter; left time: 5543.3304s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0813371\n",
      "\tspeed: 0.1151s/iter; left time: 5674.4658s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0910764\n",
      "\tspeed: 0.1130s/iter; left time: 5562.6063s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0874856\n",
      "\tspeed: 0.1129s/iter; left time: 5545.6735s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1003650\n",
      "\tspeed: 0.1132s/iter; left time: 5548.0881s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0853289\n",
      "\tspeed: 0.1134s/iter; left time: 5544.9215s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0867084\n",
      "\tspeed: 0.1125s/iter; left time: 5489.3666s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0767493\n",
      "\tspeed: 0.1130s/iter; left time: 5503.8645s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0793778\n",
      "\tspeed: 0.1144s/iter; left time: 5559.9907s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0787130\n",
      "\tspeed: 0.1138s/iter; left time: 5518.6469s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0944487\n",
      "\tspeed: 0.1144s/iter; left time: 5536.3076s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0862754\n",
      "\tspeed: 0.1154s/iter; left time: 5573.3013s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0699872\n",
      "\tspeed: 0.1159s/iter; left time: 5585.2136s\n",
      "Epoch: 2 cost time: 00h:05m:03.44s\n",
      "Epoch: 2 | Train Loss: 0.0951379 Vali Loss: 0.0802616 Test Loss: 0.0852105\n",
      "Validation loss decreased (0.084165 --> 0.080262).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0913631\n",
      "\tspeed: 1.0426s/iter; left time: 50079.0402s\n",
      "\titers: 200, epoch: 3 | loss: 0.0941604\n",
      "\tspeed: 0.1148s/iter; left time: 5503.5955s\n",
      "\titers: 300, epoch: 3 | loss: 0.0937610\n",
      "\tspeed: 0.1146s/iter; left time: 5481.5325s\n",
      "\titers: 400, epoch: 3 | loss: 0.0962613\n",
      "\tspeed: 0.1151s/iter; left time: 5493.5033s\n",
      "\titers: 500, epoch: 3 | loss: 0.1008476\n",
      "\tspeed: 0.1144s/iter; left time: 5446.8518s\n",
      "\titers: 600, epoch: 3 | loss: 0.1072278\n",
      "\tspeed: 0.1149s/iter; left time: 5461.5053s\n",
      "\titers: 700, epoch: 3 | loss: 0.0809577\n",
      "\tspeed: 0.1160s/iter; left time: 5500.3849s\n",
      "\titers: 800, epoch: 3 | loss: 0.0869908\n",
      "\tspeed: 0.1144s/iter; left time: 5417.0870s\n",
      "\titers: 900, epoch: 3 | loss: 0.1061004\n",
      "\tspeed: 0.1130s/iter; left time: 5337.2130s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1013325\n",
      "\tspeed: 0.1142s/iter; left time: 5382.7579s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0924709\n",
      "\tspeed: 0.1162s/iter; left time: 5467.3840s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0991043\n",
      "\tspeed: 0.1144s/iter; left time: 5368.7871s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1008329\n",
      "\tspeed: 0.1141s/iter; left time: 5345.9778s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0831598\n",
      "\tspeed: 0.1160s/iter; left time: 5421.5980s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1090582\n",
      "\tspeed: 0.1150s/iter; left time: 5364.5424s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1031286\n",
      "\tspeed: 0.1160s/iter; left time: 5395.8119s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0970030\n",
      "\tspeed: 0.1178s/iter; left time: 5468.1474s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0891229\n",
      "\tspeed: 0.1160s/iter; left time: 5376.3383s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0952215\n",
      "\tspeed: 0.1142s/iter; left time: 5281.1271s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0831259\n",
      "\tspeed: 0.1140s/iter; left time: 5257.9225s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1124142\n",
      "\tspeed: 0.1154s/iter; left time: 5312.1351s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1149694\n",
      "\tspeed: 0.1158s/iter; left time: 5321.2063s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0783673\n",
      "\tspeed: 0.1138s/iter; left time: 5214.8664s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0836947\n",
      "\tspeed: 0.1117s/iter; left time: 5108.9117s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0893349\n",
      "\tspeed: 0.1122s/iter; left time: 5118.3652s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0813950\n",
      "\tspeed: 0.1163s/iter; left time: 5294.5987s\n",
      "Epoch: 3 cost time: 00h:05m:08.15s\n",
      "Epoch: 3 | Train Loss: 0.0912028 Vali Loss: 0.0809163 Test Loss: 0.0860651\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0766129\n",
      "\tspeed: 1.0843s/iter; left time: 49181.3535s\n",
      "\titers: 200, epoch: 4 | loss: 0.1203493\n",
      "\tspeed: 0.1202s/iter; left time: 5437.9964s\n",
      "\titers: 300, epoch: 4 | loss: 0.0836161\n",
      "\tspeed: 0.1240s/iter; left time: 5599.3588s\n",
      "\titers: 400, epoch: 4 | loss: 0.0988006\n",
      "\tspeed: 0.1207s/iter; left time: 5437.8985s\n",
      "\titers: 500, epoch: 4 | loss: 0.0826896\n",
      "\tspeed: 0.1217s/iter; left time: 5472.9887s\n",
      "\titers: 600, epoch: 4 | loss: 0.0930348\n",
      "\tspeed: 0.1205s/iter; left time: 5403.7398s\n",
      "\titers: 700, epoch: 4 | loss: 0.0988240\n",
      "\tspeed: 0.1237s/iter; left time: 5534.5948s\n",
      "\titers: 800, epoch: 4 | loss: 0.0805480\n",
      "\tspeed: 0.1199s/iter; left time: 5354.3449s\n",
      "\titers: 900, epoch: 4 | loss: 0.0826487\n",
      "\tspeed: 0.1188s/iter; left time: 5293.6080s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0710772\n",
      "\tspeed: 0.1274s/iter; left time: 5665.2333s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0831637\n",
      "\tspeed: 0.1225s/iter; left time: 5435.6249s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0868283\n",
      "\tspeed: 0.1206s/iter; left time: 5336.0502s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1062559\n",
      "\tspeed: 0.1246s/iter; left time: 5502.1697s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0913987\n",
      "\tspeed: 0.1224s/iter; left time: 5394.5232s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0884490\n",
      "\tspeed: 0.1231s/iter; left time: 5411.4485s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0850192\n",
      "\tspeed: 0.1212s/iter; left time: 5317.7873s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0802786\n",
      "\tspeed: 0.1164s/iter; left time: 5095.0696s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0982129\n",
      "\tspeed: 0.1213s/iter; left time: 5295.9952s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0785628\n",
      "\tspeed: 0.1255s/iter; left time: 5467.6361s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0918725\n",
      "\tspeed: 0.1253s/iter; left time: 5444.4988s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0956076\n",
      "\tspeed: 0.1261s/iter; left time: 5466.6697s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0837968\n",
      "\tspeed: 0.1245s/iter; left time: 5384.6818s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1122651\n",
      "\tspeed: 0.1187s/iter; left time: 5123.9010s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0925200\n",
      "\tspeed: 0.1248s/iter; left time: 5374.0130s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0905496\n",
      "\tspeed: 0.1263s/iter; left time: 5426.8022s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0913031\n",
      "\tspeed: 0.1276s/iter; left time: 5469.0224s\n",
      "Epoch: 4 cost time: 00h:05m:28.50s\n",
      "Epoch: 4 | Train Loss: 0.0887905 Vali Loss: 0.0801354 Test Loss: 0.0864428\n",
      "Validation loss decreased (0.080262 --> 0.080135).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0905916\n",
      "\tspeed: 1.0963s/iter; left time: 46795.9889s\n",
      "\titers: 200, epoch: 5 | loss: 0.0949473\n",
      "\tspeed: 0.1195s/iter; left time: 5087.5571s\n",
      "\titers: 300, epoch: 5 | loss: 0.0949715\n",
      "\tspeed: 0.1325s/iter; left time: 5630.9090s\n",
      "\titers: 400, epoch: 5 | loss: 0.0939259\n",
      "\tspeed: 0.1198s/iter; left time: 5077.4282s\n",
      "\titers: 500, epoch: 5 | loss: 0.0792014\n",
      "\tspeed: 0.1198s/iter; left time: 5065.8468s\n",
      "\titers: 600, epoch: 5 | loss: 0.1021533\n",
      "\tspeed: 0.1211s/iter; left time: 5107.1698s\n",
      "\titers: 700, epoch: 5 | loss: 0.0821311\n",
      "\tspeed: 0.1221s/iter; left time: 5138.0385s\n",
      "\titers: 800, epoch: 5 | loss: 0.0785077\n",
      "\tspeed: 0.1227s/iter; left time: 5152.3935s\n",
      "\titers: 900, epoch: 5 | loss: 0.0880126\n",
      "\tspeed: 0.1273s/iter; left time: 5331.1461s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0763944\n",
      "\tspeed: 0.1233s/iter; left time: 5152.4257s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0758381\n",
      "\tspeed: 0.1199s/iter; left time: 4997.8302s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0830716\n",
      "\tspeed: 0.1227s/iter; left time: 5101.1974s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0800337\n",
      "\tspeed: 0.1215s/iter; left time: 5039.0160s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0928819\n",
      "\tspeed: 0.1201s/iter; left time: 4972.1698s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0703389\n",
      "\tspeed: 0.1223s/iter; left time: 5049.8767s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0925038\n",
      "\tspeed: 0.1186s/iter; left time: 4885.4127s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0736254\n",
      "\tspeed: 0.1254s/iter; left time: 5152.7273s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0827256\n",
      "\tspeed: 0.1205s/iter; left time: 4939.7981s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0770165\n",
      "\tspeed: 0.1248s/iter; left time: 5100.7339s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0748519\n",
      "\tspeed: 0.1214s/iter; left time: 4951.9783s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0818176\n",
      "\tspeed: 0.1238s/iter; left time: 5038.1865s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0815690\n",
      "\tspeed: 0.1294s/iter; left time: 5250.8622s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1056789\n",
      "\tspeed: 0.1219s/iter; left time: 4933.6225s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0869661\n",
      "\tspeed: 0.1203s/iter; left time: 4858.3086s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0985921\n",
      "\tspeed: 0.1241s/iter; left time: 5000.0069s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0684036\n",
      "\tspeed: 0.1212s/iter; left time: 4869.5123s\n",
      "Epoch: 5 cost time: 00h:05m:28.74s\n",
      "Epoch: 5 | Train Loss: 0.0866818 Vali Loss: 0.0797001 Test Loss: 0.0868311\n",
      "Validation loss decreased (0.080135 --> 0.079700).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1188883\n",
      "\tspeed: 1.0801s/iter; left time: 43215.1434s\n",
      "\titers: 200, epoch: 6 | loss: 0.0875507\n",
      "\tspeed: 0.1203s/iter; left time: 4802.4070s\n",
      "\titers: 300, epoch: 6 | loss: 0.0807057\n",
      "\tspeed: 0.1217s/iter; left time: 4846.0808s\n",
      "\titers: 400, epoch: 6 | loss: 0.0704877\n",
      "\tspeed: 0.1247s/iter; left time: 4951.6835s\n",
      "\titers: 500, epoch: 6 | loss: 0.0778315\n",
      "\tspeed: 0.1205s/iter; left time: 4772.8739s\n",
      "\titers: 600, epoch: 6 | loss: 0.0950918\n",
      "\tspeed: 0.1252s/iter; left time: 4948.2841s\n",
      "\titers: 700, epoch: 6 | loss: 0.0813851\n",
      "\tspeed: 0.1257s/iter; left time: 4954.3791s\n",
      "\titers: 800, epoch: 6 | loss: 0.0890235\n",
      "\tspeed: 0.1145s/iter; left time: 4502.3709s\n",
      "\titers: 900, epoch: 6 | loss: 0.0926933\n",
      "\tspeed: 0.1220s/iter; left time: 4782.3417s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0856443\n",
      "\tspeed: 0.1242s/iter; left time: 4856.5758s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0886120\n",
      "\tspeed: 0.1291s/iter; left time: 5037.0379s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0603817\n",
      "\tspeed: 0.1206s/iter; left time: 4691.3295s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0879021\n",
      "\tspeed: 0.1214s/iter; left time: 4710.2026s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1013661\n",
      "\tspeed: 0.1237s/iter; left time: 4790.2180s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0748302\n",
      "\tspeed: 0.1179s/iter; left time: 4550.8062s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0890142\n",
      "\tspeed: 0.1261s/iter; left time: 4855.4099s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0706096\n",
      "\tspeed: 0.1201s/iter; left time: 4611.3015s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0830537\n",
      "\tspeed: 0.1210s/iter; left time: 4634.2511s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0720259\n",
      "\tspeed: 0.1204s/iter; left time: 4602.0593s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0876311\n",
      "\tspeed: 0.1194s/iter; left time: 4549.9251s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0846190\n",
      "\tspeed: 0.1153s/iter; left time: 4381.5095s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0942824\n",
      "\tspeed: 0.1197s/iter; left time: 4537.4921s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0858440\n",
      "\tspeed: 0.1261s/iter; left time: 4767.5138s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0759794\n",
      "\tspeed: 0.1173s/iter; left time: 4423.4440s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0880383\n",
      "\tspeed: 0.1215s/iter; left time: 4568.3632s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0596848\n",
      "\tspeed: 0.1271s/iter; left time: 4768.3723s\n",
      "Epoch: 6 cost time: 00h:05m:27.11s\n",
      "Epoch: 6 | Train Loss: 0.0846411 Vali Loss: 0.0798222 Test Loss: 0.0859858\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0920638\n",
      "\tspeed: 1.0889s/iter; left time: 40654.9404s\n",
      "\titers: 200, epoch: 7 | loss: 0.1065395\n",
      "\tspeed: 0.1244s/iter; left time: 4630.9616s\n",
      "\titers: 300, epoch: 7 | loss: 0.0627838\n",
      "\tspeed: 0.1228s/iter; left time: 4560.1871s\n",
      "\titers: 400, epoch: 7 | loss: 0.0834983\n",
      "\tspeed: 0.1213s/iter; left time: 4493.4944s\n",
      "\titers: 500, epoch: 7 | loss: 0.0813256\n",
      "\tspeed: 0.1187s/iter; left time: 4383.3373s\n",
      "\titers: 600, epoch: 7 | loss: 0.0936052\n",
      "\tspeed: 0.1211s/iter; left time: 4461.3375s\n",
      "\titers: 700, epoch: 7 | loss: 0.0897368\n",
      "\tspeed: 0.1188s/iter; left time: 4363.0497s\n",
      "\titers: 800, epoch: 7 | loss: 0.0712877\n",
      "\tspeed: 0.1177s/iter; left time: 4311.6725s\n",
      "\titers: 900, epoch: 7 | loss: 0.0922524\n",
      "\tspeed: 0.1268s/iter; left time: 4634.0975s\n",
      "\titers: 1000, epoch: 7 | loss: 0.1083432\n",
      "\tspeed: 0.1203s/iter; left time: 4384.3760s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0795755\n",
      "\tspeed: 0.1130s/iter; left time: 4104.9531s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0830358\n",
      "\tspeed: 0.1221s/iter; left time: 4423.9972s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0777278\n",
      "\tspeed: 0.1252s/iter; left time: 4522.7798s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0791193\n",
      "\tspeed: 0.1157s/iter; left time: 4169.6404s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0937434\n",
      "\tspeed: 0.1221s/iter; left time: 4389.5757s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0894421\n",
      "\tspeed: 0.1193s/iter; left time: 4273.6029s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0789753\n",
      "\tspeed: 0.1208s/iter; left time: 4316.0113s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0762554\n",
      "\tspeed: 0.1264s/iter; left time: 4503.6826s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0578167\n",
      "\tspeed: 0.1265s/iter; left time: 4497.0209s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0893710\n",
      "\tspeed: 0.1247s/iter; left time: 4420.4862s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0829187\n",
      "\tspeed: 0.1202s/iter; left time: 4246.8789s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1052914\n",
      "\tspeed: 0.1248s/iter; left time: 4396.5759s\n",
      "\titers: 2300, epoch: 7 | loss: 0.1023362\n",
      "\tspeed: 0.1258s/iter; left time: 4421.6090s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0633124\n",
      "\tspeed: 0.1253s/iter; left time: 4390.6589s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0897403\n",
      "\tspeed: 0.1147s/iter; left time: 4008.3266s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0877524\n",
      "\tspeed: 0.1172s/iter; left time: 4084.1017s\n",
      "Epoch: 7 cost time: 00h:05m:25.04s\n",
      "Epoch: 7 | Train Loss: 0.0827914 Vali Loss: 0.0802376 Test Loss: 0.0865027\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0741210\n",
      "\tspeed: 1.0234s/iter; left time: 35472.6250s\n",
      "\titers: 200, epoch: 8 | loss: 0.0855546\n",
      "\tspeed: 0.1134s/iter; left time: 3919.1548s\n",
      "\titers: 300, epoch: 8 | loss: 0.0862117\n",
      "\tspeed: 0.1145s/iter; left time: 3944.8104s\n",
      "\titers: 400, epoch: 8 | loss: 0.0810832\n",
      "\tspeed: 0.1152s/iter; left time: 3958.8356s\n",
      "\titers: 500, epoch: 8 | loss: 0.0992447\n",
      "\tspeed: 0.1152s/iter; left time: 3947.5852s\n",
      "\titers: 600, epoch: 8 | loss: 0.0759280\n",
      "\tspeed: 0.1157s/iter; left time: 3951.6902s\n",
      "\titers: 700, epoch: 8 | loss: 0.0706180\n",
      "\tspeed: 0.1156s/iter; left time: 3939.2951s\n",
      "\titers: 800, epoch: 8 | loss: 0.0777954\n",
      "\tspeed: 0.1154s/iter; left time: 3918.7120s\n",
      "\titers: 900, epoch: 8 | loss: 0.0899965\n",
      "\tspeed: 0.1138s/iter; left time: 3854.2748s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0714895\n",
      "\tspeed: 0.1137s/iter; left time: 3838.9223s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0831028\n",
      "\tspeed: 0.1137s/iter; left time: 3828.9141s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0776870\n",
      "\tspeed: 0.1142s/iter; left time: 3834.2195s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0858583\n",
      "\tspeed: 0.1170s/iter; left time: 3916.1538s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0750962\n",
      "\tspeed: 0.1143s/iter; left time: 3813.1582s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0934992\n",
      "\tspeed: 0.1136s/iter; left time: 3777.5693s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0680919\n",
      "\tspeed: 0.1178s/iter; left time: 3905.2622s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0809952\n",
      "\tspeed: 0.1163s/iter; left time: 3844.2126s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0644422\n",
      "\tspeed: 0.1131s/iter; left time: 3727.5083s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0749647\n",
      "\tspeed: 0.1140s/iter; left time: 3746.4699s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0769117\n",
      "\tspeed: 0.1155s/iter; left time: 3783.7151s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0895205\n",
      "\tspeed: 0.1166s/iter; left time: 3809.9989s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0856249\n",
      "\tspeed: 0.1153s/iter; left time: 3755.7292s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0781523\n",
      "\tspeed: 0.1150s/iter; left time: 3731.7042s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0775857\n",
      "\tspeed: 0.1160s/iter; left time: 3754.0185s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0808454\n",
      "\tspeed: 0.1143s/iter; left time: 3686.3988s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0844256\n",
      "\tspeed: 0.1156s/iter; left time: 3716.4597s\n",
      "Epoch: 8 cost time: 00h:05m:08.22s\n",
      "Epoch: 8 | Train Loss: 0.0808754 Vali Loss: 0.0805583 Test Loss: 0.0866693\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0862420\n",
      "\tspeed: 1.0197s/iter; left time: 32618.5266s\n",
      "\titers: 200, epoch: 9 | loss: 0.0791138\n",
      "\tspeed: 0.1145s/iter; left time: 3651.2684s\n",
      "\titers: 300, epoch: 9 | loss: 0.0770254\n",
      "\tspeed: 0.1177s/iter; left time: 3741.3610s\n",
      "\titers: 400, epoch: 9 | loss: 0.0769348\n",
      "\tspeed: 0.1145s/iter; left time: 3629.4086s\n",
      "\titers: 500, epoch: 9 | loss: 0.0745477\n",
      "\tspeed: 0.1163s/iter; left time: 3672.3151s\n",
      "\titers: 600, epoch: 9 | loss: 0.0885054\n",
      "\tspeed: 0.1173s/iter; left time: 3693.0668s\n",
      "\titers: 700, epoch: 9 | loss: 0.0813209\n",
      "\tspeed: 0.1145s/iter; left time: 3595.4994s\n",
      "\titers: 800, epoch: 9 | loss: 0.0839284\n",
      "\tspeed: 0.1154s/iter; left time: 3609.6311s\n",
      "\titers: 900, epoch: 9 | loss: 0.0726186\n",
      "\tspeed: 0.1157s/iter; left time: 3610.0175s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0716390\n",
      "\tspeed: 0.1154s/iter; left time: 3587.6555s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0776923\n",
      "\tspeed: 0.1136s/iter; left time: 3519.9077s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0776154\n",
      "\tspeed: 0.1131s/iter; left time: 3492.6586s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0860990\n",
      "\tspeed: 0.1160s/iter; left time: 3570.2019s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0690729\n",
      "\tspeed: 0.1139s/iter; left time: 3496.1104s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0735717\n",
      "\tspeed: 0.1116s/iter; left time: 3412.3343s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0749671\n",
      "\tspeed: 0.1152s/iter; left time: 3511.0997s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0911820\n",
      "\tspeed: 0.1138s/iter; left time: 3458.6774s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0734640\n",
      "\tspeed: 0.1157s/iter; left time: 3505.0243s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0728578\n",
      "\tspeed: 0.1153s/iter; left time: 3481.0690s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0706099\n",
      "\tspeed: 0.1147s/iter; left time: 3452.3748s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0727396\n",
      "\tspeed: 0.1155s/iter; left time: 3462.6380s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0882496\n",
      "\tspeed: 0.1152s/iter; left time: 3443.8173s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0900344\n",
      "\tspeed: 0.1159s/iter; left time: 3451.6548s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0780393\n",
      "\tspeed: 0.1155s/iter; left time: 3428.7190s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0849080\n",
      "\tspeed: 0.1150s/iter; left time: 3403.7123s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0787509\n",
      "\tspeed: 0.1151s/iter; left time: 3394.7966s\n",
      "Epoch: 9 cost time: 00h:05m:08.25s\n",
      "Epoch: 9 | Train Loss: 0.0790067 Vali Loss: 0.0807893 Test Loss: 0.0869210\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0815329\n",
      "\tspeed: 1.0193s/iter; left time: 29881.4935s\n",
      "\titers: 200, epoch: 10 | loss: 0.0723726\n",
      "\tspeed: 0.1165s/iter; left time: 3403.3836s\n",
      "\titers: 300, epoch: 10 | loss: 0.0746200\n",
      "\tspeed: 0.1142s/iter; left time: 3324.3854s\n",
      "\titers: 400, epoch: 10 | loss: 0.0674459\n",
      "\tspeed: 0.1128s/iter; left time: 3273.8902s\n",
      "\titers: 500, epoch: 10 | loss: 0.0791470\n",
      "\tspeed: 0.1145s/iter; left time: 3310.3644s\n",
      "\titers: 600, epoch: 10 | loss: 0.0762916\n",
      "\tspeed: 0.1190s/iter; left time: 3428.5021s\n",
      "\titers: 700, epoch: 10 | loss: 0.0842900\n",
      "\tspeed: 0.1147s/iter; left time: 3294.8868s\n",
      "\titers: 800, epoch: 10 | loss: 0.0661934\n",
      "\tspeed: 0.1133s/iter; left time: 3242.7917s\n",
      "\titers: 900, epoch: 10 | loss: 0.0785321\n",
      "\tspeed: 0.1158s/iter; left time: 3300.6932s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0710449\n",
      "\tspeed: 0.1159s/iter; left time: 3294.2681s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0821774\n",
      "\tspeed: 0.1136s/iter; left time: 3215.6312s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0766501\n",
      "\tspeed: 0.1129s/iter; left time: 3184.4876s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0781828\n",
      "\tspeed: 0.1144s/iter; left time: 3216.5979s\n",
      "\titers: 1400, epoch: 10 | loss: 0.1004712\n",
      "\tspeed: 0.1140s/iter; left time: 3194.0989s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0783593\n",
      "\tspeed: 0.1131s/iter; left time: 3158.2657s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0835777\n",
      "\tspeed: 0.1176s/iter; left time: 3271.0312s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0684503\n",
      "\tspeed: 0.1142s/iter; left time: 3165.4326s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0780601\n",
      "\tspeed: 0.1144s/iter; left time: 3159.8146s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0802295\n",
      "\tspeed: 0.1141s/iter; left time: 3140.6876s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0798366\n",
      "\tspeed: 0.1135s/iter; left time: 3110.5581s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0814212\n",
      "\tspeed: 0.1143s/iter; left time: 3121.5129s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0743018\n",
      "\tspeed: 0.1154s/iter; left time: 3139.6100s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0766831\n",
      "\tspeed: 0.1115s/iter; left time: 3024.2273s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0754861\n",
      "\tspeed: 0.1085s/iter; left time: 2931.5074s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0673950\n",
      "\tspeed: 0.1141s/iter; left time: 3071.9885s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0699747\n",
      "\tspeed: 0.1135s/iter; left time: 3044.8236s\n",
      "Epoch: 10 cost time: 00h:05m:05.94s\n",
      "Epoch: 10 | Train Loss: 0.0774372 Vali Loss: 0.0823554 Test Loss: 0.0883417\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019759919494390488, rmse:0.14056998491287231, mae:0.08683112263679504, rse:0.5315272808074951\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 96: 01h:08m:14.72s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-03 20:18:56,405] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-03 20:18:57,458] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-03 20:18:57,458] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-03 20:18:57,458] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-03 20:18:57,560] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-03 20:18:57,561] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-03 20:18:58,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-03 20:18:58,201] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-03 20:18:58,201] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-03 20:18:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-03 20:18:58,202] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-03 20:18:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-03 20:18:58,202] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-03 20:18:58,588] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-03 20:18:58,589] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.29 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,741] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-03 20:18:58,742] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,743] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.33 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,743] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-03 20:18:58,910] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-03 20:18:58,911] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-03 20:18:58,911] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.34 GB, percent = 25.6%\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-03 20:18:58,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-03 20:18:58,913] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-03 20:18:58,914] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc06dfa3110>\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-03 20:18:58,915] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-03 20:18:58,916] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-03 20:18:58,917] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2159382\n",
      "\tspeed: 0.1688s/iter; left time: 8987.9947s\n",
      "\titers: 200, epoch: 1 | loss: 0.2169471\n",
      "\tspeed: 0.1266s/iter; left time: 6725.1401s\n",
      "\titers: 300, epoch: 1 | loss: 0.1850180\n",
      "\tspeed: 0.1284s/iter; left time: 6811.7981s\n",
      "\titers: 400, epoch: 1 | loss: 0.1335363\n",
      "\tspeed: 0.1263s/iter; left time: 6688.5373s\n",
      "\titers: 500, epoch: 1 | loss: 0.1362359\n",
      "\tspeed: 0.1277s/iter; left time: 6745.5131s\n",
      "\titers: 600, epoch: 1 | loss: 0.1168138\n",
      "\tspeed: 0.1284s/iter; left time: 6770.0533s\n",
      "\titers: 700, epoch: 1 | loss: 0.1086815\n",
      "\tspeed: 0.1276s/iter; left time: 6718.7430s\n",
      "\titers: 800, epoch: 1 | loss: 0.1136740\n",
      "\tspeed: 0.1258s/iter; left time: 6611.2388s\n",
      "\titers: 900, epoch: 1 | loss: 0.1117396\n",
      "\tspeed: 0.1259s/iter; left time: 6603.8215s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1147921\n",
      "\tspeed: 0.1270s/iter; left time: 6647.8004s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1080129\n",
      "\tspeed: 0.1250s/iter; left time: 6528.0576s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1242537\n",
      "\tspeed: 0.1255s/iter; left time: 6544.4916s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1189426\n",
      "\tspeed: 0.1259s/iter; left time: 6549.9950s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0914909\n",
      "\tspeed: 0.1247s/iter; left time: 6476.1490s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1163723\n",
      "\tspeed: 0.1269s/iter; left time: 6580.0809s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1010581\n",
      "\tspeed: 0.1263s/iter; left time: 6535.9959s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0958032\n",
      "\tspeed: 0.1242s/iter; left time: 6414.0186s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0969449\n",
      "\tspeed: 0.1264s/iter; left time: 6515.8776s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0934629\n",
      "\tspeed: 0.1251s/iter; left time: 6433.0175s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1103176\n",
      "\tspeed: 0.1234s/iter; left time: 6336.3626s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1056070\n",
      "\tspeed: 0.1263s/iter; left time: 6473.2160s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1026355\n",
      "\tspeed: 0.1239s/iter; left time: 6335.1837s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1064908\n",
      "\tspeed: 0.1255s/iter; left time: 6407.7300s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1117751\n",
      "\tspeed: 0.1287s/iter; left time: 6554.4221s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1078130\n",
      "\tspeed: 0.1267s/iter; left time: 6440.2062s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1026039\n",
      "\tspeed: 0.1235s/iter; left time: 6264.4007s\n",
      "Epoch: 1 cost time: 00h:05m:37.43s\n",
      "Epoch: 1 | Train Loss: 0.1204411 Vali Loss: 0.0876045 Test Loss: 0.0915034\n",
      "Validation loss decreased (inf --> 0.087604).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0972830\n",
      "\tspeed: 1.1562s/iter; left time: 58472.3019s\n",
      "\titers: 200, epoch: 2 | loss: 0.0874374\n",
      "\tspeed: 0.1141s/iter; left time: 5757.8750s\n",
      "\titers: 300, epoch: 2 | loss: 0.0931379\n",
      "\tspeed: 0.1170s/iter; left time: 5896.2167s\n",
      "\titers: 400, epoch: 2 | loss: 0.1136137\n",
      "\tspeed: 0.1134s/iter; left time: 5699.3104s\n",
      "\titers: 500, epoch: 2 | loss: 0.1128266\n",
      "\tspeed: 0.1152s/iter; left time: 5782.3263s\n",
      "\titers: 600, epoch: 2 | loss: 0.1001407\n",
      "\tspeed: 0.1126s/iter; left time: 5639.9692s\n",
      "\titers: 700, epoch: 2 | loss: 0.0840503\n",
      "\tspeed: 0.1158s/iter; left time: 5788.7249s\n",
      "\titers: 800, epoch: 2 | loss: 0.0859213\n",
      "\tspeed: 0.1116s/iter; left time: 5564.6505s\n",
      "\titers: 900, epoch: 2 | loss: 0.1103960\n",
      "\tspeed: 0.1139s/iter; left time: 5670.9123s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0919214\n",
      "\tspeed: 0.1186s/iter; left time: 5891.5262s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0923761\n",
      "\tspeed: 0.1154s/iter; left time: 5719.5178s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0950010\n",
      "\tspeed: 0.1120s/iter; left time: 5539.8004s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0965657\n",
      "\tspeed: 0.1130s/iter; left time: 5578.7222s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0979968\n",
      "\tspeed: 0.1154s/iter; left time: 5686.1529s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1063472\n",
      "\tspeed: 0.1147s/iter; left time: 5640.1787s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1021644\n",
      "\tspeed: 0.1122s/iter; left time: 5506.7581s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1312233\n",
      "\tspeed: 0.1127s/iter; left time: 5521.2830s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0861884\n",
      "\tspeed: 0.1138s/iter; left time: 5564.2598s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0921369\n",
      "\tspeed: 0.1164s/iter; left time: 5677.3142s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0864059\n",
      "\tspeed: 0.1137s/iter; left time: 5535.1938s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1039751\n",
      "\tspeed: 0.1140s/iter; left time: 5537.6795s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1015228\n",
      "\tspeed: 0.1138s/iter; left time: 5514.8010s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0781528\n",
      "\tspeed: 0.1124s/iter; left time: 5438.5495s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0949763\n",
      "\tspeed: 0.1110s/iter; left time: 5360.2614s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0828070\n",
      "\tspeed: 0.1103s/iter; left time: 5313.6583s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0963483\n",
      "\tspeed: 0.1130s/iter; left time: 5430.7824s\n",
      "Epoch: 2 cost time: 00h:05m:03.97s\n",
      "Epoch: 2 | Train Loss: 0.0979559 Vali Loss: 0.0849390 Test Loss: 0.0888758\n",
      "Validation loss decreased (0.087604 --> 0.084939).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0922209\n",
      "\tspeed: 1.0196s/iter; left time: 48844.0855s\n",
      "\titers: 200, epoch: 3 | loss: 0.0644474\n",
      "\tspeed: 0.1129s/iter; left time: 5399.0376s\n",
      "\titers: 300, epoch: 3 | loss: 0.1041549\n",
      "\tspeed: 0.1143s/iter; left time: 5450.7034s\n",
      "\titers: 400, epoch: 3 | loss: 0.1109336\n",
      "\tspeed: 0.1132s/iter; left time: 5388.7711s\n",
      "\titers: 500, epoch: 3 | loss: 0.1064243\n",
      "\tspeed: 0.1131s/iter; left time: 5373.5663s\n",
      "\titers: 600, epoch: 3 | loss: 0.0900191\n",
      "\tspeed: 0.1145s/iter; left time: 5429.0218s\n",
      "\titers: 700, epoch: 3 | loss: 0.0858806\n",
      "\tspeed: 0.1136s/iter; left time: 5374.8444s\n",
      "\titers: 800, epoch: 3 | loss: 0.1083349\n",
      "\tspeed: 0.1112s/iter; left time: 5249.5299s\n",
      "\titers: 900, epoch: 3 | loss: 0.1029609\n",
      "\tspeed: 0.1125s/iter; left time: 5301.7130s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0872559\n",
      "\tspeed: 0.1136s/iter; left time: 5340.4887s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0983983\n",
      "\tspeed: 0.1154s/iter; left time: 5411.2824s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1123618\n",
      "\tspeed: 0.1119s/iter; left time: 5239.3426s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0855980\n",
      "\tspeed: 0.1120s/iter; left time: 5231.0785s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0841101\n",
      "\tspeed: 0.1118s/iter; left time: 5209.5184s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0859778\n",
      "\tspeed: 0.1124s/iter; left time: 5226.8275s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1071441\n",
      "\tspeed: 0.1128s/iter; left time: 5235.5990s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0972808\n",
      "\tspeed: 0.1125s/iter; left time: 5209.3522s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0949871\n",
      "\tspeed: 0.1119s/iter; left time: 5170.4263s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1109390\n",
      "\tspeed: 0.1138s/iter; left time: 5244.8320s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0801679\n",
      "\tspeed: 0.1160s/iter; left time: 5336.6812s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1097496\n",
      "\tspeed: 0.1120s/iter; left time: 5142.8642s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0926881\n",
      "\tspeed: 0.1125s/iter; left time: 5151.9609s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0823457\n",
      "\tspeed: 0.1121s/iter; left time: 5125.4586s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0971059\n",
      "\tspeed: 0.1141s/iter; left time: 5201.8496s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0847562\n",
      "\tspeed: 0.1130s/iter; left time: 5144.3606s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0877398\n",
      "\tspeed: 0.1137s/iter; left time: 5161.3596s\n",
      "Epoch: 3 cost time: 00h:05m:02.27s\n",
      "Epoch: 3 | Train Loss: 0.0939890 Vali Loss: 0.0849982 Test Loss: 0.0889016\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0809531\n",
      "\tspeed: 1.0044s/iter; left time: 45438.6496s\n",
      "\titers: 200, epoch: 4 | loss: 0.1035346\n",
      "\tspeed: 0.1128s/iter; left time: 5093.4320s\n",
      "\titers: 300, epoch: 4 | loss: 0.0931954\n",
      "\tspeed: 0.1140s/iter; left time: 5133.4622s\n",
      "\titers: 400, epoch: 4 | loss: 0.0936391\n",
      "\tspeed: 0.1149s/iter; left time: 5163.9889s\n",
      "\titers: 500, epoch: 4 | loss: 0.0769154\n",
      "\tspeed: 0.1148s/iter; left time: 5146.0266s\n",
      "\titers: 600, epoch: 4 | loss: 0.1011256\n",
      "\tspeed: 0.1127s/iter; left time: 5042.1903s\n",
      "\titers: 700, epoch: 4 | loss: 0.1071842\n",
      "\tspeed: 0.1131s/iter; left time: 5050.2225s\n",
      "\titers: 800, epoch: 4 | loss: 0.0825684\n",
      "\tspeed: 0.1130s/iter; left time: 5032.1413s\n",
      "\titers: 900, epoch: 4 | loss: 0.0715358\n",
      "\tspeed: 0.1175s/iter; left time: 5222.1823s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0911235\n",
      "\tspeed: 0.1122s/iter; left time: 4976.4855s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0931380\n",
      "\tspeed: 0.1121s/iter; left time: 4957.5722s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0906525\n",
      "\tspeed: 0.1126s/iter; left time: 4970.0985s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0844347\n",
      "\tspeed: 0.1125s/iter; left time: 4952.3628s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0868495\n",
      "\tspeed: 0.1140s/iter; left time: 5010.1955s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0898872\n",
      "\tspeed: 0.1120s/iter; left time: 4909.2841s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0944612\n",
      "\tspeed: 0.1123s/iter; left time: 4912.6020s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0744505\n",
      "\tspeed: 0.1140s/iter; left time: 4976.5667s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0840901\n",
      "\tspeed: 0.1121s/iter; left time: 4879.4054s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0785889\n",
      "\tspeed: 0.1157s/iter; left time: 5027.3227s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0908988\n",
      "\tspeed: 0.1145s/iter; left time: 4963.6864s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1051483\n",
      "\tspeed: 0.1126s/iter; left time: 4870.4880s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0749808\n",
      "\tspeed: 0.1124s/iter; left time: 4847.8524s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0964035\n",
      "\tspeed: 0.1123s/iter; left time: 4833.6900s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0797054\n",
      "\tspeed: 0.1128s/iter; left time: 4841.5484s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0989241\n",
      "\tspeed: 0.1122s/iter; left time: 4808.3043s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0763184\n",
      "\tspeed: 0.1124s/iter; left time: 4803.4697s\n",
      "Epoch: 4 cost time: 00h:05m:02.73s\n",
      "Epoch: 4 | Train Loss: 0.0912334 Vali Loss: 0.0860498 Test Loss: 0.0906333\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1012761\n",
      "\tspeed: 0.9978s/iter; left time: 42479.9415s\n",
      "\titers: 200, epoch: 5 | loss: 0.0788181\n",
      "\tspeed: 0.1127s/iter; left time: 4784.7008s\n",
      "\titers: 300, epoch: 5 | loss: 0.0916079\n",
      "\tspeed: 0.1132s/iter; left time: 4795.1335s\n",
      "\titers: 400, epoch: 5 | loss: 0.0774744\n",
      "\tspeed: 0.1142s/iter; left time: 4828.3127s\n",
      "\titers: 500, epoch: 5 | loss: 0.0913393\n",
      "\tspeed: 0.1145s/iter; left time: 4827.0348s\n",
      "\titers: 600, epoch: 5 | loss: 0.0951685\n",
      "\tspeed: 0.1133s/iter; left time: 4768.1042s\n",
      "\titers: 700, epoch: 5 | loss: 0.0887147\n",
      "\tspeed: 0.1131s/iter; left time: 4745.9684s\n",
      "\titers: 800, epoch: 5 | loss: 0.0910846\n",
      "\tspeed: 0.1139s/iter; left time: 4768.2605s\n",
      "\titers: 900, epoch: 5 | loss: 0.0891837\n",
      "\tspeed: 0.1128s/iter; left time: 4711.0590s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0790758\n",
      "\tspeed: 0.1133s/iter; left time: 4722.2869s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0895170\n",
      "\tspeed: 0.1145s/iter; left time: 4760.3129s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0942188\n",
      "\tspeed: 0.1131s/iter; left time: 4690.9434s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0948133\n",
      "\tspeed: 0.1126s/iter; left time: 4659.9481s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0987849\n",
      "\tspeed: 0.1167s/iter; left time: 4815.8104s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0812767\n",
      "\tspeed: 0.1149s/iter; left time: 4732.1020s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0904108\n",
      "\tspeed: 0.1145s/iter; left time: 4700.9119s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1042102\n",
      "\tspeed: 0.1130s/iter; left time: 4631.5456s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0898959\n",
      "\tspeed: 0.1130s/iter; left time: 4617.0331s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0911765\n",
      "\tspeed: 0.1132s/iter; left time: 4616.6145s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0696084\n",
      "\tspeed: 0.1135s/iter; left time: 4617.9026s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0943199\n",
      "\tspeed: 0.1152s/iter; left time: 4672.8086s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0874961\n",
      "\tspeed: 0.1113s/iter; left time: 4505.1616s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0919802\n",
      "\tspeed: 0.1122s/iter; left time: 4529.2977s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0831057\n",
      "\tspeed: 0.1130s/iter; left time: 4550.3049s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0805359\n",
      "\tspeed: 0.1137s/iter; left time: 4569.3736s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0857960\n",
      "\tspeed: 0.1137s/iter; left time: 4554.3336s\n",
      "Epoch: 5 cost time: 00h:05m:03.27s\n",
      "Epoch: 5 | Train Loss: 0.0882231 Vali Loss: 0.0871280 Test Loss: 0.0905764\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0777886\n",
      "\tspeed: 0.9981s/iter; left time: 39828.6754s\n",
      "\titers: 200, epoch: 6 | loss: 0.0941440\n",
      "\tspeed: 0.1153s/iter; left time: 4590.3012s\n",
      "\titers: 300, epoch: 6 | loss: 0.0958172\n",
      "\tspeed: 0.1138s/iter; left time: 4518.4601s\n",
      "\titers: 400, epoch: 6 | loss: 0.0725196\n",
      "\tspeed: 0.1136s/iter; left time: 4497.4792s\n",
      "\titers: 500, epoch: 6 | loss: 0.0889861\n",
      "\tspeed: 0.1136s/iter; left time: 4486.5884s\n",
      "\titers: 600, epoch: 6 | loss: 0.0903357\n",
      "\tspeed: 0.1140s/iter; left time: 4494.2167s\n",
      "\titers: 700, epoch: 6 | loss: 0.0943096\n",
      "\tspeed: 0.1158s/iter; left time: 4552.7988s\n",
      "\titers: 800, epoch: 6 | loss: 0.0846683\n",
      "\tspeed: 0.1139s/iter; left time: 4465.7063s\n",
      "\titers: 900, epoch: 6 | loss: 0.0848038\n",
      "\tspeed: 0.1174s/iter; left time: 4591.0282s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0868105\n",
      "\tspeed: 0.1137s/iter; left time: 4436.4564s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0821280\n",
      "\tspeed: 0.1144s/iter; left time: 4449.0651s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0953802\n",
      "\tspeed: 0.1142s/iter; left time: 4430.3935s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0808989\n",
      "\tspeed: 0.1159s/iter; left time: 4487.6126s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0898427\n",
      "\tspeed: 0.1138s/iter; left time: 4394.5421s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0939664\n",
      "\tspeed: 0.1166s/iter; left time: 4490.2968s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0925003\n",
      "\tspeed: 0.1140s/iter; left time: 4380.1347s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0959591\n",
      "\tspeed: 0.1141s/iter; left time: 4369.6036s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0839964\n",
      "\tspeed: 0.1109s/iter; left time: 4235.9092s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0757705\n",
      "\tspeed: 0.1123s/iter; left time: 4278.5917s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0871647\n",
      "\tspeed: 0.1121s/iter; left time: 4261.2119s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0874589\n",
      "\tspeed: 0.1125s/iter; left time: 4263.0298s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0961731\n",
      "\tspeed: 0.1130s/iter; left time: 4273.7239s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0795345\n",
      "\tspeed: 0.1139s/iter; left time: 4296.2716s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0825180\n",
      "\tspeed: 0.1130s/iter; left time: 4251.1629s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0753969\n",
      "\tspeed: 0.1122s/iter; left time: 4208.6607s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0764885\n",
      "\tspeed: 0.1132s/iter; left time: 4234.7952s\n",
      "Epoch: 6 cost time: 00h:05m:04.35s\n",
      "Epoch: 6 | Train Loss: 0.0857598 Vali Loss: 0.0890246 Test Loss: 0.0932542\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0879309\n",
      "\tspeed: 1.0021s/iter; left time: 37316.4604s\n",
      "\titers: 200, epoch: 7 | loss: 0.0852456\n",
      "\tspeed: 0.1156s/iter; left time: 4293.4604s\n",
      "\titers: 300, epoch: 7 | loss: 0.0813645\n",
      "\tspeed: 0.1148s/iter; left time: 4253.5320s\n",
      "\titers: 400, epoch: 7 | loss: 0.0748249\n",
      "\tspeed: 0.1139s/iter; left time: 4207.1884s\n",
      "\titers: 500, epoch: 7 | loss: 0.0818025\n",
      "\tspeed: 0.1149s/iter; left time: 4232.5474s\n",
      "\titers: 600, epoch: 7 | loss: 0.0957716\n",
      "\tspeed: 0.1157s/iter; left time: 4249.1404s\n",
      "\titers: 700, epoch: 7 | loss: 0.0731218\n",
      "\tspeed: 0.1143s/iter; left time: 4186.5099s\n",
      "\titers: 800, epoch: 7 | loss: 0.0727848\n",
      "\tspeed: 0.1144s/iter; left time: 4181.6516s\n",
      "\titers: 900, epoch: 7 | loss: 0.0822178\n",
      "\tspeed: 0.1148s/iter; left time: 4181.4862s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0866405\n",
      "\tspeed: 0.1143s/iter; left time: 4152.4235s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0839373\n",
      "\tspeed: 0.1160s/iter; left time: 4202.3776s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0864395\n",
      "\tspeed: 0.1117s/iter; left time: 4035.0252s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1001431\n",
      "\tspeed: 0.1156s/iter; left time: 4165.4831s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0870203\n",
      "\tspeed: 0.1135s/iter; left time: 4079.4270s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0783804\n",
      "\tspeed: 0.1138s/iter; left time: 4079.6902s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0840335\n",
      "\tspeed: 0.1152s/iter; left time: 4116.2249s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0848608\n",
      "\tspeed: 0.1136s/iter; left time: 4048.5709s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0887816\n",
      "\tspeed: 0.1140s/iter; left time: 4049.6803s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0716856\n",
      "\tspeed: 0.1126s/iter; left time: 3990.8076s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0809039\n",
      "\tspeed: 0.1142s/iter; left time: 4034.9774s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0809592\n",
      "\tspeed: 0.1132s/iter; left time: 3990.2337s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0854933\n",
      "\tspeed: 0.1133s/iter; left time: 3982.5259s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0758755\n",
      "\tspeed: 0.1122s/iter; left time: 3932.4847s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0947762\n",
      "\tspeed: 0.1110s/iter; left time: 3877.5687s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0809882\n",
      "\tspeed: 0.1130s/iter; left time: 3937.3034s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0749397\n",
      "\tspeed: 0.1143s/iter; left time: 3971.5130s\n",
      "Epoch: 7 cost time: 00h:05m:04.25s\n",
      "Epoch: 7 | Train Loss: 0.0833416 Vali Loss: 0.0886686 Test Loss: 0.0912572\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019771769642829895, rmse:0.14061212539672852, mae:0.08887572586536407, rse:0.531994640827179\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 168: 00h:46m:46.48s\n",
      "\n",
      "Intermediate time for IT: 03h:27m:29.51s\n",
      "\n",
      "Total time: 19h:53m:35.94s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>0.1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.2049</td>\n",
       "      <td>0.1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1445</td>\n",
       "      <td>0.0956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.0968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.0817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0227  0.1508  0.0954\n",
       "        96        0.0358  0.1892  0.1282\n",
       "        168       0.0377  0.1941  0.1336\n",
       "GB      24        0.0256  0.1599  0.1040\n",
       "        96        0.0420  0.2049  0.1405\n",
       "        168       0.0428  0.2068  0.1438\n",
       "ES      24        0.0107  0.1033  0.0665\n",
       "        96        0.0209  0.1445  0.0956\n",
       "        168       0.0211  0.1454  0.0968\n",
       "FR      24        0.0111  0.1052  0.0600\n",
       "        96        0.0185  0.1359  0.0817\n",
       "        168       0.0204  0.1428  0.0872\n",
       "IT      24        0.0108  0.1038  0.0620\n",
       "        96        0.0198  0.1406  0.0868\n",
       "        168       0.0198  0.1406  0.0889"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm.csv'))\n",
    "timellm_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
