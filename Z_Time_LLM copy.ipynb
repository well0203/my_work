{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=10\n",
    "learning_rate=0.001 # 10^-3\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=16\n",
    "d_ff=64\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 - shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 87051\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-31 10:16:13,269] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 10:16:14,434] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 10:16:14,434] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 10:16:14,434] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 10:16:14,542] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 10:16:14,542] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 10:16:15,238] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 10:16:15,239] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 10:16:15,239] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 10:16:15,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 10:16:15,241] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 10:16:15,241] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 10:16:15,241] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 10:16:15,241] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 10:16:15,241] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 10:16:15,242] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 10:16:15,550] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 10:16:15,551] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 10:16:15,552] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.76 GB, percent = 9.9%\n",
      "[2024-10-31 10:16:15,668] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 10:16:15,669] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 10:16:15,669] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.76 GB, percent = 9.9%\n",
      "[2024-10-31 10:16:15,669] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 10:16:15,782] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 10:16:15,783] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 10:16:15,783] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.76 GB, percent = 9.9%\n",
      "[2024-10-31 10:16:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 10:16:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 10:16:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 10:16:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 10:16:15,785] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f18b1d037d0>\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 10:16:15,786] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 10:16:15,787] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 10:16:15,788] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1253250\n",
      "\tspeed: 0.1544s/iter; left time: 5585.3557s\n",
      "\titers: 200, epoch: 1 | loss: 0.1021231\n",
      "\tspeed: 0.1105s/iter; left time: 3986.1413s\n",
      "\titers: 300, epoch: 1 | loss: 0.1171103\n",
      "\tspeed: 0.1106s/iter; left time: 3977.1664s\n",
      "\titers: 400, epoch: 1 | loss: 0.1162789\n",
      "\tspeed: 0.1105s/iter; left time: 3964.1508s\n",
      "\titers: 500, epoch: 1 | loss: 0.1428971\n",
      "\tspeed: 0.1105s/iter; left time: 3950.9266s\n",
      "\titers: 600, epoch: 1 | loss: 0.1143428\n",
      "\tspeed: 0.1109s/iter; left time: 3957.0957s\n",
      "\titers: 700, epoch: 1 | loss: 0.1217580\n",
      "\tspeed: 0.1109s/iter; left time: 3945.1048s\n",
      "\titers: 800, epoch: 1 | loss: 0.0895846\n",
      "\tspeed: 0.1106s/iter; left time: 3923.3871s\n",
      "\titers: 900, epoch: 1 | loss: 0.0948934\n",
      "\tspeed: 0.1110s/iter; left time: 3927.4469s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0878272\n",
      "\tspeed: 0.1106s/iter; left time: 3899.8166s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0601976\n",
      "\tspeed: 0.1106s/iter; left time: 3890.5982s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0668006\n",
      "\tspeed: 0.1106s/iter; left time: 3879.9601s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0563509\n",
      "\tspeed: 0.1104s/iter; left time: 3861.7988s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0729959\n",
      "\tspeed: 0.1109s/iter; left time: 3867.3587s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0963133\n",
      "\tspeed: 0.1104s/iter; left time: 3838.8626s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0723569\n",
      "\tspeed: 0.1103s/iter; left time: 3823.3074s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0683628\n",
      "\tspeed: 0.1082s/iter; left time: 3741.8047s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0682905\n",
      "\tspeed: 0.1103s/iter; left time: 3802.5923s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0678902\n",
      "\tspeed: 0.1108s/iter; left time: 3809.1287s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0897312\n",
      "\tspeed: 0.1104s/iter; left time: 3784.9022s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0700516\n",
      "\tspeed: 0.1111s/iter; left time: 3797.9132s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0578480\n",
      "\tspeed: 0.1106s/iter; left time: 3766.7433s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0774600\n",
      "\tspeed: 0.1107s/iter; left time: 3760.2886s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0529886\n",
      "\tspeed: 0.1107s/iter; left time: 3749.7533s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0783439\n",
      "\tspeed: 0.1110s/iter; left time: 3747.1430s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0777916\n",
      "\tspeed: 0.1102s/iter; left time: 3712.0514s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0717537\n",
      "\tspeed: 0.1109s/iter; left time: 3721.5740s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0538910\n",
      "\tspeed: 0.0995s/iter; left time: 3328.8810s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0740641\n",
      "\tspeed: 0.1105s/iter; left time: 3686.2288s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0713672\n",
      "\tspeed: 0.1104s/iter; left time: 3672.9459s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0659280\n",
      "\tspeed: 0.1101s/iter; left time: 3651.7809s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0547582\n",
      "\tspeed: 0.1097s/iter; left time: 3627.3845s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0646752\n",
      "\tspeed: 0.0974s/iter; left time: 3211.8971s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0649581\n",
      "\tspeed: 0.1103s/iter; left time: 3626.7889s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0636399\n",
      "\tspeed: 0.1108s/iter; left time: 3631.4149s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0661442\n",
      "\tspeed: 0.1109s/iter; left time: 3623.9676s\n",
      "Epoch: 1 cost time: 00h:06m:39.68s\n",
      "Epoch: 1 | Train Loss: 0.0828758 Vali Loss: 0.0657178 Test Loss: 0.0690873\n",
      "Validation loss decreased (inf --> 0.065718).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0641395\n",
      "\tspeed: 1.0368s/iter; left time: 33742.7926s\n",
      "\titers: 200, epoch: 2 | loss: 0.0605974\n",
      "\tspeed: 0.1006s/iter; left time: 3264.2622s\n",
      "\titers: 300, epoch: 2 | loss: 0.0702055\n",
      "\tspeed: 0.1007s/iter; left time: 3257.5667s\n",
      "\titers: 400, epoch: 2 | loss: 0.0842534\n",
      "\tspeed: 0.1011s/iter; left time: 3258.7110s\n",
      "\titers: 500, epoch: 2 | loss: 0.0583125\n",
      "\tspeed: 0.1009s/iter; left time: 3241.9193s\n",
      "\titers: 600, epoch: 2 | loss: 0.0591843\n",
      "\tspeed: 0.1007s/iter; left time: 3227.6873s\n",
      "\titers: 700, epoch: 2 | loss: 0.0897410\n",
      "\tspeed: 0.1007s/iter; left time: 3217.5165s\n",
      "\titers: 800, epoch: 2 | loss: 0.0574091\n",
      "\tspeed: 0.1014s/iter; left time: 3228.3320s\n",
      "\titers: 900, epoch: 2 | loss: 0.0805708\n",
      "\tspeed: 0.1007s/iter; left time: 3198.1302s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0752786\n",
      "\tspeed: 0.1009s/iter; left time: 3191.7904s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0684123\n",
      "\tspeed: 0.1006s/iter; left time: 3171.7924s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0658052\n",
      "\tspeed: 0.1013s/iter; left time: 3184.2507s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0646981\n",
      "\tspeed: 0.1008s/iter; left time: 3158.4148s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0675445\n",
      "\tspeed: 0.1007s/iter; left time: 3147.3111s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0594096\n",
      "\tspeed: 0.1013s/iter; left time: 3154.9731s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0673212\n",
      "\tspeed: 0.1006s/iter; left time: 3124.0183s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0634067\n",
      "\tspeed: 0.1009s/iter; left time: 3121.3693s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0698779\n",
      "\tspeed: 0.1005s/iter; left time: 3099.0370s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0664236\n",
      "\tspeed: 0.1009s/iter; left time: 3100.8779s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0610143\n",
      "\tspeed: 0.1005s/iter; left time: 3079.2681s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0621212\n",
      "\tspeed: 0.1007s/iter; left time: 3075.2805s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0552139\n",
      "\tspeed: 0.1009s/iter; left time: 3071.5231s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0587330\n",
      "\tspeed: 0.1009s/iter; left time: 3060.8022s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0624281\n",
      "\tspeed: 0.1007s/iter; left time: 3047.0689s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0672231\n",
      "\tspeed: 0.1006s/iter; left time: 3032.2378s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0766141\n",
      "\tspeed: 0.1014s/iter; left time: 3045.8468s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0596296\n",
      "\tspeed: 0.1007s/iter; left time: 3016.2204s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0592866\n",
      "\tspeed: 0.0906s/iter; left time: 2703.3921s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0505566\n",
      "\tspeed: 0.1007s/iter; left time: 2995.8994s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0617194\n",
      "\tspeed: 0.1001s/iter; left time: 2966.3300s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0665330\n",
      "\tspeed: 0.1009s/iter; left time: 2982.3521s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0583781\n",
      "\tspeed: 0.1009s/iter; left time: 2970.8445s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0605434\n",
      "\tspeed: 0.1011s/iter; left time: 2965.3791s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0657654\n",
      "\tspeed: 0.0923s/iter; left time: 2699.7922s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0490196\n",
      "\tspeed: 0.0968s/iter; left time: 2820.8067s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0598664\n",
      "\tspeed: 0.1010s/iter; left time: 2932.1797s\n",
      "Epoch: 2 cost time: 00h:06m:03.85s\n",
      "Epoch: 2 | Train Loss: 0.0632200 Vali Loss: 0.0632346 Test Loss: 0.0667238\n",
      "Validation loss decreased (0.065718 --> 0.063235).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0495207\n",
      "\tspeed: 0.9179s/iter; left time: 26542.4298s\n",
      "\titers: 200, epoch: 3 | loss: 0.0581172\n",
      "\tspeed: 0.1009s/iter; left time: 2908.3892s\n",
      "\titers: 300, epoch: 3 | loss: 0.0567204\n",
      "\tspeed: 0.1007s/iter; left time: 2891.1748s\n",
      "\titers: 400, epoch: 3 | loss: 0.0617661\n",
      "\tspeed: 0.1013s/iter; left time: 2898.1649s\n",
      "\titers: 500, epoch: 3 | loss: 0.0692774\n",
      "\tspeed: 0.1010s/iter; left time: 2880.0909s\n",
      "\titers: 600, epoch: 3 | loss: 0.0563664\n",
      "\tspeed: 0.1004s/iter; left time: 2851.6718s\n",
      "\titers: 700, epoch: 3 | loss: 0.0559464\n",
      "\tspeed: 0.1005s/iter; left time: 2844.9354s\n",
      "\titers: 800, epoch: 3 | loss: 0.0570201\n",
      "\tspeed: 0.1005s/iter; left time: 2835.4995s\n",
      "\titers: 900, epoch: 3 | loss: 0.0575496\n",
      "\tspeed: 0.1007s/iter; left time: 2831.0699s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0575499\n",
      "\tspeed: 0.1005s/iter; left time: 2816.1570s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0515742\n",
      "\tspeed: 0.1008s/iter; left time: 2813.4777s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0564314\n",
      "\tspeed: 0.1008s/iter; left time: 2804.9835s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0654939\n",
      "\tspeed: 0.1005s/iter; left time: 2786.2388s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0651760\n",
      "\tspeed: 0.1002s/iter; left time: 2768.5842s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0613425\n",
      "\tspeed: 0.1006s/iter; left time: 2767.7164s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0716897\n",
      "\tspeed: 0.1011s/iter; left time: 2772.6253s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0471442\n",
      "\tspeed: 0.1045s/iter; left time: 2853.7666s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0589232\n",
      "\tspeed: 0.1005s/iter; left time: 2735.8454s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0913787\n",
      "\tspeed: 0.1012s/iter; left time: 2742.9030s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0737848\n",
      "\tspeed: 0.1011s/iter; left time: 2732.4933s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0483308\n",
      "\tspeed: 0.1007s/iter; left time: 2711.5032s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0547054\n",
      "\tspeed: 0.0972s/iter; left time: 2607.2268s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0610089\n",
      "\tspeed: 0.1004s/iter; left time: 2681.9959s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0682351\n",
      "\tspeed: 0.1007s/iter; left time: 2680.2180s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0535607\n",
      "\tspeed: 0.0981s/iter; left time: 2600.0923s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0573487\n",
      "\tspeed: 0.0981s/iter; left time: 2592.2055s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0681689\n",
      "\tspeed: 0.1004s/iter; left time: 2642.4343s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0589341\n",
      "\tspeed: 0.1007s/iter; left time: 2641.1948s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0729253\n",
      "\tspeed: 0.1010s/iter; left time: 2636.8797s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0694794\n",
      "\tspeed: 0.1011s/iter; left time: 2631.4207s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0549058\n",
      "\tspeed: 0.1012s/iter; left time: 2622.7124s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0573769\n",
      "\tspeed: 0.1008s/iter; left time: 2602.2329s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0519171\n",
      "\tspeed: 0.1011s/iter; left time: 2599.4215s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0659145\n",
      "\tspeed: 0.1013s/iter; left time: 2594.0863s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0666064\n",
      "\tspeed: 0.1004s/iter; left time: 2560.9177s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0598675\n",
      "\tspeed: 0.1013s/iter; left time: 2575.1989s\n",
      "Epoch: 3 cost time: 00h:06m:05.60s\n",
      "Epoch: 3 | Train Loss: 0.0612112 Vali Loss: 0.0622638 Test Loss: 0.0656645\n",
      "Validation loss decreased (0.063235 --> 0.062264).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0769365\n",
      "\tspeed: 0.9082s/iter; left time: 22967.6400s\n",
      "\titers: 200, epoch: 4 | loss: 0.0678117\n",
      "\tspeed: 0.1008s/iter; left time: 2538.4028s\n",
      "\titers: 300, epoch: 4 | loss: 0.0503367\n",
      "\tspeed: 0.1008s/iter; left time: 2528.8016s\n",
      "\titers: 400, epoch: 4 | loss: 0.0696090\n",
      "\tspeed: 0.1016s/iter; left time: 2538.1214s\n",
      "\titers: 500, epoch: 4 | loss: 0.0703934\n",
      "\tspeed: 0.1007s/iter; left time: 2507.2744s\n",
      "\titers: 600, epoch: 4 | loss: 0.0465980\n",
      "\tspeed: 0.1005s/iter; left time: 2492.5821s\n",
      "\titers: 700, epoch: 4 | loss: 0.0533346\n",
      "\tspeed: 0.1004s/iter; left time: 2479.2961s\n",
      "\titers: 800, epoch: 4 | loss: 0.0982846\n",
      "\tspeed: 0.1007s/iter; left time: 2476.8263s\n",
      "\titers: 900, epoch: 4 | loss: 0.0560940\n",
      "\tspeed: 0.1013s/iter; left time: 2482.0550s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0532002\n",
      "\tspeed: 0.1010s/iter; left time: 2462.5663s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0589694\n",
      "\tspeed: 0.1007s/iter; left time: 2445.2313s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0572653\n",
      "\tspeed: 0.1005s/iter; left time: 2431.9916s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0562861\n",
      "\tspeed: 0.1010s/iter; left time: 2433.3766s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0638494\n",
      "\tspeed: 0.1008s/iter; left time: 2419.2932s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0456899\n",
      "\tspeed: 0.1011s/iter; left time: 2414.5572s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0646571\n",
      "\tspeed: 0.0995s/iter; left time: 2368.1259s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0507534\n",
      "\tspeed: 0.1011s/iter; left time: 2393.8863s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0702571\n",
      "\tspeed: 0.1007s/iter; left time: 2376.1905s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0598839\n",
      "\tspeed: 0.1008s/iter; left time: 2368.0544s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0720136\n",
      "\tspeed: 0.1006s/iter; left time: 2353.4479s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0608016\n",
      "\tspeed: 0.1007s/iter; left time: 2345.4162s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0723014\n",
      "\tspeed: 0.1010s/iter; left time: 2341.3418s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0649720\n",
      "\tspeed: 0.1009s/iter; left time: 2328.9993s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0563692\n",
      "\tspeed: 0.1009s/iter; left time: 2319.7002s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0551559\n",
      "\tspeed: 0.1008s/iter; left time: 2306.4271s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0573570\n",
      "\tspeed: 0.1006s/iter; left time: 2291.7257s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0580379\n",
      "\tspeed: 0.1008s/iter; left time: 2286.1180s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0584222\n",
      "\tspeed: 0.1012s/iter; left time: 2285.8322s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0522651\n",
      "\tspeed: 0.1011s/iter; left time: 2272.7857s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0613821\n",
      "\tspeed: 0.1011s/iter; left time: 2263.0342s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0548703\n",
      "\tspeed: 0.1007s/iter; left time: 2245.4440s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0707316\n",
      "\tspeed: 0.1009s/iter; left time: 2238.4317s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0467648\n",
      "\tspeed: 0.1009s/iter; left time: 2228.5070s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0677512\n",
      "\tspeed: 0.1008s/iter; left time: 2216.1990s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0605054\n",
      "\tspeed: 0.1007s/iter; left time: 2204.5105s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0551683\n",
      "\tspeed: 0.1009s/iter; left time: 2199.3833s\n",
      "Epoch: 4 cost time: 00h:06m:06.15s\n",
      "Epoch: 4 | Train Loss: 0.0596988 Vali Loss: 0.0608321 Test Loss: 0.0649735\n",
      "Validation loss decreased (0.062264 --> 0.060832).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0421814\n",
      "\tspeed: 0.9026s/iter; left time: 19554.1033s\n",
      "\titers: 200, epoch: 5 | loss: 0.0596779\n",
      "\tspeed: 0.1008s/iter; left time: 2174.2549s\n",
      "\titers: 300, epoch: 5 | loss: 0.0674294\n",
      "\tspeed: 0.0913s/iter; left time: 1959.2025s\n",
      "\titers: 400, epoch: 5 | loss: 0.0638640\n",
      "\tspeed: 0.1004s/iter; left time: 2144.2147s\n",
      "\titers: 500, epoch: 5 | loss: 0.0659714\n",
      "\tspeed: 0.1008s/iter; left time: 2144.2238s\n",
      "\titers: 600, epoch: 5 | loss: 0.0466051\n",
      "\tspeed: 0.1010s/iter; left time: 2136.4424s\n",
      "\titers: 700, epoch: 5 | loss: 0.0620384\n",
      "\tspeed: 0.1007s/iter; left time: 2121.5172s\n",
      "\titers: 800, epoch: 5 | loss: 0.0657692\n",
      "\tspeed: 0.1011s/iter; left time: 2118.4625s\n",
      "\titers: 900, epoch: 5 | loss: 0.0889024\n",
      "\tspeed: 0.1010s/iter; left time: 2106.8842s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0601121\n",
      "\tspeed: 0.1009s/iter; left time: 2095.2822s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0639189\n",
      "\tspeed: 0.1012s/iter; left time: 2090.0699s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0570241\n",
      "\tspeed: 0.1004s/iter; left time: 2065.3788s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0446014\n",
      "\tspeed: 0.1016s/iter; left time: 2078.4773s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0653667\n",
      "\tspeed: 0.1009s/iter; left time: 2055.2265s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0502729\n",
      "\tspeed: 0.1018s/iter; left time: 2062.4543s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0712510\n",
      "\tspeed: 0.1007s/iter; left time: 2031.3092s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0697132\n",
      "\tspeed: 0.1011s/iter; left time: 2029.1035s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0620848\n",
      "\tspeed: 0.1009s/iter; left time: 2014.6234s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0644220\n",
      "\tspeed: 0.1009s/iter; left time: 2003.3361s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0671872\n",
      "\tspeed: 0.1011s/iter; left time: 1997.9566s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0558427\n",
      "\tspeed: 0.1009s/iter; left time: 1984.3569s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0600384\n",
      "\tspeed: 0.1011s/iter; left time: 1977.2682s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0589903\n",
      "\tspeed: 0.1009s/iter; left time: 1962.9006s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0700589\n",
      "\tspeed: 0.1008s/iter; left time: 1952.4466s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0645664\n",
      "\tspeed: 0.1009s/iter; left time: 1943.0181s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0585323\n",
      "\tspeed: 0.1002s/iter; left time: 1920.9973s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0385193\n",
      "\tspeed: 0.1000s/iter; left time: 1905.4410s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0597674\n",
      "\tspeed: 0.1007s/iter; left time: 1909.6255s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0455539\n",
      "\tspeed: 0.1004s/iter; left time: 1894.6341s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0393745\n",
      "\tspeed: 0.1008s/iter; left time: 1892.1967s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0595335\n",
      "\tspeed: 0.1012s/iter; left time: 1888.4401s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0467035\n",
      "\tspeed: 0.1008s/iter; left time: 1871.5554s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0693364\n",
      "\tspeed: 0.1007s/iter; left time: 1859.0657s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0691323\n",
      "\tspeed: 0.1010s/iter; left time: 1854.0423s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0629323\n",
      "\tspeed: 0.1007s/iter; left time: 1839.2564s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0596608\n",
      "\tspeed: 0.1008s/iter; left time: 1830.0995s\n",
      "Epoch: 5 cost time: 00h:06m:05.40s\n",
      "Epoch: 5 | Train Loss: 0.0584383 Vali Loss: 0.0592592 Test Loss: 0.0630640\n",
      "Validation loss decreased (0.060832 --> 0.059259).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0412966\n",
      "\tspeed: 0.9069s/iter; left time: 16356.8122s\n",
      "\titers: 200, epoch: 6 | loss: 0.0685626\n",
      "\tspeed: 0.0980s/iter; left time: 1757.2308s\n",
      "\titers: 300, epoch: 6 | loss: 0.0524663\n",
      "\tspeed: 0.0741s/iter; left time: 1320.7606s\n",
      "\titers: 400, epoch: 6 | loss: 0.0648706\n",
      "\tspeed: 0.0873s/iter; left time: 1548.2366s\n",
      "\titers: 500, epoch: 6 | loss: 0.0775328\n",
      "\tspeed: 0.1008s/iter; left time: 1777.0593s\n",
      "\titers: 600, epoch: 6 | loss: 0.0562397\n",
      "\tspeed: 0.1007s/iter; left time: 1766.4606s\n",
      "\titers: 700, epoch: 6 | loss: 0.0473584\n",
      "\tspeed: 0.1007s/iter; left time: 1755.0321s\n",
      "\titers: 800, epoch: 6 | loss: 0.0514995\n",
      "\tspeed: 0.1011s/iter; left time: 1753.4940s\n",
      "\titers: 900, epoch: 6 | loss: 0.0643374\n",
      "\tspeed: 0.1011s/iter; left time: 1741.7396s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0432567\n",
      "\tspeed: 0.1010s/iter; left time: 1731.3129s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0582200\n",
      "\tspeed: 0.1008s/iter; left time: 1717.7687s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0513337\n",
      "\tspeed: 0.1009s/iter; left time: 1708.2941s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0463711\n",
      "\tspeed: 0.1008s/iter; left time: 1696.4651s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0648535\n",
      "\tspeed: 0.1011s/iter; left time: 1692.5554s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0624466\n",
      "\tspeed: 0.1017s/iter; left time: 1691.3012s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0532007\n",
      "\tspeed: 0.1008s/iter; left time: 1667.5097s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0453120\n",
      "\tspeed: 0.1008s/iter; left time: 1656.2737s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0620884\n",
      "\tspeed: 0.1008s/iter; left time: 1647.1449s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0463145\n",
      "\tspeed: 0.1011s/iter; left time: 1640.9393s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0530511\n",
      "\tspeed: 0.1006s/iter; left time: 1623.7334s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0600467\n",
      "\tspeed: 0.1008s/iter; left time: 1615.7490s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0553319\n",
      "\tspeed: 0.1009s/iter; left time: 1607.3142s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0633792\n",
      "\tspeed: 0.1007s/iter; left time: 1595.0826s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0425149\n",
      "\tspeed: 0.1010s/iter; left time: 1588.5575s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0520189\n",
      "\tspeed: 0.0910s/iter; left time: 1422.9576s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0562708\n",
      "\tspeed: 0.0806s/iter; left time: 1251.8916s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0456553\n",
      "\tspeed: 0.0821s/iter; left time: 1267.5314s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0536861\n",
      "\tspeed: 0.0741s/iter; left time: 1136.1096s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0519671\n",
      "\tspeed: 0.0766s/iter; left time: 1167.1577s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0505474\n",
      "\tspeed: 0.1015s/iter; left time: 1535.8522s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0647396\n",
      "\tspeed: 0.0782s/iter; left time: 1176.0110s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0473092\n",
      "\tspeed: 0.0741s/iter; left time: 1106.8601s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0622138\n",
      "\tspeed: 0.0892s/iter; left time: 1322.9559s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0526910\n",
      "\tspeed: 0.0741s/iter; left time: 1092.1299s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0465786\n",
      "\tspeed: 0.0900s/iter; left time: 1317.2855s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0598951\n",
      "\tspeed: 0.1006s/iter; left time: 1461.6882s\n",
      "Epoch: 6 cost time: 00h:05m:42.17s\n",
      "Epoch: 6 | Train Loss: 0.0573057 Vali Loss: 0.0579225 Test Loss: 0.0615203\n",
      "Validation loss decreased (0.059259 --> 0.057923).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0497068\n",
      "\tspeed: 0.9166s/iter; left time: 13207.0591s\n",
      "\titers: 200, epoch: 7 | loss: 0.0593571\n",
      "\tspeed: 0.1009s/iter; left time: 1443.2291s\n",
      "\titers: 300, epoch: 7 | loss: 0.0671686\n",
      "\tspeed: 0.1008s/iter; left time: 1432.9370s\n",
      "\titers: 400, epoch: 7 | loss: 0.0568853\n",
      "\tspeed: 0.1006s/iter; left time: 1419.1680s\n",
      "\titers: 500, epoch: 7 | loss: 0.0585472\n",
      "\tspeed: 0.1007s/iter; left time: 1410.9516s\n",
      "\titers: 600, epoch: 7 | loss: 0.0606201\n",
      "\tspeed: 0.1006s/iter; left time: 1398.6033s\n",
      "\titers: 700, epoch: 7 | loss: 0.0594972\n",
      "\tspeed: 0.1010s/iter; left time: 1394.0357s\n",
      "\titers: 800, epoch: 7 | loss: 0.0471607\n",
      "\tspeed: 0.1009s/iter; left time: 1383.5322s\n",
      "\titers: 900, epoch: 7 | loss: 0.0786800\n",
      "\tspeed: 0.1010s/iter; left time: 1374.7953s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0657087\n",
      "\tspeed: 0.1008s/iter; left time: 1362.3486s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0608326\n",
      "\tspeed: 0.1008s/iter; left time: 1351.3837s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0636445\n",
      "\tspeed: 0.1011s/iter; left time: 1345.3221s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0693407\n",
      "\tspeed: 0.1013s/iter; left time: 1337.9512s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0533294\n",
      "\tspeed: 0.1007s/iter; left time: 1319.6941s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0499788\n",
      "\tspeed: 0.1011s/iter; left time: 1315.4832s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0495753\n",
      "\tspeed: 0.1008s/iter; left time: 1301.2998s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0488599\n",
      "\tspeed: 0.1013s/iter; left time: 1297.0890s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0641755\n",
      "\tspeed: 0.1007s/iter; left time: 1280.3298s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0705614\n",
      "\tspeed: 0.1007s/iter; left time: 1269.3542s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0575744\n",
      "\tspeed: 0.1005s/iter; left time: 1256.6129s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0416317\n",
      "\tspeed: 0.1008s/iter; left time: 1250.4661s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0499019\n",
      "\tspeed: 0.1008s/iter; left time: 1241.1009s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0491204\n",
      "\tspeed: 0.1009s/iter; left time: 1231.6337s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0566759\n",
      "\tspeed: 0.1007s/iter; left time: 1219.4951s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0648431\n",
      "\tspeed: 0.1007s/iter; left time: 1208.9288s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0489153\n",
      "\tspeed: 0.1009s/iter; left time: 1201.8218s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0619887\n",
      "\tspeed: 0.1008s/iter; left time: 1189.8797s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0413250\n",
      "\tspeed: 0.1007s/iter; left time: 1179.3136s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0706107\n",
      "\tspeed: 0.0837s/iter; left time: 971.9688s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0528892\n",
      "\tspeed: 0.0945s/iter; left time: 1087.3152s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0561556\n",
      "\tspeed: 0.0997s/iter; left time: 1137.9596s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0627407\n",
      "\tspeed: 0.0948s/iter; left time: 1071.9755s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0737613\n",
      "\tspeed: 0.0996s/iter; left time: 1116.5062s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0462471\n",
      "\tspeed: 0.1001s/iter; left time: 1112.4180s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0369232\n",
      "\tspeed: 0.1013s/iter; left time: 1114.9624s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0469571\n",
      "\tspeed: 0.1006s/iter; left time: 1097.0825s\n",
      "Epoch: 7 cost time: 00h:06m:02.96s\n",
      "Epoch: 7 | Train Loss: 0.0563991 Vali Loss: 0.0570018 Test Loss: 0.0617092\n",
      "Validation loss decreased (0.057923 --> 0.057002).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0519227\n",
      "\tspeed: 0.9122s/iter; left time: 9835.4055s\n",
      "\titers: 200, epoch: 8 | loss: 0.0745292\n",
      "\tspeed: 0.1008s/iter; left time: 1076.3594s\n",
      "\titers: 300, epoch: 8 | loss: 0.0468175\n",
      "\tspeed: 0.1007s/iter; left time: 1065.7702s\n",
      "\titers: 400, epoch: 8 | loss: 0.0625759\n",
      "\tspeed: 0.1012s/iter; left time: 1061.2066s\n",
      "\titers: 500, epoch: 8 | loss: 0.0551540\n",
      "\tspeed: 0.1010s/iter; left time: 1048.6956s\n",
      "\titers: 600, epoch: 8 | loss: 0.0463570\n",
      "\tspeed: 0.0831s/iter; left time: 854.4752s\n",
      "\titers: 700, epoch: 8 | loss: 0.0593237\n",
      "\tspeed: 0.0740s/iter; left time: 753.1582s\n",
      "\titers: 800, epoch: 8 | loss: 0.0563329\n",
      "\tspeed: 0.0836s/iter; left time: 843.1846s\n",
      "\titers: 900, epoch: 8 | loss: 0.0529911\n",
      "\tspeed: 0.1009s/iter; left time: 1007.1298s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0466789\n",
      "\tspeed: 0.1008s/iter; left time: 995.7477s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0446758\n",
      "\tspeed: 0.1005s/iter; left time: 983.1917s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0380010\n",
      "\tspeed: 0.1007s/iter; left time: 974.5055s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0602951\n",
      "\tspeed: 0.0918s/iter; left time: 879.2360s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0543634\n",
      "\tspeed: 0.0910s/iter; left time: 862.9559s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0538214\n",
      "\tspeed: 0.1010s/iter; left time: 948.0322s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0589658\n",
      "\tspeed: 0.1011s/iter; left time: 938.5166s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0457566\n",
      "\tspeed: 0.1002s/iter; left time: 920.2848s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0614366\n",
      "\tspeed: 0.1012s/iter; left time: 918.9723s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0551149\n",
      "\tspeed: 0.1007s/iter; left time: 904.3413s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0446189\n",
      "\tspeed: 0.1007s/iter; left time: 894.4351s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0510369\n",
      "\tspeed: 0.1008s/iter; left time: 885.1474s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0485257\n",
      "\tspeed: 0.1007s/iter; left time: 874.6648s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0433204\n",
      "\tspeed: 0.1010s/iter; left time: 866.9769s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0499586\n",
      "\tspeed: 0.1008s/iter; left time: 854.5989s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0700116\n",
      "\tspeed: 0.1008s/iter; left time: 844.5074s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0619013\n",
      "\tspeed: 0.1012s/iter; left time: 837.7576s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0568440\n",
      "\tspeed: 0.1008s/iter; left time: 824.5752s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0573367\n",
      "\tspeed: 0.1006s/iter; left time: 813.3691s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0478784\n",
      "\tspeed: 0.1009s/iter; left time: 805.1395s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0558106\n",
      "\tspeed: 0.1009s/iter; left time: 795.2331s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0657972\n",
      "\tspeed: 0.1007s/iter; left time: 783.7690s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0568907\n",
      "\tspeed: 0.1007s/iter; left time: 773.8990s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0529016\n",
      "\tspeed: 0.1008s/iter; left time: 763.9427s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0664156\n",
      "\tspeed: 0.1007s/iter; left time: 753.5454s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0526277\n",
      "\tspeed: 0.0969s/iter; left time: 715.2612s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0458799\n",
      "\tspeed: 0.0755s/iter; left time: 550.0675s\n",
      "Epoch: 8 cost time: 00h:05m:55.19s\n",
      "Epoch: 8 | Train Loss: 0.0554903 Vali Loss: 0.0562172 Test Loss: 0.0600664\n",
      "Validation loss decreased (0.057002 --> 0.056217).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0410241\n",
      "\tspeed: 0.9101s/iter; left time: 6511.9175s\n",
      "\titers: 200, epoch: 9 | loss: 0.0438558\n",
      "\tspeed: 0.1010s/iter; left time: 712.8063s\n",
      "\titers: 300, epoch: 9 | loss: 0.0573859\n",
      "\tspeed: 0.1006s/iter; left time: 699.3963s\n",
      "\titers: 400, epoch: 9 | loss: 0.0471512\n",
      "\tspeed: 0.1017s/iter; left time: 696.8395s\n",
      "\titers: 500, epoch: 9 | loss: 0.0461001\n",
      "\tspeed: 0.0993s/iter; left time: 671.0409s\n",
      "\titers: 600, epoch: 9 | loss: 0.0707894\n",
      "\tspeed: 0.0962s/iter; left time: 640.4720s\n",
      "\titers: 700, epoch: 9 | loss: 0.0548274\n",
      "\tspeed: 0.1011s/iter; left time: 662.8396s\n",
      "\titers: 800, epoch: 9 | loss: 0.0472955\n",
      "\tspeed: 0.1009s/iter; left time: 651.6192s\n",
      "\titers: 900, epoch: 9 | loss: 0.0773195\n",
      "\tspeed: 0.1008s/iter; left time: 640.5699s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0543193\n",
      "\tspeed: 0.1007s/iter; left time: 629.6972s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0482565\n",
      "\tspeed: 0.1009s/iter; left time: 621.0179s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0515400\n",
      "\tspeed: 0.1019s/iter; left time: 616.9427s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0582468\n",
      "\tspeed: 0.1009s/iter; left time: 600.6712s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0624413\n",
      "\tspeed: 0.1009s/iter; left time: 590.5309s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0623868\n",
      "\tspeed: 0.1011s/iter; left time: 581.5765s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0538385\n",
      "\tspeed: 0.1014s/iter; left time: 573.2198s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0521328\n",
      "\tspeed: 0.1006s/iter; left time: 558.9698s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0618994\n",
      "\tspeed: 0.1013s/iter; left time: 552.4945s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0642067\n",
      "\tspeed: 0.1018s/iter; left time: 545.1917s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0493218\n",
      "\tspeed: 0.1006s/iter; left time: 528.8769s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0707670\n",
      "\tspeed: 0.1007s/iter; left time: 519.1015s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0610348\n",
      "\tspeed: 0.0982s/iter; left time: 496.6221s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0466000\n",
      "\tspeed: 0.0987s/iter; left time: 489.1502s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0572287\n",
      "\tspeed: 0.1007s/iter; left time: 489.1283s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0700380\n",
      "\tspeed: 0.1007s/iter; left time: 479.0518s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0535235\n",
      "\tspeed: 0.1010s/iter; left time: 470.3481s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0577309\n",
      "\tspeed: 0.1008s/iter; left time: 458.9296s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0672236\n",
      "\tspeed: 0.1005s/iter; left time: 447.8451s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0453178\n",
      "\tspeed: 0.1008s/iter; left time: 439.0557s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0675185\n",
      "\tspeed: 0.1009s/iter; left time: 429.1978s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0638553\n",
      "\tspeed: 0.1012s/iter; left time: 420.3080s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0451474\n",
      "\tspeed: 0.1010s/iter; left time: 409.6888s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0645056\n",
      "\tspeed: 0.1005s/iter; left time: 397.3964s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0548450\n",
      "\tspeed: 0.1013s/iter; left time: 390.5264s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0711983\n",
      "\tspeed: 0.1016s/iter; left time: 381.3499s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0580304\n",
      "\tspeed: 0.1017s/iter; left time: 371.8584s\n",
      "Epoch: 9 cost time: 00h:06m:05.76s\n",
      "Epoch: 9 | Train Loss: 0.0548941 Vali Loss: 0.0567484 Test Loss: 0.0616054\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0595498\n",
      "\tspeed: 0.8944s/iter; left time: 3155.4681s\n",
      "\titers: 200, epoch: 10 | loss: 0.0515962\n",
      "\tspeed: 0.1015s/iter; left time: 347.8412s\n",
      "\titers: 300, epoch: 10 | loss: 0.0686309\n",
      "\tspeed: 0.1009s/iter; left time: 335.6340s\n",
      "\titers: 400, epoch: 10 | loss: 0.0704564\n",
      "\tspeed: 0.0862s/iter; left time: 278.3468s\n",
      "\titers: 500, epoch: 10 | loss: 0.0558669\n",
      "\tspeed: 0.0804s/iter; left time: 251.5585s\n",
      "\titers: 600, epoch: 10 | loss: 0.0490901\n",
      "\tspeed: 0.0797s/iter; left time: 241.2903s\n",
      "\titers: 700, epoch: 10 | loss: 0.0688823\n",
      "\tspeed: 0.1007s/iter; left time: 294.9188s\n",
      "\titers: 800, epoch: 10 | loss: 0.0436001\n",
      "\tspeed: 0.1003s/iter; left time: 283.6165s\n",
      "\titers: 900, epoch: 10 | loss: 0.0606400\n",
      "\tspeed: 0.1006s/iter; left time: 274.5633s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0485917\n",
      "\tspeed: 0.1008s/iter; left time: 264.9147s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0515312\n",
      "\tspeed: 0.1000s/iter; left time: 252.7061s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0512043\n",
      "\tspeed: 0.1008s/iter; left time: 244.8279s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0437651\n",
      "\tspeed: 0.1014s/iter; left time: 236.0305s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0399656\n",
      "\tspeed: 0.1008s/iter; left time: 224.5249s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0542896\n",
      "\tspeed: 0.0938s/iter; left time: 199.5385s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0524097\n",
      "\tspeed: 0.0747s/iter; left time: 151.3961s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0416506\n",
      "\tspeed: 0.0750s/iter; left time: 144.6704s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0575458\n",
      "\tspeed: 0.1006s/iter; left time: 183.8095s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0599817\n",
      "\tspeed: 0.1012s/iter; left time: 174.8209s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0555415\n",
      "\tspeed: 0.1007s/iter; left time: 163.8670s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0635784\n",
      "\tspeed: 0.1008s/iter; left time: 153.9474s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0630910\n",
      "\tspeed: 0.1012s/iter; left time: 144.5623s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0516529\n",
      "\tspeed: 0.1006s/iter; left time: 133.5450s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0580106\n",
      "\tspeed: 0.1006s/iter; left time: 123.5264s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0449881\n",
      "\tspeed: 0.1008s/iter; left time: 113.6820s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0559825\n",
      "\tspeed: 0.1008s/iter; left time: 103.6665s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0746538\n",
      "\tspeed: 0.1008s/iter; left time: 93.5797s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0605203\n",
      "\tspeed: 0.1009s/iter; left time: 83.5389s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0479751\n",
      "\tspeed: 0.1007s/iter; left time: 73.3250s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0619233\n",
      "\tspeed: 0.1008s/iter; left time: 63.2973s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0540915\n",
      "\tspeed: 0.1018s/iter; left time: 53.7638s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0626322\n",
      "\tspeed: 0.1013s/iter; left time: 43.3460s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0593155\n",
      "\tspeed: 0.1009s/iter; left time: 33.0820s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0501555\n",
      "\tspeed: 0.1008s/iter; left time: 22.9781s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0620754\n",
      "\tspeed: 0.1013s/iter; left time: 12.9663s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0470215\n",
      "\tspeed: 0.1011s/iter; left time: 2.8316s\n",
      "Epoch: 10 cost time: 00h:05m:54.85s\n",
      "Epoch: 10 | Train Loss: 0.0542839 Vali Loss: 0.0547844 Test Loss: 0.0594678\n",
      "Validation loss decreased (0.056217 --> 0.054784).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "Scaled mse:0.010848339647054672, rmse:0.10415536165237427, mae:0.05946776270866394, rse:0.40204980969429016\n",
      "success delete checkpoints\n",
      "Total time: 75.43539918661118 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"24\"]\n",
    "\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96 - shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 00:57:31,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 00:57:32,497] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 00:57:32,497] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 00:57:32,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 00:57:32,594] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 00:57:32,594] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 00:57:33,254] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 00:57:33,256] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 00:57:33,256] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 00:57:33,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 00:57:33,258] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 00:57:33,258] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 00:57:33,570] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 00:57:33,571] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,571] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,695] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 00:57:33,696] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,696] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,697] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 00:57:33,813] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 00:57:33,814] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,814] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 00:57:33,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 00:57:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 00:57:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 00:57:33,815] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a0d1309d0>\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1332849\n",
      "\tspeed: 0.1525s/iter; left time: 5422.3506s\n",
      "\titers: 200, epoch: 1 | loss: 0.1226809\n",
      "\tspeed: 0.1111s/iter; left time: 3937.9748s\n",
      "\titers: 300, epoch: 1 | loss: 0.0829579\n",
      "\tspeed: 0.1109s/iter; left time: 3922.9468s\n",
      "\titers: 400, epoch: 1 | loss: 0.0719160\n",
      "\tspeed: 0.1002s/iter; left time: 3534.6732s\n",
      "\titers: 500, epoch: 1 | loss: 0.1208322\n",
      "\tspeed: 0.1098s/iter; left time: 3861.2302s\n",
      "\titers: 600, epoch: 1 | loss: 0.0824153\n",
      "\tspeed: 0.1110s/iter; left time: 3890.1517s\n",
      "\titers: 700, epoch: 1 | loss: 0.0565333\n",
      "\tspeed: 0.1136s/iter; left time: 3971.6596s\n",
      "\titers: 800, epoch: 1 | loss: 0.0793769\n",
      "\tspeed: 0.1146s/iter; left time: 3993.7417s\n",
      "\titers: 900, epoch: 1 | loss: 0.0705150\n",
      "\tspeed: 0.1139s/iter; left time: 3959.8387s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0586823\n",
      "\tspeed: 0.1131s/iter; left time: 3920.6815s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0585032\n",
      "\tspeed: 0.1126s/iter; left time: 3890.7956s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0598272\n",
      "\tspeed: 0.1127s/iter; left time: 3883.5807s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0872594\n",
      "\tspeed: 0.1121s/iter; left time: 3851.2699s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0588569\n",
      "\tspeed: 0.1111s/iter; left time: 3804.7949s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0763347\n",
      "\tspeed: 0.1110s/iter; left time: 3793.3456s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0903879\n",
      "\tspeed: 0.1111s/iter; left time: 3783.2268s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801517\n",
      "\tspeed: 0.1099s/iter; left time: 3732.3117s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0630893\n",
      "\tspeed: 0.1110s/iter; left time: 3757.7620s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0696212\n",
      "\tspeed: 0.1112s/iter; left time: 3755.4751s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0660686\n",
      "\tspeed: 0.1135s/iter; left time: 3820.2270s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0738638\n",
      "\tspeed: 0.1107s/iter; left time: 3716.4403s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0562153\n",
      "\tspeed: 0.1025s/iter; left time: 3428.4043s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0773825\n",
      "\tspeed: 0.1061s/iter; left time: 3540.6957s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0946050\n",
      "\tspeed: 0.1083s/iter; left time: 3601.5181s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0675518\n",
      "\tspeed: 0.1136s/iter; left time: 3765.8907s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0667586\n",
      "\tspeed: 0.1134s/iter; left time: 3747.5797s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0709311\n",
      "\tspeed: 0.1121s/iter; left time: 3695.7801s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0718190\n",
      "\tspeed: 0.1112s/iter; left time: 3654.7393s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0784791\n",
      "\tspeed: 0.1111s/iter; left time: 3639.2803s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0715083\n",
      "\tspeed: 0.1118s/iter; left time: 3652.8302s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0626798\n",
      "\tspeed: 0.1143s/iter; left time: 3720.9594s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0780201\n",
      "\tspeed: 0.1126s/iter; left time: 3653.6404s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0764208\n",
      "\tspeed: 0.1113s/iter; left time: 3602.0657s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0627120\n",
      "\tspeed: 0.1111s/iter; left time: 3585.2850s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0708910\n",
      "\tspeed: 0.1141s/iter; left time: 3669.5756s\n",
      "Epoch: 1 cost time: 00h:06m:37.81s\n",
      "Epoch: 1 | Train Loss: 0.0782478 Vali Loss: 0.0741555 Test Loss: 0.0831796\n",
      "Validation loss decreased (inf --> 0.074155).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0658899\n",
      "\tspeed: 1.1564s/iter; left time: 36998.0622s\n",
      "\titers: 200, epoch: 2 | loss: 0.1048066\n",
      "\tspeed: 0.1083s/iter; left time: 3455.3250s\n",
      "\titers: 300, epoch: 2 | loss: 0.0702246\n",
      "\tspeed: 0.1063s/iter; left time: 3380.0659s\n",
      "\titers: 400, epoch: 2 | loss: 0.0755464\n",
      "\tspeed: 0.1084s/iter; left time: 3435.8489s\n",
      "\titers: 500, epoch: 2 | loss: 0.0727920\n",
      "\tspeed: 0.1027s/iter; left time: 3244.7229s\n",
      "\titers: 600, epoch: 2 | loss: 0.0613693\n",
      "\tspeed: 0.1009s/iter; left time: 3178.7378s\n",
      "\titers: 700, epoch: 2 | loss: 0.0999854\n",
      "\tspeed: 0.1026s/iter; left time: 3220.8845s\n",
      "\titers: 800, epoch: 2 | loss: 0.0538424\n",
      "\tspeed: 0.1045s/iter; left time: 3270.4582s\n",
      "\titers: 900, epoch: 2 | loss: 0.0550089\n",
      "\tspeed: 0.1068s/iter; left time: 3332.8771s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0533296\n",
      "\tspeed: 0.1076s/iter; left time: 3346.2745s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0814121\n",
      "\tspeed: 0.1066s/iter; left time: 3305.1335s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0633737\n",
      "\tspeed: 0.1062s/iter; left time: 3280.5800s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0798584\n",
      "\tspeed: 0.1067s/iter; left time: 3284.7576s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0698421\n",
      "\tspeed: 0.1041s/iter; left time: 3195.5801s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0627612\n",
      "\tspeed: 0.1011s/iter; left time: 3092.0542s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0672406\n",
      "\tspeed: 0.1052s/iter; left time: 3209.2672s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0693460\n",
      "\tspeed: 0.1069s/iter; left time: 3250.4720s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0610231\n",
      "\tspeed: 0.1051s/iter; left time: 3183.4856s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0713101\n",
      "\tspeed: 0.1052s/iter; left time: 3175.3243s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0816538\n",
      "\tspeed: 0.1015s/iter; left time: 3053.6582s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0808621\n",
      "\tspeed: 0.1053s/iter; left time: 3158.4254s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0554627\n",
      "\tspeed: 0.1037s/iter; left time: 3101.0450s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0546244\n",
      "\tspeed: 0.1011s/iter; left time: 3012.7038s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0925839\n",
      "\tspeed: 0.1009s/iter; left time: 2997.6863s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0774217\n",
      "\tspeed: 0.1017s/iter; left time: 3009.2214s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0685905\n",
      "\tspeed: 0.1027s/iter; left time: 3029.5529s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0614267\n",
      "\tspeed: 0.1071s/iter; left time: 3149.5926s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0766011\n",
      "\tspeed: 0.1010s/iter; left time: 2959.3594s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0837857\n",
      "\tspeed: 0.1009s/iter; left time: 2947.1969s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0537917\n",
      "\tspeed: 0.1027s/iter; left time: 2988.3968s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0577093\n",
      "\tspeed: 0.1034s/iter; left time: 2999.4673s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0555598\n",
      "\tspeed: 0.1036s/iter; left time: 2993.8941s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0752852\n",
      "\tspeed: 0.1013s/iter; left time: 2916.1508s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0711872\n",
      "\tspeed: 0.1011s/iter; left time: 2899.7425s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0683053\n",
      "\tspeed: 0.1008s/iter; left time: 2883.6118s\n",
      "Epoch: 2 cost time: 00h:06m:10.53s\n",
      "Epoch: 2 | Train Loss: 0.0687149 Vali Loss: 0.0723769 Test Loss: 0.0818021\n",
      "Validation loss decreased (0.074155 --> 0.072377).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0715792\n",
      "\tspeed: 1.0501s/iter; left time: 29852.5583s\n",
      "\titers: 200, epoch: 3 | loss: 0.0788853\n",
      "\tspeed: 0.1079s/iter; left time: 3056.2498s\n",
      "\titers: 300, epoch: 3 | loss: 0.0674003\n",
      "\tspeed: 0.1092s/iter; left time: 3081.2454s\n",
      "\titers: 400, epoch: 3 | loss: 0.0723566\n",
      "\tspeed: 0.1055s/iter; left time: 2967.7069s\n",
      "\titers: 500, epoch: 3 | loss: 0.0499699\n",
      "\tspeed: 0.1023s/iter; left time: 2868.4844s\n",
      "\titers: 600, epoch: 3 | loss: 0.0738916\n",
      "\tspeed: 0.1011s/iter; left time: 2822.6192s\n",
      "\titers: 700, epoch: 3 | loss: 0.0666234\n",
      "\tspeed: 0.1007s/iter; left time: 2801.0269s\n",
      "\titers: 800, epoch: 3 | loss: 0.0641793\n",
      "\tspeed: 0.1006s/iter; left time: 2790.9017s\n",
      "\titers: 900, epoch: 3 | loss: 0.0726167\n",
      "\tspeed: 0.1023s/iter; left time: 2825.8826s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0578101\n",
      "\tspeed: 0.1032s/iter; left time: 2841.2733s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0814463\n",
      "\tspeed: 0.1040s/iter; left time: 2853.6983s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0677088\n",
      "\tspeed: 0.1024s/iter; left time: 2798.3725s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0701640\n",
      "\tspeed: 0.1038s/iter; left time: 2827.0669s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0820722\n",
      "\tspeed: 0.1078s/iter; left time: 2924.3541s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0492643\n",
      "\tspeed: 0.1074s/iter; left time: 2903.9063s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0652639\n",
      "\tspeed: 0.1076s/iter; left time: 2897.2064s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0597799\n",
      "\tspeed: 0.1043s/iter; left time: 2799.4180s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0708796\n",
      "\tspeed: 0.1085s/iter; left time: 2901.4328s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0690726\n",
      "\tspeed: 0.1076s/iter; left time: 2865.3361s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0778807\n",
      "\tspeed: 0.1065s/iter; left time: 2826.1841s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0669764\n",
      "\tspeed: 0.1065s/iter; left time: 2815.6669s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0535536\n",
      "\tspeed: 0.1027s/iter; left time: 2703.3007s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0868591\n",
      "\tspeed: 0.1015s/iter; left time: 2662.3715s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0679954\n",
      "\tspeed: 0.0900s/iter; left time: 2351.6746s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0618816\n",
      "\tspeed: 0.0832s/iter; left time: 2165.2316s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0856984\n",
      "\tspeed: 0.0918s/iter; left time: 2379.5403s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0775335\n",
      "\tspeed: 0.0922s/iter; left time: 2380.2053s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0633479\n",
      "\tspeed: 0.1013s/iter; left time: 2605.7964s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0762474\n",
      "\tspeed: 0.1061s/iter; left time: 2720.3866s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0556791\n",
      "\tspeed: 0.1084s/iter; left time: 2766.2124s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0651474\n",
      "\tspeed: 0.1066s/iter; left time: 2711.4251s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0652379\n",
      "\tspeed: 0.1068s/iter; left time: 2704.3028s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0641431\n",
      "\tspeed: 0.1059s/iter; left time: 2671.1666s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0688879\n",
      "\tspeed: 0.1026s/iter; left time: 2578.7920s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0765588\n",
      "\tspeed: 0.1004s/iter; left time: 2513.3767s\n",
      "Epoch: 3 cost time: 00h:06m:07.63s\n",
      "Epoch: 3 | Train Loss: 0.0664232 Vali Loss: 0.0730986 Test Loss: 0.0813632\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0668811\n",
      "\tspeed: 0.9960s/iter; left time: 24764.2108s\n",
      "\titers: 200, epoch: 4 | loss: 0.0618269\n",
      "\tspeed: 0.1047s/iter; left time: 2593.6831s\n",
      "\titers: 300, epoch: 4 | loss: 0.0673992\n",
      "\tspeed: 0.1009s/iter; left time: 2487.5053s\n",
      "\titers: 400, epoch: 4 | loss: 0.0552185\n",
      "\tspeed: 0.0941s/iter; left time: 2311.3379s\n",
      "\titers: 500, epoch: 4 | loss: 0.0635973\n",
      "\tspeed: 0.1013s/iter; left time: 2478.1625s\n",
      "\titers: 600, epoch: 4 | loss: 0.0599482\n",
      "\tspeed: 0.0996s/iter; left time: 2425.6772s\n",
      "\titers: 700, epoch: 4 | loss: 0.0629725\n",
      "\tspeed: 0.1010s/iter; left time: 2451.3936s\n",
      "\titers: 800, epoch: 4 | loss: 0.0704623\n",
      "\tspeed: 0.1012s/iter; left time: 2444.8091s\n",
      "\titers: 900, epoch: 4 | loss: 0.0448214\n",
      "\tspeed: 0.1027s/iter; left time: 2470.7113s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0590776\n",
      "\tspeed: 0.1043s/iter; left time: 2498.4632s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0764958\n",
      "\tspeed: 0.1037s/iter; left time: 2474.1831s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0797139\n",
      "\tspeed: 0.1034s/iter; left time: 2456.0600s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0617398\n",
      "\tspeed: 0.1005s/iter; left time: 2377.8538s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0606667\n",
      "\tspeed: 0.0941s/iter; left time: 2218.3238s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0683119\n",
      "\tspeed: 0.0908s/iter; left time: 2130.2372s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0832711\n",
      "\tspeed: 0.1080s/iter; left time: 2523.0359s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0639146\n",
      "\tspeed: 0.1068s/iter; left time: 2484.5848s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0657482\n",
      "\tspeed: 0.1045s/iter; left time: 2419.5254s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0762344\n",
      "\tspeed: 0.1030s/iter; left time: 2376.5488s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0764676\n",
      "\tspeed: 0.0989s/iter; left time: 2271.7865s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0657827\n",
      "\tspeed: 0.1073s/iter; left time: 2452.4932s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0669597\n",
      "\tspeed: 0.1040s/iter; left time: 2367.0266s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0562412\n",
      "\tspeed: 0.1008s/iter; left time: 2285.4928s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0579680\n",
      "\tspeed: 0.1010s/iter; left time: 2278.0797s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0637907\n",
      "\tspeed: 0.1009s/iter; left time: 2265.8098s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0796028\n",
      "\tspeed: 0.1016s/iter; left time: 2271.1449s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0815872\n",
      "\tspeed: 0.1012s/iter; left time: 2252.2328s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0738015\n",
      "\tspeed: 0.1021s/iter; left time: 2262.7412s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0625055\n",
      "\tspeed: 0.1017s/iter; left time: 2243.9369s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0475844\n",
      "\tspeed: 0.1078s/iter; left time: 2368.3269s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0625621\n",
      "\tspeed: 0.1014s/iter; left time: 2216.4136s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0543961\n",
      "\tspeed: 0.1018s/iter; left time: 2215.4833s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0665312\n",
      "\tspeed: 0.1044s/iter; left time: 2262.6529s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0553873\n",
      "\tspeed: 0.1015s/iter; left time: 2187.5671s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0641216\n",
      "\tspeed: 0.1010s/iter; left time: 2167.2104s\n",
      "Epoch: 4 cost time: 00h:06m:03.64s\n",
      "Epoch: 4 | Train Loss: 0.0638932 Vali Loss: 0.0744487 Test Loss: 0.0830895\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0693577\n",
      "\tspeed: 0.9849s/iter; left time: 20976.2253s\n",
      "\titers: 200, epoch: 5 | loss: 0.0654913\n",
      "\tspeed: 0.1011s/iter; left time: 2143.0017s\n",
      "\titers: 300, epoch: 5 | loss: 0.0657027\n",
      "\tspeed: 0.1054s/iter; left time: 2222.7227s\n",
      "\titers: 400, epoch: 5 | loss: 0.0641252\n",
      "\tspeed: 0.1054s/iter; left time: 2212.3505s\n",
      "\titers: 500, epoch: 5 | loss: 0.0781336\n",
      "\tspeed: 0.1035s/iter; left time: 2163.6012s\n",
      "\titers: 600, epoch: 5 | loss: 0.0593817\n",
      "\tspeed: 0.1009s/iter; left time: 2097.3820s\n",
      "\titers: 700, epoch: 5 | loss: 0.0620136\n",
      "\tspeed: 0.1009s/iter; left time: 2089.1806s\n",
      "\titers: 800, epoch: 5 | loss: 0.0565020\n",
      "\tspeed: 0.1049s/iter; left time: 2161.0705s\n",
      "\titers: 900, epoch: 5 | loss: 0.0513705\n",
      "\tspeed: 0.1045s/iter; left time: 2142.4728s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0668830\n",
      "\tspeed: 0.1026s/iter; left time: 2092.2332s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0586694\n",
      "\tspeed: 0.1032s/iter; left time: 2095.2850s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0762751\n",
      "\tspeed: 0.1092s/iter; left time: 2205.2618s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0615793\n",
      "\tspeed: 0.1030s/iter; left time: 2069.5538s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0658676\n",
      "\tspeed: 0.1009s/iter; left time: 2017.2203s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0459896\n",
      "\tspeed: 0.1010s/iter; left time: 2008.7170s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0505971\n",
      "\tspeed: 0.1007s/iter; left time: 1993.2489s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0715075\n",
      "\tspeed: 0.1009s/iter; left time: 1986.4897s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0574153\n",
      "\tspeed: 0.1009s/iter; left time: 1976.9396s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0615106\n",
      "\tspeed: 0.1007s/iter; left time: 1962.6543s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0518319\n",
      "\tspeed: 0.1020s/iter; left time: 1977.7919s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0631665\n",
      "\tspeed: 0.1019s/iter; left time: 1966.8690s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0617508\n",
      "\tspeed: 0.1033s/iter; left time: 1983.3061s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0547799\n",
      "\tspeed: 0.1008s/iter; left time: 1925.6670s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0817395\n",
      "\tspeed: 0.1007s/iter; left time: 1913.7312s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0702271\n",
      "\tspeed: 0.1009s/iter; left time: 1905.9542s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0471027\n",
      "\tspeed: 0.1008s/iter; left time: 1895.6628s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0532453\n",
      "\tspeed: 0.1007s/iter; left time: 1883.5455s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0648276\n",
      "\tspeed: 0.1006s/iter; left time: 1870.3500s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0574297\n",
      "\tspeed: 0.1014s/iter; left time: 1875.3598s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0605013\n",
      "\tspeed: 0.1017s/iter; left time: 1871.6813s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0599723\n",
      "\tspeed: 0.1053s/iter; left time: 1926.9828s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0612394\n",
      "\tspeed: 0.1063s/iter; left time: 1934.2763s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0544313\n",
      "\tspeed: 0.1063s/iter; left time: 1923.7660s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0706279\n",
      "\tspeed: 0.1016s/iter; left time: 1828.7376s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0658244\n",
      "\tspeed: 0.1007s/iter; left time: 1801.5072s\n",
      "Epoch: 5 cost time: 00h:06m:05.66s\n",
      "Epoch: 5 | Train Loss: 0.0611344 Vali Loss: 0.0769676 Test Loss: 0.0859656\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018535790964961052, rmse:0.13614621758460999, mae:0.08180207759141922, rse:0.5266919136047363\n",
      "success delete checkpoints\n",
      "Total time: 136.75494347016016 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96, lr descrease: 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 02:07:44,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 02:07:45,926] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 02:07:45,926] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 02:07:45,926] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 02:07:46,031] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 02:07:46,032] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 02:07:46,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 02:07:46,702] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 02:07:46,702] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 02:07:46,704] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 02:07:46,704] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 02:07:46,704] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 02:07:46,704] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 02:07:46,704] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 02:07:46,705] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 02:07:46,705] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 02:07:47,035] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 02:07:47,036] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,078] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,204] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 02:07:47,205] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,205] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,205] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 02:07:47,316] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 02:07:47,317] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 02:07:47,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 02:07:47,319] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f734caa3990>\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1440575\n",
      "\tspeed: 0.1564s/iter; left time: 5562.2778s\n",
      "\titers: 200, epoch: 1 | loss: 0.1299801\n",
      "\tspeed: 0.1117s/iter; left time: 3962.5847s\n",
      "\titers: 300, epoch: 1 | loss: 0.1354454\n",
      "\tspeed: 0.1105s/iter; left time: 3907.9259s\n",
      "\titers: 400, epoch: 1 | loss: 0.1193806\n",
      "\tspeed: 0.1107s/iter; left time: 3904.6076s\n",
      "\titers: 500, epoch: 1 | loss: 0.1538889\n",
      "\tspeed: 0.1118s/iter; left time: 3930.2469s\n",
      "\titers: 600, epoch: 1 | loss: 0.1388368\n",
      "\tspeed: 0.1132s/iter; left time: 3968.5100s\n",
      "\titers: 700, epoch: 1 | loss: 0.0986667\n",
      "\tspeed: 0.1119s/iter; left time: 3913.1953s\n",
      "\titers: 800, epoch: 1 | loss: 0.1530047\n",
      "\tspeed: 0.1110s/iter; left time: 3870.9251s\n",
      "\titers: 900, epoch: 1 | loss: 0.1101738\n",
      "\tspeed: 0.1106s/iter; left time: 3844.3441s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1091491\n",
      "\tspeed: 0.1105s/iter; left time: 3831.7075s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1089654\n",
      "\tspeed: 0.1112s/iter; left time: 3843.4040s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1089270\n",
      "\tspeed: 0.1049s/iter; left time: 3613.4193s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1542389\n",
      "\tspeed: 0.1109s/iter; left time: 3811.1323s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1063095\n",
      "\tspeed: 0.1121s/iter; left time: 3839.2299s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1481955\n",
      "\tspeed: 0.1129s/iter; left time: 3856.7061s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1341569\n",
      "\tspeed: 0.1130s/iter; left time: 3849.2429s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1185152\n",
      "\tspeed: 0.1113s/iter; left time: 3780.9541s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0908604\n",
      "\tspeed: 0.1116s/iter; left time: 3780.1918s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0852027\n",
      "\tspeed: 0.1120s/iter; left time: 3779.5514s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0970242\n",
      "\tspeed: 0.1121s/iter; left time: 3771.7236s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0929401\n",
      "\tspeed: 0.1133s/iter; left time: 3804.1063s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0797752\n",
      "\tspeed: 0.0920s/iter; left time: 3078.5064s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0933316\n",
      "\tspeed: 0.0912s/iter; left time: 3043.5019s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1154153\n",
      "\tspeed: 0.0911s/iter; left time: 3031.5459s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0875616\n",
      "\tspeed: 0.1061s/iter; left time: 3518.3055s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0781510\n",
      "\tspeed: 0.1118s/iter; left time: 3696.5591s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0878930\n",
      "\tspeed: 0.0991s/iter; left time: 3268.0365s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0852987\n",
      "\tspeed: 0.0913s/iter; left time: 3001.2234s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0928331\n",
      "\tspeed: 0.1104s/iter; left time: 3617.3128s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0779679\n",
      "\tspeed: 0.1117s/iter; left time: 3649.3693s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0705386\n",
      "\tspeed: 0.1122s/iter; left time: 3654.3081s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0952635\n",
      "\tspeed: 0.1138s/iter; left time: 3694.4805s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0840503\n",
      "\tspeed: 0.1139s/iter; left time: 3684.7276s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0746481\n",
      "\tspeed: 0.1127s/iter; left time: 3637.0387s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0773572\n",
      "\tspeed: 0.1154s/iter; left time: 3710.1554s\n",
      "Epoch: 1 cost time: 00h:06m:30.02s\n",
      "Epoch: 1 | Train Loss: 0.1092172 Vali Loss: 0.0821468 Test Loss: 0.0903220\n",
      "Validation loss decreased (inf --> 0.082147).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 2 | loss: 0.0720677\n",
      "\tspeed: 1.1443s/iter; left time: 36610.7429s\n",
      "\titers: 200, epoch: 2 | loss: 0.1120776\n",
      "\tspeed: 0.1005s/iter; left time: 3206.0013s\n",
      "\titers: 300, epoch: 2 | loss: 0.0849378\n",
      "\tspeed: 0.1067s/iter; left time: 3393.0777s\n",
      "\titers: 400, epoch: 2 | loss: 0.0827252\n",
      "\tspeed: 0.0955s/iter; left time: 3025.3250s\n",
      "\titers: 500, epoch: 2 | loss: 0.0828264\n",
      "\tspeed: 0.0965s/iter; left time: 3049.7708s\n",
      "\titers: 600, epoch: 2 | loss: 0.0653720\n",
      "\tspeed: 0.1096s/iter; left time: 3452.1596s\n",
      "\titers: 700, epoch: 2 | loss: 0.0999948\n",
      "\tspeed: 0.1071s/iter; left time: 3362.1505s\n",
      "\titers: 800, epoch: 2 | loss: 0.0638315\n",
      "\tspeed: 0.1060s/iter; left time: 3317.2806s\n",
      "\titers: 900, epoch: 2 | loss: 0.0631677\n",
      "\tspeed: 0.1053s/iter; left time: 3285.9780s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0622543\n",
      "\tspeed: 0.1056s/iter; left time: 3284.6763s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0878805\n",
      "\tspeed: 0.1018s/iter; left time: 3156.1915s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0738014\n",
      "\tspeed: 0.1041s/iter; left time: 3214.8003s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0870380\n",
      "\tspeed: 0.1070s/iter; left time: 3296.2095s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0739821\n",
      "\tspeed: 0.1038s/iter; left time: 3185.2227s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0658785\n",
      "\tspeed: 0.1022s/iter; left time: 3126.3635s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0713090\n",
      "\tspeed: 0.0870s/iter; left time: 2653.1261s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0756991\n",
      "\tspeed: 0.1043s/iter; left time: 3171.3060s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0680061\n",
      "\tspeed: 0.1019s/iter; left time: 3087.8064s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0778164\n",
      "\tspeed: 0.1015s/iter; left time: 3063.8876s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0860281\n",
      "\tspeed: 0.1018s/iter; left time: 3064.3624s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0792604\n",
      "\tspeed: 0.1010s/iter; left time: 3030.9466s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0639079\n",
      "\tspeed: 0.1008s/iter; left time: 3011.9821s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0611842\n",
      "\tspeed: 0.1011s/iter; left time: 3013.6160s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0911761\n",
      "\tspeed: 0.1056s/iter; left time: 3136.2659s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0837464\n",
      "\tspeed: 0.1070s/iter; left time: 3167.2947s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0753662\n",
      "\tspeed: 0.1070s/iter; left time: 3156.0905s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0695116\n",
      "\tspeed: 0.1066s/iter; left time: 3133.4190s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0876709\n",
      "\tspeed: 0.1014s/iter; left time: 2971.5360s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0859168\n",
      "\tspeed: 0.1040s/iter; left time: 3037.4123s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0632741\n",
      "\tspeed: 0.1049s/iter; left time: 3051.3375s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0662211\n",
      "\tspeed: 0.0835s/iter; left time: 2420.8845s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0628801\n",
      "\tspeed: 0.1033s/iter; left time: 2984.6067s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0792838\n",
      "\tspeed: 0.1069s/iter; left time: 3077.0895s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0802057\n",
      "\tspeed: 0.1008s/iter; left time: 2891.3333s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0778065\n",
      "\tspeed: 0.1011s/iter; left time: 2890.2946s\n",
      "Epoch: 2 cost time: 00h:06m:03.99s\n",
      "Epoch: 2 | Train Loss: 0.0755411 Vali Loss: 0.0772852 Test Loss: 0.0860607\n",
      "Validation loss decreased (0.082147 --> 0.077285).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 3 | loss: 0.0756057\n",
      "\tspeed: 1.0066s/iter; left time: 28616.7319s\n",
      "\titers: 200, epoch: 3 | loss: 0.0905635\n",
      "\tspeed: 0.1103s/iter; left time: 3125.7538s\n",
      "\titers: 300, epoch: 3 | loss: 0.0720346\n",
      "\tspeed: 0.1094s/iter; left time: 3087.5008s\n",
      "\titers: 400, epoch: 3 | loss: 0.0809741\n",
      "\tspeed: 0.0925s/iter; left time: 2602.0558s\n",
      "\titers: 500, epoch: 3 | loss: 0.0564593\n",
      "\tspeed: 0.1021s/iter; left time: 2861.8181s\n",
      "\titers: 600, epoch: 3 | loss: 0.0814185\n",
      "\tspeed: 0.1066s/iter; left time: 2977.3762s\n",
      "\titers: 700, epoch: 3 | loss: 0.0671464\n",
      "\tspeed: 0.1026s/iter; left time: 2854.7497s\n",
      "\titers: 800, epoch: 3 | loss: 0.0729751\n",
      "\tspeed: 0.0989s/iter; left time: 2742.2934s\n",
      "\titers: 900, epoch: 3 | loss: 0.0777468\n",
      "\tspeed: 0.0857s/iter; left time: 2368.2647s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0633177\n",
      "\tspeed: 0.0856s/iter; left time: 2357.0306s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0806149\n",
      "\tspeed: 0.0858s/iter; left time: 2353.4333s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0724397\n",
      "\tspeed: 0.0874s/iter; left time: 2388.3086s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0745694\n",
      "\tspeed: 0.0962s/iter; left time: 2620.3398s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0905064\n",
      "\tspeed: 0.1044s/iter; left time: 2831.4224s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0517099\n",
      "\tspeed: 0.1022s/iter; left time: 2762.8428s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0697932\n",
      "\tspeed: 0.1043s/iter; left time: 2808.3605s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0658498\n",
      "\tspeed: 0.1041s/iter; left time: 2793.2337s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0793835\n",
      "\tspeed: 0.1052s/iter; left time: 2810.7302s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0733241\n",
      "\tspeed: 0.1075s/iter; left time: 2863.0171s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0833095\n",
      "\tspeed: 0.1007s/iter; left time: 2671.2858s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0751705\n",
      "\tspeed: 0.1053s/iter; left time: 2783.3810s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0607194\n",
      "\tspeed: 0.0893s/iter; left time: 2351.6436s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0892512\n",
      "\tspeed: 0.0863s/iter; left time: 2262.8958s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0748885\n",
      "\tspeed: 0.1071s/iter; left time: 2798.0246s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0673093\n",
      "\tspeed: 0.1075s/iter; left time: 2798.7562s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0889892\n",
      "\tspeed: 0.1014s/iter; left time: 2628.5001s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0834593\n",
      "\tspeed: 0.1016s/iter; left time: 2623.6263s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0762946\n",
      "\tspeed: 0.1037s/iter; left time: 2666.8250s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0863989\n",
      "\tspeed: 0.0864s/iter; left time: 2214.4667s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0592902\n",
      "\tspeed: 0.0947s/iter; left time: 2417.8473s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0716888\n",
      "\tspeed: 0.1078s/iter; left time: 2741.3800s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0727923\n",
      "\tspeed: 0.1088s/iter; left time: 2755.5958s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0686482\n",
      "\tspeed: 0.1091s/iter; left time: 2751.3488s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0795544\n",
      "\tspeed: 0.1074s/iter; left time: 2699.9272s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0822845\n",
      "\tspeed: 0.1069s/iter; left time: 2674.7470s\n",
      "Epoch: 3 cost time: 00h:05m:59.42s\n",
      "Epoch: 3 | Train Loss: 0.0725358 Vali Loss: 0.0751297 Test Loss: 0.0837127\n",
      "Validation loss decreased (0.077285 --> 0.075130).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 4 | loss: 0.0762659\n",
      "\tspeed: 1.0202s/iter; left time: 25364.8510s\n",
      "\titers: 200, epoch: 4 | loss: 0.0644627\n",
      "\tspeed: 0.1012s/iter; left time: 2505.8385s\n",
      "\titers: 300, epoch: 4 | loss: 0.0728707\n",
      "\tspeed: 0.1034s/iter; left time: 2550.4710s\n",
      "\titers: 400, epoch: 4 | loss: 0.0636023\n",
      "\tspeed: 0.1037s/iter; left time: 2547.2959s\n",
      "\titers: 500, epoch: 4 | loss: 0.0680254\n",
      "\tspeed: 0.1078s/iter; left time: 2637.3666s\n",
      "\titers: 600, epoch: 4 | loss: 0.0695731\n",
      "\tspeed: 0.1006s/iter; left time: 2451.2244s\n",
      "\titers: 700, epoch: 4 | loss: 0.0686705\n",
      "\tspeed: 0.0938s/iter; left time: 2275.6373s\n",
      "\titers: 800, epoch: 4 | loss: 0.0767548\n",
      "\tspeed: 0.0911s/iter; left time: 2201.5052s\n",
      "\titers: 900, epoch: 4 | loss: 0.0490004\n",
      "\tspeed: 0.1055s/iter; left time: 2538.7019s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0628900\n",
      "\tspeed: 0.1017s/iter; left time: 2438.1246s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0884633\n",
      "\tspeed: 0.1040s/iter; left time: 2482.2773s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0896056\n",
      "\tspeed: 0.1047s/iter; left time: 2488.0615s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0692411\n",
      "\tspeed: 0.1023s/iter; left time: 2420.5901s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0611375\n",
      "\tspeed: 0.1035s/iter; left time: 2439.3405s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0737480\n",
      "\tspeed: 0.0986s/iter; left time: 2312.8777s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0901903\n",
      "\tspeed: 0.1002s/iter; left time: 2341.6529s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0699836\n",
      "\tspeed: 0.1008s/iter; left time: 2344.0112s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0721211\n",
      "\tspeed: 0.1018s/iter; left time: 2358.0334s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0878837\n",
      "\tspeed: 0.1080s/iter; left time: 2490.1195s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0864065\n",
      "\tspeed: 0.1094s/iter; left time: 2511.2406s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0674135\n",
      "\tspeed: 0.1066s/iter; left time: 2438.1556s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0739628\n",
      "\tspeed: 0.1059s/iter; left time: 2411.2000s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0581623\n",
      "\tspeed: 0.0839s/iter; left time: 1901.3476s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0650130\n",
      "\tspeed: 0.1014s/iter; left time: 2288.8360s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0726438\n",
      "\tspeed: 0.1015s/iter; left time: 2279.9304s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0954770\n",
      "\tspeed: 0.0978s/iter; left time: 2186.4606s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0952775\n",
      "\tspeed: 0.1020s/iter; left time: 2270.0160s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877698\n",
      "\tspeed: 0.1017s/iter; left time: 2253.9375s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0685904\n",
      "\tspeed: 0.1011s/iter; left time: 2231.3363s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0533736\n",
      "\tspeed: 0.1025s/iter; left time: 2251.7943s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0675482\n",
      "\tspeed: 0.1015s/iter; left time: 2218.4196s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0609828\n",
      "\tspeed: 0.1015s/iter; left time: 2207.9479s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0703463\n",
      "\tspeed: 0.0980s/iter; left time: 2123.7699s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0636662\n",
      "\tspeed: 0.1023s/iter; left time: 2204.9183s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0718687\n",
      "\tspeed: 0.1042s/iter; left time: 2236.1337s\n",
      "Epoch: 4 cost time: 00h:06m:03.04s\n",
      "Epoch: 4 | Train Loss: 0.0711842 Vali Loss: 0.0750714 Test Loss: 0.0840366\n",
      "Validation loss decreased (0.075130 --> 0.075071).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 5 | loss: 0.0734329\n",
      "\tspeed: 1.0343s/iter; left time: 22026.5958s\n",
      "\titers: 200, epoch: 5 | loss: 0.0771091\n",
      "\tspeed: 0.1095s/iter; left time: 2320.5177s\n",
      "\titers: 300, epoch: 5 | loss: 0.0714111\n",
      "\tspeed: 0.1070s/iter; left time: 2257.0338s\n",
      "\titers: 400, epoch: 5 | loss: 0.0683239\n",
      "\tspeed: 0.1071s/iter; left time: 2247.8196s\n",
      "\titers: 500, epoch: 5 | loss: 0.0850428\n",
      "\tspeed: 0.1086s/iter; left time: 2268.4453s\n",
      "\titers: 600, epoch: 5 | loss: 0.0713976\n",
      "\tspeed: 0.1097s/iter; left time: 2280.7754s\n",
      "\titers: 700, epoch: 5 | loss: 0.0713975\n",
      "\tspeed: 0.1081s/iter; left time: 2236.9963s\n",
      "\titers: 800, epoch: 5 | loss: 0.0631024\n",
      "\tspeed: 0.1077s/iter; left time: 2217.7825s\n",
      "\titers: 900, epoch: 5 | loss: 0.0584639\n",
      "\tspeed: 0.1068s/iter; left time: 2189.2806s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0775655\n",
      "\tspeed: 0.1023s/iter; left time: 2085.9071s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0632220\n",
      "\tspeed: 0.1051s/iter; left time: 2133.5271s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0836069\n",
      "\tspeed: 0.1055s/iter; left time: 2130.7154s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0737632\n",
      "\tspeed: 0.1038s/iter; left time: 2086.0883s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0728359\n",
      "\tspeed: 0.0958s/iter; left time: 1915.9544s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0541096\n",
      "\tspeed: 0.0835s/iter; left time: 1661.6913s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0603107\n",
      "\tspeed: 0.0836s/iter; left time: 1654.8538s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0888485\n",
      "\tspeed: 0.0883s/iter; left time: 1740.0984s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0655391\n",
      "\tspeed: 0.1030s/iter; left time: 2018.5900s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0643000\n",
      "\tspeed: 0.1075s/iter; left time: 2095.5647s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0545355\n",
      "\tspeed: 0.1084s/iter; left time: 2102.7656s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0691103\n",
      "\tspeed: 0.1078s/iter; left time: 2080.2286s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0727787\n",
      "\tspeed: 0.1078s/iter; left time: 2069.8696s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0637093\n",
      "\tspeed: 0.1070s/iter; left time: 2043.8875s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0946663\n",
      "\tspeed: 0.1069s/iter; left time: 2029.9371s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0769749\n",
      "\tspeed: 0.1072s/iter; left time: 2026.5928s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0556214\n",
      "\tspeed: 0.1065s/iter; left time: 2002.2442s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0592888\n",
      "\tspeed: 0.1080s/iter; left time: 2018.8116s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0776287\n",
      "\tspeed: 0.1026s/iter; left time: 1908.8558s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0624219\n",
      "\tspeed: 0.1025s/iter; left time: 1895.3924s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0679386\n",
      "\tspeed: 0.0990s/iter; left time: 1820.9712s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0714045\n",
      "\tspeed: 0.1004s/iter; left time: 1837.8219s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0672832\n",
      "\tspeed: 0.0980s/iter; left time: 1782.8004s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0646246\n",
      "\tspeed: 0.0972s/iter; left time: 1758.8962s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0775822\n",
      "\tspeed: 0.1003s/iter; left time: 1805.8413s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0754609\n",
      "\tspeed: 0.0958s/iter; left time: 1714.8076s\n",
      "Epoch: 5 cost time: 00h:06m:06.96s\n",
      "Epoch: 5 | Train Loss: 0.0702659 Vali Loss: 0.0733054 Test Loss: 0.0824435\n",
      "Validation loss decreased (0.075071 --> 0.073305).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 6 | loss: 0.0595976\n",
      "\tspeed: 1.0136s/iter; left time: 17971.8989s\n",
      "\titers: 200, epoch: 6 | loss: 0.0787350\n",
      "\tspeed: 0.1030s/iter; left time: 1815.3165s\n",
      "\titers: 300, epoch: 6 | loss: 0.0611242\n",
      "\tspeed: 0.0992s/iter; left time: 1738.6500s\n",
      "\titers: 400, epoch: 6 | loss: 0.0818065\n",
      "\tspeed: 0.1012s/iter; left time: 1764.7002s\n",
      "\titers: 500, epoch: 6 | loss: 0.0810350\n",
      "\tspeed: 0.1012s/iter; left time: 1753.4961s\n",
      "\titers: 600, epoch: 6 | loss: 0.0587074\n",
      "\tspeed: 0.1012s/iter; left time: 1743.0571s\n",
      "\titers: 700, epoch: 6 | loss: 0.0703090\n",
      "\tspeed: 0.1014s/iter; left time: 1737.5313s\n",
      "\titers: 800, epoch: 6 | loss: 0.0853346\n",
      "\tspeed: 0.1021s/iter; left time: 1738.1915s\n",
      "\titers: 900, epoch: 6 | loss: 0.0771472\n",
      "\tspeed: 0.0972s/iter; left time: 1646.5343s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0621617\n",
      "\tspeed: 0.1077s/iter; left time: 1813.1015s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0535752\n",
      "\tspeed: 0.0971s/iter; left time: 1624.8196s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0628508\n",
      "\tspeed: 0.0966s/iter; left time: 1606.6685s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0734800\n",
      "\tspeed: 0.0979s/iter; left time: 1617.5997s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0604587\n",
      "\tspeed: 0.0981s/iter; left time: 1612.6754s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0726916\n",
      "\tspeed: 0.1059s/iter; left time: 1730.2608s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0776222\n",
      "\tspeed: 0.1062s/iter; left time: 1723.5669s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0849937\n",
      "\tspeed: 0.1090s/iter; left time: 1758.2728s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0622263\n",
      "\tspeed: 0.1093s/iter; left time: 1752.2426s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0734927\n",
      "\tspeed: 0.1085s/iter; left time: 1728.1436s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0703206\n",
      "\tspeed: 0.0886s/iter; left time: 1402.3041s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0559907\n",
      "\tspeed: 0.0858s/iter; left time: 1349.7198s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0740385\n",
      "\tspeed: 0.0876s/iter; left time: 1369.5207s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0530195\n",
      "\tspeed: 0.0986s/iter; left time: 1531.0705s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0743242\n",
      "\tspeed: 0.1035s/iter; left time: 1597.4454s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0696135\n",
      "\tspeed: 0.1057s/iter; left time: 1619.7819s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0634545\n",
      "\tspeed: 0.0857s/iter; left time: 1304.7124s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0739783\n",
      "\tspeed: 0.1015s/iter; left time: 1535.6904s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0659225\n",
      "\tspeed: 0.1008s/iter; left time: 1514.4381s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0529330\n",
      "\tspeed: 0.1014s/iter; left time: 1514.4653s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0607648\n",
      "\tspeed: 0.0977s/iter; left time: 1449.1347s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0755738\n",
      "\tspeed: 0.1012s/iter; left time: 1490.5446s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0704174\n",
      "\tspeed: 0.1024s/iter; left time: 1498.5939s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0676856\n",
      "\tspeed: 0.1071s/iter; left time: 1555.6111s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0815410\n",
      "\tspeed: 0.1090s/iter; left time: 1572.9567s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0631517\n",
      "\tspeed: 0.1049s/iter; left time: 1503.1704s\n",
      "Epoch: 6 cost time: 00h:06m:00.42s\n",
      "Epoch: 6 | Train Loss: 0.0695557 Vali Loss: 0.0730720 Test Loss: 0.0821654\n",
      "Validation loss decreased (0.073305 --> 0.073072).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 7 | loss: 0.0669560\n",
      "\tspeed: 1.0079s/iter; left time: 14277.5987s\n",
      "\titers: 200, epoch: 7 | loss: 0.0734779\n",
      "\tspeed: 0.1085s/iter; left time: 1526.3299s\n",
      "\titers: 300, epoch: 7 | loss: 0.0512895\n",
      "\tspeed: 0.1079s/iter; left time: 1506.2456s\n",
      "\titers: 400, epoch: 7 | loss: 0.0642988\n",
      "\tspeed: 0.1034s/iter; left time: 1433.1695s\n",
      "\titers: 500, epoch: 7 | loss: 0.0720399\n",
      "\tspeed: 0.1009s/iter; left time: 1388.3358s\n",
      "\titers: 600, epoch: 7 | loss: 0.0552755\n",
      "\tspeed: 0.1014s/iter; left time: 1385.8126s\n",
      "\titers: 700, epoch: 7 | loss: 0.0749275\n",
      "\tspeed: 0.1084s/iter; left time: 1469.8335s\n",
      "\titers: 800, epoch: 7 | loss: 0.0594688\n",
      "\tspeed: 0.1036s/iter; left time: 1395.0888s\n",
      "\titers: 900, epoch: 7 | loss: 0.0731955\n",
      "\tspeed: 0.1055s/iter; left time: 1409.7643s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0660539\n",
      "\tspeed: 0.1033s/iter; left time: 1370.4773s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0686746\n",
      "\tspeed: 0.1036s/iter; left time: 1363.3828s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0661069\n",
      "\tspeed: 0.0963s/iter; left time: 1258.3479s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0664915\n",
      "\tspeed: 0.0985s/iter; left time: 1277.3296s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0683779\n",
      "\tspeed: 0.1088s/iter; left time: 1399.3438s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0584100\n",
      "\tspeed: 0.1097s/iter; left time: 1400.2374s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0580849\n",
      "\tspeed: 0.1103s/iter; left time: 1396.3528s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0624045\n",
      "\tspeed: 0.1062s/iter; left time: 1334.6355s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0655668\n",
      "\tspeed: 0.1077s/iter; left time: 1342.5522s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0759777\n",
      "\tspeed: 0.1017s/iter; left time: 1258.0215s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0877122\n",
      "\tspeed: 0.1001s/iter; left time: 1228.2696s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0533816\n",
      "\tspeed: 0.1088s/iter; left time: 1323.4795s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0552529\n",
      "\tspeed: 0.1099s/iter; left time: 1325.9781s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0825727\n",
      "\tspeed: 0.1073s/iter; left time: 1283.2558s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0895109\n",
      "\tspeed: 0.1095s/iter; left time: 1298.8025s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0637051\n",
      "\tspeed: 0.1096s/iter; left time: 1289.2543s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0541311\n",
      "\tspeed: 0.1091s/iter; left time: 1272.5105s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0766268\n",
      "\tspeed: 0.1084s/iter; left time: 1253.8492s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0613706\n",
      "\tspeed: 0.1067s/iter; left time: 1223.3179s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0596100\n",
      "\tspeed: 0.1058s/iter; left time: 1202.2584s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0525368\n",
      "\tspeed: 0.1028s/iter; left time: 1158.4118s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0703490\n",
      "\tspeed: 0.1026s/iter; left time: 1145.9779s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0707452\n",
      "\tspeed: 0.1021s/iter; left time: 1129.6262s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0723773\n",
      "\tspeed: 0.1099s/iter; left time: 1205.5060s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0639014\n",
      "\tspeed: 0.1091s/iter; left time: 1185.5665s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0704285\n",
      "\tspeed: 0.1102s/iter; left time: 1186.2372s\n",
      "Epoch: 7 cost time: 00h:06m:18.03s\n",
      "Epoch: 7 | Train Loss: 0.0689114 Vali Loss: 0.0735753 Test Loss: 0.0827112\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 8 | loss: 0.0563023\n",
      "\tspeed: 1.0070s/iter; left time: 10673.6317s\n",
      "\titers: 200, epoch: 8 | loss: 0.0657397\n",
      "\tspeed: 0.1096s/iter; left time: 1150.6756s\n",
      "\titers: 300, epoch: 8 | loss: 0.0645681\n",
      "\tspeed: 0.1095s/iter; left time: 1138.9919s\n",
      "\titers: 400, epoch: 8 | loss: 0.0765049\n",
      "\tspeed: 0.1046s/iter; left time: 1077.1493s\n",
      "\titers: 500, epoch: 8 | loss: 0.0836551\n",
      "\tspeed: 0.1090s/iter; left time: 1112.1008s\n",
      "\titers: 600, epoch: 8 | loss: 0.0688213\n",
      "\tspeed: 0.1095s/iter; left time: 1105.7303s\n",
      "\titers: 700, epoch: 8 | loss: 0.0578617\n",
      "\tspeed: 0.1096s/iter; left time: 1096.1797s\n",
      "\titers: 800, epoch: 8 | loss: 0.0770525\n",
      "\tspeed: 0.1098s/iter; left time: 1087.2904s\n",
      "\titers: 900, epoch: 8 | loss: 0.0914728\n",
      "\tspeed: 0.1102s/iter; left time: 1079.6141s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0698659\n",
      "\tspeed: 0.1100s/iter; left time: 1067.1698s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0682746\n",
      "\tspeed: 0.1107s/iter; left time: 1062.8348s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0841383\n",
      "\tspeed: 0.1089s/iter; left time: 1034.3435s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0946120\n",
      "\tspeed: 0.1036s/iter; left time: 973.4134s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0625262\n",
      "\tspeed: 0.1096s/iter; left time: 1018.8756s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0607226\n",
      "\tspeed: 0.1081s/iter; left time: 994.1267s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0746914\n",
      "\tspeed: 0.1109s/iter; left time: 1009.3840s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0729473\n",
      "\tspeed: 0.1104s/iter; left time: 993.7550s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0622026\n",
      "\tspeed: 0.1070s/iter; left time: 951.9821s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1025201\n",
      "\tspeed: 0.1058s/iter; left time: 930.6765s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0526033\n",
      "\tspeed: 0.1061s/iter; left time: 923.0345s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0618048\n",
      "\tspeed: 0.1074s/iter; left time: 923.2574s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0660734\n",
      "\tspeed: 0.1089s/iter; left time: 925.4242s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0746935\n",
      "\tspeed: 0.1076s/iter; left time: 903.5153s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0763705\n",
      "\tspeed: 0.1079s/iter; left time: 895.1121s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0762459\n",
      "\tspeed: 0.1072s/iter; left time: 878.8657s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0688709\n",
      "\tspeed: 0.1037s/iter; left time: 839.9614s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0732419\n",
      "\tspeed: 0.1109s/iter; left time: 886.7421s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0577185\n",
      "\tspeed: 0.1112s/iter; left time: 878.4821s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0800271\n",
      "\tspeed: 0.1089s/iter; left time: 849.3381s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0612930\n",
      "\tspeed: 0.1059s/iter; left time: 815.5166s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0570172\n",
      "\tspeed: 0.1094s/iter; left time: 831.3189s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0578851\n",
      "\tspeed: 0.1098s/iter; left time: 823.6814s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0545044\n",
      "\tspeed: 0.1106s/iter; left time: 818.1400s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0698128\n",
      "\tspeed: 0.1099s/iter; left time: 802.0030s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0574988\n",
      "\tspeed: 0.1097s/iter; left time: 790.0378s\n",
      "Epoch: 8 cost time: 00h:06m:27.75s\n",
      "Epoch: 8 | Train Loss: 0.0684275 Vali Loss: 0.0742783 Test Loss: 0.0835485\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 9 | loss: 0.0675568\n",
      "\tspeed: 1.0066s/iter; left time: 7079.1278s\n",
      "\titers: 200, epoch: 9 | loss: 0.0651613\n",
      "\tspeed: 0.1109s/iter; left time: 769.0460s\n",
      "\titers: 300, epoch: 9 | loss: 0.0539076\n",
      "\tspeed: 0.1103s/iter; left time: 753.5748s\n",
      "\titers: 400, epoch: 9 | loss: 0.0818302\n",
      "\tspeed: 0.1071s/iter; left time: 721.2492s\n",
      "\titers: 500, epoch: 9 | loss: 0.0801425\n",
      "\tspeed: 0.1069s/iter; left time: 708.8507s\n",
      "\titers: 600, epoch: 9 | loss: 0.0488272\n",
      "\tspeed: 0.1063s/iter; left time: 694.6422s\n",
      "\titers: 700, epoch: 9 | loss: 0.0668959\n",
      "\tspeed: 0.1061s/iter; left time: 682.6070s\n",
      "\titers: 800, epoch: 9 | loss: 0.0686616\n",
      "\tspeed: 0.1064s/iter; left time: 673.7968s\n",
      "\titers: 900, epoch: 9 | loss: 0.0655853\n",
      "\tspeed: 0.1048s/iter; left time: 653.4292s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0672343\n",
      "\tspeed: 0.1073s/iter; left time: 658.1628s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0556217\n",
      "\tspeed: 0.1070s/iter; left time: 645.3589s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0660226\n",
      "\tspeed: 0.1098s/iter; left time: 651.6077s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0594931\n",
      "\tspeed: 0.1099s/iter; left time: 641.2956s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0579290\n",
      "\tspeed: 0.1109s/iter; left time: 636.0000s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0606761\n",
      "\tspeed: 0.1112s/iter; left time: 626.4839s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0589912\n",
      "\tspeed: 0.1096s/iter; left time: 606.5320s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0680620\n",
      "\tspeed: 0.1111s/iter; left time: 603.6467s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0745860\n",
      "\tspeed: 0.1106s/iter; left time: 589.6628s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0524615\n",
      "\tspeed: 0.1057s/iter; left time: 553.2018s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0780820\n",
      "\tspeed: 0.1055s/iter; left time: 541.5679s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0632932\n",
      "\tspeed: 0.1073s/iter; left time: 540.2419s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0586291\n",
      "\tspeed: 0.1110s/iter; left time: 547.3800s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0707322\n",
      "\tspeed: 0.1099s/iter; left time: 531.2579s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0634186\n",
      "\tspeed: 0.1070s/iter; left time: 506.4072s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0595348\n",
      "\tspeed: 0.1074s/iter; left time: 497.6064s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0578244\n",
      "\tspeed: 0.1056s/iter; left time: 478.7451s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0661644\n",
      "\tspeed: 0.1067s/iter; left time: 473.1409s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0896046\n",
      "\tspeed: 0.1113s/iter; left time: 482.4414s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 200, in <module>\n",
      "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/data_loader.py\", line 462, in __iter__\n",
      "    next_batch = next(dataloader_iter)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1370, in _process_data\n",
      "    self._try_put_index()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1363, in _try_put_index\n",
      "    self._index_queues[worker_queue_idx].put((self._send_idx, index))\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/multiprocessing/queues.py\", line 96, in put\n",
      "    self._notempty.notify()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/threading.py\", line 375, in notify\n",
      "    waiter.release()\n",
      "KeyboardInterrupt\n",
      "Total time: 233.94862820307415 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.0001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrease complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=16\n",
    "d_ff=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 03:14:11,347] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 03:14:12,513] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 03:14:12,513] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 03:14:12,513] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 03:14:12,618] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 03:14:12,619] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 03:14:13,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 03:14:13,289] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 03:14:13,289] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 03:14:13,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 03:14:13,291] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 03:14:13,291] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 03:14:13,621] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 03:14:13,622] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,622] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,744] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 03:14:13,745] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,745] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,745] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 03:14:13,863] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 03:14:13,864] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,864] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 03:14:13,865] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51c44ff910>\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1424896\n",
      "\tspeed: 0.1603s/iter; left time: 5701.1658s\n",
      "\titers: 200, epoch: 1 | loss: 0.1216655\n",
      "\tspeed: 0.1146s/iter; left time: 4064.7797s\n",
      "\titers: 300, epoch: 1 | loss: 0.1135953\n",
      "\tspeed: 0.1123s/iter; left time: 3969.4581s\n",
      "\titers: 400, epoch: 1 | loss: 0.0812779\n",
      "\tspeed: 0.1113s/iter; left time: 3923.4805s\n",
      "\titers: 500, epoch: 1 | loss: 0.0967521\n",
      "\tspeed: 0.1124s/iter; left time: 3950.9017s\n",
      "\titers: 600, epoch: 1 | loss: 0.1014863\n",
      "\tspeed: 0.1159s/iter; left time: 4062.8865s\n",
      "\titers: 700, epoch: 1 | loss: 0.0915894\n",
      "\tspeed: 0.1163s/iter; left time: 4067.3004s\n",
      "\titers: 800, epoch: 1 | loss: 0.0760651\n",
      "\tspeed: 0.1163s/iter; left time: 4055.9484s\n",
      "\titers: 900, epoch: 1 | loss: 0.0788776\n",
      "\tspeed: 0.1163s/iter; left time: 4042.5046s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0763655\n",
      "\tspeed: 0.1156s/iter; left time: 4007.0653s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0659469\n",
      "\tspeed: 0.1137s/iter; left time: 3929.2533s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0801397\n",
      "\tspeed: 0.1112s/iter; left time: 3830.5278s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0902781\n",
      "\tspeed: 0.1159s/iter; left time: 3982.1460s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0845647\n",
      "\tspeed: 0.1169s/iter; left time: 4006.0340s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0701626\n",
      "\tspeed: 0.1155s/iter; left time: 3946.4431s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0684285\n",
      "\tspeed: 0.1132s/iter; left time: 3854.3305s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0824512\n",
      "\tspeed: 0.1119s/iter; left time: 3800.7159s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0605585\n",
      "\tspeed: 0.1114s/iter; left time: 3772.5395s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0753151\n",
      "\tspeed: 0.1123s/iter; left time: 3792.8493s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0716163\n",
      "\tspeed: 0.1123s/iter; left time: 3778.8760s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0605470\n",
      "\tspeed: 0.1155s/iter; left time: 3876.3892s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0658180\n",
      "\tspeed: 0.1130s/iter; left time: 3782.5097s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0823635\n",
      "\tspeed: 0.1129s/iter; left time: 3767.1556s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0742289\n",
      "\tspeed: 0.1109s/iter; left time: 3689.3527s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0678603\n",
      "\tspeed: 0.1109s/iter; left time: 3678.4523s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0672556\n",
      "\tspeed: 0.1110s/iter; left time: 3669.3329s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0807025\n",
      "\tspeed: 0.0998s/iter; left time: 3289.3044s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0851708\n",
      "\tspeed: 0.0938s/iter; left time: 3082.1063s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0664944\n",
      "\tspeed: 0.1111s/iter; left time: 3640.1699s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0880232\n",
      "\tspeed: 0.1120s/iter; left time: 3656.4382s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0832897\n",
      "\tspeed: 0.1124s/iter; left time: 3658.7083s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0608761\n",
      "\tspeed: 0.1109s/iter; left time: 3600.5166s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0698125\n",
      "\tspeed: 0.1108s/iter; left time: 3584.4131s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0667007\n",
      "\tspeed: 0.1113s/iter; left time: 3591.6992s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0908326\n",
      "\tspeed: 0.1113s/iter; left time: 3579.9216s\n",
      "Epoch: 1 cost time: 00h:06m:41.55s\n",
      "Epoch: 1 | Train Loss: 0.0822845 Vali Loss: 0.0750525 Test Loss: 0.0838669\n",
      "Validation loss decreased (inf --> 0.075053).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0939135\n",
      "\tspeed: 1.1464s/iter; left time: 36678.9163s\n",
      "\titers: 200, epoch: 2 | loss: 0.0608462\n",
      "\tspeed: 0.1069s/iter; left time: 3410.7039s\n",
      "\titers: 300, epoch: 2 | loss: 0.0878162\n",
      "\tspeed: 0.1081s/iter; left time: 3437.1474s\n",
      "\titers: 400, epoch: 2 | loss: 0.0681369\n",
      "\tspeed: 0.1067s/iter; left time: 3380.4697s\n",
      "\titers: 500, epoch: 2 | loss: 0.0737712\n",
      "\tspeed: 0.1011s/iter; left time: 3195.6630s\n",
      "\titers: 600, epoch: 2 | loss: 0.0719987\n",
      "\tspeed: 0.1020s/iter; left time: 3210.9693s\n",
      "\titers: 700, epoch: 2 | loss: 0.0598605\n",
      "\tspeed: 0.1061s/iter; left time: 3329.8293s\n",
      "\titers: 800, epoch: 2 | loss: 0.0766290\n",
      "\tspeed: 0.1067s/iter; left time: 3338.8028s\n",
      "\titers: 900, epoch: 2 | loss: 0.0968150\n",
      "\tspeed: 0.1036s/iter; left time: 3233.0538s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0777338\n",
      "\tspeed: 0.1010s/iter; left time: 3140.0640s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0742226\n",
      "\tspeed: 0.1009s/iter; left time: 3128.9297s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0758907\n",
      "\tspeed: 0.1059s/iter; left time: 3273.1533s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0546048\n",
      "\tspeed: 0.1088s/iter; left time: 3351.3600s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0569216\n",
      "\tspeed: 0.1074s/iter; left time: 3295.8490s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0623859\n",
      "\tspeed: 0.1076s/iter; left time: 3293.1682s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0900649\n",
      "\tspeed: 0.1027s/iter; left time: 3131.5982s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0843206\n",
      "\tspeed: 0.1010s/iter; left time: 3070.1040s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0545513\n",
      "\tspeed: 0.1027s/iter; left time: 3111.2605s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0740867\n",
      "\tspeed: 0.1054s/iter; left time: 3181.2844s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0662565\n",
      "\tspeed: 0.1007s/iter; left time: 3031.9609s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0770249\n",
      "\tspeed: 0.1042s/iter; left time: 3124.9109s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0680723\n",
      "\tspeed: 0.1039s/iter; left time: 3107.0215s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0657464\n",
      "\tspeed: 0.1016s/iter; left time: 3026.7458s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731734\n",
      "\tspeed: 0.1009s/iter; left time: 2995.8395s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0560416\n",
      "\tspeed: 0.1009s/iter; left time: 2985.1869s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0750969\n",
      "\tspeed: 0.1020s/iter; left time: 3009.5806s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0850314\n",
      "\tspeed: 0.1014s/iter; left time: 2981.7625s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0651263\n",
      "\tspeed: 0.0987s/iter; left time: 2892.1512s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0757619\n",
      "\tspeed: 0.1006s/iter; left time: 2936.1241s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0654464\n",
      "\tspeed: 0.1019s/iter; left time: 2963.4939s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0730120\n",
      "\tspeed: 0.1021s/iter; left time: 2960.6720s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0728501\n",
      "\tspeed: 0.1000s/iter; left time: 2888.3388s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0673412\n",
      "\tspeed: 0.0987s/iter; left time: 2841.7319s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0700444\n",
      "\tspeed: 0.1029s/iter; left time: 2951.3566s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0758945\n",
      "\tspeed: 0.1054s/iter; left time: 3014.3754s\n",
      "Epoch: 2 cost time: 00h:06m:09.13s\n",
      "Epoch: 2 | Train Loss: 0.0703329 Vali Loss: 0.0737976 Test Loss: 0.0828000\n",
      "Validation loss decreased (0.075053 --> 0.073798).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0808824\n",
      "\tspeed: 1.0209s/iter; left time: 29022.5604s\n",
      "\titers: 200, epoch: 3 | loss: 0.0815184\n",
      "\tspeed: 0.1076s/iter; left time: 3047.8401s\n",
      "\titers: 300, epoch: 3 | loss: 0.0870420\n",
      "\tspeed: 0.1061s/iter; left time: 2993.8540s\n",
      "\titers: 400, epoch: 3 | loss: 0.0793327\n",
      "\tspeed: 0.1017s/iter; left time: 2861.8225s\n",
      "\titers: 500, epoch: 3 | loss: 0.0821626\n",
      "\tspeed: 0.1014s/iter; left time: 2842.0881s\n",
      "\titers: 600, epoch: 3 | loss: 0.0724274\n",
      "\tspeed: 0.1076s/iter; left time: 3004.3496s\n",
      "\titers: 700, epoch: 3 | loss: 0.0620296\n",
      "\tspeed: 0.1093s/iter; left time: 3042.4611s\n",
      "\titers: 800, epoch: 3 | loss: 0.0657922\n",
      "\tspeed: 0.1088s/iter; left time: 3015.5622s\n",
      "\titers: 900, epoch: 3 | loss: 0.0808211\n",
      "\tspeed: 0.1013s/iter; left time: 2797.9522s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0673185\n",
      "\tspeed: 0.1050s/iter; left time: 2889.8331s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0721133\n",
      "\tspeed: 0.0971s/iter; left time: 2662.6821s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0694294\n",
      "\tspeed: 0.0956s/iter; left time: 2613.9948s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0946639\n",
      "\tspeed: 0.0917s/iter; left time: 2496.9612s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0837892\n",
      "\tspeed: 0.1073s/iter; left time: 2910.8547s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0588859\n",
      "\tspeed: 0.1095s/iter; left time: 2960.4854s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0794964\n",
      "\tspeed: 0.1087s/iter; left time: 2927.0299s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0612484\n",
      "\tspeed: 0.1086s/iter; left time: 2912.4573s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0708212\n",
      "\tspeed: 0.1103s/iter; left time: 2949.4036s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0597865\n",
      "\tspeed: 0.1060s/iter; left time: 2822.2823s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0816800\n",
      "\tspeed: 0.1088s/iter; left time: 2887.3360s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0771033\n",
      "\tspeed: 0.1005s/iter; left time: 2656.8888s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0774621\n",
      "\tspeed: 0.1059s/iter; left time: 2788.2415s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0717404\n",
      "\tspeed: 0.1079s/iter; left time: 2828.9438s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0534040\n",
      "\tspeed: 0.1051s/iter; left time: 2744.9886s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0659776\n",
      "\tspeed: 0.0980s/iter; left time: 2551.6795s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0575099\n",
      "\tspeed: 0.1023s/iter; left time: 2651.9897s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0676808\n",
      "\tspeed: 0.1006s/iter; left time: 2597.5943s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0968859\n",
      "\tspeed: 0.1015s/iter; left time: 2610.8087s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0860504\n",
      "\tspeed: 0.1094s/iter; left time: 2804.9158s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0572925\n",
      "\tspeed: 0.1084s/iter; left time: 2768.0889s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0730199\n",
      "\tspeed: 0.1110s/iter; left time: 2823.7652s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0750952\n",
      "\tspeed: 0.1035s/iter; left time: 2622.4682s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0757238\n",
      "\tspeed: 0.1101s/iter; left time: 2777.5383s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0755035\n",
      "\tspeed: 0.1091s/iter; left time: 2742.7412s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0583412\n",
      "\tspeed: 0.0997s/iter; left time: 2494.1664s\n",
      "Epoch: 3 cost time: 00h:06m:14.03s\n",
      "Epoch: 3 | Train Loss: 0.0681861 Vali Loss: 0.0728567 Test Loss: 0.0819308\n",
      "Validation loss decreased (0.073798 --> 0.072857).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0610442\n",
      "\tspeed: 1.0287s/iter; left time: 25576.5615s\n",
      "\titers: 200, epoch: 4 | loss: 0.0598010\n",
      "\tspeed: 0.1076s/iter; left time: 2665.0954s\n",
      "\titers: 300, epoch: 4 | loss: 0.0655513\n",
      "\tspeed: 0.1038s/iter; left time: 2559.1799s\n",
      "\titers: 400, epoch: 4 | loss: 0.0600016\n",
      "\tspeed: 0.0991s/iter; left time: 2434.3049s\n",
      "\titers: 500, epoch: 4 | loss: 0.0873438\n",
      "\tspeed: 0.1060s/iter; left time: 2593.5896s\n",
      "\titers: 600, epoch: 4 | loss: 0.0668786\n",
      "\tspeed: 0.1028s/iter; left time: 2504.2986s\n",
      "\titers: 700, epoch: 4 | loss: 0.0627359\n",
      "\tspeed: 0.1012s/iter; left time: 2454.4416s\n",
      "\titers: 800, epoch: 4 | loss: 0.0704189\n",
      "\tspeed: 0.1008s/iter; left time: 2434.4562s\n",
      "\titers: 900, epoch: 4 | loss: 0.0536439\n",
      "\tspeed: 0.1021s/iter; left time: 2456.8231s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0776928\n",
      "\tspeed: 0.1002s/iter; left time: 2400.5758s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0541186\n",
      "\tspeed: 0.0956s/iter; left time: 2281.1087s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0635243\n",
      "\tspeed: 0.0981s/iter; left time: 2331.9075s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0505764\n",
      "\tspeed: 0.1100s/iter; left time: 2604.0829s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0618619\n",
      "\tspeed: 0.1091s/iter; left time: 2570.6092s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0737609\n",
      "\tspeed: 0.1099s/iter; left time: 2578.4430s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0633889\n",
      "\tspeed: 0.1085s/iter; left time: 2535.8943s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0555208\n",
      "\tspeed: 0.1091s/iter; left time: 2537.5645s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0555905\n",
      "\tspeed: 0.1051s/iter; left time: 2434.6118s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0751671\n",
      "\tspeed: 0.1088s/iter; left time: 2510.0542s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0532623\n",
      "\tspeed: 0.1075s/iter; left time: 2469.0551s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0908642\n",
      "\tspeed: 0.1027s/iter; left time: 2348.0870s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0650437\n",
      "\tspeed: 0.1049s/iter; left time: 2386.9413s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0775387\n",
      "\tspeed: 0.1044s/iter; left time: 2365.1472s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0678262\n",
      "\tspeed: 0.1003s/iter; left time: 2263.8003s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0665249\n",
      "\tspeed: 0.1041s/iter; left time: 2339.1909s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0753792\n",
      "\tspeed: 0.0934s/iter; left time: 2088.6770s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0603315\n",
      "\tspeed: 0.0981s/iter; left time: 2185.0102s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0798816\n",
      "\tspeed: 0.1054s/iter; left time: 2334.8924s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 256, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1976, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2051, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Total time: 262.93945128122965 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
