{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.11s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning.\n",
      "d_llm 4096\n",
      "[2024-05-12 01:06:51,313] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-12 01:06:52,122] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-12 01:06:52,122] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-12 01:06:52,123] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-12 01:06:52,965] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-12 01:06:52,965] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-12 01:07:07,310] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-12 01:07:07,312] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-12 01:07:07,312] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-12 01:07:07,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-12 01:07:07,314] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-12 01:07:07,314] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-12 01:07:07,314] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-12 01:07:07,314] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-12 01:07:07,314] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-12 01:07:07,314] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-12 01:07:07,871] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-12 01:07:07,871] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-12 01:07:07,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 85.51 GB, percent = 11.3%\n",
      "[2024-05-12 01:07:07,980] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-12 01:07:07,981] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-12 01:07:07,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 85.5 GB, percent = 11.3%\n",
      "[2024-05-12 01:07:07,981] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-12 01:07:08,086] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-12 01:07:08,086] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-12 01:07:08,086] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 85.5 GB, percent = 11.3%\n",
      "[2024-05-12 01:07:08,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-12 01:07:08,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-12 01:07:08,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-12 01:07:08,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f28109754d0>\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-12 01:07:08,088] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  3\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-12 01:07:08,089] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-12 01:07:08,090] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-12 01:07:08,090] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-12 01:07:08,090] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 3, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "29633it [2:39:19,  3.10it/s]\n",
      "Epoch: 1 cost time: 9559.54402923584\n",
      "6409it [16:35,  6.44it/s]\n",
      "6385it [16:45,  6.35it/s]\n",
      "Epoch: 1 | Train Loss: 0.4256446 Vali Loss: 0.4992706 Test Loss: 0.6191180 MAE Loss: 0.4996524\n",
      "lr = 0.0008535165\n",
      "Updating learning rate to 0.0008535164907166175\n",
      "29633it [2:36:38,  3.15it/s]\n",
      "Epoch: 2 cost time: 9398.202253103256\n",
      "6409it [16:39,  6.41it/s]\n",
      "6385it [16:32,  6.44it/s]\n",
      "Epoch: 2 | Train Loss: 0.3807057 Vali Loss: 0.4826660 Test Loss: 0.6057713 MAE Loss: 0.4987586\n",
      "Updating learning rate to 0.00042675824535830877\n",
      "29633it [2:38:30,  3.12it/s]\n",
      "Epoch: 3 cost time: 9510.922381401062\n",
      "6409it [26:35,  4.02it/s]\n",
      "6385it [27:13,  3.91it/s]\n",
      "Epoch: 3 | Train Loss: 0.3562837 Vali Loss: 0.4709076 Test Loss: 0.5996581 MAE Loss: 0.4932190\n",
      "Updating learning rate to 0.00021337912267915438\n",
      "29633it [3:34:16,  2.30it/s]\n",
      "Epoch: 4 cost time: 12856.44803762436\n",
      "6409it [32:02,  3.33it/s]\n",
      "6385it [34:00,  3.13it/s]\n",
      "Epoch: 4 | Train Loss: 0.3576360 Vali Loss: 0.4714439 Test Loss: 0.6033064 MAE Loss: 0.4958152\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00010668956133957719\n",
      "689it [06:37,  1.62it/s]^C\n",
      "689it [06:37,  1.73it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_copy_copy.py\", line 176, in <module>\n",
      "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader)):\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/tqdm/std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/data_loader.py\", line 461, in __iter__\n",
      "    current_batch = send_to_device(current_batch, self.device)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 177, in send_to_device\n",
      "    return honor_type(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
      "    return type(obj)(generator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 178, in <genexpr>\n",
      "    tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 155, in send_to_device\n",
      "    return tensor.to(device, non_blocking=non_blocking)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Total time: 889.5978506286939 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=3\n",
    "d_model=16\n",
    "d_ff=64\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.09s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-12 15:59:39,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-12 15:59:40,365] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-12 15:59:40,365] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-12 15:59:40,365] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-12 15:59:41,229] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-12 15:59:41,229] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-12 15:59:52,969] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-12 15:59:52,971] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-12 15:59:52,971] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-12 15:59:52,973] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-12 15:59:52,973] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-12 15:59:52,973] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-12 15:59:52,973] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-12 15:59:52,973] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-12 15:59:52,973] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-12 15:59:52,973] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-12 15:59:53,247] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-12 15:59:53,248] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.68 GB         CA 12.68 GB         Max_CA 13 GB \n",
      "[2024-05-12 15:59:53,248] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.18 GB, percent = 19.5%\n",
      "[2024-05-12 15:59:53,359] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-12 15:59:53,360] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.76 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-12 15:59:53,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.18 GB, percent = 19.5%\n",
      "[2024-05-12 15:59:53,360] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-12 15:59:53,468] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-12 15:59:53,469] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.59 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-12 15:59:53,469] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.18 GB, percent = 19.5%\n",
      "[2024-05-12 15:59:53,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-12 15:59:53,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-12 15:59:53,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-12 15:59:53,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-12 15:59:53,470] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc88c2e7ad0>\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-12 15:59:53,471] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   train_batch_size ............. 48\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  6\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-12 15:59:53,472] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 48, \n",
      "    \"train_micro_batch_size_per_gpu\": 6, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "89it [00:27,  3.34it/s]^C\n",
      "89it [00:27,  3.19it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_copy_copy.py\", line 208, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 2002, in backward\n",
      "    self.allreduce_gradients()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1918, in allreduce_gradients\n",
      "    self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 859, in overlapping_partition_gradients_reduce_epilogue\n",
      "    self.independent_gradient_partition_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 760, in independent_gradient_partition_epilogue\n",
      "    get_accelerator().synchronize()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/accelerator/cuda_accelerator.py\", line 77, in synchronize\n",
      "    return torch.cuda.synchronize(device_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 801, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Total time: 1.5839220484097798 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=6\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.10s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-12 16:01:38,074] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-12 16:01:38,855] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-12 16:01:38,855] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-12 16:01:38,855] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-12 16:01:39,713] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-12 16:01:39,714] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-12 16:01:50,902] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-12 16:01:50,904] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-12 16:01:50,904] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-12 16:01:50,905] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-12 16:01:50,905] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-12 16:01:50,905] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-12 16:01:50,905] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-12 16:01:50,905] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-12 16:01:50,905] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-12 16:01:50,905] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-12 16:01:51,457] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-12 16:01:51,458] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.68 GB         CA 12.68 GB         Max_CA 13 GB \n",
      "[2024-05-12 16:01:51,458] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.13 GB, percent = 19.5%\n",
      "[2024-05-12 16:01:51,571] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-12 16:01:51,572] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.76 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-12 16:01:51,572] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.12 GB, percent = 19.5%\n",
      "[2024-05-12 16:01:51,572] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-12 16:01:51,682] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-12 16:01:51,682] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.59 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-12 16:01:51,682] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 147.12 GB, percent = 19.5%\n",
      "[2024-05-12 16:01:51,683] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-12 16:01:51,683] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-12 16:01:51,683] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-12 16:01:51,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9ec0b65d10>\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-12 16:01:51,684] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-12 16:01:51,685] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [01:52,  1.13s/it]\titers: 100, epoch: 1 | loss: 0.7942106\n",
      "\tspeed: 1.2716s/iter; left time: 94074.7166s\n",
      "199it [03:46,  1.14s/it]\titers: 200, epoch: 1 | loss: 0.8900085\n",
      "\tspeed: 1.1425s/iter; left time: 84410.1441s\n",
      "299it [05:41,  1.14s/it]\titers: 300, epoch: 1 | loss: 0.5105940\n",
      "\tspeed: 1.1452s/iter; left time: 84492.8655s\n",
      "399it [07:35,  1.15s/it]\titers: 400, epoch: 1 | loss: 0.3975725\n",
      "\tspeed: 1.1419s/iter; left time: 84137.9366s\n",
      "499it [09:29,  1.15s/it]\titers: 500, epoch: 1 | loss: 0.5365859\n",
      "\tspeed: 1.1430s/iter; left time: 84101.3643s\n",
      "599it [11:23,  1.14s/it]\titers: 600, epoch: 1 | loss: 0.3627321\n",
      "\tspeed: 1.1415s/iter; left time: 83881.1104s\n",
      "699it [13:18,  1.15s/it]\titers: 700, epoch: 1 | loss: 0.5468324\n",
      "\tspeed: 1.1430s/iter; left time: 83874.3112s\n",
      "799it [15:12,  1.14s/it]\titers: 800, epoch: 1 | loss: 0.4090172\n",
      "\tspeed: 1.1420s/iter; left time: 83685.0763s\n",
      "899it [17:06,  1.14s/it]\titers: 900, epoch: 1 | loss: 0.5000148\n",
      "\tspeed: 1.1424s/iter; left time: 83603.5787s\n",
      "999it [19:00,  1.14s/it]\titers: 1000, epoch: 1 | loss: 0.4034886\n",
      "\tspeed: 1.1393s/iter; left time: 83259.4837s\n",
      "1099it [20:54,  1.14s/it]\titers: 1100, epoch: 1 | loss: 0.6325399\n",
      "\tspeed: 1.1391s/iter; left time: 83129.5486s\n",
      "1199it [22:48,  1.14s/it]\titers: 1200, epoch: 1 | loss: 0.3048071\n",
      "\tspeed: 1.1392s/iter; left time: 83028.1633s\n",
      "1299it [24:42,  1.15s/it]\titers: 1300, epoch: 1 | loss: 0.2897484\n",
      "\tspeed: 1.1388s/iter; left time: 82881.6511s\n",
      "1399it [26:36,  1.14s/it]\titers: 1400, epoch: 1 | loss: 0.4528076\n",
      "\tspeed: 1.1401s/iter; left time: 82865.8731s\n",
      "1499it [28:30,  1.13s/it]\titers: 1500, epoch: 1 | loss: 0.2937332\n",
      "\tspeed: 1.1384s/iter; left time: 82625.9335s\n",
      "1599it [30:24,  1.14s/it]\titers: 1600, epoch: 1 | loss: 0.5204234\n",
      "\tspeed: 1.1412s/iter; left time: 82716.9483s\n",
      "1699it [32:18,  1.14s/it]\titers: 1700, epoch: 1 | loss: 0.4439207\n",
      "\tspeed: 1.1406s/iter; left time: 82558.9691s\n",
      "1799it [34:12,  1.14s/it]\titers: 1800, epoch: 1 | loss: 0.4923120\n",
      "\tspeed: 1.1406s/iter; left time: 82442.6041s\n",
      "1899it [36:06,  1.14s/it]\titers: 1900, epoch: 1 | loss: 0.4759522\n",
      "\tspeed: 1.1411s/iter; left time: 82366.0696s\n",
      "1999it [38:00,  1.15s/it]\titers: 2000, epoch: 1 | loss: 0.6245456\n",
      "\tspeed: 1.1443s/iter; left time: 82480.8905s\n",
      "2099it [39:55,  1.14s/it]\titers: 2100, epoch: 1 | loss: 0.3546993\n",
      "\tspeed: 1.1445s/iter; left time: 82379.0791s\n",
      "2199it [41:49,  1.14s/it]\titers: 2200, epoch: 1 | loss: 0.4053877\n",
      "\tspeed: 1.1444s/iter; left time: 82258.2979s\n",
      "2299it [43:44,  1.14s/it]\titers: 2300, epoch: 1 | loss: 0.2722742\n",
      "\tspeed: 1.1427s/iter; left time: 82026.2554s\n",
      "2399it [45:38,  1.14s/it]\titers: 2400, epoch: 1 | loss: 0.3426402\n",
      "\tspeed: 1.1408s/iter; left time: 81773.2372s\n",
      "2499it [47:32,  1.14s/it]\titers: 2500, epoch: 1 | loss: 0.4169562\n",
      "\tspeed: 1.1394s/iter; left time: 81559.4244s\n",
      "2599it [49:26,  1.14s/it]\titers: 2600, epoch: 1 | loss: 0.2953346\n",
      "\tspeed: 1.1426s/iter; left time: 81671.1657s\n",
      "2699it [51:20,  1.14s/it]\titers: 2700, epoch: 1 | loss: 0.2802501\n",
      "\tspeed: 1.1426s/iter; left time: 81561.9357s\n",
      "2799it [53:14,  1.14s/it]\titers: 2800, epoch: 1 | loss: 0.3445387\n",
      "\tspeed: 1.1410s/iter; left time: 81330.8611s\n",
      "2899it [55:08,  1.14s/it]\titers: 2900, epoch: 1 | loss: 0.2136370\n",
      "\tspeed: 1.1413s/iter; left time: 81235.8371s\n",
      "2999it [57:02,  1.14s/it]\titers: 3000, epoch: 1 | loss: 0.2525303\n",
      "\tspeed: 1.1419s/iter; left time: 81164.4774s\n",
      "3099it [58:57,  1.14s/it]\titers: 3100, epoch: 1 | loss: 0.2172107\n",
      "\tspeed: 1.1424s/iter; left time: 81091.1403s\n",
      "3199it [1:00:51,  1.14s/it]\titers: 3200, epoch: 1 | loss: 0.3740815\n",
      "\tspeed: 1.1421s/iter; left time: 80955.0249s\n",
      "3299it [1:02:45,  1.15s/it]\titers: 3300, epoch: 1 | loss: 0.3799628\n",
      "\tspeed: 1.1437s/iter; left time: 80950.6570s\n",
      "3399it [1:04:40,  1.14s/it]\titers: 3400, epoch: 1 | loss: 0.3285590\n",
      "\tspeed: 1.1425s/iter; left time: 80755.5000s\n",
      "3499it [1:06:34,  1.14s/it]\titers: 3500, epoch: 1 | loss: 0.2536146\n",
      "\tspeed: 1.1435s/iter; left time: 80706.6662s\n",
      "3599it [1:08:28,  1.15s/it]\titers: 3600, epoch: 1 | loss: 0.5178335\n",
      "\tspeed: 1.1426s/iter; left time: 80529.2821s\n",
      "3699it [1:10:22,  1.14s/it]\titers: 3700, epoch: 1 | loss: 0.2569439\n",
      "\tspeed: 1.1381s/iter; left time: 80103.4039s\n",
      "3704it [1:10:28,  1.14s/it]\n",
      "Epoch: 1 cost time: 4228.250293493271\n",
      "801it [07:05,  1.88it/s]\n",
      "798it [07:02,  1.89it/s]\n",
      "Epoch: 1 | Train Loss: 0.3884364 Vali Loss: 0.5070970 Test Loss: 0.6595285 MAE Loss: 0.5139003\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [01:51,  1.13s/it]\titers: 100, epoch: 2 | loss: 0.2496161\n",
      "\tspeed: 10.8085s/iter; left time: 759590.7709s\n",
      "199it [03:45,  1.14s/it]\titers: 200, epoch: 2 | loss: 0.2373862\n",
      "\tspeed: 1.1354s/iter; left time: 79680.7465s\n",
      "299it [05:39,  1.14s/it]\titers: 300, epoch: 2 | loss: 0.2356217\n",
      "\tspeed: 1.1412s/iter; left time: 79969.3658s\n",
      "399it [07:33,  1.14s/it]\titers: 400, epoch: 2 | loss: 0.3258728\n",
      "\tspeed: 1.1428s/iter; left time: 79969.1392s\n",
      "499it [09:27,  1.15s/it]\titers: 500, epoch: 2 | loss: 0.2357808\n",
      "\tspeed: 1.1393s/iter; left time: 79607.5353s\n",
      "599it [11:21,  1.13s/it]\titers: 600, epoch: 2 | loss: 0.3752329\n",
      "\tspeed: 1.1380s/iter; left time: 79407.9849s\n",
      "699it [13:15,  1.14s/it]\titers: 700, epoch: 2 | loss: 0.1907516\n",
      "\tspeed: 1.1417s/iter; left time: 79548.5134s\n",
      "799it [15:09,  1.14s/it]\titers: 800, epoch: 2 | loss: 0.2455124\n",
      "\tspeed: 1.1432s/iter; left time: 79543.3018s\n",
      "899it [17:04,  1.15s/it]\titers: 900, epoch: 2 | loss: 0.2168186\n",
      "\tspeed: 1.1443s/iter; left time: 79501.8576s\n",
      "999it [18:58,  1.14s/it]\titers: 1000, epoch: 2 | loss: 0.2404876\n",
      "\tspeed: 1.1434s/iter; left time: 79324.6220s\n",
      "1099it [20:53,  1.15s/it]\titers: 1100, epoch: 2 | loss: 0.2691953\n",
      "\tspeed: 1.1455s/iter; left time: 79356.9982s\n",
      "1199it [22:47,  1.15s/it]\titers: 1200, epoch: 2 | loss: 0.2947649\n",
      "\tspeed: 1.1481s/iter; left time: 79422.3018s\n",
      "1299it [24:42,  1.15s/it]\titers: 1300, epoch: 2 | loss: 0.3369990\n",
      "\tspeed: 1.1490s/iter; left time: 79371.7979s\n",
      "1399it [26:37,  1.14s/it]\titers: 1400, epoch: 2 | loss: 0.2502471\n",
      "\tspeed: 1.1481s/iter; left time: 79193.3102s\n",
      "1499it [28:32,  1.14s/it]\titers: 1500, epoch: 2 | loss: 0.2948978\n",
      "\tspeed: 1.1453s/iter; left time: 78884.8985s\n",
      "1599it [30:26,  1.15s/it]\titers: 1600, epoch: 2 | loss: 0.2375594\n",
      "\tspeed: 1.1432s/iter; left time: 78626.2122s\n",
      "1699it [32:20,  1.14s/it]\titers: 1700, epoch: 2 | loss: 0.2854396\n",
      "\tspeed: 1.1418s/iter; left time: 78416.7649s\n",
      "1799it [34:14,  1.14s/it]\titers: 1800, epoch: 2 | loss: 0.1517044\n",
      "\tspeed: 1.1410s/iter; left time: 78246.6609s\n",
      "1899it [36:09,  1.15s/it]\titers: 1900, epoch: 2 | loss: 0.2364000\n",
      "\tspeed: 1.1441s/iter; left time: 78342.4795s\n",
      "1999it [38:03,  1.15s/it]\titers: 2000, epoch: 2 | loss: 0.2220182\n",
      "\tspeed: 1.1412s/iter; left time: 78034.9858s\n",
      "2099it [39:57,  1.15s/it]\titers: 2100, epoch: 2 | loss: 0.2794600\n",
      "\tspeed: 1.1434s/iter; left time: 78069.7571s\n",
      "2199it [41:51,  1.15s/it]\titers: 2200, epoch: 2 | loss: 0.2781130\n",
      "\tspeed: 1.1410s/iter; left time: 77788.7604s\n",
      "2299it [43:45,  1.14s/it]\titers: 2300, epoch: 2 | loss: 0.2817612\n",
      "\tspeed: 1.1413s/iter; left time: 77694.3096s\n",
      "2399it [45:39,  1.15s/it]\titers: 2400, epoch: 2 | loss: 0.2242345\n",
      "\tspeed: 1.1407s/iter; left time: 77542.8643s\n",
      "2499it [47:34,  1.14s/it]\titers: 2500, epoch: 2 | loss: 0.2598532\n",
      "\tspeed: 1.1426s/iter; left time: 77555.9121s\n",
      "2599it [49:28,  1.14s/it]\titers: 2600, epoch: 2 | loss: 0.2311611\n",
      "\tspeed: 1.1451s/iter; left time: 77611.2116s\n",
      "2699it [51:23,  1.14s/it]\titers: 2700, epoch: 2 | loss: 0.1952908\n",
      "\tspeed: 1.1459s/iter; left time: 77549.9057s\n",
      "2799it [53:17,  1.14s/it]\titers: 2800, epoch: 2 | loss: 0.2126166\n",
      "\tspeed: 1.1447s/iter; left time: 77355.4423s\n",
      "2899it [55:11,  1.14s/it]\titers: 2900, epoch: 2 | loss: 0.3221222\n",
      "\tspeed: 1.1413s/iter; left time: 77013.0644s\n",
      "2999it [57:05,  1.14s/it]\titers: 3000, epoch: 2 | loss: 0.2026970\n",
      "\tspeed: 1.1410s/iter; left time: 76874.0765s\n",
      "3099it [59:00,  1.15s/it]\titers: 3100, epoch: 2 | loss: 0.1820432\n",
      "\tspeed: 1.1411s/iter; left time: 76766.8778s\n",
      "3199it [1:00:53,  1.14s/it]\titers: 3200, epoch: 2 | loss: 0.1910595\n",
      "\tspeed: 1.1386s/iter; left time: 76490.9463s\n",
      "3299it [1:02:47,  1.14s/it]\titers: 3300, epoch: 2 | loss: 0.1242481\n",
      "\tspeed: 1.1403s/iter; left time: 76489.4305s\n",
      "3399it [1:04:41,  1.14s/it]\titers: 3400, epoch: 2 | loss: 0.1699875\n",
      "\tspeed: 1.1390s/iter; left time: 76285.0780s\n",
      "3499it [1:06:36,  1.14s/it]\titers: 3500, epoch: 2 | loss: 0.2088378\n",
      "\tspeed: 1.1420s/iter; left time: 76371.6365s\n",
      "3599it [1:08:30,  1.14s/it]\titers: 3600, epoch: 2 | loss: 0.1914460\n",
      "\tspeed: 1.1399s/iter; left time: 76120.2841s\n",
      "3699it [1:10:24,  1.15s/it]\titers: 3700, epoch: 2 | loss: 0.2009026\n",
      "\tspeed: 1.1448s/iter; left time: 76332.0301s\n",
      "3704it [1:10:30,  1.14s/it]\n",
      "Epoch: 2 cost time: 4230.344032287598\n",
      "801it [07:07,  1.87it/s]\n",
      "798it [07:02,  1.89it/s]\n",
      "Epoch: 2 | Train Loss: 0.2387700 Vali Loss: 0.5356529 Test Loss: 0.7203391 MAE Loss: 0.5260581\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [01:53,  1.15s/it]\titers: 100, epoch: 3 | loss: 0.1981649\n",
      "\tspeed: 9.6915s/iter; left time: 645194.7398s\n",
      "199it [03:47,  1.14s/it]\titers: 200, epoch: 3 | loss: 0.1671719\n",
      "\tspeed: 1.1406s/iter; left time: 75822.2898s\n",
      "299it [05:41,  1.14s/it]\titers: 300, epoch: 3 | loss: 0.1477648\n",
      "\tspeed: 1.1411s/iter; left time: 75735.4155s\n",
      "399it [07:36,  1.16s/it]\titers: 400, epoch: 3 | loss: 0.2145543\n",
      "\tspeed: 1.1484s/iter; left time: 76106.2395s\n",
      "499it [09:30,  1.15s/it]\titers: 500, epoch: 3 | loss: 0.2003082\n",
      "\tspeed: 1.1452s/iter; left time: 75782.9708s\n",
      "599it [11:25,  1.14s/it]\titers: 600, epoch: 3 | loss: 0.1845745\n",
      "\tspeed: 1.1465s/iter; left time: 75753.4094s\n",
      "699it [13:19,  1.14s/it]\titers: 700, epoch: 3 | loss: 0.2044496\n",
      "\tspeed: 1.1426s/iter; left time: 75379.7200s\n",
      "799it [15:13,  1.14s/it]\titers: 800, epoch: 3 | loss: 0.1822916\n",
      "\tspeed: 1.1427s/iter; left time: 75272.2155s\n",
      "899it [17:08,  1.14s/it]\titers: 900, epoch: 3 | loss: 0.1741346\n",
      "\tspeed: 1.1416s/iter; left time: 75085.4418s\n",
      "999it [19:02,  1.14s/it]\titers: 1000, epoch: 3 | loss: 0.2428913\n",
      "\tspeed: 1.1407s/iter; left time: 74913.5430s\n",
      "1099it [20:56,  1.14s/it]\titers: 1100, epoch: 3 | loss: 0.2275534\n",
      "\tspeed: 1.1422s/iter; left time: 74899.4983s\n",
      "1199it [22:50,  1.14s/it]\titers: 1200, epoch: 3 | loss: 0.1962683\n",
      "\tspeed: 1.1407s/iter; left time: 74683.8257s\n",
      "1299it [24:44,  1.14s/it]\titers: 1300, epoch: 3 | loss: 0.2128856\n",
      "\tspeed: 1.1428s/iter; left time: 74708.0339s\n",
      "1399it [26:39,  1.14s/it]\titers: 1400, epoch: 3 | loss: 0.2071234\n",
      "\tspeed: 1.1437s/iter; left time: 74651.9532s\n",
      "1499it [28:33,  1.15s/it]\titers: 1500, epoch: 3 | loss: 0.1524867\n",
      "\tspeed: 1.1473s/iter; left time: 74772.0191s\n",
      "1599it [30:28,  1.14s/it]\titers: 1600, epoch: 3 | loss: 0.1437101\n",
      "\tspeed: 1.1443s/iter; left time: 74466.0520s\n",
      "1699it [32:22,  1.14s/it]\titers: 1700, epoch: 3 | loss: 0.1942015\n",
      "\tspeed: 1.1456s/iter; left time: 74432.8894s\n",
      "1799it [34:17,  1.14s/it]\titers: 1800, epoch: 3 | loss: 0.1745816\n",
      "\tspeed: 1.1418s/iter; left time: 74073.9755s\n",
      "1899it [36:11,  1.14s/it]\titers: 1900, epoch: 3 | loss: 0.1553350\n",
      "\tspeed: 1.1416s/iter; left time: 73943.6280s\n",
      "1999it [38:05,  1.14s/it]\titers: 2000, epoch: 3 | loss: 0.2056632\n",
      "\tspeed: 1.1432s/iter; left time: 73932.4074s\n",
      "2099it [39:59,  1.14s/it]\titers: 2100, epoch: 3 | loss: 0.1481360\n",
      "\tspeed: 1.1428s/iter; left time: 73791.2535s\n",
      "2199it [41:54,  1.15s/it]\titers: 2200, epoch: 3 | loss: 0.1682478\n",
      "\tspeed: 1.1447s/iter; left time: 73805.4157s\n",
      "2299it [43:48,  1.14s/it]\titers: 2300, epoch: 3 | loss: 0.1286860\n",
      "\tspeed: 1.1424s/iter; left time: 73541.3365s\n",
      "2399it [45:42,  1.15s/it]\titers: 2400, epoch: 3 | loss: 0.1636152\n",
      "\tspeed: 1.1448s/iter; left time: 73577.3485s\n",
      "2499it [47:37,  1.15s/it]\titers: 2500, epoch: 3 | loss: 0.1528060\n",
      "\tspeed: 1.1451s/iter; left time: 73486.3501s\n",
      "2599it [49:31,  1.15s/it]\titers: 2600, epoch: 3 | loss: 0.1265991\n",
      "\tspeed: 1.1453s/iter; left time: 73381.8814s\n",
      "2699it [51:26,  1.14s/it]\titers: 2700, epoch: 3 | loss: 0.2028771\n",
      "\tspeed: 1.1454s/iter; left time: 73272.9488s\n",
      "2799it [53:20,  1.15s/it]\titers: 2800, epoch: 3 | loss: 0.2176809\n",
      "\tspeed: 1.1442s/iter; left time: 73085.8707s\n",
      "2899it [55:15,  1.14s/it]\titers: 2900, epoch: 3 | loss: 0.2048361\n",
      "\tspeed: 1.1430s/iter; left time: 72895.7107s\n",
      "2999it [57:09,  1.14s/it]\titers: 3000, epoch: 3 | loss: 0.1638101\n",
      "\tspeed: 1.1407s/iter; left time: 72629.6521s\n",
      "3099it [59:03,  1.14s/it]\titers: 3100, epoch: 3 | loss: 0.1602094\n",
      "\tspeed: 1.1414s/iter; left time: 72565.2694s\n",
      "3199it [1:00:57,  1.14s/it]\titers: 3200, epoch: 3 | loss: 0.1493931\n",
      "\tspeed: 1.1399s/iter; left time: 72350.0738s\n",
      "3299it [1:02:51,  1.14s/it]\titers: 3300, epoch: 3 | loss: 0.1506281\n",
      "\tspeed: 1.1403s/iter; left time: 72264.8819s\n",
      "3399it [1:04:45,  1.14s/it]\titers: 3400, epoch: 3 | loss: 0.1994124\n",
      "\tspeed: 1.1397s/iter; left time: 72115.0989s\n",
      "3499it [1:06:39,  1.16s/it]\titers: 3500, epoch: 3 | loss: 0.2208976\n",
      "\tspeed: 1.1435s/iter; left time: 72237.0274s\n",
      "3599it [1:08:34,  1.15s/it]\titers: 3600, epoch: 3 | loss: 0.1881442\n",
      "\tspeed: 1.1461s/iter; left time: 72287.1330s\n",
      "3699it [1:10:29,  1.14s/it]\titers: 3700, epoch: 3 | loss: 0.1292236\n",
      "\tspeed: 1.1462s/iter; left time: 72180.4846s\n",
      "3704it [1:10:34,  1.14s/it]\n",
      "Epoch: 3 cost time: 4234.8148465156555\n",
      "801it [07:06,  1.88it/s]\n",
      "798it [07:01,  1.90it/s]\n",
      "Epoch: 3 | Train Loss: 0.1881123 Vali Loss: 0.5399619 Test Loss: 0.7291847 MAE Loss: 0.5171707\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [01:53,  1.14s/it]\titers: 100, epoch: 4 | loss: 0.1258122\n",
      "\tspeed: 9.6654s/iter; left time: 607653.6204s\n",
      "199it [03:47,  1.14s/it]\titers: 200, epoch: 4 | loss: 0.1603238\n",
      "\tspeed: 1.1422s/iter; left time: 71693.3774s\n",
      "299it [05:41,  1.15s/it]\titers: 300, epoch: 4 | loss: 0.1764121\n",
      "\tspeed: 1.1447s/iter; left time: 71734.2392s\n",
      "399it [07:36,  1.14s/it]\titers: 400, epoch: 4 | loss: 0.1664973\n",
      "\tspeed: 1.1419s/iter; left time: 71447.1467s\n",
      "499it [09:30,  1.14s/it]\titers: 500, epoch: 4 | loss: 0.1963220\n",
      "\tspeed: 1.1430s/iter; left time: 71399.1636s\n",
      "599it [11:24,  1.14s/it]\titers: 600, epoch: 4 | loss: 0.2017150\n",
      "\tspeed: 1.1425s/iter; left time: 71256.0665s\n",
      "699it [13:18,  1.15s/it]\titers: 700, epoch: 4 | loss: 0.1769782\n",
      "\tspeed: 1.1440s/iter; left time: 71232.9760s\n",
      "799it [15:13,  1.14s/it]\titers: 800, epoch: 4 | loss: 0.1567376\n",
      "\tspeed: 1.1441s/iter; left time: 71126.3316s\n",
      "899it [17:08,  1.14s/it]\titers: 900, epoch: 4 | loss: 0.1422540\n",
      "\tspeed: 1.1464s/iter; left time: 71155.6698s\n",
      "999it [19:02,  1.15s/it]\titers: 1000, epoch: 4 | loss: 0.1226303\n",
      "\tspeed: 1.1420s/iter; left time: 70767.8958s\n",
      "1099it [20:56,  1.14s/it]\titers: 1100, epoch: 4 | loss: 0.1914262\n",
      "\tspeed: 1.1393s/iter; left time: 70488.7852s\n",
      "1199it [22:50,  1.15s/it]\titers: 1200, epoch: 4 | loss: 0.1657564\n",
      "\tspeed: 1.1401s/iter; left time: 70424.9296s\n",
      "1299it [24:44,  1.15s/it]\titers: 1300, epoch: 4 | loss: 0.1305773\n",
      "\tspeed: 1.1412s/iter; left time: 70379.0661s\n",
      "1399it [26:38,  1.14s/it]\titers: 1400, epoch: 4 | loss: 0.1234899\n",
      "\tspeed: 1.1386s/iter; left time: 70102.5040s\n",
      "1499it [28:31,  1.13s/it]\titers: 1500, epoch: 4 | loss: 0.1879370\n",
      "\tspeed: 1.1369s/iter; left time: 69883.7100s\n",
      "1599it [30:25,  1.14s/it]\titers: 1600, epoch: 4 | loss: 0.1356809\n",
      "\tspeed: 1.1372s/iter; left time: 69788.5746s\n",
      "1699it [32:19,  1.14s/it]\titers: 1700, epoch: 4 | loss: 0.1326072\n",
      "\tspeed: 1.1353s/iter; left time: 69556.6265s\n",
      "1799it [34:12,  1.14s/it]\titers: 1800, epoch: 4 | loss: 0.1461110\n",
      "\tspeed: 1.1374s/iter; left time: 69575.5482s\n",
      "1899it [36:06,  1.13s/it]\titers: 1900, epoch: 4 | loss: 0.1350383\n",
      "\tspeed: 1.1365s/iter; left time: 69404.6349s\n",
      "1999it [38:00,  1.13s/it]\titers: 2000, epoch: 4 | loss: 0.1466377\n",
      "\tspeed: 1.1381s/iter; left time: 69386.7589s\n",
      "2099it [39:54,  1.14s/it]\titers: 2100, epoch: 4 | loss: 0.1741998\n",
      "\tspeed: 1.1422s/iter; left time: 69525.4482s\n",
      "2199it [41:49,  1.14s/it]\titers: 2200, epoch: 4 | loss: 0.1427097\n",
      "\tspeed: 1.1473s/iter; left time: 69718.4915s\n",
      "2299it [43:43,  1.15s/it]\titers: 2300, epoch: 4 | loss: 0.1842903\n",
      "\tspeed: 1.1452s/iter; left time: 69475.7815s\n",
      "2399it [45:38,  1.14s/it]\titers: 2400, epoch: 4 | loss: 0.2023287\n",
      "\tspeed: 1.1459s/iter; left time: 69407.2631s\n",
      "2499it [47:32,  1.14s/it]\titers: 2500, epoch: 4 | loss: 0.1201938\n",
      "\tspeed: 1.1404s/iter; left time: 68961.2961s\n",
      "2599it [49:26,  1.15s/it]\titers: 2600, epoch: 4 | loss: 0.1439459\n",
      "\tspeed: 1.1404s/iter; left time: 68846.4962s\n",
      "2699it [51:20,  1.15s/it]\titers: 2700, epoch: 4 | loss: 0.1720724\n",
      "\tspeed: 1.1379s/iter; left time: 68582.2533s\n",
      "2799it [53:13,  1.13s/it]\titers: 2800, epoch: 4 | loss: 0.1642767\n",
      "\tspeed: 1.1367s/iter; left time: 68391.1123s\n",
      "2899it [55:07,  1.14s/it]\titers: 2900, epoch: 4 | loss: 0.1712390\n",
      "\tspeed: 1.1370s/iter; left time: 68298.5934s\n",
      "2999it [57:01,  1.14s/it]\titers: 3000, epoch: 4 | loss: 0.0812750\n",
      "\tspeed: 1.1389s/iter; left time: 68296.1956s\n",
      "3099it [58:55,  1.15s/it]\titers: 3100, epoch: 4 | loss: 0.2254092\n",
      "\tspeed: 1.1418s/iter; left time: 68355.4377s\n",
      "3199it [1:00:50,  1.14s/it]\titers: 3200, epoch: 4 | loss: 0.1766175\n",
      "\tspeed: 1.1446s/iter; left time: 68411.6038s\n",
      "3299it [1:02:44,  1.14s/it]\titers: 3300, epoch: 4 | loss: 0.1389549\n",
      "\tspeed: 1.1418s/iter; left time: 68127.3311s\n",
      "3399it [1:04:38,  1.14s/it]\titers: 3400, epoch: 4 | loss: 0.1492424\n",
      "\tspeed: 1.1411s/iter; left time: 67976.9091s\n",
      "3499it [1:06:32,  1.15s/it]\titers: 3500, epoch: 4 | loss: 0.1603483\n",
      "\tspeed: 1.1430s/iter; left time: 67975.3785s\n",
      "3599it [1:08:27,  1.14s/it]\titers: 3600, epoch: 4 | loss: 0.1145240\n",
      "\tspeed: 1.1436s/iter; left time: 67893.3444s\n",
      "3699it [1:10:21,  1.14s/it]\titers: 3700, epoch: 4 | loss: 0.1443605\n",
      "\tspeed: 1.1418s/iter; left time: 67671.1636s\n",
      "3704it [1:10:27,  1.14s/it]\n",
      "Epoch: 4 cost time: 4227.00959777832\n",
      "801it [07:06,  1.88it/s]\n",
      "798it [07:01,  1.89it/s]\n",
      "Epoch: 4 | Train Loss: 0.1620493 Vali Loss: 0.5389603 Test Loss: 0.7156035 MAE Loss: 0.5135629\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 341.8319720784823 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 89115\n",
      "val 19443\n",
      "test 19371\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:18<00:00,  9.22s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-13 09:15:41,001] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-13 09:15:41,747] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-13 09:15:41,747] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-13 09:15:41,747] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-13 09:15:43,126] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-13 09:15:43,126] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-13 09:15:58,467] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-13 09:15:58,468] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-13 09:15:58,468] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-13 09:15:58,470] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-13 09:15:58,470] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-13 09:15:58,470] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-13 09:15:58,470] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-13 09:15:58,470] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-13 09:15:58,470] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-13 09:15:58,470] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-13 09:15:58,749] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-13 09:15:58,750] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-13 09:15:58,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 355.43 GB, percent = 47.1%\n",
      "[2024-05-13 09:15:59,191] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-13 09:15:59,192] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-13 09:15:59,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 355.4 GB, percent = 47.1%\n",
      "[2024-05-13 09:15:59,192] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-13 09:15:59,302] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-13 09:15:59,302] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-13 09:15:59,302] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 355.4 GB, percent = 47.1%\n",
      "[2024-05-13 09:15:59,303] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-13 09:15:59,303] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-13 09:15:59,303] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-13 09:15:59,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f79084879d0>\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-13 09:15:59,304] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   train_batch_size ............. 192\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-13 09:15:59,305] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 192, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "3713it [1:10:05,  1.13s/it]\n",
      "Epoch: 1 cost time: 4205.88530087471\n",
      "810it [07:10,  1.88it/s]\n",
      "807it [07:06,  1.89it/s]\n",
      "Epoch: 1 | Train Loss: 0.3620554 Vali Loss: 0.2914843 Test Loss: 0.3527382 MAE Loss: 0.3792774\n",
      "lr = 0.0008532546\n",
      "Updating learning rate to 0.000853254626655132\n",
      "3713it [1:09:52,  1.13s/it]\n",
      "Epoch: 2 cost time: 4192.854341983795\n",
      "810it [07:15,  1.86it/s]\n",
      "807it [07:09,  1.88it/s]\n",
      "Epoch: 2 | Train Loss: 0.2213862 Vali Loss: 0.2873185 Test Loss: 0.3556855 MAE Loss: 0.3604915\n",
      "Updating learning rate to 0.000426627313327566\n",
      "3713it [1:09:52,  1.13s/it]\n",
      "Epoch: 3 cost time: 4192.42392539978\n",
      "810it [07:08,  1.89it/s]\n",
      "807it [07:05,  1.90it/s]\n",
      "Epoch: 3 | Train Loss: 0.1961084 Vali Loss: 0.2852678 Test Loss: 0.3498624 MAE Loss: 0.3578888\n",
      "Updating learning rate to 0.000213313656663783\n",
      "3713it [1:10:03,  1.13s/it]\n",
      "Epoch: 4 cost time: 4203.58775639534\n",
      "810it [07:12,  1.87it/s]\n",
      "807it [07:06,  1.89it/s]\n",
      "Epoch: 4 | Train Loss: 0.1875043 Vali Loss: 0.3292947 Test Loss: 0.3846990 MAE Loss: 0.3680908\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001066568283318915\n",
      "3713it [1:10:15,  1.14s/it]\n",
      "Epoch: 5 cost time: 4215.950402736664\n",
      "810it [07:10,  1.88it/s]\n",
      "807it [07:07,  1.89it/s]\n",
      "Epoch: 5 | Train Loss: 0.1585037 Vali Loss: 0.3393146 Test Loss: 0.3916302 MAE Loss: 0.3714357\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.332841416594575e-05\n",
      "1120it [21:16,  1.14s/it]^C\n",
      "Total time: 449.6097183863322 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.80s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-13 18:20:22,354] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-13 18:20:23,269] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-13 18:20:23,269] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-13 18:20:23,269] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-13 18:20:24,545] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-13 18:20:24,545] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-13 18:20:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-13 18:20:38,423] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-13 18:20:38,424] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-13 18:20:38,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-13 18:20:38,425] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-13 18:20:38,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-13 18:20:38,425] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-13 18:20:38,425] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-13 18:20:38,425] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-13 18:20:38,425] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-13 18:20:38,708] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-13 18:20:38,709] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-13 18:20:38,709] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 409.74 GB, percent = 54.3%\n",
      "[2024-05-13 18:20:38,978] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-13 18:20:38,979] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-13 18:20:38,979] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 409.74 GB, percent = 54.3%\n",
      "[2024-05-13 18:20:38,979] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-13 18:20:39,100] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-13 18:20:39,101] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-13 18:20:39,101] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 409.74 GB, percent = 54.3%\n",
      "[2024-05-13 18:20:39,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-13 18:20:39,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-13 18:20:39,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-13 18:20:39,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-13 18:20:39,102] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-13 18:20:39,102] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-13 18:20:39,102] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-13 18:20:39,102] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0c5e931e90>\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-13 18:20:39,103] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   train_batch_size ............. 64\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-13 18:20:39,104] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "11112it [1:16:17,  2.43it/s]\n",
      "Epoch: 1 cost time: 4577.907831907272\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:42,  5.17it/s]\n",
      "Epoch: 1 | Train Loss: 0.5845817 Vali Loss: 0.5162795 Test Loss: 0.6531724 MAE Loss: 0.5504519\n",
      "lr = 0.0008535040\n",
      "Updating learning rate to 0.0008535039946125413\n",
      "11112it [1:16:13,  2.43it/s]\n",
      "Epoch: 2 cost time: 4573.918137550354\n",
      "2403it [07:49,  5.12it/s]\n",
      "2394it [07:42,  5.17it/s]\n",
      "Epoch: 2 | Train Loss: 0.3882939 Vali Loss: 0.4784698 Test Loss: 0.6099185 MAE Loss: 0.5035575\n",
      "Updating learning rate to 0.00042675199730627067\n",
      "11112it [1:16:09,  2.43it/s]\n",
      "Epoch: 3 cost time: 4569.567757606506\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:45,  5.15it/s]\n",
      "Epoch: 3 | Train Loss: 0.3711735 Vali Loss: 0.4758393 Test Loss: 0.6070744 MAE Loss: 0.5045897\n",
      "Updating learning rate to 0.00021337599865313533\n",
      "11112it [1:16:34,  2.42it/s]\n",
      "Epoch: 4 cost time: 4594.011620998383\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:44,  5.16it/s]\n",
      "Epoch: 4 | Train Loss: 0.3755636 Vali Loss: 0.4804722 Test Loss: 0.6090945 MAE Loss: 0.5082585\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00010668799932656767\n",
      "11112it [1:16:33,  2.42it/s]\n",
      "Epoch: 5 cost time: 4593.269432067871\n",
      "2403it [07:47,  5.14it/s]\n",
      "2394it [07:52,  5.07it/s]\n",
      "Epoch: 5 | Train Loss: 0.3636292 Vali Loss: 0.4695873 Test Loss: 0.6086259 MAE Loss: 0.5061424\n",
      "Updating learning rate to 5.3343999663283834e-05\n",
      "11112it [1:16:35,  2.42it/s]\n",
      "Epoch: 6 cost time: 4595.505662918091\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:44,  5.16it/s]\n",
      "Epoch: 6 | Train Loss: 0.3567783 Vali Loss: 0.4654327 Test Loss: 0.6034448 MAE Loss: 0.4965199\n",
      "Updating learning rate to 2.6671999831641917e-05\n",
      "11112it [1:16:47,  2.41it/s]\n",
      "Epoch: 7 cost time: 4607.091552495956\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:46,  5.13it/s]\n",
      "Epoch: 7 | Train Loss: 0.3513852 Vali Loss: 0.4611705 Test Loss: 0.5983002 MAE Loss: 0.4878006\n",
      "Updating learning rate to 1.3335999915820958e-05\n",
      "11112it [1:16:27,  2.42it/s]\n",
      "Epoch: 8 cost time: 4587.353976964951\n",
      "2403it [07:46,  5.15it/s]\n",
      "2394it [07:44,  5.16it/s]\n",
      "Epoch: 8 | Train Loss: 0.3488322 Vali Loss: 0.4614353 Test Loss: 0.5982859 MAE Loss: 0.4885469\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.667999957910479e-06\n",
      "11112it [1:16:45,  2.41it/s]\n",
      "Epoch: 9 cost time: 4605.998658180237\n",
      "2403it [07:46,  5.16it/s]\n",
      "2394it [07:44,  5.16it/s]\n",
      "Epoch: 9 | Train Loss: 0.3472566 Vali Loss: 0.4603102 Test Loss: 0.5975427 MAE Loss: 0.4881981\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=8 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again same experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:14<00:00,  7.24s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-14 19:35:38,417] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-14 19:35:39,212] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-14 19:35:39,212] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-14 19:35:39,212] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-14 19:35:40,316] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-14 19:35:40,316] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-14 19:35:52,650] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-14 19:35:52,652] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-14 19:35:52,652] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-14 19:35:52,653] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-14 19:35:52,653] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-14 19:35:52,654] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-14 19:35:52,654] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-14 19:35:52,654] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-14 19:35:52,654] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-14 19:35:52,654] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-14 19:35:52,932] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-14 19:35:52,933] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-14 19:35:52,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 529.48 GB, percent = 70.2%\n",
      "[2024-05-14 19:35:53,045] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-14 19:35:53,045] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-14 19:35:53,045] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 529.48 GB, percent = 70.2%\n",
      "[2024-05-14 19:35:53,046] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-14 19:35:53,149] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-14 19:35:53,150] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-14 19:35:53,150] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 529.48 GB, percent = 70.2%\n",
      "[2024-05-14 19:35:53,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-14 19:35:53,151] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-14 19:35:53,151] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-14 19:35:53,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-14 19:35:53,151] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f78c8f36390>\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-14 19:35:53,152] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   train_batch_size ............. 64\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-14 19:35:53,153] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "11112it [1:15:45,  2.44it/s]\n",
      "Epoch: 1 cost time: 4545.665615797043\n",
      "2403it [07:42,  5.20it/s]\n",
      "2394it [07:40,  5.20it/s]\n",
      "Epoch: 1 | Train Loss: 0.5845817 Vali Loss: 0.5162795 Test Loss: 0.6531724 MAE Loss: 0.5504519\n",
      "lr = 0.0008535040\n",
      "Updating learning rate to 0.0008535039946125413\n",
      "11112it [1:16:30,  2.42it/s]\n",
      "Epoch: 2 cost time: 4590.4296724796295\n",
      "2403it [07:47,  5.15it/s]\n",
      "2394it [07:44,  5.16it/s]\n",
      "Epoch: 2 | Train Loss: 0.3882939 Vali Loss: 0.4784698 Test Loss: 0.6099185 MAE Loss: 0.5035575\n",
      "Updating learning rate to 0.00042675199730627067\n",
      "11112it [1:16:51,  2.41it/s]\n",
      "Epoch: 3 cost time: 4611.932502508163\n",
      "2403it [07:51,  5.10it/s]\n",
      "2394it [07:46,  5.13it/s]\n",
      "Epoch: 3 | Train Loss: 0.3711735 Vali Loss: 0.4758393 Test Loss: 0.6070744 MAE Loss: 0.5045897\n",
      "Updating learning rate to 0.00021337599865313533\n",
      "11112it [1:16:27,  2.42it/s]\n",
      "Epoch: 4 cost time: 4587.426616668701\n",
      "2403it [07:45,  5.16it/s]\n",
      "2394it [07:44,  5.15it/s]\n",
      "Epoch: 4 | Train Loss: 0.3755636 Vali Loss: 0.4804722 Test Loss: 0.6090945 MAE Loss: 0.5082585\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00010668799932656767\n",
      "11112it [1:16:35,  2.42it/s]\n",
      "Epoch: 5 cost time: 4595.413790464401\n",
      "2403it [07:46,  5.16it/s]\n",
      "2394it [07:44,  5.15it/s]\n",
      "Epoch: 5 | Train Loss: 0.3636292 Vali Loss: 0.4695873 Test Loss: 0.6086259 MAE Loss: 0.5061424\n",
      "Updating learning rate to 5.3343999663283834e-05\n",
      "11112it [1:16:33,  2.42it/s]\n",
      "Epoch: 6 cost time: 4593.832879304886\n",
      "2403it [07:47,  5.14it/s]\n",
      "2394it [07:44,  5.15it/s]\n",
      "Epoch: 6 | Train Loss: 0.3567783 Vali Loss: 0.4654327 Test Loss: 0.6034448 MAE Loss: 0.4965199\n",
      "Updating learning rate to 2.6671999831641917e-05\n",
      "11112it [1:16:26,  2.42it/s]\n",
      "Epoch: 7 cost time: 4586.6405527591705\n",
      "2403it [07:47,  5.14it/s]\n",
      "2394it [07:44,  5.15it/s]\n",
      "Epoch: 7 | Train Loss: 0.3513852 Vali Loss: 0.4611705 Test Loss: 0.5983002 MAE Loss: 0.4878006\n",
      "Updating learning rate to 1.3335999915820958e-05\n",
      "11112it [1:16:24,  2.42it/s]\n",
      "Epoch: 8 cost time: 4584.418305397034\n",
      "2403it [07:48,  5.13it/s]\n",
      "2394it [07:46,  5.13it/s]\n",
      "Epoch: 8 | Train Loss: 0.3488322 Vali Loss: 0.4614353 Test Loss: 0.5982859 MAE Loss: 0.4885469\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.667999957910479e-06\n",
      "11112it [1:16:38,  2.42it/s]\n",
      "Epoch: 9 cost time: 4598.2007625103\n",
      "2403it [07:47,  5.14it/s]\n",
      "2394it [07:46,  5.13it/s]\n",
      "Epoch: 9 | Train Loss: 0.3472566 Vali Loss: 0.4603102 Test Loss: 0.5975427 MAE Loss: 0.4881981\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=8 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small batch_size - BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.24s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-14 11:10:31,899] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-14 11:10:32,741] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-14 11:10:32,741] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-14 11:10:32,742] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-14 11:10:34,142] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-14 11:10:34,142] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-14 11:10:48,217] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-14 11:10:48,218] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-14 11:10:48,218] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-14 11:10:48,220] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-14 11:10:48,220] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-14 11:10:48,220] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-14 11:10:48,220] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-14 11:10:48,220] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-14 11:10:48,220] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-14 11:10:48,220] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-14 11:10:48,484] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-14 11:10:48,484] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-14 11:10:48,484] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 472.47 GB, percent = 62.6%\n",
      "[2024-05-14 11:10:48,597] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-14 11:10:48,597] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-14 11:10:48,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 472.47 GB, percent = 62.6%\n",
      "[2024-05-14 11:10:48,597] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-14 11:10:48,709] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-14 11:10:48,710] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-14 11:10:48,710] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 472.47 GB, percent = 62.6%\n",
      "[2024-05-14 11:10:48,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-14 11:10:48,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-14 11:10:48,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-14 11:10:48,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-14 11:10:48,711] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa2ec229490>\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-14 11:10:48,712] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   train_batch_size ............. 8\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-14 11:10:48,713] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:41,  2.45it/s]\titers: 100, epoch: 1 | loss: 0.5560858\n",
      "\tspeed: 0.5969s/iter; left time: 132597.6155s\n",
      "199it [01:22,  2.44it/s]\titers: 200, epoch: 1 | loss: 0.5926039\n",
      "\tspeed: 0.4102s/iter; left time: 91072.8708s\n",
      "299it [02:03,  2.42it/s]\titers: 300, epoch: 1 | loss: 0.7119445\n",
      "\tspeed: 0.4126s/iter; left time: 91581.9648s\n",
      "399it [02:45,  2.42it/s]\titers: 400, epoch: 1 | loss: 0.5372669\n",
      "\tspeed: 0.4147s/iter; left time: 91997.4575s\n",
      "499it [03:26,  2.39it/s]\titers: 500, epoch: 1 | loss: 0.3508989\n",
      "\tspeed: 0.4159s/iter; left time: 92231.6729s\n",
      "599it [04:08,  2.40it/s]\titers: 600, epoch: 1 | loss: 0.4268429\n",
      "\tspeed: 0.4162s/iter; left time: 92247.6096s\n",
      "699it [04:50,  2.38it/s]\titers: 700, epoch: 1 | loss: 0.2970035\n",
      "\tspeed: 0.4173s/iter; left time: 92448.6933s\n",
      "799it [05:31,  2.38it/s]\titers: 800, epoch: 1 | loss: 1.2074847\n",
      "\tspeed: 0.4171s/iter; left time: 92358.7315s\n",
      "899it [06:13,  2.38it/s]\titers: 900, epoch: 1 | loss: 0.4488295\n",
      "\tspeed: 0.4182s/iter; left time: 92571.8590s\n",
      "999it [06:55,  2.38it/s]\titers: 1000, epoch: 1 | loss: 0.2195104\n",
      "\tspeed: 0.4194s/iter; left time: 92780.3328s\n",
      "1099it [07:37,  2.38it/s]\titers: 1100, epoch: 1 | loss: 0.5192387\n",
      "\tspeed: 0.4181s/iter; left time: 92457.7260s\n",
      "1199it [08:19,  2.38it/s]\titers: 1200, epoch: 1 | loss: 0.6771013\n",
      "\tspeed: 0.4193s/iter; left time: 92685.8869s\n",
      "1299it [09:01,  2.38it/s]\titers: 1300, epoch: 1 | loss: 0.2098472\n",
      "\tspeed: 0.4180s/iter; left time: 92362.9171s\n",
      "1399it [09:42,  2.41it/s]\titers: 1400, epoch: 1 | loss: 0.5234021\n",
      "\tspeed: 0.4185s/iter; left time: 92415.8711s\n",
      "1499it [10:24,  2.38it/s]\titers: 1500, epoch: 1 | loss: 0.4305739\n",
      "\tspeed: 0.4192s/iter; left time: 92524.0564s\n",
      "1599it [11:06,  2.39it/s]\titers: 1600, epoch: 1 | loss: 0.4476689\n",
      "\tspeed: 0.4191s/iter; left time: 92468.1735s\n",
      "1699it [11:48,  2.37it/s]\titers: 1700, epoch: 1 | loss: 0.1756542\n",
      "\tspeed: 0.4195s/iter; left time: 92520.7602s\n",
      "1799it [12:30,  2.37it/s]\titers: 1800, epoch: 1 | loss: 0.2950811\n",
      "\tspeed: 0.4194s/iter; left time: 92455.2754s\n",
      "1899it [13:12,  2.38it/s]\titers: 1900, epoch: 1 | loss: 0.4339724\n",
      "\tspeed: 0.4190s/iter; left time: 92322.3229s\n",
      "1999it [13:54,  2.39it/s]\titers: 2000, epoch: 1 | loss: 0.5655191\n",
      "\tspeed: 0.4193s/iter; left time: 92343.1652s\n",
      "2099it [14:36,  2.39it/s]\titers: 2100, epoch: 1 | loss: 0.1939467\n",
      "\tspeed: 0.4190s/iter; left time: 92230.0123s\n",
      "2199it [15:18,  2.38it/s]\titers: 2200, epoch: 1 | loss: 0.3373198\n",
      "\tspeed: 0.4183s/iter; left time: 92046.8099s\n",
      "2299it [15:59,  2.38it/s]\titers: 2300, epoch: 1 | loss: 0.2464068\n",
      "\tspeed: 0.4184s/iter; left time: 92013.8979s\n",
      "2399it [16:41,  2.40it/s]\titers: 2400, epoch: 1 | loss: 0.2159364\n",
      "\tspeed: 0.4183s/iter; left time: 91958.0566s\n",
      "2499it [17:23,  2.38it/s]\titers: 2500, epoch: 1 | loss: 0.5139627\n",
      "\tspeed: 0.4182s/iter; left time: 91902.7870s\n",
      "2599it [18:05,  2.40it/s]\titers: 2600, epoch: 1 | loss: 0.1734377\n",
      "\tspeed: 0.4177s/iter; left time: 91754.0330s\n",
      "2699it [18:47,  2.41it/s]\titers: 2700, epoch: 1 | loss: 0.2804104\n",
      "\tspeed: 0.4184s/iter; left time: 91847.5318s\n",
      "2799it [19:29,  2.39it/s]\titers: 2800, epoch: 1 | loss: 0.4349535\n",
      "\tspeed: 0.4176s/iter; left time: 91643.6888s\n",
      "2899it [20:10,  2.38it/s]\titers: 2900, epoch: 1 | loss: 0.5225322\n",
      "\tspeed: 0.4180s/iter; left time: 91679.2132s\n",
      "2999it [20:52,  2.41it/s]\titers: 3000, epoch: 1 | loss: 0.3044367\n",
      "\tspeed: 0.4169s/iter; left time: 91410.4696s\n",
      "3099it [21:34,  2.39it/s]\titers: 3100, epoch: 1 | loss: 0.5567851\n",
      "\tspeed: 0.4182s/iter; left time: 91635.0016s\n",
      "3199it [22:16,  2.40it/s]\titers: 3200, epoch: 1 | loss: 0.2979866\n",
      "\tspeed: 0.4173s/iter; left time: 91411.0212s\n",
      "3299it [22:57,  2.38it/s]\titers: 3300, epoch: 1 | loss: 0.4076260\n",
      "\tspeed: 0.4176s/iter; left time: 91427.4131s\n",
      "3399it [23:39,  2.40it/s]\titers: 3400, epoch: 1 | loss: 0.3942325\n",
      "\tspeed: 0.4178s/iter; left time: 91441.9169s\n",
      "3499it [24:21,  2.40it/s]\titers: 3500, epoch: 1 | loss: 0.4189551\n",
      "\tspeed: 0.4173s/iter; left time: 91285.6520s\n",
      "3599it [25:03,  2.39it/s]\titers: 3600, epoch: 1 | loss: 0.3359551\n",
      "\tspeed: 0.4181s/iter; left time: 91403.9842s\n",
      "3699it [25:44,  2.40it/s]\titers: 3700, epoch: 1 | loss: 0.3027801\n",
      "\tspeed: 0.4177s/iter; left time: 91274.7227s\n",
      "3799it [26:26,  2.39it/s]\titers: 3800, epoch: 1 | loss: 0.6984205\n",
      "\tspeed: 0.4183s/iter; left time: 91372.1291s\n",
      "3899it [27:08,  2.39it/s]\titers: 3900, epoch: 1 | loss: 0.2852122\n",
      "\tspeed: 0.4184s/iter; left time: 91348.7940s\n",
      "3999it [27:50,  2.39it/s]\titers: 4000, epoch: 1 | loss: 0.1093436\n",
      "\tspeed: 0.4182s/iter; left time: 91267.1269s\n",
      "4099it [28:32,  2.40it/s]\titers: 4100, epoch: 1 | loss: 0.5760270\n",
      "\tspeed: 0.4185s/iter; left time: 91294.0268s\n",
      "4199it [29:14,  2.37it/s]\titers: 4200, epoch: 1 | loss: 0.3835126\n",
      "\tspeed: 0.4184s/iter; left time: 91236.4790s\n",
      "4299it [29:55,  2.39it/s]\titers: 4300, epoch: 1 | loss: 0.3099016\n",
      "\tspeed: 0.4191s/iter; left time: 91331.2547s\n",
      "4399it [30:37,  2.40it/s]\titers: 4400, epoch: 1 | loss: 0.4124514\n",
      "\tspeed: 0.4195s/iter; left time: 91393.3081s\n",
      "4499it [31:19,  2.39it/s]\titers: 4500, epoch: 1 | loss: 0.3198408\n",
      "\tspeed: 0.4198s/iter; left time: 91399.2561s\n",
      "4599it [32:01,  2.38it/s]\titers: 4600, epoch: 1 | loss: 0.5359207\n",
      "\tspeed: 0.4187s/iter; left time: 91119.8454s\n",
      "4699it [32:43,  2.40it/s]\titers: 4700, epoch: 1 | loss: 0.6018137\n",
      "\tspeed: 0.4190s/iter; left time: 91139.4751s\n",
      "4799it [33:25,  2.38it/s]\titers: 4800, epoch: 1 | loss: 0.1802735\n",
      "\tspeed: 0.4200s/iter; left time: 91332.5385s\n",
      "4899it [34:07,  2.39it/s]\titers: 4900, epoch: 1 | loss: 0.8731657\n",
      "\tspeed: 0.4195s/iter; left time: 91175.4367s\n",
      "4999it [34:49,  2.37it/s]\titers: 5000, epoch: 1 | loss: 0.2120547\n",
      "\tspeed: 0.4191s/iter; left time: 91041.7773s\n",
      "5099it [35:31,  2.37it/s]\titers: 5100, epoch: 1 | loss: 0.6277844\n",
      "\tspeed: 0.4198s/iter; left time: 91160.4693s\n",
      "5199it [36:13,  2.40it/s]\titers: 5200, epoch: 1 | loss: 0.1383549\n",
      "\tspeed: 0.4190s/iter; left time: 90946.6113s\n",
      "5299it [36:55,  2.38it/s]\titers: 5300, epoch: 1 | loss: 0.1843629\n",
      "\tspeed: 0.4182s/iter; left time: 90727.2662s\n",
      "5399it [37:37,  2.39it/s]\titers: 5400, epoch: 1 | loss: 0.3186321\n",
      "\tspeed: 0.4182s/iter; left time: 90683.4732s\n",
      "5499it [38:18,  2.40it/s]\titers: 5500, epoch: 1 | loss: 0.2484787\n",
      "\tspeed: 0.4181s/iter; left time: 90613.8476s\n",
      "5599it [39:00,  2.39it/s]\titers: 5600, epoch: 1 | loss: 0.2993711\n",
      "\tspeed: 0.4186s/iter; left time: 90678.3203s\n",
      "5699it [39:42,  2.39it/s]\titers: 5700, epoch: 1 | loss: 0.6279997\n",
      "\tspeed: 0.4172s/iter; left time: 90350.3236s\n",
      "5799it [40:24,  2.40it/s]\titers: 5800, epoch: 1 | loss: 0.3507278\n",
      "\tspeed: 0.4170s/iter; left time: 90247.3846s\n",
      "5899it [41:05,  2.41it/s]\titers: 5900, epoch: 1 | loss: 0.4332654\n",
      "\tspeed: 0.4169s/iter; left time: 90193.0211s\n",
      "5999it [41:47,  2.38it/s]\titers: 6000, epoch: 1 | loss: 0.1641367\n",
      "\tspeed: 0.4171s/iter; left time: 90197.6863s\n",
      "6099it [42:29,  2.40it/s]\titers: 6100, epoch: 1 | loss: 0.3073261\n",
      "\tspeed: 0.4174s/iter; left time: 90208.2333s\n",
      "6199it [43:11,  2.40it/s]\titers: 6200, epoch: 1 | loss: 0.2652695\n",
      "\tspeed: 0.4179s/iter; left time: 90288.7475s\n",
      "6299it [43:52,  2.38it/s]\titers: 6300, epoch: 1 | loss: 0.2370418\n",
      "\tspeed: 0.4187s/iter; left time: 90412.3856s\n",
      "6399it [44:34,  2.38it/s]\titers: 6400, epoch: 1 | loss: 0.2181122\n",
      "\tspeed: 0.4180s/iter; left time: 90213.3269s\n",
      "6499it [45:16,  2.38it/s]\titers: 6500, epoch: 1 | loss: 0.2154739\n",
      "\tspeed: 0.4180s/iter; left time: 90187.5429s\n",
      "6599it [45:58,  2.42it/s]\titers: 6600, epoch: 1 | loss: 0.2897965\n",
      "\tspeed: 0.4175s/iter; left time: 90032.6532s\n",
      "6699it [46:40,  2.39it/s]\titers: 6700, epoch: 1 | loss: 0.4544679\n",
      "\tspeed: 0.4175s/iter; left time: 89977.6925s\n",
      "6799it [47:21,  2.39it/s]\titers: 6800, epoch: 1 | loss: 0.2926627\n",
      "\tspeed: 0.4182s/iter; left time: 90097.2571s\n",
      "6899it [48:03,  2.39it/s]\titers: 6900, epoch: 1 | loss: 0.3040235\n",
      "\tspeed: 0.4178s/iter; left time: 89980.0276s\n",
      "6999it [48:45,  2.40it/s]\titers: 7000, epoch: 1 | loss: 0.2199938\n",
      "\tspeed: 0.4185s/iter; left time: 90075.6852s\n",
      "7099it [49:27,  2.37it/s]\titers: 7100, epoch: 1 | loss: 0.2673074\n",
      "\tspeed: 0.4188s/iter; left time: 90092.4789s\n",
      "7199it [50:09,  2.38it/s]\titers: 7200, epoch: 1 | loss: 0.1688741\n",
      "\tspeed: 0.4197s/iter; left time: 90244.9127s\n",
      "7299it [50:51,  2.39it/s]\titers: 7300, epoch: 1 | loss: 0.2337481\n",
      "\tspeed: 0.4209s/iter; left time: 90459.8130s\n",
      "7399it [51:33,  2.36it/s]\titers: 7400, epoch: 1 | loss: 0.1223745\n",
      "\tspeed: 0.4197s/iter; left time: 90176.1263s\n",
      "7499it [52:15,  2.38it/s]\titers: 7500, epoch: 1 | loss: 0.3247262\n",
      "\tspeed: 0.4198s/iter; left time: 90151.2695s\n",
      "7599it [52:57,  2.38it/s]\titers: 7600, epoch: 1 | loss: 0.2245117\n",
      "\tspeed: 0.4193s/iter; left time: 89999.9218s\n",
      "7699it [53:39,  2.39it/s]\titers: 7700, epoch: 1 | loss: 1.2023956\n",
      "\tspeed: 0.4209s/iter; left time: 90292.3257s\n",
      "7799it [54:21,  2.37it/s]\titers: 7800, epoch: 1 | loss: 0.2428230\n",
      "\tspeed: 0.4205s/iter; left time: 90180.5972s\n",
      "7899it [55:03,  2.37it/s]\titers: 7900, epoch: 1 | loss: 0.2881995\n",
      "\tspeed: 0.4214s/iter; left time: 90314.2580s\n",
      "7999it [55:45,  2.37it/s]\titers: 8000, epoch: 1 | loss: 0.4353144\n",
      "\tspeed: 0.4214s/iter; left time: 90275.7938s\n",
      "8099it [56:27,  2.34it/s]\titers: 8100, epoch: 1 | loss: 0.2477140\n",
      "\tspeed: 0.4221s/iter; left time: 90392.2419s\n",
      "8199it [57:11,  2.32it/s]\titers: 8200, epoch: 1 | loss: 0.2047540\n",
      "\tspeed: 0.4308s/iter; left time: 92213.0676s\n",
      "8299it [57:53,  2.34it/s]\titers: 8300, epoch: 1 | loss: 0.2331889\n",
      "\tspeed: 0.4294s/iter; left time: 91874.3686s\n",
      "8399it [58:36,  2.34it/s]\titers: 8400, epoch: 1 | loss: 0.3493819\n",
      "\tspeed: 0.4275s/iter; left time: 91412.2167s\n",
      "8499it [59:19,  2.33it/s]\titers: 8500, epoch: 1 | loss: 0.3486018\n",
      "\tspeed: 0.4266s/iter; left time: 91176.9809s\n",
      "8599it [1:00:02,  2.34it/s]\titers: 8600, epoch: 1 | loss: 0.4253492\n",
      "\tspeed: 0.4285s/iter; left time: 91544.2453s\n",
      "8699it [1:00:44,  2.35it/s]\titers: 8700, epoch: 1 | loss: 0.2182874\n",
      "\tspeed: 0.4263s/iter; left time: 91041.9860s\n",
      "8799it [1:01:27,  2.33it/s]\titers: 8800, epoch: 1 | loss: 0.3293222\n",
      "\tspeed: 0.4258s/iter; left time: 90882.8392s\n",
      "8899it [1:02:10,  2.36it/s]\titers: 8900, epoch: 1 | loss: 0.1601382\n",
      "\tspeed: 0.4276s/iter; left time: 91214.6340s\n",
      "8999it [1:02:52,  2.33it/s]\titers: 9000, epoch: 1 | loss: 0.3032715\n",
      "\tspeed: 0.4264s/iter; left time: 90922.7048s\n",
      "9099it [1:03:35,  2.36it/s]\titers: 9100, epoch: 1 | loss: 0.1903152\n",
      "\tspeed: 0.4241s/iter; left time: 90387.3707s\n",
      "9199it [1:04:18,  2.33it/s]\titers: 9200, epoch: 1 | loss: 0.2413430\n",
      "\tspeed: 0.4280s/iter; left time: 91184.5909s\n",
      "9299it [1:05:00,  2.37it/s]\titers: 9300, epoch: 1 | loss: 0.1887757\n",
      "\tspeed: 0.4250s/iter; left time: 90494.3409s\n",
      "9399it [1:05:42,  2.37it/s]\titers: 9400, epoch: 1 | loss: 0.2206841\n",
      "\tspeed: 0.4194s/iter; left time: 89273.1090s\n",
      "9499it [1:06:24,  2.38it/s]\titers: 9500, epoch: 1 | loss: 0.1690228\n",
      "\tspeed: 0.4204s/iter; left time: 89426.0285s\n",
      "9599it [1:07:06,  2.39it/s]\titers: 9600, epoch: 1 | loss: 0.2756483\n",
      "\tspeed: 0.4184s/iter; left time: 88974.4701s\n",
      "9699it [1:07:48,  2.38it/s]\titers: 9700, epoch: 1 | loss: 0.2991610\n",
      "\tspeed: 0.4181s/iter; left time: 88872.2883s\n",
      "9799it [1:08:30,  2.37it/s]\titers: 9800, epoch: 1 | loss: 0.2567869\n",
      "\tspeed: 0.4189s/iter; left time: 88994.3751s\n",
      "9899it [1:09:11,  2.38it/s]\titers: 9900, epoch: 1 | loss: 0.2064472\n",
      "\tspeed: 0.4188s/iter; left time: 88931.0497s\n",
      "9999it [1:09:54,  2.39it/s]\titers: 10000, epoch: 1 | loss: 0.1827006\n",
      "\tspeed: 0.4208s/iter; left time: 89317.2210s\n",
      "10099it [1:10:36,  2.38it/s]\titers: 10100, epoch: 1 | loss: 0.2289336\n",
      "\tspeed: 0.4203s/iter; left time: 89159.2990s\n",
      "10199it [1:11:18,  2.39it/s]\titers: 10200, epoch: 1 | loss: 0.2421528\n",
      "\tspeed: 0.4198s/iter; left time: 89019.1838s\n",
      "10299it [1:11:59,  2.39it/s]\titers: 10300, epoch: 1 | loss: 0.3891946\n",
      "\tspeed: 0.4193s/iter; left time: 88868.3137s\n",
      "10399it [1:12:41,  2.39it/s]\titers: 10400, epoch: 1 | loss: 0.1251569\n",
      "\tspeed: 0.4186s/iter; left time: 88681.3021s\n",
      "10499it [1:13:23,  2.39it/s]\titers: 10500, epoch: 1 | loss: 0.1575943\n",
      "\tspeed: 0.4190s/iter; left time: 88718.6018s\n",
      "10599it [1:14:05,  2.39it/s]\titers: 10600, epoch: 1 | loss: 0.4520520\n",
      "\tspeed: 0.4193s/iter; left time: 88736.3853s\n",
      "10699it [1:14:47,  2.37it/s]\titers: 10700, epoch: 1 | loss: 0.1287676\n",
      "\tspeed: 0.4191s/iter; left time: 88664.0173s\n",
      "10799it [1:15:29,  2.39it/s]\titers: 10800, epoch: 1 | loss: 0.4900165\n",
      "\tspeed: 0.4185s/iter; left time: 88488.1339s\n",
      "10899it [1:16:11,  2.38it/s]\titers: 10900, epoch: 1 | loss: 0.5770792\n",
      "\tspeed: 0.4185s/iter; left time: 88440.7428s\n",
      "10999it [1:16:53,  2.38it/s]\titers: 11000, epoch: 1 | loss: 0.1517943\n",
      "\tspeed: 0.4184s/iter; left time: 88386.2057s\n",
      "11099it [1:17:34,  2.42it/s]\titers: 11100, epoch: 1 | loss: 0.1861620\n",
      "\tspeed: 0.4169s/iter; left time: 88023.4784s\n",
      "11112it [1:17:40,  2.38it/s]\n",
      "Epoch: 1 cost time: 4660.238497257233\n",
      "2403it [07:45,  5.16it/s]\n",
      "2394it [07:45,  5.14it/s]\n",
      "Epoch: 1 | Train Loss: 0.3473882 Vali Loss: 0.5301859 Test Loss: 0.6792957 MAE Loss: 0.5161061\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:40,  2.47it/s]\titers: 100, epoch: 2 | loss: 0.3201235\n",
      "\tspeed: 11.0578s/iter; left time: 2333509.5088s\n",
      "199it [01:21,  2.45it/s]\titers: 200, epoch: 2 | loss: 0.1651681\n",
      "\tspeed: 0.4090s/iter; left time: 86265.3748s\n",
      "299it [02:02,  2.42it/s]\titers: 300, epoch: 2 | loss: 0.2243153\n",
      "\tspeed: 0.4121s/iter; left time: 86876.6077s\n",
      "399it [02:43,  2.43it/s]\titers: 400, epoch: 2 | loss: 0.2669446\n",
      "\tspeed: 0.4124s/iter; left time: 86903.9096s\n",
      "499it [03:25,  2.41it/s]\titers: 500, epoch: 2 | loss: 0.2331312\n",
      "\tspeed: 0.4134s/iter; left time: 87064.6322s\n",
      "599it [04:06,  2.42it/s]\titers: 600, epoch: 2 | loss: 0.2291772\n",
      "\tspeed: 0.4132s/iter; left time: 86999.6367s\n",
      "699it [04:47,  2.40it/s]\titers: 700, epoch: 2 | loss: 0.1709987\n",
      "\tspeed: 0.4140s/iter; left time: 87118.5745s\n",
      "799it [05:29,  2.41it/s]\titers: 800, epoch: 2 | loss: 0.2417526\n",
      "\tspeed: 0.4166s/iter; left time: 87621.9584s\n",
      "899it [06:10,  2.41it/s]\titers: 900, epoch: 2 | loss: 0.1847837\n",
      "\tspeed: 0.4144s/iter; left time: 87116.0879s\n",
      "999it [06:52,  2.41it/s]\titers: 1000, epoch: 2 | loss: 0.2042855\n",
      "\tspeed: 0.4141s/iter; left time: 87022.7074s\n",
      "1099it [07:33,  2.43it/s]\titers: 1100, epoch: 2 | loss: 0.2879457\n",
      "\tspeed: 0.4145s/iter; left time: 87062.3711s\n",
      "1199it [08:15,  2.41it/s]\titers: 1200, epoch: 2 | loss: 0.2144345\n",
      "\tspeed: 0.4142s/iter; left time: 86948.9067s\n",
      "1299it [08:56,  2.41it/s]\titers: 1300, epoch: 2 | loss: 0.1774169\n",
      "\tspeed: 0.4152s/iter; left time: 87117.0326s\n",
      "1399it [09:38,  2.43it/s]\titers: 1400, epoch: 2 | loss: 0.4412537\n",
      "\tspeed: 0.4139s/iter; left time: 86804.7772s\n",
      "1499it [10:19,  2.40it/s]\titers: 1500, epoch: 2 | loss: 0.1263387\n",
      "\tspeed: 0.4164s/iter; left time: 87288.3188s\n",
      "1599it [11:01,  2.44it/s]\titers: 1600, epoch: 2 | loss: 0.2118963\n",
      "\tspeed: 0.4144s/iter; left time: 86825.0541s\n",
      "1699it [11:42,  2.42it/s]\titers: 1700, epoch: 2 | loss: 0.3478212\n",
      "\tspeed: 0.4150s/iter; left time: 86907.0002s\n",
      "1799it [12:24,  2.40it/s]\titers: 1800, epoch: 2 | loss: 0.2401073\n",
      "\tspeed: 0.4148s/iter; left time: 86835.1027s\n",
      "1899it [13:06,  2.40it/s]\titers: 1900, epoch: 2 | loss: 0.3127717\n",
      "\tspeed: 0.4229s/iter; left time: 88482.1139s\n",
      "1999it [13:47,  2.40it/s]\titers: 2000, epoch: 2 | loss: 0.1205927\n",
      "\tspeed: 0.4152s/iter; left time: 86819.9928s\n",
      "2099it [14:29,  2.41it/s]\titers: 2100, epoch: 2 | loss: 0.1898945\n",
      "\tspeed: 0.4154s/iter; left time: 86820.9434s\n",
      "2199it [15:10,  2.43it/s]\titers: 2200, epoch: 2 | loss: 0.2723805\n",
      "\tspeed: 0.4142s/iter; left time: 86541.8011s\n",
      "2299it [15:52,  2.41it/s]\titers: 2300, epoch: 2 | loss: 0.3156155\n",
      "\tspeed: 0.4174s/iter; left time: 87159.1808s\n",
      "2399it [16:34,  2.40it/s]\titers: 2400, epoch: 2 | loss: 0.2020420\n",
      "\tspeed: 0.4151s/iter; left time: 86637.8108s\n",
      "2499it [17:15,  2.40it/s]\titers: 2500, epoch: 2 | loss: 0.1206244\n",
      "\tspeed: 0.4159s/iter; left time: 86763.2394s\n",
      "2599it [17:57,  2.39it/s]\titers: 2600, epoch: 2 | loss: 0.2559618\n",
      "\tspeed: 0.4153s/iter; left time: 86597.2636s\n",
      "2699it [18:38,  2.42it/s]\titers: 2700, epoch: 2 | loss: 0.2575459\n",
      "\tspeed: 0.4152s/iter; left time: 86536.2238s\n",
      "2799it [19:20,  2.42it/s]\titers: 2800, epoch: 2 | loss: 0.3346995\n",
      "\tspeed: 0.4153s/iter; left time: 86524.3458s\n",
      "2899it [20:02,  2.40it/s]\titers: 2900, epoch: 2 | loss: 0.1746227\n",
      "\tspeed: 0.4168s/iter; left time: 86789.5127s\n",
      "2999it [20:43,  2.40it/s]\titers: 3000, epoch: 2 | loss: 0.2533033\n",
      "\tspeed: 0.4148s/iter; left time: 86324.3886s\n",
      "3099it [21:25,  2.39it/s]\titers: 3100, epoch: 2 | loss: 0.2333574\n",
      "\tspeed: 0.4159s/iter; left time: 86509.9053s\n",
      "3199it [22:06,  2.41it/s]\titers: 3200, epoch: 2 | loss: 0.2411904\n",
      "\tspeed: 0.4156s/iter; left time: 86420.1813s\n",
      "3299it [22:48,  2.42it/s]\titers: 3300, epoch: 2 | loss: 0.1767074\n",
      "\tspeed: 0.4156s/iter; left time: 86365.2914s\n",
      "3399it [23:29,  2.41it/s]\titers: 3400, epoch: 2 | loss: 0.3125331\n",
      "\tspeed: 0.4148s/iter; left time: 86174.9580s\n",
      "3499it [24:11,  2.41it/s]\titers: 3500, epoch: 2 | loss: 0.1276557\n",
      "\tspeed: 0.4172s/iter; left time: 86612.6500s\n",
      "3599it [24:52,  2.41it/s]\titers: 3600, epoch: 2 | loss: 0.1657962\n",
      "\tspeed: 0.4148s/iter; left time: 86073.0716s\n",
      "3699it [25:34,  2.42it/s]\titers: 3700, epoch: 2 | loss: 0.2290050\n",
      "\tspeed: 0.4155s/iter; left time: 86179.0626s\n",
      "3799it [26:15,  2.41it/s]\titers: 3800, epoch: 2 | loss: 0.1317518\n",
      "\tspeed: 0.4156s/iter; left time: 86160.0856s\n",
      "3899it [26:57,  2.40it/s]\titers: 3900, epoch: 2 | loss: 0.2239380\n",
      "\tspeed: 0.4152s/iter; left time: 86042.7214s\n",
      "3999it [27:39,  2.41it/s]\titers: 4000, epoch: 2 | loss: 0.1571864\n",
      "\tspeed: 0.4154s/iter; left time: 86031.2318s\n",
      "4099it [28:20,  2.40it/s]\titers: 4100, epoch: 2 | loss: 0.1931511\n",
      "\tspeed: 0.4155s/iter; left time: 86016.0128s\n",
      "4199it [29:02,  2.40it/s]\titers: 4200, epoch: 2 | loss: 0.2093149\n",
      "\tspeed: 0.4146s/iter; left time: 85789.2054s\n",
      "4299it [29:43,  2.41it/s]\titers: 4300, epoch: 2 | loss: 0.1756566\n",
      "\tspeed: 0.4149s/iter; left time: 85814.7121s\n",
      "4399it [30:25,  2.42it/s]\titers: 4400, epoch: 2 | loss: 0.1786650\n",
      "\tspeed: 0.4148s/iter; left time: 85758.0206s\n",
      "4499it [31:06,  2.40it/s]\titers: 4500, epoch: 2 | loss: 0.2505858\n",
      "\tspeed: 0.4149s/iter; left time: 85720.4868s\n",
      "4599it [31:48,  2.42it/s]\titers: 4600, epoch: 2 | loss: 0.1789051\n",
      "\tspeed: 0.4176s/iter; left time: 86248.2151s\n",
      "4699it [32:29,  2.41it/s]\titers: 4700, epoch: 2 | loss: 0.1781871\n",
      "\tspeed: 0.4144s/iter; left time: 85537.2210s\n",
      "4799it [33:11,  2.41it/s]\titers: 4800, epoch: 2 | loss: 0.3448933\n",
      "\tspeed: 0.4144s/iter; left time: 85504.7030s\n",
      "4899it [33:52,  2.43it/s]\titers: 4900, epoch: 2 | loss: 0.1160360\n",
      "\tspeed: 0.4146s/iter; left time: 85500.6328s\n",
      "4999it [34:34,  2.41it/s]\titers: 5000, epoch: 2 | loss: 0.1272358\n",
      "\tspeed: 0.4155s/iter; left time: 85649.8130s\n",
      "5099it [35:15,  2.41it/s]\titers: 5100, epoch: 2 | loss: 0.2067949\n",
      "\tspeed: 0.4168s/iter; left time: 85868.5416s\n",
      "5199it [35:57,  2.40it/s]\titers: 5200, epoch: 2 | loss: 0.1524641\n",
      "\tspeed: 0.4156s/iter; left time: 85589.9275s\n",
      "5299it [36:38,  2.42it/s]\titers: 5300, epoch: 2 | loss: 0.1428147\n",
      "\tspeed: 0.4149s/iter; left time: 85407.7334s\n",
      "5399it [37:20,  2.43it/s]\titers: 5400, epoch: 2 | loss: 0.2431973\n",
      "\tspeed: 0.4152s/iter; left time: 85409.2284s\n",
      "5499it [38:01,  2.41it/s]\titers: 5500, epoch: 2 | loss: 0.2650889\n",
      "\tspeed: 0.4149s/iter; left time: 85319.6388s\n",
      "5599it [38:43,  2.42it/s]\titers: 5600, epoch: 2 | loss: 0.2161244\n",
      "\tspeed: 0.4155s/iter; left time: 85404.4816s\n",
      "5699it [39:25,  2.40it/s]\titers: 5700, epoch: 2 | loss: 0.3131232\n",
      "\tspeed: 0.4157s/iter; left time: 85393.8524s\n",
      "5799it [40:06,  2.41it/s]\titers: 5800, epoch: 2 | loss: 0.4116155\n",
      "\tspeed: 0.4152s/iter; left time: 85252.0897s\n",
      "5899it [40:48,  2.41it/s]\titers: 5900, epoch: 2 | loss: 0.1359613\n",
      "\tspeed: 0.4163s/iter; left time: 85428.2647s\n",
      "5999it [41:29,  2.40it/s]\titers: 6000, epoch: 2 | loss: 0.1752853\n",
      "\tspeed: 0.4154s/iter; left time: 85211.4977s\n",
      "6099it [42:11,  2.40it/s]\titers: 6100, epoch: 2 | loss: 0.2130355\n",
      "\tspeed: 0.4148s/iter; left time: 85043.0010s\n",
      "6199it [42:52,  2.40it/s]\titers: 6200, epoch: 2 | loss: 0.1830363\n",
      "\tspeed: 0.4144s/iter; left time: 84929.0769s\n",
      "6299it [43:34,  2.42it/s]\titers: 6300, epoch: 2 | loss: 0.3182978\n",
      "\tspeed: 0.4150s/iter; left time: 85004.7905s\n",
      "6399it [44:15,  2.41it/s]\titers: 6400, epoch: 2 | loss: 0.3499987\n",
      "\tspeed: 0.4146s/iter; left time: 84880.5039s\n",
      "6499it [44:56,  2.41it/s]\titers: 6500, epoch: 2 | loss: 0.2572977\n",
      "\tspeed: 0.4137s/iter; left time: 84650.0632s\n",
      "6599it [45:38,  2.41it/s]\titers: 6600, epoch: 2 | loss: 0.1843626\n",
      "\tspeed: 0.4139s/iter; left time: 84648.1702s\n",
      "6699it [46:19,  2.44it/s]\titers: 6700, epoch: 2 | loss: 0.1435384\n",
      "\tspeed: 0.4134s/iter; left time: 84508.2892s\n",
      "6799it [47:01,  2.42it/s]\titers: 6800, epoch: 2 | loss: 0.2053385\n",
      "\tspeed: 0.4143s/iter; left time: 84652.0132s\n",
      "6899it [47:42,  2.40it/s]\titers: 6900, epoch: 2 | loss: 0.1596327\n",
      "\tspeed: 0.4145s/iter; left time: 84652.6577s\n",
      "6999it [48:23,  2.42it/s]\titers: 7000, epoch: 2 | loss: 0.1241152\n",
      "\tspeed: 0.4134s/iter; left time: 84383.9791s\n",
      "7099it [49:05,  2.41it/s]\titers: 7100, epoch: 2 | loss: 0.3143073\n",
      "\tspeed: 0.4135s/iter; left time: 84369.0554s\n",
      "7199it [49:46,  2.41it/s]\titers: 7200, epoch: 2 | loss: 0.0778127\n",
      "\tspeed: 0.4141s/iter; left time: 84438.2362s\n",
      "7299it [50:28,  2.41it/s]\titers: 7300, epoch: 2 | loss: 0.1426154\n",
      "\tspeed: 0.4158s/iter; left time: 84745.5127s\n",
      "7399it [51:09,  2.41it/s]\titers: 7400, epoch: 2 | loss: 0.1600899\n",
      "\tspeed: 0.4153s/iter; left time: 84611.2037s\n",
      "7499it [51:51,  2.41it/s]\titers: 7500, epoch: 2 | loss: 0.4502344\n",
      "\tspeed: 0.4143s/iter; left time: 84363.9235s\n",
      "7599it [52:32,  2.39it/s]\titers: 7600, epoch: 2 | loss: 0.1558790\n",
      "\tspeed: 0.4148s/iter; left time: 84416.4136s\n",
      "7699it [53:14,  2.40it/s]\titers: 7700, epoch: 2 | loss: 0.1526189\n",
      "\tspeed: 0.4153s/iter; left time: 84487.9030s\n",
      "7799it [53:55,  2.41it/s]\titers: 7800, epoch: 2 | loss: 0.2432477\n",
      "\tspeed: 0.4154s/iter; left time: 84463.2699s\n",
      "7899it [54:37,  2.40it/s]\titers: 7900, epoch: 2 | loss: 0.3080455\n",
      "\tspeed: 0.4151s/iter; left time: 84353.5666s\n",
      "7999it [55:18,  2.41it/s]\titers: 8000, epoch: 2 | loss: 0.1289282\n",
      "\tspeed: 0.4144s/iter; left time: 84168.5525s\n",
      "8099it [56:00,  2.41it/s]\titers: 8100, epoch: 2 | loss: 0.1733506\n",
      "\tspeed: 0.4145s/iter; left time: 84148.6784s\n",
      "8199it [56:41,  2.41it/s]\titers: 8200, epoch: 2 | loss: 0.2111068\n",
      "\tspeed: 0.4145s/iter; left time: 84115.7475s\n",
      "8299it [57:22,  2.41it/s]\titers: 8300, epoch: 2 | loss: 0.2465812\n",
      "\tspeed: 0.4138s/iter; left time: 83937.0977s\n",
      "8399it [58:04,  2.41it/s]\titers: 8400, epoch: 2 | loss: 0.2161760\n",
      "\tspeed: 0.4142s/iter; left time: 83974.9877s\n",
      "8499it [58:45,  2.42it/s]\titers: 8500, epoch: 2 | loss: 0.2076061\n",
      "\tspeed: 0.4148s/iter; left time: 84055.1818s\n",
      "8599it [59:27,  2.40it/s]\titers: 8600, epoch: 2 | loss: 0.2683805\n",
      "\tspeed: 0.4155s/iter; left time: 84143.7495s\n",
      "8699it [1:00:08,  2.39it/s]\titers: 8700, epoch: 2 | loss: 0.1593878\n",
      "\tspeed: 0.4148s/iter; left time: 83976.7703s\n",
      "8799it [1:00:50,  2.40it/s]\titers: 8800, epoch: 2 | loss: 0.2303716\n",
      "\tspeed: 0.4157s/iter; left time: 84108.3644s\n",
      "8899it [1:01:32,  2.43it/s]\titers: 8900, epoch: 2 | loss: 0.1952000\n",
      "\tspeed: 0.4158s/iter; left time: 84089.4583s\n",
      "8999it [1:02:13,  2.42it/s]\titers: 9000, epoch: 2 | loss: 0.1359818\n",
      "\tspeed: 0.4154s/iter; left time: 83960.8035s\n",
      "9099it [1:02:55,  2.41it/s]\titers: 9100, epoch: 2 | loss: 0.1731744\n",
      "\tspeed: 0.4187s/iter; left time: 84580.3228s\n",
      "9199it [1:03:36,  2.40it/s]\titers: 9200, epoch: 2 | loss: 0.0911536\n",
      "\tspeed: 0.4152s/iter; left time: 83848.0792s\n",
      "9299it [1:04:18,  2.40it/s]\titers: 9300, epoch: 2 | loss: 0.0828895\n",
      "\tspeed: 0.4162s/iter; left time: 83994.5842s\n",
      "9399it [1:05:00,  2.39it/s]\titers: 9400, epoch: 2 | loss: 0.1606227\n",
      "\tspeed: 0.4150s/iter; left time: 83720.9647s\n",
      "9499it [1:05:41,  2.41it/s]\titers: 9500, epoch: 2 | loss: 0.1382502\n",
      "\tspeed: 0.4157s/iter; left time: 83822.8909s\n",
      "9599it [1:06:23,  2.42it/s]\titers: 9600, epoch: 2 | loss: 0.1658123\n",
      "\tspeed: 0.4146s/iter; left time: 83557.4305s\n",
      "9699it [1:07:04,  2.43it/s]\titers: 9700, epoch: 2 | loss: 0.1922623\n",
      "\tspeed: 0.4137s/iter; left time: 83338.6540s\n",
      "9799it [1:07:45,  2.40it/s]\titers: 9800, epoch: 2 | loss: 0.1387543\n",
      "\tspeed: 0.4149s/iter; left time: 83522.0118s\n",
      "9899it [1:08:27,  2.41it/s]\titers: 9900, epoch: 2 | loss: 0.1806325\n",
      "\tspeed: 0.4138s/iter; left time: 83268.1358s\n",
      "9999it [1:09:09,  2.42it/s]\titers: 10000, epoch: 2 | loss: 0.2481135\n",
      "\tspeed: 0.4173s/iter; left time: 83931.1370s\n",
      "10099it [1:09:50,  2.41it/s]\titers: 10100, epoch: 2 | loss: 0.2302111\n",
      "\tspeed: 0.4140s/iter; left time: 83231.4073s\n",
      "10199it [1:10:31,  2.40it/s]\titers: 10200, epoch: 2 | loss: 0.1739226\n",
      "\tspeed: 0.4147s/iter; left time: 83320.5410s\n",
      "10299it [1:11:13,  2.42it/s]\titers: 10300, epoch: 2 | loss: 0.3091066\n",
      "\tspeed: 0.4146s/iter; left time: 83271.0715s\n",
      "10399it [1:11:54,  2.39it/s]\titers: 10400, epoch: 2 | loss: 0.1588849\n",
      "\tspeed: 0.4141s/iter; left time: 83129.6473s\n",
      "10499it [1:12:36,  2.40it/s]\titers: 10500, epoch: 2 | loss: 0.1808278\n",
      "\tspeed: 0.4157s/iter; left time: 83396.2450s\n",
      "10599it [1:13:17,  2.41it/s]\titers: 10600, epoch: 2 | loss: 0.1344849\n",
      "\tspeed: 0.4152s/iter; left time: 83253.3088s\n",
      "10699it [1:13:59,  2.39it/s]\titers: 10700, epoch: 2 | loss: 0.3289126\n",
      "\tspeed: 0.4155s/iter; left time: 83272.8773s\n",
      "10799it [1:14:40,  2.41it/s]\titers: 10800, epoch: 2 | loss: 0.1743057\n",
      "\tspeed: 0.4144s/iter; left time: 83026.3506s\n",
      "10899it [1:15:22,  2.40it/s]\titers: 10900, epoch: 2 | loss: 0.2835832\n",
      "\tspeed: 0.4146s/iter; left time: 83024.4744s\n",
      "10999it [1:16:03,  2.40it/s]\titers: 11000, epoch: 2 | loss: 0.1670136\n",
      "\tspeed: 0.4150s/iter; left time: 83056.3478s\n",
      "11099it [1:16:45,  2.41it/s]\titers: 11100, epoch: 2 | loss: 0.1522763\n",
      "\tspeed: 0.4157s/iter; left time: 83152.0981s\n",
      "11112it [1:16:50,  2.41it/s]\n",
      "Epoch: 2 cost time: 4610.889633178711\n",
      "2403it [07:42,  5.19it/s]\n",
      "2394it [07:42,  5.17it/s]\n",
      "Epoch: 2 | Train Loss: 0.2146526 Vali Loss: 0.5321827 Test Loss: 0.6846158 MAE Loss: 0.5096173\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:41,  2.41it/s]\titers: 100, epoch: 3 | loss: 0.1686763\n",
      "\tspeed: 9.7298s/iter; left time: 1945146.2823s\n",
      "199it [01:23,  2.39it/s]\titers: 200, epoch: 3 | loss: 0.1526550\n",
      "\tspeed: 0.4152s/iter; left time: 82970.4402s\n",
      "299it [02:04,  2.40it/s]\titers: 300, epoch: 3 | loss: 0.1707615\n",
      "\tspeed: 0.4158s/iter; left time: 83035.4669s\n",
      "399it [02:46,  2.40it/s]\titers: 400, epoch: 3 | loss: 0.2161759\n",
      "\tspeed: 0.4150s/iter; left time: 82836.0854s\n",
      "499it [03:27,  2.42it/s]\titers: 500, epoch: 3 | loss: 0.1152257\n",
      "\tspeed: 0.4134s/iter; left time: 82480.1649s\n",
      "599it [04:09,  2.40it/s]\titers: 600, epoch: 3 | loss: 0.1512857\n",
      "\tspeed: 0.4149s/iter; left time: 82730.7082s\n",
      "699it [04:50,  2.41it/s]\titers: 700, epoch: 3 | loss: 0.1841852\n",
      "\tspeed: 0.4148s/iter; left time: 82668.4130s\n",
      "799it [05:32,  2.41it/s]\titers: 800, epoch: 3 | loss: 0.1322679\n",
      "\tspeed: 0.4154s/iter; left time: 82751.6948s\n",
      "899it [06:13,  2.41it/s]\titers: 900, epoch: 3 | loss: 0.1456490\n",
      "\tspeed: 0.4141s/iter; left time: 82458.5944s\n",
      "999it [06:54,  2.41it/s]\titers: 1000, epoch: 3 | loss: 0.1547551\n",
      "\tspeed: 0.4146s/iter; left time: 82516.4074s\n",
      "1099it [07:36,  2.41it/s]\titers: 1100, epoch: 3 | loss: 0.0918558\n",
      "\tspeed: 0.4151s/iter; left time: 82571.4066s\n",
      "1199it [08:17,  2.43it/s]\titers: 1200, epoch: 3 | loss: 0.0956414\n",
      "\tspeed: 0.4136s/iter; left time: 82233.6153s\n",
      "1299it [08:59,  2.40it/s]\titers: 1300, epoch: 3 | loss: 0.1821756\n",
      "\tspeed: 0.4153s/iter; left time: 82525.5047s\n",
      "1399it [09:40,  2.43it/s]\titers: 1400, epoch: 3 | loss: 0.1414262\n",
      "\tspeed: 0.4150s/iter; left time: 82428.2977s\n",
      "1499it [10:22,  2.39it/s]\titers: 1500, epoch: 3 | loss: 0.1462583\n",
      "\tspeed: 0.4153s/iter; left time: 82448.6249s\n",
      "1599it [11:04,  2.41it/s]\titers: 1600, epoch: 3 | loss: 0.2272522\n",
      "\tspeed: 0.4170s/iter; left time: 82743.2261s\n",
      "1699it [11:45,  2.40it/s]\titers: 1700, epoch: 3 | loss: 0.1048223\n",
      "\tspeed: 0.4165s/iter; left time: 82599.9645s\n",
      "1799it [12:27,  2.39it/s]\titers: 1800, epoch: 3 | loss: 0.1274397\n",
      "\tspeed: 0.4159s/iter; left time: 82433.0296s\n",
      "1899it [13:09,  2.41it/s]\titers: 1900, epoch: 3 | loss: 0.1549326\n",
      "\tspeed: 0.4165s/iter; left time: 82509.7838s\n",
      "1999it [13:50,  2.41it/s]\titers: 2000, epoch: 3 | loss: 0.1271647\n",
      "\tspeed: 0.4162s/iter; left time: 82407.9084s\n",
      "2099it [14:32,  2.42it/s]\titers: 2100, epoch: 3 | loss: 0.1674475\n",
      "\tspeed: 0.4157s/iter; left time: 82275.4982s\n",
      "2199it [15:13,  2.41it/s]\titers: 2200, epoch: 3 | loss: 0.1281521\n",
      "\tspeed: 0.4153s/iter; left time: 82151.4074s\n",
      "2299it [15:55,  2.42it/s]\titers: 2300, epoch: 3 | loss: 0.2168236\n",
      "\tspeed: 0.4153s/iter; left time: 82103.7225s\n",
      "2399it [16:36,  2.41it/s]\titers: 2400, epoch: 3 | loss: 0.2670994\n",
      "\tspeed: 0.4164s/iter; left time: 82292.8080s\n",
      "2499it [17:18,  2.40it/s]\titers: 2500, epoch: 3 | loss: 0.1646959\n",
      "\tspeed: 0.4157s/iter; left time: 82101.7970s\n",
      "2599it [18:00,  2.42it/s]\titers: 2600, epoch: 3 | loss: 0.1680902\n",
      "\tspeed: 0.4175s/iter; left time: 82425.2198s\n",
      "2699it [18:41,  2.39it/s]\titers: 2700, epoch: 3 | loss: 0.1803954\n",
      "\tspeed: 0.4162s/iter; left time: 82131.1050s\n",
      "2799it [19:23,  2.42it/s]\titers: 2800, epoch: 3 | loss: 0.1625809\n",
      "\tspeed: 0.4157s/iter; left time: 81989.3214s\n",
      "2899it [20:04,  2.42it/s]\titers: 2900, epoch: 3 | loss: 0.1206483\n",
      "\tspeed: 0.4143s/iter; left time: 81673.7948s\n",
      "2999it [20:46,  2.41it/s]\titers: 3000, epoch: 3 | loss: 0.3316368\n",
      "\tspeed: 0.4156s/iter; left time: 81881.3915s\n",
      "3099it [21:28,  2.42it/s]\titers: 3100, epoch: 3 | loss: 0.0957330\n",
      "\tspeed: 0.4163s/iter; left time: 81970.3379s\n",
      "3199it [22:09,  2.41it/s]\titers: 3200, epoch: 3 | loss: 0.1744123\n",
      "\tspeed: 0.4161s/iter; left time: 81897.4394s\n",
      "3299it [22:51,  2.34it/s]\titers: 3300, epoch: 3 | loss: 0.1241943\n",
      "\tspeed: 0.4209s/iter; left time: 82804.2715s\n",
      "3399it [23:34,  2.41it/s]\titers: 3400, epoch: 3 | loss: 0.1287436\n",
      "\tspeed: 0.4227s/iter; left time: 83118.3598s\n",
      "3499it [24:16,  2.36it/s]\titers: 3500, epoch: 3 | loss: 0.1609690\n",
      "\tspeed: 0.4254s/iter; left time: 83605.0317s\n",
      "3599it [24:59,  2.38it/s]\titers: 3600, epoch: 3 | loss: 0.1046227\n",
      "\tspeed: 0.4251s/iter; left time: 83496.9566s\n",
      "3699it [25:41,  2.38it/s]\titers: 3700, epoch: 3 | loss: 0.3179365\n",
      "\tspeed: 0.4232s/iter; left time: 83087.9585s\n",
      "3799it [26:22,  2.41it/s]\titers: 3800, epoch: 3 | loss: 0.2659004\n",
      "\tspeed: 0.4161s/iter; left time: 81638.6213s\n",
      "3899it [27:04,  2.39it/s]\titers: 3900, epoch: 3 | loss: 0.1409836\n",
      "\tspeed: 0.4164s/iter; left time: 81669.3285s\n",
      "3999it [27:46,  2.40it/s]\titers: 4000, epoch: 3 | loss: 0.1593448\n",
      "\tspeed: 0.4150s/iter; left time: 81344.9024s\n",
      "4099it [28:27,  2.39it/s]\titers: 4100, epoch: 3 | loss: 0.1242076\n",
      "\tspeed: 0.4148s/iter; left time: 81269.0673s\n",
      "4199it [29:09,  2.40it/s]\titers: 4200, epoch: 3 | loss: 0.1653405\n",
      "\tspeed: 0.4168s/iter; left time: 81625.0273s\n",
      "4299it [29:50,  2.40it/s]\titers: 4300, epoch: 3 | loss: 0.0869003\n",
      "\tspeed: 0.4163s/iter; left time: 81480.4931s\n",
      "4399it [30:32,  2.42it/s]\titers: 4400, epoch: 3 | loss: 0.1256846\n",
      "\tspeed: 0.4143s/iter; left time: 81036.2175s\n",
      "4499it [31:14,  2.38it/s]\titers: 4500, epoch: 3 | loss: 0.1193904\n",
      "\tspeed: 0.4174s/iter; left time: 81599.3717s\n",
      "4599it [31:55,  2.38it/s]\titers: 4600, epoch: 3 | loss: 0.1750633\n",
      "\tspeed: 0.4169s/iter; left time: 81467.7857s\n",
      "4699it [32:37,  2.41it/s]\titers: 4700, epoch: 3 | loss: 0.1795508\n",
      "\tspeed: 0.4191s/iter; left time: 81849.5339s\n",
      "4799it [33:19,  2.40it/s]\titers: 4800, epoch: 3 | loss: 0.1199754\n",
      "\tspeed: 0.4200s/iter; left time: 81982.3144s\n",
      "4899it [34:01,  2.38it/s]\titers: 4900, epoch: 3 | loss: 0.1435041\n",
      "\tspeed: 0.4196s/iter; left time: 81868.6552s\n",
      "4999it [34:43,  2.40it/s]\titers: 5000, epoch: 3 | loss: 0.1778907\n",
      "\tspeed: 0.4178s/iter; left time: 81487.7048s\n",
      "5099it [35:25,  2.40it/s]\titers: 5100, epoch: 3 | loss: 0.1810158\n",
      "\tspeed: 0.4180s/iter; left time: 81465.8006s\n",
      "5199it [36:07,  2.37it/s]\titers: 5200, epoch: 3 | loss: 0.1079426\n",
      "\tspeed: 0.4179s/iter; left time: 81419.4698s\n",
      "5299it [36:49,  2.41it/s]\titers: 5300, epoch: 3 | loss: 0.1817649\n",
      "\tspeed: 0.4209s/iter; left time: 81956.9789s\n",
      "5399it [37:30,  2.42it/s]\titers: 5400, epoch: 3 | loss: 0.0916818\n",
      "\tspeed: 0.4184s/iter; left time: 81434.2621s\n",
      "5499it [38:12,  2.41it/s]\titers: 5500, epoch: 3 | loss: 0.1405512\n",
      "\tspeed: 0.4184s/iter; left time: 81393.9580s\n",
      "5599it [38:54,  2.37it/s]\titers: 5600, epoch: 3 | loss: 0.1714265\n",
      "\tspeed: 0.4193s/iter; left time: 81517.1468s\n",
      "5699it [39:36,  2.37it/s]\titers: 5700, epoch: 3 | loss: 0.1304661\n",
      "\tspeed: 0.4184s/iter; left time: 81306.4940s\n",
      "5799it [40:18,  2.38it/s]\titers: 5800, epoch: 3 | loss: 0.1350618\n",
      "\tspeed: 0.4181s/iter; left time: 81194.6128s\n",
      "5899it [41:00,  2.37it/s]\titers: 5900, epoch: 3 | loss: 0.1738965\n",
      "\tspeed: 0.4216s/iter; left time: 81833.2707s\n",
      "5999it [41:42,  2.38it/s]\titers: 6000, epoch: 3 | loss: 0.1595595\n",
      "\tspeed: 0.4175s/iter; left time: 80992.7045s\n",
      "6099it [42:24,  2.39it/s]\titers: 6100, epoch: 3 | loss: 0.1754947\n",
      "\tspeed: 0.4177s/iter; left time: 81003.7440s\n",
      "6199it [43:05,  2.38it/s]\titers: 6200, epoch: 3 | loss: 0.1379329\n",
      "\tspeed: 0.4182s/iter; left time: 81055.6449s\n",
      "6299it [43:47,  2.38it/s]\titers: 6300, epoch: 3 | loss: 0.2167154\n",
      "\tspeed: 0.4188s/iter; left time: 81127.6194s\n",
      "6399it [44:29,  2.37it/s]\titers: 6400, epoch: 3 | loss: 0.2069631\n",
      "\tspeed: 0.4181s/iter; left time: 80949.1275s\n",
      "6499it [45:11,  2.42it/s]\titers: 6500, epoch: 3 | loss: 0.0985121\n",
      "\tspeed: 0.4182s/iter; left time: 80935.0388s\n",
      "6599it [45:53,  2.37it/s]\titers: 6600, epoch: 3 | loss: 0.2035644\n",
      "\tspeed: 0.4187s/iter; left time: 80991.6745s\n",
      "6699it [46:35,  2.40it/s]\titers: 6700, epoch: 3 | loss: 0.1591375\n",
      "\tspeed: 0.4179s/iter; left time: 80795.7466s\n",
      "6799it [47:17,  2.36it/s]\titers: 6800, epoch: 3 | loss: 0.1234892\n",
      "\tspeed: 0.4208s/iter; left time: 81308.4857s\n",
      "6899it [47:59,  2.37it/s]\titers: 6900, epoch: 3 | loss: 0.1082004\n",
      "\tspeed: 0.4188s/iter; left time: 80884.2671s\n",
      "6999it [48:41,  2.39it/s]\titers: 7000, epoch: 3 | loss: 0.1480959\n",
      "\tspeed: 0.4209s/iter; left time: 81232.5971s\n",
      "7099it [49:22,  2.39it/s]\titers: 7100, epoch: 3 | loss: 0.1705837\n",
      "\tspeed: 0.4169s/iter; left time: 80434.1241s\n",
      "7199it [50:04,  2.41it/s]\titers: 7200, epoch: 3 | loss: 0.2807274\n",
      "\tspeed: 0.4168s/iter; left time: 80368.0207s\n",
      "7299it [50:46,  2.39it/s]\titers: 7300, epoch: 3 | loss: 0.2221222\n",
      "\tspeed: 0.4167s/iter; left time: 80314.2725s\n",
      "7399it [51:27,  2.39it/s]\titers: 7400, epoch: 3 | loss: 0.1449561\n",
      "\tspeed: 0.4186s/iter; left time: 80628.5047s\n",
      "7499it [52:09,  2.39it/s]\titers: 7500, epoch: 3 | loss: 0.1573366\n",
      "\tspeed: 0.4182s/iter; left time: 80510.4321s\n",
      "7599it [52:51,  2.39it/s]\titers: 7600, epoch: 3 | loss: 0.0933883\n",
      "\tspeed: 0.4191s/iter; left time: 80642.4195s\n",
      "7699it [53:33,  2.40it/s]\titers: 7700, epoch: 3 | loss: 0.2718772\n",
      "\tspeed: 0.4174s/iter; left time: 80263.8959s\n",
      "7799it [54:15,  2.38it/s]\titers: 7800, epoch: 3 | loss: 0.1204808\n",
      "\tspeed: 0.4183s/iter; left time: 80412.4880s\n",
      "7899it [54:57,  2.40it/s]\titers: 7900, epoch: 3 | loss: 0.1505170\n",
      "\tspeed: 0.4174s/iter; left time: 80195.0687s\n",
      "7999it [55:38,  2.40it/s]\titers: 8000, epoch: 3 | loss: 0.2024041\n",
      "\tspeed: 0.4163s/iter; left time: 79930.9282s\n",
      "8099it [56:20,  2.39it/s]\titers: 8100, epoch: 3 | loss: 0.1154343\n",
      "\tspeed: 0.4184s/iter; left time: 80305.3488s\n",
      "8199it [57:02,  2.37it/s]\titers: 8200, epoch: 3 | loss: 0.1256891\n",
      "\tspeed: 0.4180s/iter; left time: 80173.8629s\n",
      "8299it [57:44,  2.38it/s]\titers: 8300, epoch: 3 | loss: 0.1991487\n",
      "\tspeed: 0.4181s/iter; left time: 80147.7980s\n",
      "8399it [58:26,  2.30it/s]\titers: 8400, epoch: 3 | loss: 0.1484776\n",
      "\tspeed: 0.4199s/iter; left time: 80468.8266s\n",
      "8499it [59:08,  2.37it/s]\titers: 8500, epoch: 3 | loss: 0.2849517\n",
      "\tspeed: 0.4229s/iter; left time: 81000.8715s\n",
      "8599it [59:50,  2.38it/s]\titers: 8600, epoch: 3 | loss: 0.1981317\n",
      "\tspeed: 0.4192s/iter; left time: 80248.3981s\n",
      "8699it [1:00:32,  2.39it/s]\titers: 8700, epoch: 3 | loss: 0.0806941\n",
      "\tspeed: 0.4205s/iter; left time: 80442.7560s\n",
      "8799it [1:01:14,  2.39it/s]\titers: 8800, epoch: 3 | loss: 0.2020537\n",
      "\tspeed: 0.4206s/iter; left time: 80429.6791s\n",
      "8899it [1:01:56,  2.39it/s]\titers: 8900, epoch: 3 | loss: 0.1068323\n",
      "\tspeed: 0.4181s/iter; left time: 79900.5201s\n",
      "8999it [1:02:38,  2.39it/s]\titers: 9000, epoch: 3 | loss: 0.2218529\n",
      "\tspeed: 0.4178s/iter; left time: 79800.4178s\n",
      "9099it [1:03:19,  2.38it/s]\titers: 9100, epoch: 3 | loss: 0.1005588\n",
      "\tspeed: 0.4177s/iter; left time: 79751.4926s\n",
      "9199it [1:04:01,  2.39it/s]\titers: 9200, epoch: 3 | loss: 0.1325813\n",
      "\tspeed: 0.4187s/iter; left time: 79887.7231s\n",
      "9299it [1:04:43,  2.41it/s]\titers: 9300, epoch: 3 | loss: 0.1077472\n",
      "\tspeed: 0.4185s/iter; left time: 79807.4421s\n",
      "9399it [1:05:25,  2.40it/s]\titers: 9400, epoch: 3 | loss: 0.1258620\n",
      "\tspeed: 0.4158s/iter; left time: 79249.0206s\n",
      "9499it [1:06:06,  2.42it/s]\titers: 9500, epoch: 3 | loss: 0.1479212\n",
      "\tspeed: 0.4148s/iter; left time: 79031.9410s\n",
      "9599it [1:06:48,  2.42it/s]\titers: 9600, epoch: 3 | loss: 0.1493704\n",
      "\tspeed: 0.4146s/iter; left time: 78937.4680s\n",
      "9699it [1:07:29,  2.41it/s]\titers: 9700, epoch: 3 | loss: 0.2428837\n",
      "\tspeed: 0.4149s/iter; left time: 78962.2202s\n",
      "9799it [1:08:10,  2.42it/s]\titers: 9800, epoch: 3 | loss: 0.2351782\n",
      "\tspeed: 0.4149s/iter; left time: 78912.3526s\n",
      "9899it [1:08:52,  2.41it/s]\titers: 9900, epoch: 3 | loss: 0.0857674\n",
      "\tspeed: 0.4155s/iter; left time: 78985.8878s\n",
      "9999it [1:09:34,  2.42it/s]\titers: 10000, epoch: 3 | loss: 0.3115132\n",
      "\tspeed: 0.4153s/iter; left time: 78922.9119s\n",
      "10099it [1:10:15,  2.43it/s]\titers: 10100, epoch: 3 | loss: 0.1327861\n",
      "\tspeed: 0.4151s/iter; left time: 78831.5256s\n",
      "10199it [1:10:57,  2.42it/s]\titers: 10200, epoch: 3 | loss: 0.1337738\n",
      "\tspeed: 0.4150s/iter; left time: 78778.8576s\n",
      "10299it [1:11:38,  2.42it/s]\titers: 10300, epoch: 3 | loss: 0.1736941\n",
      "\tspeed: 0.4159s/iter; left time: 78903.0287s\n",
      "10399it [1:12:20,  2.40it/s]\titers: 10400, epoch: 3 | loss: 0.1925260\n",
      "\tspeed: 0.4148s/iter; left time: 78659.8089s\n",
      "10499it [1:13:01,  2.40it/s]\titers: 10500, epoch: 3 | loss: 0.1358135\n",
      "\tspeed: 0.4152s/iter; left time: 78688.5591s\n",
      "10599it [1:13:43,  2.41it/s]\titers: 10600, epoch: 3 | loss: 0.1096081\n",
      "\tspeed: 0.4150s/iter; left time: 78610.8098s\n",
      "10699it [1:14:24,  2.40it/s]\titers: 10700, epoch: 3 | loss: 0.2470344\n",
      "\tspeed: 0.4145s/iter; left time: 78469.6917s\n",
      "10799it [1:15:06,  2.40it/s]\titers: 10800, epoch: 3 | loss: 0.1509354\n",
      "\tspeed: 0.4156s/iter; left time: 78631.8949s\n",
      "10899it [1:15:47,  2.43it/s]\titers: 10900, epoch: 3 | loss: 0.1774833\n",
      "\tspeed: 0.4156s/iter; left time: 78601.9628s\n",
      "10999it [1:16:29,  2.39it/s]\titers: 11000, epoch: 3 | loss: 0.1974659\n",
      "\tspeed: 0.4163s/iter; left time: 78693.3830s\n",
      "11099it [1:17:10,  2.40it/s]\titers: 11100, epoch: 3 | loss: 0.1106689\n",
      "\tspeed: 0.4156s/iter; left time: 78519.2015s\n",
      "11112it [1:17:16,  2.40it/s]\n",
      "Epoch: 3 cost time: 4636.408310413361\n",
      "2403it [07:43,  5.18it/s]\n",
      "2394it [07:41,  5.19it/s]\n",
      "Epoch: 3 | Train Loss: 0.1724091 Vali Loss: 0.5412295 Test Loss: 0.7183052 MAE Loss: 0.5192307\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:41,  2.42it/s]\titers: 100, epoch: 4 | loss: 0.1012936\n",
      "\tspeed: 9.7241s/iter; left time: 1835964.6552s\n",
      "199it [01:22,  2.40it/s]\titers: 200, epoch: 4 | loss: 0.0652638\n",
      "\tspeed: 0.4150s/iter; left time: 78318.6087s\n",
      "299it [02:04,  2.40it/s]\titers: 300, epoch: 4 | loss: 0.1527784\n",
      "\tspeed: 0.4155s/iter; left time: 78356.8670s\n",
      "399it [02:45,  2.42it/s]\titers: 400, epoch: 4 | loss: 0.1069076\n",
      "\tspeed: 0.4146s/iter; left time: 78155.4288s\n",
      "499it [03:27,  2.43it/s]\titers: 500, epoch: 4 | loss: 0.1434005\n",
      "\tspeed: 0.4146s/iter; left time: 78120.7388s\n",
      "599it [04:08,  2.40it/s]\titers: 600, epoch: 4 | loss: 0.1489300\n",
      "\tspeed: 0.4141s/iter; left time: 77982.9272s\n",
      "699it [04:50,  2.41it/s]\titers: 700, epoch: 4 | loss: 0.0742263\n",
      "\tspeed: 0.4156s/iter; left time: 78221.4247s\n",
      "799it [05:31,  2.40it/s]\titers: 800, epoch: 4 | loss: 0.1219792\n",
      "\tspeed: 0.4148s/iter; left time: 78018.9179s\n",
      "899it [06:13,  2.41it/s]\titers: 900, epoch: 4 | loss: 0.0937192\n",
      "\tspeed: 0.4158s/iter; left time: 78166.6033s\n",
      "999it [06:55,  2.40it/s]\titers: 1000, epoch: 4 | loss: 0.2275370\n",
      "\tspeed: 0.4158s/iter; left time: 78124.1762s\n",
      "1099it [07:36,  2.40it/s]\titers: 1100, epoch: 4 | loss: 0.2038683\n",
      "\tspeed: 0.4146s/iter; left time: 77858.3350s\n",
      "1199it [08:17,  2.42it/s]\titers: 1200, epoch: 4 | loss: 0.0582432\n",
      "\tspeed: 0.4149s/iter; left time: 77874.4577s\n",
      "1299it [08:59,  2.42it/s]\titers: 1300, epoch: 4 | loss: 0.1686001\n",
      "\tspeed: 0.4152s/iter; left time: 77894.9396s\n",
      "1399it [09:41,  2.40it/s]\titers: 1400, epoch: 4 | loss: 0.1092873\n",
      "\tspeed: 0.4153s/iter; left time: 77865.5379s\n",
      "1499it [10:22,  2.42it/s]\titers: 1500, epoch: 4 | loss: 0.1642672\n",
      "\tspeed: 0.4155s/iter; left time: 77864.5688s\n",
      "1599it [11:04,  2.40it/s]\titers: 1600, epoch: 4 | loss: 0.1125792\n",
      "\tspeed: 0.4154s/iter; left time: 77805.8266s\n",
      "1699it [11:45,  2.41it/s]\titers: 1700, epoch: 4 | loss: 0.2014031\n",
      "\tspeed: 0.4162s/iter; left time: 77915.2533s\n",
      "1799it [12:27,  2.42it/s]\titers: 1800, epoch: 4 | loss: 0.1181526\n",
      "\tspeed: 0.4144s/iter; left time: 77533.9652s\n",
      "1899it [13:08,  2.41it/s]\titers: 1900, epoch: 4 | loss: 0.3060267\n",
      "\tspeed: 0.4147s/iter; left time: 77544.8301s\n",
      "1999it [13:50,  2.40it/s]\titers: 2000, epoch: 4 | loss: 0.1501505\n",
      "\tspeed: 0.4142s/iter; left time: 77413.9813s\n",
      "2099it [14:31,  2.42it/s]\titers: 2100, epoch: 4 | loss: 0.1609570\n",
      "\tspeed: 0.4142s/iter; left time: 77369.7587s\n",
      "2199it [15:12,  2.39it/s]\titers: 2200, epoch: 4 | loss: 0.1755734\n",
      "\tspeed: 0.4146s/iter; left time: 77401.4519s\n",
      "2299it [15:54,  2.40it/s]\titers: 2300, epoch: 4 | loss: 0.1564468\n",
      "\tspeed: 0.4156s/iter; left time: 77545.4617s\n",
      "2399it [16:35,  2.41it/s]\titers: 2400, epoch: 4 | loss: 0.1168640\n",
      "\tspeed: 0.4151s/iter; left time: 77410.4763s\n",
      "2499it [17:17,  2.42it/s]\titers: 2500, epoch: 4 | loss: 0.0813184\n",
      "\tspeed: 0.4154s/iter; left time: 77424.3182s\n",
      "2599it [17:58,  2.41it/s]\titers: 2600, epoch: 4 | loss: 0.0931918\n",
      "\tspeed: 0.4146s/iter; left time: 77250.3171s\n",
      "2699it [18:40,  2.40it/s]\titers: 2700, epoch: 4 | loss: 0.1100467\n",
      "\tspeed: 0.4164s/iter; left time: 77532.7447s\n",
      "2799it [19:22,  2.41it/s]\titers: 2800, epoch: 4 | loss: 0.1484302\n",
      "\tspeed: 0.4146s/iter; left time: 77165.2784s\n",
      "2899it [20:03,  2.40it/s]\titers: 2900, epoch: 4 | loss: 0.1949035\n",
      "\tspeed: 0.4140s/iter; left time: 77011.2371s\n",
      "2999it [20:45,  2.40it/s]\titers: 3000, epoch: 4 | loss: 0.2517786\n",
      "\tspeed: 0.4165s/iter; left time: 77437.6861s\n",
      "3099it [21:26,  2.41it/s]\titers: 3100, epoch: 4 | loss: 0.1438765\n",
      "\tspeed: 0.4157s/iter; left time: 77240.7420s\n",
      "3199it [22:08,  2.39it/s]\titers: 3200, epoch: 4 | loss: 0.2457809\n",
      "\tspeed: 0.4158s/iter; left time: 77215.9418s\n",
      "3299it [22:49,  2.41it/s]\titers: 3300, epoch: 4 | loss: 0.1380180\n",
      "\tspeed: 0.4151s/iter; left time: 77045.6904s\n",
      "3399it [23:31,  2.44it/s]\titers: 3400, epoch: 4 | loss: 0.2081069\n",
      "\tspeed: 0.4150s/iter; left time: 76987.1234s\n",
      "3499it [24:12,  2.40it/s]\titers: 3500, epoch: 4 | loss: 0.1354339\n",
      "\tspeed: 0.4152s/iter; left time: 76977.3159s\n",
      "3599it [24:54,  2.40it/s]\titers: 3600, epoch: 4 | loss: 0.1389515\n",
      "\tspeed: 0.4148s/iter; left time: 76873.6116s\n",
      "3699it [25:35,  2.42it/s]\titers: 3700, epoch: 4 | loss: 0.1494206\n",
      "\tspeed: 0.4152s/iter; left time: 76893.3057s\n",
      "3799it [26:17,  2.40it/s]\titers: 3800, epoch: 4 | loss: 0.2537641\n",
      "\tspeed: 0.4150s/iter; left time: 76823.5090s\n",
      "3899it [26:58,  2.40it/s]\titers: 3900, epoch: 4 | loss: 0.2773714\n",
      "\tspeed: 0.4147s/iter; left time: 76722.6727s\n",
      "3999it [27:40,  2.41it/s]\titers: 4000, epoch: 4 | loss: 0.1771640\n",
      "\tspeed: 0.4152s/iter; left time: 76781.0251s\n",
      "4099it [28:21,  2.38it/s]\titers: 4100, epoch: 4 | loss: 0.1144396\n",
      "\tspeed: 0.4165s/iter; left time: 76976.7868s\n",
      "4199it [29:03,  2.43it/s]\titers: 4200, epoch: 4 | loss: 0.1635796\n",
      "\tspeed: 0.4155s/iter; left time: 76743.9881s\n",
      "4299it [29:45,  2.42it/s]\titers: 4300, epoch: 4 | loss: 0.0699783\n",
      "\tspeed: 0.4151s/iter; left time: 76625.6914s\n",
      "4399it [30:26,  2.42it/s]\titers: 4400, epoch: 4 | loss: 0.1351004\n",
      "\tspeed: 0.4164s/iter; left time: 76834.0785s\n",
      "4499it [31:08,  2.41it/s]\titers: 4500, epoch: 4 | loss: 0.1910454\n",
      "\tspeed: 0.4146s/iter; left time: 76458.5382s\n",
      "4599it [31:49,  2.41it/s]\titers: 4600, epoch: 4 | loss: 0.2159099\n",
      "\tspeed: 0.4153s/iter; left time: 76545.3761s\n",
      "4699it [32:31,  2.40it/s]\titers: 4700, epoch: 4 | loss: 0.1405540\n",
      "\tspeed: 0.4145s/iter; left time: 76344.5755s\n",
      "4799it [33:12,  2.42it/s]\titers: 4800, epoch: 4 | loss: 0.1101416\n",
      "\tspeed: 0.4150s/iter; left time: 76411.1033s\n",
      "4899it [33:54,  2.40it/s]\titers: 4900, epoch: 4 | loss: 0.1899449\n",
      "\tspeed: 0.4149s/iter; left time: 76345.3104s\n",
      "4999it [34:35,  2.43it/s]\titers: 5000, epoch: 4 | loss: 0.1688189\n",
      "\tspeed: 0.4148s/iter; left time: 76283.2039s\n",
      "5099it [35:16,  2.43it/s]\titers: 5100, epoch: 4 | loss: 0.1186070\n",
      "\tspeed: 0.4140s/iter; left time: 76091.8284s\n",
      "5199it [35:58,  2.43it/s]\titers: 5200, epoch: 4 | loss: 0.1263182\n",
      "\tspeed: 0.4147s/iter; left time: 76181.3519s\n",
      "5299it [36:39,  2.40it/s]\titers: 5300, epoch: 4 | loss: 0.1166640\n",
      "\tspeed: 0.4150s/iter; left time: 76189.9185s\n",
      "5399it [37:21,  2.42it/s]\titers: 5400, epoch: 4 | loss: 0.1386712\n",
      "\tspeed: 0.4135s/iter; left time: 75887.7381s\n",
      "5499it [38:02,  2.43it/s]\titers: 5500, epoch: 4 | loss: 0.0832743\n",
      "\tspeed: 0.4148s/iter; left time: 76081.8936s\n",
      "5599it [38:44,  2.39it/s]\titers: 5600, epoch: 4 | loss: 0.1229360\n",
      "\tspeed: 0.4143s/iter; left time: 75949.6821s\n",
      "5699it [39:25,  2.42it/s]\titers: 5700, epoch: 4 | loss: 0.1174342\n",
      "\tspeed: 0.4138s/iter; left time: 75812.0131s\n",
      "5799it [40:06,  2.41it/s]\titers: 5800, epoch: 4 | loss: 0.1801942\n",
      "\tspeed: 0.4137s/iter; left time: 75758.3273s\n",
      "5899it [40:48,  2.41it/s]\titers: 5900, epoch: 4 | loss: 0.1524122\n",
      "\tspeed: 0.4144s/iter; left time: 75833.4421s\n",
      "5999it [41:29,  2.40it/s]\titers: 6000, epoch: 4 | loss: 0.1105933\n",
      "\tspeed: 0.4154s/iter; left time: 75987.3726s\n",
      "6099it [42:11,  2.39it/s]\titers: 6100, epoch: 4 | loss: 0.1536725\n",
      "\tspeed: 0.4151s/iter; left time: 75873.6796s\n",
      "6199it [42:53,  2.41it/s]\titers: 6200, epoch: 4 | loss: 0.1352343\n",
      "\tspeed: 0.4155s/iter; left time: 75912.1968s\n",
      "6299it [43:34,  2.43it/s]\titers: 6300, epoch: 4 | loss: 0.1879131\n",
      "\tspeed: 0.4145s/iter; left time: 75697.6200s\n",
      "6399it [44:15,  2.42it/s]\titers: 6400, epoch: 4 | loss: 0.1893851\n",
      "\tspeed: 0.4149s/iter; left time: 75726.8471s\n",
      "6499it [44:57,  2.41it/s]\titers: 6500, epoch: 4 | loss: 0.1486067\n",
      "\tspeed: 0.4152s/iter; left time: 75735.5558s\n",
      "6599it [45:39,  2.35it/s]\titers: 6600, epoch: 4 | loss: 0.1920364\n",
      "\tspeed: 0.4191s/iter; left time: 76395.5078s\n",
      "6699it [46:20,  2.40it/s]\titers: 6700, epoch: 4 | loss: 0.0974353\n",
      "\tspeed: 0.4151s/iter; left time: 75632.7681s\n",
      "6799it [47:02,  2.42it/s]\titers: 6800, epoch: 4 | loss: 0.1493663\n",
      "\tspeed: 0.4160s/iter; left time: 75760.9511s\n",
      "6899it [47:44,  2.41it/s]\titers: 6900, epoch: 4 | loss: 0.0848516\n",
      "\tspeed: 0.4151s/iter; left time: 75551.3907s\n",
      "6999it [48:25,  2.39it/s]\titers: 7000, epoch: 4 | loss: 0.2211801\n",
      "\tspeed: 0.4157s/iter; left time: 75609.9214s\n",
      "7099it [49:07,  2.40it/s]\titers: 7100, epoch: 4 | loss: 0.1146707\n",
      "\tspeed: 0.4159s/iter; left time: 75613.0179s\n",
      "7199it [49:48,  2.41it/s]\titers: 7200, epoch: 4 | loss: 0.1748883\n",
      "\tspeed: 0.4148s/iter; left time: 75362.5198s\n",
      "7299it [50:30,  2.41it/s]\titers: 7300, epoch: 4 | loss: 0.1822684\n",
      "\tspeed: 0.4155s/iter; left time: 75461.3284s\n",
      "7399it [51:11,  2.41it/s]\titers: 7400, epoch: 4 | loss: 0.1403108\n",
      "\tspeed: 0.4159s/iter; left time: 75492.6536s\n",
      "7499it [51:53,  2.41it/s]\titers: 7500, epoch: 4 | loss: 0.1036443\n",
      "\tspeed: 0.4166s/iter; left time: 75568.5444s\n",
      "7599it [52:35,  2.40it/s]\titers: 7600, epoch: 4 | loss: 0.1920288\n",
      "\tspeed: 0.4160s/iter; left time: 75424.7217s\n",
      "7699it [53:16,  2.40it/s]\titers: 7700, epoch: 4 | loss: 0.1120195\n",
      "\tspeed: 0.4168s/iter; left time: 75528.4513s\n",
      "7799it [53:58,  2.41it/s]\titers: 7800, epoch: 4 | loss: 0.1057369\n",
      "\tspeed: 0.4172s/iter; left time: 75558.2859s\n",
      "7899it [54:40,  2.41it/s]\titers: 7900, epoch: 4 | loss: 0.1180235\n",
      "\tspeed: 0.4158s/iter; left time: 75268.1437s\n",
      "7999it [55:21,  2.42it/s]\titers: 8000, epoch: 4 | loss: 0.2345064\n",
      "\tspeed: 0.4160s/iter; left time: 75261.8160s\n",
      "8099it [56:03,  2.41it/s]\titers: 8100, epoch: 4 | loss: 0.1420816\n",
      "\tspeed: 0.4155s/iter; left time: 75116.1974s\n",
      "8199it [56:44,  2.38it/s]\titers: 8200, epoch: 4 | loss: 0.2484317\n",
      "\tspeed: 0.4161s/iter; left time: 75183.6423s\n",
      "8299it [57:26,  2.43it/s]\titers: 8300, epoch: 4 | loss: 0.2029040\n",
      "\tspeed: 0.4169s/iter; left time: 75296.6463s\n",
      "8399it [58:08,  2.38it/s]\titers: 8400, epoch: 4 | loss: 0.1540239\n",
      "\tspeed: 0.4180s/iter; left time: 75447.2589s\n",
      "8499it [58:50,  2.38it/s]\titers: 8500, epoch: 4 | loss: 0.1464683\n",
      "\tspeed: 0.4216s/iter; left time: 76053.7549s\n",
      "8599it [59:32,  2.38it/s]\titers: 8600, epoch: 4 | loss: 0.0900600\n",
      "\tspeed: 0.4203s/iter; left time: 75779.4505s\n",
      "8699it [1:00:14,  2.36it/s]\titers: 8700, epoch: 4 | loss: 0.1643771\n",
      "\tspeed: 0.4203s/iter; left time: 75742.0948s\n",
      "8799it [1:00:56,  2.38it/s]\titers: 8800, epoch: 4 | loss: 0.1596625\n",
      "\tspeed: 0.4175s/iter; left time: 75197.5789s\n",
      "8899it [1:01:37,  2.39it/s]\titers: 8900, epoch: 4 | loss: 0.1123981\n",
      "\tspeed: 0.4162s/iter; left time: 74920.2262s\n",
      "8999it [1:02:19,  2.39it/s]\titers: 9000, epoch: 4 | loss: 0.2157497\n",
      "\tspeed: 0.4165s/iter; left time: 74930.1946s\n",
      "9099it [1:03:01,  2.41it/s]\titers: 9100, epoch: 4 | loss: 0.1438991\n",
      "\tspeed: 0.4152s/iter; left time: 74652.0189s\n",
      "9199it [1:03:42,  2.40it/s]\titers: 9200, epoch: 4 | loss: 0.1117752\n",
      "\tspeed: 0.4146s/iter; left time: 74513.9407s\n",
      "9299it [1:04:23,  2.41it/s]\titers: 9300, epoch: 4 | loss: 0.1064036\n",
      "\tspeed: 0.4150s/iter; left time: 74542.7915s\n",
      "9399it [1:05:05,  2.40it/s]\titers: 9400, epoch: 4 | loss: 0.1825961\n",
      "\tspeed: 0.4141s/iter; left time: 74332.0663s\n",
      "9499it [1:05:46,  2.41it/s]\titers: 9500, epoch: 4 | loss: 0.0556809\n",
      "\tspeed: 0.4150s/iter; left time: 74460.1929s\n",
      "9599it [1:06:28,  2.43it/s]\titers: 9600, epoch: 4 | loss: 0.1823741\n",
      "\tspeed: 0.4180s/iter; left time: 74946.3867s\n",
      "9699it [1:07:10,  2.40it/s]\titers: 9700, epoch: 4 | loss: 0.0967442\n",
      "\tspeed: 0.4147s/iter; left time: 74308.4665s\n",
      "9799it [1:07:51,  2.41it/s]\titers: 9800, epoch: 4 | loss: 0.1265950\n",
      "\tspeed: 0.4152s/iter; left time: 74358.9451s\n",
      "9899it [1:08:33,  2.40it/s]\titers: 9900, epoch: 4 | loss: 0.1938934\n",
      "\tspeed: 0.4161s/iter; left time: 74475.5211s\n",
      "9999it [1:09:14,  2.41it/s]\titers: 10000, epoch: 4 | loss: 0.1855404\n",
      "\tspeed: 0.4157s/iter; left time: 74366.9307s\n",
      "10099it [1:09:56,  2.41it/s]\titers: 10100, epoch: 4 | loss: 0.1548697\n",
      "\tspeed: 0.4151s/iter; left time: 74217.7722s\n",
      "10199it [1:10:37,  2.41it/s]\titers: 10200, epoch: 4 | loss: 0.1839354\n",
      "\tspeed: 0.4153s/iter; left time: 74210.2876s\n",
      "10299it [1:11:19,  2.42it/s]\titers: 10300, epoch: 4 | loss: 0.1452067\n",
      "\tspeed: 0.4146s/iter; left time: 74058.2723s\n",
      "10399it [1:12:00,  2.40it/s]\titers: 10400, epoch: 4 | loss: 0.1239675\n",
      "\tspeed: 0.4147s/iter; left time: 74031.5945s\n",
      "10499it [1:12:42,  2.41it/s]\titers: 10500, epoch: 4 | loss: 0.1320890\n",
      "\tspeed: 0.4157s/iter; left time: 74164.5807s\n",
      "10599it [1:13:23,  2.44it/s]\titers: 10600, epoch: 4 | loss: 0.2089911\n",
      "\tspeed: 0.4149s/iter; left time: 73987.4160s\n",
      "10699it [1:14:05,  2.43it/s]\titers: 10700, epoch: 4 | loss: 0.1086294\n",
      "\tspeed: 0.4148s/iter; left time: 73926.8290s\n",
      "10799it [1:14:46,  2.39it/s]\titers: 10800, epoch: 4 | loss: 0.1406850\n",
      "\tspeed: 0.4158s/iter; left time: 74059.4114s\n",
      "10899it [1:15:28,  2.40it/s]\titers: 10900, epoch: 4 | loss: 0.2216805\n",
      "\tspeed: 0.4146s/iter; left time: 73794.8390s\n",
      "10999it [1:16:09,  2.40it/s]\titers: 11000, epoch: 4 | loss: 0.0666457\n",
      "\tspeed: 0.4155s/iter; left time: 73927.7463s\n",
      "11099it [1:16:51,  2.40it/s]\titers: 11100, epoch: 4 | loss: 0.1670696\n",
      "\tspeed: 0.4151s/iter; left time: 73797.9655s\n",
      "11112it [1:16:56,  2.41it/s]\n",
      "Epoch: 4 cost time: 4616.936789512634\n",
      "2403it [07:44,  5.17it/s]\n",
      "2394it [07:42,  5.18it/s]\n",
      "Epoch: 4 | Train Loss: 0.1515254 Vali Loss: 0.5261233 Test Loss: 0.7103973 MAE Loss: 0.5159250\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:40,  2.45it/s]\titers: 100, epoch: 5 | loss: 0.0994111\n",
      "\tspeed: 10.8471s/iter; left time: 1927455.9739s\n",
      "199it [01:21,  2.42it/s]\titers: 200, epoch: 5 | loss: 0.0772695\n",
      "\tspeed: 0.4110s/iter; left time: 72992.4504s\n",
      "299it [02:03,  2.43it/s]\titers: 300, epoch: 5 | loss: 0.0904963\n",
      "\tspeed: 0.4119s/iter; left time: 73100.8131s\n",
      "399it [02:44,  2.42it/s]\titers: 400, epoch: 5 | loss: 0.1078293\n",
      "\tspeed: 0.4135s/iter; left time: 73345.8567s\n",
      "499it [03:25,  2.41it/s]\titers: 500, epoch: 5 | loss: 0.1609081\n",
      "\tspeed: 0.4146s/iter; left time: 73510.9988s\n",
      "599it [04:07,  2.43it/s]\titers: 600, epoch: 5 | loss: 0.0992907\n",
      "\tspeed: 0.4137s/iter; left time: 73298.1514s\n",
      "699it [04:48,  2.41it/s]\titers: 700, epoch: 5 | loss: 0.0947273\n",
      "\tspeed: 0.4138s/iter; left time: 73277.7400s\n",
      "799it [05:30,  2.40it/s]\titers: 800, epoch: 5 | loss: 0.0934523\n",
      "\tspeed: 0.4154s/iter; left time: 73518.3862s\n",
      "899it [06:11,  2.43it/s]\titers: 900, epoch: 5 | loss: 0.1397657\n",
      "\tspeed: 0.4162s/iter; left time: 73628.6076s\n",
      "999it [06:53,  2.38it/s]\titers: 1000, epoch: 5 | loss: 0.1187636\n",
      "\tspeed: 0.4159s/iter; left time: 73533.0711s\n",
      "1099it [07:34,  2.39it/s]\titers: 1100, epoch: 5 | loss: 0.1009278\n",
      "\tspeed: 0.4152s/iter; left time: 73370.2813s\n",
      "1199it [08:16,  2.41it/s]\titers: 1200, epoch: 5 | loss: 0.1425256\n",
      "\tspeed: 0.4161s/iter; left time: 73472.5762s\n",
      "1299it [08:57,  2.39it/s]\titers: 1300, epoch: 5 | loss: 0.2037637\n",
      "\tspeed: 0.4150s/iter; left time: 73241.5624s\n",
      "1399it [09:39,  2.40it/s]\titers: 1400, epoch: 5 | loss: 0.1268691\n",
      "\tspeed: 0.4148s/iter; left time: 73164.9470s\n",
      "1499it [10:20,  2.42it/s]\titers: 1500, epoch: 5 | loss: 0.0953779\n",
      "\tspeed: 0.4150s/iter; left time: 73167.4824s\n",
      "1599it [11:02,  2.42it/s]\titers: 1600, epoch: 5 | loss: 0.0921455\n",
      "\tspeed: 0.4149s/iter; left time: 73096.9807s\n",
      "1699it [11:44,  2.41it/s]\titers: 1700, epoch: 5 | loss: 0.1395869\n",
      "\tspeed: 0.4156s/iter; left time: 73185.3200s\n",
      "1799it [12:25,  2.41it/s]\titers: 1800, epoch: 5 | loss: 0.1559575\n",
      "\tspeed: 0.4159s/iter; left time: 73188.2760s\n",
      "1899it [13:07,  2.41it/s]\titers: 1900, epoch: 5 | loss: 0.1482582\n",
      "\tspeed: 0.4158s/iter; left time: 73144.9376s\n",
      "1999it [13:48,  2.40it/s]\titers: 2000, epoch: 5 | loss: 0.0919582\n",
      "\tspeed: 0.4158s/iter; left time: 73102.8783s\n",
      "2099it [14:30,  2.40it/s]\titers: 2100, epoch: 5 | loss: 0.2126956\n",
      "\tspeed: 0.4155s/iter; left time: 73007.0091s\n",
      "2199it [15:11,  2.41it/s]\titers: 2200, epoch: 5 | loss: 0.1003983\n",
      "\tspeed: 0.4160s/iter; left time: 73050.5264s\n",
      "2299it [15:53,  2.43it/s]\titers: 2300, epoch: 5 | loss: 0.1650820\n",
      "\tspeed: 0.4142s/iter; left time: 72693.8727s\n",
      "2399it [16:34,  2.40it/s]\titers: 2400, epoch: 5 | loss: 0.1380780\n",
      "\tspeed: 0.4156s/iter; left time: 72885.4302s\n",
      "2499it [17:16,  2.41it/s]\titers: 2500, epoch: 5 | loss: 0.1080687\n",
      "\tspeed: 0.4165s/iter; left time: 73017.7646s\n",
      "2599it [17:58,  2.39it/s]\titers: 2600, epoch: 5 | loss: 0.1576822\n",
      "\tspeed: 0.4159s/iter; left time: 72864.8336s\n",
      "2699it [18:39,  2.40it/s]\titers: 2700, epoch: 5 | loss: 0.1671578\n",
      "\tspeed: 0.4168s/iter; left time: 72977.5490s\n",
      "2799it [19:21,  2.40it/s]\titers: 2800, epoch: 5 | loss: 0.2248096\n",
      "\tspeed: 0.4156s/iter; left time: 72728.2998s\n",
      "2899it [20:03,  2.38it/s]\titers: 2900, epoch: 5 | loss: 0.1031551\n",
      "\tspeed: 0.4181s/iter; left time: 73119.5108s\n",
      "2999it [20:45,  2.39it/s]\titers: 3000, epoch: 5 | loss: 0.1201722\n",
      "\tspeed: 0.4184s/iter; left time: 73125.8385s\n",
      "3099it [21:26,  2.41it/s]\titers: 3100, epoch: 5 | loss: 0.1582423\n",
      "\tspeed: 0.4170s/iter; left time: 72855.5647s\n",
      "3199it [22:08,  2.40it/s]\titers: 3200, epoch: 5 | loss: 0.1404586\n",
      "\tspeed: 0.4168s/iter; left time: 72774.8040s\n",
      "3299it [22:50,  2.36it/s]\titers: 3300, epoch: 5 | loss: 0.1537190\n",
      "\tspeed: 0.4179s/iter; left time: 72918.7733s\n",
      "3399it [23:31,  2.41it/s]\titers: 3400, epoch: 5 | loss: 0.1880650\n",
      "\tspeed: 0.4164s/iter; left time: 72620.6906s\n",
      "3499it [24:13,  2.36it/s]\titers: 3500, epoch: 5 | loss: 0.1493383\n",
      "\tspeed: 0.4182s/iter; left time: 72895.8632s\n",
      "3599it [24:55,  2.38it/s]\titers: 3600, epoch: 5 | loss: 0.1360827\n",
      "\tspeed: 0.4195s/iter; left time: 73081.2680s\n",
      "3699it [25:37,  2.40it/s]\titers: 3700, epoch: 5 | loss: 0.2118748\n",
      "\tspeed: 0.4217s/iter; left time: 73406.5334s\n",
      "3799it [26:19,  2.40it/s]\titers: 3800, epoch: 5 | loss: 0.0716480\n",
      "\tspeed: 0.4187s/iter; left time: 72842.2010s\n",
      "3899it [27:01,  2.38it/s]\titers: 3900, epoch: 5 | loss: 0.0962503\n",
      "\tspeed: 0.4185s/iter; left time: 72767.4873s\n",
      "3999it [27:43,  2.39it/s]\titers: 4000, epoch: 5 | loss: 0.0967430\n",
      "\tspeed: 0.4171s/iter; left time: 72489.6222s\n",
      "4099it [28:24,  2.40it/s]\titers: 4100, epoch: 5 | loss: 0.1867458\n",
      "\tspeed: 0.4154s/iter; left time: 72159.4819s\n",
      "4199it [29:06,  2.39it/s]\titers: 4200, epoch: 5 | loss: 0.1277389\n",
      "\tspeed: 0.4169s/iter; left time: 72362.2664s\n",
      "4299it [29:48,  2.41it/s]\titers: 4300, epoch: 5 | loss: 0.0889440\n",
      "\tspeed: 0.4173s/iter; left time: 72403.1719s\n",
      "4399it [30:29,  2.39it/s]\titers: 4400, epoch: 5 | loss: 0.1000180\n",
      "\tspeed: 0.4165s/iter; left time: 72218.4009s\n",
      "4499it [31:11,  2.40it/s]\titers: 4500, epoch: 5 | loss: 0.1843119\n",
      "\tspeed: 0.4176s/iter; left time: 72358.8844s\n",
      "4599it [31:53,  2.39it/s]\titers: 4600, epoch: 5 | loss: 0.1178630\n",
      "\tspeed: 0.4168s/iter; left time: 72180.7564s\n",
      "4699it [32:35,  2.42it/s]\titers: 4700, epoch: 5 | loss: 0.1563264\n",
      "\tspeed: 0.4179s/iter; left time: 72329.2258s\n",
      "4799it [33:16,  2.41it/s]\titers: 4800, epoch: 5 | loss: 0.1694314\n",
      "\tspeed: 0.4167s/iter; left time: 72088.5841s\n",
      "4899it [33:58,  2.39it/s]\titers: 4900, epoch: 5 | loss: 0.1707765\n",
      "\tspeed: 0.4173s/iter; left time: 72143.3284s\n",
      "4999it [34:40,  2.41it/s]\titers: 5000, epoch: 5 | loss: 0.1297672\n",
      "\tspeed: 0.4161s/iter; left time: 71897.7705s\n",
      "5099it [35:21,  2.39it/s]\titers: 5100, epoch: 5 | loss: 0.1480218\n",
      "\tspeed: 0.4177s/iter; left time: 72127.0877s\n",
      "5199it [36:03,  2.40it/s]\titers: 5200, epoch: 5 | loss: 0.1406185\n",
      "\tspeed: 0.4168s/iter; left time: 71940.9980s\n",
      "5299it [36:45,  2.41it/s]\titers: 5300, epoch: 5 | loss: 0.1101819\n",
      "\tspeed: 0.4163s/iter; left time: 71810.2150s\n",
      "5399it [37:26,  2.42it/s]\titers: 5400, epoch: 5 | loss: 0.1276549\n",
      "\tspeed: 0.4163s/iter; left time: 71769.4827s\n",
      "5499it [38:08,  2.38it/s]\titers: 5500, epoch: 5 | loss: 0.1434563\n",
      "\tspeed: 0.4178s/iter; left time: 71982.8130s\n",
      "5599it [38:52,  2.41it/s]\titers: 5600, epoch: 5 | loss: 0.1842203\n",
      "\tspeed: 0.4386s/iter; left time: 75532.2936s\n",
      "5699it [39:34,  2.38it/s]\titers: 5700, epoch: 5 | loss: 0.1631183\n",
      "\tspeed: 0.4179s/iter; left time: 71917.2531s\n",
      "5799it [40:15,  2.39it/s]\titers: 5800, epoch: 5 | loss: 0.1011515\n",
      "\tspeed: 0.4177s/iter; left time: 71847.6233s\n",
      "5899it [40:57,  2.41it/s]\titers: 5900, epoch: 5 | loss: 0.1003942\n",
      "\tspeed: 0.4169s/iter; left time: 71659.6394s\n",
      "5999it [41:39,  2.39it/s]\titers: 6000, epoch: 5 | loss: 0.1806028\n",
      "\tspeed: 0.4157s/iter; left time: 71417.3158s\n",
      "6099it [42:20,  2.43it/s]\titers: 6100, epoch: 5 | loss: 0.2236818\n",
      "\tspeed: 0.4161s/iter; left time: 71445.0725s\n",
      "6199it [43:02,  2.42it/s]\titers: 6200, epoch: 5 | loss: 0.1356362\n",
      "\tspeed: 0.4155s/iter; left time: 71296.8771s\n",
      "6299it [43:43,  2.42it/s]\titers: 6300, epoch: 5 | loss: 0.1042953\n",
      "\tspeed: 0.4147s/iter; left time: 71113.8683s\n",
      "6399it [44:25,  2.41it/s]\titers: 6400, epoch: 5 | loss: 0.1150036\n",
      "\tspeed: 0.4144s/iter; left time: 71031.7517s\n",
      "6499it [45:06,  2.43it/s]\titers: 6500, epoch: 5 | loss: 0.0665344\n",
      "\tspeed: 0.4145s/iter; left time: 71003.6452s\n",
      "6599it [45:48,  2.40it/s]\titers: 6600, epoch: 5 | loss: 0.2359942\n",
      "\tspeed: 0.4132s/iter; left time: 70736.6067s\n",
      "6699it [46:29,  2.40it/s]\titers: 6700, epoch: 5 | loss: 0.1383991\n",
      "\tspeed: 0.4138s/iter; left time: 70799.7933s\n",
      "6799it [47:10,  2.43it/s]\titers: 6800, epoch: 5 | loss: 0.1354567\n",
      "\tspeed: 0.4138s/iter; left time: 70762.6596s\n",
      "6899it [47:52,  2.41it/s]\titers: 6900, epoch: 5 | loss: 0.1532941\n",
      "\tspeed: 0.4134s/iter; left time: 70655.5769s\n",
      "6999it [48:33,  2.41it/s]\titers: 7000, epoch: 5 | loss: 0.0839677\n",
      "\tspeed: 0.4140s/iter; left time: 70708.4345s\n",
      "7099it [49:14,  2.41it/s]\titers: 7100, epoch: 5 | loss: 0.1808469\n",
      "\tspeed: 0.4135s/iter; left time: 70580.7951s\n",
      "7199it [49:56,  2.41it/s]\titers: 7200, epoch: 5 | loss: 0.1995201\n",
      "\tspeed: 0.4141s/iter; left time: 70645.2759s\n",
      "7299it [50:37,  2.41it/s]\titers: 7300, epoch: 5 | loss: 0.1031142\n",
      "\tspeed: 0.4143s/iter; left time: 70634.6456s\n",
      "7399it [51:19,  2.41it/s]\titers: 7400, epoch: 5 | loss: 0.0968051\n",
      "\tspeed: 0.4136s/iter; left time: 70470.4439s\n",
      "7499it [52:00,  2.42it/s]\titers: 7500, epoch: 5 | loss: 0.1333803\n",
      "\tspeed: 0.4135s/iter; left time: 70408.9681s\n",
      "7599it [52:42,  2.41it/s]\titers: 7600, epoch: 5 | loss: 0.1472930\n",
      "\tspeed: 0.4168s/iter; left time: 70944.4171s\n",
      "7699it [53:23,  2.40it/s]\titers: 7700, epoch: 5 | loss: 0.1754477\n",
      "\tspeed: 0.4143s/iter; left time: 70468.6987s\n",
      "7799it [54:05,  2.40it/s]\titers: 7800, epoch: 5 | loss: 0.1530815\n",
      "\tspeed: 0.4151s/iter; left time: 70558.6196s\n",
      "7899it [54:46,  2.42it/s]\titers: 7900, epoch: 5 | loss: 0.1554404\n",
      "\tspeed: 0.4141s/iter; left time: 70350.9521s\n",
      "7999it [55:28,  2.40it/s]\titers: 8000, epoch: 5 | loss: 0.1340554\n",
      "\tspeed: 0.4150s/iter; left time: 70469.9568s\n",
      "8099it [56:09,  2.40it/s]\titers: 8100, epoch: 5 | loss: 0.1854642\n",
      "\tspeed: 0.4183s/iter; left time: 70985.3851s\n",
      "8199it [56:51,  2.40it/s]\titers: 8200, epoch: 5 | loss: 0.1841123\n",
      "\tspeed: 0.4144s/iter; left time: 70283.2562s\n",
      "8299it [57:32,  2.40it/s]\titers: 8300, epoch: 5 | loss: 0.0776538\n",
      "\tspeed: 0.4154s/iter; left time: 70405.5737s\n",
      "8399it [58:14,  2.41it/s]\titers: 8400, epoch: 5 | loss: 0.0878618\n",
      "\tspeed: 0.4151s/iter; left time: 70310.3297s\n",
      "8499it [58:55,  2.42it/s]\titers: 8500, epoch: 5 | loss: 0.1766627\n",
      "\tspeed: 0.4143s/iter; left time: 70131.2106s\n",
      "8599it [59:37,  2.42it/s]\titers: 8600, epoch: 5 | loss: 0.0811785\n",
      "\tspeed: 0.4146s/iter; left time: 70139.0137s\n",
      "8699it [1:00:18,  2.42it/s]\titers: 8700, epoch: 5 | loss: 0.1445749\n",
      "\tspeed: 0.4140s/iter; left time: 70011.0376s\n",
      "8799it [1:01:00,  2.42it/s]\titers: 8800, epoch: 5 | loss: 0.1120759\n",
      "\tspeed: 0.4155s/iter; left time: 70214.4141s\n",
      "8899it [1:01:41,  2.42it/s]\titers: 8900, epoch: 5 | loss: 0.1416490\n",
      "\tspeed: 0.4148s/iter; left time: 70058.8922s\n",
      "8999it [1:02:23,  2.40it/s]\titers: 9000, epoch: 5 | loss: 0.0982630\n",
      "\tspeed: 0.4154s/iter; left time: 70118.2311s\n",
      "9099it [1:03:04,  2.41it/s]\titers: 9100, epoch: 5 | loss: 0.1098401\n",
      "\tspeed: 0.4152s/iter; left time: 70034.8120s\n",
      "9199it [1:03:46,  2.41it/s]\titers: 9200, epoch: 5 | loss: 0.1278469\n",
      "\tspeed: 0.4165s/iter; left time: 70211.2667s\n",
      "9299it [1:04:27,  2.42it/s]\titers: 9300, epoch: 5 | loss: 0.1364074\n",
      "\tspeed: 0.4156s/iter; left time: 70026.3720s\n",
      "9399it [1:05:09,  2.41it/s]\titers: 9400, epoch: 5 | loss: 0.1667111\n",
      "\tspeed: 0.4147s/iter; left time: 69838.6315s\n",
      "9499it [1:05:50,  2.41it/s]\titers: 9500, epoch: 5 | loss: 0.0876849\n",
      "\tspeed: 0.4155s/iter; left time: 69921.9260s\n",
      "9599it [1:06:32,  2.41it/s]\titers: 9600, epoch: 5 | loss: 0.1225113\n",
      "\tspeed: 0.4151s/iter; left time: 69823.5296s\n",
      "9699it [1:07:13,  2.43it/s]\titers: 9700, epoch: 5 | loss: 0.0887922\n",
      "\tspeed: 0.4152s/iter; left time: 69785.8795s\n",
      "9799it [1:07:55,  2.40it/s]\titers: 9800, epoch: 5 | loss: 0.1463975\n",
      "\tspeed: 0.4143s/iter; left time: 69603.9547s\n",
      "9899it [1:08:36,  2.41it/s]\titers: 9900, epoch: 5 | loss: 0.1782485\n",
      "\tspeed: 0.4140s/iter; left time: 69508.2702s\n",
      "9999it [1:09:18,  2.41it/s]\titers: 10000, epoch: 5 | loss: 0.1240239\n",
      "\tspeed: 0.4152s/iter; left time: 69663.5627s\n",
      "10099it [1:09:59,  2.41it/s]\titers: 10100, epoch: 5 | loss: 0.1800409\n",
      "\tspeed: 0.4153s/iter; left time: 69648.9708s\n",
      "10199it [1:10:41,  2.40it/s]\titers: 10200, epoch: 5 | loss: 0.1009617\n",
      "\tspeed: 0.4164s/iter; left time: 69793.0016s\n",
      "10299it [1:11:22,  2.42it/s]\titers: 10300, epoch: 5 | loss: 0.1901780\n",
      "\tspeed: 0.4143s/iter; left time: 69394.5012s\n",
      "10399it [1:12:04,  2.41it/s]\titers: 10400, epoch: 5 | loss: 0.1315816\n",
      "\tspeed: 0.4143s/iter; left time: 69358.2071s\n",
      "10499it [1:12:45,  2.41it/s]\titers: 10500, epoch: 5 | loss: 0.1369898\n",
      "\tspeed: 0.4150s/iter; left time: 69425.8499s\n",
      "10599it [1:13:27,  2.42it/s]\titers: 10600, epoch: 5 | loss: 0.1404830\n",
      "\tspeed: 0.4147s/iter; left time: 69339.2992s\n",
      "10699it [1:14:08,  2.43it/s]\titers: 10700, epoch: 5 | loss: 0.1549888\n",
      "\tspeed: 0.4141s/iter; left time: 69200.2488s\n",
      "10799it [1:14:50,  2.41it/s]\titers: 10800, epoch: 5 | loss: 0.1572377\n",
      "\tspeed: 0.4143s/iter; left time: 69187.7676s\n",
      "10899it [1:15:31,  2.41it/s]\titers: 10900, epoch: 5 | loss: 0.0859239\n",
      "\tspeed: 0.4149s/iter; left time: 69247.2199s\n",
      "10999it [1:16:13,  2.43it/s]\titers: 11000, epoch: 5 | loss: 0.1152708\n",
      "\tspeed: 0.4141s/iter; left time: 69067.9849s\n",
      "11099it [1:16:54,  2.41it/s]\titers: 11100, epoch: 5 | loss: 0.1128216\n",
      "\tspeed: 0.4143s/iter; left time: 69059.8994s\n",
      "11112it [1:16:59,  2.41it/s]\n",
      "Epoch: 5 cost time: 4619.942528486252\n",
      "2403it [07:42,  5.20it/s]\n",
      "2394it [07:42,  5.17it/s]\n",
      "Epoch: 5 | Train Loss: 0.1397077 Vali Loss: 0.5184820 Test Loss: 0.7209068 MAE Loss: 0.5228893\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:40,  2.46it/s]\titers: 100, epoch: 6 | loss: 0.0674464\n",
      "\tspeed: 11.3933s/iter; left time: 1897904.1033s\n",
      "199it [01:21,  2.44it/s]\titers: 200, epoch: 6 | loss: 0.1362365\n",
      "\tspeed: 0.4113s/iter; left time: 68471.1246s\n",
      "299it [02:03,  2.40it/s]\titers: 300, epoch: 6 | loss: 0.1503354\n",
      "\tspeed: 0.4134s/iter; left time: 68779.6580s\n",
      "399it [02:44,  2.43it/s]\titers: 400, epoch: 6 | loss: 0.0939484\n",
      "\tspeed: 0.4139s/iter; left time: 68823.5919s\n",
      "499it [03:26,  2.43it/s]\titers: 500, epoch: 6 | loss: 0.1454464\n",
      "\tspeed: 0.4147s/iter; left time: 68915.6625s\n",
      "599it [04:07,  2.41it/s]\titers: 600, epoch: 6 | loss: 0.1868323\n",
      "\tspeed: 0.4146s/iter; left time: 68854.2854s\n",
      "699it [04:48,  2.42it/s]\titers: 700, epoch: 6 | loss: 0.1058892\n",
      "\tspeed: 0.4142s/iter; left time: 68755.4491s\n",
      "799it [05:30,  2.42it/s]\titers: 800, epoch: 6 | loss: 0.1247733\n",
      "\tspeed: 0.4140s/iter; left time: 68675.4749s\n",
      "899it [06:11,  2.38it/s]\titers: 900, epoch: 6 | loss: 0.1299745\n",
      "\tspeed: 0.4148s/iter; left time: 68765.3221s\n",
      "999it [06:53,  2.41it/s]\titers: 1000, epoch: 6 | loss: 0.1497836\n",
      "\tspeed: 0.4146s/iter; left time: 68695.6357s\n",
      "1099it [07:34,  2.39it/s]\titers: 1100, epoch: 6 | loss: 0.1190594\n",
      "\tspeed: 0.4148s/iter; left time: 68688.0350s\n",
      "1199it [08:16,  2.40it/s]\titers: 1200, epoch: 6 | loss: 0.1611002\n",
      "\tspeed: 0.4147s/iter; left time: 68631.9710s\n",
      "1299it [08:57,  2.41it/s]\titers: 1300, epoch: 6 | loss: 0.1032175\n",
      "\tspeed: 0.4162s/iter; left time: 68826.1066s\n",
      "1399it [09:39,  2.42it/s]\titers: 1400, epoch: 6 | loss: 0.1473047\n",
      "\tspeed: 0.4148s/iter; left time: 68561.8161s\n",
      "1499it [10:20,  2.38it/s]\titers: 1500, epoch: 6 | loss: 0.1652235\n",
      "\tspeed: 0.4161s/iter; left time: 68725.0914s\n",
      "1599it [11:02,  2.39it/s]\titers: 1600, epoch: 6 | loss: 0.2466597\n",
      "\tspeed: 0.4168s/iter; left time: 68805.1697s\n",
      "1699it [11:44,  2.41it/s]\titers: 1700, epoch: 6 | loss: 0.1575016\n",
      "\tspeed: 0.4159s/iter; left time: 68609.7743s\n",
      "1799it [12:26,  2.40it/s]\titers: 1800, epoch: 6 | loss: 0.1387943\n",
      "\tspeed: 0.4237s/iter; left time: 69852.6742s\n",
      "1899it [13:09,  2.34it/s]\titers: 1900, epoch: 6 | loss: 0.1572050\n",
      "\tspeed: 0.4315s/iter; left time: 71096.5586s\n",
      "1999it [13:51,  2.36it/s]\titers: 2000, epoch: 6 | loss: 0.1059614\n",
      "\tspeed: 0.4224s/iter; left time: 69565.9130s\n",
      "2099it [14:33,  2.41it/s]\titers: 2100, epoch: 6 | loss: 0.0976659\n",
      "\tspeed: 0.4180s/iter; left time: 68796.7528s\n",
      "2199it [15:15,  2.37it/s]\titers: 2200, epoch: 6 | loss: 0.1295298\n",
      "\tspeed: 0.4222s/iter; left time: 69436.0764s\n",
      "2299it [15:58,  2.34it/s]\titers: 2300, epoch: 6 | loss: 0.1427288\n",
      "\tspeed: 0.4221s/iter; left time: 69391.1071s\n",
      "2399it [16:40,  2.39it/s]\titers: 2400, epoch: 6 | loss: 0.0948783\n",
      "\tspeed: 0.4215s/iter; left time: 69240.4368s\n",
      "2499it [17:21,  2.40it/s]\titers: 2500, epoch: 6 | loss: 0.2019916\n",
      "\tspeed: 0.4167s/iter; left time: 68412.2472s\n",
      "2599it [18:03,  2.38it/s]\titers: 2600, epoch: 6 | loss: 0.1383031\n",
      "\tspeed: 0.4169s/iter; left time: 68411.0300s\n",
      "2699it [18:45,  2.36it/s]\titers: 2700, epoch: 6 | loss: 0.0992250\n",
      "\tspeed: 0.4165s/iter; left time: 68301.6569s\n",
      "2799it [19:27,  2.38it/s]\titers: 2800, epoch: 6 | loss: 0.1897985\n",
      "\tspeed: 0.4178s/iter; left time: 68469.2829s\n",
      "2899it [20:09,  2.39it/s]\titers: 2900, epoch: 6 | loss: 0.2029994\n",
      "\tspeed: 0.4199s/iter; left time: 68771.7509s\n",
      "2999it [20:50,  2.41it/s]\titers: 3000, epoch: 6 | loss: 0.1671937\n",
      "\tspeed: 0.4176s/iter; left time: 68354.5784s\n",
      "3099it [21:32,  2.41it/s]\titers: 3100, epoch: 6 | loss: 0.1514723\n",
      "\tspeed: 0.4194s/iter; left time: 68608.8976s\n",
      "3199it [22:15,  2.33it/s]\titers: 3200, epoch: 6 | loss: 0.1076844\n",
      "\tspeed: 0.4227s/iter; left time: 69107.2096s\n",
      "3299it [22:57,  2.37it/s]\titers: 3300, epoch: 6 | loss: 0.1018799\n",
      "\tspeed: 0.4212s/iter; left time: 68809.8570s\n",
      "3399it [23:39,  2.42it/s]\titers: 3400, epoch: 6 | loss: 0.2595521\n",
      "\tspeed: 0.4187s/iter; left time: 68357.6125s\n",
      "3499it [24:20,  2.41it/s]\titers: 3500, epoch: 6 | loss: 0.1731306\n",
      "\tspeed: 0.4170s/iter; left time: 68048.3239s\n",
      "3599it [25:02,  2.37it/s]\titers: 3600, epoch: 6 | loss: 0.0752740\n",
      "\tspeed: 0.4154s/iter; left time: 67748.1519s\n",
      "3699it [25:43,  2.42it/s]\titers: 3700, epoch: 6 | loss: 0.1019362\n",
      "\tspeed: 0.4165s/iter; left time: 67878.5561s\n",
      "3799it [26:25,  2.42it/s]\titers: 3800, epoch: 6 | loss: 0.1596445\n",
      "\tspeed: 0.4145s/iter; left time: 67510.8221s\n",
      "3899it [27:07,  2.42it/s]\titers: 3900, epoch: 6 | loss: 0.1817279\n",
      "\tspeed: 0.4182s/iter; left time: 68070.6022s\n",
      "3999it [27:48,  2.41it/s]\titers: 4000, epoch: 6 | loss: 0.2247557\n",
      "\tspeed: 0.4153s/iter; left time: 67558.8737s\n",
      "4099it [28:30,  2.42it/s]\titers: 4100, epoch: 6 | loss: 0.1638569\n",
      "\tspeed: 0.4155s/iter; left time: 67552.6822s\n",
      "4199it [29:11,  2.41it/s]\titers: 4200, epoch: 6 | loss: 0.1171530\n",
      "\tspeed: 0.4148s/iter; left time: 67403.8810s\n",
      "4299it [29:53,  2.40it/s]\titers: 4300, epoch: 6 | loss: 0.1163774\n",
      "\tspeed: 0.4155s/iter; left time: 67466.0720s\n",
      "4399it [30:34,  2.41it/s]\titers: 4400, epoch: 6 | loss: 0.0927446\n",
      "\tspeed: 0.4153s/iter; left time: 67390.6711s\n",
      "4499it [31:16,  2.41it/s]\titers: 4500, epoch: 6 | loss: 0.1053730\n",
      "\tspeed: 0.4152s/iter; left time: 67330.9770s\n",
      "4599it [31:57,  2.41it/s]\titers: 4600, epoch: 6 | loss: 0.1013372\n",
      "\tspeed: 0.4158s/iter; left time: 67400.1875s\n",
      "4699it [32:39,  2.42it/s]\titers: 4700, epoch: 6 | loss: 0.0705410\n",
      "\tspeed: 0.4156s/iter; left time: 67325.0624s\n",
      "4788it [33:16,  2.42it/s]^C\n",
      "4788it [33:16,  2.40it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 259, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1976, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2051, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Total time: 504.49380140304567 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=8 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d_ff 64, d_model 16, batch size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 11:30:42,493] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 11:30:43,252] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 11:30:43,252] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 11:30:43,252] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 11:30:43,994] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 11:30:43,995] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 11:30:44,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 11:30:44,855] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 11:30:44,855] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 11:30:44,856] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 11:30:44,856] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 11:30:44,856] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 11:30:44,856] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 11:30:44,857] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 11:30:44,857] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 11:30:44,857] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 11:30:45,142] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 11:30:45,143] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-05-15 11:30:45,143] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 584.96 GB, percent = 77.5%\n",
      "[2024-05-15 11:30:45,257] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 11:30:45,257] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-15 11:30:45,257] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 584.95 GB, percent = 77.5%\n",
      "[2024-05-15 11:30:45,257] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 11:30:45,365] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 11:30:45,365] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-15 11:30:45,366] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 584.95 GB, percent = 77.5%\n",
      "[2024-05-15 11:30:45,366] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 11:30:45,366] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 11:30:45,366] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 11:30:45,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2402fdf010>\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 11:30:45,367] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 11:30:45,368] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:30,  3.46it/s]\titers: 100, epoch: 1 | loss: 0.8842112\n",
      "\tspeed: 0.3456s/iter; left time: 4763.1071s\n",
      "199it [01:00,  3.67it/s]\titers: 200, epoch: 1 | loss: 0.7626203\n",
      "\tspeed: 0.2981s/iter; left time: 4078.7925s\n",
      "299it [01:29,  3.51it/s]\titers: 300, epoch: 1 | loss: 0.7351176\n",
      "\tspeed: 0.2884s/iter; left time: 3916.8449s\n",
      "399it [01:59,  3.38it/s]\titers: 400, epoch: 1 | loss: 0.8029838\n",
      "\tspeed: 0.2978s/iter; left time: 4014.7230s\n",
      "499it [02:30,  2.87it/s]\titers: 500, epoch: 1 | loss: 0.7494620\n",
      "\tspeed: 0.3135s/iter; left time: 4194.5795s\n",
      "599it [03:01,  2.82it/s]\titers: 600, epoch: 1 | loss: 0.8611829\n",
      "\tspeed: 0.3091s/iter; left time: 4105.5907s\n",
      "694it [03:30,  3.30it/s]\n",
      "Epoch: 1 cost time: 210.23775959014893\n",
      "150it [00:25,  5.92it/s]\n",
      "149it [00:24,  6.20it/s]\n",
      "Epoch: 1 | Train Loss: 0.9104888 Vali Loss: 1.0891369 Test Loss: 1.3444265 MAE Loss: 0.8870973\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.30it/s]\titers: 100, epoch: 2 | loss: 0.9108827\n",
      "\tspeed: 1.0929s/iter; left time: 14303.4290s\n",
      "199it [00:57,  3.55it/s]\titers: 200, epoch: 2 | loss: 0.7286992\n",
      "\tspeed: 0.2901s/iter; left time: 3767.2765s\n",
      "299it [01:27,  3.64it/s]\titers: 300, epoch: 2 | loss: 0.4562438\n",
      "\tspeed: 0.2946s/iter; left time: 3797.0967s\n",
      "399it [01:55,  3.75it/s]\titers: 400, epoch: 2 | loss: 0.4339146\n",
      "\tspeed: 0.2875s/iter; left time: 3676.4760s\n",
      "499it [02:24,  3.66it/s]\titers: 500, epoch: 2 | loss: 0.3820442\n",
      "\tspeed: 0.2895s/iter; left time: 3673.3713s\n",
      "599it [02:53,  3.82it/s]\titers: 600, epoch: 2 | loss: 0.5160459\n",
      "\tspeed: 0.2865s/iter; left time: 3606.7627s\n",
      "694it [03:21,  3.45it/s]\n",
      "Epoch: 2 cost time: 201.22011542320251\n",
      "150it [00:21,  6.90it/s]\n",
      "149it [00:21,  6.99it/s]\n",
      "Epoch: 2 | Train Loss: 0.5647182 Vali Loss: 0.4878379 Test Loss: 0.5914022 MAE Loss: 0.5033054\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.77it/s]\titers: 100, epoch: 3 | loss: 0.4071434\n",
      "\tspeed: 1.0152s/iter; left time: 12581.8587s\n",
      "199it [00:57,  3.62it/s]\titers: 200, epoch: 3 | loss: 0.4370949\n",
      "\tspeed: 0.2854s/iter; left time: 3508.4016s\n",
      "299it [01:25,  3.71it/s]\titers: 300, epoch: 3 | loss: 0.3958214\n",
      "\tspeed: 0.2845s/iter; left time: 3468.6568s\n",
      "399it [01:54,  3.77it/s]\titers: 400, epoch: 3 | loss: 0.3545278\n",
      "\tspeed: 0.2865s/iter; left time: 3465.0694s\n",
      "499it [02:23,  3.66it/s]\titers: 500, epoch: 3 | loss: 0.3681699\n",
      "\tspeed: 0.2889s/iter; left time: 3464.2794s\n",
      "599it [02:52,  3.62it/s]\titers: 600, epoch: 3 | loss: 0.4017632\n",
      "\tspeed: 0.2939s/iter; left time: 3495.1370s\n",
      "694it [03:20,  3.46it/s]\n",
      "Epoch: 3 cost time: 200.83755493164062\n",
      "150it [00:21,  6.86it/s]\n",
      "149it [00:21,  6.96it/s]\n",
      "Epoch: 3 | Train Loss: 0.3989446 Vali Loss: 0.4725496 Test Loss: 0.5715821 MAE Loss: 0.4867522\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.31it/s]\titers: 100, epoch: 4 | loss: 0.3819491\n",
      "\tspeed: 1.0248s/iter; left time: 11989.5970s\n",
      "199it [00:57,  3.25it/s]\titers: 200, epoch: 4 | loss: 0.5128795\n",
      "\tspeed: 0.2839s/iter; left time: 3293.1978s\n",
      "299it [01:26,  3.46it/s]\titers: 300, epoch: 4 | loss: 0.3453858\n",
      "\tspeed: 0.2889s/iter; left time: 3322.3159s\n",
      "399it [01:55,  3.57it/s]\titers: 400, epoch: 4 | loss: 0.4768394\n",
      "\tspeed: 0.2896s/iter; left time: 3300.6758s\n",
      "499it [02:23,  3.70it/s]\titers: 500, epoch: 4 | loss: 0.3524556\n",
      "\tspeed: 0.2891s/iter; left time: 3266.4195s\n",
      "599it [02:52,  3.63it/s]\titers: 600, epoch: 4 | loss: 0.4269343\n",
      "\tspeed: 0.2906s/iter; left time: 3254.8911s\n",
      "694it [03:20,  3.47it/s]\n",
      "Epoch: 4 cost time: 200.2058219909668\n",
      "150it [00:21,  6.92it/s]\n",
      "149it [00:21,  7.05it/s]\n",
      "Epoch: 4 | Train Loss: 0.3844736 Vali Loss: 0.4619189 Test Loss: 0.5680255 MAE Loss: 0.4881820\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.21it/s]\titers: 100, epoch: 5 | loss: 0.4577494\n",
      "\tspeed: 1.0086s/iter; left time: 11099.9160s\n",
      "199it [00:57,  3.32it/s]\titers: 200, epoch: 5 | loss: 0.3442118\n",
      "\tspeed: 0.2874s/iter; left time: 3134.5932s\n",
      "299it [01:26,  3.33it/s]\titers: 300, epoch: 5 | loss: 0.3763731\n",
      "\tspeed: 0.2918s/iter; left time: 3152.8318s\n",
      "399it [01:55,  3.47it/s]\titers: 400, epoch: 5 | loss: 0.4299109\n",
      "\tspeed: 0.2881s/iter; left time: 3083.9088s\n",
      "499it [02:24,  3.52it/s]\titers: 500, epoch: 5 | loss: 0.3935875\n",
      "\tspeed: 0.2904s/iter; left time: 3080.1281s\n",
      "599it [02:53,  3.40it/s]\titers: 600, epoch: 5 | loss: 0.4233167\n",
      "\tspeed: 0.2888s/iter; left time: 3033.8808s\n",
      "694it [03:21,  3.45it/s]\n",
      "Epoch: 5 cost time: 201.15409469604492\n",
      "150it [00:21,  6.87it/s]\n",
      "149it [00:21,  6.96it/s]\n",
      "Epoch: 5 | Train Loss: 0.3748863 Vali Loss: 0.4572975 Test Loss: 0.5704545 MAE Loss: 0.4960686\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.21it/s]\titers: 100, epoch: 6 | loss: 0.4269586\n",
      "\tspeed: 1.0181s/iter; left time: 10497.7902s\n",
      "199it [00:58,  3.33it/s]\titers: 200, epoch: 6 | loss: 0.4210969\n",
      "\tspeed: 0.2934s/iter; left time: 2995.8767s\n",
      "299it [01:27,  3.52it/s]\titers: 300, epoch: 6 | loss: 0.3974001\n",
      "\tspeed: 0.2924s/iter; left time: 2956.0890s\n",
      "399it [01:56,  3.54it/s]\titers: 400, epoch: 6 | loss: 0.3206915\n",
      "\tspeed: 0.2904s/iter; left time: 2907.1727s\n",
      "499it [02:26,  3.74it/s]\titers: 500, epoch: 6 | loss: 0.3614732\n",
      "\tspeed: 0.2974s/iter; left time: 2947.6210s\n",
      "599it [02:55,  3.64it/s]\titers: 600, epoch: 6 | loss: 0.3757707\n",
      "\tspeed: 0.2904s/iter; left time: 2848.9018s\n",
      "694it [03:23,  3.42it/s]\n",
      "Epoch: 6 cost time: 203.2027039527893\n",
      "150it [00:22,  6.77it/s]\n",
      "149it [00:21,  6.84it/s]\n",
      "Epoch: 6 | Train Loss: 0.3679104 Vali Loss: 0.4593152 Test Loss: 0.5687287 MAE Loss: 0.4914150\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.10it/s]\titers: 100, epoch: 7 | loss: 0.3114282\n",
      "\tspeed: 1.0178s/iter; left time: 9788.6566s\n",
      "199it [00:58,  3.19it/s]\titers: 200, epoch: 7 | loss: 0.4711249\n",
      "\tspeed: 0.2890s/iter; left time: 2750.0726s\n",
      "299it [01:27,  3.39it/s]\titers: 300, epoch: 7 | loss: 0.3909038\n",
      "\tspeed: 0.2903s/iter; left time: 2733.9058s\n",
      "399it [01:56,  3.63it/s]\titers: 400, epoch: 7 | loss: 0.3047278\n",
      "\tspeed: 0.2885s/iter; left time: 2688.1683s\n",
      "499it [02:24,  3.51it/s]\titers: 500, epoch: 7 | loss: 0.2875497\n",
      "\tspeed: 0.2875s/iter; left time: 2649.7205s\n",
      "599it [02:53,  3.39it/s]\titers: 600, epoch: 7 | loss: 0.2629882\n",
      "\tspeed: 0.2908s/iter; left time: 2650.8697s\n",
      "694it [03:21,  3.44it/s]\n",
      "Epoch: 7 cost time: 201.65501379966736\n",
      "150it [00:21,  6.97it/s]\n",
      "149it [00:21,  7.00it/s]\n",
      "Epoch: 7 | Train Loss: 0.3599993 Vali Loss: 0.4448959 Test Loss: 0.5569292 MAE Loss: 0.4676390\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.25it/s]\titers: 100, epoch: 8 | loss: 0.3604038\n",
      "\tspeed: 1.0362s/iter; left time: 9246.3890s\n",
      "199it [00:58,  3.11it/s]\titers: 200, epoch: 8 | loss: 0.3247879\n",
      "\tspeed: 0.2898s/iter; left time: 2557.2280s\n",
      "299it [01:27,  3.25it/s]\titers: 300, epoch: 8 | loss: 0.4180290\n",
      "\tspeed: 0.2896s/iter; left time: 2526.1569s\n",
      "399it [01:56,  3.19it/s]\titers: 400, epoch: 8 | loss: 0.3446910\n",
      "\tspeed: 0.2890s/iter; left time: 2491.7014s\n",
      "499it [02:24,  3.31it/s]\titers: 500, epoch: 8 | loss: 0.4064158\n",
      "\tspeed: 0.2888s/iter; left time: 2461.8067s\n",
      "599it [02:53,  3.11it/s]\titers: 600, epoch: 8 | loss: 0.3844486\n",
      "\tspeed: 0.2902s/iter; left time: 2444.0066s\n",
      "694it [03:21,  3.45it/s]\n",
      "Epoch: 8 cost time: 201.43269681930542\n",
      "150it [00:21,  6.93it/s]\n",
      "149it [00:21,  6.93it/s]\n",
      "Epoch: 8 | Train Loss: 0.3510811 Vali Loss: 0.4529370 Test Loss: 0.5769964 MAE Loss: 0.4721643\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.42it/s]\titers: 100, epoch: 9 | loss: 0.3453438\n",
      "\tspeed: 0.9990s/iter; left time: 8220.4334s\n",
      "199it [00:58,  3.21it/s]\titers: 200, epoch: 9 | loss: 0.3493319\n",
      "\tspeed: 0.2908s/iter; left time: 2363.5770s\n",
      "299it [01:27,  3.57it/s]\titers: 300, epoch: 9 | loss: 0.2829166\n",
      "\tspeed: 0.2903s/iter; left time: 2330.7197s\n",
      "399it [01:55,  3.60it/s]\titers: 400, epoch: 9 | loss: 0.3496809\n",
      "\tspeed: 0.2871s/iter; left time: 2276.2066s\n",
      "499it [02:24,  3.58it/s]\titers: 500, epoch: 9 | loss: 0.3836863\n",
      "\tspeed: 0.2877s/iter; left time: 2252.5040s\n",
      "599it [02:53,  3.63it/s]\titers: 600, epoch: 9 | loss: 0.3229220\n",
      "\tspeed: 0.2877s/iter; left time: 2223.5358s\n",
      "694it [03:21,  3.45it/s]\n",
      "Epoch: 9 cost time: 201.29332995414734\n",
      "150it [00:21,  6.88it/s]\n",
      "149it [00:21,  6.81it/s]\n",
      "Epoch: 9 | Train Loss: 0.3420725 Vali Loss: 0.4625975 Test Loss: 0.5838436 MAE Loss: 0.4741270\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.60it/s]\titers: 100, epoch: 10 | loss: 0.3429279\n",
      "\tspeed: 1.0062s/iter; left time: 7581.4564s\n",
      "199it [00:57,  3.61it/s]\titers: 200, epoch: 10 | loss: 0.3862309\n",
      "\tspeed: 0.2912s/iter; left time: 2165.4260s\n",
      "299it [01:26,  3.72it/s]\titers: 300, epoch: 10 | loss: 0.3431814\n",
      "\tspeed: 0.2876s/iter; left time: 2109.5461s\n",
      "399it [01:55,  3.66it/s]\titers: 400, epoch: 10 | loss: 0.3113597\n",
      "\tspeed: 0.2881s/iter; left time: 2084.5355s\n",
      "499it [02:23,  3.72it/s]\titers: 500, epoch: 10 | loss: 0.3869158\n",
      "\tspeed: 0.2867s/iter; left time: 2045.3759s\n",
      "599it [02:52,  3.69it/s]\titers: 600, epoch: 10 | loss: 0.2981574\n",
      "\tspeed: 0.2887s/iter; left time: 2031.1460s\n",
      "694it [03:20,  3.46it/s]\n",
      "Epoch: 10 cost time: 200.49787139892578\n",
      "150it [00:21,  6.94it/s]\n",
      "149it [00:21,  6.99it/s]\n",
      "Epoch: 10 | Train Loss: 0.3314455 Vali Loss: 0.4898667 Test Loss: 0.6134745 MAE Loss: 0.4846304\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 41.5058463493983 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=128 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 12:13:53,516] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 12:13:54,342] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 12:13:54,343] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 12:13:54,343] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 12:13:55,154] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 12:13:55,154] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 12:13:56,198] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 12:13:56,199] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 12:13:56,199] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 12:13:56,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 12:13:56,200] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 12:13:56,201] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 12:13:56,201] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 12:13:56,201] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 12:13:56,201] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 12:13:56,201] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 12:13:56,511] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 12:13:56,511] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 12:13:56,512] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 598.35 GB, percent = 79.3%\n",
      "[2024-05-15 12:13:56,624] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 12:13:56,624] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 12:13:56,625] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 598.35 GB, percent = 79.3%\n",
      "[2024-05-15 12:13:56,625] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 12:13:56,734] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 12:13:56,734] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 12:13:56,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 598.35 GB, percent = 79.3%\n",
      "[2024-05-15 12:13:56,735] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 12:13:56,735] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 12:13:56,735] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 12:13:56,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 12:13:56,735] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa4a060b590>\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 12:13:56,736] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 12:13:56,737] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:30,  3.65it/s]\titers: 100, epoch: 1 | loss: 0.8407222\n",
      "\tspeed: 0.3406s/iter; left time: 4693.6232s\n",
      "199it [01:00,  3.17it/s]\titers: 200, epoch: 1 | loss: 0.7518957\n",
      "\tspeed: 0.3002s/iter; left time: 4106.9055s\n",
      "299it [01:31,  3.51it/s]\titers: 300, epoch: 1 | loss: 0.7412251\n",
      "\tspeed: 0.3100s/iter; left time: 4209.5679s\n",
      "399it [02:01,  3.46it/s]\titers: 400, epoch: 1 | loss: 0.5111101\n",
      "\tspeed: 0.3002s/iter; left time: 4047.6637s\n",
      "499it [02:31,  3.37it/s]\titers: 500, epoch: 1 | loss: 0.4471187\n",
      "\tspeed: 0.3072s/iter; left time: 4110.7953s\n",
      "599it [03:01,  3.36it/s]\titers: 600, epoch: 1 | loss: 0.4856511\n",
      "\tspeed: 0.2983s/iter; left time: 3961.6231s\n",
      "694it [03:30,  3.30it/s]\n",
      "Epoch: 1 cost time: 210.6184380054474\n",
      "150it [00:24,  6.00it/s]\n",
      "149it [00:24,  6.14it/s]\n",
      "Epoch: 1 | Train Loss: 0.6521282 Vali Loss: 0.4950464 Test Loss: 0.5991161 MAE Loss: 0.5089218\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.39it/s]\titers: 100, epoch: 2 | loss: 0.4171833\n",
      "\tspeed: 1.1406s/iter; left time: 14927.2131s\n",
      "199it [00:58,  3.58it/s]\titers: 200, epoch: 2 | loss: 0.4136519\n",
      "\tspeed: 0.2957s/iter; left time: 3840.5355s\n",
      "299it [01:27,  3.68it/s]\titers: 300, epoch: 2 | loss: 0.4717973\n",
      "\tspeed: 0.2883s/iter; left time: 3715.0204s\n",
      "399it [01:56,  2.91it/s]\titers: 400, epoch: 2 | loss: 0.3424344\n",
      "\tspeed: 0.2946s/iter; left time: 3766.5150s\n",
      "499it [02:25,  3.27it/s]\titers: 500, epoch: 2 | loss: 0.2960540\n",
      "\tspeed: 0.2877s/iter; left time: 3649.8519s\n",
      "599it [02:54,  3.44it/s]\titers: 600, epoch: 2 | loss: 0.4580581\n",
      "\tspeed: 0.2879s/iter; left time: 3623.4790s\n",
      "694it [03:21,  3.44it/s]\n",
      "Epoch: 2 cost time: 201.82312846183777\n",
      "150it [00:22,  6.79it/s]\n",
      "149it [00:21,  6.97it/s]\n",
      "Epoch: 2 | Train Loss: 0.3860844 Vali Loss: 0.4690894 Test Loss: 0.5647209 MAE Loss: 0.4838637\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.56it/s]\titers: 100, epoch: 3 | loss: 0.3493198\n",
      "\tspeed: 1.0261s/iter; left time: 12716.2008s\n",
      "199it [00:57,  3.63it/s]\titers: 200, epoch: 3 | loss: 0.3858086\n",
      "\tspeed: 0.2903s/iter; left time: 3568.9628s\n",
      "299it [01:26,  3.58it/s]\titers: 300, epoch: 3 | loss: 0.2948742\n",
      "\tspeed: 0.2873s/iter; left time: 3502.8384s\n",
      "399it [01:55,  3.53it/s]\titers: 400, epoch: 3 | loss: 0.3943480\n",
      "\tspeed: 0.2868s/iter; left time: 3467.7165s\n",
      "499it [02:24,  3.67it/s]\titers: 500, epoch: 3 | loss: 0.3618994\n",
      "\tspeed: 0.2865s/iter; left time: 3436.2693s\n",
      "599it [02:52,  3.66it/s]\titers: 600, epoch: 3 | loss: 0.3749486\n",
      "\tspeed: 0.2852s/iter; left time: 3391.3718s\n",
      "694it [03:19,  3.48it/s]\n",
      "Epoch: 3 cost time: 199.6304578781128\n",
      "150it [00:21,  6.86it/s]\n",
      "149it [00:21,  7.01it/s]\n",
      "Epoch: 3 | Train Loss: 0.3690826 Vali Loss: 0.4631291 Test Loss: 0.5723212 MAE Loss: 0.5032248\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.44it/s]\titers: 100, epoch: 4 | loss: 0.3247547\n",
      "\tspeed: 1.0190s/iter; left time: 11920.8779s\n",
      "199it [00:58,  3.44it/s]\titers: 200, epoch: 4 | loss: 0.3515896\n",
      "\tspeed: 0.2898s/iter; left time: 3360.9292s\n",
      "299it [01:26,  3.47it/s]\titers: 300, epoch: 4 | loss: 0.3922097\n",
      "\tspeed: 0.2855s/iter; left time: 3283.1653s\n",
      "399it [01:55,  3.25it/s]\titers: 400, epoch: 4 | loss: 0.2924359\n",
      "\tspeed: 0.2878s/iter; left time: 3281.1040s\n",
      "499it [02:24,  3.38it/s]\titers: 500, epoch: 4 | loss: 0.3779572\n",
      "\tspeed: 0.2876s/iter; left time: 3250.0144s\n",
      "599it [02:53,  3.56it/s]\titers: 600, epoch: 4 | loss: 0.4511427\n",
      "\tspeed: 0.2896s/iter; left time: 3243.6564s\n",
      "694it [03:20,  3.46it/s]\n",
      "Epoch: 4 cost time: 200.8187220096588\n",
      "150it [00:21,  6.83it/s]\n",
      "149it [00:21,  6.96it/s]\n",
      "Epoch: 4 | Train Loss: 0.3575291 Vali Loss: 0.4548049 Test Loss: 0.5653496 MAE Loss: 0.4797450\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.38it/s]\titers: 100, epoch: 5 | loss: 0.3392158\n",
      "\tspeed: 1.0262s/iter; left time: 11292.8903s\n",
      "199it [00:57,  3.22it/s]\titers: 200, epoch: 5 | loss: 0.3060585\n",
      "\tspeed: 0.2864s/iter; left time: 3123.2236s\n",
      "299it [01:26,  3.36it/s]\titers: 300, epoch: 5 | loss: 0.3485951\n",
      "\tspeed: 0.2847s/iter; left time: 3075.9082s\n",
      "399it [01:54,  3.16it/s]\titers: 400, epoch: 5 | loss: 0.3564121\n",
      "\tspeed: 0.2846s/iter; left time: 3046.2677s\n",
      "499it [02:23,  3.51it/s]\titers: 500, epoch: 5 | loss: 0.3844394\n",
      "\tspeed: 0.2872s/iter; left time: 3046.1550s\n",
      "599it [02:52,  3.55it/s]\titers: 600, epoch: 5 | loss: 0.3060312\n",
      "\tspeed: 0.2883s/iter; left time: 3028.3560s\n",
      "694it [03:19,  3.48it/s]\n",
      "Epoch: 5 cost time: 199.4922354221344\n",
      "150it [00:21,  6.90it/s]\n",
      "149it [00:21,  6.92it/s]\n",
      "Epoch: 5 | Train Loss: 0.3481171 Vali Loss: 0.4553198 Test Loss: 0.5773332 MAE Loss: 0.4872676\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.48it/s]\titers: 100, epoch: 6 | loss: 0.3611078\n",
      "\tspeed: 0.9962s/iter; left time: 10271.4302s\n",
      "199it [00:57,  3.37it/s]\titers: 200, epoch: 6 | loss: 0.3280023\n",
      "\tspeed: 0.2875s/iter; left time: 2935.4595s\n",
      "299it [01:26,  3.50it/s]\titers: 300, epoch: 6 | loss: 0.3774423\n",
      "\tspeed: 0.2868s/iter; left time: 2899.4722s\n",
      "399it [01:55,  3.68it/s]\titers: 400, epoch: 6 | loss: 0.3770026\n",
      "\tspeed: 0.2939s/iter; left time: 2941.9986s\n",
      "499it [02:25,  3.15it/s]\titers: 500, epoch: 6 | loss: 0.3075535\n",
      "\tspeed: 0.2937s/iter; left time: 2910.6663s\n",
      "599it [02:53,  3.40it/s]\titers: 600, epoch: 6 | loss: 0.2570413\n",
      "\tspeed: 0.2888s/iter; left time: 2833.0166s\n",
      "694it [03:21,  3.44it/s]\n",
      "Epoch: 6 cost time: 201.70238327980042\n",
      "150it [00:21,  6.92it/s]\n",
      "149it [00:21,  7.00it/s]\n",
      "Epoch: 6 | Train Loss: 0.3352179 Vali Loss: 0.4628054 Test Loss: 0.5907153 MAE Loss: 0.4743707\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.40it/s]\titers: 100, epoch: 7 | loss: 0.3053172\n",
      "\tspeed: 1.0056s/iter; left time: 9671.2027s\n",
      "199it [00:58,  3.38it/s]\titers: 200, epoch: 7 | loss: 0.2282240\n",
      "\tspeed: 0.2919s/iter; left time: 2778.2101s\n",
      "299it [01:27,  3.53it/s]\titers: 300, epoch: 7 | loss: 0.2297260\n",
      "\tspeed: 0.2925s/iter; left time: 2754.7260s\n",
      "399it [01:56,  3.50it/s]\titers: 400, epoch: 7 | loss: 0.3430172\n",
      "\tspeed: 0.2904s/iter; left time: 2705.9802s\n",
      "499it [02:25,  3.50it/s]\titers: 500, epoch: 7 | loss: 0.2628118\n",
      "\tspeed: 0.2894s/iter; left time: 2667.0404s\n",
      "599it [02:55,  3.13it/s]\titers: 600, epoch: 7 | loss: 0.3583399\n",
      "\tspeed: 0.2912s/iter; left time: 2654.8792s\n",
      "694it [03:22,  3.42it/s]\n",
      "Epoch: 7 cost time: 202.77934312820435\n",
      "150it [00:21,  6.94it/s]\n",
      "149it [00:21,  6.89it/s]\n",
      "Epoch: 7 | Train Loss: 0.3213069 Vali Loss: 0.4701438 Test Loss: 0.6318865 MAE Loss: 0.5010115\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 29.321557863553366 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=128 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# affine = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 88899\n",
      "val 19227\n",
      "test 19155\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 14:01:19,027] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 14:01:19,880] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 14:01:19,880] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 14:01:19,880] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 14:01:20,843] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 14:01:20,844] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 14:01:21,975] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 14:01:21,976] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 14:01:21,976] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 14:01:21,977] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 14:01:21,977] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 14:01:21,977] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 14:01:21,977] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 14:01:21,977] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 14:01:21,977] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 14:01:21,977] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 14:01:22,295] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 14:01:22,295] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 14:01:22,295] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.16 GB, percent = 52.6%\n",
      "[2024-05-15 14:01:22,625] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 14:01:22,626] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 14:01:22,626] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.14 GB, percent = 52.6%\n",
      "[2024-05-15 14:01:22,626] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 14:01:22,748] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 14:01:22,748] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 14:01:22,748] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.14 GB, percent = 52.6%\n",
      "[2024-05-15 14:01:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 14:01:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 14:01:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 14:01:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9f8ca9bd90>\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 14:01:22,750] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 14:01:22,751] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:35,  3.28it/s]\titers: 100, epoch: 1 | loss: 0.8407222\n",
      "\tspeed: 0.4066s/iter; left time: 5603.6621s\n",
      "199it [01:12,  3.20it/s]\titers: 200, epoch: 1 | loss: 0.7518957\n",
      "\tspeed: 0.3590s/iter; left time: 4911.2689s\n",
      "299it [01:43,  3.43it/s]\titers: 300, epoch: 1 | loss: 0.7412251\n",
      "\tspeed: 0.3089s/iter; left time: 4195.7046s\n",
      "399it [02:12,  3.33it/s]\titers: 400, epoch: 1 | loss: 0.5111101\n",
      "\tspeed: 0.2981s/iter; left time: 4019.2935s\n",
      "499it [02:42,  3.42it/s]\titers: 500, epoch: 1 | loss: 0.4471187\n",
      "\tspeed: 0.2966s/iter; left time: 3968.9181s\n",
      "599it [03:12,  3.47it/s]\titers: 600, epoch: 1 | loss: 0.4856511\n",
      "\tspeed: 0.3021s/iter; left time: 4012.2148s\n",
      "694it [03:41,  3.14it/s]\n",
      "Epoch: 1 cost time: 221.0977394580841\n",
      "150it [00:25,  5.90it/s]\n",
      "149it [00:24,  6.11it/s]\n",
      "Epoch: 1 | Train Loss: 0.6521282 Vali Loss: 0.4950464 Test Loss: 0.5991161 MAE Loss: 0.5089218\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.46it/s]\titers: 100, epoch: 2 | loss: 0.4171833\n",
      "\tspeed: 1.0944s/iter; left time: 14322.9621s\n",
      "199it [00:57,  3.50it/s]\titers: 200, epoch: 2 | loss: 0.4136519\n",
      "\tspeed: 0.2850s/iter; left time: 3700.7134s\n",
      "299it [01:25,  3.71it/s]\titers: 300, epoch: 2 | loss: 0.4717973\n",
      "\tspeed: 0.2820s/iter; left time: 3634.7621s\n",
      "399it [01:53,  3.66it/s]\titers: 400, epoch: 2 | loss: 0.3424344\n",
      "\tspeed: 0.2842s/iter; left time: 3633.4625s\n",
      "499it [02:22,  3.55it/s]\titers: 500, epoch: 2 | loss: 0.2960540\n",
      "\tspeed: 0.2830s/iter; left time: 3589.9578s\n",
      "599it [02:51,  3.32it/s]\titers: 600, epoch: 2 | loss: 0.4580581\n",
      "\tspeed: 0.2921s/iter; left time: 3676.3916s\n",
      "694it [03:18,  3.50it/s]\n",
      "Epoch: 2 cost time: 198.36720061302185\n",
      "150it [00:21,  6.83it/s]\n",
      "149it [00:21,  6.87it/s]\n",
      "Epoch: 2 | Train Loss: 0.3860844 Vali Loss: 0.4690894 Test Loss: 0.5647209 MAE Loss: 0.4838637\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:29,  3.37it/s]\titers: 100, epoch: 3 | loss: 0.3493198\n",
      "\tspeed: 1.0220s/iter; left time: 12665.5652s\n",
      "199it [00:57,  3.47it/s]\titers: 200, epoch: 3 | loss: 0.3858086\n",
      "\tspeed: 0.2856s/iter; left time: 3510.5116s\n",
      "299it [01:26,  3.18it/s]\titers: 300, epoch: 3 | loss: 0.2948742\n",
      "\tspeed: 0.2843s/iter; left time: 3466.6959s\n",
      "399it [01:54,  3.49it/s]\titers: 400, epoch: 3 | loss: 0.3943480\n",
      "\tspeed: 0.2835s/iter; left time: 3428.5540s\n",
      "499it [02:22,  3.58it/s]\titers: 500, epoch: 3 | loss: 0.3618994\n",
      "\tspeed: 0.2820s/iter; left time: 3381.5461s\n",
      "599it [02:50,  3.55it/s]\titers: 600, epoch: 3 | loss: 0.3749486\n",
      "\tspeed: 0.2827s/iter; left time: 3361.9277s\n",
      "694it [03:18,  3.50it/s]\n",
      "Epoch: 3 cost time: 198.26892971992493\n",
      "150it [00:21,  6.94it/s]\n",
      "149it [00:21,  6.91it/s]\n",
      "Epoch: 3 | Train Loss: 0.3690826 Vali Loss: 0.4631291 Test Loss: 0.5723212 MAE Loss: 0.5032248\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.56it/s]\titers: 100, epoch: 4 | loss: 0.3247547\n",
      "\tspeed: 1.0174s/iter; left time: 11902.6112s\n",
      "199it [00:57,  3.16it/s]\titers: 200, epoch: 4 | loss: 0.3515896\n",
      "\tspeed: 0.2862s/iter; left time: 3319.6320s\n",
      "299it [01:25,  3.47it/s]\titers: 300, epoch: 4 | loss: 0.3922097\n",
      "\tspeed: 0.2845s/iter; left time: 3271.6205s\n",
      "399it [01:54,  3.55it/s]\titers: 400, epoch: 4 | loss: 0.2924359\n",
      "\tspeed: 0.2826s/iter; left time: 3221.8974s\n",
      "499it [02:22,  3.62it/s]\titers: 500, epoch: 4 | loss: 0.3779572\n",
      "\tspeed: 0.2854s/iter; left time: 3225.1571s\n",
      "599it [02:50,  3.65it/s]\titers: 600, epoch: 4 | loss: 0.4511427\n",
      "\tspeed: 0.2844s/iter; left time: 3184.9719s\n",
      "694it [03:18,  3.50it/s]\n",
      "Epoch: 4 cost time: 198.4723415374756\n",
      "150it [00:21,  6.88it/s]\n",
      "149it [00:21,  6.99it/s]\n",
      "Epoch: 4 | Train Loss: 0.3575291 Vali Loss: 0.4548049 Test Loss: 0.5653496 MAE Loss: 0.4797450\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.41it/s]\titers: 100, epoch: 5 | loss: 0.3392158\n",
      "\tspeed: 1.0169s/iter; left time: 11191.2348s\n",
      "199it [00:57,  3.45it/s]\titers: 200, epoch: 5 | loss: 0.3060585\n",
      "\tspeed: 0.2890s/iter; left time: 3151.9952s\n",
      "299it [01:27,  3.30it/s]\titers: 300, epoch: 5 | loss: 0.3485951\n",
      "\tspeed: 0.2934s/iter; left time: 3169.7217s\n",
      "399it [01:55,  3.14it/s]\titers: 400, epoch: 5 | loss: 0.3564121\n",
      "\tspeed: 0.2838s/iter; left time: 3038.5274s\n",
      "499it [02:23,  3.24it/s]\titers: 500, epoch: 5 | loss: 0.3844394\n",
      "\tspeed: 0.2839s/iter; left time: 3010.9423s\n",
      "599it [02:52,  3.45it/s]\titers: 600, epoch: 5 | loss: 0.3060312\n",
      "\tspeed: 0.2858s/iter; left time: 3001.8645s\n",
      "694it [03:19,  3.48it/s]\n",
      "Epoch: 5 cost time: 199.55983519554138\n",
      "150it [00:21,  6.96it/s]\n",
      "149it [00:21,  6.85it/s]\n",
      "Epoch: 5 | Train Loss: 0.3481171 Vali Loss: 0.4553198 Test Loss: 0.5773332 MAE Loss: 0.4872676\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.37it/s]\titers: 100, epoch: 6 | loss: 0.3611078\n",
      "\tspeed: 0.9954s/iter; left time: 10263.1891s\n",
      "199it [00:57,  3.56it/s]\titers: 200, epoch: 6 | loss: 0.3280023\n",
      "\tspeed: 0.2875s/iter; left time: 2935.8525s\n",
      "299it [01:26,  3.60it/s]\titers: 300, epoch: 6 | loss: 0.3774423\n",
      "\tspeed: 0.2864s/iter; left time: 2895.8498s\n",
      "399it [01:54,  3.65it/s]\titers: 400, epoch: 6 | loss: 0.3770026\n",
      "\tspeed: 0.2862s/iter; left time: 2864.6914s\n",
      "499it [02:23,  3.72it/s]\titers: 500, epoch: 6 | loss: 0.3075535\n",
      "\tspeed: 0.2880s/iter; left time: 2854.5731s\n",
      "599it [02:51,  3.71it/s]\titers: 600, epoch: 6 | loss: 0.2570413\n",
      "\tspeed: 0.2833s/iter; left time: 2779.5681s\n",
      "694it [03:19,  3.48it/s]\n",
      "Epoch: 6 cost time: 199.16414189338684\n",
      "150it [00:22,  6.76it/s]\n",
      "149it [00:21,  6.93it/s]\n",
      "Epoch: 6 | Train Loss: 0.3352179 Vali Loss: 0.4628054 Test Loss: 0.5907153 MAE Loss: 0.4743707\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:28,  3.29it/s]\titers: 100, epoch: 7 | loss: 0.3053172\n",
      "\tspeed: 0.9984s/iter; left time: 9601.5028s\n",
      "199it [00:57,  3.56it/s]\titers: 200, epoch: 7 | loss: 0.2282240\n",
      "\tspeed: 0.2874s/iter; left time: 2735.5484s\n",
      "299it [01:25,  3.61it/s]\titers: 300, epoch: 7 | loss: 0.2297260\n",
      "\tspeed: 0.2828s/iter; left time: 2662.7908s\n",
      "399it [01:54,  3.39it/s]\titers: 400, epoch: 7 | loss: 0.3430172\n",
      "\tspeed: 0.2927s/iter; left time: 2726.8697s\n",
      "499it [02:23,  3.32it/s]\titers: 500, epoch: 7 | loss: 0.2628118\n",
      "\tspeed: 0.2844s/iter; left time: 2621.7175s\n",
      "599it [02:52,  3.30it/s]\titers: 600, epoch: 7 | loss: 0.3583399\n",
      "\tspeed: 0.2926s/iter; left time: 2667.9960s\n",
      "694it [03:19,  3.47it/s]\n",
      "Epoch: 7 cost time: 199.90713834762573\n",
      "150it [00:21,  6.84it/s]\n",
      "149it [00:21,  6.89it/s]\n",
      "Epoch: 7 | Train Loss: 0.3213069 Vali Loss: 0.4701438 Test Loss: 0.6318865 MAE Loss: 0.5010115\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 29.25867251555125 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=128 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 17:06:16,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 17:06:17,541] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 17:06:17,542] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 17:06:17,542] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 17:06:18,362] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 17:06:18,363] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 17:06:19,471] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 17:06:19,472] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 17:06:19,472] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 17:06:19,473] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 17:06:19,473] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 17:06:19,473] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 17:06:19,474] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 17:06:19,474] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 17:06:19,474] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 17:06:19,474] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 17:06:19,766] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 17:06:19,767] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 17:06:19,767] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 331.17 GB, percent = 43.9%\n",
      "[2024-05-15 17:06:19,884] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 17:06:19,885] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 17:06:19,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 331.17 GB, percent = 43.9%\n",
      "[2024-05-15 17:06:19,885] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 17:06:20,003] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 17:06:20,004] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 17:06:20,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 331.18 GB, percent = 43.9%\n",
      "[2024-05-15 17:06:20,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 17:06:20,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 17:06:20,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 17:06:20,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 17:06:20,005] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 17:06:20,005] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 17:06:20,005] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 17:06:20,005] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 17:06:20,005] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7eff68ac1210>\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 17:06:20,006] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 17:06:20,007] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:13,  7.81it/s]\titers: 100, epoch: 1 | loss: 0.7632146\n",
      "\tspeed: 0.1714s/iter; left time: 21143.1467s\n",
      "199it [00:25,  8.04it/s]\titers: 200, epoch: 1 | loss: 0.8840415\n",
      "\tspeed: 0.1263s/iter; left time: 15568.7916s\n",
      "299it [00:38,  7.47it/s]\titers: 300, epoch: 1 | loss: 0.5233962\n",
      "\tspeed: 0.1266s/iter; left time: 15593.6441s\n",
      "399it [00:51,  7.93it/s]\titers: 400, epoch: 1 | loss: 0.5995572\n",
      "\tspeed: 0.1256s/iter; left time: 15453.5099s\n",
      "499it [01:03,  7.85it/s]\titers: 500, epoch: 1 | loss: 0.4733974\n",
      "\tspeed: 0.1256s/iter; left time: 15441.7468s\n",
      "599it [01:16,  7.52it/s]\titers: 600, epoch: 1 | loss: 0.5669985\n",
      "\tspeed: 0.1270s/iter; left time: 15603.8690s\n",
      "699it [01:29,  8.23it/s]\titers: 700, epoch: 1 | loss: 0.5744451\n",
      "\tspeed: 0.1287s/iter; left time: 15801.1002s\n",
      "799it [01:42,  7.84it/s]\titers: 800, epoch: 1 | loss: 0.6591185\n",
      "\tspeed: 0.1284s/iter; left time: 15750.4993s\n",
      "899it [01:54,  7.91it/s]\titers: 900, epoch: 1 | loss: 0.5333726\n",
      "\tspeed: 0.1265s/iter; left time: 15498.2341s\n",
      "999it [02:07,  7.90it/s]\titers: 1000, epoch: 1 | loss: 0.4726268\n",
      "\tspeed: 0.1301s/iter; left time: 15926.6938s\n",
      "1099it [02:20,  7.65it/s]\titers: 1100, epoch: 1 | loss: 0.5757155\n",
      "\tspeed: 0.1291s/iter; left time: 15790.9837s\n",
      "1199it [02:33,  7.79it/s]\titers: 1200, epoch: 1 | loss: 0.5441144\n",
      "\tspeed: 0.1246s/iter; left time: 15229.2142s\n",
      "1299it [02:45,  8.15it/s]\titers: 1300, epoch: 1 | loss: 0.4975622\n",
      "\tspeed: 0.1253s/iter; left time: 15309.3238s\n",
      "1399it [02:58,  7.53it/s]\titers: 1400, epoch: 1 | loss: 0.7060887\n",
      "\tspeed: 0.1270s/iter; left time: 15500.2128s\n",
      "1499it [03:11,  7.86it/s]\titers: 1500, epoch: 1 | loss: 0.5721908\n",
      "\tspeed: 0.1284s/iter; left time: 15655.3719s\n",
      "1599it [03:24,  7.73it/s]\titers: 1600, epoch: 1 | loss: 0.4900986\n",
      "\tspeed: 0.1317s/iter; left time: 16046.7535s\n",
      "1699it [03:37,  7.91it/s]\titers: 1700, epoch: 1 | loss: 0.5379415\n",
      "\tspeed: 0.1292s/iter; left time: 15731.8067s\n",
      "1799it [03:49,  8.46it/s]\titers: 1800, epoch: 1 | loss: 0.6773072\n",
      "\tspeed: 0.1263s/iter; left time: 15367.1506s\n",
      "1899it [04:02,  7.87it/s]\titers: 1900, epoch: 1 | loss: 0.4367243\n",
      "\tspeed: 0.1291s/iter; left time: 15695.4111s\n",
      "1999it [04:15,  7.83it/s]\titers: 2000, epoch: 1 | loss: 0.3931311\n",
      "\tspeed: 0.1271s/iter; left time: 15439.1677s\n",
      "2099it [04:28,  8.05it/s]\titers: 2100, epoch: 1 | loss: 0.4821811\n",
      "\tspeed: 0.1269s/iter; left time: 15406.0943s\n",
      "2199it [04:40,  8.19it/s]\titers: 2200, epoch: 1 | loss: 0.5902278\n",
      "\tspeed: 0.1269s/iter; left time: 15383.3949s\n",
      "2299it [04:53,  7.65it/s]\titers: 2300, epoch: 1 | loss: 0.5264985\n",
      "\tspeed: 0.1271s/iter; left time: 15405.1279s\n",
      "2399it [05:06,  7.39it/s]\titers: 2400, epoch: 1 | loss: 0.5026017\n",
      "\tspeed: 0.1297s/iter; left time: 15701.0261s\n",
      "2499it [05:19,  7.69it/s]\titers: 2500, epoch: 1 | loss: 0.7311843\n",
      "\tspeed: 0.1311s/iter; left time: 15859.7274s\n",
      "2599it [05:32,  7.61it/s]\titers: 2600, epoch: 1 | loss: 0.4824869\n",
      "\tspeed: 0.1267s/iter; left time: 15311.6837s\n",
      "2699it [05:44,  7.87it/s]\titers: 2700, epoch: 1 | loss: 0.4749164\n",
      "\tspeed: 0.1262s/iter; left time: 15242.3561s\n",
      "2799it [05:57,  8.14it/s]\titers: 2800, epoch: 1 | loss: 0.5125622\n",
      "\tspeed: 0.1266s/iter; left time: 15278.6524s\n",
      "2899it [06:10,  8.14it/s]\titers: 2900, epoch: 1 | loss: 0.4854303\n",
      "\tspeed: 0.1280s/iter; left time: 15434.6616s\n",
      "2999it [06:22,  8.17it/s]\titers: 3000, epoch: 1 | loss: 0.6366180\n",
      "\tspeed: 0.1259s/iter; left time: 15169.3952s\n",
      "3099it [06:35,  7.59it/s]\titers: 3100, epoch: 1 | loss: 0.5680846\n",
      "\tspeed: 0.1281s/iter; left time: 15416.7507s\n",
      "3199it [06:48,  7.83it/s]\titers: 3200, epoch: 1 | loss: 0.6411605\n",
      "\tspeed: 0.1283s/iter; left time: 15428.6547s\n",
      "3299it [07:01,  7.76it/s]\titers: 3300, epoch: 1 | loss: 0.6505884\n",
      "\tspeed: 0.1276s/iter; left time: 15337.9040s\n",
      "3399it [07:14,  7.53it/s]\titers: 3400, epoch: 1 | loss: 0.5505480\n",
      "\tspeed: 0.1303s/iter; left time: 15642.3556s\n",
      "3499it [07:27,  7.04it/s]\titers: 3500, epoch: 1 | loss: 0.5428913\n",
      "\tspeed: 0.1307s/iter; left time: 15680.8280s\n",
      "3599it [07:40,  7.56it/s]\titers: 3600, epoch: 1 | loss: 0.5651151\n",
      "\tspeed: 0.1266s/iter; left time: 15179.2925s\n",
      "3699it [07:52,  7.73it/s]\titers: 3700, epoch: 1 | loss: 0.5822366\n",
      "\tspeed: 0.1277s/iter; left time: 15289.9211s\n",
      "3799it [08:05,  8.22it/s]\titers: 3800, epoch: 1 | loss: 0.5511368\n",
      "\tspeed: 0.1265s/iter; left time: 15133.0369s\n",
      "3899it [08:18,  7.56it/s]\titers: 3900, epoch: 1 | loss: 0.5274125\n",
      "\tspeed: 0.1288s/iter; left time: 15395.5856s\n",
      "3999it [08:31,  8.55it/s]\titers: 4000, epoch: 1 | loss: 0.5281338\n",
      "\tspeed: 0.1256s/iter; left time: 15007.9347s\n",
      "4099it [08:44,  7.26it/s]\titers: 4100, epoch: 1 | loss: 0.5932840\n",
      "\tspeed: 0.1314s/iter; left time: 15681.3613s\n",
      "4199it [08:56,  7.93it/s]\titers: 4200, epoch: 1 | loss: 0.5893905\n",
      "\tspeed: 0.1275s/iter; left time: 15208.1074s\n",
      "4299it [09:09,  7.85it/s]\titers: 4300, epoch: 1 | loss: 0.5280132\n",
      "\tspeed: 0.1264s/iter; left time: 15063.2642s\n",
      "4399it [09:22,  7.33it/s]\titers: 4400, epoch: 1 | loss: 0.6806080\n",
      "\tspeed: 0.1279s/iter; left time: 15231.2188s\n",
      "4499it [09:35,  8.04it/s]\titers: 4500, epoch: 1 | loss: 0.4017764\n",
      "\tspeed: 0.1281s/iter; left time: 15244.4157s\n",
      "4599it [09:47,  8.32it/s]\titers: 4600, epoch: 1 | loss: 0.5289353\n",
      "\tspeed: 0.1242s/iter; left time: 14759.4638s\n",
      "4699it [10:00,  8.19it/s]\titers: 4700, epoch: 1 | loss: 0.7522417\n",
      "\tspeed: 0.1276s/iter; left time: 15158.3558s\n",
      "4799it [10:13,  7.70it/s]\titers: 4800, epoch: 1 | loss: 0.6035030\n",
      "\tspeed: 0.1290s/iter; left time: 15312.0121s\n",
      "4899it [10:26,  7.74it/s]\titers: 4900, epoch: 1 | loss: 0.6541688\n",
      "\tspeed: 0.1282s/iter; left time: 15197.5273s\n",
      "4999it [10:38,  7.81it/s]\titers: 5000, epoch: 1 | loss: 0.5653636\n",
      "\tspeed: 0.1285s/iter; left time: 15219.6982s\n",
      "5099it [10:51,  7.42it/s]\titers: 5100, epoch: 1 | loss: 0.7777289\n",
      "\tspeed: 0.1275s/iter; left time: 15096.5898s\n",
      "5199it [11:04,  7.66it/s]\titers: 5200, epoch: 1 | loss: 0.6978247\n",
      "\tspeed: 0.1275s/iter; left time: 15079.5706s\n",
      "5299it [11:17,  7.70it/s]\titers: 5300, epoch: 1 | loss: 0.4345267\n",
      "\tspeed: 0.1283s/iter; left time: 15161.8237s\n",
      "5399it [11:30,  7.74it/s]\titers: 5400, epoch: 1 | loss: 0.5183298\n",
      "\tspeed: 0.1297s/iter; left time: 15318.1999s\n",
      "5499it [11:42,  7.44it/s]\titers: 5500, epoch: 1 | loss: 0.5444614\n",
      "\tspeed: 0.1262s/iter; left time: 14886.4491s\n",
      "5599it [11:55,  7.51it/s]\titers: 5600, epoch: 1 | loss: 0.5327252\n",
      "\tspeed: 0.1303s/iter; left time: 15355.7439s\n",
      "5699it [12:08,  7.65it/s]\titers: 5700, epoch: 1 | loss: 0.4955851\n",
      "\tspeed: 0.1299s/iter; left time: 15300.7668s\n",
      "5799it [12:21,  7.96it/s]\titers: 5800, epoch: 1 | loss: 0.5902805\n",
      "\tspeed: 0.1282s/iter; left time: 15079.1442s\n",
      "5899it [12:34,  7.54it/s]\titers: 5900, epoch: 1 | loss: 0.6334838\n",
      "\tspeed: 0.1268s/iter; left time: 14911.4140s\n",
      "5999it [12:47,  7.74it/s]\titers: 6000, epoch: 1 | loss: 0.7208981\n",
      "\tspeed: 0.1279s/iter; left time: 15018.0314s\n",
      "6099it [12:59,  7.83it/s]\titers: 6100, epoch: 1 | loss: 0.8047390\n",
      "\tspeed: 0.1273s/iter; left time: 14935.3050s\n",
      "6173it [13:09,  7.82it/s]\n",
      "Epoch: 1 cost time: 789.3352973461151\n",
      "1335it [01:38, 13.57it/s]\n",
      "1330it [01:38, 13.53it/s]\n",
      "Epoch: 1 | Train Loss: 0.5904894 Vali Loss: 0.6927116 Test Loss: 0.9756789 MAE Loss: 0.7239639\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:12,  8.31it/s]\titers: 100, epoch: 2 | loss: 0.6549179\n",
      "\tspeed: 2.2100s/iter; left time: 258985.2931s\n",
      "199it [00:24,  8.50it/s]\titers: 200, epoch: 2 | loss: 0.5701374\n",
      "\tspeed: 0.1209s/iter; left time: 14150.2442s\n",
      "299it [00:36,  8.10it/s]\titers: 300, epoch: 2 | loss: 0.6332349\n",
      "\tspeed: 0.1223s/iter; left time: 14313.1509s\n",
      "399it [00:49,  8.15it/s]\titers: 400, epoch: 2 | loss: 0.6024325\n",
      "\tspeed: 0.1258s/iter; left time: 14706.3315s\n",
      "499it [01:01,  8.09it/s]\titers: 500, epoch: 2 | loss: 0.5748675\n",
      "\tspeed: 0.1211s/iter; left time: 14148.6279s\n",
      "599it [01:13,  8.39it/s]\titers: 600, epoch: 2 | loss: 0.5112473\n",
      "\tspeed: 0.1230s/iter; left time: 14354.8379s\n",
      "699it [01:26,  8.27it/s]\titers: 700, epoch: 2 | loss: 0.5762221\n",
      "\tspeed: 0.1237s/iter; left time: 14422.4205s\n",
      "799it [01:38,  7.68it/s]\titers: 800, epoch: 2 | loss: 0.4658368\n",
      "\tspeed: 0.1243s/iter; left time: 14484.0857s\n",
      "899it [01:51,  8.02it/s]\titers: 900, epoch: 2 | loss: 0.4837061\n",
      "\tspeed: 0.1245s/iter; left time: 14484.8184s\n",
      "999it [02:03,  8.26it/s]\titers: 1000, epoch: 2 | loss: 0.5641392\n",
      "\tspeed: 0.1219s/iter; left time: 14178.0014s\n",
      "1099it [02:15,  7.98it/s]\titers: 1100, epoch: 2 | loss: 0.6936741\n",
      "\tspeed: 0.1229s/iter; left time: 14285.0831s\n",
      "1199it [02:28,  7.63it/s]\titers: 1200, epoch: 2 | loss: 0.6160870\n",
      "\tspeed: 0.1244s/iter; left time: 14443.5844s\n",
      "1299it [02:40,  8.56it/s]\titers: 1300, epoch: 2 | loss: 0.5037493\n",
      "\tspeed: 0.1259s/iter; left time: 14599.4977s\n",
      "1399it [02:53,  8.17it/s]\titers: 1400, epoch: 2 | loss: 0.6154495\n",
      "\tspeed: 0.1245s/iter; left time: 14433.3157s\n",
      "1499it [03:05,  8.24it/s]\titers: 1500, epoch: 2 | loss: 0.5846322\n",
      "\tspeed: 0.1195s/iter; left time: 13833.4726s\n",
      "1599it [03:17,  8.69it/s]\titers: 1600, epoch: 2 | loss: 0.4624341\n",
      "\tspeed: 0.1192s/iter; left time: 13784.6400s\n",
      "1699it [03:29,  8.58it/s]\titers: 1700, epoch: 2 | loss: 0.4731096\n",
      "\tspeed: 0.1221s/iter; left time: 14109.0318s\n",
      "1799it [03:41,  8.38it/s]\titers: 1800, epoch: 2 | loss: 0.5216296\n",
      "\tspeed: 0.1182s/iter; left time: 13652.3633s\n",
      "1899it [03:52,  9.08it/s]\titers: 1900, epoch: 2 | loss: 0.4330316\n",
      "\tspeed: 0.1130s/iter; left time: 13042.1697s\n",
      "1999it [04:04,  9.20it/s]\titers: 2000, epoch: 2 | loss: 0.5874999\n",
      "\tspeed: 0.1177s/iter; left time: 13572.3552s\n",
      "2099it [04:15,  8.08it/s]\titers: 2100, epoch: 2 | loss: 0.4425403\n",
      "\tspeed: 0.1178s/iter; left time: 13566.7337s\n",
      "2199it [04:27,  7.82it/s]\titers: 2200, epoch: 2 | loss: 0.5441217\n",
      "\tspeed: 0.1190s/iter; left time: 13695.3827s\n",
      "2299it [04:39,  8.68it/s]\titers: 2300, epoch: 2 | loss: 0.5303289\n",
      "\tspeed: 0.1195s/iter; left time: 13744.3085s\n",
      "2399it [04:51,  8.35it/s]\titers: 2400, epoch: 2 | loss: 0.8392668\n",
      "\tspeed: 0.1221s/iter; left time: 14023.6351s\n",
      "2499it [05:03,  8.20it/s]\titers: 2500, epoch: 2 | loss: 0.4899607\n",
      "\tspeed: 0.1196s/iter; left time: 13723.7827s\n",
      "2599it [05:15,  8.14it/s]\titers: 2600, epoch: 2 | loss: 0.5364912\n",
      "\tspeed: 0.1160s/iter; left time: 13306.0648s\n",
      "2699it [05:27,  8.06it/s]\titers: 2700, epoch: 2 | loss: 0.3684947\n",
      "\tspeed: 0.1202s/iter; left time: 13776.0571s\n",
      "2799it [05:39,  8.55it/s]\titers: 2800, epoch: 2 | loss: 0.4981417\n",
      "\tspeed: 0.1194s/iter; left time: 13674.7896s\n",
      "2899it [05:51,  7.70it/s]\titers: 2900, epoch: 2 | loss: 0.5210074\n",
      "\tspeed: 0.1199s/iter; left time: 13712.7922s\n",
      "2999it [06:03,  8.95it/s]\titers: 3000, epoch: 2 | loss: 0.6633897\n",
      "\tspeed: 0.1201s/iter; left time: 13723.2443s\n",
      "3099it [06:15,  8.59it/s]\titers: 3100, epoch: 2 | loss: 0.4590971\n",
      "\tspeed: 0.1196s/iter; left time: 13656.5217s\n",
      "3199it [06:27,  8.26it/s]\titers: 3200, epoch: 2 | loss: 0.7309795\n",
      "\tspeed: 0.1177s/iter; left time: 13426.5761s\n",
      "3299it [06:39,  8.38it/s]\titers: 3300, epoch: 2 | loss: 0.6413550\n",
      "\tspeed: 0.1193s/iter; left time: 13604.4262s\n",
      "3399it [06:51,  8.54it/s]\titers: 3400, epoch: 2 | loss: 0.4173091\n",
      "\tspeed: 0.1202s/iter; left time: 13692.4998s\n",
      "3499it [07:03,  7.67it/s]\titers: 3500, epoch: 2 | loss: 0.6104181\n",
      "\tspeed: 0.1228s/iter; left time: 13974.5852s\n",
      "3599it [07:15,  8.76it/s]\titers: 3600, epoch: 2 | loss: 0.6267264\n",
      "\tspeed: 0.1222s/iter; left time: 13896.5787s\n",
      "3699it [07:27,  8.55it/s]\titers: 3700, epoch: 2 | loss: 0.6771969\n",
      "\tspeed: 0.1161s/iter; left time: 13192.0828s\n",
      "3799it [07:38,  8.82it/s]\titers: 3800, epoch: 2 | loss: 0.6306971\n",
      "\tspeed: 0.1155s/iter; left time: 13102.8293s\n",
      "3899it [07:50,  8.06it/s]\titers: 3900, epoch: 2 | loss: 0.6012335\n",
      "\tspeed: 0.1190s/iter; left time: 13495.0734s\n",
      "3999it [08:02,  7.93it/s]\titers: 4000, epoch: 2 | loss: 0.5299385\n",
      "\tspeed: 0.1184s/iter; left time: 13416.1222s\n",
      "4099it [08:14,  8.63it/s]\titers: 4100, epoch: 2 | loss: 0.5010763\n",
      "\tspeed: 0.1209s/iter; left time: 13680.3358s\n",
      "4199it [08:26,  9.01it/s]\titers: 4200, epoch: 2 | loss: 0.5753731\n",
      "\tspeed: 0.1195s/iter; left time: 13514.7996s\n",
      "4299it [08:38,  8.04it/s]\titers: 4300, epoch: 2 | loss: 0.4561860\n",
      "\tspeed: 0.1205s/iter; left time: 13615.1164s\n",
      "4399it [08:50,  8.55it/s]\titers: 4400, epoch: 2 | loss: 0.6013403\n",
      "\tspeed: 0.1217s/iter; left time: 13735.3253s\n",
      "4499it [09:02,  8.43it/s]\titers: 4500, epoch: 2 | loss: 0.5424422\n",
      "\tspeed: 0.1199s/iter; left time: 13528.5453s\n",
      "4599it [09:14,  8.46it/s]\titers: 4600, epoch: 2 | loss: 0.4028132\n",
      "\tspeed: 0.1190s/iter; left time: 13407.8195s\n",
      "4699it [09:26,  8.59it/s]\titers: 4700, epoch: 2 | loss: 0.7508287\n",
      "\tspeed: 0.1195s/iter; left time: 13456.6653s\n",
      "4799it [09:38,  9.04it/s]\titers: 4800, epoch: 2 | loss: 0.5244523\n",
      "\tspeed: 0.1174s/iter; left time: 13203.4704s\n",
      "4899it [09:49,  8.42it/s]\titers: 4900, epoch: 2 | loss: 0.6821591\n",
      "\tspeed: 0.1145s/iter; left time: 12872.5866s\n",
      "4999it [10:01,  7.97it/s]\titers: 5000, epoch: 2 | loss: 0.7635208\n",
      "\tspeed: 0.1178s/iter; left time: 13224.3681s\n",
      "5099it [10:13,  8.43it/s]\titers: 5100, epoch: 2 | loss: 0.4883216\n",
      "\tspeed: 0.1197s/iter; left time: 13432.4351s\n",
      "5199it [10:25,  8.44it/s]\titers: 5200, epoch: 2 | loss: 0.6203337\n",
      "\tspeed: 0.1216s/iter; left time: 13631.1262s\n",
      "5299it [10:37,  8.71it/s]\titers: 5300, epoch: 2 | loss: 0.5775690\n",
      "\tspeed: 0.1203s/iter; left time: 13477.1503s\n",
      "5399it [10:48,  9.75it/s]\titers: 5400, epoch: 2 | loss: 0.4329383\n",
      "\tspeed: 0.1105s/iter; left time: 12359.4088s\n",
      "5499it [11:00,  8.29it/s]\titers: 5500, epoch: 2 | loss: 0.7816756\n",
      "\tspeed: 0.1161s/iter; left time: 12980.7732s\n",
      "5599it [11:12,  8.19it/s]\titers: 5600, epoch: 2 | loss: 0.4915040\n",
      "\tspeed: 0.1175s/iter; left time: 13120.5320s\n",
      "5699it [11:24,  8.49it/s]\titers: 5700, epoch: 2 | loss: 0.5739419\n",
      "\tspeed: 0.1180s/iter; left time: 13168.1342s\n",
      "5799it [11:35,  8.34it/s]\titers: 5800, epoch: 2 | loss: 0.7396359\n",
      "\tspeed: 0.1167s/iter; left time: 13015.6761s\n",
      "5899it [11:47,  8.51it/s]\titers: 5900, epoch: 2 | loss: 0.4901571\n",
      "\tspeed: 0.1185s/iter; left time: 13197.4065s\n",
      "5999it [11:59,  8.48it/s]\titers: 6000, epoch: 2 | loss: 0.4858738\n",
      "\tspeed: 0.1150s/iter; left time: 12793.7322s\n",
      "6099it [12:12,  8.05it/s]\titers: 6100, epoch: 2 | loss: 0.6519263\n",
      "\tspeed: 0.1335s/iter; left time: 14838.5964s\n",
      "6173it [12:20,  8.33it/s]\n",
      "Epoch: 2 cost time: 740.7339553833008\n",
      "1335it [01:31, 14.66it/s]\n",
      "1330it [01:31, 14.60it/s]\n",
      "Epoch: 2 | Train Loss: 0.5814893 Vali Loss: 0.6893850 Test Loss: 0.9777752 MAE Loss: 0.7250197\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:12,  8.37it/s]\titers: 100, epoch: 3 | loss: 0.3577270\n",
      "\tspeed: 2.0696s/iter; left time: 229751.8316s\n",
      "199it [00:25,  8.33it/s]\titers: 200, epoch: 3 | loss: 0.5616339\n",
      "\tspeed: 0.1260s/iter; left time: 13972.8745s\n",
      "299it [00:37,  9.38it/s]\titers: 300, epoch: 3 | loss: 0.7466713\n",
      "\tspeed: 0.1195s/iter; left time: 13237.9708s\n",
      "399it [00:49,  8.09it/s]\titers: 400, epoch: 3 | loss: 0.5147050\n",
      "\tspeed: 0.1192s/iter; left time: 13197.5013s\n",
      "499it [01:01,  8.12it/s]\titers: 500, epoch: 3 | loss: 0.5246176\n",
      "\tspeed: 0.1190s/iter; left time: 13159.6815s\n",
      "599it [01:13,  8.73it/s]\titers: 600, epoch: 3 | loss: 0.5271204\n",
      "\tspeed: 0.1201s/iter; left time: 13267.4929s\n",
      "699it [01:25,  8.42it/s]\titers: 700, epoch: 3 | loss: 0.6417077\n",
      "\tspeed: 0.1199s/iter; left time: 13241.0677s\n",
      "799it [01:37,  8.14it/s]\titers: 800, epoch: 3 | loss: 0.5755561\n",
      "\tspeed: 0.1186s/iter; left time: 13087.4374s\n",
      "899it [01:48,  8.97it/s]\titers: 900, epoch: 3 | loss: 0.6784921\n",
      "\tspeed: 0.1130s/iter; left time: 12458.9473s\n",
      "999it [02:00,  8.07it/s]\titers: 1000, epoch: 3 | loss: 0.6756874\n",
      "\tspeed: 0.1170s/iter; left time: 12879.7095s\n",
      "1099it [02:12,  8.44it/s]\titers: 1100, epoch: 3 | loss: 0.5863556\n",
      "\tspeed: 0.1191s/iter; left time: 13097.7340s\n",
      "1199it [02:23,  8.25it/s]\titers: 1200, epoch: 3 | loss: 0.4606990\n",
      "\tspeed: 0.1168s/iter; left time: 12836.2463s\n",
      "1299it [02:35,  8.73it/s]\titers: 1300, epoch: 3 | loss: 0.5964088\n",
      "\tspeed: 0.1180s/iter; left time: 12963.0911s\n",
      "1399it [02:46,  8.41it/s]\titers: 1400, epoch: 3 | loss: 0.4764197\n",
      "\tspeed: 0.1144s/iter; left time: 12554.2986s\n",
      "1499it [02:59,  4.71it/s]\titers: 1500, epoch: 3 | loss: 0.4150121\n",
      "\tspeed: 0.1304s/iter; left time: 14290.8768s\n",
      "1599it [03:20,  4.72it/s]\titers: 1600, epoch: 3 | loss: 0.5484100\n",
      "\tspeed: 0.2088s/iter; left time: 22863.1346s\n",
      "1699it [03:41,  4.69it/s]\titers: 1700, epoch: 3 | loss: 0.4862700\n",
      "\tspeed: 0.2073s/iter; left time: 22680.4521s\n",
      "1799it [04:02,  4.58it/s]\titers: 1800, epoch: 3 | loss: 0.6202707\n",
      "\tspeed: 0.2090s/iter; left time: 22843.2697s\n",
      "1899it [04:23,  4.94it/s]\titers: 1900, epoch: 3 | loss: 0.5364928\n",
      "\tspeed: 0.2097s/iter; left time: 22899.8478s\n",
      "1999it [04:44,  4.67it/s]\titers: 2000, epoch: 3 | loss: 0.6386610\n",
      "\tspeed: 0.2066s/iter; left time: 22544.6409s\n",
      "2099it [05:04,  5.11it/s]\titers: 2100, epoch: 3 | loss: 0.6280620\n",
      "\tspeed: 0.2073s/iter; left time: 22593.9998s\n",
      "2199it [05:25,  4.82it/s]\titers: 2200, epoch: 3 | loss: 0.7272115\n",
      "\tspeed: 0.2095s/iter; left time: 22815.3973s\n",
      "2299it [05:46,  4.93it/s]\titers: 2300, epoch: 3 | loss: 0.6007355\n",
      "\tspeed: 0.2076s/iter; left time: 22595.1983s\n",
      "2399it [06:07,  4.83it/s]\titers: 2400, epoch: 3 | loss: 0.4669668\n",
      "\tspeed: 0.2109s/iter; left time: 22929.6061s\n",
      "2499it [06:28,  4.63it/s]\titers: 2500, epoch: 3 | loss: 0.5147607\n",
      "\tspeed: 0.2106s/iter; left time: 22879.0030s\n",
      "2599it [06:49,  4.82it/s]\titers: 2600, epoch: 3 | loss: 0.5898887\n",
      "\tspeed: 0.2106s/iter; left time: 22858.1850s\n",
      "2699it [07:10,  4.77it/s]\titers: 2700, epoch: 3 | loss: 0.7366917\n",
      "\tspeed: 0.2102s/iter; left time: 22792.5218s\n",
      "2799it [07:31,  5.01it/s]\titers: 2800, epoch: 3 | loss: 0.6174267\n",
      "\tspeed: 0.2083s/iter; left time: 22563.5729s\n",
      "2899it [07:52,  4.83it/s]\titers: 2900, epoch: 3 | loss: 0.6664240\n",
      "\tspeed: 0.2098s/iter; left time: 22707.5100s\n",
      "2999it [08:13,  4.95it/s]\titers: 3000, epoch: 3 | loss: 0.6518285\n",
      "\tspeed: 0.2082s/iter; left time: 22513.7863s\n",
      "3099it [08:34,  6.49it/s]\titers: 3100, epoch: 3 | loss: 0.4981328\n",
      "\tspeed: 0.2070s/iter; left time: 22363.0754s\n",
      "3199it [08:51,  4.72it/s]\titers: 3200, epoch: 3 | loss: 0.8314076\n",
      "\tspeed: 0.1786s/iter; left time: 19268.5042s\n",
      "3299it [09:12,  4.91it/s]\titers: 3300, epoch: 3 | loss: 0.5666850\n",
      "\tspeed: 0.2090s/iter; left time: 22531.3535s\n",
      "3399it [09:33,  4.67it/s]\titers: 3400, epoch: 3 | loss: 0.5416598\n",
      "\tspeed: 0.2092s/iter; left time: 22538.7158s\n",
      "3499it [09:54,  4.66it/s]\titers: 3500, epoch: 3 | loss: 0.5477471\n",
      "\tspeed: 0.2091s/iter; left time: 22502.9960s\n",
      "3599it [10:15,  4.77it/s]\titers: 3600, epoch: 3 | loss: 0.6795385\n",
      "\tspeed: 0.2103s/iter; left time: 22609.5537s\n",
      "3699it [10:37,  4.75it/s]\titers: 3700, epoch: 3 | loss: 0.6674494\n",
      "\tspeed: 0.2132s/iter; left time: 22897.7676s\n",
      "3799it [10:58,  4.62it/s]\titers: 3800, epoch: 3 | loss: 0.5993128\n",
      "\tspeed: 0.2102s/iter; left time: 22562.4661s\n",
      "3899it [11:19,  4.70it/s]\titers: 3900, epoch: 3 | loss: 0.5489172\n",
      "\tspeed: 0.2108s/iter; left time: 22605.4566s\n",
      "3999it [11:40,  4.53it/s]\titers: 4000, epoch: 3 | loss: 0.5733499\n",
      "\tspeed: 0.2114s/iter; left time: 22644.5573s\n",
      "4099it [12:01,  4.62it/s]\titers: 4100, epoch: 3 | loss: 0.5092515\n",
      "\tspeed: 0.2123s/iter; left time: 22720.9820s\n",
      "4199it [12:22,  4.62it/s]\titers: 4200, epoch: 3 | loss: 0.5741461\n",
      "\tspeed: 0.2122s/iter; left time: 22692.1040s\n",
      "4299it [12:44,  4.51it/s]\titers: 4300, epoch: 3 | loss: 0.5859118\n",
      "\tspeed: 0.2123s/iter; left time: 22678.7068s\n",
      "4399it [13:05,  4.61it/s]\titers: 4400, epoch: 3 | loss: 0.4655320\n",
      "\tspeed: 0.2114s/iter; left time: 22562.6824s\n",
      "4499it [13:26,  4.68it/s]\titers: 4500, epoch: 3 | loss: 0.4510511\n",
      "\tspeed: 0.2111s/iter; left time: 22511.5666s\n",
      "4599it [13:47,  4.94it/s]\titers: 4600, epoch: 3 | loss: 0.6076874\n",
      "\tspeed: 0.2094s/iter; left time: 22309.2495s\n",
      "4699it [14:08,  4.69it/s]\titers: 4700, epoch: 3 | loss: 0.5860209\n",
      "\tspeed: 0.2103s/iter; left time: 22378.9679s\n",
      "4799it [14:28,  5.60it/s]\titers: 4800, epoch: 3 | loss: 0.4088084\n",
      "\tspeed: 0.2039s/iter; left time: 21672.7931s\n",
      "4899it [14:47,  4.83it/s]\titers: 4900, epoch: 3 | loss: 0.6493546\n",
      "\tspeed: 0.1842s/iter; left time: 19563.6045s\n",
      "4999it [15:08,  4.64it/s]\titers: 5000, epoch: 3 | loss: 0.5612037\n",
      "\tspeed: 0.2108s/iter; left time: 22373.2042s\n",
      "5099it [15:29,  4.80it/s]\titers: 5100, epoch: 3 | loss: 0.3866943\n",
      "\tspeed: 0.2110s/iter; left time: 22371.0639s\n",
      "5199it [15:50,  4.61it/s]\titers: 5200, epoch: 3 | loss: 0.6557259\n",
      "\tspeed: 0.2119s/iter; left time: 22442.3568s\n",
      "5299it [16:11,  4.72it/s]\titers: 5300, epoch: 3 | loss: 0.5529500\n",
      "\tspeed: 0.2136s/iter; left time: 22606.5135s\n",
      "5399it [16:32,  4.83it/s]\titers: 5400, epoch: 3 | loss: 0.6636403\n",
      "\tspeed: 0.2108s/iter; left time: 22284.4527s\n",
      "5499it [16:54,  4.69it/s]\titers: 5500, epoch: 3 | loss: 0.6037099\n",
      "\tspeed: 0.2114s/iter; left time: 22330.8282s\n",
      "5599it [17:15,  4.72it/s]\titers: 5600, epoch: 3 | loss: 0.4755835\n",
      "\tspeed: 0.2107s/iter; left time: 22235.6094s\n",
      "5699it [17:36,  4.85it/s]\titers: 5700, epoch: 3 | loss: 0.4188799\n",
      "\tspeed: 0.2114s/iter; left time: 22285.7208s\n",
      "5799it [17:57,  4.65it/s]\titers: 5800, epoch: 3 | loss: 0.7305996\n",
      "\tspeed: 0.2157s/iter; left time: 22712.4689s\n",
      "5899it [18:18,  4.74it/s]\titers: 5900, epoch: 3 | loss: 0.6705845\n",
      "\tspeed: 0.2123s/iter; left time: 22339.1167s\n",
      "5999it [18:40,  4.86it/s]\titers: 6000, epoch: 3 | loss: 0.3943285\n",
      "\tspeed: 0.2127s/iter; left time: 22355.3724s\n",
      "6099it [19:01,  4.93it/s]\titers: 6100, epoch: 3 | loss: 0.5070918\n",
      "\tspeed: 0.2104s/iter; left time: 22095.4289s\n",
      "6173it [19:16,  5.34it/s]\n",
      "Epoch: 3 cost time: 1156.9633522033691\n",
      "1335it [02:58,  7.47it/s]\n",
      "1330it [03:09,  7.01it/s]\n",
      "Epoch: 3 | Train Loss: 0.5749995 Vali Loss: 0.6773716 Test Loss: 0.9602837 MAE Loss: 0.7225953\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:18,  4.80it/s]\titers: 100, epoch: 4 | loss: 0.4307658\n",
      "\tspeed: 4.0526s/iter; left time: 424886.1542s\n",
      "199it [00:39,  4.67it/s]\titers: 200, epoch: 4 | loss: 0.5891964\n",
      "\tspeed: 0.2105s/iter; left time: 22053.0571s\n",
      "299it [01:00,  4.91it/s]\titers: 300, epoch: 4 | loss: 0.5736468\n",
      "\tspeed: 0.2127s/iter; left time: 22254.4918s\n",
      "399it [01:22,  4.83it/s]\titers: 400, epoch: 4 | loss: 0.7131975\n",
      "\tspeed: 0.2105s/iter; left time: 22003.0713s\n",
      "499it [01:43,  4.62it/s]\titers: 500, epoch: 4 | loss: 0.6027048\n",
      "\tspeed: 0.2130s/iter; left time: 22244.9836s\n",
      "599it [02:04,  4.70it/s]\titers: 600, epoch: 4 | loss: 0.6935390\n",
      "\tspeed: 0.2135s/iter; left time: 22278.3988s\n",
      "699it [02:25,  4.77it/s]\titers: 700, epoch: 4 | loss: 0.4986646\n",
      "\tspeed: 0.2103s/iter; left time: 21924.2705s\n",
      "799it [02:46,  4.73it/s]\titers: 800, epoch: 4 | loss: 0.3192557\n",
      "\tspeed: 0.2119s/iter; left time: 22072.6587s\n",
      "899it [03:08,  4.55it/s]\titers: 900, epoch: 4 | loss: 0.6866747\n",
      "\tspeed: 0.2137s/iter; left time: 22234.5343s\n",
      "999it [03:29,  4.62it/s]\titers: 1000, epoch: 4 | loss: 0.7105703\n",
      "\tspeed: 0.2125s/iter; left time: 22087.3925s\n",
      "1099it [03:50,  4.58it/s]\titers: 1100, epoch: 4 | loss: 0.5870566\n",
      "\tspeed: 0.2107s/iter; left time: 21882.5833s\n",
      "1199it [04:11,  5.12it/s]\titers: 1200, epoch: 4 | loss: 0.5486459\n",
      "\tspeed: 0.2103s/iter; left time: 21817.0986s\n",
      "1299it [04:32,  4.86it/s]\titers: 1300, epoch: 4 | loss: 0.6606057\n",
      "\tspeed: 0.2116s/iter; left time: 21932.6782s\n",
      "1399it [04:54,  4.67it/s]\titers: 1400, epoch: 4 | loss: 0.5877388\n",
      "\tspeed: 0.2139s/iter; left time: 22152.3060s\n",
      "1499it [05:15,  4.49it/s]\titers: 1500, epoch: 4 | loss: 0.4676531\n",
      "\tspeed: 0.2130s/iter; left time: 22036.1128s\n",
      "1599it [05:36,  4.74it/s]\titers: 1600, epoch: 4 | loss: 0.6274912\n",
      "\tspeed: 0.2147s/iter; left time: 22183.1974s\n",
      "1699it [05:56,  7.19it/s]\titers: 1700, epoch: 4 | loss: 0.5761579\n",
      "\tspeed: 0.1930s/iter; left time: 19928.3333s\n",
      "1799it [06:16,  4.66it/s]\titers: 1800, epoch: 4 | loss: 0.5719915\n",
      "\tspeed: 0.1980s/iter; left time: 20417.0123s\n",
      "1899it [06:37,  4.64it/s]\titers: 1900, epoch: 4 | loss: 0.5509753\n",
      "\tspeed: 0.2125s/iter; left time: 21896.5943s\n",
      "1999it [06:58,  4.75it/s]\titers: 2000, epoch: 4 | loss: 0.5599143\n",
      "\tspeed: 0.2120s/iter; left time: 21818.9019s\n",
      "2099it [07:19,  4.54it/s]\titers: 2100, epoch: 4 | loss: 0.5639138\n",
      "\tspeed: 0.2125s/iter; left time: 21856.8943s\n",
      "2199it [07:40,  4.98it/s]\titers: 2200, epoch: 4 | loss: 0.5027871\n",
      "\tspeed: 0.2113s/iter; left time: 21706.8453s\n",
      "2299it [08:02,  4.64it/s]\titers: 2300, epoch: 4 | loss: 0.4242846\n",
      "\tspeed: 0.2130s/iter; left time: 21863.7571s\n",
      "2399it [08:23,  4.68it/s]\titers: 2400, epoch: 4 | loss: 0.6989288\n",
      "\tspeed: 0.2150s/iter; left time: 22043.9297s\n",
      "2499it [08:44,  4.63it/s]\titers: 2500, epoch: 4 | loss: 0.6779951\n",
      "\tspeed: 0.2115s/iter; left time: 21666.1416s\n",
      "2599it [09:06,  4.64it/s]\titers: 2600, epoch: 4 | loss: 0.5823784\n",
      "\tspeed: 0.2126s/iter; left time: 21752.8208s\n",
      "2699it [09:27,  4.88it/s]\titers: 2700, epoch: 4 | loss: 0.5086272\n",
      "\tspeed: 0.2110s/iter; left time: 21568.9516s\n",
      "2799it [09:48,  4.83it/s]\titers: 2800, epoch: 4 | loss: 0.6891410\n",
      "\tspeed: 0.2108s/iter; left time: 21526.9304s\n",
      "2899it [10:09,  4.59it/s]\titers: 2900, epoch: 4 | loss: 0.6949841\n",
      "\tspeed: 0.2100s/iter; left time: 21429.4910s\n",
      "2999it [10:30,  4.76it/s]\titers: 3000, epoch: 4 | loss: 0.4473737\n",
      "\tspeed: 0.2120s/iter; left time: 21613.8261s\n",
      "3099it [10:51,  4.62it/s]\titers: 3100, epoch: 4 | loss: 0.4947166\n",
      "\tspeed: 0.2109s/iter; left time: 21477.4268s\n",
      "3199it [11:12,  4.81it/s]\titers: 3200, epoch: 4 | loss: 0.5281814\n",
      "\tspeed: 0.2122s/iter; left time: 21591.8691s\n",
      "3299it [11:34,  4.62it/s]\titers: 3300, epoch: 4 | loss: 0.5873595\n",
      "\tspeed: 0.2177s/iter; left time: 22125.3078s\n",
      "3399it [11:53,  4.99it/s]\titers: 3400, epoch: 4 | loss: 0.6100045\n",
      "\tspeed: 0.1906s/iter; left time: 19354.2165s\n",
      "3499it [12:14,  4.93it/s]\titers: 3500, epoch: 4 | loss: 0.6034686\n",
      "\tspeed: 0.2111s/iter; left time: 21418.3174s\n",
      "3599it [12:35,  4.80it/s]\titers: 3600, epoch: 4 | loss: 0.6826246\n",
      "\tspeed: 0.2123s/iter; left time: 21519.1117s\n",
      "3699it [12:57,  4.74it/s]\titers: 3700, epoch: 4 | loss: 0.5066280\n",
      "\tspeed: 0.2131s/iter; left time: 21577.7552s\n",
      "3799it [13:18,  4.91it/s]\titers: 3800, epoch: 4 | loss: 0.4379955\n",
      "\tspeed: 0.2129s/iter; left time: 21529.9376s\n",
      "3899it [13:39,  4.94it/s]\titers: 3900, epoch: 4 | loss: 0.5870035\n",
      "\tspeed: 0.2115s/iter; left time: 21373.5493s\n",
      "3999it [14:01,  4.75it/s]\titers: 4000, epoch: 4 | loss: 0.5728959\n",
      "\tspeed: 0.2137s/iter; left time: 21571.7226s\n",
      "4099it [14:22,  4.58it/s]\titers: 4100, epoch: 4 | loss: 0.5731766\n",
      "\tspeed: 0.2135s/iter; left time: 21530.7433s\n",
      "4199it [14:43,  4.68it/s]\titers: 4200, epoch: 4 | loss: 0.5618702\n",
      "\tspeed: 0.2138s/iter; left time: 21540.7483s\n",
      "4299it [15:04,  4.97it/s]\titers: 4300, epoch: 4 | loss: 0.8150735\n",
      "\tspeed: 0.2121s/iter; left time: 21348.0211s\n",
      "4399it [15:26,  4.68it/s]\titers: 4400, epoch: 4 | loss: 0.5282975\n",
      "\tspeed: 0.2111s/iter; left time: 21223.6349s\n",
      "4499it [15:47,  4.81it/s]\titers: 4500, epoch: 4 | loss: 0.5867887\n",
      "\tspeed: 0.2113s/iter; left time: 21219.6463s\n",
      "4599it [16:08,  4.74it/s]\titers: 4600, epoch: 4 | loss: 0.4728411\n",
      "\tspeed: 0.2137s/iter; left time: 21440.8970s\n",
      "4699it [16:29,  4.75it/s]\titers: 4700, epoch: 4 | loss: 0.5134742\n",
      "\tspeed: 0.2137s/iter; left time: 21426.5095s\n",
      "4799it [16:51,  4.73it/s]\titers: 4800, epoch: 4 | loss: 0.5456181\n",
      "\tspeed: 0.2139s/iter; left time: 21420.2946s\n",
      "4899it [17:12,  4.76it/s]\titers: 4900, epoch: 4 | loss: 0.5024219\n",
      "\tspeed: 0.2133s/iter; left time: 21335.9949s\n",
      "4999it [17:32,  6.88it/s]\titers: 5000, epoch: 4 | loss: 0.6601782\n",
      "\tspeed: 0.1956s/iter; left time: 19552.4102s\n",
      "5099it [17:52,  4.80it/s]\titers: 5100, epoch: 4 | loss: 0.4293348\n",
      "\tspeed: 0.2001s/iter; left time: 19976.3585s\n",
      "5199it [18:13,  4.71it/s]\titers: 5200, epoch: 4 | loss: 0.5421291\n",
      "\tspeed: 0.2109s/iter; left time: 21038.5415s\n",
      "5299it [18:34,  4.82it/s]\titers: 5300, epoch: 4 | loss: 0.4043629\n",
      "\tspeed: 0.2083s/iter; left time: 20750.8737s\n",
      "5399it [18:55,  4.67it/s]\titers: 5400, epoch: 4 | loss: 0.5989574\n",
      "\tspeed: 0.2134s/iter; left time: 21242.8266s\n",
      "5499it [19:16,  4.68it/s]\titers: 5500, epoch: 4 | loss: 0.6964184\n",
      "\tspeed: 0.2123s/iter; left time: 21109.4467s\n",
      "5599it [19:38,  4.72it/s]\titers: 5600, epoch: 4 | loss: 0.5818158\n",
      "\tspeed: 0.2123s/iter; left time: 21093.5553s\n",
      "5699it [19:59,  4.76it/s]\titers: 5700, epoch: 4 | loss: 0.6353354\n",
      "\tspeed: 0.2129s/iter; left time: 21127.8653s\n",
      "5799it [20:20,  4.75it/s]\titers: 5800, epoch: 4 | loss: 0.5890097\n",
      "\tspeed: 0.2137s/iter; left time: 21187.2960s\n",
      "5899it [20:41,  4.70it/s]\titers: 5900, epoch: 4 | loss: 0.5257157\n",
      "\tspeed: 0.2109s/iter; left time: 20889.2184s\n",
      "5999it [21:03,  4.58it/s]\titers: 6000, epoch: 4 | loss: 0.4894216\n",
      "\tspeed: 0.2143s/iter; left time: 21199.6204s\n",
      "6099it [21:24,  4.78it/s]\titers: 6100, epoch: 4 | loss: 0.6102594\n",
      "\tspeed: 0.2082s/iter; left time: 20577.6565s\n",
      "6173it [21:39,  4.75it/s]\n",
      "Epoch: 4 cost time: 1299.7136342525482\n",
      "1335it [02:18,  9.63it/s]\n",
      "1330it [01:37, 13.65it/s]\n",
      "Epoch: 4 | Train Loss: 0.5707311 Vali Loss: 0.6749991 Test Loss: 0.9552386 MAE Loss: 0.7182932\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:11,  8.71it/s]\titers: 100, epoch: 5 | loss: 0.7197388\n",
      "\tspeed: 2.6566s/iter; left time: 262126.2396s\n",
      "199it [00:24,  7.60it/s]\titers: 200, epoch: 5 | loss: 0.6723690\n",
      "\tspeed: 0.1234s/iter; left time: 12164.1174s\n",
      "299it [00:36,  8.44it/s]\titers: 300, epoch: 5 | loss: 0.6017015\n",
      "\tspeed: 0.1234s/iter; left time: 12150.6911s\n",
      "399it [00:48,  9.17it/s]\titers: 400, epoch: 5 | loss: 0.5278537\n",
      "\tspeed: 0.1167s/iter; left time: 11483.1510s\n",
      "499it [00:59,  8.31it/s]\titers: 500, epoch: 5 | loss: 0.5353554\n",
      "\tspeed: 0.1146s/iter; left time: 11263.9364s\n",
      "599it [01:11,  8.00it/s]\titers: 600, epoch: 5 | loss: 0.6006016\n",
      "\tspeed: 0.1216s/iter; left time: 11932.5601s\n",
      "699it [01:23,  8.47it/s]\titers: 700, epoch: 5 | loss: 0.7665150\n",
      "\tspeed: 0.1220s/iter; left time: 11962.1236s\n",
      "799it [01:36,  7.97it/s]\titers: 800, epoch: 5 | loss: 0.5600566\n",
      "\tspeed: 0.1232s/iter; left time: 12073.3937s\n",
      "899it [01:48,  8.24it/s]\titers: 900, epoch: 5 | loss: 0.5196498\n",
      "\tspeed: 0.1199s/iter; left time: 11731.9878s\n",
      "999it [02:00,  8.61it/s]\titers: 1000, epoch: 5 | loss: 0.4603030\n",
      "\tspeed: 0.1240s/iter; left time: 12123.1633s\n",
      "1099it [02:12,  8.29it/s]\titers: 1100, epoch: 5 | loss: 0.5490630\n",
      "\tspeed: 0.1229s/iter; left time: 12002.1345s\n",
      "1199it [02:25,  7.78it/s]\titers: 1200, epoch: 5 | loss: 0.4662113\n",
      "\tspeed: 0.1239s/iter; left time: 12088.5577s\n",
      "1299it [02:37,  8.03it/s]\titers: 1300, epoch: 5 | loss: 0.4719550\n",
      "\tspeed: 0.1195s/iter; left time: 11646.8331s\n",
      "1399it [02:49,  9.16it/s]\titers: 1400, epoch: 5 | loss: 0.6909527\n",
      "\tspeed: 0.1186s/iter; left time: 11549.2485s\n",
      "1499it [03:01,  8.08it/s]\titers: 1500, epoch: 5 | loss: 0.5767331\n",
      "\tspeed: 0.1187s/iter; left time: 11546.2019s\n",
      "1598it [03:12,  8.69it/s]\titers: 1600, epoch: 5 | loss: 0.4249845\n",
      "\tspeed: 0.1184s/iter; left time: 11506.7474s\n",
      "1699it [03:24,  8.62it/s]\titers: 1700, epoch: 5 | loss: 0.4597760\n",
      "\tspeed: 0.1172s/iter; left time: 11380.5632s\n",
      "1799it [03:36,  7.90it/s]\titers: 1800, epoch: 5 | loss: 0.4643101\n",
      "\tspeed: 0.1185s/iter; left time: 11489.3579s\n",
      "1899it [03:48,  8.34it/s]\titers: 1900, epoch: 5 | loss: 0.4425587\n",
      "\tspeed: 0.1186s/iter; left time: 11491.8202s\n",
      "1999it [04:00,  8.77it/s]\titers: 2000, epoch: 5 | loss: 0.5534421\n",
      "\tspeed: 0.1195s/iter; left time: 11565.9337s\n",
      "2099it [04:12,  8.81it/s]\titers: 2100, epoch: 5 | loss: 0.4491745\n",
      "\tspeed: 0.1187s/iter; left time: 11475.6100s\n",
      "2199it [04:24,  8.41it/s]\titers: 2200, epoch: 5 | loss: 0.4139954\n",
      "\tspeed: 0.1200s/iter; left time: 11587.3683s\n",
      "2299it [04:36,  8.22it/s]\titers: 2300, epoch: 5 | loss: 0.6167009\n",
      "\tspeed: 0.1201s/iter; left time: 11586.8892s\n",
      "2399it [04:48,  8.13it/s]\titers: 2400, epoch: 5 | loss: 0.6438671\n",
      "\tspeed: 0.1197s/iter; left time: 11536.7711s\n",
      "2499it [05:00,  8.33it/s]\titers: 2500, epoch: 5 | loss: 0.5036230\n",
      "\tspeed: 0.1197s/iter; left time: 11523.4462s\n",
      "2599it [05:12,  8.29it/s]\titers: 2600, epoch: 5 | loss: 0.6714202\n",
      "\tspeed: 0.1219s/iter; left time: 11720.4453s\n",
      "2699it [05:24,  8.41it/s]\titers: 2700, epoch: 5 | loss: 0.5661246\n",
      "\tspeed: 0.1210s/iter; left time: 11620.1968s\n",
      "2799it [05:36,  8.62it/s]\titers: 2800, epoch: 5 | loss: 0.6864578\n",
      "\tspeed: 0.1184s/iter; left time: 11359.5139s\n",
      "2899it [05:48,  8.08it/s]\titers: 2900, epoch: 5 | loss: 0.5900971\n",
      "\tspeed: 0.1200s/iter; left time: 11508.3069s\n",
      "2999it [06:00,  8.55it/s]\titers: 3000, epoch: 5 | loss: 0.6723349\n",
      "\tspeed: 0.1198s/iter; left time: 11475.7424s\n",
      "3099it [06:12,  8.23it/s]\titers: 3100, epoch: 5 | loss: 0.5403499\n",
      "\tspeed: 0.1186s/iter; left time: 11350.5566s\n",
      "3199it [06:24,  8.19it/s]\titers: 3200, epoch: 5 | loss: 0.6141508\n",
      "\tspeed: 0.1203s/iter; left time: 11494.4823s\n",
      "3299it [06:36,  8.69it/s]\titers: 3300, epoch: 5 | loss: 0.6019301\n",
      "\tspeed: 0.1204s/iter; left time: 11490.9962s\n",
      "3399it [06:47,  8.29it/s]\titers: 3400, epoch: 5 | loss: 0.5772216\n",
      "\tspeed: 0.1186s/iter; left time: 11314.5041s\n",
      "3499it [07:00,  8.97it/s]\titers: 3500, epoch: 5 | loss: 0.4877359\n",
      "\tspeed: 0.1205s/iter; left time: 11480.1121s\n",
      "3599it [07:12,  8.15it/s]\titers: 3600, epoch: 5 | loss: 0.4855958\n",
      "\tspeed: 0.1226s/iter; left time: 11668.0908s\n",
      "3699it [07:23,  8.81it/s]\titers: 3700, epoch: 5 | loss: 0.5712053\n",
      "\tspeed: 0.1163s/iter; left time: 11060.1659s\n",
      "3799it [07:36,  8.10it/s]\titers: 3800, epoch: 5 | loss: 0.6435326\n",
      "\tspeed: 0.1207s/iter; left time: 11465.0594s\n",
      "3899it [07:47,  9.07it/s]\titers: 3900, epoch: 5 | loss: 0.8830101\n",
      "\tspeed: 0.1191s/iter; left time: 11299.8661s\n",
      "3999it [07:59,  8.83it/s]\titers: 4000, epoch: 5 | loss: 0.6613908\n",
      "\tspeed: 0.1186s/iter; left time: 11239.2657s\n",
      "4099it [08:11,  8.10it/s]\titers: 4100, epoch: 5 | loss: 0.5237954\n",
      "\tspeed: 0.1175s/iter; left time: 11119.3829s\n",
      "4199it [08:23,  8.22it/s]\titers: 4200, epoch: 5 | loss: 0.5866869\n",
      "\tspeed: 0.1184s/iter; left time: 11201.4210s\n",
      "4299it [08:34,  9.62it/s]\titers: 4300, epoch: 5 | loss: 0.5988020\n",
      "\tspeed: 0.1092s/iter; left time: 10317.4687s\n",
      "4398it [08:44,  9.56it/s]\titers: 4400, epoch: 5 | loss: 0.5419378\n",
      "\tspeed: 0.1053s/iter; left time: 9936.6894s\n",
      "4499it [08:56,  8.73it/s]\titers: 4500, epoch: 5 | loss: 0.6537161\n",
      "\tspeed: 0.1191s/iter; left time: 11226.7900s\n",
      "4599it [09:08,  8.08it/s]\titers: 4600, epoch: 5 | loss: 0.7881358\n",
      "\tspeed: 0.1189s/iter; left time: 11195.7182s\n",
      "4699it [09:20,  8.98it/s]\titers: 4700, epoch: 5 | loss: 0.5784363\n",
      "\tspeed: 0.1177s/iter; left time: 11075.5860s\n",
      "4799it [09:32,  8.29it/s]\titers: 4800, epoch: 5 | loss: 0.5733222\n",
      "\tspeed: 0.1197s/iter; left time: 11245.2837s\n",
      "4899it [09:43,  8.53it/s]\titers: 4900, epoch: 5 | loss: 0.8021736\n",
      "\tspeed: 0.1153s/iter; left time: 10820.5602s\n",
      "4999it [09:55,  9.04it/s]\titers: 5000, epoch: 5 | loss: 0.6434231\n",
      "\tspeed: 0.1180s/iter; left time: 11064.0579s\n",
      "5099it [10:07,  8.30it/s]\titers: 5100, epoch: 5 | loss: 0.4728524\n",
      "\tspeed: 0.1202s/iter; left time: 11262.2850s\n",
      "5199it [10:19,  8.53it/s]\titers: 5200, epoch: 5 | loss: 0.4545078\n",
      "\tspeed: 0.1190s/iter; left time: 11135.7919s\n",
      "5299it [10:31,  8.21it/s]\titers: 5300, epoch: 5 | loss: 0.6506145\n",
      "\tspeed: 0.1184s/iter; left time: 11071.1216s\n",
      "5399it [10:43,  8.48it/s]\titers: 5400, epoch: 5 | loss: 0.7454448\n",
      "\tspeed: 0.1188s/iter; left time: 11089.7020s\n",
      "5499it [10:55,  8.03it/s]\titers: 5500, epoch: 5 | loss: 0.5037539\n",
      "\tspeed: 0.1203s/iter; left time: 11216.7675s\n",
      "5599it [11:07,  8.25it/s]\titers: 5600, epoch: 5 | loss: 0.5969961\n",
      "\tspeed: 0.1255s/iter; left time: 11691.7059s\n",
      "5699it [11:20,  8.57it/s]\titers: 5700, epoch: 5 | loss: 0.5830021\n",
      "\tspeed: 0.1221s/iter; left time: 11367.5016s\n",
      "5799it [11:32,  8.83it/s]\titers: 5800, epoch: 5 | loss: 0.5505312\n",
      "\tspeed: 0.1202s/iter; left time: 11176.3662s\n",
      "5899it [11:44,  8.01it/s]\titers: 5900, epoch: 5 | loss: 0.6978744\n",
      "\tspeed: 0.1213s/iter; left time: 11268.2629s\n",
      "5999it [11:56,  8.32it/s]\titers: 6000, epoch: 5 | loss: 0.5775065\n",
      "\tspeed: 0.1214s/iter; left time: 11257.9815s\n",
      "6099it [12:08,  8.24it/s]\titers: 6100, epoch: 5 | loss: 0.4145487\n",
      "\tspeed: 0.1213s/iter; left time: 11242.7765s\n",
      "6173it [12:17,  8.37it/s]\n",
      "Epoch: 5 cost time: 737.6429948806763\n",
      "1335it [01:37, 13.65it/s]\n",
      "1330it [01:39, 13.35it/s]\n",
      "Epoch: 5 | Train Loss: 0.5723025 Vali Loss: 0.6838046 Test Loss: 0.9632824 MAE Loss: 0.7203413\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:12,  8.43it/s]\titers: 100, epoch: 6 | loss: 0.5353568\n",
      "\tspeed: 2.1925s/iter; left time: 202794.8429s\n",
      "199it [00:24,  7.64it/s]\titers: 200, epoch: 6 | loss: 0.5483151\n",
      "\tspeed: 0.1248s/iter; left time: 11531.6469s\n",
      "299it [00:36,  7.89it/s]\titers: 300, epoch: 6 | loss: 0.4272746\n",
      "\tspeed: 0.1193s/iter; left time: 11007.8189s\n",
      "399it [00:48,  8.54it/s]\titers: 400, epoch: 6 | loss: 0.5281785\n",
      "\tspeed: 0.1198s/iter; left time: 11043.8947s\n",
      "499it [01:00,  8.16it/s]\titers: 500, epoch: 6 | loss: 0.6402125\n",
      "\tspeed: 0.1187s/iter; left time: 10932.9485s\n",
      "599it [01:12,  8.11it/s]\titers: 600, epoch: 6 | loss: 0.5290207\n",
      "\tspeed: 0.1202s/iter; left time: 11059.3104s\n",
      "699it [01:24,  8.61it/s]\titers: 700, epoch: 6 | loss: 0.3451240\n",
      "\tspeed: 0.1193s/iter; left time: 10967.5010s\n",
      "799it [01:36,  8.11it/s]\titers: 800, epoch: 6 | loss: 0.5542553\n",
      "\tspeed: 0.1201s/iter; left time: 11023.8690s\n",
      "899it [01:48,  8.60it/s]\titers: 900, epoch: 6 | loss: 0.6871211\n",
      "\tspeed: 0.1201s/iter; left time: 11012.4920s\n",
      "999it [02:00,  8.93it/s]\titers: 1000, epoch: 6 | loss: 0.6383198\n",
      "\tspeed: 0.1192s/iter; left time: 10917.5960s\n",
      "1099it [02:12,  8.60it/s]\titers: 1100, epoch: 6 | loss: 0.5820944\n",
      "\tspeed: 0.1160s/iter; left time: 10616.9296s\n",
      "1199it [02:23,  8.46it/s]\titers: 1200, epoch: 6 | loss: 0.6074889\n",
      "\tspeed: 0.1167s/iter; left time: 10665.3820s\n",
      "1299it [02:35,  8.52it/s]\titers: 1300, epoch: 6 | loss: 0.7864135\n",
      "\tspeed: 0.1225s/iter; left time: 11179.4524s\n",
      "1399it [02:48,  8.24it/s]\titers: 1400, epoch: 6 | loss: 0.5516699\n",
      "\tspeed: 0.1210s/iter; left time: 11038.9738s\n",
      "1499it [03:00,  7.94it/s]\titers: 1500, epoch: 6 | loss: 0.5945444\n",
      "\tspeed: 0.1270s/iter; left time: 11570.3199s\n",
      "1599it [03:13,  7.55it/s]\titers: 1600, epoch: 6 | loss: 0.7479055\n",
      "\tspeed: 0.1249s/iter; left time: 11367.1323s\n",
      "1699it [03:25,  8.24it/s]\titers: 1700, epoch: 6 | loss: 0.7227657\n",
      "\tspeed: 0.1232s/iter; left time: 11196.1286s\n",
      "1799it [03:37,  8.35it/s]\titers: 1800, epoch: 6 | loss: 0.6320150\n",
      "\tspeed: 0.1174s/iter; left time: 10663.7091s\n",
      "1898it [03:49,  9.09it/s]\titers: 1900, epoch: 6 | loss: 0.6565335\n",
      "\tspeed: 0.1224s/iter; left time: 11100.4150s\n",
      "1999it [04:01,  8.53it/s]\titers: 2000, epoch: 6 | loss: 0.5131229\n",
      "\tspeed: 0.1178s/iter; left time: 10674.6975s\n",
      "2099it [04:13,  8.32it/s]\titers: 2100, epoch: 6 | loss: 0.5447799\n",
      "\tspeed: 0.1178s/iter; left time: 10662.0694s\n",
      "2199it [04:25,  8.50it/s]\titers: 2200, epoch: 6 | loss: 0.5439060\n",
      "\tspeed: 0.1194s/iter; left time: 10789.4061s\n",
      "2299it [04:37,  8.37it/s]\titers: 2300, epoch: 6 | loss: 0.6401721\n",
      "\tspeed: 0.1211s/iter; left time: 10936.8881s\n",
      "2399it [04:49,  8.32it/s]\titers: 2400, epoch: 6 | loss: 0.6043202\n",
      "\tspeed: 0.1187s/iter; left time: 10710.4094s\n",
      "2499it [05:01,  7.59it/s]\titers: 2500, epoch: 6 | loss: 0.3876600\n",
      "\tspeed: 0.1238s/iter; left time: 11156.4877s\n",
      "2599it [05:13,  7.94it/s]\titers: 2600, epoch: 6 | loss: 0.5481002\n",
      "\tspeed: 0.1238s/iter; left time: 11142.2078s\n",
      "2699it [05:26,  8.24it/s]\titers: 2700, epoch: 6 | loss: 0.5635677\n",
      "\tspeed: 0.1223s/iter; left time: 10992.9685s\n",
      "2799it [05:38,  7.85it/s]\titers: 2800, epoch: 6 | loss: 0.4579565\n",
      "\tspeed: 0.1216s/iter; left time: 10916.3803s\n",
      "2899it [05:50,  8.00it/s]\titers: 2900, epoch: 6 | loss: 0.4552781\n",
      "\tspeed: 0.1190s/iter; left time: 10677.1942s\n",
      "2999it [06:02,  8.63it/s]\titers: 3000, epoch: 6 | loss: 0.5546107\n",
      "\tspeed: 0.1223s/iter; left time: 10959.4367s\n",
      "3099it [06:14,  7.79it/s]\titers: 3100, epoch: 6 | loss: 0.6077936\n",
      "\tspeed: 0.1206s/iter; left time: 10796.8831s\n",
      "3199it [06:26,  8.35it/s]\titers: 3200, epoch: 6 | loss: 0.8401738\n",
      "\tspeed: 0.1212s/iter; left time: 10831.7892s\n",
      "3299it [06:38,  8.40it/s]\titers: 3300, epoch: 6 | loss: 0.7324317\n",
      "\tspeed: 0.1196s/iter; left time: 10678.8053s\n",
      "3399it [06:50,  8.46it/s]\titers: 3400, epoch: 6 | loss: 0.5501433\n",
      "\tspeed: 0.1176s/iter; left time: 10490.9199s\n",
      "3499it [07:02,  8.59it/s]\titers: 3500, epoch: 6 | loss: 0.6322832\n",
      "\tspeed: 0.1205s/iter; left time: 10738.7627s\n",
      "3599it [07:14,  8.39it/s]\titers: 3600, epoch: 6 | loss: 0.6973584\n",
      "\tspeed: 0.1216s/iter; left time: 10819.9289s\n",
      "3699it [07:26,  8.86it/s]\titers: 3700, epoch: 6 | loss: 0.5041248\n",
      "\tspeed: 0.1177s/iter; left time: 10460.4209s\n",
      "3799it [07:38,  7.73it/s]\titers: 3800, epoch: 6 | loss: 0.3960837\n",
      "\tspeed: 0.1221s/iter; left time: 10840.8220s\n",
      "3899it [07:51,  6.90it/s]\titers: 3900, epoch: 6 | loss: 0.4146070\n",
      "\tspeed: 0.1266s/iter; left time: 11229.5726s\n",
      "3999it [08:03,  7.98it/s]\titers: 4000, epoch: 6 | loss: 0.8845859\n",
      "\tspeed: 0.1218s/iter; left time: 10788.9298s\n",
      "4099it [08:15,  8.15it/s]\titers: 4100, epoch: 6 | loss: 0.5886806\n",
      "\tspeed: 0.1240s/iter; left time: 10976.7962s\n",
      "4199it [08:27,  8.17it/s]\titers: 4200, epoch: 6 | loss: 0.6344356\n",
      "\tspeed: 0.1224s/iter; left time: 10816.8335s\n",
      "4298it [08:40,  8.00it/s]\titers: 4300, epoch: 6 | loss: 0.5515956\n",
      "\tspeed: 0.1238s/iter; left time: 10933.2660s\n",
      "4399it [08:52,  7.99it/s]\titers: 4400, epoch: 6 | loss: 0.5017285\n",
      "\tspeed: 0.1227s/iter; left time: 10825.9703s\n",
      "4499it [09:04,  7.67it/s]\titers: 4500, epoch: 6 | loss: 0.6239752\n",
      "\tspeed: 0.1231s/iter; left time: 10845.3985s\n",
      "4599it [09:17,  7.64it/s]\titers: 4600, epoch: 6 | loss: 0.6112328\n",
      "\tspeed: 0.1255s/iter; left time: 11045.6635s\n",
      "4699it [09:30,  7.76it/s]\titers: 4700, epoch: 6 | loss: 0.5278112\n",
      "\tspeed: 0.1266s/iter; left time: 11125.4733s\n",
      "4799it [09:42,  8.05it/s]\titers: 4800, epoch: 6 | loss: 0.5448828\n",
      "\tspeed: 0.1260s/iter; left time: 11065.8789s\n",
      "4899it [09:55,  7.48it/s]\titers: 4900, epoch: 6 | loss: 0.7062623\n",
      "\tspeed: 0.1289s/iter; left time: 11302.3220s\n",
      "4999it [10:08,  7.61it/s]\titers: 5000, epoch: 6 | loss: 0.4317902\n",
      "\tspeed: 0.1270s/iter; left time: 11126.8154s\n",
      "5099it [10:20,  7.92it/s]\titers: 5100, epoch: 6 | loss: 0.5968004\n",
      "\tspeed: 0.1255s/iter; left time: 10979.4085s\n",
      "5199it [10:33,  7.80it/s]\titers: 5200, epoch: 6 | loss: 0.4582001\n",
      "\tspeed: 0.1250s/iter; left time: 10921.2538s\n",
      "5299it [10:45,  7.45it/s]\titers: 5300, epoch: 6 | loss: 0.4760480\n",
      "\tspeed: 0.1268s/iter; left time: 11066.0115s\n",
      "5399it [10:59,  8.11it/s]\titers: 5400, epoch: 6 | loss: 0.6338747\n",
      "\tspeed: 0.1307s/iter; left time: 11398.9112s\n",
      "5499it [11:11,  7.79it/s]\titers: 5500, epoch: 6 | loss: 0.5387878\n",
      "\tspeed: 0.1293s/iter; left time: 11264.0543s\n",
      "5599it [11:24,  7.80it/s]\titers: 5600, epoch: 6 | loss: 0.4532418\n",
      "\tspeed: 0.1256s/iter; left time: 10923.5112s\n",
      "5699it [11:37,  7.76it/s]\titers: 5700, epoch: 6 | loss: 0.3468440\n",
      "\tspeed: 0.1277s/iter; left time: 11096.0822s\n",
      "5799it [11:49,  7.45it/s]\titers: 5800, epoch: 6 | loss: 0.4170742\n",
      "\tspeed: 0.1254s/iter; left time: 10888.4894s\n",
      "5899it [12:02,  8.24it/s]\titers: 5900, epoch: 6 | loss: 0.5390517\n",
      "\tspeed: 0.1237s/iter; left time: 10722.7225s\n",
      "5999it [12:14,  8.10it/s]\titers: 6000, epoch: 6 | loss: 0.6086187\n",
      "\tspeed: 0.1243s/iter; left time: 10761.2238s\n",
      "6099it [12:27,  8.04it/s]\titers: 6100, epoch: 6 | loss: 0.5292019\n",
      "\tspeed: 0.1269s/iter; left time: 10976.9320s\n",
      "6173it [12:36,  8.16it/s]\n",
      "Epoch: 6 cost time: 756.8158748149872\n",
      "1335it [01:40, 13.24it/s]\n",
      "1330it [01:41, 13.08it/s]\n",
      "Epoch: 6 | Train Loss: 0.5714675 Vali Loss: 0.6718848 Test Loss: 0.9572547 MAE Loss: 0.7187769\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:13,  7.97it/s]\titers: 100, epoch: 7 | loss: 0.4657540\n",
      "\tspeed: 2.2816s/iter; left time: 196957.0461s\n",
      "199it [00:25,  8.00it/s]\titers: 200, epoch: 7 | loss: 0.5105084\n",
      "\tspeed: 0.1246s/iter; left time: 10740.0313s\n",
      "299it [00:37,  8.45it/s]\titers: 300, epoch: 7 | loss: 0.5830492\n",
      "\tspeed: 0.1206s/iter; left time: 10383.1868s\n",
      "399it [00:49,  8.81it/s]\titers: 400, epoch: 7 | loss: 0.5361826\n",
      "\tspeed: 0.1193s/iter; left time: 10259.0629s\n",
      "499it [01:01,  8.18it/s]\titers: 500, epoch: 7 | loss: 0.5669404\n",
      "\tspeed: 0.1200s/iter; left time: 10308.0436s\n",
      "599it [01:13,  8.49it/s]\titers: 600, epoch: 7 | loss: 0.4905106\n",
      "\tspeed: 0.1197s/iter; left time: 10274.2873s\n",
      "699it [01:25,  8.50it/s]\titers: 700, epoch: 7 | loss: 0.5609587\n",
      "\tspeed: 0.1220s/iter; left time: 10453.9269s\n",
      "799it [01:37,  8.31it/s]\titers: 800, epoch: 7 | loss: 0.4100024\n",
      "\tspeed: 0.1196s/iter; left time: 10244.7386s\n",
      "899it [01:49,  8.65it/s]\titers: 900, epoch: 7 | loss: 0.3827762\n",
      "\tspeed: 0.1218s/iter; left time: 10415.5857s\n",
      "999it [02:02,  8.57it/s]\titers: 1000, epoch: 7 | loss: 0.4914892\n",
      "\tspeed: 0.1225s/iter; left time: 10467.3629s\n",
      "1099it [02:14,  8.10it/s]\titers: 1100, epoch: 7 | loss: 0.5652027\n",
      "\tspeed: 0.1191s/iter; left time: 10158.0512s\n",
      "1199it [02:25,  8.61it/s]\titers: 1200, epoch: 7 | loss: 0.6747630\n",
      "\tspeed: 0.1181s/iter; left time: 10067.4350s\n",
      "1299it [02:37,  8.33it/s]\titers: 1300, epoch: 7 | loss: 0.6841806\n",
      "\tspeed: 0.1173s/iter; left time: 9985.1999s\n",
      "1399it [02:49,  8.65it/s]\titers: 1400, epoch: 7 | loss: 0.6932393\n",
      "\tspeed: 0.1228s/iter; left time: 10439.4034s\n",
      "1499it [03:02,  8.10it/s]\titers: 1500, epoch: 7 | loss: 0.6350797\n",
      "\tspeed: 0.1210s/iter; left time: 10277.7601s\n",
      "1599it [03:14,  7.97it/s]\titers: 1600, epoch: 7 | loss: 0.5147582\n",
      "\tspeed: 0.1200s/iter; left time: 10178.9918s\n",
      "1699it [03:26,  8.09it/s]\titers: 1700, epoch: 7 | loss: 0.6127247\n",
      "\tspeed: 0.1240s/iter; left time: 10509.1115s\n",
      "1799it [03:38,  8.37it/s]\titers: 1800, epoch: 7 | loss: 0.6895745\n",
      "\tspeed: 0.1220s/iter; left time: 10326.1075s\n",
      "1899it [03:50,  7.99it/s]\titers: 1900, epoch: 7 | loss: 0.6225856\n",
      "\tspeed: 0.1209s/iter; left time: 10220.9131s\n",
      "1999it [04:02,  8.19it/s]\titers: 2000, epoch: 7 | loss: 0.4574223\n",
      "\tspeed: 0.1223s/iter; left time: 10323.4922s\n",
      "2099it [04:14,  8.39it/s]\titers: 2100, epoch: 7 | loss: 0.5560763\n",
      "\tspeed: 0.1176s/iter; left time: 9917.4427s\n",
      "2199it [04:26,  8.57it/s]\titers: 2200, epoch: 7 | loss: 0.5801270\n",
      "\tspeed: 0.1188s/iter; left time: 10006.7802s\n",
      "2299it [04:38,  8.15it/s]\titers: 2300, epoch: 7 | loss: 0.6259835\n",
      "\tspeed: 0.1207s/iter; left time: 10154.1041s\n",
      "2399it [04:51,  7.72it/s]\titers: 2400, epoch: 7 | loss: 0.4498402\n",
      "\tspeed: 0.1240s/iter; left time: 10421.5913s\n",
      "2499it [05:03,  7.88it/s]\titers: 2500, epoch: 7 | loss: 0.5856102\n",
      "\tspeed: 0.1249s/iter; left time: 10485.2101s\n",
      "2599it [05:16,  7.78it/s]\titers: 2600, epoch: 7 | loss: 0.5270554\n",
      "\tspeed: 0.1254s/iter; left time: 10515.3932s\n",
      "2699it [05:28,  8.52it/s]\titers: 2700, epoch: 7 | loss: 0.6931132\n",
      "\tspeed: 0.1236s/iter; left time: 10347.3605s\n",
      "2799it [05:40,  8.09it/s]\titers: 2800, epoch: 7 | loss: 0.5202774\n",
      "\tspeed: 0.1184s/iter; left time: 9898.7961s\n",
      "2899it [05:53,  7.50it/s]\titers: 2900, epoch: 7 | loss: 0.5611065\n",
      "\tspeed: 0.1317s/iter; left time: 11001.8940s\n",
      "2999it [06:06,  7.80it/s]\titers: 3000, epoch: 7 | loss: 0.4658671\n",
      "\tspeed: 0.1278s/iter; left time: 10665.6274s\n",
      "3099it [06:18,  7.63it/s]\titers: 3100, epoch: 7 | loss: 0.8324874\n",
      "\tspeed: 0.1221s/iter; left time: 10175.2056s\n",
      "3199it [06:30,  8.36it/s]\titers: 3200, epoch: 7 | loss: 0.7379453\n",
      "\tspeed: 0.1232s/iter; left time: 10256.5715s\n",
      "3299it [06:43,  8.04it/s]\titers: 3300, epoch: 7 | loss: 0.4817637\n",
      "\tspeed: 0.1230s/iter; left time: 10225.8997s\n",
      "3399it [06:55,  8.18it/s]\titers: 3400, epoch: 7 | loss: 0.6489006\n",
      "\tspeed: 0.1279s/iter; left time: 10621.8297s\n",
      "3499it [07:08,  8.45it/s]\titers: 3500, epoch: 7 | loss: 0.4173515\n",
      "\tspeed: 0.1226s/iter; left time: 10168.1419s\n",
      "3599it [07:20,  7.68it/s]\titers: 3600, epoch: 7 | loss: 0.7144881\n",
      "\tspeed: 0.1269s/iter; left time: 10507.8814s\n",
      "3699it [07:33,  7.98it/s]\titers: 3700, epoch: 7 | loss: 0.6350386\n",
      "\tspeed: 0.1248s/iter; left time: 10325.6159s\n",
      "3799it [07:45,  8.01it/s]\titers: 3800, epoch: 7 | loss: 0.5512437\n",
      "\tspeed: 0.1238s/iter; left time: 10231.0116s\n",
      "3899it [07:58,  8.01it/s]\titers: 3900, epoch: 7 | loss: 0.3801798\n",
      "\tspeed: 0.1247s/iter; left time: 10289.0317s\n",
      "3999it [08:10,  8.49it/s]\titers: 4000, epoch: 7 | loss: 0.4029887\n",
      "\tspeed: 0.1220s/iter; left time: 10055.3335s\n",
      "4099it [08:23,  8.03it/s]\titers: 4100, epoch: 7 | loss: 0.4616282\n",
      "\tspeed: 0.1268s/iter; left time: 10440.0020s\n",
      "4199it [08:35,  8.24it/s]\titers: 4200, epoch: 7 | loss: 0.4522084\n",
      "\tspeed: 0.1268s/iter; left time: 10425.4311s\n",
      "4299it [08:48,  8.05it/s]\titers: 4300, epoch: 7 | loss: 0.7944344\n",
      "\tspeed: 0.1223s/iter; left time: 10044.7960s\n",
      "4399it [09:00,  8.03it/s]\titers: 4400, epoch: 7 | loss: 0.8072873\n",
      "\tspeed: 0.1201s/iter; left time: 9850.5086s\n",
      "4499it [09:12,  7.92it/s]\titers: 4500, epoch: 7 | loss: 0.6391719\n",
      "\tspeed: 0.1210s/iter; left time: 9914.9959s\n",
      "4599it [09:24,  7.91it/s]\titers: 4600, epoch: 7 | loss: 0.3943827\n",
      "\tspeed: 0.1206s/iter; left time: 9869.2063s\n",
      "4699it [09:36,  8.29it/s]\titers: 4700, epoch: 7 | loss: 0.6417586\n",
      "\tspeed: 0.1207s/iter; left time: 9868.0414s\n",
      "4799it [09:48,  7.77it/s]\titers: 4800, epoch: 7 | loss: 0.4687161\n",
      "\tspeed: 0.1238s/iter; left time: 10105.3467s\n",
      "4899it [10:00,  7.99it/s]\titers: 4900, epoch: 7 | loss: 0.7043536\n",
      "\tspeed: 0.1233s/iter; left time: 10051.7542s\n",
      "4999it [10:13,  8.37it/s]\titers: 5000, epoch: 7 | loss: 0.4265506\n",
      "\tspeed: 0.1226s/iter; left time: 9983.9014s\n",
      "5099it [10:25,  7.87it/s]\titers: 5100, epoch: 7 | loss: 0.3774584\n",
      "\tspeed: 0.1239s/iter; left time: 10075.3779s\n",
      "5199it [10:37,  8.20it/s]\titers: 5200, epoch: 7 | loss: 0.5450326\n",
      "\tspeed: 0.1225s/iter; left time: 9948.7623s\n",
      "5299it [10:49,  7.98it/s]\titers: 5300, epoch: 7 | loss: 0.6577868\n",
      "\tspeed: 0.1183s/iter; left time: 9594.6748s\n",
      "5399it [11:01,  8.50it/s]\titers: 5400, epoch: 7 | loss: 0.5829059\n",
      "\tspeed: 0.1219s/iter; left time: 9875.6643s\n",
      "5499it [11:14,  8.13it/s]\titers: 5500, epoch: 7 | loss: 0.4181142\n",
      "\tspeed: 0.1216s/iter; left time: 9839.3239s\n",
      "5599it [11:25,  8.61it/s]\titers: 5600, epoch: 7 | loss: 0.6285875\n",
      "\tspeed: 0.1181s/iter; left time: 9544.3183s\n",
      "5699it [11:37,  8.57it/s]\titers: 5700, epoch: 7 | loss: 0.5286419\n",
      "\tspeed: 0.1189s/iter; left time: 9599.3665s\n",
      "5799it [11:49,  8.03it/s]\titers: 5800, epoch: 7 | loss: 0.4825345\n",
      "\tspeed: 0.1191s/iter; left time: 9602.1561s\n",
      "5899it [12:01,  8.98it/s]\titers: 5900, epoch: 7 | loss: 0.5233198\n",
      "\tspeed: 0.1194s/iter; left time: 9613.5409s\n",
      "5999it [12:13,  8.59it/s]\titers: 6000, epoch: 7 | loss: 0.7841169\n",
      "\tspeed: 0.1166s/iter; left time: 9378.4777s\n",
      "6099it [12:25,  8.30it/s]\titers: 6100, epoch: 7 | loss: 0.4386671\n",
      "\tspeed: 0.1184s/iter; left time: 9506.8066s\n",
      "6173it [12:34,  8.19it/s]\n",
      "Epoch: 7 cost time: 754.0852782726288\n",
      "1335it [01:33, 14.35it/s]\n",
      "1330it [01:32, 14.31it/s]\n",
      "Epoch: 7 | Train Loss: 0.5662563 Vali Loss: 0.6767456 Test Loss: 0.9720357 MAE Loss: 0.7252281\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:12,  7.65it/s]\titers: 100, epoch: 8 | loss: 0.4956252\n",
      "\tspeed: 2.0794s/iter; left time: 166660.9919s\n",
      "199it [00:25,  8.77it/s]\titers: 200, epoch: 8 | loss: 0.6993201\n",
      "\tspeed: 0.1263s/iter; left time: 10112.6452s\n",
      "299it [00:37,  8.16it/s]\titers: 300, epoch: 8 | loss: 0.4276136\n",
      "\tspeed: 0.1221s/iter; left time: 9760.2750s\n",
      "399it [00:49,  7.95it/s]\titers: 400, epoch: 8 | loss: 0.7146426\n",
      "\tspeed: 0.1228s/iter; left time: 9808.5780s\n",
      "499it [01:01,  8.12it/s]\titers: 500, epoch: 8 | loss: 0.3407570\n",
      "\tspeed: 0.1204s/iter; left time: 9598.6108s\n",
      "599it [01:13,  8.72it/s]\titers: 600, epoch: 8 | loss: 0.5549252\n",
      "\tspeed: 0.1187s/iter; left time: 9457.2624s\n",
      "699it [01:25,  7.90it/s]\titers: 700, epoch: 8 | loss: 0.5646704\n",
      "\tspeed: 0.1193s/iter; left time: 9491.8779s\n",
      "799it [01:37,  8.38it/s]\titers: 800, epoch: 8 | loss: 0.4732073\n",
      "\tspeed: 0.1212s/iter; left time: 9628.9311s\n",
      "899it [01:49,  8.40it/s]\titers: 900, epoch: 8 | loss: 0.5837883\n",
      "\tspeed: 0.1211s/iter; left time: 9611.7036s\n",
      "999it [02:01,  8.37it/s]\titers: 1000, epoch: 8 | loss: 0.4989638\n",
      "\tspeed: 0.1193s/iter; left time: 9451.0086s\n",
      "1099it [02:13,  8.03it/s]\titers: 1100, epoch: 8 | loss: 0.5703362\n",
      "\tspeed: 0.1215s/iter; left time: 9618.0700s\n",
      "1199it [02:25,  8.11it/s]\titers: 1200, epoch: 8 | loss: 0.5282631\n",
      "\tspeed: 0.1202s/iter; left time: 9504.6280s\n",
      "1299it [02:37,  9.65it/s]\titers: 1300, epoch: 8 | loss: 0.4706481\n",
      "\tspeed: 0.1162s/iter; left time: 9175.4491s\n",
      "1399it [02:48,  8.35it/s]\titers: 1400, epoch: 8 | loss: 0.6313403\n",
      "\tspeed: 0.1078s/iter; left time: 8497.0251s\n",
      "1499it [03:00,  8.07it/s]\titers: 1500, epoch: 8 | loss: 0.7398061\n",
      "\tspeed: 0.1203s/iter; left time: 9476.0134s\n",
      "1599it [03:12,  8.29it/s]\titers: 1600, epoch: 8 | loss: 0.5838195\n",
      "\tspeed: 0.1212s/iter; left time: 9536.1244s\n",
      "1699it [03:24,  8.20it/s]\titers: 1700, epoch: 8 | loss: 0.6610854\n",
      "\tspeed: 0.1196s/iter; left time: 9395.5799s\n",
      "1799it [03:36,  9.19it/s]\titers: 1800, epoch: 8 | loss: 0.5826563\n",
      "\tspeed: 0.1163s/iter; left time: 9123.1635s\n",
      "1899it [03:48,  8.13it/s]\titers: 1900, epoch: 8 | loss: 0.4408518\n",
      "\tspeed: 0.1202s/iter; left time: 9417.7858s\n",
      "1999it [03:59,  8.85it/s]\titers: 2000, epoch: 8 | loss: 0.4579613\n",
      "\tspeed: 0.1168s/iter; left time: 9139.0919s\n",
      "2099it [04:11,  8.21it/s]\titers: 2100, epoch: 8 | loss: 0.5479696\n",
      "\tspeed: 0.1204s/iter; left time: 9410.0789s\n",
      "2199it [04:23,  8.33it/s]\titers: 2200, epoch: 8 | loss: 0.3909400\n",
      "\tspeed: 0.1215s/iter; left time: 9482.2892s\n",
      "2299it [04:35,  9.08it/s]\titers: 2300, epoch: 8 | loss: 0.6663407\n",
      "\tspeed: 0.1119s/iter; left time: 8719.5709s\n",
      "2399it [04:46,  8.46it/s]\titers: 2400, epoch: 8 | loss: 0.5360225\n",
      "\tspeed: 0.1190s/iter; left time: 9265.4259s\n",
      "2499it [04:58,  8.66it/s]\titers: 2500, epoch: 8 | loss: 0.7364123\n",
      "\tspeed: 0.1164s/iter; left time: 9052.5850s\n",
      "2599it [05:10,  9.10it/s]\titers: 2600, epoch: 8 | loss: 0.5772399\n",
      "\tspeed: 0.1202s/iter; left time: 9333.7334s\n",
      "2699it [05:22,  8.56it/s]\titers: 2700, epoch: 8 | loss: 0.4216800\n",
      "\tspeed: 0.1231s/iter; left time: 9547.6308s\n",
      "2799it [05:35,  7.79it/s]\titers: 2800, epoch: 8 | loss: 0.6634163\n",
      "\tspeed: 0.1211s/iter; left time: 9380.5292s\n",
      "2899it [05:46,  9.48it/s]\titers: 2900, epoch: 8 | loss: 0.5850192\n",
      "\tspeed: 0.1182s/iter; left time: 9141.0453s\n",
      "2999it [05:58,  8.08it/s]\titers: 3000, epoch: 8 | loss: 0.5925654\n",
      "\tspeed: 0.1154s/iter; left time: 8914.9539s\n",
      "3099it [06:10,  8.52it/s]\titers: 3100, epoch: 8 | loss: 0.5363227\n",
      "\tspeed: 0.1227s/iter; left time: 9463.2128s\n",
      "3199it [06:22,  7.69it/s]\titers: 3200, epoch: 8 | loss: 0.6003495\n",
      "\tspeed: 0.1194s/iter; left time: 9197.5283s\n",
      "3299it [06:34,  8.00it/s]\titers: 3300, epoch: 8 | loss: 0.5302350\n",
      "\tspeed: 0.1201s/iter; left time: 9238.6123s\n",
      "3399it [06:46,  8.22it/s]\titers: 3400, epoch: 8 | loss: 0.5201328\n",
      "\tspeed: 0.1172s/iter; left time: 9004.0513s\n",
      "3499it [06:57,  8.76it/s]\titers: 3500, epoch: 8 | loss: 0.6483572\n",
      "\tspeed: 0.1149s/iter; left time: 8819.2383s\n",
      "3599it [07:09,  8.77it/s]\titers: 3600, epoch: 8 | loss: 0.7351997\n",
      "\tspeed: 0.1205s/iter; left time: 9237.9087s\n",
      "3699it [07:21,  8.32it/s]\titers: 3700, epoch: 8 | loss: 0.6197022\n",
      "\tspeed: 0.1176s/iter; left time: 9003.5263s\n",
      "3799it [07:33,  8.80it/s]\titers: 3800, epoch: 8 | loss: 0.6484547\n",
      "\tspeed: 0.1160s/iter; left time: 8868.6532s\n",
      "3899it [07:44,  8.16it/s]\titers: 3900, epoch: 8 | loss: 0.6494599\n",
      "\tspeed: 0.1168s/iter; left time: 8918.7222s\n",
      "3999it [07:57,  8.44it/s]\titers: 4000, epoch: 8 | loss: 0.4549088\n",
      "\tspeed: 0.1211s/iter; left time: 9231.3311s\n",
      "4099it [08:08,  8.26it/s]\titers: 4100, epoch: 8 | loss: 0.6758612\n",
      "\tspeed: 0.1188s/iter; left time: 9046.0841s\n",
      "4199it [08:20,  8.36it/s]\titers: 4200, epoch: 8 | loss: 0.7794430\n",
      "\tspeed: 0.1197s/iter; left time: 9099.6722s\n",
      "4299it [08:32,  8.09it/s]\titers: 4300, epoch: 8 | loss: 0.5283393\n",
      "\tspeed: 0.1200s/iter; left time: 9114.8777s\n",
      "4399it [08:44,  8.42it/s]\titers: 4400, epoch: 8 | loss: 0.6296264\n",
      "\tspeed: 0.1204s/iter; left time: 9128.6288s\n",
      "4499it [08:56,  8.51it/s]\titers: 4500, epoch: 8 | loss: 0.7326350\n",
      "\tspeed: 0.1202s/iter; left time: 9108.5336s\n",
      "4599it [09:08,  9.11it/s]\titers: 4600, epoch: 8 | loss: 0.7713224\n",
      "\tspeed: 0.1190s/iter; left time: 9003.6487s\n",
      "4699it [09:20,  8.73it/s]\titers: 4700, epoch: 8 | loss: 0.6005565\n",
      "\tspeed: 0.1187s/iter; left time: 8964.2545s\n",
      "4799it [09:32,  8.15it/s]\titers: 4800, epoch: 8 | loss: 0.4933463\n",
      "\tspeed: 0.1193s/iter; left time: 9001.8900s\n",
      "4899it [09:44,  8.18it/s]\titers: 4900, epoch: 8 | loss: 0.8060750\n",
      "\tspeed: 0.1181s/iter; left time: 8896.2002s\n",
      "4999it [09:56,  8.53it/s]\titers: 5000, epoch: 8 | loss: 0.6930606\n",
      "\tspeed: 0.1192s/iter; left time: 8969.4556s\n",
      "5099it [10:08,  8.82it/s]\titers: 5100, epoch: 8 | loss: 0.8188760\n",
      "\tspeed: 0.1182s/iter; left time: 8884.6888s\n",
      "5199it [10:19,  8.48it/s]\titers: 5200, epoch: 8 | loss: 0.7227136\n",
      "\tspeed: 0.1169s/iter; left time: 8770.1663s\n",
      "5299it [10:31,  7.99it/s]\titers: 5300, epoch: 8 | loss: 0.6686040\n",
      "\tspeed: 0.1169s/iter; left time: 8761.3366s\n",
      "5399it [10:43,  8.55it/s]\titers: 5400, epoch: 8 | loss: 0.5344461\n",
      "\tspeed: 0.1175s/iter; left time: 8792.9345s\n",
      "5499it [10:54,  8.05it/s]\titers: 5500, epoch: 8 | loss: 0.6009458\n",
      "\tspeed: 0.1160s/iter; left time: 8668.7800s\n",
      "5599it [11:06,  8.34it/s]\titers: 5600, epoch: 8 | loss: 0.6735930\n",
      "\tspeed: 0.1161s/iter; left time: 8669.9032s\n",
      "5699it [11:18,  8.42it/s]\titers: 5700, epoch: 8 | loss: 0.5724707\n",
      "\tspeed: 0.1172s/iter; left time: 8740.9307s\n",
      "5799it [11:29,  8.16it/s]\titers: 5800, epoch: 8 | loss: 0.7853655\n",
      "\tspeed: 0.1152s/iter; left time: 8579.5620s\n",
      "5899it [11:41,  8.96it/s]\titers: 5900, epoch: 8 | loss: 0.8096478\n",
      "\tspeed: 0.1151s/iter; left time: 8555.9785s\n",
      "5999it [11:53,  8.77it/s]\titers: 6000, epoch: 8 | loss: 0.5539635\n",
      "\tspeed: 0.1182s/iter; left time: 8775.0281s\n",
      "6099it [12:05,  8.67it/s]\titers: 6100, epoch: 8 | loss: 0.6714419\n",
      "\tspeed: 0.1192s/iter; left time: 8841.0838s\n",
      "6173it [12:13,  8.41it/s]\n",
      "Epoch: 8 cost time: 733.6326794624329\n",
      "1335it [01:29, 14.94it/s]\n",
      "1330it [01:35, 13.88it/s]\n",
      "Epoch: 8 | Train Loss: 0.5644058 Vali Loss: 0.6806304 Test Loss: 0.9647706 MAE Loss: 0.7199007\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0004000000\n",
      "learning_rate 0.01\n",
      "lr 0.0003999999999999993\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 9 | loss: 0.6294437\n",
      "\tspeed: 2.0653s/iter; left time: 152785.4219s\n",
      "199it [00:24,  8.37it/s]\titers: 200, epoch: 9 | loss: 0.6759732\n",
      "\tspeed: 0.1211s/iter; left time: 8946.0623s\n",
      "299it [00:36,  8.20it/s]\titers: 300, epoch: 9 | loss: 0.6831900\n",
      "\tspeed: 0.1230s/iter; left time: 9074.0658s\n",
      "399it [00:48,  8.05it/s]\titers: 400, epoch: 9 | loss: 0.5348594\n",
      "\tspeed: 0.1204s/iter; left time: 8867.2689s\n",
      "499it [01:01,  8.10it/s]\titers: 500, epoch: 9 | loss: 0.5974728\n",
      "\tspeed: 0.1228s/iter; left time: 9037.6678s\n",
      "599it [01:13,  8.07it/s]\titers: 600, epoch: 9 | loss: 0.6679230\n",
      "\tspeed: 0.1222s/iter; left time: 8975.3593s\n",
      "699it [01:25,  8.08it/s]\titers: 700, epoch: 9 | loss: 0.5873758\n",
      "\tspeed: 0.1233s/iter; left time: 9043.9807s\n",
      "799it [01:37,  8.14it/s]\titers: 800, epoch: 9 | loss: 0.6474904\n",
      "\tspeed: 0.1221s/iter; left time: 8949.0882s\n",
      "899it [01:50,  8.06it/s]\titers: 900, epoch: 9 | loss: 0.5985667\n",
      "\tspeed: 0.1231s/iter; left time: 9006.1479s\n",
      "999it [02:02,  7.94it/s]\titers: 1000, epoch: 9 | loss: 0.6543392\n",
      "\tspeed: 0.1245s/iter; left time: 9099.8231s\n",
      "1099it [02:14,  8.41it/s]\titers: 1100, epoch: 9 | loss: 0.6081644\n",
      "\tspeed: 0.1235s/iter; left time: 9009.1981s\n",
      "1199it [02:26,  8.15it/s]\titers: 1200, epoch: 9 | loss: 0.6736346\n",
      "\tspeed: 0.1182s/iter; left time: 8610.9222s\n",
      "1299it [02:39,  7.99it/s]\titers: 1300, epoch: 9 | loss: 0.5415317\n",
      "\tspeed: 0.1248s/iter; left time: 9081.1222s\n",
      "1399it [02:51,  8.15it/s]\titers: 1400, epoch: 9 | loss: 0.7397463\n",
      "\tspeed: 0.1231s/iter; left time: 8947.0893s\n",
      "1499it [03:03,  7.99it/s]\titers: 1500, epoch: 9 | loss: 0.8288015\n",
      "\tspeed: 0.1231s/iter; left time: 8933.7618s\n",
      "1599it [03:15,  8.13it/s]\titers: 1600, epoch: 9 | loss: 0.4755182\n",
      "\tspeed: 0.1198s/iter; left time: 8680.2218s\n",
      "1699it [03:28,  7.97it/s]\titers: 1700, epoch: 9 | loss: 0.5091479\n",
      "\tspeed: 0.1234s/iter; left time: 8930.0400s\n",
      "1799it [03:40,  8.28it/s]\titers: 1800, epoch: 9 | loss: 0.4503031\n",
      "\tspeed: 0.1250s/iter; left time: 9033.3183s\n",
      "1899it [03:52,  8.10it/s]\titers: 1900, epoch: 9 | loss: 0.4327848\n",
      "\tspeed: 0.1217s/iter; left time: 8786.3959s\n",
      "1999it [04:05,  8.21it/s]\titers: 2000, epoch: 9 | loss: 0.4828230\n",
      "\tspeed: 0.1228s/iter; left time: 8851.1150s\n",
      "2099it [04:17,  8.08it/s]\titers: 2100, epoch: 9 | loss: 0.4799967\n",
      "\tspeed: 0.1235s/iter; left time: 8888.4131s\n",
      "2199it [04:29,  7.95it/s]\titers: 2200, epoch: 9 | loss: 0.4986416\n",
      "\tspeed: 0.1233s/iter; left time: 8859.4762s\n",
      "2299it [04:42,  8.00it/s]\titers: 2300, epoch: 9 | loss: 0.6990933\n",
      "\tspeed: 0.1249s/iter; left time: 8962.5630s\n",
      "2399it [04:54,  7.92it/s]\titers: 2400, epoch: 9 | loss: 0.6469290\n",
      "\tspeed: 0.1229s/iter; left time: 8810.4766s\n",
      "2499it [05:06,  8.16it/s]\titers: 2500, epoch: 9 | loss: 0.6095014\n",
      "\tspeed: 0.1212s/iter; left time: 8672.1264s\n",
      "2599it [05:19,  7.85it/s]\titers: 2600, epoch: 9 | loss: 0.5407240\n",
      "\tspeed: 0.1266s/iter; left time: 9050.9155s\n",
      "2699it [05:31,  8.30it/s]\titers: 2700, epoch: 9 | loss: 0.3976075\n",
      "\tspeed: 0.1238s/iter; left time: 8836.6937s\n",
      "2799it [05:44,  7.62it/s]\titers: 2800, epoch: 9 | loss: 0.5779778\n",
      "\tspeed: 0.1244s/iter; left time: 8867.4074s\n",
      "2899it [05:56,  7.86it/s]\titers: 2900, epoch: 9 | loss: 0.7608888\n",
      "\tspeed: 0.1266s/iter; left time: 9011.4926s\n",
      "2999it [06:09,  8.62it/s]\titers: 3000, epoch: 9 | loss: 0.6176359\n",
      "\tspeed: 0.1220s/iter; left time: 8670.3219s\n",
      "3099it [06:21,  8.67it/s]\titers: 3100, epoch: 9 | loss: 0.5002189\n",
      "\tspeed: 0.1212s/iter; left time: 8603.8111s\n",
      "3199it [06:33,  8.17it/s]\titers: 3200, epoch: 9 | loss: 0.5093759\n",
      "\tspeed: 0.1234s/iter; left time: 8743.2469s\n",
      "3299it [06:45,  7.95it/s]\titers: 3300, epoch: 9 | loss: 0.5424361\n",
      "\tspeed: 0.1198s/iter; left time: 8479.1423s\n",
      "3399it [06:57,  7.81it/s]\titers: 3400, epoch: 9 | loss: 0.5451214\n",
      "\tspeed: 0.1228s/iter; left time: 8675.7836s\n",
      "3499it [07:09,  8.45it/s]\titers: 3500, epoch: 9 | loss: 0.4430877\n",
      "\tspeed: 0.1219s/iter; left time: 8603.9249s\n",
      "3599it [07:22,  8.10it/s]\titers: 3600, epoch: 9 | loss: 0.5885896\n",
      "\tspeed: 0.1210s/iter; left time: 8527.8947s\n",
      "3699it [07:33,  8.20it/s]\titers: 3700, epoch: 9 | loss: 0.4627658\n",
      "\tspeed: 0.1164s/iter; left time: 8190.3248s\n",
      "3799it [07:45,  8.36it/s]\titers: 3800, epoch: 9 | loss: 0.5398180\n",
      "\tspeed: 0.1194s/iter; left time: 8393.5600s\n",
      "3899it [07:57,  8.22it/s]\titers: 3900, epoch: 9 | loss: 0.6182476\n",
      "\tspeed: 0.1198s/iter; left time: 8404.4187s\n",
      "3999it [08:09,  8.00it/s]\titers: 4000, epoch: 9 | loss: 0.6263523\n",
      "\tspeed: 0.1146s/iter; left time: 8029.8218s\n",
      "4099it [08:21,  8.09it/s]\titers: 4100, epoch: 9 | loss: 0.5694700\n",
      "\tspeed: 0.1206s/iter; left time: 8436.2336s\n",
      "4199it [08:32,  8.36it/s]\titers: 4200, epoch: 9 | loss: 0.5217465\n",
      "\tspeed: 0.1163s/iter; left time: 8127.7837s\n",
      "4299it [08:44,  7.85it/s]\titers: 4300, epoch: 9 | loss: 0.6974280\n",
      "\tspeed: 0.1215s/iter; left time: 8479.6865s\n",
      "4399it [08:56,  8.44it/s]\titers: 4400, epoch: 9 | loss: 0.4025957\n",
      "\tspeed: 0.1167s/iter; left time: 8128.3834s\n",
      "4499it [09:08,  8.47it/s]\titers: 4500, epoch: 9 | loss: 0.5768776\n",
      "\tspeed: 0.1197s/iter; left time: 8328.3400s\n",
      "4599it [09:20,  8.44it/s]\titers: 4600, epoch: 9 | loss: 0.4982260\n",
      "\tspeed: 0.1186s/iter; left time: 8239.3730s\n",
      "4699it [09:32,  8.06it/s]\titers: 4700, epoch: 9 | loss: 0.6921902\n",
      "\tspeed: 0.1171s/iter; left time: 8123.4465s\n",
      "4799it [09:44,  7.92it/s]\titers: 4800, epoch: 9 | loss: 0.5952088\n",
      "\tspeed: 0.1213s/iter; left time: 8401.2935s\n",
      "4899it [09:56,  8.14it/s]\titers: 4900, epoch: 9 | loss: 0.4125597\n",
      "\tspeed: 0.1197s/iter; left time: 8281.3121s\n",
      "4999it [10:08,  8.33it/s]\titers: 5000, epoch: 9 | loss: 0.4481761\n",
      "\tspeed: 0.1188s/iter; left time: 8209.4322s\n",
      "5099it [10:20,  8.30it/s]\titers: 5100, epoch: 9 | loss: 0.5184730\n",
      "\tspeed: 0.1205s/iter; left time: 8312.7489s\n",
      "5199it [10:32,  7.90it/s]\titers: 5200, epoch: 9 | loss: 0.6613244\n",
      "\tspeed: 0.1257s/iter; left time: 8654.7731s\n",
      "5299it [10:44,  7.91it/s]\titers: 5300, epoch: 9 | loss: 0.5527714\n",
      "\tspeed: 0.1227s/iter; left time: 8442.3383s\n",
      "5399it [10:57,  7.71it/s]\titers: 5400, epoch: 9 | loss: 0.6089861\n",
      "\tspeed: 0.1224s/iter; left time: 8408.6597s\n",
      "5499it [11:09,  8.78it/s]\titers: 5500, epoch: 9 | loss: 0.5281377\n",
      "\tspeed: 0.1192s/iter; left time: 8172.5420s\n",
      "5599it [11:20,  8.14it/s]\titers: 5600, epoch: 9 | loss: 0.6091911\n",
      "\tspeed: 0.1176s/iter; left time: 8055.3768s\n",
      "5699it [11:32,  8.58it/s]\titers: 5700, epoch: 9 | loss: 0.5924188\n",
      "\tspeed: 0.1140s/iter; left time: 7797.0170s\n",
      "5799it [11:43,  8.45it/s]\titers: 5800, epoch: 9 | loss: 0.6770678\n",
      "\tspeed: 0.1155s/iter; left time: 7882.9421s\n",
      "5899it [11:55,  7.99it/s]\titers: 5900, epoch: 9 | loss: 0.5834612\n",
      "\tspeed: 0.1181s/iter; left time: 8054.4556s\n",
      "5999it [12:07,  8.50it/s]\titers: 6000, epoch: 9 | loss: 0.6344473\n",
      "\tspeed: 0.1167s/iter; left time: 7941.4284s\n",
      "6099it [12:19,  8.88it/s]\titers: 6100, epoch: 9 | loss: 0.6557243\n",
      "\tspeed: 0.1176s/iter; left time: 7994.8219s\n",
      "6173it [12:28,  8.25it/s]\n",
      "Epoch: 9 cost time: 748.1382048130035\n",
      "1335it [01:35, 13.97it/s]\n",
      "1330it [01:35, 13.93it/s]\n",
      "Epoch: 9 | Train Loss: 0.5643199 Vali Loss: 0.6741087 Test Loss: 0.9674726 MAE Loss: 0.7246464\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 161.61633711655935 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.01 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 19:47:52,852] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 19:47:53,706] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 19:47:53,707] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 19:47:53,707] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 19:47:54,544] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 19:47:54,544] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 19:47:55,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 19:47:55,444] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 19:47:55,444] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 19:47:55,445] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 19:47:55,445] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 19:47:55,445] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 19:47:55,445] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 19:47:55,445] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 19:47:55,445] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 19:47:55,445] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 19:47:55,711] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 19:47:55,711] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 19:47:55,712] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 174.47 GB, percent = 23.1%\n",
      "[2024-05-15 19:47:55,822] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 19:47:55,823] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 19:47:55,823] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 174.47 GB, percent = 23.1%\n",
      "[2024-05-15 19:47:55,823] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 19:47:55,933] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 19:47:55,933] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 19:47:55,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 174.47 GB, percent = 23.1%\n",
      "[2024-05-15 19:47:55,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 19:47:55,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 19:47:55,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 19:47:55,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 19:47:55,934] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f75d1d57650>\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 19:47:55,935] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 19:47:55,936] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:13,  8.21it/s]\titers: 100, epoch: 1 | loss: 1.0350595\n",
      "\tspeed: 0.1697s/iter; left time: 20935.3932s\n",
      "199it [00:26,  7.57it/s]\titers: 200, epoch: 1 | loss: 1.1736686\n",
      "\tspeed: 0.1283s/iter; left time: 15808.4108s\n",
      "299it [00:38,  7.59it/s]\titers: 300, epoch: 1 | loss: 0.8380752\n",
      "\tspeed: 0.1257s/iter; left time: 15481.2510s\n",
      "399it [00:51,  7.56it/s]\titers: 400, epoch: 1 | loss: 0.9216660\n",
      "\tspeed: 0.1273s/iter; left time: 15669.5620s\n",
      "499it [01:03,  7.72it/s]\titers: 500, epoch: 1 | loss: 0.7760668\n",
      "\tspeed: 0.1258s/iter; left time: 15465.7861s\n",
      "599it [01:16,  7.69it/s]\titers: 600, epoch: 1 | loss: 0.8311495\n",
      "\tspeed: 0.1286s/iter; left time: 15798.4836s\n",
      "699it [01:29,  7.44it/s]\titers: 700, epoch: 1 | loss: 0.6404149\n",
      "\tspeed: 0.1274s/iter; left time: 15645.5674s\n",
      "799it [01:42,  8.70it/s]\titers: 800, epoch: 1 | loss: 0.7590930\n",
      "\tspeed: 0.1283s/iter; left time: 15742.2570s\n",
      "899it [01:54,  7.66it/s]\titers: 900, epoch: 1 | loss: 0.6377093\n",
      "\tspeed: 0.1253s/iter; left time: 15361.2796s\n",
      "999it [02:07,  7.51it/s]\titers: 1000, epoch: 1 | loss: 0.5605145\n",
      "\tspeed: 0.1276s/iter; left time: 15624.4686s\n",
      "1099it [02:20,  8.39it/s]\titers: 1100, epoch: 1 | loss: 0.6151934\n",
      "\tspeed: 0.1238s/iter; left time: 15149.7296s\n",
      "1199it [02:32,  7.68it/s]\titers: 1200, epoch: 1 | loss: 0.5872863\n",
      "\tspeed: 0.1243s/iter; left time: 15192.3980s\n",
      "1299it [02:44,  8.02it/s]\titers: 1300, epoch: 1 | loss: 0.5223733\n",
      "\tspeed: 0.1254s/iter; left time: 15321.3762s\n",
      "1399it [02:57,  8.16it/s]\titers: 1400, epoch: 1 | loss: 0.7048261\n",
      "\tspeed: 0.1276s/iter; left time: 15573.4063s\n",
      "1499it [03:09,  8.04it/s]\titers: 1500, epoch: 1 | loss: 0.5831348\n",
      "\tspeed: 0.1214s/iter; left time: 14810.8160s\n",
      "1599it [03:22,  8.46it/s]\titers: 1600, epoch: 1 | loss: 0.4898421\n",
      "\tspeed: 0.1257s/iter; left time: 15320.7101s\n",
      "1699it [03:34,  8.03it/s]\titers: 1700, epoch: 1 | loss: 0.5495403\n",
      "\tspeed: 0.1245s/iter; left time: 15156.0350s\n",
      "1799it [03:47,  7.93it/s]\titers: 1800, epoch: 1 | loss: 0.6627052\n",
      "\tspeed: 0.1270s/iter; left time: 15455.0825s\n",
      "1899it [04:00,  8.44it/s]\titers: 1900, epoch: 1 | loss: 0.4176158\n",
      "\tspeed: 0.1246s/iter; left time: 15144.1702s\n",
      "1999it [04:12,  8.72it/s]\titers: 2000, epoch: 1 | loss: 0.4127894\n",
      "\tspeed: 0.1230s/iter; left time: 14942.5394s\n",
      "2099it [04:24,  7.89it/s]\titers: 2100, epoch: 1 | loss: 0.5145430\n",
      "\tspeed: 0.1231s/iter; left time: 14935.9358s\n",
      "2199it [04:36,  7.86it/s]\titers: 2200, epoch: 1 | loss: 0.5516882\n",
      "\tspeed: 0.1228s/iter; left time: 14886.2990s\n",
      "2299it [04:48,  8.33it/s]\titers: 2300, epoch: 1 | loss: 0.5310477\n",
      "\tspeed: 0.1194s/iter; left time: 14467.9176s\n",
      "2399it [05:01,  8.09it/s]\titers: 2400, epoch: 1 | loss: 0.5281458\n",
      "\tspeed: 0.1231s/iter; left time: 14898.2888s\n",
      "2499it [05:13,  7.91it/s]\titers: 2500, epoch: 1 | loss: 0.6565997\n",
      "\tspeed: 0.1240s/iter; left time: 14995.1932s\n",
      "2599it [05:26,  8.23it/s]\titers: 2600, epoch: 1 | loss: 0.4705327\n",
      "\tspeed: 0.1255s/iter; left time: 15169.0476s\n",
      "2699it [05:38,  8.19it/s]\titers: 2700, epoch: 1 | loss: 0.4374727\n",
      "\tspeed: 0.1231s/iter; left time: 14865.5285s\n",
      "2799it [05:50,  8.60it/s]\titers: 2800, epoch: 1 | loss: 0.5021568\n",
      "\tspeed: 0.1228s/iter; left time: 14817.0474s\n",
      "2899it [06:02,  8.38it/s]\titers: 2900, epoch: 1 | loss: 0.4474609\n",
      "\tspeed: 0.1202s/iter; left time: 14496.8924s\n",
      "2999it [06:14,  8.32it/s]\titers: 3000, epoch: 1 | loss: 0.5924911\n",
      "\tspeed: 0.1210s/iter; left time: 14578.0185s\n",
      "3099it [06:27,  7.87it/s]\titers: 3100, epoch: 1 | loss: 0.5596270\n",
      "\tspeed: 0.1213s/iter; left time: 14605.5979s\n",
      "3199it [06:39,  8.08it/s]\titers: 3200, epoch: 1 | loss: 0.6114394\n",
      "\tspeed: 0.1241s/iter; left time: 14924.3076s\n",
      "3299it [06:51,  7.69it/s]\titers: 3300, epoch: 1 | loss: 0.6192849\n",
      "\tspeed: 0.1249s/iter; left time: 15012.4879s\n",
      "3399it [07:04,  7.91it/s]\titers: 3400, epoch: 1 | loss: 0.5337247\n",
      "\tspeed: 0.1246s/iter; left time: 14964.5006s\n",
      "3499it [07:16,  7.88it/s]\titers: 3500, epoch: 1 | loss: 0.5050324\n",
      "\tspeed: 0.1229s/iter; left time: 14748.5701s\n",
      "3599it [07:28,  7.89it/s]\titers: 3600, epoch: 1 | loss: 0.6035306\n",
      "\tspeed: 0.1228s/iter; left time: 14717.8134s\n",
      "3699it [07:41,  7.91it/s]\titers: 3700, epoch: 1 | loss: 0.5298890\n",
      "\tspeed: 0.1235s/iter; left time: 14790.0422s\n",
      "3799it [07:53,  7.98it/s]\titers: 3800, epoch: 1 | loss: 0.4801223\n",
      "\tspeed: 0.1173s/iter; left time: 14032.0920s\n",
      "3899it [08:05,  8.15it/s]\titers: 3900, epoch: 1 | loss: 0.4947606\n",
      "\tspeed: 0.1230s/iter; left time: 14709.1691s\n",
      "3999it [08:17,  8.21it/s]\titers: 4000, epoch: 1 | loss: 0.5229055\n",
      "\tspeed: 0.1229s/iter; left time: 14677.7987s\n",
      "4099it [08:29,  8.21it/s]\titers: 4100, epoch: 1 | loss: 0.5663961\n",
      "\tspeed: 0.1220s/iter; left time: 14565.3748s\n",
      "4199it [08:42,  7.78it/s]\titers: 4200, epoch: 1 | loss: 0.5450186\n",
      "\tspeed: 0.1262s/iter; left time: 15051.3301s\n",
      "4299it [08:55,  7.66it/s]\titers: 4300, epoch: 1 | loss: 0.5040300\n",
      "\tspeed: 0.1260s/iter; left time: 15012.9966s\n",
      "4399it [09:07,  8.03it/s]\titers: 4400, epoch: 1 | loss: 0.7067549\n",
      "\tspeed: 0.1284s/iter; left time: 15287.2711s\n",
      "4499it [09:20,  7.74it/s]\titers: 4500, epoch: 1 | loss: 0.3867581\n",
      "\tspeed: 0.1249s/iter; left time: 14860.8020s\n",
      "4599it [09:32,  8.40it/s]\titers: 4600, epoch: 1 | loss: 0.5167680\n",
      "\tspeed: 0.1246s/iter; left time: 14810.5369s\n",
      "4699it [09:45,  7.99it/s]\titers: 4700, epoch: 1 | loss: 0.6035526\n",
      "\tspeed: 0.1266s/iter; left time: 15038.6178s\n",
      "4799it [09:57,  7.84it/s]\titers: 4800, epoch: 1 | loss: 0.5852122\n",
      "\tspeed: 0.1216s/iter; left time: 14431.0288s\n",
      "4899it [10:09,  7.96it/s]\titers: 4900, epoch: 1 | loss: 0.6320381\n",
      "\tspeed: 0.1232s/iter; left time: 14603.5444s\n",
      "4999it [10:22,  8.23it/s]\titers: 5000, epoch: 1 | loss: 0.5272874\n",
      "\tspeed: 0.1242s/iter; left time: 14716.5923s\n",
      "5099it [10:34,  7.84it/s]\titers: 5100, epoch: 1 | loss: 0.7049454\n",
      "\tspeed: 0.1230s/iter; left time: 14558.8920s\n",
      "5199it [10:47,  7.79it/s]\titers: 5200, epoch: 1 | loss: 0.6210777\n",
      "\tspeed: 0.1252s/iter; left time: 14812.1359s\n",
      "5299it [10:59,  8.08it/s]\titers: 5300, epoch: 1 | loss: 0.4026238\n",
      "\tspeed: 0.1265s/iter; left time: 14941.9892s\n",
      "5399it [11:12,  7.99it/s]\titers: 5400, epoch: 1 | loss: 0.4931925\n",
      "\tspeed: 0.1245s/iter; left time: 14699.0914s\n",
      "5499it [11:24,  7.65it/s]\titers: 5500, epoch: 1 | loss: 0.4814030\n",
      "\tspeed: 0.1258s/iter; left time: 14839.2968s\n",
      "5599it [11:37,  8.20it/s]\titers: 5600, epoch: 1 | loss: 0.5377865\n",
      "\tspeed: 0.1230s/iter; left time: 14496.8133s\n",
      "5699it [11:49,  7.93it/s]\titers: 5700, epoch: 1 | loss: 0.4487399\n",
      "\tspeed: 0.1208s/iter; left time: 14224.7201s\n",
      "5799it [12:01,  8.27it/s]\titers: 5800, epoch: 1 | loss: 0.5207012\n",
      "\tspeed: 0.1210s/iter; left time: 14239.8970s\n",
      "5899it [12:13,  8.73it/s]\titers: 5900, epoch: 1 | loss: 0.5477594\n",
      "\tspeed: 0.1214s/iter; left time: 14269.5685s\n",
      "5999it [12:25,  7.80it/s]\titers: 6000, epoch: 1 | loss: 0.6814324\n",
      "\tspeed: 0.1203s/iter; left time: 14126.5446s\n",
      "6099it [12:37,  8.29it/s]\titers: 6100, epoch: 1 | loss: 0.6583470\n",
      "\tspeed: 0.1192s/iter; left time: 13984.1275s\n",
      "6173it [12:46,  8.05it/s]\n",
      "Epoch: 1 cost time: 766.4189453125\n",
      "1335it [01:32, 14.39it/s]\n",
      "1330it [01:32, 14.44it/s]\n",
      "Epoch: 1 | Train Loss: 0.6122268 Vali Loss: 0.6642407 Test Loss: 0.9579989 MAE Loss: 0.7194460\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:12,  8.77it/s]\titers: 100, epoch: 2 | loss: 0.5205378\n",
      "\tspeed: 2.0768s/iter; left time: 243373.6239s\n",
      "199it [00:24,  8.30it/s]\titers: 200, epoch: 2 | loss: 0.5372984\n",
      "\tspeed: 0.1193s/iter; left time: 13964.0591s\n",
      "299it [00:35,  8.63it/s]\titers: 300, epoch: 2 | loss: 0.5898291\n",
      "\tspeed: 0.1140s/iter; left time: 13335.3563s\n",
      "399it [00:46,  9.41it/s]\titers: 400, epoch: 2 | loss: 0.5012470\n",
      "\tspeed: 0.1154s/iter; left time: 13487.2585s\n",
      "499it [00:58,  8.80it/s]\titers: 500, epoch: 2 | loss: 0.5011083\n",
      "\tspeed: 0.1166s/iter; left time: 13613.6470s\n",
      "599it [01:10,  8.06it/s]\titers: 600, epoch: 2 | loss: 0.4356012\n",
      "\tspeed: 0.1170s/iter; left time: 13650.8313s\n",
      "699it [01:22,  8.41it/s]\titers: 700, epoch: 2 | loss: 0.5387341\n",
      "\tspeed: 0.1189s/iter; left time: 13860.6013s\n",
      "799it [01:33,  8.21it/s]\titers: 800, epoch: 2 | loss: 0.4451123\n",
      "\tspeed: 0.1143s/iter; left time: 13316.0329s\n",
      "899it [01:45,  8.78it/s]\titers: 900, epoch: 2 | loss: 0.4656076\n",
      "\tspeed: 0.1156s/iter; left time: 13457.7779s\n",
      "999it [01:56,  9.03it/s]\titers: 1000, epoch: 2 | loss: 0.5463808\n",
      "\tspeed: 0.1156s/iter; left time: 13446.6553s\n",
      "1099it [02:08,  8.36it/s]\titers: 1100, epoch: 2 | loss: 0.5914419\n",
      "\tspeed: 0.1173s/iter; left time: 13631.6541s\n",
      "1199it [02:20,  8.01it/s]\titers: 1200, epoch: 2 | loss: 0.5185384\n",
      "\tspeed: 0.1172s/iter; left time: 13606.9382s\n",
      "1299it [02:31,  8.66it/s]\titers: 1300, epoch: 2 | loss: 0.4920901\n",
      "\tspeed: 0.1153s/iter; left time: 13374.8826s\n",
      "1398it [02:43,  8.28it/s]\titers: 1400, epoch: 2 | loss: 0.5547030\n",
      "\tspeed: 0.1145s/iter; left time: 13273.8325s\n",
      "1499it [02:54,  8.02it/s]\titers: 1500, epoch: 2 | loss: 0.4339053\n",
      "\tspeed: 0.1141s/iter; left time: 13214.0741s\n",
      "1599it [03:06,  8.31it/s]\titers: 1600, epoch: 2 | loss: 0.4285789\n",
      "\tspeed: 0.1169s/iter; left time: 13528.8926s\n",
      "1699it [03:18,  8.18it/s]\titers: 1700, epoch: 2 | loss: 0.4473134\n",
      "\tspeed: 0.1179s/iter; left time: 13623.8171s\n",
      "1799it [03:29,  8.05it/s]\titers: 1800, epoch: 2 | loss: 0.4656596\n",
      "\tspeed: 0.1181s/iter; left time: 13639.9329s\n",
      "1899it [03:41,  8.42it/s]\titers: 1900, epoch: 2 | loss: 0.3861623\n",
      "\tspeed: 0.1176s/iter; left time: 13571.2522s\n",
      "1999it [03:53,  8.93it/s]\titers: 2000, epoch: 2 | loss: 0.5311062\n",
      "\tspeed: 0.1150s/iter; left time: 13261.2271s\n",
      "2099it [04:05,  8.19it/s]\titers: 2100, epoch: 2 | loss: 0.4162920\n",
      "\tspeed: 0.1231s/iter; left time: 14180.8437s\n",
      "2199it [04:17,  8.43it/s]\titers: 2200, epoch: 2 | loss: 0.5439001\n",
      "\tspeed: 0.1180s/iter; left time: 13585.0255s\n",
      "2299it [04:29,  8.01it/s]\titers: 2300, epoch: 2 | loss: 0.5021603\n",
      "\tspeed: 0.1188s/iter; left time: 13661.7153s\n",
      "2399it [04:40,  8.93it/s]\titers: 2400, epoch: 2 | loss: 0.7235342\n",
      "\tspeed: 0.1163s/iter; left time: 13357.8087s\n",
      "2499it [04:52,  8.28it/s]\titers: 2500, epoch: 2 | loss: 0.4622709\n",
      "\tspeed: 0.1198s/iter; left time: 13749.4308s\n",
      "2599it [05:04,  8.52it/s]\titers: 2600, epoch: 2 | loss: 0.4757464\n",
      "\tspeed: 0.1180s/iter; left time: 13531.0686s\n",
      "2699it [05:16,  8.46it/s]\titers: 2700, epoch: 2 | loss: 0.3584988\n",
      "\tspeed: 0.1198s/iter; left time: 13725.6874s\n",
      "2799it [05:28,  8.16it/s]\titers: 2800, epoch: 2 | loss: 0.4692369\n",
      "\tspeed: 0.1222s/iter; left time: 13993.5394s\n",
      "2899it [05:40,  8.39it/s]\titers: 2900, epoch: 2 | loss: 0.4303490\n",
      "\tspeed: 0.1191s/iter; left time: 13624.1337s\n",
      "2999it [05:52,  8.57it/s]\titers: 3000, epoch: 2 | loss: 0.5514916\n",
      "\tspeed: 0.1191s/iter; left time: 13610.1298s\n",
      "3099it [06:04,  8.59it/s]\titers: 3100, epoch: 2 | loss: 0.4184789\n",
      "\tspeed: 0.1223s/iter; left time: 13970.1740s\n",
      "3199it [06:16,  8.45it/s]\titers: 3200, epoch: 2 | loss: 0.6503226\n",
      "\tspeed: 0.1184s/iter; left time: 13513.4193s\n",
      "3299it [06:28,  8.80it/s]\titers: 3300, epoch: 2 | loss: 0.5554497\n",
      "\tspeed: 0.1190s/iter; left time: 13562.5703s\n",
      "3399it [06:40,  8.96it/s]\titers: 3400, epoch: 2 | loss: 0.3302726\n",
      "\tspeed: 0.1174s/iter; left time: 13366.7847s\n",
      "3498it [06:51,  8.72it/s]\titers: 3500, epoch: 2 | loss: 0.5345624\n",
      "\tspeed: 0.1178s/iter; left time: 13404.3061s\n",
      "3599it [07:03,  8.85it/s]\titers: 3600, epoch: 2 | loss: 0.6130776\n",
      "\tspeed: 0.1192s/iter; left time: 13556.4824s\n",
      "3699it [07:16,  8.26it/s]\titers: 3700, epoch: 2 | loss: 0.5894774\n",
      "\tspeed: 0.1231s/iter; left time: 13986.2047s\n",
      "3799it [07:28,  7.80it/s]\titers: 3800, epoch: 2 | loss: 0.5528323\n",
      "\tspeed: 0.1241s/iter; left time: 14088.4937s\n",
      "3899it [07:40,  7.87it/s]\titers: 3900, epoch: 2 | loss: 0.5666353\n",
      "\tspeed: 0.1223s/iter; left time: 13861.7180s\n",
      "3999it [07:52,  8.15it/s]\titers: 4000, epoch: 2 | loss: 0.5071144\n",
      "\tspeed: 0.1205s/iter; left time: 13647.0244s\n",
      "4099it [08:05,  8.68it/s]\titers: 4100, epoch: 2 | loss: 0.4071115\n",
      "\tspeed: 0.1207s/iter; left time: 13666.5225s\n",
      "4199it [08:17,  7.78it/s]\titers: 4200, epoch: 2 | loss: 0.5372678\n",
      "\tspeed: 0.1202s/iter; left time: 13597.3791s\n",
      "4299it [08:29,  8.83it/s]\titers: 4300, epoch: 2 | loss: 0.4467373\n",
      "\tspeed: 0.1227s/iter; left time: 13865.1208s\n",
      "4399it [08:41,  7.97it/s]\titers: 4400, epoch: 2 | loss: 0.5741723\n",
      "\tspeed: 0.1215s/iter; left time: 13714.3685s\n",
      "4499it [08:53,  8.17it/s]\titers: 4500, epoch: 2 | loss: 0.4381566\n",
      "\tspeed: 0.1202s/iter; left time: 13559.3051s\n",
      "4599it [09:05,  8.42it/s]\titers: 4600, epoch: 2 | loss: 0.3550802\n",
      "\tspeed: 0.1186s/iter; left time: 13365.6258s\n",
      "4699it [09:17,  8.46it/s]\titers: 4700, epoch: 2 | loss: 0.6530964\n",
      "\tspeed: 0.1205s/iter; left time: 13571.0994s\n",
      "4799it [09:29,  8.44it/s]\titers: 4800, epoch: 2 | loss: 0.4277906\n",
      "\tspeed: 0.1187s/iter; left time: 13347.0938s\n",
      "4899it [09:41,  8.59it/s]\titers: 4900, epoch: 2 | loss: 0.6093649\n",
      "\tspeed: 0.1206s/iter; left time: 13549.8293s\n",
      "4999it [09:53,  8.38it/s]\titers: 5000, epoch: 2 | loss: 0.6208227\n",
      "\tspeed: 0.1181s/iter; left time: 13261.6895s\n",
      "5099it [10:05,  8.05it/s]\titers: 5100, epoch: 2 | loss: 0.4099069\n",
      "\tspeed: 0.1215s/iter; left time: 13631.6881s\n",
      "5199it [10:17,  8.81it/s]\titers: 5200, epoch: 2 | loss: 0.5756317\n",
      "\tspeed: 0.1171s/iter; left time: 13122.2742s\n",
      "5299it [10:28,  8.79it/s]\titers: 5300, epoch: 2 | loss: 0.4456032\n",
      "\tspeed: 0.1189s/iter; left time: 13320.4565s\n",
      "5399it [10:40,  7.92it/s]\titers: 5400, epoch: 2 | loss: 0.3497004\n",
      "\tspeed: 0.1204s/iter; left time: 13472.5046s\n",
      "5499it [10:52,  7.90it/s]\titers: 5500, epoch: 2 | loss: 0.6001871\n",
      "\tspeed: 0.1201s/iter; left time: 13423.6642s\n",
      "5599it [11:05,  7.97it/s]\titers: 5600, epoch: 2 | loss: 0.4030420\n",
      "\tspeed: 0.1207s/iter; left time: 13486.3191s\n",
      "5699it [11:16,  8.23it/s]\titers: 5700, epoch: 2 | loss: 0.4478829\n",
      "\tspeed: 0.1174s/iter; left time: 13101.2828s\n",
      "5799it [11:28,  7.87it/s]\titers: 5800, epoch: 2 | loss: 0.5975518\n",
      "\tspeed: 0.1178s/iter; left time: 13131.6461s\n",
      "5899it [11:40,  9.01it/s]\titers: 5900, epoch: 2 | loss: 0.4035054\n",
      "\tspeed: 0.1193s/iter; left time: 13283.4842s\n",
      "5999it [11:52,  8.54it/s]\titers: 6000, epoch: 2 | loss: 0.4452696\n",
      "\tspeed: 0.1190s/iter; left time: 13241.3477s\n",
      "6099it [12:04,  8.59it/s]\titers: 6100, epoch: 2 | loss: 0.5587848\n",
      "\tspeed: 0.1196s/iter; left time: 13302.2312s\n",
      "6173it [12:13,  8.42it/s]\n",
      "Epoch: 2 cost time: 733.1959497928619\n",
      "1335it [01:32, 14.37it/s]\n",
      "1330it [01:33, 14.16it/s]\n",
      "Epoch: 2 | Train Loss: 0.5148364 Vali Loss: 0.6765449 Test Loss: 1.0261936 MAE Loss: 0.7292936\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:12,  7.73it/s]\titers: 100, epoch: 3 | loss: 0.3697711\n",
      "\tspeed: 2.0860s/iter; left time: 231578.4227s\n",
      "199it [00:24,  8.59it/s]\titers: 200, epoch: 3 | loss: 0.4254112\n",
      "\tspeed: 0.1189s/iter; left time: 13184.1411s\n",
      "299it [00:36,  8.06it/s]\titers: 300, epoch: 3 | loss: 0.5581204\n",
      "\tspeed: 0.1215s/iter; left time: 13465.1090s\n",
      "399it [00:48,  8.18it/s]\titers: 400, epoch: 3 | loss: 0.4701986\n",
      "\tspeed: 0.1187s/iter; left time: 13142.3001s\n",
      "499it [01:00,  8.40it/s]\titers: 500, epoch: 3 | loss: 0.4139951\n",
      "\tspeed: 0.1197s/iter; left time: 13245.1382s\n",
      "599it [01:12,  7.60it/s]\titers: 600, epoch: 3 | loss: 0.4671984\n",
      "\tspeed: 0.1250s/iter; left time: 13814.1850s\n",
      "699it [01:25,  7.91it/s]\titers: 700, epoch: 3 | loss: 0.5199631\n",
      "\tspeed: 0.1249s/iter; left time: 13791.2418s\n",
      "799it [01:37,  7.91it/s]\titers: 800, epoch: 3 | loss: 0.4279160\n",
      "\tspeed: 0.1218s/iter; left time: 13430.8649s\n",
      "899it [01:49,  8.20it/s]\titers: 900, epoch: 3 | loss: 0.5782223\n",
      "\tspeed: 0.1191s/iter; left time: 13129.2722s\n",
      "999it [02:01,  8.90it/s]\titers: 1000, epoch: 3 | loss: 0.5028787\n",
      "\tspeed: 0.1176s/iter; left time: 12946.3090s\n",
      "1099it [02:13,  8.48it/s]\titers: 1100, epoch: 3 | loss: 0.4296008\n",
      "\tspeed: 0.1174s/iter; left time: 12921.1564s\n",
      "1199it [02:25,  8.11it/s]\titers: 1200, epoch: 3 | loss: 0.3468249\n",
      "\tspeed: 0.1199s/iter; left time: 13174.8271s\n",
      "1299it [02:36,  8.68it/s]\titers: 1300, epoch: 3 | loss: 0.5107863\n",
      "\tspeed: 0.1185s/iter; left time: 13011.0585s\n",
      "1399it [02:48,  8.70it/s]\titers: 1400, epoch: 3 | loss: 0.4227923\n",
      "\tspeed: 0.1178s/iter; left time: 12922.0743s\n",
      "1499it [03:00,  9.00it/s]\titers: 1500, epoch: 3 | loss: 0.3873299\n",
      "\tspeed: 0.1185s/iter; left time: 12992.8164s\n",
      "1599it [03:12,  8.42it/s]\titers: 1600, epoch: 3 | loss: 0.4894361\n",
      "\tspeed: 0.1183s/iter; left time: 12953.2539s\n",
      "1699it [03:24,  8.19it/s]\titers: 1700, epoch: 3 | loss: 0.3506022\n",
      "\tspeed: 0.1189s/iter; left time: 13013.0856s\n",
      "1799it [03:36,  8.24it/s]\titers: 1800, epoch: 3 | loss: 0.4925703\n",
      "\tspeed: 0.1228s/iter; left time: 13427.3209s\n",
      "1899it [03:48,  7.88it/s]\titers: 1900, epoch: 3 | loss: 0.4536445\n",
      "\tspeed: 0.1190s/iter; left time: 12991.5448s\n",
      "1999it [04:00,  8.68it/s]\titers: 2000, epoch: 3 | loss: 0.5242587\n",
      "\tspeed: 0.1179s/iter; left time: 12863.4066s\n",
      "2099it [04:12,  8.92it/s]\titers: 2100, epoch: 3 | loss: 0.4561826\n",
      "\tspeed: 0.1184s/iter; left time: 12911.6769s\n",
      "2199it [04:24,  8.64it/s]\titers: 2200, epoch: 3 | loss: 0.5311506\n",
      "\tspeed: 0.1200s/iter; left time: 13073.0371s\n",
      "2299it [04:36,  7.75it/s]\titers: 2300, epoch: 3 | loss: 0.5523453\n",
      "\tspeed: 0.1227s/iter; left time: 13357.0050s\n",
      "2399it [04:48,  8.49it/s]\titers: 2400, epoch: 3 | loss: 0.3635672\n",
      "\tspeed: 0.1224s/iter; left time: 13305.5737s\n",
      "2499it [05:01,  7.85it/s]\titers: 2500, epoch: 3 | loss: 0.4102274\n",
      "\tspeed: 0.1260s/iter; left time: 13687.7993s\n",
      "2599it [05:13,  7.87it/s]\titers: 2600, epoch: 3 | loss: 0.4488632\n",
      "\tspeed: 0.1237s/iter; left time: 13426.1572s\n",
      "2699it [05:25,  9.19it/s]\titers: 2700, epoch: 3 | loss: 0.5172368\n",
      "\tspeed: 0.1232s/iter; left time: 13356.9988s\n",
      "2799it [05:37,  7.72it/s]\titers: 2800, epoch: 3 | loss: 0.4797277\n",
      "\tspeed: 0.1183s/iter; left time: 12814.1879s\n",
      "2899it [05:49,  8.12it/s]\titers: 2900, epoch: 3 | loss: 0.4839452\n",
      "\tspeed: 0.1166s/iter; left time: 12613.7248s\n",
      "2999it [06:01,  8.45it/s]\titers: 3000, epoch: 3 | loss: 0.4609035\n",
      "\tspeed: 0.1177s/iter; left time: 12724.1555s\n",
      "3099it [06:12,  8.76it/s]\titers: 3100, epoch: 3 | loss: 0.4360812\n",
      "\tspeed: 0.1166s/iter; left time: 12592.4927s\n",
      "3199it [06:24,  8.63it/s]\titers: 3200, epoch: 3 | loss: 0.6520332\n",
      "\tspeed: 0.1175s/iter; left time: 12684.5899s\n",
      "3299it [06:36,  8.52it/s]\titers: 3300, epoch: 3 | loss: 0.4929716\n",
      "\tspeed: 0.1195s/iter; left time: 12883.3260s\n",
      "3399it [06:48,  8.41it/s]\titers: 3400, epoch: 3 | loss: 0.3977142\n",
      "\tspeed: 0.1192s/iter; left time: 12842.7627s\n",
      "3499it [07:00,  8.27it/s]\titers: 3500, epoch: 3 | loss: 0.4407060\n",
      "\tspeed: 0.1206s/iter; left time: 12974.1621s\n",
      "3599it [07:11,  8.62it/s]\titers: 3600, epoch: 3 | loss: 0.5792210\n",
      "\tspeed: 0.1135s/iter; left time: 12205.7083s\n",
      "3699it [07:23,  8.76it/s]\titers: 3700, epoch: 3 | loss: 0.4153362\n",
      "\tspeed: 0.1197s/iter; left time: 12859.2704s\n",
      "3799it [07:35,  8.75it/s]\titers: 3800, epoch: 3 | loss: 0.4399610\n",
      "\tspeed: 0.1155s/iter; left time: 12397.3786s\n",
      "3899it [07:47,  8.21it/s]\titers: 3900, epoch: 3 | loss: 0.4216745\n",
      "\tspeed: 0.1180s/iter; left time: 12648.0793s\n",
      "3998it [07:59,  7.89it/s]\titers: 4000, epoch: 3 | loss: 0.4690093\n",
      "\tspeed: 0.1204s/iter; left time: 12898.8599s\n",
      "4099it [08:10,  8.51it/s]\titers: 4100, epoch: 3 | loss: 0.3435759\n",
      "\tspeed: 0.1181s/iter; left time: 12635.4549s\n",
      "4199it [08:23,  7.85it/s]\titers: 4200, epoch: 3 | loss: 0.3837800\n",
      "\tspeed: 0.1242s/iter; left time: 13280.1668s\n",
      "4299it [08:35,  7.82it/s]\titers: 4300, epoch: 3 | loss: 0.4005912\n",
      "\tspeed: 0.1220s/iter; left time: 13026.2598s\n",
      "4399it [08:47,  8.36it/s]\titers: 4400, epoch: 3 | loss: 0.4090838\n",
      "\tspeed: 0.1215s/iter; left time: 12966.5611s\n",
      "4499it [08:59,  8.23it/s]\titers: 4500, epoch: 3 | loss: 0.3600079\n",
      "\tspeed: 0.1207s/iter; left time: 12872.4360s\n",
      "4599it [09:11,  8.89it/s]\titers: 4600, epoch: 3 | loss: 0.4886349\n",
      "\tspeed: 0.1190s/iter; left time: 12677.4008s\n",
      "4699it [09:23,  8.38it/s]\titers: 4700, epoch: 3 | loss: 0.3892591\n",
      "\tspeed: 0.1197s/iter; left time: 12735.6697s\n",
      "4799it [09:35,  8.34it/s]\titers: 4800, epoch: 3 | loss: 0.3027906\n",
      "\tspeed: 0.1175s/iter; left time: 12495.6833s\n",
      "4899it [09:47,  7.73it/s]\titers: 4900, epoch: 3 | loss: 0.4613123\n",
      "\tspeed: 0.1182s/iter; left time: 12552.1922s\n",
      "4999it [09:58,  8.77it/s]\titers: 5000, epoch: 3 | loss: 0.4629874\n",
      "\tspeed: 0.1166s/iter; left time: 12371.1076s\n",
      "5099it [10:10,  7.80it/s]\titers: 5100, epoch: 3 | loss: 0.3521277\n",
      "\tspeed: 0.1193s/iter; left time: 12649.6478s\n",
      "5199it [10:23,  8.17it/s]\titers: 5200, epoch: 3 | loss: 0.5208230\n",
      "\tspeed: 0.1241s/iter; left time: 13141.2966s\n",
      "5299it [10:35,  9.03it/s]\titers: 5300, epoch: 3 | loss: 0.3178820\n",
      "\tspeed: 0.1193s/iter; left time: 12618.9385s\n",
      "5399it [10:47,  7.97it/s]\titers: 5400, epoch: 3 | loss: 0.4606005\n",
      "\tspeed: 0.1190s/iter; left time: 12575.5770s\n",
      "5499it [10:58,  9.09it/s]\titers: 5500, epoch: 3 | loss: 0.4766640\n",
      "\tspeed: 0.1170s/iter; left time: 12352.7955s\n",
      "5599it [11:10,  8.57it/s]\titers: 5600, epoch: 3 | loss: 0.3653629\n",
      "\tspeed: 0.1180s/iter; left time: 12448.3861s\n",
      "5699it [11:22,  7.93it/s]\titers: 5700, epoch: 3 | loss: 0.3833421\n",
      "\tspeed: 0.1205s/iter; left time: 12700.5137s\n",
      "5799it [11:34,  7.67it/s]\titers: 5800, epoch: 3 | loss: 0.5069641\n",
      "\tspeed: 0.1223s/iter; left time: 12876.0651s\n",
      "5899it [11:46,  8.61it/s]\titers: 5900, epoch: 3 | loss: 0.4801165\n",
      "\tspeed: 0.1189s/iter; left time: 12508.8043s\n",
      "5999it [11:58,  9.01it/s]\titers: 6000, epoch: 3 | loss: 0.3149749\n",
      "\tspeed: 0.1178s/iter; left time: 12382.0102s\n",
      "6099it [12:10,  8.29it/s]\titers: 6100, epoch: 3 | loss: 0.3924222\n",
      "\tspeed: 0.1229s/iter; left time: 12909.9023s\n",
      "6173it [12:19,  8.34it/s]\n",
      "Epoch: 3 cost time: 739.9066581726074\n",
      "1335it [01:31, 14.57it/s]\n",
      "1330it [01:33, 14.20it/s]\n",
      "Epoch: 3 | Train Loss: 0.4472620 Vali Loss: 0.7167768 Test Loss: 1.1286490 MAE Loss: 0.7517148\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:11,  9.00it/s]\titers: 100, epoch: 4 | loss: 0.3635342\n",
      "\tspeed: 2.0659s/iter; left time: 216590.0079s\n",
      "199it [00:23,  8.19it/s]\titers: 200, epoch: 4 | loss: 0.4018804\n",
      "\tspeed: 0.1175s/iter; left time: 12309.8266s\n",
      "299it [00:35,  8.11it/s]\titers: 300, epoch: 4 | loss: 0.4117502\n",
      "\tspeed: 0.1194s/iter; left time: 12498.6728s\n",
      "399it [00:47,  7.94it/s]\titers: 400, epoch: 4 | loss: 0.4497750\n",
      "\tspeed: 0.1220s/iter; left time: 12751.5329s\n",
      "498it [00:59,  8.47it/s]\titers: 500, epoch: 4 | loss: 0.4539578\n",
      "\tspeed: 0.1181s/iter; left time: 12329.6309s\n",
      "599it [01:11,  8.78it/s]\titers: 600, epoch: 4 | loss: 0.5487088\n",
      "\tspeed: 0.1213s/iter; left time: 12655.7988s\n",
      "699it [01:23,  8.55it/s]\titers: 700, epoch: 4 | loss: 0.4134959\n",
      "\tspeed: 0.1190s/iter; left time: 12409.7417s\n",
      "799it [01:35,  7.99it/s]\titers: 800, epoch: 4 | loss: 0.3011884\n",
      "\tspeed: 0.1218s/iter; left time: 12680.9178s\n",
      "899it [01:47,  8.23it/s]\titers: 900, epoch: 4 | loss: 0.4884232\n",
      "\tspeed: 0.1191s/iter; left time: 12392.3102s\n",
      "999it [01:59,  8.02it/s]\titers: 1000, epoch: 4 | loss: 0.4432980\n",
      "\tspeed: 0.1201s/iter; left time: 12488.6136s\n",
      "1099it [02:11,  8.50it/s]\titers: 1100, epoch: 4 | loss: 0.4458143\n",
      "\tspeed: 0.1164s/iter; left time: 12088.2292s\n",
      "1199it [02:23,  9.27it/s]\titers: 1200, epoch: 4 | loss: 0.4229415\n",
      "\tspeed: 0.1192s/iter; left time: 12369.3918s\n",
      "1299it [02:35,  8.02it/s]\titers: 1300, epoch: 4 | loss: 0.4903143\n",
      "\tspeed: 0.1196s/iter; left time: 12395.8125s\n",
      "1398it [02:46,  8.60it/s]\titers: 1400, epoch: 4 | loss: 0.3952509\n",
      "\tspeed: 0.1187s/iter; left time: 12286.7143s\n",
      "1499it [02:58,  8.89it/s]\titers: 1500, epoch: 4 | loss: 0.3206429\n",
      "\tspeed: 0.1189s/iter; left time: 12301.4489s\n",
      "1599it [03:10,  8.39it/s]\titers: 1600, epoch: 4 | loss: 0.4579321\n",
      "\tspeed: 0.1149s/iter; left time: 11873.4387s\n",
      "1699it [03:22,  8.47it/s]\titers: 1700, epoch: 4 | loss: 0.3971609\n",
      "\tspeed: 0.1171s/iter; left time: 12086.8442s\n",
      "1799it [03:34,  8.45it/s]\titers: 1800, epoch: 4 | loss: 0.4128659\n",
      "\tspeed: 0.1204s/iter; left time: 12420.1315s\n",
      "1899it [03:46,  7.93it/s]\titers: 1900, epoch: 4 | loss: 0.4812525\n",
      "\tspeed: 0.1240s/iter; left time: 12779.2364s\n",
      "1999it [03:58,  8.83it/s]\titers: 2000, epoch: 4 | loss: 0.3669342\n",
      "\tspeed: 0.1190s/iter; left time: 12247.8956s\n",
      "2099it [04:10,  8.41it/s]\titers: 2100, epoch: 4 | loss: 0.3822388\n",
      "\tspeed: 0.1196s/iter; left time: 12301.3700s\n",
      "2199it [04:22,  8.51it/s]\titers: 2200, epoch: 4 | loss: 0.3322070\n",
      "\tspeed: 0.1171s/iter; left time: 12030.6409s\n",
      "2299it [04:33,  8.41it/s]\titers: 2300, epoch: 4 | loss: 0.3111158\n",
      "\tspeed: 0.1174s/iter; left time: 12054.7749s\n",
      "2399it [04:45,  8.41it/s]\titers: 2400, epoch: 4 | loss: 0.4475155\n",
      "\tspeed: 0.1181s/iter; left time: 12105.7051s\n",
      "2499it [04:57,  8.68it/s]\titers: 2500, epoch: 4 | loss: 0.3928688\n",
      "\tspeed: 0.1181s/iter; left time: 12095.1566s\n",
      "2599it [05:09,  8.30it/s]\titers: 2600, epoch: 4 | loss: 0.4245511\n",
      "\tspeed: 0.1200s/iter; left time: 12278.9050s\n",
      "2699it [05:21,  8.42it/s]\titers: 2700, epoch: 4 | loss: 0.3889744\n",
      "\tspeed: 0.1168s/iter; left time: 11939.3330s\n",
      "2799it [05:32,  8.69it/s]\titers: 2800, epoch: 4 | loss: 0.5335788\n",
      "\tspeed: 0.1179s/iter; left time: 12044.5116s\n",
      "2899it [05:44,  8.86it/s]\titers: 2900, epoch: 4 | loss: 0.4416246\n",
      "\tspeed: 0.1180s/iter; left time: 12045.2130s\n",
      "2999it [05:56,  7.83it/s]\titers: 3000, epoch: 4 | loss: 0.3245631\n",
      "\tspeed: 0.1151s/iter; left time: 11731.0142s\n",
      "3099it [06:08,  8.73it/s]\titers: 3100, epoch: 4 | loss: 0.3183157\n",
      "\tspeed: 0.1192s/iter; left time: 12135.3030s\n",
      "3199it [06:20,  8.93it/s]\titers: 3200, epoch: 4 | loss: 0.2941281\n",
      "\tspeed: 0.1195s/iter; left time: 12159.3077s\n",
      "3299it [06:32,  7.90it/s]\titers: 3300, epoch: 4 | loss: 0.3405291\n",
      "\tspeed: 0.1247s/iter; left time: 12674.2842s\n",
      "3399it [06:44,  8.85it/s]\titers: 3400, epoch: 4 | loss: 0.4112135\n",
      "\tspeed: 0.1237s/iter; left time: 12565.2497s\n",
      "3499it [06:56,  8.28it/s]\titers: 3500, epoch: 4 | loss: 0.4502157\n",
      "\tspeed: 0.1115s/iter; left time: 11313.7212s\n",
      "3599it [07:07,  8.69it/s]\titers: 3600, epoch: 4 | loss: 0.4444681\n",
      "\tspeed: 0.1178s/iter; left time: 11934.5169s\n",
      "3699it [07:19,  8.66it/s]\titers: 3700, epoch: 4 | loss: 0.3614586\n",
      "\tspeed: 0.1149s/iter; left time: 11633.9356s\n",
      "3799it [07:31,  8.16it/s]\titers: 3800, epoch: 4 | loss: 0.3513042\n",
      "\tspeed: 0.1183s/iter; left time: 11960.3336s\n",
      "3899it [07:43,  8.68it/s]\titers: 3900, epoch: 4 | loss: 0.4327674\n",
      "\tspeed: 0.1193s/iter; left time: 12049.3488s\n",
      "3999it [07:55,  8.48it/s]\titers: 4000, epoch: 4 | loss: 0.3898522\n",
      "\tspeed: 0.1187s/iter; left time: 11977.7577s\n",
      "4099it [08:06,  8.87it/s]\titers: 4100, epoch: 4 | loss: 0.4299549\n",
      "\tspeed: 0.1178s/iter; left time: 11877.2240s\n",
      "4199it [08:18,  8.28it/s]\titers: 4200, epoch: 4 | loss: 0.3677428\n",
      "\tspeed: 0.1197s/iter; left time: 12056.0913s\n",
      "4299it [08:30,  7.67it/s]\titers: 4300, epoch: 4 | loss: 0.4615769\n",
      "\tspeed: 0.1173s/iter; left time: 11810.0379s\n",
      "4399it [08:42,  8.20it/s]\titers: 4400, epoch: 4 | loss: 0.3845814\n",
      "\tspeed: 0.1183s/iter; left time: 11893.2105s\n",
      "4499it [08:54,  8.23it/s]\titers: 4500, epoch: 4 | loss: 0.3751306\n",
      "\tspeed: 0.1190s/iter; left time: 11952.3874s\n",
      "4599it [09:05,  8.43it/s]\titers: 4600, epoch: 4 | loss: 0.3855714\n",
      "\tspeed: 0.1144s/iter; left time: 11481.8600s\n",
      "4698it [09:17,  8.69it/s]\titers: 4700, epoch: 4 | loss: 0.3680060\n",
      "\tspeed: 0.1184s/iter; left time: 11872.8060s\n",
      "4799it [09:28,  8.72it/s]\titers: 4800, epoch: 4 | loss: 0.4606707\n",
      "\tspeed: 0.1136s/iter; left time: 11375.3802s\n",
      "4899it [09:40,  8.71it/s]\titers: 4900, epoch: 4 | loss: 0.4020119\n",
      "\tspeed: 0.1155s/iter; left time: 11552.8882s\n",
      "4999it [09:52,  7.93it/s]\titers: 5000, epoch: 4 | loss: 0.3772038\n",
      "\tspeed: 0.1194s/iter; left time: 11931.6845s\n",
      "5099it [10:04,  8.39it/s]\titers: 5100, epoch: 4 | loss: 0.3175957\n",
      "\tspeed: 0.1204s/iter; left time: 12019.0284s\n",
      "5199it [10:15,  7.89it/s]\titers: 5200, epoch: 4 | loss: 0.2871996\n",
      "\tspeed: 0.1153s/iter; left time: 11503.4187s\n",
      "5299it [10:27,  8.45it/s]\titers: 5300, epoch: 4 | loss: 0.3240382\n",
      "\tspeed: 0.1169s/iter; left time: 11647.6878s\n",
      "5399it [10:39,  8.09it/s]\titers: 5400, epoch: 4 | loss: 0.3242806\n",
      "\tspeed: 0.1179s/iter; left time: 11732.7907s\n",
      "5499it [10:51,  8.17it/s]\titers: 5500, epoch: 4 | loss: 0.4660144\n",
      "\tspeed: 0.1201s/iter; left time: 11938.1177s\n",
      "5599it [11:03,  8.23it/s]\titers: 5600, epoch: 4 | loss: 0.3704344\n",
      "\tspeed: 0.1173s/iter; left time: 11652.0625s\n",
      "5699it [11:15,  7.54it/s]\titers: 5700, epoch: 4 | loss: 0.3946370\n",
      "\tspeed: 0.1268s/iter; left time: 12583.2049s\n",
      "5799it [11:27,  8.14it/s]\titers: 5800, epoch: 4 | loss: 0.3982832\n",
      "\tspeed: 0.1200s/iter; left time: 11899.1451s\n",
      "5899it [11:39,  8.56it/s]\titers: 5900, epoch: 4 | loss: 0.4147956\n",
      "\tspeed: 0.1164s/iter; left time: 11530.8691s\n",
      "5999it [11:51,  9.69it/s]\titers: 6000, epoch: 4 | loss: 0.3357751\n",
      "\tspeed: 0.1167s/iter; left time: 11547.2146s\n",
      "6099it [12:03,  8.94it/s]\titers: 6100, epoch: 4 | loss: 0.3940603\n",
      "\tspeed: 0.1192s/iter; left time: 11779.6386s\n",
      "6173it [12:11,  8.44it/s]\n",
      "Epoch: 4 cost time: 731.8051726818085\n",
      "1335it [01:31, 14.53it/s]\n",
      "1330it [01:33, 14.20it/s]\n",
      "Epoch: 4 | Train Loss: 0.3952533 Vali Loss: 0.7261929 Test Loss: 1.1646524 MAE Loss: 0.7594416\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 62.25045516888301 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 \n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 20:56:18,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 20:56:19,699] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 20:56:19,700] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 20:56:19,700] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 20:56:20,484] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 20:56:20,484] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 20:56:21,660] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 20:56:21,661] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 20:56:21,661] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 20:56:21,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 20:56:21,662] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 20:56:21,662] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 20:56:21,662] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 20:56:21,662] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 20:56:21,662] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 20:56:21,662] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 20:56:21,954] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 20:56:21,954] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 20:56:21,955] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 186.9 GB, percent = 24.8%\n",
      "[2024-05-15 20:56:22,067] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 20:56:22,067] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 20:56:22,067] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 186.9 GB, percent = 24.8%\n",
      "[2024-05-15 20:56:22,067] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 20:56:22,180] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 20:56:22,181] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 20:56:22,181] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 186.89 GB, percent = 24.8%\n",
      "[2024-05-15 20:56:22,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 20:56:22,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 20:56:22,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 20:56:22,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0020000000000000018], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 20:56:22,182] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fafad720410>\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 20:56:22,183] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 20:56:22,184] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:13,  7.66it/s]\titers: 100, epoch: 1 | loss: 0.7077039\n",
      "\tspeed: 0.1758s/iter; left time: 21692.7881s\n",
      "199it [00:25,  8.08it/s]\titers: 200, epoch: 1 | loss: 0.8846265\n",
      "\tspeed: 0.1237s/iter; left time: 15252.9149s\n",
      "299it [00:38,  7.39it/s]\titers: 300, epoch: 1 | loss: 0.5444902\n",
      "\tspeed: 0.1249s/iter; left time: 15388.5801s\n",
      "399it [00:51,  8.04it/s]\titers: 400, epoch: 1 | loss: 0.6392947\n",
      "\tspeed: 0.1307s/iter; left time: 16079.0082s\n",
      "499it [01:03,  8.97it/s]\titers: 500, epoch: 1 | loss: 0.5123323\n",
      "\tspeed: 0.1242s/iter; left time: 15267.5518s\n",
      "599it [01:16,  7.82it/s]\titers: 600, epoch: 1 | loss: 0.5926984\n",
      "\tspeed: 0.1263s/iter; left time: 15511.6919s\n",
      "699it [01:29,  9.13it/s]\titers: 700, epoch: 1 | loss: 0.5658636\n",
      "\tspeed: 0.1250s/iter; left time: 15349.9511s\n",
      "799it [01:40,  7.99it/s]\titers: 800, epoch: 1 | loss: 0.6663346\n",
      "\tspeed: 0.1161s/iter; left time: 14239.6587s\n",
      "899it [01:52,  7.80it/s]\titers: 900, epoch: 1 | loss: 0.5692824\n",
      "\tspeed: 0.1234s/iter; left time: 15119.3528s\n",
      "999it [02:05,  7.83it/s]\titers: 1000, epoch: 1 | loss: 0.5440519\n",
      "\tspeed: 0.1253s/iter; left time: 15341.9385s\n",
      "1099it [02:17,  7.67it/s]\titers: 1100, epoch: 1 | loss: 0.6446058\n",
      "\tspeed: 0.1231s/iter; left time: 15067.7402s\n",
      "1199it [02:30,  7.63it/s]\titers: 1200, epoch: 1 | loss: 0.6310812\n",
      "\tspeed: 0.1259s/iter; left time: 15396.2253s\n",
      "1299it [02:42,  7.59it/s]\titers: 1300, epoch: 1 | loss: 0.5470954\n",
      "\tspeed: 0.1239s/iter; left time: 15138.7313s\n",
      "1399it [02:55,  7.82it/s]\titers: 1400, epoch: 1 | loss: 0.7637892\n",
      "\tspeed: 0.1246s/iter; left time: 15211.1753s\n",
      "1499it [03:07,  8.25it/s]\titers: 1500, epoch: 1 | loss: 0.6128849\n",
      "\tspeed: 0.1257s/iter; left time: 15332.3095s\n",
      "1599it [03:19,  7.72it/s]\titers: 1600, epoch: 1 | loss: 0.5400527\n",
      "\tspeed: 0.1213s/iter; left time: 14776.2084s\n",
      "1699it [03:32,  7.75it/s]\titers: 1700, epoch: 1 | loss: 0.6181818\n",
      "\tspeed: 0.1258s/iter; left time: 15312.0267s\n",
      "1799it [03:44,  8.43it/s]\titers: 1800, epoch: 1 | loss: 0.8305395\n",
      "\tspeed: 0.1230s/iter; left time: 14958.5333s\n",
      "1899it [03:57,  7.81it/s]\titers: 1900, epoch: 1 | loss: 0.4917935\n",
      "\tspeed: 0.1274s/iter; left time: 15487.6667s\n",
      "1999it [04:10,  7.69it/s]\titers: 2000, epoch: 1 | loss: 0.4403341\n",
      "\tspeed: 0.1290s/iter; left time: 15667.2862s\n",
      "2099it [04:22,  7.72it/s]\titers: 2100, epoch: 1 | loss: 0.5635881\n",
      "\tspeed: 0.1238s/iter; left time: 15023.1320s\n",
      "2199it [04:35,  7.86it/s]\titers: 2200, epoch: 1 | loss: 0.6812810\n",
      "\tspeed: 0.1271s/iter; left time: 15406.5913s\n",
      "2299it [04:47,  8.34it/s]\titers: 2300, epoch: 1 | loss: 0.5731938\n",
      "\tspeed: 0.1242s/iter; left time: 15050.6968s\n",
      "2399it [05:00,  8.65it/s]\titers: 2400, epoch: 1 | loss: 0.6763653\n",
      "\tspeed: 0.1269s/iter; left time: 15360.2046s\n",
      "2499it [05:13,  8.03it/s]\titers: 2500, epoch: 1 | loss: 0.7445350\n",
      "\tspeed: 0.1272s/iter; left time: 15384.2747s\n",
      "2599it [05:25,  8.71it/s]\titers: 2600, epoch: 1 | loss: 0.5456635\n",
      "\tspeed: 0.1240s/iter; left time: 14987.5110s\n",
      "2699it [05:38,  7.58it/s]\titers: 2700, epoch: 1 | loss: 0.5646907\n",
      "\tspeed: 0.1276s/iter; left time: 15404.7886s\n",
      "2799it [05:51,  7.68it/s]\titers: 2800, epoch: 1 | loss: 0.7073709\n",
      "\tspeed: 0.1267s/iter; left time: 15283.6246s\n",
      "2899it [06:03,  8.05it/s]\titers: 2900, epoch: 1 | loss: 0.5714137\n",
      "\tspeed: 0.1252s/iter; left time: 15095.9417s\n",
      "2999it [06:16,  7.34it/s]\titers: 3000, epoch: 1 | loss: 0.7329552\n",
      "\tspeed: 0.1301s/iter; left time: 15676.9760s\n",
      "3099it [06:30,  7.08it/s]\titers: 3100, epoch: 1 | loss: 0.6885814\n",
      "\tspeed: 0.1329s/iter; left time: 15994.4554s\n",
      "3199it [06:42,  8.21it/s]\titers: 3200, epoch: 1 | loss: 0.7651354\n",
      "\tspeed: 0.1233s/iter; left time: 14828.9112s\n",
      "3299it [06:54,  8.01it/s]\titers: 3300, epoch: 1 | loss: 0.9033726\n",
      "\tspeed: 0.1252s/iter; left time: 15038.2112s\n",
      "3399it [07:07,  8.25it/s]\titers: 3400, epoch: 1 | loss: 0.7089707\n",
      "\tspeed: 0.1216s/iter; left time: 14599.1269s\n",
      "3499it [07:19,  7.99it/s]\titers: 3500, epoch: 1 | loss: 0.6183980\n",
      "\tspeed: 0.1235s/iter; left time: 14818.3146s\n",
      "3599it [07:31,  7.93it/s]\titers: 3600, epoch: 1 | loss: 0.7102731\n",
      "\tspeed: 0.1236s/iter; left time: 14819.4164s\n",
      "3699it [07:43,  8.43it/s]\titers: 3700, epoch: 1 | loss: 0.6235226\n",
      "\tspeed: 0.1221s/iter; left time: 14620.5499s\n",
      "3799it [07:56,  8.08it/s]\titers: 3800, epoch: 1 | loss: 0.5560857\n",
      "\tspeed: 0.1221s/iter; left time: 14613.3110s\n",
      "3899it [08:08,  7.80it/s]\titers: 3900, epoch: 1 | loss: 0.5734344\n",
      "\tspeed: 0.1231s/iter; left time: 14722.5350s\n",
      "3999it [08:20,  8.13it/s]\titers: 4000, epoch: 1 | loss: 0.5969166\n",
      "\tspeed: 0.1228s/iter; left time: 14665.2047s\n",
      "4099it [08:33,  7.97it/s]\titers: 4100, epoch: 1 | loss: 0.6390503\n",
      "\tspeed: 0.1227s/iter; left time: 14641.4222s\n",
      "4199it [08:45,  8.18it/s]\titers: 4200, epoch: 1 | loss: 0.6579083\n",
      "\tspeed: 0.1222s/iter; left time: 14571.1272s\n",
      "4299it [08:57,  8.11it/s]\titers: 4300, epoch: 1 | loss: 0.5656541\n",
      "\tspeed: 0.1211s/iter; left time: 14427.6051s\n",
      "4399it [09:09,  7.78it/s]\titers: 4400, epoch: 1 | loss: 0.8089625\n",
      "\tspeed: 0.1231s/iter; left time: 14661.6032s\n",
      "4499it [09:21,  8.72it/s]\titers: 4500, epoch: 1 | loss: 0.5229645\n",
      "\tspeed: 0.1219s/iter; left time: 14503.6067s\n",
      "4599it [09:34,  7.86it/s]\titers: 4600, epoch: 1 | loss: 0.6160399\n",
      "\tspeed: 0.1235s/iter; left time: 14684.2198s\n",
      "4699it [09:46,  8.61it/s]\titers: 4700, epoch: 1 | loss: 0.7599495\n",
      "\tspeed: 0.1200s/iter; left time: 14253.0254s\n",
      "4799it [09:58,  7.81it/s]\titers: 4800, epoch: 1 | loss: 0.6722426\n",
      "\tspeed: 0.1218s/iter; left time: 14456.0983s\n",
      "4899it [10:10,  7.36it/s]\titers: 4900, epoch: 1 | loss: 0.6884099\n",
      "\tspeed: 0.1227s/iter; left time: 14545.0190s\n",
      "4999it [10:22,  8.19it/s]\titers: 5000, epoch: 1 | loss: 0.6330716\n",
      "\tspeed: 0.1217s/iter; left time: 14417.6129s\n",
      "5099it [10:35,  7.96it/s]\titers: 5100, epoch: 1 | loss: 0.9232766\n",
      "\tspeed: 0.1246s/iter; left time: 14747.8786s\n",
      "5199it [10:47,  7.57it/s]\titers: 5200, epoch: 1 | loss: 0.7550642\n",
      "\tspeed: 0.1247s/iter; left time: 14746.6527s\n",
      "5299it [11:00,  8.25it/s]\titers: 5300, epoch: 1 | loss: 0.5695453\n",
      "\tspeed: 0.1253s/iter; left time: 14807.8498s\n",
      "5399it [11:12,  7.99it/s]\titers: 5400, epoch: 1 | loss: 0.6261397\n",
      "\tspeed: 0.1248s/iter; left time: 14738.1832s\n",
      "5499it [11:25,  7.77it/s]\titers: 5500, epoch: 1 | loss: 0.5858030\n",
      "\tspeed: 0.1248s/iter; left time: 14719.8091s\n",
      "5599it [11:37,  8.45it/s]\titers: 5600, epoch: 1 | loss: 0.6465148\n",
      "\tspeed: 0.1251s/iter; left time: 14743.3851s\n",
      "5699it [11:50,  7.96it/s]\titers: 5700, epoch: 1 | loss: 0.5641134\n",
      "\tspeed: 0.1247s/iter; left time: 14688.9330s\n",
      "5799it [12:02,  7.87it/s]\titers: 5800, epoch: 1 | loss: 0.6520686\n",
      "\tspeed: 0.1266s/iter; left time: 14894.9814s\n",
      "5899it [12:15,  8.29it/s]\titers: 5900, epoch: 1 | loss: 0.7273926\n",
      "\tspeed: 0.1231s/iter; left time: 14471.2405s\n",
      "5999it [12:27,  7.57it/s]\titers: 6000, epoch: 1 | loss: 0.8320255\n",
      "\tspeed: 0.1236s/iter; left time: 14519.1518s\n",
      "6099it [12:39,  8.48it/s]\titers: 6100, epoch: 1 | loss: 0.8532098\n",
      "\tspeed: 0.1227s/iter; left time: 14399.5044s\n",
      "6173it [12:49,  8.03it/s]\n",
      "Epoch: 1 cost time: 769.0455589294434\n",
      "1335it [01:33, 14.26it/s]\n",
      "1330it [01:33, 14.15it/s]\n",
      "Epoch: 1 | Train Loss: 0.6717076 Vali Loss: 0.7515029 Test Loss: 1.0568161 MAE Loss: 0.7737968\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:12,  8.09it/s]\titers: 100, epoch: 2 | loss: 0.6894274\n",
      "\tspeed: 2.1135s/iter; left time: 247678.4964s\n",
      "199it [00:24,  8.55it/s]\titers: 200, epoch: 2 | loss: 0.7664553\n",
      "\tspeed: 0.1207s/iter; left time: 14131.9545s\n",
      "299it [00:36,  8.20it/s]\titers: 300, epoch: 2 | loss: 0.7426021\n",
      "\tspeed: 0.1219s/iter; left time: 14261.9133s\n",
      "399it [00:49,  7.96it/s]\titers: 400, epoch: 2 | loss: 0.6378171\n",
      "\tspeed: 0.1223s/iter; left time: 14293.4324s\n",
      "499it [01:01,  8.09it/s]\titers: 500, epoch: 2 | loss: 0.6640931\n",
      "\tspeed: 0.1212s/iter; left time: 14152.7461s\n",
      "599it [01:12,  8.13it/s]\titers: 600, epoch: 2 | loss: 0.6366306\n",
      "\tspeed: 0.1156s/iter; left time: 13485.6312s\n",
      "699it [01:24,  8.21it/s]\titers: 700, epoch: 2 | loss: 0.7172560\n",
      "\tspeed: 0.1183s/iter; left time: 13792.1076s\n",
      "799it [01:36,  8.45it/s]\titers: 800, epoch: 2 | loss: 0.6551113\n",
      "\tspeed: 0.1206s/iter; left time: 14053.5710s\n",
      "899it [01:48,  8.17it/s]\titers: 900, epoch: 2 | loss: 0.5912023\n",
      "\tspeed: 0.1198s/iter; left time: 13945.6816s\n",
      "999it [02:00,  8.11it/s]\titers: 1000, epoch: 2 | loss: 0.6779618\n",
      "\tspeed: 0.1194s/iter; left time: 13880.1031s\n",
      "1099it [02:12,  8.11it/s]\titers: 1100, epoch: 2 | loss: 0.7628306\n",
      "\tspeed: 0.1198s/iter; left time: 13913.8395s\n",
      "1199it [02:24,  8.10it/s]\titers: 1200, epoch: 2 | loss: 0.6640381\n",
      "\tspeed: 0.1199s/iter; left time: 13919.4320s\n",
      "1299it [02:36,  8.02it/s]\titers: 1300, epoch: 2 | loss: 0.5882042\n",
      "\tspeed: 0.1202s/iter; left time: 13938.4129s\n",
      "1399it [02:48,  7.88it/s]\titers: 1400, epoch: 2 | loss: 0.7141836\n",
      "\tspeed: 0.1227s/iter; left time: 14222.1429s\n",
      "1499it [03:01,  8.37it/s]\titers: 1500, epoch: 2 | loss: 0.6594862\n",
      "\tspeed: 0.1226s/iter; left time: 14196.8417s\n",
      "1599it [03:13,  8.13it/s]\titers: 1600, epoch: 2 | loss: 0.5258417\n",
      "\tspeed: 0.1223s/iter; left time: 14147.3560s\n",
      "1699it [03:25,  7.99it/s]\titers: 1700, epoch: 2 | loss: 0.5860528\n",
      "\tspeed: 0.1220s/iter; left time: 14097.3934s\n",
      "1799it [03:37,  8.48it/s]\titers: 1800, epoch: 2 | loss: 0.5855860\n",
      "\tspeed: 0.1218s/iter; left time: 14070.5224s\n",
      "1899it [03:49,  7.62it/s]\titers: 1900, epoch: 2 | loss: 0.5262867\n",
      "\tspeed: 0.1225s/iter; left time: 14133.3477s\n",
      "1999it [04:02,  8.47it/s]\titers: 2000, epoch: 2 | loss: 0.6557859\n",
      "\tspeed: 0.1216s/iter; left time: 14017.7290s\n",
      "2099it [04:14,  8.04it/s]\titers: 2100, epoch: 2 | loss: 0.5214762\n",
      "\tspeed: 0.1239s/iter; left time: 14272.8215s\n",
      "2199it [04:26,  8.29it/s]\titers: 2200, epoch: 2 | loss: 0.6922092\n",
      "\tspeed: 0.1203s/iter; left time: 13850.7298s\n",
      "2299it [04:38,  7.84it/s]\titers: 2300, epoch: 2 | loss: 0.5699934\n",
      "\tspeed: 0.1200s/iter; left time: 13802.5698s\n",
      "2399it [04:50,  7.92it/s]\titers: 2400, epoch: 2 | loss: 0.9296500\n",
      "\tspeed: 0.1210s/iter; left time: 13895.8798s\n",
      "2499it [05:02,  8.23it/s]\titers: 2500, epoch: 2 | loss: 0.6625931\n",
      "\tspeed: 0.1224s/iter; left time: 14050.5772s\n",
      "2599it [05:14,  8.24it/s]\titers: 2600, epoch: 2 | loss: 0.5902045\n",
      "\tspeed: 0.1202s/iter; left time: 13788.7416s\n",
      "2699it [05:26,  8.35it/s]\titers: 2700, epoch: 2 | loss: 0.4374116\n",
      "\tspeed: 0.1214s/iter; left time: 13907.9341s\n",
      "2799it [05:38,  7.64it/s]\titers: 2800, epoch: 2 | loss: 0.5717490\n",
      "\tspeed: 0.1197s/iter; left time: 13707.6701s\n",
      "2899it [05:50,  7.85it/s]\titers: 2900, epoch: 2 | loss: 0.6689234\n",
      "\tspeed: 0.1191s/iter; left time: 13617.9615s\n",
      "2999it [06:03,  8.29it/s]\titers: 3000, epoch: 2 | loss: 0.7500266\n",
      "\tspeed: 0.1236s/iter; left time: 14125.9645s\n",
      "3099it [06:15,  8.66it/s]\titers: 3100, epoch: 2 | loss: 0.5778413\n",
      "\tspeed: 0.1211s/iter; left time: 13824.8055s\n",
      "3199it [06:27,  7.90it/s]\titers: 3200, epoch: 2 | loss: 0.7884160\n",
      "\tspeed: 0.1209s/iter; left time: 13788.9419s\n",
      "3299it [06:39,  7.98it/s]\titers: 3300, epoch: 2 | loss: 0.7419913\n",
      "\tspeed: 0.1229s/iter; left time: 14014.1021s\n",
      "3399it [06:51,  9.05it/s]\titers: 3400, epoch: 2 | loss: 0.4898773\n",
      "\tspeed: 0.1179s/iter; left time: 13431.2818s\n",
      "3499it [07:03,  8.10it/s]\titers: 3500, epoch: 2 | loss: 0.7498540\n",
      "\tspeed: 0.1201s/iter; left time: 13665.5462s\n",
      "3599it [07:15,  7.97it/s]\titers: 3600, epoch: 2 | loss: 0.7418373\n",
      "\tspeed: 0.1207s/iter; left time: 13724.6569s\n",
      "3699it [07:28,  7.95it/s]\titers: 3700, epoch: 2 | loss: 0.7198861\n",
      "\tspeed: 0.1270s/iter; left time: 14427.6192s\n",
      "3799it [07:40,  7.70it/s]\titers: 3800, epoch: 2 | loss: 0.7020556\n",
      "\tspeed: 0.1258s/iter; left time: 14276.7064s\n",
      "3899it [07:53,  7.71it/s]\titers: 3900, epoch: 2 | loss: 0.7378017\n",
      "\tspeed: 0.1299s/iter; left time: 14728.6927s\n",
      "3999it [08:06,  8.34it/s]\titers: 4000, epoch: 2 | loss: 0.6058432\n",
      "\tspeed: 0.1253s/iter; left time: 14196.9794s\n",
      "4099it [08:18,  8.33it/s]\titers: 4100, epoch: 2 | loss: 0.5322354\n",
      "\tspeed: 0.1235s/iter; left time: 13982.5278s\n",
      "4199it [08:31,  8.14it/s]\titers: 4200, epoch: 2 | loss: 0.7116097\n",
      "\tspeed: 0.1234s/iter; left time: 13958.9147s\n",
      "4299it [08:43,  8.11it/s]\titers: 4300, epoch: 2 | loss: 0.5419536\n",
      "\tspeed: 0.1228s/iter; left time: 13878.3015s\n",
      "4399it [08:55,  8.08it/s]\titers: 4400, epoch: 2 | loss: 0.6849950\n",
      "\tspeed: 0.1234s/iter; left time: 13934.4265s\n",
      "4499it [09:08,  8.50it/s]\titers: 4500, epoch: 2 | loss: 0.6115397\n",
      "\tspeed: 0.1236s/iter; left time: 13938.2720s\n",
      "4599it [09:20,  8.10it/s]\titers: 4600, epoch: 2 | loss: 0.4679059\n",
      "\tspeed: 0.1282s/iter; left time: 14443.4409s\n",
      "4699it [09:33,  8.11it/s]\titers: 4700, epoch: 2 | loss: 0.8719603\n",
      "\tspeed: 0.1237s/iter; left time: 13924.7400s\n",
      "4799it [09:45,  7.96it/s]\titers: 4800, epoch: 2 | loss: 0.6641534\n",
      "\tspeed: 0.1249s/iter; left time: 14045.9745s\n",
      "4899it [09:58,  8.32it/s]\titers: 4900, epoch: 2 | loss: 0.6844010\n",
      "\tspeed: 0.1240s/iter; left time: 13933.0306s\n",
      "4999it [10:10,  7.79it/s]\titers: 5000, epoch: 2 | loss: 0.8636635\n",
      "\tspeed: 0.1213s/iter; left time: 13622.2533s\n",
      "5099it [10:22,  7.56it/s]\titers: 5100, epoch: 2 | loss: 0.5840023\n",
      "\tspeed: 0.1269s/iter; left time: 14232.5954s\n",
      "5199it [10:34,  8.28it/s]\titers: 5200, epoch: 2 | loss: 0.6464534\n",
      "\tspeed: 0.1201s/iter; left time: 13458.9700s\n",
      "5299it [10:47,  8.91it/s]\titers: 5300, epoch: 2 | loss: 0.7085236\n",
      "\tspeed: 0.1209s/iter; left time: 13535.3235s\n",
      "5399it [10:58,  8.14it/s]\titers: 5400, epoch: 2 | loss: 0.4589159\n",
      "\tspeed: 0.1160s/iter; left time: 12982.4414s\n",
      "5499it [11:10,  8.24it/s]\titers: 5500, epoch: 2 | loss: 0.8428977\n",
      "\tspeed: 0.1234s/iter; left time: 13797.9299s\n",
      "5599it [11:23,  8.05it/s]\titers: 5600, epoch: 2 | loss: 0.5222809\n",
      "\tspeed: 0.1229s/iter; left time: 13724.6026s\n",
      "5698it [11:35,  8.41it/s]\titers: 5700, epoch: 2 | loss: 0.6771845\n",
      "\tspeed: 0.1210s/iter; left time: 13502.2687s\n",
      "5799it [11:47,  8.26it/s]\titers: 5800, epoch: 2 | loss: 0.8527070\n",
      "\tspeed: 0.1219s/iter; left time: 13588.3594s\n",
      "5899it [11:59,  8.49it/s]\titers: 5900, epoch: 2 | loss: 0.5187934\n",
      "\tspeed: 0.1226s/iter; left time: 13653.7978s\n",
      "5999it [12:11,  8.44it/s]\titers: 6000, epoch: 2 | loss: 0.6409558\n",
      "\tspeed: 0.1198s/iter; left time: 13337.4955s\n",
      "6099it [12:23,  9.31it/s]\titers: 6100, epoch: 2 | loss: 0.7059897\n",
      "\tspeed: 0.1202s/iter; left time: 13363.9397s\n",
      "6173it [12:32,  8.20it/s]\n",
      "Epoch: 2 cost time: 752.4582493305206\n",
      "1335it [01:32, 14.39it/s]\n",
      "1330it [01:33, 14.17it/s]\n",
      "Epoch: 2 | Train Loss: 0.6685536 Vali Loss: 0.7502165 Test Loss: 1.0539005 MAE Loss: 0.7689379\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:12,  7.43it/s]\titers: 100, epoch: 3 | loss: 0.4096831\n",
      "\tspeed: 2.0994s/iter; left time: 233064.1404s\n",
      "199it [00:24,  8.21it/s]\titers: 200, epoch: 3 | loss: 0.6104873\n",
      "\tspeed: 0.1229s/iter; left time: 13629.0912s\n",
      "299it [00:36,  8.66it/s]\titers: 300, epoch: 3 | loss: 0.7676280\n",
      "\tspeed: 0.1187s/iter; left time: 13151.3250s\n",
      "399it [00:48,  7.74it/s]\titers: 400, epoch: 3 | loss: 0.5954800\n",
      "\tspeed: 0.1200s/iter; left time: 13288.4560s\n",
      "499it [01:00,  8.09it/s]\titers: 500, epoch: 3 | loss: 0.5605651\n",
      "\tspeed: 0.1211s/iter; left time: 13398.0018s\n",
      "599it [01:12,  9.13it/s]\titers: 600, epoch: 3 | loss: 0.7365743\n",
      "\tspeed: 0.1198s/iter; left time: 13244.7626s\n",
      "699it [01:24,  8.64it/s]\titers: 700, epoch: 3 | loss: 0.6966975\n",
      "\tspeed: 0.1204s/iter; left time: 13298.7023s\n",
      "799it [01:36,  7.89it/s]\titers: 800, epoch: 3 | loss: 0.6337537\n",
      "\tspeed: 0.1187s/iter; left time: 13093.1079s\n",
      "899it [01:48,  8.07it/s]\titers: 900, epoch: 3 | loss: 0.6630055\n",
      "\tspeed: 0.1174s/iter; left time: 12934.7949s\n",
      "999it [02:00,  7.86it/s]\titers: 1000, epoch: 3 | loss: 0.7128114\n",
      "\tspeed: 0.1225s/iter; left time: 13487.9644s\n",
      "1099it [02:12,  7.98it/s]\titers: 1100, epoch: 3 | loss: 0.7297856\n",
      "\tspeed: 0.1201s/iter; left time: 13217.1042s\n",
      "1199it [02:24,  8.51it/s]\titers: 1200, epoch: 3 | loss: 0.4998933\n",
      "\tspeed: 0.1196s/iter; left time: 13146.0049s\n",
      "1299it [02:37,  8.12it/s]\titers: 1300, epoch: 3 | loss: 0.6780714\n",
      "\tspeed: 0.1235s/iter; left time: 13559.8028s\n",
      "1399it [02:49,  8.19it/s]\titers: 1400, epoch: 3 | loss: 0.5947507\n",
      "\tspeed: 0.1207s/iter; left time: 13242.3064s\n",
      "1499it [03:01,  7.99it/s]\titers: 1500, epoch: 3 | loss: 0.5693490\n",
      "\tspeed: 0.1238s/iter; left time: 13565.8512s\n",
      "1599it [03:13,  8.54it/s]\titers: 1600, epoch: 3 | loss: 0.6498333\n",
      "\tspeed: 0.1228s/iter; left time: 13448.3613s\n",
      "1699it [03:25,  8.63it/s]\titers: 1700, epoch: 3 | loss: 0.6184540\n",
      "\tspeed: 0.1188s/iter; left time: 12996.2342s\n",
      "1799it [03:37,  7.51it/s]\titers: 1800, epoch: 3 | loss: 0.7217347\n",
      "\tspeed: 0.1216s/iter; left time: 13293.6366s\n",
      "1899it [03:50,  8.91it/s]\titers: 1900, epoch: 3 | loss: 0.6141191\n",
      "\tspeed: 0.1230s/iter; left time: 13430.6465s\n",
      "1999it [04:02,  8.18it/s]\titers: 2000, epoch: 3 | loss: 0.7544956\n",
      "\tspeed: 0.1217s/iter; left time: 13277.8213s\n",
      "2099it [04:14,  7.64it/s]\titers: 2100, epoch: 3 | loss: 0.6342972\n",
      "\tspeed: 0.1225s/iter; left time: 13355.4738s\n",
      "2199it [04:26,  8.07it/s]\titers: 2200, epoch: 3 | loss: 0.8258610\n",
      "\tspeed: 0.1245s/iter; left time: 13562.6449s\n",
      "2299it [04:39,  7.48it/s]\titers: 2300, epoch: 3 | loss: 0.6924235\n",
      "\tspeed: 0.1244s/iter; left time: 13539.4583s\n",
      "2399it [04:51,  8.02it/s]\titers: 2400, epoch: 3 | loss: 0.5311827\n",
      "\tspeed: 0.1252s/iter; left time: 13606.5981s\n",
      "2499it [05:04,  8.10it/s]\titers: 2500, epoch: 3 | loss: 0.6456027\n",
      "\tspeed: 0.1255s/iter; left time: 13632.4933s\n",
      "2599it [05:16,  8.36it/s]\titers: 2600, epoch: 3 | loss: 0.6682160\n",
      "\tspeed: 0.1235s/iter; left time: 13399.4483s\n",
      "2699it [05:29,  8.62it/s]\titers: 2700, epoch: 3 | loss: 0.8834933\n",
      "\tspeed: 0.1231s/iter; left time: 13343.2975s\n",
      "2799it [05:41,  8.25it/s]\titers: 2800, epoch: 3 | loss: 0.7280555\n",
      "\tspeed: 0.1209s/iter; left time: 13097.1327s\n",
      "2899it [05:53,  8.59it/s]\titers: 2900, epoch: 3 | loss: 0.8014239\n",
      "\tspeed: 0.1216s/iter; left time: 13161.9318s\n",
      "2999it [06:05,  7.72it/s]\titers: 3000, epoch: 3 | loss: 0.8054075\n",
      "\tspeed: 0.1218s/iter; left time: 13171.5660s\n",
      "3099it [06:17,  8.23it/s]\titers: 3100, epoch: 3 | loss: 0.5741761\n",
      "\tspeed: 0.1211s/iter; left time: 13078.3937s\n",
      "3199it [06:30,  8.13it/s]\titers: 3200, epoch: 3 | loss: 0.9364762\n",
      "\tspeed: 0.1238s/iter; left time: 13359.5414s\n",
      "3299it [06:42,  8.10it/s]\titers: 3300, epoch: 3 | loss: 0.7280094\n",
      "\tspeed: 0.1233s/iter; left time: 13293.2954s\n",
      "3399it [06:54,  8.42it/s]\titers: 3400, epoch: 3 | loss: 0.5544878\n",
      "\tspeed: 0.1249s/iter; left time: 13453.3662s\n",
      "3499it [07:07,  8.30it/s]\titers: 3500, epoch: 3 | loss: 0.6451662\n",
      "\tspeed: 0.1238s/iter; left time: 13323.2903s\n",
      "3599it [07:19,  7.96it/s]\titers: 3600, epoch: 3 | loss: 0.8442000\n",
      "\tspeed: 0.1233s/iter; left time: 13261.7710s\n",
      "3699it [07:31,  8.27it/s]\titers: 3700, epoch: 3 | loss: 0.7453490\n",
      "\tspeed: 0.1222s/iter; left time: 13125.6093s\n",
      "3799it [07:44,  7.87it/s]\titers: 3800, epoch: 3 | loss: 0.6329286\n",
      "\tspeed: 0.1245s/iter; left time: 13363.1247s\n",
      "3899it [07:56,  7.67it/s]\titers: 3900, epoch: 3 | loss: 0.6174514\n",
      "\tspeed: 0.1247s/iter; left time: 13365.0700s\n",
      "3999it [08:09,  8.13it/s]\titers: 4000, epoch: 3 | loss: 0.6423288\n",
      "\tspeed: 0.1237s/iter; left time: 13248.9499s\n",
      "4099it [08:21,  8.48it/s]\titers: 4100, epoch: 3 | loss: 0.6439524\n",
      "\tspeed: 0.1228s/iter; left time: 13146.0825s\n",
      "4199it [08:33,  7.85it/s]\titers: 4200, epoch: 3 | loss: 0.6321095\n",
      "\tspeed: 0.1238s/iter; left time: 13238.3983s\n",
      "4299it [08:46,  8.13it/s]\titers: 4300, epoch: 3 | loss: 0.6528170\n",
      "\tspeed: 0.1220s/iter; left time: 13027.4532s\n",
      "4399it [08:58,  8.48it/s]\titers: 4400, epoch: 3 | loss: 0.5782089\n",
      "\tspeed: 0.1255s/iter; left time: 13391.7300s\n",
      "4499it [09:11,  7.72it/s]\titers: 4500, epoch: 3 | loss: 0.5357323\n",
      "\tspeed: 0.1271s/iter; left time: 13551.8420s\n",
      "4599it [09:23,  8.66it/s]\titers: 4600, epoch: 3 | loss: 0.7318455\n",
      "\tspeed: 0.1215s/iter; left time: 12946.8500s\n",
      "4699it [09:35,  7.42it/s]\titers: 4700, epoch: 3 | loss: 0.6826214\n",
      "\tspeed: 0.1229s/iter; left time: 13082.8386s\n",
      "4799it [09:47,  8.13it/s]\titers: 4800, epoch: 3 | loss: 0.4754902\n",
      "\tspeed: 0.1203s/iter; left time: 12785.9466s\n",
      "4899it [09:59,  7.64it/s]\titers: 4900, epoch: 3 | loss: 0.7480960\n",
      "\tspeed: 0.1224s/iter; left time: 12997.3470s\n",
      "4999it [10:12,  8.08it/s]\titers: 5000, epoch: 3 | loss: 0.5958805\n",
      "\tspeed: 0.1231s/iter; left time: 13062.6365s\n",
      "5099it [10:24,  7.96it/s]\titers: 5100, epoch: 3 | loss: 0.4279775\n",
      "\tspeed: 0.1215s/iter; left time: 12885.2697s\n",
      "5199it [10:36,  8.36it/s]\titers: 5200, epoch: 3 | loss: 0.7300346\n",
      "\tspeed: 0.1221s/iter; left time: 12933.6998s\n",
      "5299it [10:48,  8.68it/s]\titers: 5300, epoch: 3 | loss: 0.5236185\n",
      "\tspeed: 0.1225s/iter; left time: 12965.9724s\n",
      "5399it [11:01,  8.10it/s]\titers: 5400, epoch: 3 | loss: 0.7850830\n",
      "\tspeed: 0.1255s/iter; left time: 13271.4595s\n",
      "5499it [11:13,  8.01it/s]\titers: 5500, epoch: 3 | loss: 0.6870089\n",
      "\tspeed: 0.1252s/iter; left time: 13225.8988s\n",
      "5599it [11:25,  8.54it/s]\titers: 5600, epoch: 3 | loss: 0.5632825\n",
      "\tspeed: 0.1168s/iter; left time: 12324.6381s\n",
      "5699it [11:37,  8.63it/s]\titers: 5700, epoch: 3 | loss: 0.5167628\n",
      "\tspeed: 0.1190s/iter; left time: 12541.0055s\n",
      "5799it [11:49,  8.69it/s]\titers: 5800, epoch: 3 | loss: 0.8384146\n",
      "\tspeed: 0.1152s/iter; left time: 12134.5674s\n",
      "5899it [12:00,  8.36it/s]\titers: 5900, epoch: 3 | loss: 0.8056844\n",
      "\tspeed: 0.1159s/iter; left time: 12195.0077s\n",
      "5999it [12:12,  8.73it/s]\titers: 6000, epoch: 3 | loss: 0.4432939\n",
      "\tspeed: 0.1180s/iter; left time: 12402.9133s\n",
      "6099it [12:24,  7.82it/s]\titers: 6100, epoch: 3 | loss: 0.7394956\n",
      "\tspeed: 0.1170s/iter; left time: 12289.5277s\n",
      "6173it [12:32,  8.20it/s]\n",
      "Epoch: 3 cost time: 752.9517638683319\n",
      "1335it [01:29, 14.96it/s]\n",
      "1330it [01:32, 14.42it/s]\n",
      "Epoch: 3 | Train Loss: 0.6579565 Vali Loss: 0.7264658 Test Loss: 1.0131015 MAE Loss: 0.7557789\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:12,  8.96it/s]\titers: 100, epoch: 4 | loss: 0.5111652\n",
      "\tspeed: 2.0438s/iter; left time: 214275.8143s\n",
      "199it [00:23,  8.65it/s]\titers: 200, epoch: 4 | loss: 0.6359840\n",
      "\tspeed: 0.1149s/iter; left time: 12036.4233s\n",
      "299it [00:35,  8.11it/s]\titers: 300, epoch: 4 | loss: 0.6745830\n",
      "\tspeed: 0.1167s/iter; left time: 12207.9330s\n",
      "399it [00:47,  7.99it/s]\titers: 400, epoch: 4 | loss: 0.8568417\n",
      "\tspeed: 0.1194s/iter; left time: 12481.4811s\n",
      "499it [00:59,  8.75it/s]\titers: 500, epoch: 4 | loss: 0.6949803\n",
      "\tspeed: 0.1190s/iter; left time: 12429.6786s\n",
      "599it [01:10,  8.25it/s]\titers: 600, epoch: 4 | loss: 0.7653177\n",
      "\tspeed: 0.1154s/iter; left time: 12042.1317s\n",
      "699it [01:22,  9.01it/s]\titers: 700, epoch: 4 | loss: 0.6095369\n",
      "\tspeed: 0.1187s/iter; left time: 12375.5861s\n",
      "799it [01:33,  8.60it/s]\titers: 800, epoch: 4 | loss: 0.3877661\n",
      "\tspeed: 0.1141s/iter; left time: 11878.7160s\n",
      "899it [01:45,  9.10it/s]\titers: 900, epoch: 4 | loss: 0.8587191\n",
      "\tspeed: 0.1158s/iter; left time: 12049.3035s\n",
      "999it [01:57,  8.23it/s]\titers: 1000, epoch: 4 | loss: 0.7792495\n",
      "\tspeed: 0.1175s/iter; left time: 12209.0071s\n",
      "1099it [02:09,  8.58it/s]\titers: 1100, epoch: 4 | loss: 0.6659447\n",
      "\tspeed: 0.1198s/iter; left time: 12440.8973s\n",
      "1199it [02:21,  8.75it/s]\titers: 1200, epoch: 4 | loss: 0.6068806\n",
      "\tspeed: 0.1184s/iter; left time: 12278.5561s\n",
      "1299it [02:33,  8.88it/s]\titers: 1300, epoch: 4 | loss: 0.7011850\n",
      "\tspeed: 0.1191s/iter; left time: 12341.7667s\n",
      "1399it [02:44,  8.54it/s]\titers: 1400, epoch: 4 | loss: 0.6369728\n",
      "\tspeed: 0.1183s/iter; left time: 12249.0743s\n",
      "1499it [02:57,  7.91it/s]\titers: 1500, epoch: 4 | loss: 0.5298790\n",
      "\tspeed: 0.1237s/iter; left time: 12796.4281s\n",
      "1599it [03:09,  8.08it/s]\titers: 1600, epoch: 4 | loss: 0.7321669\n",
      "\tspeed: 0.1273s/iter; left time: 13158.2149s\n",
      "1699it [03:22,  8.22it/s]\titers: 1700, epoch: 4 | loss: 0.6023193\n",
      "\tspeed: 0.1257s/iter; left time: 12980.8701s\n",
      "1799it [03:34,  8.08it/s]\titers: 1800, epoch: 4 | loss: 0.5566152\n",
      "\tspeed: 0.1211s/iter; left time: 12485.3802s\n",
      "1899it [03:46,  8.10it/s]\titers: 1900, epoch: 4 | loss: 0.6265987\n",
      "\tspeed: 0.1212s/iter; left time: 12490.9141s\n",
      "1999it [03:59,  7.63it/s]\titers: 2000, epoch: 4 | loss: 0.6597928\n",
      "\tspeed: 0.1243s/iter; left time: 12793.8303s\n",
      "2099it [04:10,  8.00it/s]\titers: 2100, epoch: 4 | loss: 0.5786980\n",
      "\tspeed: 0.1169s/iter; left time: 12023.8170s\n",
      "2199it [04:22,  8.79it/s]\titers: 2200, epoch: 4 | loss: 0.5371538\n",
      "\tspeed: 0.1204s/iter; left time: 12365.0030s\n",
      "2299it [04:35,  8.08it/s]\titers: 2300, epoch: 4 | loss: 0.5878653\n",
      "\tspeed: 0.1216s/iter; left time: 12480.2011s\n",
      "2399it [04:46,  8.66it/s]\titers: 2400, epoch: 4 | loss: 0.8017356\n",
      "\tspeed: 0.1185s/iter; left time: 12155.5539s\n",
      "2499it [04:58,  8.09it/s]\titers: 2500, epoch: 4 | loss: 0.7124053\n",
      "\tspeed: 0.1160s/iter; left time: 11883.0267s\n",
      "2599it [05:10,  8.12it/s]\titers: 2600, epoch: 4 | loss: 0.6675104\n",
      "\tspeed: 0.1206s/iter; left time: 12341.1905s\n",
      "2699it [05:22,  8.48it/s]\titers: 2700, epoch: 4 | loss: 0.5649776\n",
      "\tspeed: 0.1194s/iter; left time: 12210.1449s\n",
      "2799it [05:34,  8.65it/s]\titers: 2800, epoch: 4 | loss: 0.7169674\n",
      "\tspeed: 0.1196s/iter; left time: 12220.2552s\n",
      "2899it [05:46,  8.07it/s]\titers: 2900, epoch: 4 | loss: 0.6749012\n",
      "\tspeed: 0.1202s/iter; left time: 12266.4831s\n",
      "2999it [05:58,  8.48it/s]\titers: 3000, epoch: 4 | loss: 0.4858488\n",
      "\tspeed: 0.1196s/iter; left time: 12189.6667s\n",
      "3099it [06:10,  8.31it/s]\titers: 3100, epoch: 4 | loss: 0.5419345\n",
      "\tspeed: 0.1211s/iter; left time: 12335.6979s\n",
      "3199it [06:22,  7.41it/s]\titers: 3200, epoch: 4 | loss: 0.5918074\n",
      "\tspeed: 0.1235s/iter; left time: 12562.6319s\n",
      "3299it [06:35,  8.27it/s]\titers: 3300, epoch: 4 | loss: 0.5573754\n",
      "\tspeed: 0.1233s/iter; left time: 12534.3158s\n",
      "3399it [06:47,  8.06it/s]\titers: 3400, epoch: 4 | loss: 0.6292348\n",
      "\tspeed: 0.1218s/iter; left time: 12366.0685s\n",
      "3499it [06:59,  7.87it/s]\titers: 3500, epoch: 4 | loss: 0.6479694\n",
      "\tspeed: 0.1199s/iter; left time: 12163.2250s\n",
      "3599it [07:11,  8.22it/s]\titers: 3600, epoch: 4 | loss: 0.7075433\n",
      "\tspeed: 0.1197s/iter; left time: 12128.3843s\n",
      "3699it [07:23,  8.34it/s]\titers: 3700, epoch: 4 | loss: 0.5903510\n",
      "\tspeed: 0.1175s/iter; left time: 11897.7825s\n",
      "3799it [07:35,  8.86it/s]\titers: 3800, epoch: 4 | loss: 0.5305083\n",
      "\tspeed: 0.1193s/iter; left time: 12068.2843s\n",
      "3899it [07:46,  8.64it/s]\titers: 3900, epoch: 4 | loss: 0.6579697\n",
      "\tspeed: 0.1173s/iter; left time: 11850.2570s\n",
      "3999it [07:58,  8.00it/s]\titers: 4000, epoch: 4 | loss: 0.6533204\n",
      "\tspeed: 0.1187s/iter; left time: 11978.2366s\n",
      "4099it [08:10,  7.68it/s]\titers: 4100, epoch: 4 | loss: 0.6161623\n",
      "\tspeed: 0.1230s/iter; left time: 12398.7767s\n",
      "4199it [08:23,  8.07it/s]\titers: 4200, epoch: 4 | loss: 0.6292722\n",
      "\tspeed: 0.1237s/iter; left time: 12465.3999s\n",
      "4299it [08:35,  8.64it/s]\titers: 4300, epoch: 4 | loss: 0.8361017\n",
      "\tspeed: 0.1236s/iter; left time: 12437.6213s\n",
      "4399it [08:47,  7.87it/s]\titers: 4400, epoch: 4 | loss: 0.6663599\n",
      "\tspeed: 0.1205s/iter; left time: 12113.0124s\n",
      "4499it [08:59,  9.13it/s]\titers: 4500, epoch: 4 | loss: 0.7166002\n",
      "\tspeed: 0.1212s/iter; left time: 12175.2183s\n",
      "4599it [09:12,  7.83it/s]\titers: 4600, epoch: 4 | loss: 0.5726159\n",
      "\tspeed: 0.1230s/iter; left time: 12345.0899s\n",
      "4699it [09:24,  8.33it/s]\titers: 4700, epoch: 4 | loss: 0.6148465\n",
      "\tspeed: 0.1226s/iter; left time: 12292.7484s\n",
      "4799it [09:36,  8.08it/s]\titers: 4800, epoch: 4 | loss: 0.6351760\n",
      "\tspeed: 0.1208s/iter; left time: 12101.2924s\n",
      "4899it [09:48,  8.31it/s]\titers: 4900, epoch: 4 | loss: 0.6769361\n",
      "\tspeed: 0.1209s/iter; left time: 12091.8837s\n",
      "4999it [10:00,  9.18it/s]\titers: 5000, epoch: 4 | loss: 0.7656662\n",
      "\tspeed: 0.1209s/iter; left time: 12078.3798s\n",
      "5099it [10:12,  8.79it/s]\titers: 5100, epoch: 4 | loss: 0.5396982\n",
      "\tspeed: 0.1168s/iter; left time: 11661.9715s\n",
      "5199it [10:24,  9.15it/s]\titers: 5200, epoch: 4 | loss: 0.5893866\n",
      "\tspeed: 0.1224s/iter; left time: 12207.0356s\n",
      "5299it [10:36,  7.95it/s]\titers: 5300, epoch: 4 | loss: 0.5204805\n",
      "\tspeed: 0.1182s/iter; left time: 11776.8058s\n",
      "5399it [10:48,  8.13it/s]\titers: 5400, epoch: 4 | loss: 0.6493788\n",
      "\tspeed: 0.1218s/iter; left time: 12123.4880s\n",
      "5499it [11:00,  8.31it/s]\titers: 5500, epoch: 4 | loss: 0.9598983\n",
      "\tspeed: 0.1218s/iter; left time: 12114.9134s\n",
      "5599it [11:13,  8.28it/s]\titers: 5600, epoch: 4 | loss: 0.6992377\n",
      "\tspeed: 0.1230s/iter; left time: 12221.7961s\n",
      "5699it [11:25,  8.00it/s]\titers: 5700, epoch: 4 | loss: 0.6760113\n",
      "\tspeed: 0.1248s/iter; left time: 12382.2847s\n",
      "5799it [11:37,  8.06it/s]\titers: 5800, epoch: 4 | loss: 0.6869395\n",
      "\tspeed: 0.1205s/iter; left time: 11946.1025s\n",
      "5899it [11:49,  8.46it/s]\titers: 5900, epoch: 4 | loss: 0.6087454\n",
      "\tspeed: 0.1217s/iter; left time: 12050.3304s\n",
      "5999it [12:01,  7.65it/s]\titers: 6000, epoch: 4 | loss: 0.5390847\n",
      "\tspeed: 0.1197s/iter; left time: 11844.0010s\n",
      "6099it [12:13,  9.23it/s]\titers: 6100, epoch: 4 | loss: 0.6304100\n",
      "\tspeed: 0.1162s/iter; left time: 11483.8060s\n",
      "6173it [12:22,  8.32it/s]\n",
      "Epoch: 4 cost time: 742.1463685035706\n",
      "1335it [01:30, 14.79it/s]\n",
      "1330it [01:29, 14.94it/s]\n",
      "Epoch: 4 | Train Loss: 0.6483892 Vali Loss: 0.7214208 Test Loss: 1.0189342 MAE Loss: 0.7531949\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:12,  8.01it/s]\titers: 100, epoch: 5 | loss: 0.8126152\n",
      "\tspeed: 2.0283s/iter; left time: 200131.4802s\n",
      "199it [00:24,  7.87it/s]\titers: 200, epoch: 5 | loss: 0.7573357\n",
      "\tspeed: 0.1210s/iter; left time: 11925.0894s\n",
      "299it [00:36,  8.91it/s]\titers: 300, epoch: 5 | loss: 0.6715704\n",
      "\tspeed: 0.1198s/iter; left time: 11793.8157s\n",
      "399it [00:48,  8.56it/s]\titers: 400, epoch: 5 | loss: 0.6005729\n",
      "\tspeed: 0.1189s/iter; left time: 11693.9913s\n",
      "499it [01:00,  7.94it/s]\titers: 500, epoch: 5 | loss: 0.6521170\n",
      "\tspeed: 0.1237s/iter; left time: 12154.6930s\n",
      "599it [01:12,  8.76it/s]\titers: 600, epoch: 5 | loss: 0.6633800\n",
      "\tspeed: 0.1210s/iter; left time: 11878.7607s\n",
      "699it [01:24,  8.17it/s]\titers: 700, epoch: 5 | loss: 0.8123664\n",
      "\tspeed: 0.1227s/iter; left time: 12034.4659s\n",
      "799it [01:37,  8.44it/s]\titers: 800, epoch: 5 | loss: 0.6120521\n",
      "\tspeed: 0.1210s/iter; left time: 11856.8463s\n",
      "899it [01:48,  8.69it/s]\titers: 900, epoch: 5 | loss: 0.6434280\n",
      "\tspeed: 0.1190s/iter; left time: 11642.9498s\n",
      "999it [02:00,  8.06it/s]\titers: 1000, epoch: 5 | loss: 0.5582605\n",
      "\tspeed: 0.1173s/iter; left time: 11463.8832s\n",
      "1099it [02:12,  8.61it/s]\titers: 1100, epoch: 5 | loss: 0.5725062\n",
      "\tspeed: 0.1180s/iter; left time: 11526.9343s\n",
      "1199it [02:24,  7.78it/s]\titers: 1200, epoch: 5 | loss: 0.4973342\n",
      "\tspeed: 0.1188s/iter; left time: 11589.5620s\n",
      "1299it [02:36,  8.20it/s]\titers: 1300, epoch: 5 | loss: 0.4734999\n",
      "\tspeed: 0.1211s/iter; left time: 11803.0429s\n",
      "1399it [02:48,  8.44it/s]\titers: 1400, epoch: 5 | loss: 0.6924845\n",
      "\tspeed: 0.1186s/iter; left time: 11545.2542s\n",
      "1499it [03:00,  8.50it/s]\titers: 1500, epoch: 5 | loss: 0.6285315\n",
      "\tspeed: 0.1192s/iter; left time: 11594.8716s\n",
      "1599it [03:12,  8.42it/s]\titers: 1600, epoch: 5 | loss: 0.4533915\n",
      "\tspeed: 0.1180s/iter; left time: 11461.4508s\n",
      "1699it [03:23,  8.51it/s]\titers: 1700, epoch: 5 | loss: 0.5363076\n",
      "\tspeed: 0.1192s/iter; left time: 11574.2755s\n",
      "1799it [03:35,  7.92it/s]\titers: 1800, epoch: 5 | loss: 0.5549452\n",
      "\tspeed: 0.1142s/iter; left time: 11078.2218s\n",
      "1899it [03:46,  8.12it/s]\titers: 1900, epoch: 5 | loss: 0.5264734\n",
      "\tspeed: 0.1159s/iter; left time: 11224.6765s\n",
      "1999it [03:58,  8.32it/s]\titers: 2000, epoch: 5 | loss: 0.6440559\n",
      "\tspeed: 0.1155s/iter; left time: 11173.9037s\n",
      "2099it [04:10,  8.66it/s]\titers: 2100, epoch: 5 | loss: 0.5187454\n",
      "\tspeed: 0.1199s/iter; left time: 11594.2563s\n",
      "2199it [04:22,  8.47it/s]\titers: 2200, epoch: 5 | loss: 0.5275553\n",
      "\tspeed: 0.1188s/iter; left time: 11469.4321s\n",
      "2299it [04:34,  8.18it/s]\titers: 2300, epoch: 5 | loss: 0.7307553\n",
      "\tspeed: 0.1177s/iter; left time: 11349.8505s\n",
      "2399it [04:46,  8.15it/s]\titers: 2400, epoch: 5 | loss: 0.7603579\n",
      "\tspeed: 0.1214s/iter; left time: 11701.5839s\n",
      "2499it [04:58,  8.50it/s]\titers: 2500, epoch: 5 | loss: 0.6522530\n",
      "\tspeed: 0.1172s/iter; left time: 11281.0883s\n",
      "2599it [05:10,  8.11it/s]\titers: 2600, epoch: 5 | loss: 0.7926790\n",
      "\tspeed: 0.1228s/iter; left time: 11811.0871s\n",
      "2699it [05:22,  8.01it/s]\titers: 2700, epoch: 5 | loss: 0.5723141\n",
      "\tspeed: 0.1209s/iter; left time: 11617.6010s\n",
      "2799it [05:34,  8.29it/s]\titers: 2800, epoch: 5 | loss: 0.7403272\n",
      "\tspeed: 0.1196s/iter; left time: 11481.1070s\n",
      "2899it [05:46,  8.25it/s]\titers: 2900, epoch: 5 | loss: 0.7327533\n",
      "\tspeed: 0.1208s/iter; left time: 11584.2505s\n",
      "2999it [05:58,  8.51it/s]\titers: 3000, epoch: 5 | loss: 0.7350285\n",
      "\tspeed: 0.1203s/iter; left time: 11523.5897s\n",
      "3099it [06:10,  8.55it/s]\titers: 3100, epoch: 5 | loss: 0.6069111\n",
      "\tspeed: 0.1196s/iter; left time: 11445.7048s\n",
      "3199it [06:22,  8.75it/s]\titers: 3200, epoch: 5 | loss: 0.6668432\n",
      "\tspeed: 0.1208s/iter; left time: 11546.7797s\n",
      "3299it [06:34,  8.46it/s]\titers: 3300, epoch: 5 | loss: 0.6804645\n",
      "\tspeed: 0.1174s/iter; left time: 11208.2045s\n",
      "3399it [06:46,  8.42it/s]\titers: 3400, epoch: 5 | loss: 0.6012676\n",
      "\tspeed: 0.1186s/iter; left time: 11312.2642s\n",
      "3499it [06:58,  8.62it/s]\titers: 3500, epoch: 5 | loss: 0.5425645\n",
      "\tspeed: 0.1196s/iter; left time: 11392.7887s\n",
      "3598it [07:09,  8.39it/s]\titers: 3600, epoch: 5 | loss: 0.5281562\n",
      "\tspeed: 0.1201s/iter; left time: 11430.0124s\n",
      "3699it [07:22,  8.49it/s]\titers: 3700, epoch: 5 | loss: 0.6783710\n",
      "\tspeed: 0.1215s/iter; left time: 11554.5453s\n",
      "3799it [07:34,  8.24it/s]\titers: 3800, epoch: 5 | loss: 0.5911309\n",
      "\tspeed: 0.1240s/iter; left time: 11775.6383s\n",
      "3899it [07:46,  7.92it/s]\titers: 3900, epoch: 5 | loss: 0.8730040\n",
      "\tspeed: 0.1164s/iter; left time: 11042.9820s\n",
      "3999it [07:57,  8.18it/s]\titers: 4000, epoch: 5 | loss: 0.7742999\n",
      "\tspeed: 0.1166s/iter; left time: 11050.3583s\n",
      "4099it [08:09,  8.16it/s]\titers: 4100, epoch: 5 | loss: 0.6368327\n",
      "\tspeed: 0.1198s/iter; left time: 11343.7667s\n",
      "4199it [08:22,  8.44it/s]\titers: 4200, epoch: 5 | loss: 0.6238154\n",
      "\tspeed: 0.1216s/iter; left time: 11495.5591s\n",
      "4299it [08:34,  8.77it/s]\titers: 4300, epoch: 5 | loss: 0.6801120\n",
      "\tspeed: 0.1191s/iter; left time: 11246.9720s\n",
      "4399it [08:45,  8.24it/s]\titers: 4400, epoch: 5 | loss: 0.6134070\n",
      "\tspeed: 0.1171s/iter; left time: 11050.3165s\n",
      "4499it [08:57,  7.85it/s]\titers: 4500, epoch: 5 | loss: 0.7212514\n",
      "\tspeed: 0.1195s/iter; left time: 11265.1984s\n",
      "4599it [09:09,  8.07it/s]\titers: 4600, epoch: 5 | loss: 0.8245180\n",
      "\tspeed: 0.1201s/iter; left time: 11310.6500s\n",
      "4699it [09:21,  8.14it/s]\titers: 4700, epoch: 5 | loss: 0.6148605\n",
      "\tspeed: 0.1218s/iter; left time: 11456.6852s\n",
      "4799it [09:33,  8.59it/s]\titers: 4800, epoch: 5 | loss: 0.5840098\n",
      "\tspeed: 0.1184s/iter; left time: 11125.2549s\n",
      "4899it [09:45,  9.06it/s]\titers: 4900, epoch: 5 | loss: 0.9425678\n",
      "\tspeed: 0.1178s/iter; left time: 11053.6402s\n",
      "4999it [09:56,  9.63it/s]\titers: 5000, epoch: 5 | loss: 0.7292655\n",
      "\tspeed: 0.1151s/iter; left time: 10792.4891s\n",
      "5099it [10:08,  9.36it/s]\titers: 5100, epoch: 5 | loss: 0.5553244\n",
      "\tspeed: 0.1182s/iter; left time: 11070.1344s\n",
      "5199it [10:20,  8.59it/s]\titers: 5200, epoch: 5 | loss: 0.4855934\n",
      "\tspeed: 0.1170s/iter; left time: 10947.8622s\n",
      "5299it [10:32,  7.96it/s]\titers: 5300, epoch: 5 | loss: 0.7182826\n",
      "\tspeed: 0.1183s/iter; left time: 11060.3440s\n",
      "5399it [10:44,  8.57it/s]\titers: 5400, epoch: 5 | loss: 0.7478994\n",
      "\tspeed: 0.1172s/iter; left time: 10945.2973s\n",
      "5499it [10:55,  8.47it/s]\titers: 5500, epoch: 5 | loss: 0.5109714\n",
      "\tspeed: 0.1175s/iter; left time: 10956.8818s\n",
      "5599it [11:07,  8.69it/s]\titers: 5600, epoch: 5 | loss: 0.6087073\n",
      "\tspeed: 0.1207s/iter; left time: 11245.8810s\n",
      "5699it [11:19,  8.14it/s]\titers: 5700, epoch: 5 | loss: 0.6483462\n",
      "\tspeed: 0.1196s/iter; left time: 11130.8738s\n",
      "5799it [11:31,  8.07it/s]\titers: 5800, epoch: 5 | loss: 0.5759965\n",
      "\tspeed: 0.1198s/iter; left time: 11141.8398s\n",
      "5899it [11:43,  8.55it/s]\titers: 5900, epoch: 5 | loss: 0.7657604\n",
      "\tspeed: 0.1187s/iter; left time: 11025.8864s\n",
      "5999it [11:55,  7.91it/s]\titers: 6000, epoch: 5 | loss: 0.5493364\n",
      "\tspeed: 0.1191s/iter; left time: 11047.8794s\n",
      "6099it [12:07,  8.54it/s]\titers: 6100, epoch: 5 | loss: 0.5454033\n",
      "\tspeed: 0.1209s/iter; left time: 11203.6325s\n",
      "6173it [12:16,  8.38it/s]\n",
      "Epoch: 5 cost time: 736.5943758487701\n",
      "1335it [01:29, 14.84it/s]\n",
      "1330it [01:30, 14.64it/s]\n",
      "Epoch: 5 | Train Loss: 0.6442938 Vali Loss: 0.6968613 Test Loss: 0.9829489 MAE Loss: 0.7333452\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:12,  7.81it/s]\titers: 100, epoch: 6 | loss: 0.5675007\n",
      "\tspeed: 2.0431s/iter; left time: 188979.6866s\n",
      "199it [00:24,  8.32it/s]\titers: 200, epoch: 6 | loss: 0.5552901\n",
      "\tspeed: 0.1199s/iter; left time: 11077.7431s\n",
      "299it [00:36,  8.03it/s]\titers: 300, epoch: 6 | loss: 0.5235605\n",
      "\tspeed: 0.1228s/iter; left time: 11336.0520s\n",
      "399it [00:48,  7.78it/s]\titers: 400, epoch: 6 | loss: 0.6013843\n",
      "\tspeed: 0.1210s/iter; left time: 11154.6509s\n",
      "499it [01:00,  8.48it/s]\titers: 500, epoch: 6 | loss: 0.6597791\n",
      "\tspeed: 0.1215s/iter; left time: 11191.8653s\n",
      "599it [01:13,  7.89it/s]\titers: 600, epoch: 6 | loss: 0.6541992\n",
      "\tspeed: 0.1231s/iter; left time: 11327.0097s\n",
      "699it [01:24,  8.77it/s]\titers: 700, epoch: 6 | loss: 0.4194359\n",
      "\tspeed: 0.1179s/iter; left time: 10838.5200s\n",
      "798it [01:36,  8.59it/s]\titers: 800, epoch: 6 | loss: 0.6628745\n",
      "\tspeed: 0.1176s/iter; left time: 10794.7086s\n",
      "899it [01:48,  8.28it/s]\titers: 900, epoch: 6 | loss: 0.6971866\n",
      "\tspeed: 0.1167s/iter; left time: 10703.1018s\n",
      "999it [02:00,  8.30it/s]\titers: 1000, epoch: 6 | loss: 0.6352412\n",
      "\tspeed: 0.1193s/iter; left time: 10924.7906s\n",
      "1099it [02:11,  9.03it/s]\titers: 1100, epoch: 6 | loss: 0.7140856\n",
      "\tspeed: 0.1159s/iter; left time: 10600.4368s\n",
      "1199it [02:23,  8.48it/s]\titers: 1200, epoch: 6 | loss: 0.6487141\n",
      "\tspeed: 0.1175s/iter; left time: 10737.4883s\n",
      "1299it [02:35,  9.10it/s]\titers: 1300, epoch: 6 | loss: 0.8453677\n",
      "\tspeed: 0.1196s/iter; left time: 10919.5099s\n",
      "1399it [02:47,  8.64it/s]\titers: 1400, epoch: 6 | loss: 0.6885756\n",
      "\tspeed: 0.1204s/iter; left time: 10977.7097s\n",
      "1499it [02:59,  7.89it/s]\titers: 1500, epoch: 6 | loss: 0.6439379\n",
      "\tspeed: 0.1197s/iter; left time: 10899.9938s\n",
      "1599it [03:11,  8.63it/s]\titers: 1600, epoch: 6 | loss: 0.9012244\n",
      "\tspeed: 0.1174s/iter; left time: 10686.2124s\n",
      "1699it [03:23,  8.18it/s]\titers: 1700, epoch: 6 | loss: 0.7970987\n",
      "\tspeed: 0.1203s/iter; left time: 10932.5534s\n",
      "1799it [03:35,  8.46it/s]\titers: 1800, epoch: 6 | loss: 0.6954539\n",
      "\tspeed: 0.1179s/iter; left time: 10705.0133s\n",
      "1899it [03:47,  8.65it/s]\titers: 1900, epoch: 6 | loss: 0.7858862\n",
      "\tspeed: 0.1194s/iter; left time: 10826.4603s\n",
      "1999it [03:58,  8.39it/s]\titers: 2000, epoch: 6 | loss: 0.5245412\n",
      "\tspeed: 0.1181s/iter; left time: 10697.0016s\n",
      "2099it [04:10,  7.65it/s]\titers: 2100, epoch: 6 | loss: 0.5691040\n",
      "\tspeed: 0.1182s/iter; left time: 10700.6375s\n",
      "2199it [04:22,  8.68it/s]\titers: 2200, epoch: 6 | loss: 0.5889071\n",
      "\tspeed: 0.1202s/iter; left time: 10864.2843s\n",
      "2299it [04:34,  8.83it/s]\titers: 2300, epoch: 6 | loss: 0.7564570\n",
      "\tspeed: 0.1197s/iter; left time: 10806.1454s\n",
      "2399it [04:46,  8.33it/s]\titers: 2400, epoch: 6 | loss: 0.7308481\n",
      "\tspeed: 0.1199s/iter; left time: 10811.8573s\n",
      "2499it [04:58,  9.13it/s]\titers: 2500, epoch: 6 | loss: 0.3713342\n",
      "\tspeed: 0.1188s/iter; left time: 10706.8433s\n",
      "2599it [05:10,  7.88it/s]\titers: 2600, epoch: 6 | loss: 0.6182031\n",
      "\tspeed: 0.1208s/iter; left time: 10874.6586s\n",
      "2699it [05:22,  9.01it/s]\titers: 2700, epoch: 6 | loss: 0.6468084\n",
      "\tspeed: 0.1189s/iter; left time: 10687.7690s\n",
      "2799it [05:34,  7.98it/s]\titers: 2800, epoch: 6 | loss: 0.5231996\n",
      "\tspeed: 0.1224s/iter; left time: 10988.2611s\n",
      "2899it [05:46,  8.27it/s]\titers: 2900, epoch: 6 | loss: 0.4942888\n",
      "\tspeed: 0.1169s/iter; left time: 10488.4003s\n",
      "2999it [05:58,  8.06it/s]\titers: 3000, epoch: 6 | loss: 0.5977453\n",
      "\tspeed: 0.1213s/iter; left time: 10867.7260s\n",
      "3099it [06:10,  8.07it/s]\titers: 3100, epoch: 6 | loss: 0.6699147\n",
      "\tspeed: 0.1208s/iter; left time: 10813.0770s\n",
      "3199it [06:22,  8.57it/s]\titers: 3200, epoch: 6 | loss: 0.9513096\n",
      "\tspeed: 0.1203s/iter; left time: 10750.6157s\n",
      "3299it [06:34,  8.70it/s]\titers: 3300, epoch: 6 | loss: 0.8499225\n",
      "\tspeed: 0.1194s/iter; left time: 10664.2247s\n",
      "3399it [06:46,  8.25it/s]\titers: 3400, epoch: 6 | loss: 0.6240779\n",
      "\tspeed: 0.1197s/iter; left time: 10676.8444s\n",
      "3499it [06:58,  8.14it/s]\titers: 3500, epoch: 6 | loss: 0.6785550\n",
      "\tspeed: 0.1212s/iter; left time: 10798.6960s\n",
      "3599it [07:10,  8.05it/s]\titers: 3600, epoch: 6 | loss: 0.7837846\n",
      "\tspeed: 0.1216s/iter; left time: 10818.9554s\n",
      "3699it [07:22,  8.55it/s]\titers: 3700, epoch: 6 | loss: 0.6005113\n",
      "\tspeed: 0.1206s/iter; left time: 10723.2661s\n",
      "3799it [07:35,  8.49it/s]\titers: 3800, epoch: 6 | loss: 0.4475955\n",
      "\tspeed: 0.1219s/iter; left time: 10828.0315s\n",
      "3899it [07:47,  7.84it/s]\titers: 3900, epoch: 6 | loss: 0.5308958\n",
      "\tspeed: 0.1190s/iter; left time: 10558.8862s\n",
      "3999it [07:59,  8.35it/s]\titers: 4000, epoch: 6 | loss: 0.8648143\n",
      "\tspeed: 0.1204s/iter; left time: 10664.6593s\n",
      "4099it [08:11,  8.50it/s]\titers: 4100, epoch: 6 | loss: 0.7382227\n",
      "\tspeed: 0.1208s/iter; left time: 10687.2447s\n",
      "4199it [08:23,  8.13it/s]\titers: 4200, epoch: 6 | loss: 0.7245885\n",
      "\tspeed: 0.1201s/iter; left time: 10620.3397s\n",
      "4299it [08:35,  8.06it/s]\titers: 4300, epoch: 6 | loss: 0.6032653\n",
      "\tspeed: 0.1220s/iter; left time: 10775.5272s\n",
      "4399it [08:47,  8.38it/s]\titers: 4400, epoch: 6 | loss: 0.5548098\n",
      "\tspeed: 0.1195s/iter; left time: 10535.6486s\n",
      "4498it [08:59,  7.99it/s]\titers: 4500, epoch: 6 | loss: 0.6726308\n",
      "\tspeed: 0.1197s/iter; left time: 10545.8694s\n",
      "4599it [09:11,  8.76it/s]\titers: 4600, epoch: 6 | loss: 0.7305983\n",
      "\tspeed: 0.1177s/iter; left time: 10357.7372s\n",
      "4699it [09:23,  8.55it/s]\titers: 4700, epoch: 6 | loss: 0.6464282\n",
      "\tspeed: 0.1205s/iter; left time: 10593.6300s\n",
      "4799it [09:34,  8.40it/s]\titers: 4800, epoch: 6 | loss: 0.6864323\n",
      "\tspeed: 0.1183s/iter; left time: 10382.5902s\n",
      "4899it [09:46,  8.37it/s]\titers: 4900, epoch: 6 | loss: 0.7686394\n",
      "\tspeed: 0.1183s/iter; left time: 10373.5835s\n",
      "4999it [09:58,  8.18it/s]\titers: 5000, epoch: 6 | loss: 0.5052314\n",
      "\tspeed: 0.1193s/iter; left time: 10449.0576s\n",
      "5099it [10:10,  8.17it/s]\titers: 5100, epoch: 6 | loss: 0.6501489\n",
      "\tspeed: 0.1229s/iter; left time: 10752.5045s\n",
      "5199it [10:22,  8.78it/s]\titers: 5200, epoch: 6 | loss: 0.4809220\n",
      "\tspeed: 0.1174s/iter; left time: 10260.8684s\n",
      "5299it [10:34,  8.01it/s]\titers: 5300, epoch: 6 | loss: 0.5980497\n",
      "\tspeed: 0.1218s/iter; left time: 10632.5860s\n",
      "5399it [10:47,  8.53it/s]\titers: 5400, epoch: 6 | loss: 0.6753514\n",
      "\tspeed: 0.1220s/iter; left time: 10637.2670s\n",
      "5499it [10:59,  8.60it/s]\titers: 5500, epoch: 6 | loss: 0.5785861\n",
      "\tspeed: 0.1218s/iter; left time: 10609.4293s\n",
      "5599it [11:11,  8.17it/s]\titers: 5600, epoch: 6 | loss: 0.5040081\n",
      "\tspeed: 0.1213s/iter; left time: 10551.3490s\n",
      "5699it [11:23,  8.72it/s]\titers: 5700, epoch: 6 | loss: 0.4276469\n",
      "\tspeed: 0.1208s/iter; left time: 10501.0309s\n",
      "5799it [11:35,  8.28it/s]\titers: 5800, epoch: 6 | loss: 0.5032586\n",
      "\tspeed: 0.1216s/iter; left time: 10555.7512s\n",
      "5899it [11:47,  8.41it/s]\titers: 5900, epoch: 6 | loss: 0.5571391\n",
      "\tspeed: 0.1207s/iter; left time: 10460.7474s\n",
      "5999it [12:00,  7.74it/s]\titers: 6000, epoch: 6 | loss: 0.6518285\n",
      "\tspeed: 0.1232s/iter; left time: 10665.0401s\n",
      "6099it [12:12,  8.51it/s]\titers: 6100, epoch: 6 | loss: 0.5524634\n",
      "\tspeed: 0.1224s/iter; left time: 10582.8585s\n",
      "6173it [12:21,  8.33it/s]\n",
      "Epoch: 6 cost time: 741.2432270050049\n",
      "1335it [01:34, 14.16it/s]\n",
      "1330it [01:34, 14.01it/s]\n",
      "Epoch: 6 | Train Loss: 0.6396702 Vali Loss: 0.7386021 Test Loss: 1.0377797 MAE Loss: 0.7621854\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:13,  8.47it/s]\titers: 100, epoch: 7 | loss: 0.5043622\n",
      "\tspeed: 2.1195s/iter; left time: 182963.9223s\n",
      "199it [00:25,  8.79it/s]\titers: 200, epoch: 7 | loss: 0.5196992\n",
      "\tspeed: 0.1229s/iter; left time: 10600.6433s\n",
      "299it [00:37,  8.82it/s]\titers: 300, epoch: 7 | loss: 0.6001461\n",
      "\tspeed: 0.1195s/iter; left time: 10293.0371s\n",
      "399it [00:49,  8.40it/s]\titers: 400, epoch: 7 | loss: 0.6327181\n",
      "\tspeed: 0.1207s/iter; left time: 10384.1488s\n",
      "499it [01:01,  8.74it/s]\titers: 500, epoch: 7 | loss: 0.6984051\n",
      "\tspeed: 0.1218s/iter; left time: 10461.8213s\n",
      "599it [01:13,  8.13it/s]\titers: 600, epoch: 7 | loss: 0.5684279\n",
      "\tspeed: 0.1204s/iter; left time: 10336.1187s\n",
      "699it [01:25,  8.67it/s]\titers: 700, epoch: 7 | loss: 0.6511567\n",
      "\tspeed: 0.1231s/iter; left time: 10551.5225s\n",
      "799it [01:38,  8.06it/s]\titers: 800, epoch: 7 | loss: 0.4851464\n",
      "\tspeed: 0.1220s/iter; left time: 10447.6918s\n",
      "899it [01:49,  8.80it/s]\titers: 900, epoch: 7 | loss: 0.4360399\n",
      "\tspeed: 0.1172s/iter; left time: 10019.1250s\n",
      "999it [02:01,  8.30it/s]\titers: 1000, epoch: 7 | loss: 0.5556105\n",
      "\tspeed: 0.1205s/iter; left time: 10290.7758s\n",
      "1099it [02:14,  7.94it/s]\titers: 1100, epoch: 7 | loss: 0.5824332\n",
      "\tspeed: 0.1225s/iter; left time: 10453.0915s\n",
      "1199it [02:25,  8.25it/s]\titers: 1200, epoch: 7 | loss: 0.7346212\n",
      "\tspeed: 0.1181s/iter; left time: 10062.5636s\n",
      "1299it [02:38,  7.79it/s]\titers: 1300, epoch: 7 | loss: 0.7410363\n",
      "\tspeed: 0.1238s/iter; left time: 10539.7654s\n",
      "1399it [02:50,  8.36it/s]\titers: 1400, epoch: 7 | loss: 0.7214725\n",
      "\tspeed: 0.1194s/iter; left time: 10154.4356s\n",
      "1499it [03:02,  8.29it/s]\titers: 1500, epoch: 7 | loss: 0.7576197\n",
      "\tspeed: 0.1210s/iter; left time: 10272.3549s\n",
      "1599it [03:14,  8.80it/s]\titers: 1600, epoch: 7 | loss: 0.6035191\n",
      "\tspeed: 0.1186s/iter; left time: 10062.9866s\n",
      "1699it [03:26,  8.66it/s]\titers: 1700, epoch: 7 | loss: 0.6667004\n",
      "\tspeed: 0.1196s/iter; left time: 10136.4296s\n",
      "1799it [03:38,  8.40it/s]\titers: 1800, epoch: 7 | loss: 0.7430739\n",
      "\tspeed: 0.1205s/iter; left time: 10200.9081s\n",
      "1899it [03:50,  7.48it/s]\titers: 1900, epoch: 7 | loss: 0.7470289\n",
      "\tspeed: 0.1216s/iter; left time: 10277.1217s\n",
      "1999it [04:02,  8.94it/s]\titers: 2000, epoch: 7 | loss: 0.5344787\n",
      "\tspeed: 0.1249s/iter; left time: 10545.9707s\n",
      "2099it [04:15,  8.59it/s]\titers: 2100, epoch: 7 | loss: 0.6264910\n",
      "\tspeed: 0.1221s/iter; left time: 10297.3069s\n",
      "2199it [04:26,  8.36it/s]\titers: 2200, epoch: 7 | loss: 0.6916106\n",
      "\tspeed: 0.1171s/iter; left time: 9858.8780s\n",
      "2299it [04:39,  7.99it/s]\titers: 2300, epoch: 7 | loss: 0.7216309\n",
      "\tspeed: 0.1247s/iter; left time: 10487.7129s\n",
      "2399it [04:51,  8.56it/s]\titers: 2400, epoch: 7 | loss: 0.5342805\n",
      "\tspeed: 0.1206s/iter; left time: 10134.0394s\n",
      "2499it [05:03,  8.76it/s]\titers: 2500, epoch: 7 | loss: 0.6974407\n",
      "\tspeed: 0.1209s/iter; left time: 10148.8454s\n",
      "2599it [05:15,  8.11it/s]\titers: 2600, epoch: 7 | loss: 0.7096314\n",
      "\tspeed: 0.1200s/iter; left time: 10059.0670s\n",
      "2699it [05:27,  7.99it/s]\titers: 2700, epoch: 7 | loss: 0.7871054\n",
      "\tspeed: 0.1205s/iter; left time: 10085.4820s\n",
      "2799it [05:39,  7.98it/s]\titers: 2800, epoch: 7 | loss: 0.5503089\n",
      "\tspeed: 0.1183s/iter; left time: 9892.3885s\n",
      "2899it [05:50,  8.92it/s]\titers: 2900, epoch: 7 | loss: 0.6806296\n",
      "\tspeed: 0.1157s/iter; left time: 9662.3718s\n",
      "2999it [06:02,  8.69it/s]\titers: 3000, epoch: 7 | loss: 0.5371307\n",
      "\tspeed: 0.1186s/iter; left time: 9895.2796s\n",
      "3099it [06:14,  8.94it/s]\titers: 3100, epoch: 7 | loss: 0.8869063\n",
      "\tspeed: 0.1176s/iter; left time: 9802.3988s\n",
      "3199it [06:26,  8.06it/s]\titers: 3200, epoch: 7 | loss: 0.7764519\n",
      "\tspeed: 0.1179s/iter; left time: 9809.3956s\n",
      "3299it [06:38,  8.69it/s]\titers: 3300, epoch: 7 | loss: 0.4802125\n",
      "\tspeed: 0.1198s/iter; left time: 9957.3373s\n",
      "3399it [06:50,  8.27it/s]\titers: 3400, epoch: 7 | loss: 0.6555362\n",
      "\tspeed: 0.1175s/iter; left time: 9755.1807s\n",
      "3499it [07:02,  8.14it/s]\titers: 3500, epoch: 7 | loss: 0.4798719\n",
      "\tspeed: 0.1221s/iter; left time: 10122.2382s\n",
      "3599it [07:14,  8.27it/s]\titers: 3600, epoch: 7 | loss: 0.7603977\n",
      "\tspeed: 0.1214s/iter; left time: 10058.0579s\n",
      "3699it [07:26,  8.57it/s]\titers: 3700, epoch: 7 | loss: 0.7049726\n",
      "\tspeed: 0.1211s/iter; left time: 10019.6474s\n",
      "3799it [07:38,  8.38it/s]\titers: 3800, epoch: 7 | loss: 0.6088646\n",
      "\tspeed: 0.1190s/iter; left time: 9831.0481s\n",
      "3899it [07:50,  8.52it/s]\titers: 3900, epoch: 7 | loss: 0.4450479\n",
      "\tspeed: 0.1215s/iter; left time: 10024.2562s\n",
      "3999it [08:02,  8.23it/s]\titers: 4000, epoch: 7 | loss: 0.4579464\n",
      "\tspeed: 0.1194s/iter; left time: 9837.3866s\n",
      "4099it [08:14,  8.42it/s]\titers: 4100, epoch: 7 | loss: 0.5198249\n",
      "\tspeed: 0.1190s/iter; left time: 9799.6966s\n",
      "4199it [08:26,  8.61it/s]\titers: 4200, epoch: 7 | loss: 0.5351925\n",
      "\tspeed: 0.1177s/iter; left time: 9680.9324s\n",
      "4299it [08:38,  8.30it/s]\titers: 4300, epoch: 7 | loss: 0.8543972\n",
      "\tspeed: 0.1183s/iter; left time: 9717.7878s\n",
      "4399it [08:50,  7.48it/s]\titers: 4400, epoch: 7 | loss: 0.9149176\n",
      "\tspeed: 0.1222s/iter; left time: 10026.7798s\n",
      "4499it [09:02,  8.14it/s]\titers: 4500, epoch: 7 | loss: 0.7106903\n",
      "\tspeed: 0.1216s/iter; left time: 9959.2106s\n",
      "4599it [09:14,  7.98it/s]\titers: 4600, epoch: 7 | loss: 0.4573101\n",
      "\tspeed: 0.1200s/iter; left time: 9817.1207s\n",
      "4699it [09:26,  8.12it/s]\titers: 4700, epoch: 7 | loss: 0.7656150\n",
      "\tspeed: 0.1251s/iter; left time: 10227.4574s\n",
      "4799it [09:39,  8.68it/s]\titers: 4800, epoch: 7 | loss: 0.5225155\n",
      "\tspeed: 0.1215s/iter; left time: 9913.4862s\n",
      "4899it [09:50,  8.31it/s]\titers: 4900, epoch: 7 | loss: 0.8578312\n",
      "\tspeed: 0.1183s/iter; left time: 9646.2953s\n",
      "4999it [10:02,  8.24it/s]\titers: 5000, epoch: 7 | loss: 0.5376955\n",
      "\tspeed: 0.1192s/iter; left time: 9705.3167s\n",
      "5099it [10:15,  8.03it/s]\titers: 5100, epoch: 7 | loss: 0.4459453\n",
      "\tspeed: 0.1223s/iter; left time: 9948.2002s\n",
      "5199it [10:27,  7.85it/s]\titers: 5200, epoch: 7 | loss: 0.6272886\n",
      "\tspeed: 0.1242s/iter; left time: 10088.7236s\n",
      "5299it [10:39,  8.35it/s]\titers: 5300, epoch: 7 | loss: 0.7295015\n",
      "\tspeed: 0.1211s/iter; left time: 9821.8686s\n",
      "5399it [10:51,  8.24it/s]\titers: 5400, epoch: 7 | loss: 0.6025521\n",
      "\tspeed: 0.1199s/iter; left time: 9714.2012s\n",
      "5498it [11:03,  8.77it/s]\titers: 5500, epoch: 7 | loss: 0.4102928\n",
      "\tspeed: 0.1210s/iter; left time: 9790.3430s\n",
      "5599it [11:15,  8.22it/s]\titers: 5600, epoch: 7 | loss: 0.7524583\n",
      "\tspeed: 0.1192s/iter; left time: 9637.9259s\n",
      "5699it [11:27,  8.36it/s]\titers: 5700, epoch: 7 | loss: 0.6099494\n",
      "\tspeed: 0.1181s/iter; left time: 9531.7883s\n",
      "5799it [11:39,  8.42it/s]\titers: 5800, epoch: 7 | loss: 0.5665967\n",
      "\tspeed: 0.1215s/iter; left time: 9797.5503s\n",
      "5899it [11:51,  8.05it/s]\titers: 5900, epoch: 7 | loss: 0.5955113\n",
      "\tspeed: 0.1193s/iter; left time: 9604.0673s\n",
      "5999it [12:03,  8.51it/s]\titers: 6000, epoch: 7 | loss: 0.8353598\n",
      "\tspeed: 0.1168s/iter; left time: 9391.2290s\n",
      "6099it [12:14,  8.02it/s]\titers: 6100, epoch: 7 | loss: 0.5290121\n",
      "\tspeed: 0.1179s/iter; left time: 9471.3186s\n",
      "6173it [12:23,  8.30it/s]\n",
      "Epoch: 7 cost time: 743.7027688026428\n",
      "1335it [01:30, 14.68it/s]\n",
      "1330it [01:29, 14.82it/s]\n",
      "Epoch: 7 | Train Loss: 0.6404414 Vali Loss: 0.7249770 Test Loss: 1.0149461 MAE Loss: 0.7483824\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0020000000\n",
      "learning_rate 0.05\n",
      "lr 0.0020000000000000018\n",
      "99it [00:11,  8.23it/s]\titers: 100, epoch: 8 | loss: 0.5858895\n",
      "\tspeed: 2.0178s/iter; left time: 161723.7940s\n",
      "199it [00:23,  8.85it/s]\titers: 200, epoch: 8 | loss: 0.7255648\n",
      "\tspeed: 0.1135s/iter; left time: 9087.9823s\n",
      "299it [00:35,  7.77it/s]\titers: 300, epoch: 8 | loss: 0.5044036\n",
      "\tspeed: 0.1247s/iter; left time: 9967.5297s\n",
      "399it [00:48,  7.92it/s]\titers: 400, epoch: 8 | loss: 0.7978352\n",
      "\tspeed: 0.1258s/iter; left time: 10044.2367s\n",
      "499it [01:00,  8.30it/s]\titers: 500, epoch: 8 | loss: 0.4492918\n",
      "\tspeed: 0.1228s/iter; left time: 9793.0051s\n",
      "599it [01:12,  8.40it/s]\titers: 600, epoch: 8 | loss: 0.6343281\n",
      "\tspeed: 0.1197s/iter; left time: 9532.9261s\n",
      "699it [01:24,  8.96it/s]\titers: 700, epoch: 8 | loss: 0.6025944\n",
      "\tspeed: 0.1189s/iter; left time: 9457.5014s\n",
      "799it [01:36,  8.64it/s]\titers: 800, epoch: 8 | loss: 0.5820878\n",
      "\tspeed: 0.1176s/iter; left time: 9340.4656s\n",
      "899it [01:47,  8.75it/s]\titers: 900, epoch: 8 | loss: 0.7469094\n",
      "\tspeed: 0.1143s/iter; left time: 9066.3869s\n",
      "999it [01:59,  8.91it/s]\titers: 1000, epoch: 8 | loss: 0.5704411\n",
      "\tspeed: 0.1162s/iter; left time: 9211.7038s\n",
      "1099it [02:11,  8.25it/s]\titers: 1100, epoch: 8 | loss: 0.6857221\n",
      "\tspeed: 0.1174s/iter; left time: 9293.5044s\n",
      "1199it [02:22,  8.58it/s]\titers: 1200, epoch: 8 | loss: 0.6490406\n",
      "\tspeed: 0.1154s/iter; left time: 9121.4534s\n",
      "1299it [02:34,  8.98it/s]\titers: 1300, epoch: 8 | loss: 0.5321296\n",
      "\tspeed: 0.1188s/iter; left time: 9381.5911s\n",
      "1399it [02:46,  8.48it/s]\titers: 1400, epoch: 8 | loss: 0.7313747\n",
      "\tspeed: 0.1208s/iter; left time: 9526.0182s\n",
      "1499it [02:58,  8.08it/s]\titers: 1500, epoch: 8 | loss: 0.7689141\n",
      "\tspeed: 0.1157s/iter; left time: 9112.3438s\n",
      "1599it [03:09,  8.82it/s]\titers: 1600, epoch: 8 | loss: 0.6580412\n",
      "\tspeed: 0.1144s/iter; left time: 8996.1730s\n",
      "1699it [03:21,  8.70it/s]\titers: 1700, epoch: 8 | loss: 0.7116360\n",
      "\tspeed: 0.1171s/iter; left time: 9201.2249s\n",
      "1799it [03:32,  8.82it/s]\titers: 1800, epoch: 8 | loss: 0.6514261\n",
      "\tspeed: 0.1159s/iter; left time: 9095.0706s\n",
      "1898it [03:44,  8.30it/s]\titers: 1900, epoch: 8 | loss: 0.5361025\n",
      "\tspeed: 0.1156s/iter; left time: 9060.5843s\n",
      "1999it [03:55,  9.10it/s]\titers: 2000, epoch: 8 | loss: 0.4678637\n",
      "\tspeed: 0.1145s/iter; left time: 8958.6456s\n",
      "2099it [04:07,  8.96it/s]\titers: 2100, epoch: 8 | loss: 0.6509590\n",
      "\tspeed: 0.1153s/iter; left time: 9008.6016s\n",
      "2198it [04:18,  9.05it/s]\titers: 2200, epoch: 8 | loss: 0.4784549\n",
      "\tspeed: 0.1147s/iter; left time: 8949.2134s\n",
      "2299it [04:30,  8.65it/s]\titers: 2300, epoch: 8 | loss: 0.6780125\n",
      "\tspeed: 0.1181s/iter; left time: 9202.6188s\n",
      "2399it [04:42,  8.18it/s]\titers: 2400, epoch: 8 | loss: 0.6261269\n",
      "\tspeed: 0.1154s/iter; left time: 8984.2075s\n",
      "2499it [04:53,  9.11it/s]\titers: 2500, epoch: 8 | loss: 0.7692373\n",
      "\tspeed: 0.1172s/iter; left time: 9111.2471s\n",
      "2599it [05:05,  8.54it/s]\titers: 2600, epoch: 8 | loss: 0.6287482\n",
      "\tspeed: 0.1160s/iter; left time: 9008.3845s\n",
      "2699it [05:17,  9.38it/s]\titers: 2700, epoch: 8 | loss: 0.5352294\n",
      "\tspeed: 0.1147s/iter; left time: 8892.7881s\n",
      "2799it [05:28,  8.27it/s]\titers: 2800, epoch: 8 | loss: 0.7158191\n",
      "\tspeed: 0.1157s/iter; left time: 8959.7383s\n",
      "2899it [05:40,  8.08it/s]\titers: 2900, epoch: 8 | loss: 0.6128228\n",
      "\tspeed: 0.1203s/iter; left time: 9304.3085s\n",
      "2999it [05:52,  8.27it/s]\titers: 3000, epoch: 8 | loss: 0.6581031\n",
      "\tspeed: 0.1192s/iter; left time: 9210.7611s\n",
      "3099it [06:04,  8.35it/s]\titers: 3100, epoch: 8 | loss: 0.6626729\n",
      "\tspeed: 0.1161s/iter; left time: 8953.8608s\n",
      "3199it [06:15,  8.83it/s]\titers: 3200, epoch: 8 | loss: 0.7482229\n",
      "\tspeed: 0.1152s/iter; left time: 8875.9651s\n",
      "3299it [06:27,  8.26it/s]\titers: 3300, epoch: 8 | loss: 0.5897548\n",
      "\tspeed: 0.1169s/iter; left time: 8991.6854s\n",
      "3399it [06:38,  8.36it/s]\titers: 3400, epoch: 8 | loss: 0.5935475\n",
      "\tspeed: 0.1161s/iter; left time: 8922.3210s\n",
      "3499it [06:50,  8.94it/s]\titers: 3500, epoch: 8 | loss: 0.7561337\n",
      "\tspeed: 0.1153s/iter; left time: 8845.8215s\n",
      "3599it [07:02,  8.44it/s]\titers: 3600, epoch: 8 | loss: 0.8075387\n",
      "\tspeed: 0.1151s/iter; left time: 8819.5491s\n",
      "3699it [07:13,  8.70it/s]\titers: 3700, epoch: 8 | loss: 0.6411141\n",
      "\tspeed: 0.1160s/iter; left time: 8878.2800s\n",
      "3799it [07:24,  8.57it/s]\titers: 3800, epoch: 8 | loss: 0.7475184\n",
      "\tspeed: 0.1137s/iter; left time: 8692.1249s\n",
      "3899it [07:36,  8.37it/s]\titers: 3900, epoch: 8 | loss: 0.7038430\n",
      "\tspeed: 0.1160s/iter; left time: 8855.3417s\n",
      "3999it [07:48,  8.77it/s]\titers: 4000, epoch: 8 | loss: 0.4993102\n",
      "\tspeed: 0.1153s/iter; left time: 8789.5338s\n",
      "4099it [07:59,  8.42it/s]\titers: 4100, epoch: 8 | loss: 0.7201823\n",
      "\tspeed: 0.1163s/iter; left time: 8855.5452s\n",
      "4199it [08:11,  8.51it/s]\titers: 4200, epoch: 8 | loss: 0.8398255\n",
      "\tspeed: 0.1130s/iter; left time: 8591.5278s\n",
      "4299it [08:22,  9.11it/s]\titers: 4300, epoch: 8 | loss: 0.5449139\n",
      "\tspeed: 0.1164s/iter; left time: 8844.2973s\n",
      "4399it [08:34,  9.04it/s]\titers: 4400, epoch: 8 | loss: 0.6772476\n",
      "\tspeed: 0.1172s/iter; left time: 8893.3750s\n",
      "4499it [08:45,  9.11it/s]\titers: 4500, epoch: 8 | loss: 0.7419820\n",
      "\tspeed: 0.1157s/iter; left time: 8764.7597s\n",
      "4599it [08:57,  9.22it/s]\titers: 4600, epoch: 8 | loss: 0.8548761\n",
      "\tspeed: 0.1175s/iter; left time: 8886.8443s\n",
      "4699it [09:09,  8.14it/s]\titers: 4700, epoch: 8 | loss: 0.6785772\n",
      "\tspeed: 0.1175s/iter; left time: 8878.3700s\n",
      "4799it [09:21,  8.11it/s]\titers: 4800, epoch: 8 | loss: 0.5498940\n",
      "\tspeed: 0.1170s/iter; left time: 8827.7837s\n",
      "4899it [09:32,  8.70it/s]\titers: 4900, epoch: 8 | loss: 0.9121538\n",
      "\tspeed: 0.1152s/iter; left time: 8677.4072s\n",
      "4999it [09:44,  8.74it/s]\titers: 5000, epoch: 8 | loss: 0.7916043\n",
      "\tspeed: 0.1147s/iter; left time: 8629.8460s\n",
      "5099it [09:56,  8.77it/s]\titers: 5100, epoch: 8 | loss: 0.9247194\n",
      "\tspeed: 0.1202s/iter; left time: 9033.8533s\n",
      "5199it [10:08,  8.02it/s]\titers: 5200, epoch: 8 | loss: 0.8223880\n",
      "\tspeed: 0.1188s/iter; left time: 8913.8278s\n",
      "5299it [10:20,  8.31it/s]\titers: 5300, epoch: 8 | loss: 0.7026347\n",
      "\tspeed: 0.1230s/iter; left time: 9222.4800s\n",
      "5399it [10:32,  7.48it/s]\titers: 5400, epoch: 8 | loss: 0.5749762\n",
      "\tspeed: 0.1201s/iter; left time: 8987.8287s\n",
      "5499it [10:44,  7.54it/s]\titers: 5500, epoch: 8 | loss: 0.5935693\n",
      "\tspeed: 0.1216s/iter; left time: 9086.2109s\n",
      "5599it [10:56,  7.98it/s]\titers: 5600, epoch: 8 | loss: 0.7175584\n",
      "\tspeed: 0.1205s/iter; left time: 8993.8011s\n",
      "5699it [11:08,  7.70it/s]\titers: 5700, epoch: 8 | loss: 0.6041333\n",
      "\tspeed: 0.1198s/iter; left time: 8929.8698s\n",
      "5799it [11:20,  8.23it/s]\titers: 5800, epoch: 8 | loss: 0.8208687\n",
      "\tspeed: 0.1205s/iter; left time: 8972.2419s\n",
      "5899it [11:32,  8.00it/s]\titers: 5900, epoch: 8 | loss: 0.8614454\n",
      "\tspeed: 0.1188s/iter; left time: 8833.3970s\n",
      "5999it [11:44,  8.02it/s]\titers: 6000, epoch: 8 | loss: 0.5385859\n",
      "\tspeed: 0.1193s/iter; left time: 8859.1787s\n",
      "6099it [11:56,  8.46it/s]\titers: 6100, epoch: 8 | loss: 0.7683805\n",
      "\tspeed: 0.1218s/iter; left time: 9028.5506s\n",
      "6173it [12:05,  8.51it/s]\n",
      "Epoch: 8 cost time: 725.7676520347595\n",
      "1335it [01:39, 13.44it/s]\n",
      "1330it [01:36, 13.81it/s]\n",
      "Epoch: 8 | Train Loss: 0.6366165 Vali Loss: 0.7080781 Test Loss: 1.0121857 MAE Loss: 0.7508879\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 124.61139531532923 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.05 \n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vali loss gets higher\n",
    "# batch size = 24\n",
    "# Learning rate = 0.002/0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-15 23:03:16,214] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 23:03:17,029] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 23:03:17,029] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 23:03:17,029] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-15 23:03:17,848] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-15 23:03:17,848] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 23:03:19,090] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 23:03:19,091] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 23:03:19,091] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 23:03:19,092] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-15 23:03:19,092] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-15 23:03:19,092] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-15 23:03:19,092] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-15 23:03:19,092] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-15 23:03:19,092] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-15 23:03:19,092] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-15 23:03:19,392] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 23:03:19,393] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-15 23:03:19,393] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 198.74 GB, percent = 26.3%\n",
      "[2024-05-15 23:03:19,503] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 23:03:19,504] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 23:03:19,504] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 198.75 GB, percent = 26.3%\n",
      "[2024-05-15 23:03:19,504] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-15 23:03:19,605] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 23:03:19,606] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-15 23:03:19,606] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 198.74 GB, percent = 26.3%\n",
      "[2024-05-15 23:03:19,606] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-15 23:03:19,606] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 23:03:19,606] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 23:03:19,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[7.999999999999999e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1b59c79c90>\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 23:03:19,607] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 23:03:19,608] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-15 23:03:19,609] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.002\n",
      "lr 7.999999999999999e-05\n",
      "99it [00:13,  7.54it/s]\titers: 100, epoch: 1 | loss: 1.0266944\n",
      "\tspeed: 0.1725s/iter; left time: 21282.2395s\n",
      "199it [00:25,  8.65it/s]\titers: 200, epoch: 1 | loss: 1.1787531\n",
      "\tspeed: 0.1214s/iter; left time: 14965.5036s\n",
      "299it [00:37,  7.84it/s]\titers: 300, epoch: 1 | loss: 0.6541107\n",
      "\tspeed: 0.1191s/iter; left time: 14673.5939s\n",
      "399it [00:49,  8.96it/s]\titers: 400, epoch: 1 | loss: 0.7754841\n",
      "\tspeed: 0.1204s/iter; left time: 14820.2538s\n",
      "499it [01:01,  7.84it/s]\titers: 500, epoch: 1 | loss: 0.5497072\n",
      "\tspeed: 0.1231s/iter; left time: 15140.9648s\n",
      "599it [01:13,  8.11it/s]\titers: 600, epoch: 1 | loss: 0.6162207\n",
      "\tspeed: 0.1210s/iter; left time: 14864.2458s\n",
      "699it [01:26,  8.12it/s]\titers: 700, epoch: 1 | loss: 0.6120855\n",
      "\tspeed: 0.1227s/iter; left time: 15059.8569s\n",
      "799it [01:38,  8.39it/s]\titers: 800, epoch: 1 | loss: 0.6503856\n",
      "\tspeed: 0.1206s/iter; left time: 14792.1331s\n",
      "899it [01:50,  7.41it/s]\titers: 900, epoch: 1 | loss: 0.5631393\n",
      "\tspeed: 0.1223s/iter; left time: 14987.0120s\n",
      "999it [02:02,  8.03it/s]\titers: 1000, epoch: 1 | loss: 0.5102765\n",
      "\tspeed: 0.1224s/iter; left time: 14986.7584s\n",
      "1099it [02:14,  8.74it/s]\titers: 1100, epoch: 1 | loss: 0.5777243\n",
      "\tspeed: 0.1212s/iter; left time: 14834.2545s\n",
      "1199it [02:27,  7.96it/s]\titers: 1200, epoch: 1 | loss: 0.5423755\n",
      "\tspeed: 0.1240s/iter; left time: 15160.6233s\n",
      "1299it [02:39,  7.58it/s]\titers: 1300, epoch: 1 | loss: 0.5088878\n",
      "\tspeed: 0.1243s/iter; left time: 15189.6441s\n",
      "1399it [02:51,  7.93it/s]\titers: 1400, epoch: 1 | loss: 0.7063697\n",
      "\tspeed: 0.1227s/iter; left time: 14972.3326s\n",
      "1499it [03:04,  7.92it/s]\titers: 1500, epoch: 1 | loss: 0.5774297\n",
      "\tspeed: 0.1249s/iter; left time: 15230.0444s\n",
      "1599it [03:16,  8.48it/s]\titers: 1600, epoch: 1 | loss: 0.5108793\n",
      "\tspeed: 0.1234s/iter; left time: 15042.4171s\n",
      "1699it [03:28,  7.90it/s]\titers: 1700, epoch: 1 | loss: 0.5561679\n",
      "\tspeed: 0.1192s/iter; left time: 14512.7048s\n",
      "1799it [03:40,  8.27it/s]\titers: 1800, epoch: 1 | loss: 0.6498486\n",
      "\tspeed: 0.1196s/iter; left time: 14555.5409s\n",
      "1899it [03:52,  8.23it/s]\titers: 1900, epoch: 1 | loss: 0.4388873\n",
      "\tspeed: 0.1195s/iter; left time: 14528.2765s\n",
      "1999it [04:04,  8.40it/s]\titers: 2000, epoch: 1 | loss: 0.4022646\n",
      "\tspeed: 0.1235s/iter; left time: 15005.7600s\n",
      "2099it [04:17,  8.46it/s]\titers: 2100, epoch: 1 | loss: 0.5129789\n",
      "\tspeed: 0.1227s/iter; left time: 14886.9733s\n",
      "2199it [04:29,  8.28it/s]\titers: 2200, epoch: 1 | loss: 0.5552040\n",
      "\tspeed: 0.1230s/iter; left time: 14916.8723s\n",
      "2299it [04:41,  8.48it/s]\titers: 2300, epoch: 1 | loss: 0.5150111\n",
      "\tspeed: 0.1189s/iter; left time: 14407.9715s\n",
      "2399it [04:53,  7.98it/s]\titers: 2400, epoch: 1 | loss: 0.5488299\n",
      "\tspeed: 0.1239s/iter; left time: 15004.9928s\n",
      "2499it [05:06,  7.67it/s]\titers: 2500, epoch: 1 | loss: 0.6406432\n",
      "\tspeed: 0.1248s/iter; left time: 15092.5335s\n",
      "2599it [05:18,  8.52it/s]\titers: 2600, epoch: 1 | loss: 0.4799518\n",
      "\tspeed: 0.1227s/iter; left time: 14833.3276s\n",
      "2699it [05:30,  7.83it/s]\titers: 2700, epoch: 1 | loss: 0.4769745\n",
      "\tspeed: 0.1231s/iter; left time: 14871.6827s\n",
      "2799it [05:43,  7.77it/s]\titers: 2800, epoch: 1 | loss: 0.4795257\n",
      "\tspeed: 0.1231s/iter; left time: 14858.6209s\n",
      "2899it [05:55,  7.68it/s]\titers: 2900, epoch: 1 | loss: 0.4219877\n",
      "\tspeed: 0.1225s/iter; left time: 14766.9090s\n",
      "2999it [06:07,  8.86it/s]\titers: 3000, epoch: 1 | loss: 0.5999079\n",
      "\tspeed: 0.1232s/iter; left time: 14838.3350s\n",
      "3099it [06:19,  7.79it/s]\titers: 3100, epoch: 1 | loss: 0.5561753\n",
      "\tspeed: 0.1234s/iter; left time: 14848.0681s\n",
      "3199it [06:32,  7.33it/s]\titers: 3200, epoch: 1 | loss: 0.5943993\n",
      "\tspeed: 0.1229s/iter; left time: 14783.4040s\n",
      "3299it [06:44,  6.51it/s]\titers: 3300, epoch: 1 | loss: 0.6083695\n",
      "\tspeed: 0.1264s/iter; left time: 15182.7059s\n",
      "3399it [06:57,  8.93it/s]\titers: 3400, epoch: 1 | loss: 0.5300915\n",
      "\tspeed: 0.1236s/iter; left time: 14836.9675s\n",
      "3499it [07:09,  8.04it/s]\titers: 3500, epoch: 1 | loss: 0.4918887\n",
      "\tspeed: 0.1244s/iter; left time: 14924.7739s\n",
      "3599it [07:21,  8.34it/s]\titers: 3600, epoch: 1 | loss: 0.6058281\n",
      "\tspeed: 0.1216s/iter; left time: 14575.2254s\n",
      "3699it [07:34,  7.71it/s]\titers: 3700, epoch: 1 | loss: 0.5142671\n",
      "\tspeed: 0.1260s/iter; left time: 15089.7080s\n",
      "3799it [07:46,  8.70it/s]\titers: 3800, epoch: 1 | loss: 0.4690748\n",
      "\tspeed: 0.1222s/iter; left time: 14620.1179s\n",
      "3899it [07:58,  7.67it/s]\titers: 3900, epoch: 1 | loss: 0.4713155\n",
      "\tspeed: 0.1178s/iter; left time: 14079.5142s\n",
      "3999it [08:10,  9.11it/s]\titers: 4000, epoch: 1 | loss: 0.5304077\n",
      "\tspeed: 0.1193s/iter; left time: 14245.7738s\n",
      "4099it [08:22,  7.94it/s]\titers: 4100, epoch: 1 | loss: 0.5936074\n",
      "\tspeed: 0.1261s/iter; left time: 15046.1635s\n",
      "4199it [08:35,  8.00it/s]\titers: 4200, epoch: 1 | loss: 0.5491627\n",
      "\tspeed: 0.1248s/iter; left time: 14882.3568s\n",
      "4299it [08:47,  7.97it/s]\titers: 4300, epoch: 1 | loss: 0.5301222\n",
      "\tspeed: 0.1252s/iter; left time: 14922.5900s\n",
      "4399it [09:00,  8.12it/s]\titers: 4400, epoch: 1 | loss: 0.6817334\n",
      "\tspeed: 0.1231s/iter; left time: 14654.7692s\n",
      "4499it [09:12,  8.90it/s]\titers: 4500, epoch: 1 | loss: 0.3741227\n",
      "\tspeed: 0.1227s/iter; left time: 14594.2054s\n",
      "4599it [09:24,  9.39it/s]\titers: 4600, epoch: 1 | loss: 0.5335739\n",
      "\tspeed: 0.1206s/iter; left time: 14332.5154s\n",
      "4699it [09:37,  7.83it/s]\titers: 4700, epoch: 1 | loss: 0.6033793\n",
      "\tspeed: 0.1256s/iter; left time: 14917.1531s\n",
      "4799it [09:50,  7.46it/s]\titers: 4800, epoch: 1 | loss: 0.5966699\n",
      "\tspeed: 0.1318s/iter; left time: 15640.3256s\n",
      "4899it [10:03,  7.39it/s]\titers: 4900, epoch: 1 | loss: 0.6239974\n",
      "\tspeed: 0.1308s/iter; left time: 15503.8954s\n",
      "4999it [10:16,  7.65it/s]\titers: 5000, epoch: 1 | loss: 0.5212106\n",
      "\tspeed: 0.1294s/iter; left time: 15326.4887s\n",
      "5099it [10:29,  7.30it/s]\titers: 5100, epoch: 1 | loss: 0.6719149\n",
      "\tspeed: 0.1306s/iter; left time: 15452.1522s\n",
      "5199it [10:42,  8.23it/s]\titers: 5200, epoch: 1 | loss: 0.6288074\n",
      "\tspeed: 0.1273s/iter; left time: 15050.3661s\n",
      "5299it [10:54,  8.22it/s]\titers: 5300, epoch: 1 | loss: 0.4012827\n",
      "\tspeed: 0.1278s/iter; left time: 15104.9328s\n",
      "5399it [11:07,  7.50it/s]\titers: 5400, epoch: 1 | loss: 0.4740062\n",
      "\tspeed: 0.1301s/iter; left time: 15365.4493s\n",
      "5499it [11:20,  8.41it/s]\titers: 5500, epoch: 1 | loss: 0.4865403\n",
      "\tspeed: 0.1303s/iter; left time: 15367.5596s\n",
      "5599it [11:33,  8.17it/s]\titers: 5600, epoch: 1 | loss: 0.5316462\n",
      "\tspeed: 0.1243s/iter; left time: 14646.2049s\n",
      "5699it [11:45,  7.55it/s]\titers: 5700, epoch: 1 | loss: 0.4645341\n",
      "\tspeed: 0.1261s/iter; left time: 14854.4712s\n",
      "5799it [11:58,  8.36it/s]\titers: 5800, epoch: 1 | loss: 0.5285370\n",
      "\tspeed: 0.1295s/iter; left time: 15240.7363s\n",
      "5899it [12:11,  8.26it/s]\titers: 5900, epoch: 1 | loss: 0.5579756\n",
      "\tspeed: 0.1271s/iter; left time: 14945.1606s\n",
      "5999it [12:24,  7.85it/s]\titers: 6000, epoch: 1 | loss: 0.6698885\n",
      "\tspeed: 0.1250s/iter; left time: 14681.7622s\n",
      "6099it [12:36,  7.84it/s]\titers: 6100, epoch: 1 | loss: 0.7304694\n",
      "\tspeed: 0.1266s/iter; left time: 14858.7072s\n",
      "6173it [12:45,  8.06it/s]\n",
      "Epoch: 1 cost time: 765.6111912727356\n",
      "1335it [01:36, 13.81it/s]\n",
      "1330it [01:34, 14.04it/s]\n",
      "Epoch: 1 | Train Loss: 0.5943676 Vali Loss: 0.6698140 Test Loss: 0.9437051 MAE Loss: 0.7097798\n",
      "lr = 0.0000800000\n",
      "learning_rate 0.002\n",
      "lr 7.999999999999999e-05\n",
      "99it [00:12,  8.18it/s]\titers: 100, epoch: 2 | loss: 0.5848415\n",
      "\tspeed: 2.1442s/iter; left time: 251273.3186s\n",
      "199it [00:24,  8.62it/s]\titers: 200, epoch: 2 | loss: 0.5395895\n",
      "\tspeed: 0.1181s/iter; left time: 13831.1741s\n",
      "299it [00:36,  7.97it/s]\titers: 300, epoch: 2 | loss: 0.5793770\n",
      "\tspeed: 0.1205s/iter; left time: 14099.5748s\n",
      "399it [00:48,  8.87it/s]\titers: 400, epoch: 2 | loss: 0.5222319\n",
      "\tspeed: 0.1210s/iter; left time: 14147.5370s\n",
      "499it [00:59,  8.07it/s]\titers: 500, epoch: 2 | loss: 0.5108232\n",
      "\tspeed: 0.1167s/iter; left time: 13630.9802s\n",
      "599it [01:12,  8.85it/s]\titers: 600, epoch: 2 | loss: 0.4518220\n",
      "\tspeed: 0.1206s/iter; left time: 14069.3163s\n",
      "699it [01:23,  8.42it/s]\titers: 700, epoch: 2 | loss: 0.5533144\n",
      "\tspeed: 0.1176s/iter; left time: 13706.4542s\n",
      "799it [01:35,  8.65it/s]\titers: 800, epoch: 2 | loss: 0.4590554\n",
      "\tspeed: 0.1170s/iter; left time: 13629.5735s\n",
      "899it [01:47,  8.48it/s]\titers: 900, epoch: 2 | loss: 0.4531519\n",
      "\tspeed: 0.1191s/iter; left time: 13866.0457s\n",
      "999it [01:59,  8.13it/s]\titers: 1000, epoch: 2 | loss: 0.5654954\n",
      "\tspeed: 0.1181s/iter; left time: 13728.2102s\n",
      "1099it [02:11,  8.37it/s]\titers: 1100, epoch: 2 | loss: 0.6553186\n",
      "\tspeed: 0.1188s/iter; left time: 13807.2028s\n",
      "1199it [02:22,  8.15it/s]\titers: 1200, epoch: 2 | loss: 0.5295070\n",
      "\tspeed: 0.1167s/iter; left time: 13550.3650s\n",
      "1299it [02:34,  8.38it/s]\titers: 1300, epoch: 2 | loss: 0.4808678\n",
      "\tspeed: 0.1189s/iter; left time: 13792.2222s\n",
      "1399it [02:46,  8.13it/s]\titers: 1400, epoch: 2 | loss: 0.5538328\n",
      "\tspeed: 0.1210s/iter; left time: 14026.9626s\n",
      "1499it [02:58,  8.45it/s]\titers: 1500, epoch: 2 | loss: 0.4917735\n",
      "\tspeed: 0.1193s/iter; left time: 13815.6954s\n",
      "1599it [03:10,  8.82it/s]\titers: 1600, epoch: 2 | loss: 0.4466498\n",
      "\tspeed: 0.1179s/iter; left time: 13640.0057s\n",
      "1699it [03:22,  8.72it/s]\titers: 1700, epoch: 2 | loss: 0.4420082\n",
      "\tspeed: 0.1216s/iter; left time: 14051.3658s\n",
      "1799it [03:34,  8.31it/s]\titers: 1800, epoch: 2 | loss: 0.5000637\n",
      "\tspeed: 0.1213s/iter; left time: 14009.6514s\n",
      "1899it [03:46,  8.57it/s]\titers: 1900, epoch: 2 | loss: 0.4137741\n",
      "\tspeed: 0.1184s/iter; left time: 13662.2541s\n",
      "1999it [03:58,  9.11it/s]\titers: 2000, epoch: 2 | loss: 0.5348157\n",
      "\tspeed: 0.1170s/iter; left time: 13489.2598s\n",
      "2099it [04:10,  8.25it/s]\titers: 2100, epoch: 2 | loss: 0.4229688\n",
      "\tspeed: 0.1186s/iter; left time: 13665.4249s\n",
      "2199it [04:22,  8.77it/s]\titers: 2200, epoch: 2 | loss: 0.5184886\n",
      "\tspeed: 0.1188s/iter; left time: 13667.6826s\n",
      "2299it [04:33,  9.04it/s]\titers: 2300, epoch: 2 | loss: 0.5119118\n",
      "\tspeed: 0.1192s/iter; left time: 13707.6213s\n",
      "2399it [04:45,  9.11it/s]\titers: 2400, epoch: 2 | loss: 0.8305491\n",
      "\tspeed: 0.1152s/iter; left time: 13232.4088s\n",
      "2499it [04:57,  8.18it/s]\titers: 2500, epoch: 2 | loss: 0.4674868\n",
      "\tspeed: 0.1178s/iter; left time: 13518.1566s\n",
      "2599it [05:09,  7.84it/s]\titers: 2600, epoch: 2 | loss: 0.4943771\n",
      "\tspeed: 0.1180s/iter; left time: 13538.7954s\n",
      "2699it [05:21,  8.32it/s]\titers: 2700, epoch: 2 | loss: 0.3865930\n",
      "\tspeed: 0.1196s/iter; left time: 13700.2889s\n",
      "2799it [05:32,  8.29it/s]\titers: 2800, epoch: 2 | loss: 0.4946281\n",
      "\tspeed: 0.1172s/iter; left time: 13414.1402s\n",
      "2899it [05:44,  8.71it/s]\titers: 2900, epoch: 2 | loss: 0.4908765\n",
      "\tspeed: 0.1201s/iter; left time: 13739.1194s\n",
      "2999it [05:56,  8.28it/s]\titers: 3000, epoch: 2 | loss: 0.5749246\n",
      "\tspeed: 0.1187s/iter; left time: 13563.9249s\n",
      "3099it [06:08,  8.16it/s]\titers: 3100, epoch: 2 | loss: 0.4246601\n",
      "\tspeed: 0.1221s/iter; left time: 13941.8545s\n",
      "3199it [06:20,  8.28it/s]\titers: 3200, epoch: 2 | loss: 0.7097248\n",
      "\tspeed: 0.1213s/iter; left time: 13839.9645s\n",
      "3299it [06:32,  8.69it/s]\titers: 3300, epoch: 2 | loss: 0.6144112\n",
      "\tspeed: 0.1157s/iter; left time: 13188.7273s\n",
      "3399it [06:43,  8.41it/s]\titers: 3400, epoch: 2 | loss: 0.3641871\n",
      "\tspeed: 0.1142s/iter; left time: 13010.1823s\n",
      "3499it [06:55,  8.22it/s]\titers: 3500, epoch: 2 | loss: 0.5624763\n",
      "\tspeed: 0.1172s/iter; left time: 13341.4310s\n",
      "3599it [07:07,  8.11it/s]\titers: 3600, epoch: 2 | loss: 0.6066816\n",
      "\tspeed: 0.1165s/iter; left time: 13250.0472s\n",
      "3699it [07:18,  8.83it/s]\titers: 3700, epoch: 2 | loss: 0.6926651\n",
      "\tspeed: 0.1160s/iter; left time: 13174.9058s\n",
      "3798it [07:30,  8.79it/s]\titers: 3800, epoch: 2 | loss: 0.5954986\n",
      "\tspeed: 0.1164s/iter; left time: 13206.1303s\n",
      "3899it [07:41,  8.57it/s]\titers: 3900, epoch: 2 | loss: 0.5658429\n",
      "\tspeed: 0.1140s/iter; left time: 12921.4555s\n",
      "3999it [07:53,  8.45it/s]\titers: 4000, epoch: 2 | loss: 0.5016938\n",
      "\tspeed: 0.1144s/iter; left time: 12960.3326s\n",
      "4099it [08:04,  8.96it/s]\titers: 4100, epoch: 2 | loss: 0.4394307\n",
      "\tspeed: 0.1148s/iter; left time: 12990.0762s\n",
      "4199it [08:16,  8.63it/s]\titers: 4200, epoch: 2 | loss: 0.5921333\n",
      "\tspeed: 0.1143s/iter; left time: 12923.1326s\n",
      "4298it [08:27,  9.10it/s]\titers: 4300, epoch: 2 | loss: 0.4344743\n",
      "\tspeed: 0.1138s/iter; left time: 12856.9802s\n",
      "4399it [08:39,  8.56it/s]\titers: 4400, epoch: 2 | loss: 0.5898346\n",
      "\tspeed: 0.1160s/iter; left time: 13098.0996s\n",
      "4499it [08:50,  8.83it/s]\titers: 4500, epoch: 2 | loss: 0.5311592\n",
      "\tspeed: 0.1170s/iter; left time: 13191.6464s\n",
      "4599it [09:02,  7.78it/s]\titers: 4600, epoch: 2 | loss: 0.3751690\n",
      "\tspeed: 0.1160s/iter; left time: 13071.3834s\n",
      "4699it [09:14,  8.13it/s]\titers: 4700, epoch: 2 | loss: 0.6675850\n",
      "\tspeed: 0.1183s/iter; left time: 13324.5359s\n",
      "4799it [09:25,  9.11it/s]\titers: 4800, epoch: 2 | loss: 0.5021920\n",
      "\tspeed: 0.1145s/iter; left time: 12879.8436s\n",
      "4899it [09:37,  8.52it/s]\titers: 4900, epoch: 2 | loss: 0.6733995\n",
      "\tspeed: 0.1162s/iter; left time: 13057.0867s\n",
      "4999it [09:48,  8.54it/s]\titers: 5000, epoch: 2 | loss: 0.6473868\n",
      "\tspeed: 0.1147s/iter; left time: 12873.9589s\n",
      "5099it [10:00,  9.22it/s]\titers: 5100, epoch: 2 | loss: 0.4544492\n",
      "\tspeed: 0.1164s/iter; left time: 13059.1728s\n",
      "5199it [10:12,  8.50it/s]\titers: 5200, epoch: 2 | loss: 0.6059759\n",
      "\tspeed: 0.1162s/iter; left time: 13020.7366s\n",
      "5299it [10:23,  8.65it/s]\titers: 5300, epoch: 2 | loss: 0.5618952\n",
      "\tspeed: 0.1152s/iter; left time: 12900.2633s\n",
      "5399it [10:35,  9.23it/s]\titers: 5400, epoch: 2 | loss: 0.3833562\n",
      "\tspeed: 0.1167s/iter; left time: 13052.5490s\n",
      "5499it [10:47,  8.63it/s]\titers: 5500, epoch: 2 | loss: 0.6346738\n",
      "\tspeed: 0.1169s/iter; left time: 13068.5791s\n",
      "5599it [10:58,  9.24it/s]\titers: 5600, epoch: 2 | loss: 0.4674012\n",
      "\tspeed: 0.1146s/iter; left time: 12804.5980s\n",
      "5699it [11:09,  8.47it/s]\titers: 5700, epoch: 2 | loss: 0.4766916\n",
      "\tspeed: 0.1125s/iter; left time: 12548.6816s\n",
      "5799it [11:21,  9.14it/s]\titers: 5800, epoch: 2 | loss: 0.6608716\n",
      "\tspeed: 0.1146s/iter; left time: 12777.1899s\n",
      "5899it [11:32,  8.60it/s]\titers: 5900, epoch: 2 | loss: 0.4937541\n",
      "\tspeed: 0.1172s/iter; left time: 13051.9632s\n",
      "5999it [11:44,  8.64it/s]\titers: 6000, epoch: 2 | loss: 0.4677428\n",
      "\tspeed: 0.1172s/iter; left time: 13039.8731s\n",
      "6099it [11:56,  8.56it/s]\titers: 6100, epoch: 2 | loss: 0.6452962\n",
      "\tspeed: 0.1152s/iter; left time: 12810.6772s\n",
      "6173it [12:04,  8.52it/s]\n",
      "Epoch: 2 cost time: 724.7873232364655\n",
      "1335it [01:27, 15.29it/s]\n",
      "1330it [01:28, 15.09it/s]\n",
      "Epoch: 2 | Train Loss: 0.5416503 Vali Loss: 0.6802900 Test Loss: 0.9595950 MAE Loss: 0.7127434\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000800000\n",
      "learning_rate 0.002\n",
      "lr 7.999999999999999e-05\n",
      "99it [00:11,  8.17it/s]\titers: 100, epoch: 3 | loss: 0.3727075\n",
      "\tspeed: 1.9637s/iter; left time: 218005.3820s\n",
      "199it [00:24,  8.24it/s]\titers: 200, epoch: 3 | loss: 0.5138756\n",
      "\tspeed: 0.1230s/iter; left time: 13637.8490s\n",
      "299it [00:36,  8.04it/s]\titers: 300, epoch: 3 | loss: 0.6753291\n",
      "\tspeed: 0.1203s/iter; left time: 13329.3451s\n",
      "399it [00:48,  8.72it/s]\titers: 400, epoch: 3 | loss: 0.5157626\n",
      "\tspeed: 0.1226s/iter; left time: 13572.2357s\n",
      "499it [01:00,  8.39it/s]\titers: 500, epoch: 3 | loss: 0.4559565\n",
      "\tspeed: 0.1236s/iter; left time: 13668.1279s\n",
      "599it [01:13,  8.05it/s]\titers: 600, epoch: 3 | loss: 0.4783113\n",
      "\tspeed: 0.1212s/iter; left time: 13390.7340s\n",
      "699it [01:24,  8.47it/s]\titers: 700, epoch: 3 | loss: 0.6225614\n",
      "\tspeed: 0.1184s/iter; left time: 13074.8365s\n",
      "799it [01:37,  7.76it/s]\titers: 800, epoch: 3 | loss: 0.4969903\n",
      "\tspeed: 0.1243s/iter; left time: 13707.6242s\n",
      "899it [01:49,  7.97it/s]\titers: 900, epoch: 3 | loss: 0.6623712\n",
      "\tspeed: 0.1206s/iter; left time: 13287.3982s\n",
      "999it [02:01,  9.00it/s]\titers: 1000, epoch: 3 | loss: 0.6209142\n",
      "\tspeed: 0.1183s/iter; left time: 13032.1032s\n",
      "1099it [02:13,  8.94it/s]\titers: 1100, epoch: 3 | loss: 0.5486390\n",
      "\tspeed: 0.1198s/iter; left time: 13180.9265s\n",
      "1199it [02:25,  8.74it/s]\titers: 1200, epoch: 3 | loss: 0.4305839\n",
      "\tspeed: 0.1198s/iter; left time: 13169.3430s\n",
      "1299it [02:37,  9.34it/s]\titers: 1300, epoch: 3 | loss: 0.5632101\n",
      "\tspeed: 0.1196s/iter; left time: 13135.4950s\n",
      "1399it [02:48,  8.88it/s]\titers: 1400, epoch: 3 | loss: 0.4584416\n",
      "\tspeed: 0.1131s/iter; left time: 12410.9090s\n",
      "1499it [03:00,  8.63it/s]\titers: 1500, epoch: 3 | loss: 0.4240536\n",
      "\tspeed: 0.1170s/iter; left time: 12820.2880s\n",
      "1598it [03:11,  8.98it/s]\titers: 1600, epoch: 3 | loss: 0.5340726\n",
      "\tspeed: 0.1182s/iter; left time: 12949.3496s\n",
      "1699it [03:23,  8.46it/s]\titers: 1700, epoch: 3 | loss: 0.4416560\n",
      "\tspeed: 0.1187s/iter; left time: 12992.1720s\n",
      "1799it [03:35,  8.65it/s]\titers: 1800, epoch: 3 | loss: 0.5959721\n",
      "\tspeed: 0.1160s/iter; left time: 12676.2828s\n",
      "1899it [03:47,  8.53it/s]\titers: 1900, epoch: 3 | loss: 0.5505739\n",
      "\tspeed: 0.1167s/iter; left time: 12749.3441s\n",
      "1999it [03:58,  7.87it/s]\titers: 2000, epoch: 3 | loss: 0.6192906\n",
      "\tspeed: 0.1176s/iter; left time: 12828.3969s\n",
      "2099it [04:10,  8.32it/s]\titers: 2100, epoch: 3 | loss: 0.5195792\n",
      "\tspeed: 0.1193s/iter; left time: 13009.5226s\n",
      "2199it [04:22,  8.65it/s]\titers: 2200, epoch: 3 | loss: 0.6733450\n",
      "\tspeed: 0.1211s/iter; left time: 13185.9647s\n",
      "2299it [04:35,  7.96it/s]\titers: 2300, epoch: 3 | loss: 0.5765169\n",
      "\tspeed: 0.1216s/iter; left time: 13232.3119s\n",
      "2399it [04:47,  8.00it/s]\titers: 2400, epoch: 3 | loss: 0.4109266\n",
      "\tspeed: 0.1204s/iter; left time: 13093.2924s\n",
      "2499it [04:58,  8.39it/s]\titers: 2500, epoch: 3 | loss: 0.4764485\n",
      "\tspeed: 0.1183s/iter; left time: 12845.1740s\n",
      "2599it [05:10,  8.96it/s]\titers: 2600, epoch: 3 | loss: 0.5400250\n",
      "\tspeed: 0.1151s/iter; left time: 12491.5062s\n",
      "2699it [05:22,  8.00it/s]\titers: 2700, epoch: 3 | loss: 0.7036774\n",
      "\tspeed: 0.1203s/iter; left time: 13037.9179s\n",
      "2799it [05:34,  8.99it/s]\titers: 2800, epoch: 3 | loss: 0.5650422\n",
      "\tspeed: 0.1180s/iter; left time: 12782.3098s\n",
      "2899it [05:46,  8.26it/s]\titers: 2900, epoch: 3 | loss: 0.6261309\n",
      "\tspeed: 0.1178s/iter; left time: 12752.0171s\n",
      "2999it [05:57,  8.78it/s]\titers: 3000, epoch: 3 | loss: 0.5598403\n",
      "\tspeed: 0.1185s/iter; left time: 12812.2548s\n",
      "3099it [06:09,  8.34it/s]\titers: 3100, epoch: 3 | loss: 0.4940923\n",
      "\tspeed: 0.1183s/iter; left time: 12778.9290s\n",
      "3199it [06:21,  8.18it/s]\titers: 3200, epoch: 3 | loss: 0.8470762\n",
      "\tspeed: 0.1182s/iter; left time: 12758.5748s\n",
      "3299it [06:33,  8.63it/s]\titers: 3300, epoch: 3 | loss: 0.5579589\n",
      "\tspeed: 0.1179s/iter; left time: 12711.3777s\n",
      "3399it [06:45,  8.44it/s]\titers: 3400, epoch: 3 | loss: 0.4750336\n",
      "\tspeed: 0.1192s/iter; left time: 12838.9252s\n",
      "3499it [06:57,  8.14it/s]\titers: 3500, epoch: 3 | loss: 0.4931792\n",
      "\tspeed: 0.1214s/iter; left time: 13069.0684s\n",
      "3599it [07:09,  8.50it/s]\titers: 3600, epoch: 3 | loss: 0.6214109\n",
      "\tspeed: 0.1171s/iter; left time: 12593.9485s\n",
      "3699it [07:20,  8.40it/s]\titers: 3700, epoch: 3 | loss: 0.5574158\n",
      "\tspeed: 0.1174s/iter; left time: 12615.2908s\n",
      "3799it [07:32,  8.73it/s]\titers: 3800, epoch: 3 | loss: 0.5612543\n",
      "\tspeed: 0.1178s/iter; left time: 12637.1450s\n",
      "3899it [07:44,  7.75it/s]\titers: 3900, epoch: 3 | loss: 0.5588531\n",
      "\tspeed: 0.1207s/iter; left time: 12937.5593s\n",
      "3999it [07:57,  8.07it/s]\titers: 4000, epoch: 3 | loss: 0.5408795\n",
      "\tspeed: 0.1261s/iter; left time: 13503.9432s\n",
      "4099it [08:09,  8.59it/s]\titers: 4100, epoch: 3 | loss: 0.4273799\n",
      "\tspeed: 0.1216s/iter; left time: 13009.2357s\n",
      "4199it [08:22,  7.72it/s]\titers: 4200, epoch: 3 | loss: 0.4606862\n",
      "\tspeed: 0.1259s/iter; left time: 13461.9665s\n",
      "4299it [08:34,  8.47it/s]\titers: 4300, epoch: 3 | loss: 0.5404802\n",
      "\tspeed: 0.1224s/iter; left time: 13072.7991s\n",
      "4399it [08:46,  8.48it/s]\titers: 4400, epoch: 3 | loss: 0.4464156\n",
      "\tspeed: 0.1221s/iter; left time: 13028.2002s\n",
      "4498it [08:55, 12.89it/s]\titers: 4500, epoch: 3 | loss: 0.3954462\n",
      "\tspeed: 0.0900s/iter; left time: 9598.1774s\n",
      "4599it [09:07,  8.04it/s]\titers: 4600, epoch: 3 | loss: 0.5740504\n",
      "\tspeed: 0.1222s/iter; left time: 13012.7766s\n",
      "4699it [09:19,  7.99it/s]\titers: 4700, epoch: 3 | loss: 0.5850620\n",
      "\tspeed: 0.1217s/iter; left time: 12952.1208s\n",
      "4799it [09:31,  8.18it/s]\titers: 4800, epoch: 3 | loss: 0.3718603\n",
      "\tspeed: 0.1174s/iter; left time: 12482.8643s\n",
      "4899it [09:43,  7.97it/s]\titers: 4900, epoch: 3 | loss: 0.5623569\n",
      "\tspeed: 0.1204s/iter; left time: 12788.5805s\n",
      "4999it [09:55,  8.47it/s]\titers: 5000, epoch: 3 | loss: 0.5271859\n",
      "\tspeed: 0.1200s/iter; left time: 12733.3309s\n",
      "5099it [10:07,  8.76it/s]\titers: 5100, epoch: 3 | loss: 0.3316664\n",
      "\tspeed: 0.1220s/iter; left time: 12937.5150s\n",
      "5199it [10:20,  8.27it/s]\titers: 5200, epoch: 3 | loss: 0.5737737\n",
      "\tspeed: 0.1233s/iter; left time: 13062.0272s\n",
      "5296it [10:31,  8.02it/s]^C\n",
      "5296it [10:31,  8.38it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 259, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 2002, in backward\n",
      "    self.allreduce_gradients()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1918, in allreduce_gradients\n",
      "    self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 859, in overlapping_partition_gradients_reduce_epilogue\n",
      "    self.independent_gradient_partition_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 760, in independent_gradient_partition_epilogue\n",
      "    get_accelerator().synchronize()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/accelerator/cuda_accelerator.py\", line 77, in synchronize\n",
      "    return torch.cuda.synchronize(device_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 801, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Total time: 41.78453181584676 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.002 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 32 # 32\n",
    "d_ff=128 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 00:04:55,580] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 00:04:56,381] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 00:04:56,381] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 00:04:56,381] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 00:04:57,230] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-16 00:04:57,231] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 00:04:58,287] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 00:04:58,289] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 00:04:58,289] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 00:04:58,290] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 00:04:58,290] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 00:04:58,290] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 00:04:58,290] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 00:04:58,290] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 00:04:58,290] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 00:04:58,290] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 00:04:58,596] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 00:04:58,597] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:04:58,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.87 GB, percent = 27.2%\n",
      "[2024-05-16 00:04:58,860] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 00:04:58,860] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:04:58,861] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.84 GB, percent = 27.1%\n",
      "[2024-05-16 00:04:58,861] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 00:04:58,978] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 00:04:58,979] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:04:58,979] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.84 GB, percent = 27.1%\n",
      "[2024-05-16 00:04:58,979] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 00:04:58,980] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 00:04:58,980] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 00:04:58,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 00:04:58,980] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 00:04:58,980] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 00:04:58,980] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 00:04:58,980] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 00:04:58,980] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f663c9e9390>\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 00:04:58,981] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   train_batch_size ............. 64\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 00:04:58,982] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:33,  2.40it/s]\titers: 100, epoch: 1 | loss: 1.0276396\n",
      "\tspeed: 0.3789s/iter; left time: 17505.3389s\n",
      "199it [01:14,  2.40it/s]\titers: 200, epoch: 1 | loss: 0.8300536\n",
      "\tspeed: 0.4115s/iter; left time: 18972.8376s\n",
      "299it [01:56,  2.33it/s]\titers: 300, epoch: 1 | loss: 0.9544570\n",
      "\tspeed: 0.4195s/iter; left time: 19297.7365s\n",
      "399it [02:37,  2.12it/s]\titers: 400, epoch: 1 | loss: 1.0381955\n",
      "\tspeed: 0.4102s/iter; left time: 18829.3677s\n",
      "499it [03:19,  2.25it/s]\titers: 500, epoch: 1 | loss: 1.0002456\n",
      "\tspeed: 0.4138s/iter; left time: 18952.6457s\n",
      "599it [04:00,  2.42it/s]\titers: 600, epoch: 1 | loss: 1.0201256\n",
      "\tspeed: 0.4093s/iter; left time: 18705.3106s\n",
      "699it [04:42,  2.30it/s]\titers: 700, epoch: 1 | loss: 0.8801132\n",
      "\tspeed: 0.4206s/iter; left time: 19178.0303s\n",
      "799it [05:24,  2.25it/s]\titers: 800, epoch: 1 | loss: 0.8280338\n",
      "\tspeed: 0.4270s/iter; left time: 19427.7081s\n",
      "899it [06:05,  2.66it/s]\titers: 900, epoch: 1 | loss: 1.0097566\n",
      "\tspeed: 0.4087s/iter; left time: 18557.1843s\n",
      "999it [06:47,  2.20it/s]\titers: 1000, epoch: 1 | loss: 0.8511421\n",
      "\tspeed: 0.4134s/iter; left time: 18728.3548s\n",
      "1099it [07:28,  2.54it/s]\titers: 1100, epoch: 1 | loss: 0.7832329\n",
      "\tspeed: 0.4177s/iter; left time: 18881.0269s\n",
      "1199it [08:10,  2.28it/s]\titers: 1200, epoch: 1 | loss: 0.7613477\n",
      "\tspeed: 0.4182s/iter; left time: 18859.3190s\n",
      "1299it [08:52,  2.43it/s]\titers: 1300, epoch: 1 | loss: 0.8543524\n",
      "\tspeed: 0.4155s/iter; left time: 18696.4325s\n",
      "1399it [09:33,  2.48it/s]\titers: 1400, epoch: 1 | loss: 0.6430628\n",
      "\tspeed: 0.4165s/iter; left time: 18701.2980s\n",
      "1499it [10:15,  2.09it/s]\titers: 1500, epoch: 1 | loss: 0.5976496\n",
      "\tspeed: 0.4190s/iter; left time: 18770.4000s\n",
      "1599it [10:58,  2.31it/s]\titers: 1600, epoch: 1 | loss: 0.6696340\n",
      "\tspeed: 0.4237s/iter; left time: 18937.9927s\n",
      "1699it [11:39,  2.44it/s]\titers: 1700, epoch: 1 | loss: 0.6241440\n",
      "\tspeed: 0.4159s/iter; left time: 18550.0917s\n",
      "1799it [12:20,  2.45it/s]\titers: 1800, epoch: 1 | loss: 0.7488484\n",
      "\tspeed: 0.4126s/iter; left time: 18363.1452s\n",
      "1899it [13:02,  2.54it/s]\titers: 1900, epoch: 1 | loss: 0.5220980\n",
      "\tspeed: 0.4120s/iter; left time: 18292.7074s\n",
      "1999it [13:43,  2.57it/s]\titers: 2000, epoch: 1 | loss: 0.6719561\n",
      "\tspeed: 0.4122s/iter; left time: 18260.6533s\n",
      "2099it [14:25,  1.97it/s]\titers: 2100, epoch: 1 | loss: 0.4689202\n",
      "\tspeed: 0.4202s/iter; left time: 18575.0429s\n",
      "2199it [15:07,  2.35it/s]\titers: 2200, epoch: 1 | loss: 0.6080328\n",
      "\tspeed: 0.4219s/iter; left time: 18606.2030s\n",
      "2299it [15:49,  2.49it/s]\titers: 2300, epoch: 1 | loss: 0.6731280\n",
      "\tspeed: 0.4249s/iter; left time: 18695.1811s\n",
      "2315it [15:56,  2.42it/s]\n",
      "Epoch: 1 cost time: 956.7048053741455\n",
      "0it [00:00, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 268, in <module>\n",
      "    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/utils/tools.py\", line 178, in vali\n",
      "    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2242, in gather_for_metrics\n",
      "    data = self.gather(input_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2205, in gather\n",
      "    return gather(tensor)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 378, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 439, in gather\n",
      "    return _gpu_gather(tensor)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 358, in _gpu_gather\n",
      "    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
      "    return honor_type(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
      "    return type(obj)(generator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
      "    recursively_apply(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 126, in recursively_apply\n",
      "    return func(data, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 355, in _gpu_gather_one\n",
      "    torch.distributed.all_gather(output_tensors, tensor)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 72, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 2615, in all_gather\n",
      "    work = default_pg.allgather([tensor_list], [tensor])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3\n",
      "ncclUnhandledCudaError: Call to CUDA function failed.\n",
      "Last error:\n",
      "Failed to CUDA calloc async 24 bytes\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1057, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 673, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/vol/cs-hu/riabchuv/.conda/envs/val/bin/python', './Time-LLM/run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './datasets/', '--data_path', 'GB_data.csv', '--model_id', '1', '--model', 'TimeLLM', '--data', 'GB', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '96', '--factor', '3', '--enc_in', '5', '--dec_in', '5', '--c_out', '5', '--des', 'Exp', '--itr', '1', '--llm_model', 'GPT2', '--llm_dim', '768', '--d_model', '16', '--d_ff', '64', '--batch_size', '64', '--learning_rate', '0.001', '--llm_layers', '12', '--train_epochs', '20', '--model_comment', 'TimeLLM-GB']' returned non-zero exit status 1.\n",
      "Total time: 16.29517664114634 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=64 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very big batch size 128*8 BAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "d_llm 768\n",
      "999it [25:25,  1.20it/s]\titers: 1000, epoch: 1 | loss: 0.1215863\n",
      "\tspeed: 0.1910s/iter; left time: 359.2808s\n",
      "1157it [27:39,  1.43s/it]\n",
      "Epoch: 1 cost time: 1659.4052290916443\n",
      "250it [01:45,  2.36it/s]\n",
      "249it [01:44,  2.38it/s]\n",
      "Epoch: 1 | Train Loss: 1.8173011 Vali Loss: 1.0617123 Test Loss: 1.4366292 MAE Loss: 0.9732734\n",
      "lr = 0.0000400072\n",
      "Updating learning rate to 4.000716430748992e-05\n",
      "999it [13:38,  1.23it/s]\titers: 1000, epoch: 2 | loss: 0.1202567\n",
      "\tspeed: 0.3373s/iter; left time: 585.8961s\n",
      "1157it [15:48,  1.22it/s]\n",
      "Epoch: 2 cost time: 948.3282368183136\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.51it/s]\n",
      "Epoch: 2 | Train Loss: 0.9453899 Vali Loss: 1.0552500 Test Loss: 1.4272533 MAE Loss: 0.9708104\n",
      "Updating learning rate to 2.000358215374496e-05\n",
      "999it [13:35,  1.22it/s]\titers: 1000, epoch: 3 | loss: 0.1086178\n",
      "\tspeed: 0.4813s/iter; left time: 766.6317s\n",
      "1157it [15:45,  1.22it/s]\n",
      "Epoch: 3 cost time: 946.1724355220795\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 3 | Train Loss: 0.9247732 Vali Loss: 1.0545925 Test Loss: 1.4260601 MAE Loss: 0.9703135\n",
      "Updating learning rate to 1.000179107687248e-05\n",
      "999it [13:38,  1.23it/s]\titers: 1000, epoch: 4 | loss: 0.1208263\n",
      "\tspeed: 0.6255s/iter; left time: 906.4001s\n",
      "1157it [15:48,  1.22it/s]\n",
      "Epoch: 4 cost time: 948.9760584831238\n",
      "159it [01:03,  2.49it/s]^C\n",
      "159it [01:03,  2.49it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_onecycle_no_acc.py\", line 240, in <module>\n",
      "    vali_loss, vali_mae_loss = vali(args, model, vali_data, vali_loader, criterion, mae_metric)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/utils/tools_copy_no_acc.py\", line 159, in vali\n",
      "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM_copy.py\", line 238, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM_copy.py\", line 276, in forecast\n",
      "    prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2577, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2663, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2854, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 517, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line -1, in split\n",
      "KeyboardInterrupt\n",
      "Total time: 86.74729557434718 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=128 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -u ./Time-LLM/run_main_onecycle_no_acc.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grünau 10, pretty ok results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 148165\n",
      "val 32045\n",
      "test 31925\n",
      "d_llm 768\n",
      "999it [14:53,  1.07it/s]\titers: 1000, epoch: 1 | loss: 0.8977052\n",
      "\tspeed: 0.8953s/iter; left time: 19823.1777s\n",
      "1157it [17:14,  1.12it/s]\n",
      "Epoch: 1 cost time: 1035.096585035324\n",
      "250it [01:45,  2.37it/s]\n",
      "249it [01:44,  2.39it/s]\n",
      "Epoch: 1 | Train Loss: 0.9917065 Vali Loss: 0.9871420 Test Loss: 1.3496811 MAE Loss: 0.9353323\n",
      "lr = 0.0000400001\n",
      "Updating learning rate to 4.0000110639950106e-05\n",
      "999it [14:12,  1.16it/s]\titers: 1000, epoch: 2 | loss: 0.5450760\n",
      "\tspeed: 2.1043s/iter; left time: 44155.9914s\n",
      "1157it [16:27,  1.17it/s]\n",
      "Epoch: 2 cost time: 987.9425492286682\n",
      "250it [01:40,  2.48it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 2 | Train Loss: 0.6059795 Vali Loss: 0.6647810 Test Loss: 0.9336610 MAE Loss: 0.7085124\n",
      "Updating learning rate to 2.0000055319975053e-05\n",
      "999it [14:15,  1.20it/s]\titers: 1000, epoch: 3 | loss: 0.5474190\n",
      "\tspeed: 3.2996s/iter; left time: 65421.0177s\n",
      "1157it [16:30,  1.17it/s]\n",
      "Epoch: 3 cost time: 990.8328876495361\n",
      "250it [01:40,  2.48it/s]\n",
      "249it [01:39,  2.51it/s]\n",
      "Epoch: 3 | Train Loss: 0.5626342 Vali Loss: 0.6623971 Test Loss: 0.9338566 MAE Loss: 0.7125394\n",
      "Updating learning rate to 1.0000027659987527e-05\n",
      "999it [14:14,  1.15it/s]\titers: 1000, epoch: 4 | loss: 0.6083658\n",
      "\tspeed: 4.5105s/iter; left time: 84211.6383s\n",
      "1157it [16:29,  1.17it/s]\n",
      "Epoch: 4 cost time: 989.4511201381683\n",
      "250it [01:40,  2.50it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 4 | Train Loss: 0.5565918 Vali Loss: 0.6588538 Test Loss: 0.9249839 MAE Loss: 0.7023087\n",
      "Updating learning rate to 5.000013829993763e-06\n",
      "999it [14:12,  1.16it/s]\titers: 1000, epoch: 5 | loss: 0.5929440\n",
      "\tspeed: 5.7144s/iter; left time: 100076.5428s\n",
      "1157it [16:27,  1.17it/s]\n",
      "Epoch: 5 cost time: 987.8044993877411\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 5 | Train Loss: 0.5535935 Vali Loss: 0.6563976 Test Loss: 0.9243044 MAE Loss: 0.7042536\n",
      "Updating learning rate to 2.5000069149968816e-06\n",
      "999it [14:10,  1.19it/s]\titers: 1000, epoch: 6 | loss: 0.5310134\n",
      "\tspeed: 6.9061s/iter; left time: 112955.7256s\n",
      "1157it [16:25,  1.17it/s]\n",
      "Epoch: 6 cost time: 986.162045955658\n",
      "250it [01:40,  2.48it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 6 | Train Loss: 0.5521202 Vali Loss: 0.6561957 Test Loss: 0.9243819 MAE Loss: 0.7046596\n",
      "Updating learning rate to 1.2500034574984408e-06\n",
      "999it [14:12,  1.21it/s]\titers: 1000, epoch: 7 | loss: 0.5691071\n",
      "\tspeed: 8.0993s/iter; left time: 123101.7327s\n",
      "1157it [16:27,  1.17it/s]\n",
      "Epoch: 7 cost time: 987.8649070262909\n",
      "250it [01:40,  2.48it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 7 | Train Loss: 0.5512478 Vali Loss: 0.6565222 Test Loss: 0.9250377 MAE Loss: 0.7057701\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.250017287492204e-07\n",
      "999it [14:18,  1.18it/s]\titers: 1000, epoch: 8 | loss: 0.5181279\n",
      "\tspeed: 9.2943s/iter; left time: 130510.6664s\n",
      "1157it [16:34,  1.16it/s]\n",
      "Epoch: 8 cost time: 994.4580979347229\n",
      "250it [01:42,  2.43it/s]\n",
      "249it [01:41,  2.46it/s]\n",
      "Epoch: 8 | Train Loss: 0.5507932 Vali Loss: 0.6567641 Test Loss: 0.9253493 MAE Loss: 0.7068729\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125008643746102e-07\n",
      "999it [14:12,  1.19it/s]\titers: 1000, epoch: 9 | loss: 0.5687135\n",
      "\tspeed: 10.4881s/iter; left time: 135138.8358s\n",
      "1157it [16:26,  1.17it/s]\n",
      "Epoch: 9 cost time: 987.0371928215027\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 9 | Train Loss: 0.5507425 Vali Loss: 0.6559709 Test Loss: 0.9241031 MAE Loss: 0.7045357\n",
      "Updating learning rate to 1.562504321873051e-07\n",
      "999it [14:14,  1.19it/s]\titers: 1000, epoch: 10 | loss: 0.5949445\n",
      "\tspeed: 11.6818s/iter; left time: 137003.6413s\n",
      "1157it [16:29,  1.17it/s]\n",
      "Epoch: 10 cost time: 990.1766991615295\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 10 | Train Loss: 0.5507260 Vali Loss: 0.6559538 Test Loss: 0.9235057 MAE Loss: 0.7037072\n",
      "Updating learning rate to 7.812521609365255e-08\n",
      "999it [14:15,  1.15it/s]\titers: 1000, epoch: 11 | loss: 0.5541849\n",
      "\tspeed: 12.8794s/iter; left time: 136148.5115s\n",
      "1157it [16:30,  1.17it/s]\n",
      "Epoch: 11 cost time: 991.2554688453674\n",
      "250it [01:41,  2.47it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 11 | Train Loss: 0.5503782 Vali Loss: 0.6561954 Test Loss: 0.9237231 MAE Loss: 0.7040251\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.9062608046826275e-08\n",
      "999it [14:17,  1.16it/s]\titers: 1000, epoch: 12 | loss: 0.5418445\n",
      "\tspeed: 14.0744s/iter; left time: 132496.7470s\n",
      "1157it [16:33,  1.17it/s]\n",
      "Epoch: 12 cost time: 993.3671431541443\n",
      "250it [01:40,  2.49it/s]\n",
      "249it [01:39,  2.50it/s]\n",
      "Epoch: 12 | Train Loss: 0.5506101 Vali Loss: 0.6561699 Test Loss: 0.9236181 MAE Loss: 0.7038633\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.9531304023413138e-08\n",
      "999it [14:10,  1.14it/s]\titers: 1000, epoch: 13 | loss: 0.5304785\n",
      "\tspeed: 15.2617s/iter; left time: 126015.7537s\n",
      "1157it [16:25,  1.17it/s]\n",
      "Epoch: 13 cost time: 985.5802767276764\n",
      "250it [01:40,  2.48it/s]\n",
      "249it [01:40,  2.49it/s]\n",
      "Epoch: 13 | Train Loss: 0.5503468 Vali Loss: 0.6560211 Test Loss: 0.9236815 MAE Loss: 0.7039972\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 260.13286865552266 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=128 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -u ./Time-LLM/run_main_onecycle_no_acc_no_batch_accumulation.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 29250\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 00:51:52,113] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 00:51:52,976] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 00:51:52,977] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 00:51:52,977] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 00:51:53,818] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-16 00:51:53,819] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "gruenau10:63373:63373 [0] NCCL INFO Bootstrap : Using eth0:141.20.21.44<0>\n",
      "gruenau10:63373:63373 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "gruenau10:63373:63373 [0] NCCL INFO cudaDriverVersion 12040\n",
      "NCCL version 2.19.3+cuda12.3\n",
      "gruenau10:63373:63641 [0] NCCL INFO NET/IB : No device found.\n",
      "gruenau10:63373:63641 [0] NCCL INFO NET/Socket : Using [0]eth0:141.20.21.44<0>\n",
      "gruenau10:63373:63641 [0] NCCL INFO Using non-device net plugin version 0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Using network Socket\n",
      "gruenau10:63373:63641 [0] NCCL INFO comm 0xff60850 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId af000 commId 0x6e1d789e729047be - Init START\n",
      "gruenau10:63373:63641 [0] NCCL INFO Setting affinity for GPU 1 to aa,aaaaaaaa,aaaaaaaa\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 00/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 01/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 02/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 03/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 04/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 05/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 06/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 07/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 08/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 09/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 10/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 11/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 12/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 13/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 14/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 15/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 16/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 17/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 18/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 19/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 20/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 21/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 22/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 23/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 24/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 25/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 26/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 27/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 28/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 29/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 30/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Channel 31/32 :    0\n",
      "gruenau10:63373:63641 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "gruenau10:63373:63641 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "gruenau10:63373:63641 [0] NCCL INFO Connected all rings\n",
      "gruenau10:63373:63641 [0] NCCL INFO Connected all trees\n",
      "gruenau10:63373:63641 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "gruenau10:63373:63641 [0] NCCL INFO comm 0xff60850 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId af000 commId 0x6e1d789e729047be - Init COMPLETE\n",
      "[2024-05-16 00:51:54,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 00:51:54,951] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 00:51:54,951] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 00:51:54,952] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 00:51:54,952] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 00:51:54,952] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 00:51:54,952] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 00:51:54,952] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 00:51:54,952] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 00:51:54,952] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 00:51:55,288] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 00:51:55,288] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:51:55,288] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 216.4 GB, percent = 28.7%\n",
      "[2024-05-16 00:51:55,522] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 00:51:55,522] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:51:55,522] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 216.41 GB, percent = 28.7%\n",
      "[2024-05-16 00:51:55,522] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 00:51:55,635] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 00:51:55,635] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 00:51:55,636] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 216.4 GB, percent = 28.7%\n",
      "[2024-05-16 00:51:55,636] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 00:51:55,636] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 00:51:55,636] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 00:51:55,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1c5e1628d0>\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 00:51:55,637] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   train_batch_size ............. 64\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 00:51:55,638] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 00:51:55,639] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:35,  2.51it/s]\titers: 100, epoch: 1 | loss: 0.8661560\n",
      "\tspeed: 0.3986s/iter; left time: 3603.5109s\n",
      "199it [01:16,  2.44it/s]\titers: 200, epoch: 1 | loss: 0.6850823\n",
      "\tspeed: 0.4107s/iter; left time: 3671.8927s\n",
      "299it [01:58,  2.66it/s]\titers: 300, epoch: 1 | loss: 0.7261505\n",
      "\tspeed: 0.4199s/iter; left time: 3712.7534s\n",
      "399it [02:40,  2.42it/s]\titers: 400, epoch: 1 | loss: 0.8506546\n",
      "\tspeed: 0.4175s/iter; left time: 3649.7784s\n",
      "457it [03:04,  2.48it/s]\n",
      "Epoch: 1 cost time: 184.39726161956787\n",
      "0it [00:00, ?it/s]gruenau10:63373:67091 [0] NCCL INFO Using non-device net plugin version 0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Using network Socket\n",
      "gruenau10:63373:67091 [0] NCCL INFO comm 0x3f9cc450 rank 0 nranks 1 cudaDev 0 nvmlDev 1 busId af000 commId 0xcf275cc5cfd79b88 - Init START\n",
      "gruenau10:63373:67091 [0] NCCL INFO Setting affinity for GPU 1 to aa,aaaaaaaa,aaaaaaaa\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 00/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 01/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 02/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 03/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 04/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 05/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 06/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 07/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 08/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 09/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 10/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 11/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 12/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 13/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 14/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 15/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 16/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 17/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 18/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 19/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 20/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 21/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 22/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 23/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 24/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 25/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 26/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 27/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 28/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 29/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 30/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Channel 31/32 :    0\n",
      "gruenau10:63373:67091 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "gruenau10:63373:67091 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "\n",
      "gruenau10:63373:67091 [0] include/alloc.h:102 NCCL WARN Cuda failure 2 'out of memory'\n",
      "gruenau10:63373:67091 [0] NCCL INFO include/alloc.h:198 -> 1\n",
      "\n",
      "gruenau10:63373:67091 [0] include/alloc.h:205 NCCL WARN Failed to CUDA calloc async 608 bytes\n",
      "gruenau10:63373:67091 [0] NCCL INFO channel.cc:40 -> 1\n",
      "gruenau10:63373:67091 [0] NCCL INFO init.cc:513 -> 1\n",
      "gruenau10:63373:67091 [0] NCCL INFO init.cc:1113 -> 1\n",
      "gruenau10:63373:67091 [0] NCCL INFO init.cc:1396 -> 1\n",
      "gruenau10:63373:67091 [0] NCCL INFO group.cc:64 -> 1 [Async thread]\n",
      "gruenau10:63373:63373 [0] NCCL INFO group.cc:418 -> 1\n",
      "gruenau10:63373:63373 [0] NCCL INFO group.cc:95 -> 1\n",
      "0it [00:00, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 268, in <module>\n",
      "    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/utils/tools.py\", line 178, in vali\n",
      "    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2242, in gather_for_metrics\n",
      "    data = self.gather(input_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2205, in gather\n",
      "    return gather(tensor)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 378, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 439, in gather\n",
      "    return _gpu_gather(tensor)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 358, in _gpu_gather\n",
      "    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
      "    return honor_type(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
      "    return type(obj)(generator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
      "    recursively_apply(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 126, in recursively_apply\n",
      "    return func(data, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 355, in _gpu_gather_one\n",
      "    torch.distributed.all_gather(output_tensors, tensor)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 72, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 2615, in all_gather\n",
      "    work = default_pg.allgather([tensor_list], [tensor])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3\n",
      "ncclUnhandledCudaError: Call to CUDA function failed.\n",
      "Last error:\n",
      "Failed to CUDA calloc async 608 bytes\n",
      "gruenau10:63373:63373 [0] NCCL INFO comm 0x3f9cc450 rank 0 nranks 1 cudaDev 0 busId af000 - Abort COMPLETE\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1057, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 673, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/vol/cs-hu/riabchuv/.conda/envs/val/bin/python', './Time-LLM/run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './datasets/', '--data_path', 'GB_data.csv', '--model_id', '1', '--model', 'TimeLLM', '--data', 'GB', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '96', '--factor', '3', '--enc_in', '5', '--dec_in', '5', '--c_out', '5', '--des', 'Exp', '--itr', '1', '--llm_model', 'GPT2', '--llm_dim', '768', '--percent', '20', '--d_model', '16', '--d_ff', '64', '--batch_size', '64', '--learning_rate', '0.001', '--llm_layers', '12', '--train_epochs', '20', '--model_comment', 'TimeLLM-GB']' returned non-zero exit status 1.\n",
      "Total time: 3.406488517920176 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=64 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 16 00:40:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   49C    P0            190W /  300W |   57378MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   43C    P0            183W /  300W |   21425MiB /  81920MiB |     69%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   69C    P0            296W /  300W |   78822MiB /  81920MiB |     98%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      7139      G   /usr/bin/X                                      4MiB |\n",
      "|    0   N/A  N/A     10201      C   python                                       9356MiB |\n",
      "|    0   N/A  N/A     10463      C   python                                       9356MiB |\n",
      "|    0   N/A  N/A     29499      C   ...holuka/.conda/envs/py27/bin/python2        826MiB |\n",
      "|    0   N/A  N/A     38592      C   ...conda/envs/img2img-turbo/bin/python      18908MiB |\n",
      "|    0   N/A  N/A     54037      C   ...conda/envs/img2img-turbo/bin/python      18900MiB |\n",
      "|    1   N/A  N/A      7139      G   /usr/bin/X                                      4MiB |\n",
      "|    1   N/A  N/A     33678      C   python3                                      5086MiB |\n",
      "|    1   N/A  N/A     40811      C   python                                      15634MiB |\n",
      "|    1   N/A  N/A     45415      C   python                                        678MiB |\n",
      "|    2   N/A  N/A      7139      G   /usr/bin/X                                      4MiB |\n",
      "|    2   N/A  N/A     32777      C   python                                      78806MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent = 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 29250\n",
      "val 32045\n",
      "test 31925\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 01:24:35,720] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 01:24:36,550] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 01:24:36,550] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 01:24:36,550] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 01:24:37,428] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-16 01:24:37,429] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 01:24:38,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 01:24:38,074] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 01:24:38,074] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 01:24:38,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 01:24:38,075] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 01:24:38,075] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 01:24:38,075] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 01:24:38,075] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 01:24:38,075] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 01:24:38,075] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 01:24:38,663] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 01:24:38,664] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-05-16 01:24:38,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 211.87 GB, percent = 28.1%\n",
      "[2024-05-16 01:24:38,779] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 01:24:38,779] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 01:24:38,779] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 211.87 GB, percent = 28.1%\n",
      "[2024-05-16 01:24:38,780] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 01:24:38,894] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 01:24:38,894] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-05-16 01:24:38,895] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 211.87 GB, percent = 28.1%\n",
      "[2024-05-16 01:24:38,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 01:24:38,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 01:24:38,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 01:24:38,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f2ce2ac10>\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 01:24:38,896] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 01:24:38,897] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:08, 13.09it/s]\titers: 100, epoch: 1 | loss: 0.8877624\n",
      "\tspeed: 0.1281s/iter; left time: 3107.1514s\n",
      "199it [00:16, 14.09it/s]\titers: 200, epoch: 1 | loss: 0.8796080\n",
      "\tspeed: 0.0733s/iter; left time: 1771.2593s\n",
      "299it [00:23, 14.45it/s]\titers: 300, epoch: 1 | loss: 0.7779171\n",
      "\tspeed: 0.0710s/iter; left time: 1709.2580s\n",
      "399it [00:30, 13.94it/s]\titers: 400, epoch: 1 | loss: 0.7888060\n",
      "\tspeed: 0.0716s/iter; left time: 1715.7904s\n",
      "499it [00:37, 13.96it/s]\titers: 500, epoch: 1 | loss: 0.6772147\n",
      "\tspeed: 0.0714s/iter; left time: 1703.2695s\n",
      "599it [00:44, 14.17it/s]\titers: 600, epoch: 1 | loss: 0.8412825\n",
      "\tspeed: 0.0707s/iter; left time: 1680.8090s\n",
      "699it [00:51, 13.57it/s]\titers: 700, epoch: 1 | loss: 0.7715670\n",
      "\tspeed: 0.0716s/iter; left time: 1694.8409s\n",
      "799it [00:58, 13.60it/s]\titers: 800, epoch: 1 | loss: 0.6295446\n",
      "\tspeed: 0.0706s/iter; left time: 1663.5685s\n",
      "899it [01:06, 14.14it/s]\titers: 900, epoch: 1 | loss: 0.8902766\n",
      "\tspeed: 0.0723s/iter; left time: 1695.3596s\n",
      "999it [01:13, 14.13it/s]\titers: 1000, epoch: 1 | loss: 0.5683873\n",
      "\tspeed: 0.0718s/iter; left time: 1678.0015s\n",
      "1099it [01:20, 14.11it/s]\titers: 1100, epoch: 1 | loss: 0.6215774\n",
      "\tspeed: 0.0714s/iter; left time: 1659.8270s\n",
      "1199it [01:27, 14.07it/s]\titers: 1200, epoch: 1 | loss: 0.4691405\n",
      "\tspeed: 0.0716s/iter; left time: 1659.2041s\n",
      "1218it [01:28, 13.69it/s]\n",
      "Epoch: 1 cost time: 88.94011926651001\n",
      "1335it [00:49, 27.19it/s]\n",
      "1330it [00:50, 26.57it/s]\n",
      "Epoch: 1 | Train Loss: 0.7677085 Vali Loss: 0.7420060 Test Loss: 1.0429852 MAE Loss: 0.7749111\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:07, 14.16it/s]\titers: 100, epoch: 2 | loss: 0.3859607\n",
      "\tspeed: 1.1051s/iter; left time: 25465.1331s\n",
      "199it [00:14, 14.11it/s]\titers: 200, epoch: 2 | loss: 0.5095922\n",
      "\tspeed: 0.0709s/iter; left time: 1626.5598s\n",
      "299it [00:21, 14.24it/s]\titers: 300, epoch: 2 | loss: 0.4379060\n",
      "\tspeed: 0.0709s/iter; left time: 1620.7097s\n",
      "399it [00:28, 13.85it/s]\titers: 400, epoch: 2 | loss: 0.3714036\n",
      "\tspeed: 0.0711s/iter; left time: 1617.9559s\n",
      "499it [00:35, 13.73it/s]\titers: 500, epoch: 2 | loss: 0.5318789\n",
      "\tspeed: 0.0711s/iter; left time: 1610.6079s\n",
      "599it [00:42, 14.43it/s]\titers: 600, epoch: 2 | loss: 0.4540915\n",
      "\tspeed: 0.0706s/iter; left time: 1591.3982s\n",
      "699it [00:49, 14.08it/s]\titers: 700, epoch: 2 | loss: 0.4472219\n",
      "\tspeed: 0.0710s/iter; left time: 1592.7582s\n",
      "799it [00:56, 13.21it/s]\titers: 800, epoch: 2 | loss: 0.5185457\n",
      "\tspeed: 0.0706s/iter; left time: 1577.3931s\n",
      "899it [01:04, 14.06it/s]\titers: 900, epoch: 2 | loss: 0.5891929\n",
      "\tspeed: 0.0712s/iter; left time: 1583.4564s\n",
      "999it [01:11, 14.14it/s]\titers: 1000, epoch: 2 | loss: 0.3680853\n",
      "\tspeed: 0.0715s/iter; left time: 1582.2231s\n",
      "1099it [01:18, 14.02it/s]\titers: 1100, epoch: 2 | loss: 0.3443682\n",
      "\tspeed: 0.0712s/iter; left time: 1569.9921s\n",
      "1199it [01:25, 14.30it/s]\titers: 1200, epoch: 2 | loss: 0.4768879\n",
      "\tspeed: 0.0711s/iter; left time: 1561.2423s\n",
      "1218it [01:26, 14.01it/s]\n",
      "Epoch: 2 cost time: 86.92233276367188\n",
      "1335it [00:44, 30.13it/s]\n",
      "1330it [00:44, 29.61it/s]\n",
      "Epoch: 2 | Train Loss: 0.4554977 Vali Loss: 0.6952225 Test Loss: 0.9494880 MAE Loss: 0.7022478\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:07, 13.68it/s]\titers: 100, epoch: 3 | loss: 0.4898770\n",
      "\tspeed: 1.0086s/iter; left time: 22011.6669s\n",
      "199it [00:14, 14.17it/s]\titers: 200, epoch: 3 | loss: 0.4206660\n",
      "\tspeed: 0.0717s/iter; left time: 1556.6612s\n",
      "299it [00:21, 14.13it/s]\titers: 300, epoch: 3 | loss: 0.4396290\n",
      "\tspeed: 0.0719s/iter; left time: 1554.8835s\n",
      "399it [00:28, 14.00it/s]\titers: 400, epoch: 3 | loss: 0.4650536\n",
      "\tspeed: 0.0727s/iter; left time: 1563.7933s\n",
      "499it [00:36, 14.39it/s]\titers: 500, epoch: 3 | loss: 0.2924895\n",
      "\tspeed: 0.0710s/iter; left time: 1521.7786s\n",
      "599it [00:43, 13.95it/s]\titers: 600, epoch: 3 | loss: 0.5090232\n",
      "\tspeed: 0.0707s/iter; left time: 1506.6706s\n",
      "699it [00:50, 14.13it/s]\titers: 700, epoch: 3 | loss: 0.4315348\n",
      "\tspeed: 0.0709s/iter; left time: 1504.6724s\n",
      "799it [00:57, 14.14it/s]\titers: 800, epoch: 3 | loss: 0.4416969\n",
      "\tspeed: 0.0710s/iter; left time: 1500.1078s\n",
      "899it [01:04, 13.65it/s]\titers: 900, epoch: 3 | loss: 0.3402951\n",
      "\tspeed: 0.0710s/iter; left time: 1493.0907s\n",
      "999it [01:11, 13.23it/s]\titers: 1000, epoch: 3 | loss: 0.4992592\n",
      "\tspeed: 0.0711s/iter; left time: 1487.3993s\n",
      "1099it [01:18, 14.16it/s]\titers: 1100, epoch: 3 | loss: 0.4707420\n",
      "\tspeed: 0.0712s/iter; left time: 1481.7537s\n",
      "1199it [01:25, 14.28it/s]\titers: 1200, epoch: 3 | loss: 0.4085807\n",
      "\tspeed: 0.0721s/iter; left time: 1493.4270s\n",
      "1218it [01:27, 13.96it/s]\n",
      "Epoch: 3 cost time: 87.25455045700073\n",
      "1335it [00:44, 29.95it/s]\n",
      "1330it [00:44, 30.09it/s]\n",
      "Epoch: 3 | Train Loss: 0.4226589 Vali Loss: 0.6948244 Test Loss: 0.9522834 MAE Loss: 0.7074767\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:07, 14.35it/s]\titers: 100, epoch: 4 | loss: 0.3629808\n",
      "\tspeed: 1.0003s/iter; left time: 20612.5513s\n",
      "199it [00:14, 14.30it/s]\titers: 200, epoch: 4 | loss: 0.4680897\n",
      "\tspeed: 0.0687s/iter; left time: 1409.5215s\n",
      "299it [00:21, 13.85it/s]\titers: 300, epoch: 4 | loss: 0.4032621\n",
      "\tspeed: 0.0709s/iter; left time: 1446.1192s\n",
      "399it [00:28, 14.06it/s]\titers: 400, epoch: 4 | loss: 0.4006547\n",
      "\tspeed: 0.0712s/iter; left time: 1446.1410s\n",
      "499it [00:35, 14.17it/s]\titers: 500, epoch: 4 | loss: 0.4329470\n",
      "\tspeed: 0.0710s/iter; left time: 1433.7578s\n",
      "599it [00:42, 14.15it/s]\titers: 600, epoch: 4 | loss: 0.3697421\n",
      "\tspeed: 0.0713s/iter; left time: 1433.9342s\n",
      "699it [00:49, 14.23it/s]\titers: 700, epoch: 4 | loss: 0.5314137\n",
      "\tspeed: 0.0709s/iter; left time: 1419.4262s\n",
      "799it [00:56, 13.97it/s]\titers: 800, epoch: 4 | loss: 0.3663237\n",
      "\tspeed: 0.0700s/iter; left time: 1394.2707s\n",
      "899it [01:03, 14.30it/s]\titers: 900, epoch: 4 | loss: 0.4150991\n",
      "\tspeed: 0.0705s/iter; left time: 1395.5399s\n",
      "999it [01:11, 14.14it/s]\titers: 1000, epoch: 4 | loss: 0.4781530\n",
      "\tspeed: 0.0714s/iter; left time: 1407.3110s\n",
      "1099it [01:17, 15.19it/s]\titers: 1100, epoch: 4 | loss: 0.3404766\n",
      "\tspeed: 0.0692s/iter; left time: 1356.1317s\n",
      "1199it [01:24, 14.53it/s]\titers: 1200, epoch: 4 | loss: 0.3220007\n",
      "\tspeed: 0.0690s/iter; left time: 1346.0115s\n",
      "1218it [01:26, 14.12it/s]\n",
      "Epoch: 4 cost time: 86.23121643066406\n",
      "1335it [00:44, 29.92it/s]\n",
      "1330it [00:44, 29.89it/s]\n",
      "Epoch: 4 | Train Loss: 0.4006431 Vali Loss: 0.7109583 Test Loss: 0.9923273 MAE Loss: 0.7174089\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:07, 14.33it/s]\titers: 100, epoch: 5 | loss: 0.4133529\n",
      "\tspeed: 0.9828s/iter; left time: 19055.3165s\n",
      "199it [00:14, 14.21it/s]\titers: 200, epoch: 5 | loss: 0.2662100\n",
      "\tspeed: 0.0696s/iter; left time: 1342.7387s\n",
      "299it [00:21, 14.05it/s]\titers: 300, epoch: 5 | loss: 0.3048676\n",
      "\tspeed: 0.0703s/iter; left time: 1349.6001s\n",
      "399it [00:28, 14.24it/s]\titers: 400, epoch: 5 | loss: 0.4332656\n",
      "\tspeed: 0.0708s/iter; left time: 1352.2474s\n",
      "499it [00:35, 14.48it/s]\titers: 500, epoch: 5 | loss: 0.3204897\n",
      "\tspeed: 0.0723s/iter; left time: 1372.7268s\n",
      "599it [00:42, 14.43it/s]\titers: 600, epoch: 5 | loss: 0.3221377\n",
      "\tspeed: 0.0704s/iter; left time: 1329.8581s\n",
      "699it [00:49, 13.96it/s]\titers: 700, epoch: 5 | loss: 0.3296871\n",
      "\tspeed: 0.0703s/iter; left time: 1321.3327s\n",
      "799it [00:56, 14.11it/s]\titers: 800, epoch: 5 | loss: 0.2890051\n",
      "\tspeed: 0.0703s/iter; left time: 1314.2934s\n",
      "899it [01:03, 14.09it/s]\titers: 900, epoch: 5 | loss: 0.3677601\n",
      "\tspeed: 0.0709s/iter; left time: 1318.3501s\n",
      "999it [01:10, 14.12it/s]\titers: 1000, epoch: 5 | loss: 0.3746818\n",
      "\tspeed: 0.0708s/iter; left time: 1309.3982s\n",
      "1099it [01:17, 14.18it/s]\titers: 1100, epoch: 5 | loss: 0.2952358\n",
      "\tspeed: 0.0708s/iter; left time: 1301.6847s\n",
      "1199it [01:25, 14.08it/s]\titers: 1200, epoch: 5 | loss: 0.4121998\n",
      "\tspeed: 0.0707s/iter; left time: 1292.9708s\n",
      "1218it [01:26, 14.08it/s]\n",
      "Epoch: 5 cost time: 86.48413348197937\n",
      "1335it [00:43, 30.86it/s]\n",
      "1330it [00:43, 30.64it/s]\n",
      "Epoch: 5 | Train Loss: 0.3726517 Vali Loss: 0.7244294 Test Loss: 1.0321472 MAE Loss: 0.7364121\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:07, 14.04it/s]\titers: 100, epoch: 6 | loss: 0.4077153\n",
      "\tspeed: 0.9584s/iter; left time: 17414.9741s\n",
      "199it [00:14, 14.03it/s]\titers: 200, epoch: 6 | loss: 0.3206780\n",
      "\tspeed: 0.0698s/iter; left time: 1261.0996s\n",
      "299it [00:21, 13.88it/s]\titers: 300, epoch: 6 | loss: 0.3046089\n",
      "\tspeed: 0.0707s/iter; left time: 1270.7930s\n",
      "399it [00:28, 13.99it/s]\titers: 400, epoch: 6 | loss: 0.3452436\n",
      "\tspeed: 0.0708s/iter; left time: 1265.5425s\n",
      "499it [00:35, 14.10it/s]\titers: 500, epoch: 6 | loss: 0.3209588\n",
      "\tspeed: 0.0705s/iter; left time: 1252.7445s\n",
      "599it [00:42, 14.13it/s]\titers: 600, epoch: 6 | loss: 0.2921488\n",
      "\tspeed: 0.0706s/iter; left time: 1246.8856s\n",
      "699it [00:49, 14.87it/s]\titers: 700, epoch: 6 | loss: 0.3025478\n",
      "\tspeed: 0.0694s/iter; left time: 1219.9199s\n",
      "799it [00:56, 14.33it/s]\titers: 800, epoch: 6 | loss: 0.3731019\n",
      "\tspeed: 0.0695s/iter; left time: 1213.4949s\n",
      "899it [01:03, 14.10it/s]\titers: 900, epoch: 6 | loss: 0.3065099\n",
      "\tspeed: 0.0708s/iter; left time: 1230.0949s\n",
      "999it [01:10, 14.23it/s]\titers: 1000, epoch: 6 | loss: 0.2590230\n",
      "\tspeed: 0.0709s/iter; left time: 1224.6464s\n",
      "1099it [01:17, 14.22it/s]\titers: 1100, epoch: 6 | loss: 0.3734224\n",
      "\tspeed: 0.0704s/iter; left time: 1208.4448s\n",
      "1199it [01:24, 14.40it/s]\titers: 1200, epoch: 6 | loss: 0.3786067\n",
      "\tspeed: 0.0705s/iter; left time: 1202.6630s\n",
      "1218it [01:26, 14.14it/s]\n",
      "Epoch: 6 cost time: 86.10930848121643\n",
      "1335it [00:43, 30.96it/s]\n",
      "1330it [00:43, 30.87it/s]\n",
      "Epoch: 6 | Train Loss: 0.3404418 Vali Loss: 0.7512878 Test Loss: 1.1020232 MAE Loss: 0.7509210\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "success delete checkpoints\n",
      "Total time: 18.155333964029946 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001 # 0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "# 54168MiB\n",
    "batch_size=24 #24\n",
    "d_model= 16 # 32\n",
    "d_ff=64 # 128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-GB'\n",
    "\n",
    "# Unset NCCL_DEBUG environment variable\n",
    "if \"NCCL_DEBUG\" in os.environ:\n",
    "    del os.environ[\"NCCL_DEBUG\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 29250\n",
      "val 32045\n",
      "test 31925\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.31s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 01:43:38,234] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 01:43:38,928] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 01:43:38,928] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 01:43:38,928] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 01:43:39,768] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-16 01:43:39,769] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 01:43:51,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 01:43:51,386] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 01:43:51,386] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 01:43:51,388] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 01:43:51,388] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 01:43:51,388] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 01:43:51,388] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 01:43:51,388] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 01:43:51,388] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 01:43:51,388] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 01:43:51,662] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 01:43:51,662] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.68 GB         CA 12.68 GB         Max_CA 13 GB \n",
      "[2024-05-16 01:43:51,662] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 217.24 GB, percent = 28.8%\n",
      "[2024-05-16 01:43:51,769] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 01:43:51,770] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.76 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-16 01:43:51,770] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 217.24 GB, percent = 28.8%\n",
      "[2024-05-16 01:43:51,770] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 01:43:51,879] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 01:43:51,880] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.59 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-16 01:43:51,880] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 217.24 GB, percent = 28.8%\n",
      "[2024-05-16 01:43:51,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 01:43:51,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 01:43:51,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 01:43:51,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 01:43:51,881] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdc49ad7210>\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8\n",
      "[2024-05-16 01:43:51,882] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  3\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 01:43:51,883] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 3, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "9750it [26:13,  6.20it/s]\n",
      "Epoch: 1 cost time: 1573.1302688121796\n",
      "10681it [13:36, 13.08it/s]\n",
      "10641it [13:36, 13.03it/s]\n",
      "Epoch: 1 | Train Loss: 0.5541788 Vali Loss: 0.7145955 Test Loss: 0.9955056 MAE Loss: 0.7302461\n",
      "lr = 0.0008531548\n",
      "Updating learning rate to 0.0008531547871744895\n",
      "9750it [26:15,  6.19it/s]\n",
      "Epoch: 2 cost time: 1575.850299835205\n",
      "10681it [13:39, 13.04it/s]\n",
      "10641it [13:39, 12.99it/s]\n",
      "Epoch: 2 | Train Loss: 0.4171359 Vali Loss: 0.7175161 Test Loss: 1.0068026 MAE Loss: 0.7254310\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00042657739358724475\n",
      "9750it [26:18,  6.18it/s]\n",
      "Epoch: 3 cost time: 1578.8679583072662\n",
      "10681it [13:39, 13.04it/s]\n",
      "10641it [13:39, 12.99it/s]\n",
      "Epoch: 3 | Train Loss: 0.3866236 Vali Loss: 0.7146799 Test Loss: 1.0048197 MAE Loss: 0.7237418\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00021328869679362238\n",
      "9750it [26:20,  6.17it/s]\n",
      "Epoch: 4 cost time: 1580.2106108665466\n",
      "10681it [13:41, 13.00it/s]\n",
      "10641it [13:38, 13.00it/s]\n",
      "Epoch: 4 | Train Loss: 0.3897293 Vali Loss: 0.7424269 Test Loss: 1.0293637 MAE Loss: 0.7308687\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.00010664434839681119\n",
      "9750it [26:19,  6.17it/s]\n",
      "Epoch: 5 cost time: 1579.0594515800476\n",
      "10681it [13:40, 13.02it/s]\n",
      "10641it [13:36, 13.03it/s]\n",
      "Epoch: 5 | Train Loss: 0.3713321 Vali Loss: 0.7337055 Test Loss: 1.0409264 MAE Loss: 0.7342064\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3322174198405594e-05\n",
      "9750it [26:17,  6.18it/s]\n",
      "Epoch: 6 cost time: 1577.5964877605438\n",
      "10681it [13:42, 12.99it/s]\n",
      "10641it [13:41, 12.95it/s]\n",
      "Epoch: 6 | Train Loss: 0.3549584 Vali Loss: 0.7460664 Test Loss: 1.0613058 MAE Loss: 0.7368765\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.6661087099202797e-05\n",
      "9750it [26:25,  6.15it/s]\n",
      "Epoch: 7 cost time: 1585.0233018398285\n",
      "10681it [13:38, 13.05it/s]\n",
      "10641it [13:39, 12.98it/s]\n",
      "Epoch: 7 | Train Loss: 0.3447808 Vali Loss: 0.7383921 Test Loss: 1.0660111 MAE Loss: 0.7386761\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.3330543549601398e-05\n",
      "9750it [26:16,  6.18it/s]\n",
      "Epoch: 8 cost time: 1576.875869512558\n",
      "10681it [13:37, 13.06it/s]\n",
      "10641it [13:39, 12.99it/s]\n",
      "Epoch: 8 | Train Loss: 0.3391793 Vali Loss: 0.7321908 Test Loss: 1.0623161 MAE Loss: 0.7379095\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 6.665271774800699e-06\n",
      "9750it [26:32,  6.12it/s]\n",
      "Epoch: 9 cost time: 1592.8206067085266\n",
      "10681it [13:54, 12.80it/s]\n",
      "10641it [13:44, 12.91it/s]\n",
      "Epoch: 9 | Train Loss: 0.3369289 Vali Loss: 0.7407097 Test Loss: 1.0708264 MAE Loss: 0.7401426\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.3326358874003496e-06\n",
      "2725it [07:25,  6.11it/s]^C\n",
      "2725it [07:25,  6.11it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_copy_copy.py\", line 201, in <module>\n",
      "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1852, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 242, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 289, in forecast\n",
      "    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 693, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 421, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 216, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                                           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Total time: 494.350377813975 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=3\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "# Unset NCCL_DEBUG environment variable\n",
    "if \"NCCL_DEBUG\" in os.environ:\n",
    "    del os.environ[\"NCCL_DEBUG\"]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 5 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 29250\n",
      "val 32045\n",
      "test 31925\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.09s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 10:20:34,450] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 10:20:35,375] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 10:20:35,375] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 10:20:35,375] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 10:20:36,176] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.44, master_port=29500\n",
      "[2024-05-16 10:20:36,176] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 10:20:47,476] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 10:20:47,477] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 10:20:47,477] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 10:20:47,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 10:20:47,479] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 10:20:47,479] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 10:20:47,479] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 10:20:47,479] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 10:20:47,479] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 10:20:47,479] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 10:20:47,755] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 10:20:47,756] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.68 GB         CA 12.68 GB         Max_CA 13 GB \n",
      "[2024-05-16 10:20:47,756] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 255.04 GB, percent = 33.8%\n",
      "[2024-05-16 10:20:47,870] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 10:20:47,870] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.76 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-16 10:20:47,870] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 255.04 GB, percent = 33.8%\n",
      "[2024-05-16 10:20:47,870] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 10:20:47,980] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 10:20:47,981] [INFO] [utils.py:801:see_memory_usage] MA 12.59 GB         Max_MA 12.59 GB         CA 12.85 GB         Max_CA 13 GB \n",
      "[2024-05-16 10:20:47,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 255.04 GB, percent = 33.8%\n",
      "[2024-05-16 10:20:47,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 10:20:47,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 10:20:47,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 10:20:47,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 10:20:47,982] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f993c239250>\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 42\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 10:20:47,983] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   train_batch_size ............. 126\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  3\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 10:20:47,984] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 42, \n",
      "    \"train_batch_size\": 126, \n",
      "    \"train_micro_batch_size_per_gpu\": 3, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "9750it [26:21,  6.16it/s]\n",
      "Epoch: 1 cost time: 1581.8376202583313\n",
      "10681it [13:47, 12.91it/s]\n",
      "10641it [13:45, 12.90it/s]\n",
      "Epoch: 1 | Train Loss: 0.6241495 Vali Loss: 0.8269498 Test Loss: 1.1606611 MAE Loss: 0.8214512\n",
      "lr = 0.0000000042\n",
      "Updating learning rate to 4.179069679341233e-09\n",
      "9750it [26:24,  6.15it/s]\n",
      "Epoch: 2 cost time: 1584.395370721817\n",
      "10681it [13:38, 13.05it/s]\n",
      "10641it [13:38, 13.00it/s]\n",
      "Epoch: 2 | Train Loss: 0.5441948 Vali Loss: 0.8267909 Test Loss: 1.1605107 MAE Loss: 0.8213678\n",
      "Updating learning rate to 2.0895348396706167e-09\n",
      "9750it [26:09,  6.21it/s]\n",
      "Epoch: 3 cost time: 1569.1377036571503\n",
      "10681it [13:41, 13.01it/s]\n",
      "10641it [13:38, 13.00it/s]\n",
      "Epoch: 3 | Train Loss: 0.5442266 Vali Loss: 0.8267686 Test Loss: 1.1604303 MAE Loss: 0.8213227\n",
      "Updating learning rate to 1.0447674198353083e-09\n",
      "512it [01:21,  5.83it/s]^C\n",
      "512it [01:21,  6.26it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_copy_copy.py\", line 208, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 2002, in backward\n",
      "    self.allreduce_gradients()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1918, in allreduce_gradients\n",
      "    self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 859, in overlapping_partition_gradients_reduce_epilogue\n",
      "    self.independent_gradient_partition_epilogue()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 760, in independent_gradient_partition_epilogue\n",
      "    get_accelerator().synchronize()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/accelerator/cuda_accelerator.py\", line 77, in synchronize\n",
      "    return torch.cuda.synchronize(device_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 801, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Total time: 170.88218088150023 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=3\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "# Unset NCCL_DEBUG environment variable\n",
    "if \"NCCL_DEBUG\" in os.environ:\n",
    "    del os.environ[\"NCCL_DEBUG\"]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 5 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 3\n",
      "Device 0: NVIDIA A100 80GB PCIe\n",
      "Device 1: NVIDIA A100 80GB PCIe\n",
      "Device 2: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
