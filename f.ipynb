{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `3`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/usr/local/anaconda3-2023.03/envs/python311/bin/python: can't open file '/vol/cs-hu/riabchuv/hu-home/my_work/run_main.py': [Errno 2] No such file or directory\n",
      "/usr/local/anaconda3-2023.03/envs/python311/bin/python: can't open file '/vol/cs-hu/riabchuv/hu-home/my_work/run_main.py': [Errno 2] No such file or directory\n",
      "/usr/local/anaconda3-2023.03/envs/python311/bin/python: can't open file '/vol/cs-hu/riabchuv/hu-home/my_work/run_main.py': [Errno 2] No such file or directory\n",
      "^C\n",
      "[2024-04-29 19:58:47,148] torch.distributed.elastic.agent.server.api: [WARNING] Received 2 death signal, shutting down workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/.local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 259, in launch_agent\n",
      "    result = agent.run()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py\", line 123, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 727, in run\n",
      "    result = self._invoke_run(role)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 868, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 19687 got signal: 2\n"
     ]
    }
   ],
   "source": [
    "train_epochs=10\n",
    "learning_rate=0.01\n",
    "llama_layers=6\n",
    "\n",
    "#master_port=00097\n",
    "num_process=1\n",
    "batch_size=2\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --main_process_port \"00097\"\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-ETTh1'\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --dynamo_backend \"no\" run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --num_processes $num_process \\\n",
    "  --model_comment $comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.4398238\n",
      "\tspeed: 0.0615s/iter; left time: 157.3993s\n",
      "\titers: 200, epoch: 1 | loss: 0.4088117\n",
      "\tspeed: 0.0421s/iter; left time: 103.4920s\n",
      "Epoch: 1 running time: 0.20338455438613892 min.\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.4484731 Vali Loss: 0.7442934 Test Loss: 0.5967928\n",
      "Validation loss decreased (inf --> 0.744293).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3025567\n",
      "\tspeed: 0.1056s/iter; left time: 242.2739s\n",
      "\titers: 200, epoch: 2 | loss: 0.3389409\n",
      "\tspeed: 0.0412s/iter; left time: 90.4293s\n",
      "Epoch: 2 running time: 0.1889899214108785 min.\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.3259072 Vali Loss: 0.7888978 Test Loss: 0.8300452\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3365152\n",
      "\tspeed: 0.1008s/iter; left time: 204.5376s\n",
      "\titers: 200, epoch: 3 | loss: 0.3098605\n",
      "\tspeed: 0.0443s/iter; left time: 85.5222s\n",
      "Epoch: 3 running time: 0.1919772744178772 min.\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.3231263 Vali Loss: 0.7388420 Test Loss: 0.6724409\n",
      "Validation loss decreased (0.744293 --> 0.738842).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2971177\n",
      "\tspeed: 0.1036s/iter; left time: 182.5999s\n",
      "\titers: 200, epoch: 4 | loss: 0.3051086\n",
      "\tspeed: 0.0421s/iter; left time: 70.0455s\n",
      "Epoch: 4 running time: 0.18961451450983682 min.\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.2939822 Vali Loss: 0.7674699 Test Loss: 0.6608061\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2550912\n",
      "\tspeed: 0.1022s/iter; left time: 152.9219s\n",
      "\titers: 200, epoch: 5 | loss: 0.2714912\n",
      "\tspeed: 0.0416s/iter; left time: 58.1149s\n",
      "Epoch: 5 running time: 0.1904783805211385 min.\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.2918414 Vali Loss: 0.7861078 Test Loss: 0.6900674\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2783736\n",
      "\tspeed: 0.1006s/iter; left time: 123.8722s\n",
      "\titers: 200, epoch: 6 | loss: 0.2760369\n",
      "\tspeed: 0.0423s/iter; left time: 47.7909s\n",
      "Epoch: 6 running time: 0.19101029237111408 min.\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.2922943 Vali Loss: 0.7818955 Test Loss: 0.6706946\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
      "test shape: (2848, 24, 7) (2848, 24, 7)\n",
      "mse:0.6715533137321472, mae:0.6105679869651794\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.4033915\n",
      "\tspeed: 0.0448s/iter; left time: 114.7466s\n",
      "\titers: 200, epoch: 1 | loss: 0.3719254\n",
      "\tspeed: 0.0431s/iter; left time: 106.0270s\n",
      "Epoch: 1 running time: 0.19453486601511638 min.\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.4404117 Vali Loss: 0.7318097 Test Loss: 0.5701032\n",
      "Validation loss decreased (inf --> 0.731810).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3270476\n",
      "\tspeed: 0.1093s/iter; left time: 250.8669s\n",
      "\titers: 200, epoch: 2 | loss: 0.3579375\n",
      "\tspeed: 0.0426s/iter; left time: 93.4300s\n",
      "Epoch: 2 running time: 0.1949563185373942 min.\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.3267081 Vali Loss: 0.7091760 Test Loss: 0.6095570\n",
      "Validation loss decreased (0.731810 --> 0.709176).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2849426\n",
      "\tspeed: 0.1109s/iter; left time: 224.9408s\n",
      "\titers: 200, epoch: 3 | loss: 0.3405750\n",
      "\tspeed: 0.0456s/iter; left time: 88.0161s\n",
      "Epoch: 3 running time: 0.2094225565592448 min.\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.2812904 Vali Loss: 0.7478794 Test Loss: 0.7363982\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3145991\n",
      "\tspeed: 0.1043s/iter; left time: 183.8408s\n",
      "\titers: 200, epoch: 4 | loss: 0.2818594\n",
      "\tspeed: 0.0432s/iter; left time: 71.7648s\n",
      "Epoch: 4 running time: 0.1930421034495036 min.\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.2808226 Vali Loss: 0.7154712 Test Loss: 0.7434014\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2638505\n",
      "\tspeed: 0.1022s/iter; left time: 152.9238s\n",
      "\titers: 200, epoch: 5 | loss: 0.2780595\n",
      "\tspeed: 0.0413s/iter; left time: 57.6388s\n",
      "Epoch: 5 running time: 0.19026060104370118 min.\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.2794391 Vali Loss: 0.7288657 Test Loss: 0.7669990\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
      "test shape: (2848, 24, 7) (2848, 24, 7)\n",
      "mse:0.7681576013565063, mae:0.663279116153717\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationary Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take ETTh1 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Nonstationary_Transformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.4032110\n",
      "\tspeed: 0.0425s/iter; left time: 108.7739s\n",
      "\titers: 200, epoch: 1 | loss: 0.2413542\n",
      "\tspeed: 0.0303s/iter; left time: 74.4774s\n",
      "Epoch: 1 running time: 0.14791139761606853 min.\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.3329230 Vali Loss: 0.5292587 Test Loss: 0.4346158\n",
      "Validation loss decreased (inf --> 0.529259).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2218050\n",
      "\tspeed: 0.0818s/iter; left time: 187.7564s\n",
      "\titers: 200, epoch: 2 | loss: 0.1747543\n",
      "\tspeed: 0.0303s/iter; left time: 66.6066s\n",
      "Epoch: 2 running time: 0.13974119822184244 min.\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.2002304 Vali Loss: 0.5843881 Test Loss: 0.4940403\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1968130\n",
      "\tspeed: 0.0798s/iter; left time: 161.9372s\n",
      "\titers: 200, epoch: 3 | loss: 0.1974325\n",
      "\tspeed: 0.0314s/iter; left time: 60.5634s\n",
      "Epoch: 3 running time: 0.14282973607381186 min.\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.2029117 Vali Loss: 0.5480784 Test Loss: 0.5344843\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1831751\n",
      "\tspeed: 0.0798s/iter; left time: 140.6710s\n",
      "\titers: 200, epoch: 4 | loss: 0.2052629\n",
      "\tspeed: 0.0306s/iter; left time: 50.8891s\n",
      "Epoch: 4 running time: 0.13883942763010662 min.\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.2075438 Vali Loss: 0.5416115 Test Loss: 0.4574961\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Nonstationary_Transformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
      "test shape: (2848, 24, 7) (2848, 24, 7)\n",
      "mse:0.4574960470199585, mae:0.4423450231552124\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Nonstationary_Transformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.3820704\n",
      "\tspeed: 0.0333s/iter; left time: 85.2836s\n",
      "\titers: 200, epoch: 1 | loss: 0.3020872\n",
      "\tspeed: 0.0304s/iter; left time: 74.7169s\n",
      "Epoch: 1 running time: 0.14107250372568766 min.\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.3374152 Vali Loss: 0.5224627 Test Loss: 0.5286707\n",
      "Validation loss decreased (inf --> 0.522463).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2075538\n",
      "\tspeed: 0.0841s/iter; left time: 193.0941s\n",
      "\titers: 200, epoch: 2 | loss: 0.1820664\n",
      "\tspeed: 0.0315s/iter; left time: 69.0627s\n",
      "Epoch: 2 running time: 0.14458661079406737 min.\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.1996413 Vali Loss: 0.5388003 Test Loss: 0.4569951\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1800281\n",
      "\tspeed: 0.0846s/iter; left time: 171.5520s\n",
      "\titers: 200, epoch: 3 | loss: 0.2106483\n",
      "\tspeed: 0.0329s/iter; left time: 63.5486s\n",
      "Epoch: 3 running time: 0.15072846015294392 min.\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.2013317 Vali Loss: 0.5524824 Test Loss: 0.4881700\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1988516\n",
      "\tspeed: 0.0800s/iter; left time: 141.0401s\n",
      "\titers: 200, epoch: 4 | loss: 0.1926844\n",
      "\tspeed: 0.0303s/iter; left time: 50.3688s\n",
      "Epoch: 4 running time: 0.13976478974024456 min.\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.2065603 Vali Loss: 0.5147283 Test Loss: 0.4762118\n",
      "Validation loss decreased (0.522463 --> 0.514728).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1665130\n",
      "\tspeed: 0.0825s/iter; left time: 123.5455s\n",
      "\titers: 200, epoch: 5 | loss: 0.1787640\n",
      "\tspeed: 0.0306s/iter; left time: 42.7495s\n",
      "Epoch: 5 running time: 0.13996146917343139 min.\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.1806230 Vali Loss: 0.5275848 Test Loss: 0.4744250\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1520860\n",
      "\tspeed: 0.0804s/iter; left time: 98.9729s\n",
      "\titers: 200, epoch: 6 | loss: 0.1752529\n",
      "\tspeed: 0.0315s/iter; left time: 35.5976s\n",
      "Epoch: 6 running time: 0.14372676610946655 min.\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.1827901 Vali Loss: 0.5246465 Test Loss: 0.4702681\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1918269\n",
      "\tspeed: 0.0810s/iter; left time: 78.1443s\n",
      "\titers: 200, epoch: 7 | loss: 0.1843904\n",
      "\tspeed: 0.0310s/iter; left time: 26.8331s\n",
      "Epoch: 7 running time: 0.14251970052719115 min.\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.1839595 Vali Loss: 0.5199484 Test Loss: 0.4777840\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Nonstationary_Transformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
      "test shape: (2848, 24, 7) (2848, 24, 7)\n",
      "mse:0.4777839183807373, mae:0.4456425905227661\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Nonstationary_Transformer\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer custom dataloader\n",
    "## 2. Take custom dataloader (70/10/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 12049\n",
      "val 1729\n",
      "test 3457\n",
      "\titers: 100, epoch: 1 | loss: 0.3763643\n",
      "\tspeed: 0.0612s/iter; left time: 224.0905s\n",
      "\titers: 200, epoch: 1 | loss: 0.3861512\n",
      "\tspeed: 0.0426s/iter; left time: 151.6734s\n",
      "\titers: 300, epoch: 1 | loss: 0.3227999\n",
      "\tspeed: 0.0416s/iter; left time: 143.8868s\n",
      "Epoch: 1 running time: 0.279414435227712 min.\n",
      "Epoch: 1, Steps: 376 | Train Loss: 0.4270375 Vali Loss: 0.3774065 Test Loss: 0.5083119\n",
      "Validation loss decreased (inf --> 0.377407).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2685055\n",
      "\tspeed: 0.1075s/iter; left time: 353.1104s\n",
      "\titers: 200, epoch: 2 | loss: 0.3208056\n",
      "\tspeed: 0.0424s/iter; left time: 134.9305s\n",
      "\titers: 300, epoch: 2 | loss: 0.2855017\n",
      "\tspeed: 0.0419s/iter; left time: 129.1765s\n",
      "Epoch: 2 running time: 0.27010714213053383 min.\n",
      "Epoch: 2, Steps: 376 | Train Loss: 0.3087647 Vali Loss: 0.4340892 Test Loss: 0.5661222\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2973501\n",
      "\tspeed: 0.1050s/iter; left time: 305.5227s\n",
      "\titers: 200, epoch: 3 | loss: 0.3534546\n",
      "\tspeed: 0.0429s/iter; left time: 120.4476s\n",
      "\titers: 300, epoch: 3 | loss: 0.2793282\n",
      "\tspeed: 0.0428s/iter; left time: 116.0800s\n",
      "Epoch: 3 running time: 0.27220173279444376 min.\n",
      "Epoch: 3, Steps: 376 | Train Loss: 0.3058979 Vali Loss: 0.3791290 Test Loss: 0.5009486\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2573522\n",
      "\tspeed: 0.1041s/iter; left time: 263.7696s\n",
      "\titers: 200, epoch: 4 | loss: 0.3572222\n",
      "\tspeed: 0.0418s/iter; left time: 101.6115s\n",
      "\titers: 300, epoch: 4 | loss: 0.2734126\n",
      "\tspeed: 0.0419s/iter; left time: 97.7239s\n",
      "Epoch: 4 running time: 0.2683360457420349 min.\n",
      "Epoch: 4, Steps: 376 | Train Loss: 0.3056111 Vali Loss: 0.3939413 Test Loss: 0.5136320\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 3457\n",
      "test shape: (108, 32, 24, 7) (108, 32, 24, 7)\n",
      "test shape: (3456, 24, 7) (3456, 24, 7)\n",
      "mse:0.5130395889282227, mae:0.5022826790809631\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 12049\n",
      "val 1729\n",
      "test 3457\n",
      "\titers: 100, epoch: 1 | loss: 0.5019252\n",
      "\tspeed: 0.0449s/iter; left time: 164.3238s\n",
      "\titers: 200, epoch: 1 | loss: 0.4028457\n",
      "\tspeed: 0.0426s/iter; left time: 151.6056s\n",
      "\titers: 300, epoch: 1 | loss: 0.3882335\n",
      "\tspeed: 0.0418s/iter; left time: 144.8355s\n",
      "Epoch: 1 running time: 0.27097949186960857 min.\n",
      "Epoch: 1, Steps: 376 | Train Loss: 0.4198319 Vali Loss: 0.3992383 Test Loss: 0.5310426\n",
      "Validation loss decreased (inf --> 0.399238).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2972021\n",
      "\tspeed: 0.1146s/iter; left time: 376.3312s\n",
      "\titers: 200, epoch: 2 | loss: 0.2698340\n",
      "\tspeed: 0.0432s/iter; left time: 137.5767s\n",
      "\titers: 300, epoch: 2 | loss: 0.2805445\n",
      "\tspeed: 0.0426s/iter; left time: 131.3819s\n",
      "Epoch: 2 running time: 0.2731630007425944 min.\n",
      "Epoch: 2, Steps: 376 | Train Loss: 0.3083024 Vali Loss: 0.4084678 Test Loss: 0.5384812\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3182303\n",
      "\tspeed: 0.1047s/iter; left time: 304.4933s\n",
      "\titers: 200, epoch: 3 | loss: 0.3282709\n",
      "\tspeed: 0.0417s/iter; left time: 117.2655s\n",
      "\titers: 300, epoch: 3 | loss: 0.3526133\n",
      "\tspeed: 0.0419s/iter; left time: 113.5572s\n",
      "Epoch: 3 running time: 0.26793367862701417 min.\n",
      "Epoch: 3, Steps: 376 | Train Loss: 0.3042798 Vali Loss: 0.4125516 Test Loss: 0.5289951\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2659876\n",
      "\tspeed: 0.1046s/iter; left time: 264.9956s\n",
      "\titers: 200, epoch: 4 | loss: 0.2697616\n",
      "\tspeed: 0.0427s/iter; left time: 103.9628s\n",
      "\titers: 300, epoch: 4 | loss: 0.2760201\n",
      "\tspeed: 0.0421s/iter; left time: 98.3315s\n",
      "Epoch: 4 running time: 0.2722969969113668 min.\n",
      "Epoch: 4, Steps: 376 | Train Loss: 0.3038059 Vali Loss: 0.3869793 Test Loss: 0.5084223\n",
      "Validation loss decreased (0.399238 --> 0.386979).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3010871\n",
      "\tspeed: 0.1086s/iter; left time: 234.3525s\n",
      "\titers: 200, epoch: 5 | loss: 0.2351170\n",
      "\tspeed: 0.0417s/iter; left time: 85.8335s\n",
      "\titers: 300, epoch: 5 | loss: 0.2761097\n",
      "\tspeed: 0.0417s/iter; left time: 81.5498s\n",
      "Epoch: 5 running time: 0.26801210244496665 min.\n",
      "Epoch: 5, Steps: 376 | Train Loss: 0.2840520 Vali Loss: 0.4014679 Test Loss: 0.5266926\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2871152\n",
      "\tspeed: 0.1043s/iter; left time: 185.7647s\n",
      "\titers: 200, epoch: 6 | loss: 0.2237932\n",
      "\tspeed: 0.0432s/iter; left time: 72.6943s\n",
      "\titers: 300, epoch: 6 | loss: 0.2746004\n",
      "\tspeed: 0.0430s/iter; left time: 67.9642s\n",
      "Epoch: 6 running time: 0.2726265986760457 min.\n",
      "Epoch: 6, Steps: 376 | Train Loss: 0.2833494 Vali Loss: 0.3722942 Test Loss: 0.4993932\n",
      "Validation loss decreased (0.386979 --> 0.372294).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2468189\n",
      "\tspeed: 0.1086s/iter; left time: 152.5703s\n",
      "\titers: 200, epoch: 7 | loss: 0.2441047\n",
      "\tspeed: 0.0424s/iter; left time: 55.2674s\n",
      "\titers: 300, epoch: 7 | loss: 0.3101768\n",
      "\tspeed: 0.0421s/iter; left time: 50.6903s\n",
      "Epoch: 7 running time: 0.2693111658096313 min.\n",
      "Epoch: 7, Steps: 376 | Train Loss: 0.2786444 Vali Loss: 0.3816466 Test Loss: 0.5018377\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3210349\n",
      "\tspeed: 0.1044s/iter; left time: 107.4601s\n",
      "\titers: 200, epoch: 8 | loss: 0.2475036\n",
      "\tspeed: 0.0426s/iter; left time: 39.5969s\n",
      "\titers: 300, epoch: 8 | loss: 0.2491158\n",
      "\tspeed: 0.0420s/iter; left time: 34.8248s\n",
      "Epoch: 8 running time: 0.2719109932581584 min.\n",
      "Epoch: 8, Steps: 376 | Train Loss: 0.2780708 Vali Loss: 0.3791690 Test Loss: 0.5050891\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.3423782\n",
      "\tspeed: 0.1048s/iter; left time: 68.4461s\n",
      "\titers: 200, epoch: 9 | loss: 0.3209715\n",
      "\tspeed: 0.0427s/iter; left time: 23.6237s\n",
      "\titers: 300, epoch: 9 | loss: 0.2941921\n",
      "\tspeed: 0.0425s/iter; left time: 19.2650s\n",
      "Epoch: 9 running time: 0.269273833433787 min.\n",
      "Epoch: 9, Steps: 376 | Train Loss: 0.2785888 Vali Loss: 0.3869219 Test Loss: 0.5090706\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 3457\n",
      "test shape: (108, 32, 24, 7) (108, 32, 24, 7)\n",
      "test shape: (3456, 24, 7) (3456, 24, 7)\n",
      "mse:0.5089989900588989, mae:0.5047451853752136\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 12049\n",
      "val 1729\n",
      "test 3457\n",
      "\titers: 100, epoch: 1 | loss: 0.3439233\n",
      "\tspeed: 0.0499s/iter; left time: 182.7363s\n",
      "\titers: 200, epoch: 1 | loss: 0.2601243\n",
      "\tspeed: 0.0298s/iter; left time: 105.9431s\n",
      "\titers: 300, epoch: 1 | loss: 0.2160712\n",
      "\tspeed: 0.0297s/iter; left time: 102.7219s\n",
      "Epoch: 1 running time: 0.20175397793451946 min.\n",
      "Epoch: 1, Steps: 376 | Train Loss: 0.3064217 Vali Loss: 0.3423961 Test Loss: 0.4702770\n",
      "Validation loss decreased (inf --> 0.342396).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2497098\n",
      "\tspeed: 0.0836s/iter; left time: 274.5945s\n",
      "\titers: 200, epoch: 2 | loss: 0.1729961\n",
      "\tspeed: 0.0315s/iter; left time: 100.4086s\n",
      "\titers: 300, epoch: 2 | loss: 0.1423871\n",
      "\tspeed: 0.0301s/iter; left time: 92.9549s\n",
      "Epoch: 2 running time: 0.1974997361501058 min.\n",
      "Epoch: 2, Steps: 376 | Train Loss: 0.1893995 Vali Loss: 0.4065197 Test Loss: 0.5507415\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2167805\n",
      "\tspeed: 0.0814s/iter; left time: 236.8709s\n",
      "\titers: 200, epoch: 3 | loss: 0.2130049\n",
      "\tspeed: 0.0303s/iter; left time: 85.0882s\n",
      "\titers: 300, epoch: 3 | loss: 0.1654096\n",
      "\tspeed: 0.0303s/iter; left time: 82.1661s\n",
      "Epoch: 3 running time: 0.19456719557444255 min.\n",
      "Epoch: 3, Steps: 376 | Train Loss: 0.1892117 Vali Loss: 0.4059986 Test Loss: 0.6194497\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1725365\n",
      "\tspeed: 0.0812s/iter; left time: 205.7403s\n",
      "\titers: 200, epoch: 4 | loss: 0.2271558\n",
      "\tspeed: 0.0303s/iter; left time: 73.7655s\n",
      "\titers: 300, epoch: 4 | loss: 0.1790380\n",
      "\tspeed: 0.0300s/iter; left time: 70.0968s\n",
      "Epoch: 4 running time: 0.19361739953358967 min.\n",
      "Epoch: 4, Steps: 376 | Train Loss: 0.1917897 Vali Loss: 0.3334178 Test Loss: 0.4570390\n",
      "Validation loss decreased (0.342396 --> 0.333418).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1877303\n",
      "\tspeed: 0.0832s/iter; left time: 179.4736s\n",
      "\titers: 200, epoch: 5 | loss: 0.1639780\n",
      "\tspeed: 0.0305s/iter; left time: 62.8043s\n",
      "\titers: 300, epoch: 5 | loss: 0.1463934\n",
      "\tspeed: 0.0303s/iter; left time: 59.3492s\n",
      "Epoch: 5 running time: 0.1951754848162333 min.\n",
      "Epoch: 5, Steps: 376 | Train Loss: 0.1702654 Vali Loss: 0.3556059 Test Loss: 0.4975217\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1795468\n",
      "\tspeed: 0.0817s/iter; left time: 145.5653s\n",
      "\titers: 200, epoch: 6 | loss: 0.1762074\n",
      "\tspeed: 0.0303s/iter; left time: 50.9467s\n",
      "\titers: 300, epoch: 6 | loss: 0.1572039\n",
      "\tspeed: 0.0305s/iter; left time: 48.1724s\n",
      "Epoch: 6 running time: 0.19580333630243937 min.\n",
      "Epoch: 6, Steps: 376 | Train Loss: 0.1721736 Vali Loss: 0.3426207 Test Loss: 0.4705714\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1834904\n",
      "\tspeed: 0.0813s/iter; left time: 114.2165s\n",
      "\titers: 200, epoch: 7 | loss: 0.1909758\n",
      "\tspeed: 0.0303s/iter; left time: 39.6003s\n",
      "\titers: 300, epoch: 7 | loss: 0.1599865\n",
      "\tspeed: 0.0304s/iter; left time: 36.6373s\n",
      "Epoch: 7 running time: 0.19557210604349773 min.\n",
      "Epoch: 7, Steps: 376 | Train Loss: 0.1725239 Vali Loss: 0.3443522 Test Loss: 0.4725480\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 3457\n",
      "test shape: (108, 32, 24, 7) (108, 32, 24, 7)\n",
      "test shape: (3456, 24, 7) (3456, 24, 7)\n",
      "mse:0.4725480079650879, mae:0.4540554881095886\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 12049\n",
      "val 1729\n",
      "test 3457\n",
      "\titers: 100, epoch: 1 | loss: 0.4283315\n",
      "\tspeed: 0.0333s/iter; left time: 121.8596s\n",
      "\titers: 200, epoch: 1 | loss: 0.2863144\n",
      "\tspeed: 0.0303s/iter; left time: 107.9274s\n",
      "\titers: 300, epoch: 1 | loss: 0.2357405\n",
      "\tspeed: 0.0303s/iter; left time: 104.9565s\n",
      "Epoch: 1 running time: 0.19587277968724567 min.\n",
      "Epoch: 1, Steps: 376 | Train Loss: 0.3107746 Vali Loss: 0.3486830 Test Loss: 0.4519618\n",
      "Validation loss decreased (inf --> 0.348683).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1853765\n",
      "\tspeed: 0.0914s/iter; left time: 300.2315s\n",
      "\titers: 200, epoch: 2 | loss: 0.1758094\n",
      "\tspeed: 0.0303s/iter; left time: 96.5739s\n",
      "\titers: 300, epoch: 2 | loss: 0.1572566\n",
      "\tspeed: 0.0303s/iter; left time: 93.5269s\n",
      "Epoch: 2 running time: 0.19562453031539917 min.\n",
      "Epoch: 2, Steps: 376 | Train Loss: 0.1883700 Vali Loss: 0.3887401 Test Loss: 0.5013038\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1982545\n",
      "\tspeed: 0.0819s/iter; left time: 238.2690s\n",
      "\titers: 200, epoch: 3 | loss: 0.1620250\n",
      "\tspeed: 0.0305s/iter; left time: 85.5994s\n",
      "\titers: 300, epoch: 3 | loss: 0.1444402\n",
      "\tspeed: 0.0303s/iter; left time: 82.2053s\n",
      "Epoch: 3 running time: 0.1960221250851949 min.\n",
      "Epoch: 3, Steps: 376 | Train Loss: 0.1863801 Vali Loss: 0.3318806 Test Loss: 0.4623333\n",
      "Validation loss decreased (0.348683 --> 0.331881).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1407222\n",
      "\tspeed: 0.0845s/iter; left time: 214.1112s\n",
      "\titers: 200, epoch: 4 | loss: 0.1465210\n",
      "\tspeed: 0.0299s/iter; left time: 72.8648s\n",
      "\titers: 300, epoch: 4 | loss: 0.1446191\n",
      "\tspeed: 0.0303s/iter; left time: 70.7334s\n",
      "Epoch: 4 running time: 0.1930636207262675 min.\n",
      "Epoch: 4, Steps: 376 | Train Loss: 0.1518662 Vali Loss: 0.3590532 Test Loss: 0.4636130\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1559750\n",
      "\tspeed: 0.0821s/iter; left time: 177.1619s\n",
      "\titers: 200, epoch: 5 | loss: 0.1581355\n",
      "\tspeed: 0.0305s/iter; left time: 62.7827s\n",
      "\titers: 300, epoch: 5 | loss: 0.1456286\n",
      "\tspeed: 0.0303s/iter; left time: 59.3351s\n",
      "Epoch: 5 running time: 0.1960824489593506 min.\n",
      "Epoch: 5, Steps: 376 | Train Loss: 0.1528051 Vali Loss: 0.3554018 Test Loss: 0.4566228\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1734525\n",
      "\tspeed: 0.0821s/iter; left time: 146.2516s\n",
      "\titers: 200, epoch: 6 | loss: 0.1547164\n",
      "\tspeed: 0.0305s/iter; left time: 51.3322s\n",
      "\titers: 300, epoch: 6 | loss: 0.1589387\n",
      "\tspeed: 0.0306s/iter; left time: 48.3050s\n",
      "Epoch: 6 running time: 0.19663306474685668 min.\n",
      "Epoch: 6, Steps: 376 | Train Loss: 0.1539220 Vali Loss: 0.3506889 Test Loss: 0.4660089\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 3457\n",
      "test shape: (108, 32, 24, 7) (108, 32, 24, 7)\n",
      "test shape: (3456, 24, 7) (3456, 24, 7)\n",
      "mse:0.46600884199142456, mae:0.460347056388855\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Nonstationary_Transformer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.3649494\n",
      "\tspeed: 0.0503s/iter; left time: 468.2175s\n",
      "\titers: 200, epoch: 1 | loss: 0.2716310\n",
      "\tspeed: 0.0294s/iter; left time: 270.5235s\n",
      "\titers: 300, epoch: 1 | loss: 0.2221682\n",
      "\tspeed: 0.0303s/iter; left time: 275.7582s\n",
      "\titers: 400, epoch: 1 | loss: 0.2238553\n",
      "\tspeed: 0.0300s/iter; left time: 270.0177s\n",
      "\titers: 500, epoch: 1 | loss: 0.1720495\n",
      "\tspeed: 0.0298s/iter; left time: 265.8888s\n",
      "\titers: 600, epoch: 1 | loss: 0.1457593\n",
      "\tspeed: 0.0324s/iter; left time: 285.8974s\n",
      "\titers: 700, epoch: 1 | loss: 0.2382891\n",
      "\tspeed: 0.0324s/iter; left time: 282.3693s\n",
      "\titers: 800, epoch: 1 | loss: 0.1409248\n",
      "\tspeed: 0.0331s/iter; left time: 285.1973s\n",
      "\titers: 900, epoch: 1 | loss: 0.1688729\n",
      "\tspeed: 0.0336s/iter; left time: 286.2149s\n",
      "Epoch: 1 running time: 0.5064127008120219 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2424076 Vali Loss: 0.2439631 Test Loss: 0.3741519\n",
      "Validation loss decreased (inf --> 0.243963).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1753821\n",
      "\tspeed: 0.1057s/iter; left time: 884.6645s\n",
      "\titers: 200, epoch: 2 | loss: 0.1914740\n",
      "\tspeed: 0.0324s/iter; left time: 268.0927s\n",
      "\titers: 300, epoch: 2 | loss: 0.2191539\n",
      "\tspeed: 0.0336s/iter; left time: 274.1695s\n",
      "\titers: 400, epoch: 2 | loss: 0.1353402\n",
      "\tspeed: 0.0337s/iter; left time: 271.7329s\n",
      "\titers: 500, epoch: 2 | loss: 0.1659905\n",
      "\tspeed: 0.0336s/iter; left time: 268.1286s\n",
      "\titers: 600, epoch: 2 | loss: 0.1339594\n",
      "\tspeed: 0.0329s/iter; left time: 258.8487s\n",
      "\titers: 700, epoch: 2 | loss: 0.1474677\n",
      "\tspeed: 0.0328s/iter; left time: 254.6432s\n",
      "\titers: 800, epoch: 2 | loss: 0.1287255\n",
      "\tspeed: 0.0329s/iter; left time: 252.1326s\n",
      "\titers: 900, epoch: 2 | loss: 0.1421609\n",
      "\tspeed: 0.0328s/iter; left time: 248.0537s\n",
      "Epoch: 2 running time: 0.5277679999669392 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1599604 Vali Loss: 0.2334695 Test Loss: 0.4192858\n",
      "Validation loss decreased (0.243963 --> 0.233470).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1820426\n",
      "\tspeed: 0.1076s/iter; left time: 799.1201s\n",
      "\titers: 200, epoch: 3 | loss: 0.0856391\n",
      "\tspeed: 0.0332s/iter; left time: 243.4229s\n",
      "\titers: 300, epoch: 3 | loss: 0.1498381\n",
      "\tspeed: 0.0336s/iter; left time: 242.9904s\n",
      "\titers: 400, epoch: 3 | loss: 0.1241259\n",
      "\tspeed: 0.0336s/iter; left time: 239.5445s\n",
      "\titers: 500, epoch: 3 | loss: 0.1229033\n",
      "\tspeed: 0.0334s/iter; left time: 235.0287s\n",
      "\titers: 600, epoch: 3 | loss: 0.1329610\n",
      "\tspeed: 0.0328s/iter; left time: 227.0273s\n",
      "\titers: 700, epoch: 3 | loss: 0.1320104\n",
      "\tspeed: 0.0326s/iter; left time: 222.4082s\n",
      "\titers: 800, epoch: 3 | loss: 0.0782566\n",
      "\tspeed: 0.0324s/iter; left time: 218.3261s\n",
      "\titers: 900, epoch: 3 | loss: 0.1005376\n",
      "\tspeed: 0.0329s/iter; left time: 218.1353s\n",
      "Epoch: 3 running time: 0.5213340163230896 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1187469 Vali Loss: 0.2380019 Test Loss: 0.3915616\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1256370\n",
      "\tspeed: 0.0989s/iter; left time: 641.3418s\n",
      "\titers: 200, epoch: 4 | loss: 0.1112389\n",
      "\tspeed: 0.0343s/iter; left time: 219.1239s\n",
      "\titers: 300, epoch: 4 | loss: 0.1441912\n",
      "\tspeed: 0.0337s/iter; left time: 211.8790s\n",
      "\titers: 400, epoch: 4 | loss: 0.1018379\n",
      "\tspeed: 0.0339s/iter; left time: 209.9072s\n",
      "\titers: 500, epoch: 4 | loss: 0.1053373\n",
      "\tspeed: 0.0329s/iter; left time: 200.3589s\n",
      "\titers: 600, epoch: 4 | loss: 0.0954452\n",
      "\tspeed: 0.0330s/iter; left time: 197.5662s\n",
      "\titers: 700, epoch: 4 | loss: 0.0715525\n",
      "\tspeed: 0.0320s/iter; left time: 188.1421s\n",
      "\titers: 800, epoch: 4 | loss: 0.1012617\n",
      "\tspeed: 0.0333s/iter; left time: 192.5038s\n",
      "\titers: 900, epoch: 4 | loss: 0.0891881\n",
      "\tspeed: 0.0335s/iter; left time: 190.6165s\n",
      "Epoch: 4 running time: 0.5262671311696371 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1154948 Vali Loss: 0.2312799 Test Loss: 0.4190405\n",
      "Validation loss decreased (0.233470 --> 0.231280).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1281390\n",
      "\tspeed: 0.1026s/iter; left time: 569.2892s\n",
      "\titers: 200, epoch: 5 | loss: 0.1241634\n",
      "\tspeed: 0.0303s/iter; left time: 164.9879s\n",
      "\titers: 300, epoch: 5 | loss: 0.0919050\n",
      "\tspeed: 0.0303s/iter; left time: 162.2099s\n",
      "\titers: 400, epoch: 5 | loss: 0.1062234\n",
      "\tspeed: 0.0303s/iter; left time: 159.0870s\n",
      "\titers: 500, epoch: 5 | loss: 0.0997440\n",
      "\tspeed: 0.0303s/iter; left time: 155.9860s\n",
      "\titers: 600, epoch: 5 | loss: 0.0978204\n",
      "\tspeed: 0.0303s/iter; left time: 153.0221s\n",
      "\titers: 700, epoch: 5 | loss: 0.0917097\n",
      "\tspeed: 0.0304s/iter; left time: 150.4778s\n",
      "\titers: 800, epoch: 5 | loss: 0.0869811\n",
      "\tspeed: 0.0306s/iter; left time: 148.4950s\n",
      "\titers: 900, epoch: 5 | loss: 0.1033960\n",
      "\tspeed: 0.0303s/iter; left time: 143.9864s\n",
      "Epoch: 5 running time: 0.48328140179316204 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.0988040 Vali Loss: 0.2375574 Test Loss: 0.4368379\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0996406\n",
      "\tspeed: 0.0965s/iter; left time: 444.4113s\n",
      "\titers: 200, epoch: 6 | loss: 0.0902802\n",
      "\tspeed: 0.0303s/iter; left time: 136.6363s\n",
      "\titers: 300, epoch: 6 | loss: 0.0818197\n",
      "\tspeed: 0.0303s/iter; left time: 133.6743s\n",
      "\titers: 400, epoch: 6 | loss: 0.1253380\n",
      "\tspeed: 0.0303s/iter; left time: 130.5699s\n",
      "\titers: 500, epoch: 6 | loss: 0.1042850\n",
      "\tspeed: 0.0304s/iter; left time: 127.9766s\n",
      "\titers: 600, epoch: 6 | loss: 0.0933647\n",
      "\tspeed: 0.0301s/iter; left time: 123.4081s\n",
      "\titers: 700, epoch: 6 | loss: 0.1299158\n",
      "\tspeed: 0.0299s/iter; left time: 119.7504s\n",
      "\titers: 800, epoch: 6 | loss: 0.0768160\n",
      "\tspeed: 0.0294s/iter; left time: 114.9645s\n",
      "\titers: 900, epoch: 6 | loss: 0.0879112\n",
      "\tspeed: 0.0304s/iter; left time: 115.8675s\n",
      "Epoch: 6 running time: 0.478377103805542 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.0988807 Vali Loss: 0.2350616 Test Loss: 0.4207011\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1003239\n",
      "\tspeed: 0.0961s/iter; left time: 352.2474s\n",
      "\titers: 200, epoch: 7 | loss: 0.0638872\n",
      "\tspeed: 0.0303s/iter; left time: 108.0994s\n",
      "\titers: 300, epoch: 7 | loss: 0.0900476\n",
      "\tspeed: 0.0303s/iter; left time: 105.0876s\n",
      "\titers: 400, epoch: 7 | loss: 0.1184118\n",
      "\tspeed: 0.0303s/iter; left time: 102.0316s\n",
      "\titers: 500, epoch: 7 | loss: 0.1026696\n",
      "\tspeed: 0.0303s/iter; left time: 99.0204s\n",
      "\titers: 600, epoch: 7 | loss: 0.0926221\n",
      "\tspeed: 0.0303s/iter; left time: 96.0009s\n",
      "\titers: 700, epoch: 7 | loss: 0.1023391\n",
      "\tspeed: 0.0303s/iter; left time: 93.0041s\n",
      "\titers: 800, epoch: 7 | loss: 0.0725346\n",
      "\tspeed: 0.0303s/iter; left time: 89.9605s\n",
      "\titers: 900, epoch: 7 | loss: 0.1009707\n",
      "\tspeed: 0.0303s/iter; left time: 86.8875s\n",
      "Epoch: 7 running time: 0.4796355684598287 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.0992355 Vali Loss: 0.2335942 Test Loss: 0.4185007\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_2_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.4185006320476532, mae:0.3560675084590912\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.2731359\n",
      "\tspeed: 0.0339s/iter; left time: 315.4005s\n",
      "\titers: 200, epoch: 1 | loss: 0.1884836\n",
      "\tspeed: 0.0297s/iter; left time: 273.8726s\n",
      "\titers: 300, epoch: 1 | loss: 0.2569596\n",
      "\tspeed: 0.0304s/iter; left time: 276.6298s\n",
      "\titers: 400, epoch: 1 | loss: 0.1859279\n",
      "\tspeed: 0.0303s/iter; left time: 273.2460s\n",
      "\titers: 500, epoch: 1 | loss: 0.2273879\n",
      "\tspeed: 0.0303s/iter; left time: 270.2642s\n",
      "\titers: 600, epoch: 1 | loss: 0.2375675\n",
      "\tspeed: 0.0305s/iter; left time: 268.3951s\n",
      "\titers: 700, epoch: 1 | loss: 0.1646313\n",
      "\tspeed: 0.0303s/iter; left time: 263.9768s\n",
      "\titers: 800, epoch: 1 | loss: 0.1446381\n",
      "\tspeed: 0.0303s/iter; left time: 261.1127s\n",
      "\titers: 900, epoch: 1 | loss: 0.1685647\n",
      "\tspeed: 0.0303s/iter; left time: 258.0836s\n",
      "Epoch: 1 running time: 0.4821917176246643 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2409703 Vali Loss: 0.2562571 Test Loss: 0.4007153\n",
      "Validation loss decreased (inf --> 0.256257).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1871088\n",
      "\tspeed: 0.1068s/iter; left time: 894.1316s\n",
      "\titers: 200, epoch: 2 | loss: 0.2135085\n",
      "\tspeed: 0.0303s/iter; left time: 250.9347s\n",
      "\titers: 300, epoch: 2 | loss: 0.1674735\n",
      "\tspeed: 0.0303s/iter; left time: 247.9313s\n",
      "\titers: 400, epoch: 2 | loss: 0.1612284\n",
      "\tspeed: 0.0304s/iter; left time: 245.6190s\n",
      "\titers: 500, epoch: 2 | loss: 0.1620193\n",
      "\tspeed: 0.0303s/iter; left time: 241.8593s\n",
      "\titers: 600, epoch: 2 | loss: 0.1387914\n",
      "\tspeed: 0.0305s/iter; left time: 239.6551s\n",
      "\titers: 700, epoch: 2 | loss: 0.1594162\n",
      "\tspeed: 0.0299s/iter; left time: 232.2265s\n",
      "\titers: 800, epoch: 2 | loss: 0.1239896\n",
      "\tspeed: 0.0294s/iter; left time: 225.6907s\n",
      "\titers: 900, epoch: 2 | loss: 0.1156637\n",
      "\tspeed: 0.0294s/iter; left time: 222.7096s\n",
      "Epoch: 2 running time: 0.4779095451037089 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1588787 Vali Loss: 0.2401303 Test Loss: 0.4074917\n",
      "Validation loss decreased (0.256257 --> 0.240130).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1163753\n",
      "\tspeed: 0.1032s/iter; left time: 766.8405s\n",
      "\titers: 200, epoch: 3 | loss: 0.1547621\n",
      "\tspeed: 0.0303s/iter; left time: 222.0651s\n",
      "\titers: 300, epoch: 3 | loss: 0.1538612\n",
      "\tspeed: 0.0295s/iter; left time: 213.3517s\n",
      "\titers: 400, epoch: 3 | loss: 0.0704265\n",
      "\tspeed: 0.0294s/iter; left time: 209.6549s\n",
      "\titers: 500, epoch: 3 | loss: 0.0968177\n",
      "\tspeed: 0.0299s/iter; left time: 210.4290s\n",
      "\titers: 600, epoch: 3 | loss: 0.0919446\n",
      "\tspeed: 0.0303s/iter; left time: 210.1643s\n",
      "\titers: 700, epoch: 3 | loss: 0.1079663\n",
      "\tspeed: 0.0305s/iter; left time: 208.1828s\n",
      "\titers: 800, epoch: 3 | loss: 0.1015540\n",
      "\tspeed: 0.0304s/iter; left time: 204.7625s\n",
      "\titers: 900, epoch: 3 | loss: 0.0926717\n",
      "\tspeed: 0.0295s/iter; left time: 195.7463s\n",
      "Epoch: 3 running time: 0.47651477257410685 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1153157 Vali Loss: 0.2490718 Test Loss: 0.4367783\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0979401\n",
      "\tspeed: 0.0980s/iter; left time: 635.5748s\n",
      "\titers: 200, epoch: 4 | loss: 0.1521533\n",
      "\tspeed: 0.0306s/iter; left time: 195.4904s\n",
      "\titers: 300, epoch: 4 | loss: 0.1329225\n",
      "\tspeed: 0.0303s/iter; left time: 190.7201s\n",
      "\titers: 400, epoch: 4 | loss: 0.1141531\n",
      "\tspeed: 0.0303s/iter; left time: 187.7609s\n",
      "\titers: 500, epoch: 4 | loss: 0.1457931\n",
      "\tspeed: 0.0303s/iter; left time: 184.5392s\n",
      "\titers: 600, epoch: 4 | loss: 0.1227031\n",
      "\tspeed: 0.0303s/iter; left time: 181.4883s\n",
      "\titers: 700, epoch: 4 | loss: 0.0860027\n",
      "\tspeed: 0.0303s/iter; left time: 178.6051s\n",
      "\titers: 800, epoch: 4 | loss: 0.1590327\n",
      "\tspeed: 0.0303s/iter; left time: 175.3989s\n",
      "\titers: 900, epoch: 4 | loss: 0.1059401\n",
      "\tspeed: 0.0304s/iter; left time: 172.7476s\n",
      "Epoch: 4 running time: 0.48279032707214353 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1137171 Vali Loss: 0.2371623 Test Loss: 0.4192760\n",
      "Validation loss decreased (0.240130 --> 0.237162).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0769886\n",
      "\tspeed: 0.1017s/iter; left time: 564.1747s\n",
      "\titers: 200, epoch: 5 | loss: 0.0864938\n",
      "\tspeed: 0.0304s/iter; left time: 165.6313s\n",
      "\titers: 300, epoch: 5 | loss: 0.0888699\n",
      "\tspeed: 0.0303s/iter; left time: 162.2192s\n",
      "\titers: 400, epoch: 5 | loss: 0.1183761\n",
      "\tspeed: 0.0303s/iter; left time: 159.1135s\n",
      "\titers: 500, epoch: 5 | loss: 0.1200526\n",
      "\tspeed: 0.0303s/iter; left time: 155.9431s\n",
      "\titers: 600, epoch: 5 | loss: 0.1035982\n",
      "\tspeed: 0.0303s/iter; left time: 153.0473s\n",
      "\titers: 700, epoch: 5 | loss: 0.1336754\n",
      "\tspeed: 0.0303s/iter; left time: 150.0730s\n",
      "\titers: 800, epoch: 5 | loss: 0.0880266\n",
      "\tspeed: 0.0303s/iter; left time: 146.9583s\n",
      "\titers: 900, epoch: 5 | loss: 0.0900785\n",
      "\tspeed: 0.0303s/iter; left time: 144.0543s\n",
      "Epoch: 5 running time: 0.48159571886062624 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.0957260 Vali Loss: 0.2335084 Test Loss: 0.4423597\n",
      "Validation loss decreased (0.237162 --> 0.233508).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0882687\n",
      "\tspeed: 0.1015s/iter; left time: 467.4357s\n",
      "\titers: 200, epoch: 6 | loss: 0.0867047\n",
      "\tspeed: 0.0299s/iter; left time: 134.8391s\n",
      "\titers: 300, epoch: 6 | loss: 0.0696609\n",
      "\tspeed: 0.0310s/iter; left time: 136.7227s\n",
      "\titers: 400, epoch: 6 | loss: 0.0715951\n",
      "\tspeed: 0.0332s/iter; left time: 142.7629s\n",
      "\titers: 500, epoch: 6 | loss: 0.0637181\n",
      "\tspeed: 0.0386s/iter; left time: 162.3155s\n",
      "\titers: 600, epoch: 6 | loss: 0.1058032\n",
      "\tspeed: 0.0348s/iter; left time: 143.0405s\n",
      "\titers: 700, epoch: 6 | loss: 0.0872834\n",
      "\tspeed: 0.0306s/iter; left time: 122.5258s\n",
      "\titers: 800, epoch: 6 | loss: 0.0809019\n",
      "\tspeed: 0.0302s/iter; left time: 117.9903s\n",
      "\titers: 900, epoch: 6 | loss: 0.1025396\n",
      "\tspeed: 0.0303s/iter; left time: 115.1590s\n",
      "Epoch: 6 running time: 0.5082146445910136 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.0875614 Vali Loss: 0.2392100 Test Loss: 0.4456812\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1046127\n",
      "\tspeed: 0.0995s/iter; left time: 364.5205s\n",
      "\titers: 200, epoch: 7 | loss: 0.1214155\n",
      "\tspeed: 0.0303s/iter; left time: 108.1273s\n",
      "\titers: 300, epoch: 7 | loss: 0.0855777\n",
      "\tspeed: 0.0303s/iter; left time: 105.0931s\n",
      "\titers: 400, epoch: 7 | loss: 0.0945399\n",
      "\tspeed: 0.0303s/iter; left time: 102.1035s\n",
      "\titers: 500, epoch: 7 | loss: 0.0991366\n",
      "\tspeed: 0.0306s/iter; left time: 100.0204s\n",
      "\titers: 600, epoch: 7 | loss: 0.0934048\n",
      "\tspeed: 0.0308s/iter; left time: 97.3238s\n",
      "\titers: 700, epoch: 7 | loss: 0.0785186\n",
      "\tspeed: 0.0303s/iter; left time: 92.9964s\n",
      "\titers: 800, epoch: 7 | loss: 0.0857852\n",
      "\tspeed: 0.0305s/iter; left time: 90.3474s\n",
      "\titers: 900, epoch: 7 | loss: 0.0888223\n",
      "\tspeed: 0.0304s/iter; left time: 87.0131s\n",
      "Epoch: 7 running time: 0.483351735273997 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.0877508 Vali Loss: 0.2424996 Test Loss: 0.4491255\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0739731\n",
      "\tspeed: 0.0988s/iter; left time: 269.1609s\n",
      "\titers: 200, epoch: 8 | loss: 0.0761647\n",
      "\tspeed: 0.0304s/iter; left time: 79.7980s\n",
      "\titers: 300, epoch: 8 | loss: 0.0789619\n",
      "\tspeed: 0.0303s/iter; left time: 76.5358s\n",
      "\titers: 400, epoch: 8 | loss: 0.0796092\n",
      "\tspeed: 0.0303s/iter; left time: 73.4503s\n",
      "\titers: 500, epoch: 8 | loss: 0.0788658\n",
      "\tspeed: 0.0303s/iter; left time: 70.5057s\n",
      "\titers: 600, epoch: 8 | loss: 0.0703726\n",
      "\tspeed: 0.0303s/iter; left time: 67.4465s\n",
      "\titers: 700, epoch: 8 | loss: 0.0866863\n",
      "\tspeed: 0.0306s/iter; left time: 65.0544s\n",
      "\titers: 800, epoch: 8 | loss: 0.0786046\n",
      "\tspeed: 0.0303s/iter; left time: 61.3869s\n",
      "\titers: 900, epoch: 8 | loss: 0.0898354\n",
      "\tspeed: 0.0303s/iter; left time: 58.3469s\n",
      "Epoch: 8 running time: 0.4826579054196676 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.0881281 Vali Loss: 0.2373293 Test Loss: 0.4422513\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_2_Nonstationary_Transformer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.4422512650489807, mae:0.3639376163482666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 2 \\\n",
    "  --model \"Nonstationary_Transformer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4307346\n",
      "\tspeed: 0.0616s/iter; left time: 573.7312s\n",
      "\titers: 200, epoch: 1 | loss: 0.3016012\n",
      "\tspeed: 0.0413s/iter; left time: 380.0273s\n",
      "\titers: 300, epoch: 1 | loss: 0.2942957\n",
      "\tspeed: 0.0410s/iter; left time: 373.1251s\n",
      "\titers: 400, epoch: 1 | loss: 0.2671357\n",
      "\tspeed: 0.0411s/iter; left time: 370.2236s\n",
      "\titers: 500, epoch: 1 | loss: 0.2435258\n",
      "\tspeed: 0.0425s/iter; left time: 378.4515s\n",
      "\titers: 600, epoch: 1 | loss: 0.2217858\n",
      "\tspeed: 0.0416s/iter; left time: 366.8701s\n",
      "\titers: 700, epoch: 1 | loss: 0.2125417\n",
      "\tspeed: 0.0419s/iter; left time: 365.0124s\n",
      "\titers: 800, epoch: 1 | loss: 0.1906348\n",
      "\tspeed: 0.0425s/iter; left time: 365.8513s\n",
      "\titers: 900, epoch: 1 | loss: 0.2647261\n",
      "\tspeed: 0.0418s/iter; left time: 355.5297s\n",
      "Epoch: 1 running time: 0.6710273027420044 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3047330 Vali Loss: 0.2980494 Test Loss: 0.4696968\n",
      "Validation loss decreased (inf --> 0.298049).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1999377\n",
      "\tspeed: 0.1154s/iter; left time: 965.9133s\n",
      "\titers: 200, epoch: 2 | loss: 0.2411886\n",
      "\tspeed: 0.0317s/iter; left time: 262.3150s\n",
      "\titers: 300, epoch: 2 | loss: 0.1997966\n",
      "\tspeed: 0.0317s/iter; left time: 258.7489s\n",
      "\titers: 400, epoch: 2 | loss: 0.2423015\n",
      "\tspeed: 0.0317s/iter; left time: 255.7888s\n",
      "\titers: 500, epoch: 2 | loss: 0.2103719\n",
      "\tspeed: 0.0317s/iter; left time: 252.5366s\n",
      "\titers: 600, epoch: 2 | loss: 0.2181772\n",
      "\tspeed: 0.0317s/iter; left time: 249.1341s\n",
      "\titers: 700, epoch: 2 | loss: 0.1343314\n",
      "\tspeed: 0.0317s/iter; left time: 246.6029s\n",
      "\titers: 800, epoch: 2 | loss: 0.1816992\n",
      "\tspeed: 0.0317s/iter; left time: 242.8006s\n",
      "\titers: 900, epoch: 2 | loss: 0.2507245\n",
      "\tspeed: 0.0318s/iter; left time: 241.0987s\n",
      "Epoch: 2 running time: 0.5101820588111877 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2009419 Vali Loss: 0.2374241 Test Loss: 0.3929982\n",
      "Validation loss decreased (0.298049 --> 0.237424).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1378951\n",
      "\tspeed: 0.1243s/iter; left time: 923.4387s\n",
      "\titers: 200, epoch: 3 | loss: 0.1840836\n",
      "\tspeed: 0.0421s/iter; left time: 308.7552s\n",
      "\titers: 300, epoch: 3 | loss: 0.1439371\n",
      "\tspeed: 0.0417s/iter; left time: 301.4029s\n",
      "\titers: 400, epoch: 3 | loss: 0.1691880\n",
      "\tspeed: 0.0411s/iter; left time: 292.6991s\n",
      "\titers: 500, epoch: 3 | loss: 0.2097439\n",
      "\tspeed: 0.0414s/iter; left time: 291.1982s\n",
      "\titers: 600, epoch: 3 | loss: 0.1543123\n",
      "\tspeed: 0.0416s/iter; left time: 288.4141s\n",
      "\titers: 700, epoch: 3 | loss: 0.1390960\n",
      "\tspeed: 0.0417s/iter; left time: 284.5239s\n",
      "\titers: 800, epoch: 3 | loss: 0.1357650\n",
      "\tspeed: 0.0422s/iter; left time: 283.8093s\n",
      "\titers: 900, epoch: 3 | loss: 0.1640382\n",
      "\tspeed: 0.0416s/iter; left time: 275.9675s\n",
      "Epoch: 3 running time: 0.6603500644365946 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1671244 Vali Loss: 0.2403744 Test Loss: 0.3891172\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1505874\n",
      "\tspeed: 0.1201s/iter; left time: 779.1247s\n",
      "\titers: 200, epoch: 4 | loss: 0.1424225\n",
      "\tspeed: 0.0420s/iter; left time: 268.3882s\n",
      "\titers: 300, epoch: 4 | loss: 0.2556826\n",
      "\tspeed: 0.0422s/iter; left time: 265.0678s\n",
      "\titers: 400, epoch: 4 | loss: 0.1678195\n",
      "\tspeed: 0.0414s/iter; left time: 256.4716s\n",
      "\titers: 500, epoch: 4 | loss: 0.2357220\n",
      "\tspeed: 0.0416s/iter; left time: 253.0795s\n",
      "\titers: 600, epoch: 4 | loss: 0.2023128\n",
      "\tspeed: 0.0417s/iter; left time: 249.8521s\n",
      "\titers: 700, epoch: 4 | loss: 0.1220490\n",
      "\tspeed: 0.0428s/iter; left time: 251.9283s\n",
      "\titers: 800, epoch: 4 | loss: 0.1661522\n",
      "\tspeed: 0.0420s/iter; left time: 242.8434s\n",
      "\titers: 900, epoch: 4 | loss: 0.1767304\n",
      "\tspeed: 0.0417s/iter; left time: 237.1997s\n",
      "Epoch: 4 running time: 0.661781394481659 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1644363 Vali Loss: 0.2195735 Test Loss: 0.3726886\n",
      "Validation loss decreased (0.237424 --> 0.219574).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1773134\n",
      "\tspeed: 0.1244s/iter; left time: 689.9702s\n",
      "\titers: 200, epoch: 5 | loss: 0.1255830\n",
      "\tspeed: 0.0416s/iter; left time: 226.6639s\n",
      "\titers: 300, epoch: 5 | loss: 0.1193427\n",
      "\tspeed: 0.0416s/iter; left time: 222.6041s\n",
      "\titers: 400, epoch: 5 | loss: 0.1698360\n",
      "\tspeed: 0.0416s/iter; left time: 218.2126s\n",
      "\titers: 500, epoch: 5 | loss: 0.1337725\n",
      "\tspeed: 0.0423s/iter; left time: 217.7648s\n",
      "\titers: 600, epoch: 5 | loss: 0.1658793\n",
      "\tspeed: 0.0413s/iter; left time: 208.2563s\n",
      "\titers: 700, epoch: 5 | loss: 0.1875621\n",
      "\tspeed: 0.0417s/iter; left time: 206.4398s\n",
      "\titers: 800, epoch: 5 | loss: 0.1225298\n",
      "\tspeed: 0.0412s/iter; left time: 199.7934s\n",
      "\titers: 900, epoch: 5 | loss: 0.1603958\n",
      "\tspeed: 0.0418s/iter; left time: 198.5854s\n",
      "Epoch: 5 running time: 0.6587576349576314 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1532387 Vali Loss: 0.2136416 Test Loss: 0.3710597\n",
      "Validation loss decreased (0.219574 --> 0.213642).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1444450\n",
      "\tspeed: 0.1242s/iter; left time: 572.2034s\n",
      "\titers: 200, epoch: 6 | loss: 0.1285003\n",
      "\tspeed: 0.0416s/iter; left time: 187.6034s\n",
      "\titers: 300, epoch: 6 | loss: 0.1322310\n",
      "\tspeed: 0.0418s/iter; left time: 183.9824s\n",
      "\titers: 400, epoch: 6 | loss: 0.1218058\n",
      "\tspeed: 0.0422s/iter; left time: 181.8759s\n",
      "\titers: 500, epoch: 6 | loss: 0.1354525\n",
      "\tspeed: 0.0410s/iter; left time: 172.5590s\n",
      "\titers: 600, epoch: 6 | loss: 0.1586091\n",
      "\tspeed: 0.0413s/iter; left time: 169.4818s\n",
      "\titers: 700, epoch: 6 | loss: 0.1131854\n",
      "\tspeed: 0.0412s/iter; left time: 164.9178s\n",
      "\titers: 800, epoch: 6 | loss: 0.1432222\n",
      "\tspeed: 0.0411s/iter; left time: 160.5055s\n",
      "\titers: 900, epoch: 6 | loss: 0.1418107\n",
      "\tspeed: 0.0413s/iter; left time: 157.1124s\n",
      "Epoch: 6 running time: 0.6554659406344095 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1480593 Vali Loss: 0.2241328 Test Loss: 0.3779618\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1416633\n",
      "\tspeed: 0.1198s/iter; left time: 439.1899s\n",
      "\titers: 200, epoch: 7 | loss: 0.1592746\n",
      "\tspeed: 0.0410s/iter; left time: 146.2238s\n",
      "\titers: 300, epoch: 7 | loss: 0.1746203\n",
      "\tspeed: 0.0411s/iter; left time: 142.5466s\n",
      "\titers: 400, epoch: 7 | loss: 0.1233166\n",
      "\tspeed: 0.0414s/iter; left time: 139.3104s\n",
      "\titers: 500, epoch: 7 | loss: 0.1567521\n",
      "\tspeed: 0.0412s/iter; left time: 134.6476s\n",
      "\titers: 600, epoch: 7 | loss: 0.1532848\n",
      "\tspeed: 0.0435s/iter; left time: 137.5686s\n",
      "\titers: 700, epoch: 7 | loss: 0.1331247\n",
      "\tspeed: 0.0422s/iter; left time: 129.2832s\n",
      "\titers: 800, epoch: 7 | loss: 0.1659495\n",
      "\tspeed: 0.0411s/iter; left time: 121.8576s\n",
      "\titers: 900, epoch: 7 | loss: 0.1159406\n",
      "\tspeed: 0.0419s/iter; left time: 120.1569s\n",
      "Epoch: 7 running time: 0.6590675234794616 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1475889 Vali Loss: 0.2195016 Test Loss: 0.3746070\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1614273\n",
      "\tspeed: 0.1219s/iter; left time: 332.1754s\n",
      "\titers: 200, epoch: 8 | loss: 0.1280121\n",
      "\tspeed: 0.0413s/iter; left time: 108.4573s\n",
      "\titers: 300, epoch: 8 | loss: 0.1291548\n",
      "\tspeed: 0.0419s/iter; left time: 105.8604s\n",
      "\titers: 400, epoch: 8 | loss: 0.1368238\n",
      "\tspeed: 0.0413s/iter; left time: 100.1597s\n",
      "\titers: 500, epoch: 8 | loss: 0.1330273\n",
      "\tspeed: 0.0414s/iter; left time: 96.1807s\n",
      "\titers: 600, epoch: 8 | loss: 0.1594829\n",
      "\tspeed: 0.0413s/iter; left time: 91.8717s\n",
      "\titers: 700, epoch: 8 | loss: 0.1077357\n",
      "\tspeed: 0.0413s/iter; left time: 87.6540s\n",
      "\titers: 800, epoch: 8 | loss: 0.1062815\n",
      "\tspeed: 0.0412s/iter; left time: 83.4712s\n",
      "\titers: 900, epoch: 8 | loss: 0.1380857\n",
      "\tspeed: 0.0412s/iter; left time: 79.3059s\n",
      "Epoch: 8 running time: 0.6559543490409852 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1471529 Vali Loss: 0.2222877 Test Loss: 0.3764268\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.37598633766174316, mae:0.36164385080337524\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4221524\n",
      "\tspeed: 0.0442s/iter; left time: 411.7057s\n",
      "\titers: 200, epoch: 1 | loss: 0.3022940\n",
      "\tspeed: 0.0412s/iter; left time: 379.4394s\n",
      "\titers: 300, epoch: 1 | loss: 0.3190881\n",
      "\tspeed: 0.0421s/iter; left time: 383.1333s\n",
      "\titers: 400, epoch: 1 | loss: 0.3538178\n",
      "\tspeed: 0.0420s/iter; left time: 378.4202s\n",
      "\titers: 500, epoch: 1 | loss: 0.2297910\n",
      "\tspeed: 0.0421s/iter; left time: 374.9146s\n",
      "\titers: 600, epoch: 1 | loss: 0.2097442\n",
      "\tspeed: 0.0418s/iter; left time: 368.3916s\n",
      "\titers: 700, epoch: 1 | loss: 0.2324205\n",
      "\tspeed: 0.0415s/iter; left time: 361.1435s\n",
      "\titers: 800, epoch: 1 | loss: 0.2425459\n",
      "\tspeed: 0.0415s/iter; left time: 357.6179s\n",
      "\titers: 900, epoch: 1 | loss: 0.3420942\n",
      "\tspeed: 0.0424s/iter; left time: 360.6927s\n",
      "Epoch: 1 running time: 0.662228238582611 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3092180 Vali Loss: 0.2832312 Test Loss: 0.4451061\n",
      "Validation loss decreased (inf --> 0.283231).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1899024\n",
      "\tspeed: 0.1294s/iter; left time: 1082.9657s\n",
      "\titers: 200, epoch: 2 | loss: 0.2064055\n",
      "\tspeed: 0.0418s/iter; left time: 345.3500s\n",
      "\titers: 300, epoch: 2 | loss: 0.2203534\n",
      "\tspeed: 0.0412s/iter; left time: 336.9017s\n",
      "\titers: 400, epoch: 2 | loss: 0.2075656\n",
      "\tspeed: 0.0418s/iter; left time: 337.2778s\n",
      "\titers: 500, epoch: 2 | loss: 0.1645342\n",
      "\tspeed: 0.0418s/iter; left time: 333.1141s\n",
      "\titers: 600, epoch: 2 | loss: 0.1748864\n",
      "\tspeed: 0.0417s/iter; left time: 328.0103s\n",
      "\titers: 700, epoch: 2 | loss: 0.1484652\n",
      "\tspeed: 0.0414s/iter; left time: 321.6127s\n",
      "\titers: 800, epoch: 2 | loss: 0.2393705\n",
      "\tspeed: 0.0407s/iter; left time: 312.5370s\n",
      "\titers: 900, epoch: 2 | loss: 0.1936065\n",
      "\tspeed: 0.0423s/iter; left time: 319.8519s\n",
      "Epoch: 2 running time: 0.660804537932078 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2024827 Vali Loss: 0.2371813 Test Loss: 0.3859681\n",
      "Validation loss decreased (0.283231 --> 0.237181).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1590785\n",
      "\tspeed: 0.1274s/iter; left time: 946.4960s\n",
      "\titers: 200, epoch: 3 | loss: 0.1916021\n",
      "\tspeed: 0.0428s/iter; left time: 313.8181s\n",
      "\titers: 300, epoch: 3 | loss: 0.1212917\n",
      "\tspeed: 0.0426s/iter; left time: 307.8706s\n",
      "\titers: 400, epoch: 3 | loss: 0.1645994\n",
      "\tspeed: 0.0426s/iter; left time: 303.8560s\n",
      "\titers: 500, epoch: 3 | loss: 0.1566514\n",
      "\tspeed: 0.0420s/iter; left time: 295.1585s\n",
      "\titers: 600, epoch: 3 | loss: 0.1633545\n",
      "\tspeed: 0.0416s/iter; left time: 288.2636s\n",
      "\titers: 700, epoch: 3 | loss: 0.1973340\n",
      "\tspeed: 0.0408s/iter; left time: 278.3610s\n",
      "\titers: 800, epoch: 3 | loss: 0.1457533\n",
      "\tspeed: 0.0414s/iter; left time: 278.2680s\n",
      "\titers: 900, epoch: 3 | loss: 0.1036521\n",
      "\tspeed: 0.0418s/iter; left time: 276.8518s\n",
      "Epoch: 3 running time: 0.6643726348876953 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1698548 Vali Loss: 0.2314968 Test Loss: 0.3865038\n",
      "Validation loss decreased (0.237181 --> 0.231497).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1836326\n",
      "\tspeed: 0.1256s/iter; left time: 814.9567s\n",
      "\titers: 200, epoch: 4 | loss: 0.1178099\n",
      "\tspeed: 0.0418s/iter; left time: 267.2141s\n",
      "\titers: 300, epoch: 4 | loss: 0.1697673\n",
      "\tspeed: 0.0415s/iter; left time: 261.0620s\n",
      "\titers: 400, epoch: 4 | loss: 0.1538090\n",
      "\tspeed: 0.0416s/iter; left time: 257.3929s\n",
      "\titers: 500, epoch: 4 | loss: 0.1281122\n",
      "\tspeed: 0.0416s/iter; left time: 253.5237s\n",
      "\titers: 600, epoch: 4 | loss: 0.1791037\n",
      "\tspeed: 0.0423s/iter; left time: 253.1407s\n",
      "\titers: 700, epoch: 4 | loss: 0.1650171\n",
      "\tspeed: 0.0424s/iter; left time: 249.7931s\n",
      "\titers: 800, epoch: 4 | loss: 0.2254826\n",
      "\tspeed: 0.0420s/iter; left time: 243.3851s\n",
      "\titers: 900, epoch: 4 | loss: 0.1532446\n",
      "\tspeed: 0.0425s/iter; left time: 241.9472s\n",
      "Epoch: 4 running time: 0.6654042363166809 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1541656 Vali Loss: 0.2206749 Test Loss: 0.3899144\n",
      "Validation loss decreased (0.231497 --> 0.220675).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1717126\n",
      "\tspeed: 0.1267s/iter; left time: 702.7097s\n",
      "\titers: 200, epoch: 5 | loss: 0.1353924\n",
      "\tspeed: 0.0421s/iter; left time: 229.2167s\n",
      "\titers: 300, epoch: 5 | loss: 0.1549057\n",
      "\tspeed: 0.0426s/iter; left time: 227.7206s\n",
      "\titers: 400, epoch: 5 | loss: 0.1590300\n",
      "\tspeed: 0.0430s/iter; left time: 225.5789s\n",
      "\titers: 500, epoch: 5 | loss: 0.0945694\n",
      "\tspeed: 0.0433s/iter; left time: 222.6866s\n",
      "\titers: 600, epoch: 5 | loss: 0.1322617\n",
      "\tspeed: 0.0417s/iter; left time: 210.3032s\n",
      "\titers: 700, epoch: 5 | loss: 0.1762114\n",
      "\tspeed: 0.0423s/iter; left time: 209.3977s\n",
      "\titers: 800, epoch: 5 | loss: 0.2514132\n",
      "\tspeed: 0.0423s/iter; left time: 205.2091s\n",
      "\titers: 900, epoch: 5 | loss: 0.1239216\n",
      "\tspeed: 0.0430s/iter; left time: 204.1813s\n",
      "Epoch: 5 running time: 0.6732069174448649 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1442522 Vali Loss: 0.2203114 Test Loss: 0.3810680\n",
      "Validation loss decreased (0.220675 --> 0.220311).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1286100\n",
      "\tspeed: 0.1268s/iter; left time: 583.9553s\n",
      "\titers: 200, epoch: 6 | loss: 0.1503711\n",
      "\tspeed: 0.0426s/iter; left time: 191.7948s\n",
      "\titers: 300, epoch: 6 | loss: 0.1217610\n",
      "\tspeed: 0.0420s/iter; left time: 185.2128s\n",
      "\titers: 400, epoch: 6 | loss: 0.1419614\n",
      "\tspeed: 0.0422s/iter; left time: 181.9072s\n",
      "\titers: 500, epoch: 6 | loss: 0.1411858\n",
      "\tspeed: 0.0413s/iter; left time: 173.6357s\n",
      "\titers: 600, epoch: 6 | loss: 0.1317008\n",
      "\tspeed: 0.0415s/iter; left time: 170.2887s\n",
      "\titers: 700, epoch: 6 | loss: 0.1172206\n",
      "\tspeed: 0.0424s/iter; left time: 169.9581s\n",
      "\titers: 800, epoch: 6 | loss: 0.1491046\n",
      "\tspeed: 0.0425s/iter; left time: 165.8261s\n",
      "\titers: 900, epoch: 6 | loss: 0.1402738\n",
      "\tspeed: 0.0426s/iter; left time: 161.9548s\n",
      "Epoch: 6 running time: 0.6652531226476034 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1389750 Vali Loss: 0.2172811 Test Loss: 0.3756799\n",
      "Validation loss decreased (0.220311 --> 0.217281).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1235027\n",
      "\tspeed: 0.1252s/iter; left time: 458.7720s\n",
      "\titers: 200, epoch: 7 | loss: 0.0993671\n",
      "\tspeed: 0.0421s/iter; left time: 150.1616s\n",
      "\titers: 300, epoch: 7 | loss: 0.1172262\n",
      "\tspeed: 0.0416s/iter; left time: 144.0720s\n",
      "\titers: 400, epoch: 7 | loss: 0.1247887\n",
      "\tspeed: 0.0426s/iter; left time: 143.4393s\n",
      "\titers: 500, epoch: 7 | loss: 0.1151194\n",
      "\tspeed: 0.0418s/iter; left time: 136.5916s\n",
      "\titers: 600, epoch: 7 | loss: 0.1377131\n",
      "\tspeed: 0.0423s/iter; left time: 133.7232s\n",
      "\titers: 700, epoch: 7 | loss: 0.1034992\n",
      "\tspeed: 0.0423s/iter; left time: 129.5021s\n",
      "\titers: 800, epoch: 7 | loss: 0.1134244\n",
      "\tspeed: 0.0426s/iter; left time: 126.1753s\n",
      "\titers: 900, epoch: 7 | loss: 0.1425553\n",
      "\tspeed: 0.0431s/iter; left time: 123.3686s\n",
      "Epoch: 7 running time: 0.6683483322461446 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1358188 Vali Loss: 0.2179322 Test Loss: 0.3747880\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1226921\n",
      "\tspeed: 0.1209s/iter; left time: 329.4202s\n",
      "\titers: 200, epoch: 8 | loss: 0.1632318\n",
      "\tspeed: 0.0416s/iter; left time: 109.1855s\n",
      "\titers: 300, epoch: 8 | loss: 0.0959632\n",
      "\tspeed: 0.0423s/iter; left time: 106.7425s\n",
      "\titers: 400, epoch: 8 | loss: 0.1597323\n",
      "\tspeed: 0.0368s/iter; left time: 89.1604s\n",
      "\titers: 500, epoch: 8 | loss: 0.1169432\n",
      "\tspeed: 0.0339s/iter; left time: 78.6710s\n",
      "\titers: 600, epoch: 8 | loss: 0.1297148\n",
      "\tspeed: 0.0329s/iter; left time: 73.1398s\n",
      "\titers: 700, epoch: 8 | loss: 0.1734155\n",
      "\tspeed: 0.0408s/iter; left time: 86.6240s\n",
      "\titers: 800, epoch: 8 | loss: 0.1339870\n",
      "\tspeed: 0.0383s/iter; left time: 77.5757s\n",
      "\titers: 900, epoch: 8 | loss: 0.1039812\n",
      "\tspeed: 0.0316s/iter; left time: 60.8665s\n",
      "Epoch: 8 running time: 0.5940584341684977 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1357500 Vali Loss: 0.2137352 Test Loss: 0.3758407\n",
      "Validation loss decreased (0.217281 --> 0.213735).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1299870\n",
      "\tspeed: 0.1221s/iter; left time: 217.6702s\n",
      "\titers: 200, epoch: 9 | loss: 0.1501092\n",
      "\tspeed: 0.0421s/iter; left time: 70.9275s\n",
      "\titers: 300, epoch: 9 | loss: 0.1522894\n",
      "\tspeed: 0.0429s/iter; left time: 67.9648s\n",
      "\titers: 400, epoch: 9 | loss: 0.1450945\n",
      "\tspeed: 0.0427s/iter; left time: 63.2971s\n",
      "\titers: 500, epoch: 9 | loss: 0.1441568\n",
      "\tspeed: 0.0428s/iter; left time: 59.1845s\n",
      "\titers: 600, epoch: 9 | loss: 0.1496249\n",
      "\tspeed: 0.0414s/iter; left time: 53.1212s\n",
      "\titers: 700, epoch: 9 | loss: 0.1019884\n",
      "\tspeed: 0.0424s/iter; left time: 50.1710s\n",
      "\titers: 800, epoch: 9 | loss: 0.1744356\n",
      "\tspeed: 0.0426s/iter; left time: 46.1024s\n",
      "\titers: 900, epoch: 9 | loss: 0.1388854\n",
      "\tspeed: 0.0415s/iter; left time: 40.7768s\n",
      "Epoch: 9 running time: 0.6706543604532877 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1345192 Vali Loss: 0.2141867 Test Loss: 0.3740620\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1065069\n",
      "\tspeed: 0.1227s/iter; left time: 103.2917s\n",
      "\titers: 200, epoch: 10 | loss: 0.1462813\n",
      "\tspeed: 0.0422s/iter; left time: 31.3443s\n",
      "\titers: 300, epoch: 10 | loss: 0.1166390\n",
      "\tspeed: 0.0425s/iter; left time: 27.2685s\n",
      "\titers: 400, epoch: 10 | loss: 0.1132310\n",
      "\tspeed: 0.0430s/iter; left time: 23.3031s\n",
      "\titers: 500, epoch: 10 | loss: 0.1061669\n",
      "\tspeed: 0.0424s/iter; left time: 18.7513s\n",
      "\titers: 600, epoch: 10 | loss: 0.1742484\n",
      "\tspeed: 0.0427s/iter; left time: 14.6165s\n",
      "\titers: 700, epoch: 10 | loss: 0.1317799\n",
      "\tspeed: 0.0430s/iter; left time: 10.4036s\n",
      "\titers: 800, epoch: 10 | loss: 0.1238614\n",
      "\tspeed: 0.0424s/iter; left time: 6.0166s\n",
      "\titers: 900, epoch: 10 | loss: 0.1375245\n",
      "\tspeed: 0.0424s/iter; left time: 1.7799s\n",
      "Epoch: 10 running time: 0.6735942403475443 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1345916 Vali Loss: 0.2165541 Test Loss: 0.3745810\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.3768013119697571, mae:0.3561086356639862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 2 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test size = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 8641\n",
      "test 4297\n",
      "\titers: 100, epoch: 1 | loss: 0.4307346\n",
      "\tspeed: 0.0659s/iter; left time: 613.9549s\n",
      "\titers: 200, epoch: 1 | loss: 0.3016012\n",
      "\tspeed: 0.0416s/iter; left time: 382.9985s\n",
      "\titers: 300, epoch: 1 | loss: 0.2942957\n",
      "\tspeed: 0.0433s/iter; left time: 394.7428s\n",
      "\titers: 400, epoch: 1 | loss: 0.2671357\n",
      "\tspeed: 0.0464s/iter; left time: 418.4276s\n",
      "\titers: 500, epoch: 1 | loss: 0.2435258\n",
      "\tspeed: 0.0450s/iter; left time: 400.9178s\n",
      "\titers: 600, epoch: 1 | loss: 0.2217858\n",
      "\tspeed: 0.0455s/iter; left time: 401.1528s\n",
      "\titers: 700, epoch: 1 | loss: 0.2125417\n",
      "\tspeed: 0.0454s/iter; left time: 395.8421s\n",
      "\titers: 800, epoch: 1 | loss: 0.1906348\n",
      "\tspeed: 0.0456s/iter; left time: 392.4626s\n",
      "\titers: 900, epoch: 1 | loss: 0.2647261\n",
      "\tspeed: 0.0457s/iter; left time: 388.5857s\n",
      "Epoch: 1 running time: 0.7219364841779073 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3047330 Vali Loss: 0.4236676 Test Loss: 0.3926425\n",
      "Validation loss decreased (inf --> 0.423668).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1999377\n",
      "\tspeed: 0.1295s/iter; left time: 1084.2032s\n",
      "\titers: 200, epoch: 2 | loss: 0.2411886\n",
      "\tspeed: 0.0456s/iter; left time: 376.9328s\n",
      "\titers: 300, epoch: 2 | loss: 0.1997966\n",
      "\tspeed: 0.0457s/iter; left time: 373.3974s\n",
      "\titers: 400, epoch: 2 | loss: 0.2423015\n",
      "\tspeed: 0.0457s/iter; left time: 368.6864s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 2 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test size=15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_2_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4307346\n",
      "\tspeed: 0.0617s/iter; left time: 574.1480s\n",
      "\titers: 200, epoch: 1 | loss: 0.3016012\n",
      "\tspeed: 0.0419s/iter; left time: 386.2439s\n",
      "\titers: 300, epoch: 1 | loss: 0.2942957\n",
      "\tspeed: 0.0414s/iter; left time: 376.7422s\n",
      "\titers: 400, epoch: 1 | loss: 0.2671357\n",
      "\tspeed: 0.0417s/iter; left time: 376.0170s\n",
      "\titers: 500, epoch: 1 | loss: 0.2435258\n",
      "\tspeed: 0.0418s/iter; left time: 372.3048s\n",
      "\titers: 600, epoch: 1 | loss: 0.2217858\n",
      "\tspeed: 0.0418s/iter; left time: 367.9268s\n",
      "\titers: 700, epoch: 1 | loss: 0.2125417\n",
      "\tspeed: 0.0418s/iter; left time: 364.1971s\n",
      "\titers: 800, epoch: 1 | loss: 0.1906348\n",
      "\tspeed: 0.0417s/iter; left time: 359.4593s\n",
      "\titers: 900, epoch: 1 | loss: 0.2647261\n",
      "\tspeed: 0.0415s/iter; left time: 353.4896s\n",
      "Epoch: 1 running time: 0.6700607419013977 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3047330 Vali Loss: 0.3466743 Test Loss: 0.4806783\n",
      "Validation loss decreased (inf --> 0.346674).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2324897\n",
      "\tspeed: 0.1342s/iter; left time: 1123.5569s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "!python -u /vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 2 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time LLM\n",
    "## small, 15%, 15%\n",
    "\n",
    "### torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 382.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 610.88 MiB is free. Process 42461 has 2.24 GiB memory in use. Process 20767 has 1.78 GiB memory in use. Process 10768 has 2.56 GiB memory in use. Process 42631 has 13.69 GiB memory in use. Process 47813 has 7.21 GiB memory in use. Including non-PyTorch memory, this process has 3.63 GiB memory in use. Of the allocated memory 2.62 GiB is allocated by PyTorch, and 338.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: Tesla V100-PCIE-32GB\n",
      "Device 1: Tesla V100-PCIE-32GB\n",
      "Device 2: Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the number of available CUDA devices\n",
    "num_devices = torch.cuda.device_count()\n",
    "\n",
    "# Loop through each device and print its name\n",
    "for i in range(num_devices):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 90363\n",
      "val 19443\n",
      "test 19371\n",
      "[2024-05-03 15:31:27,516] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-03 15:31:28,873] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-03 15:31:28,873] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-03 15:31:28,873] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-03 15:31:29,768] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.164, master_port=29500\n",
      "[2024-05-03 15:31:29,769] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-03 15:31:30,539] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-03 15:31:30,540] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-03 15:31:30,540] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-03 15:31:30,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-03 15:31:30,541] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-03 15:31:30,541] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-03 15:31:30,541] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-03 15:31:30,541] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-03 15:31:30,541] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-03 15:31:30,541] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-03 15:31:31,003] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-03 15:31:31,004] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:31:31,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 89.18 GB, percent = 11.8%\n",
      "[2024-05-03 15:31:31,143] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-03 15:31:31,143] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:31:31,144] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 89.22 GB, percent = 11.8%\n",
      "[2024-05-03 15:31:31,144] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-03 15:31:31,313] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-03 15:31:31,315] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:31:31,315] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 89.25 GB, percent = 11.8%\n",
      "[2024-05-03 15:31:31,316] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-03 15:31:31,316] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-03 15:31:31,316] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-03 15:31:31,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-03 15:31:31,317] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faa79743610>\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-03 15:31:31,318] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-03 15:31:31,319] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-03 15:31:31,320] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-03 15:31:31,321] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:15,  6.72it/s]\titers: 100, epoch: 1 | loss: 0.5468196\n",
      "\tspeed: 0.2003s/iter; left time: 734.1552s\n",
      "199it [00:30,  7.22it/s]\titers: 200, epoch: 1 | loss: 0.4214458\n",
      "\tspeed: 0.1453s/iter; left time: 518.0681s\n",
      "299it [00:45,  6.76it/s]\titers: 300, epoch: 1 | loss: 0.4011628\n",
      "\tspeed: 0.1504s/iter; left time: 521.3495s\n",
      "399it [01:00,  6.01it/s]\titers: 400, epoch: 1 | loss: 0.5255764\n",
      "\tspeed: 0.1471s/iter; left time: 495.2141s\n",
      "499it [01:14,  6.59it/s]\titers: 500, epoch: 1 | loss: 0.2808782\n",
      "\tspeed: 0.1478s/iter; left time: 482.6114s\n",
      "599it [01:30,  6.71it/s]\titers: 600, epoch: 1 | loss: 0.5445550\n",
      "\tspeed: 0.1548s/iter; left time: 490.0809s\n",
      "699it [01:45,  7.23it/s]\titers: 700, epoch: 1 | loss: 0.2627008\n",
      "\tspeed: 0.1484s/iter; left time: 454.8922s\n",
      "799it [02:00,  7.21it/s]\titers: 800, epoch: 1 | loss: 0.3114898\n",
      "\tspeed: 0.1523s/iter; left time: 451.8232s\n",
      "899it [02:16,  5.85it/s]\titers: 900, epoch: 1 | loss: 0.2767449\n",
      "\tspeed: 0.1640s/iter; left time: 470.0796s\n",
      "999it [02:32,  6.19it/s]\titers: 1000, epoch: 1 | loss: 0.2889755\n",
      "\tspeed: 0.1538s/iter; left time: 425.2956s\n",
      "1099it [02:47,  7.41it/s]\titers: 1100, epoch: 1 | loss: 0.2072158\n",
      "\tspeed: 0.1547s/iter; left time: 412.5457s\n",
      "1199it [03:02,  6.28it/s]\titers: 1200, epoch: 1 | loss: 0.1756039\n",
      "\tspeed: 0.1485s/iter; left time: 380.9509s\n",
      "1299it [03:17,  7.17it/s]\titers: 1300, epoch: 1 | loss: 0.1881783\n",
      "\tspeed: 0.1536s/iter; left time: 378.8370s\n",
      "1399it [03:33,  7.34it/s]\titers: 1400, epoch: 1 | loss: 0.2366187\n",
      "\tspeed: 0.1521s/iter; left time: 359.9760s\n",
      "1499it [03:47,  7.32it/s]\titers: 1500, epoch: 1 | loss: 0.4451137\n",
      "\tspeed: 0.1472s/iter; left time: 333.5190s\n",
      "1599it [04:02,  7.09it/s]\titers: 1600, epoch: 1 | loss: 0.4389757\n",
      "\tspeed: 0.1503s/iter; left time: 325.5284s\n",
      "1699it [04:17,  5.95it/s]\titers: 1700, epoch: 1 | loss: 0.2678942\n",
      "\tspeed: 0.1499s/iter; left time: 309.6764s\n",
      "1799it [04:32,  7.13it/s]\titers: 1800, epoch: 1 | loss: 0.2379295\n",
      "\tspeed: 0.1463s/iter; left time: 287.5315s\n",
      "1899it [04:47,  6.17it/s]\titers: 1900, epoch: 1 | loss: 0.1710269\n",
      "\tspeed: 0.1512s/iter; left time: 282.0733s\n",
      "1999it [05:02,  6.78it/s]\titers: 2000, epoch: 1 | loss: 0.1779455\n",
      "\tspeed: 0.1474s/iter; left time: 260.2522s\n",
      "2099it [05:17,  6.90it/s]\titers: 2100, epoch: 1 | loss: 0.2390940\n",
      "\tspeed: 0.1471s/iter; left time: 245.0705s\n",
      "2199it [05:32,  7.43it/s]\titers: 2200, epoch: 1 | loss: 0.4671702\n",
      "\tspeed: 0.1508s/iter; left time: 236.0952s\n",
      "2299it [05:45,  7.29it/s]\titers: 2300, epoch: 1 | loss: 0.2660964\n",
      "\tspeed: 0.1376s/iter; left time: 201.6830s\n",
      "2399it [06:00,  6.20it/s]\titers: 2400, epoch: 1 | loss: 0.4123612\n",
      "\tspeed: 0.1493s/iter; left time: 203.9067s\n",
      "2499it [06:14,  7.27it/s]\titers: 2500, epoch: 1 | loss: 0.1834874\n",
      "\tspeed: 0.1407s/iter; left time: 178.1314s\n",
      "2599it [06:28,  7.25it/s]\titers: 2600, epoch: 1 | loss: 0.2996213\n",
      "\tspeed: 0.1413s/iter; left time: 164.7838s\n",
      "2699it [06:44,  6.28it/s]\titers: 2700, epoch: 1 | loss: 0.4658841\n",
      "\tspeed: 0.1546s/iter; left time: 164.7644s\n",
      "2799it [07:00,  6.65it/s]\titers: 2800, epoch: 1 | loss: 0.2089916\n",
      "\tspeed: 0.1591s/iter; left time: 153.7254s\n",
      "2899it [07:15,  6.61it/s]\titers: 2900, epoch: 1 | loss: 0.3584861\n",
      "\tspeed: 0.1473s/iter; left time: 127.5209s\n",
      "2999it [07:30,  6.75it/s]\titers: 3000, epoch: 1 | loss: 0.3548172\n",
      "\tspeed: 0.1499s/iter; left time: 114.8503s\n",
      "3099it [07:44,  6.36it/s]\titers: 3100, epoch: 1 | loss: 0.3082316\n",
      "\tspeed: 0.1497s/iter; left time: 99.6967s\n",
      "3199it [07:59,  6.72it/s]\titers: 3200, epoch: 1 | loss: 0.4097749\n",
      "\tspeed: 0.1485s/iter; left time: 84.0279s\n",
      "3299it [08:15,  7.03it/s]\titers: 3300, epoch: 1 | loss: 0.2170641\n",
      "\tspeed: 0.1514s/iter; left time: 70.5686s\n",
      "3399it [08:29,  6.03it/s]\titers: 3400, epoch: 1 | loss: 0.4186157\n",
      "\tspeed: 0.1447s/iter; left time: 52.9657s\n",
      "3499it [08:44,  6.84it/s]\titers: 3500, epoch: 1 | loss: 0.2876329\n",
      "\tspeed: 0.1528s/iter; left time: 40.6424s\n",
      "3599it [08:59,  6.00it/s]\titers: 3600, epoch: 1 | loss: 0.3456867\n",
      "\tspeed: 0.1467s/iter; left time: 24.3444s\n",
      "3699it [09:14,  7.14it/s]\titers: 3700, epoch: 1 | loss: 0.2103202\n",
      "\tspeed: 0.1515s/iter; left time: 9.9983s\n",
      "3765it [09:24,  6.67it/s]\n",
      "Epoch: 1 cost time: 564.7272717952728\n",
      "810it [01:06, 12.18it/s]\n",
      "807it [01:04, 12.60it/s]\n",
      "Epoch: 1 | Train Loss: 0.3082688 Vali Loss: 0.3276033 Test Loss: 0.3916928 MAE Loss: 0.3958873\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n",
      "Total time: 11.988454163074493 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=1\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" /vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 90363\n",
      "val 19443\n",
      "test 19371\n",
      "[2024-05-03 15:44:30,610] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-03 15:44:31,493] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-03 15:44:31,493] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-03 15:44:31,493] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-03 15:44:32,485] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.164, master_port=29500\n",
      "[2024-05-03 15:44:32,486] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-03 15:44:33,736] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-03 15:44:33,738] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-03 15:44:33,738] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-03 15:44:33,740] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-03 15:44:33,740] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-03 15:44:33,740] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-03 15:44:33,740] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-03 15:44:33,740] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-03 15:44:33,740] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-03 15:44:33,740] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-03 15:44:34,329] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-03 15:44:34,331] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:44:34,331] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.33 GB, percent = 12.2%\n",
      "[2024-05-03 15:44:34,569] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-03 15:44:34,570] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.74 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:44:34,571] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.34 GB, percent = 12.2%\n",
      "[2024-05-03 15:44:34,571] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-03 15:44:34,813] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-03 15:44:34,814] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-03 15:44:34,815] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.33 GB, percent = 12.2%\n",
      "[2024-05-03 15:44:34,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-03 15:44:34,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-03 15:44:34,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-03 15:44:34,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-03 15:44:34,817] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-03 15:44:34,818] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-03 15:44:34,818] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-03 15:44:34,818] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-03 15:44:34,818] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5903c8c6d0>\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-03 15:44:34,819] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-03 15:44:34,820] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-03 15:44:34,821] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-03 15:44:34,822] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-03 15:44:34,822] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-03 15:44:34,822] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-03 15:44:34,822] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-03 15:44:34,822] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:24,  4.19it/s]\titers: 100, epoch: 1 | loss: 0.7356012\n",
      "\tspeed: 0.2908s/iter; left time: 1066.1519s\n",
      "199it [00:58,  3.10it/s]\titers: 200, epoch: 1 | loss: 0.5691991\n",
      "\tspeed: 0.3444s/iter; left time: 1228.1985s\n",
      "299it [01:32,  2.10it/s]\titers: 300, epoch: 1 | loss: 0.4520031\n",
      "\tspeed: 0.3429s/iter; left time: 1188.5837s\n",
      "399it [02:06,  2.39it/s]\titers: 400, epoch: 1 | loss: 0.6152174\n",
      "\tspeed: 0.3344s/iter; left time: 1125.4562s\n",
      "499it [02:39,  3.45it/s]\titers: 500, epoch: 1 | loss: 0.4286653\n",
      "\tspeed: 0.3363s/iter; left time: 1098.4060s\n",
      "599it [03:12,  3.34it/s]\titers: 600, epoch: 1 | loss: 0.2725918\n",
      "\tspeed: 0.3255s/iter; left time: 1030.4073s\n",
      "699it [03:46,  2.90it/s]\titers: 700, epoch: 1 | loss: 0.3188488\n",
      "\tspeed: 0.3411s/iter; left time: 1045.8324s\n",
      "799it [04:17,  3.43it/s]\titers: 800, epoch: 1 | loss: 0.5181361\n",
      "\tspeed: 0.3103s/iter; left time: 920.4743s\n",
      "899it [04:49,  3.36it/s]\titers: 900, epoch: 1 | loss: 0.1869062\n",
      "\tspeed: 0.3193s/iter; left time: 914.9958s\n",
      "999it [05:21,  2.09it/s]\titers: 1000, epoch: 1 | loss: 0.2172682\n",
      "\tspeed: 0.3165s/iter; left time: 875.3748s\n",
      "1099it [05:55,  3.02it/s]\titers: 1100, epoch: 1 | loss: 0.2820303\n",
      "\tspeed: 0.3483s/iter; left time: 928.5558s\n",
      "1199it [06:27,  2.91it/s]\titers: 1200, epoch: 1 | loss: 0.4319468\n",
      "\tspeed: 0.3125s/iter; left time: 801.8844s\n",
      "1299it [07:02,  2.85it/s]\titers: 1300, epoch: 1 | loss: 0.3310315\n",
      "\tspeed: 0.3570s/iter; left time: 880.3170s\n",
      "1399it [07:35,  2.92it/s]\titers: 1400, epoch: 1 | loss: 0.3464009\n",
      "\tspeed: 0.3294s/iter; left time: 779.4637s\n",
      "1499it [08:07,  3.00it/s]\titers: 1500, epoch: 1 | loss: 0.2595419\n",
      "\tspeed: 0.3195s/iter; left time: 724.0822s\n",
      "1599it [08:44,  2.77it/s]\titers: 1600, epoch: 1 | loss: 0.3122485\n",
      "\tspeed: 0.3714s/iter; left time: 804.4426s\n",
      "1699it [09:18,  2.33it/s]\titers: 1700, epoch: 1 | loss: 0.3014346\n",
      "\tspeed: 0.3356s/iter; left time: 693.3138s\n",
      "1799it [09:50,  3.38it/s]\titers: 1800, epoch: 1 | loss: 0.3493429\n",
      "\tspeed: 0.3214s/iter; left time: 631.9039s\n",
      "1899it [10:20,  3.62it/s]\titers: 1900, epoch: 1 | loss: 0.3084237\n",
      "\tspeed: 0.3020s/iter; left time: 563.6075s\n",
      "1999it [10:49,  3.60it/s]\titers: 2000, epoch: 1 | loss: 0.2826749\n",
      "\tspeed: 0.2837s/iter; left time: 500.9693s\n",
      "2099it [11:18,  3.05it/s]\titers: 2100, epoch: 1 | loss: 0.4879281\n",
      "\tspeed: 0.2969s/iter; left time: 494.6857s\n",
      "2199it [11:47,  3.41it/s]\titers: 2200, epoch: 1 | loss: 0.2046579\n",
      "\tspeed: 0.2897s/iter; left time: 453.7468s\n",
      "2299it [12:16,  4.14it/s]\titers: 2300, epoch: 1 | loss: 0.1233690\n",
      "\tspeed: 0.2890s/iter; left time: 423.7306s\n",
      "2399it [12:51,  3.40it/s]\titers: 2400, epoch: 1 | loss: 0.2604498\n",
      "\tspeed: 0.3438s/iter; left time: 469.6103s\n",
      "2499it [13:25,  3.11it/s]\titers: 2500, epoch: 1 | loss: 0.2310107\n",
      "\tspeed: 0.3405s/iter; left time: 431.0967s\n",
      "2599it [13:58,  2.98it/s]\titers: 2600, epoch: 1 | loss: 0.2021242\n",
      "\tspeed: 0.3349s/iter; left time: 390.4528s\n",
      "2699it [14:29,  3.37it/s]\titers: 2700, epoch: 1 | loss: 0.1879779\n",
      "\tspeed: 0.3108s/iter; left time: 331.2782s\n",
      "2799it [15:00,  3.76it/s]\titers: 2800, epoch: 1 | loss: 0.4332197\n",
      "\tspeed: 0.3074s/iter; left time: 296.9775s\n",
      "2899it [15:31,  3.10it/s]\titers: 2900, epoch: 1 | loss: 0.1966374\n",
      "\tspeed: 0.3056s/iter; left time: 264.6117s\n",
      "2999it [16:01,  2.53it/s]\titers: 3000, epoch: 1 | loss: 0.1856316\n",
      "\tspeed: 0.3012s/iter; left time: 230.6993s\n",
      "3099it [16:35,  3.33it/s]\titers: 3100, epoch: 1 | loss: 0.3469718\n",
      "\tspeed: 0.3414s/iter; left time: 227.3795s\n",
      "3199it [17:11,  3.12it/s]\titers: 3200, epoch: 1 | loss: 0.5812799\n",
      "\tspeed: 0.3665s/iter; left time: 207.4195s\n",
      "3299it [17:46,  3.17it/s]\titers: 3300, epoch: 1 | loss: 0.2726511\n",
      "\tspeed: 0.3405s/iter; left time: 158.6874s\n",
      "3399it [18:18,  3.11it/s]\titers: 3400, epoch: 1 | loss: 0.5794476\n",
      "\tspeed: 0.3238s/iter; left time: 118.5144s\n",
      "3499it [18:49,  3.54it/s]\titers: 3500, epoch: 1 | loss: 0.3040229\n",
      "\tspeed: 0.3134s/iter; left time: 83.3695s\n",
      "3599it [19:22,  2.81it/s]\titers: 3600, epoch: 1 | loss: 0.3292978\n",
      "\tspeed: 0.3332s/iter; left time: 55.3053s\n",
      "3699it [19:57,  2.28it/s]\titers: 3700, epoch: 1 | loss: 0.2165164\n",
      "\tspeed: 0.3475s/iter; left time: 22.9338s\n",
      "3765it [20:18,  3.09it/s]\n",
      "Epoch: 1 cost time: 1218.1102845668793\n",
      "810it [02:00,  6.70it/s]\n",
      "807it [02:14,  6.00it/s]\n",
      "Epoch: 1 | Train Loss: 0.3600279 Vali Loss: 0.3258024 Test Loss: 0.3997062 MAE Loss: 0.4031630\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n",
      "Total time: 25.003564433256784 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=1\n",
    "learning_rate=0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" /vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
