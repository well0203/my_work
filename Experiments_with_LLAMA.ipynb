{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small dff and d model, grad acc=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 3\n",
      "Device 0: NVIDIA A100 80GB PCIe\n",
      "Device 1: NVIDIA A100 80GB PCIe\n",
      "Device 2: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 16323\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:32<00:00, 16.06s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-19 17:31:07,184] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-19 17:31:08,133] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-19 17:31:08,133] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-19 17:31:08,133] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-19 17:31:09,229] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-05-19 17:31:09,229] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-19 17:31:21,281] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-19 17:31:21,283] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-19 17:31:21,283] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-19 17:31:21,286] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-19 17:31:21,286] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-19 17:31:21,286] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-19 17:31:21,286] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-19 17:31:21,286] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-19 17:31:21,286] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-19 17:31:21,286] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-19 17:31:21,645] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-19 17:31:21,646] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.6 GB         CA 12.61 GB         Max_CA 13 GB \n",
      "[2024-05-19 17:31:21,646] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.9 GB, percent = 22.9%\n",
      "[2024-05-19 17:31:21,798] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-19 17:31:21,799] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.67 GB         CA 12.74 GB         Max_CA 13 GB \n",
      "[2024-05-19 17:31:21,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 162.98 GB, percent = 21.6%\n",
      "[2024-05-19 17:31:21,799] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-19 17:31:21,949] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-19 17:31:21,950] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.54 GB         CA 12.74 GB         Max_CA 13 GB \n",
      "[2024-05-19 17:31:21,950] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 146.65 GB, percent = 19.4%\n",
      "[2024-05-19 17:31:21,951] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-19 17:31:21,951] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-19 17:31:21,951] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-19 17:31:21,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-19 17:31:21,952] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff4d84d30d0>\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-19 17:31:21,953] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 12\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-19 17:31:21,954] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   train_batch_size ............. 72\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  6\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-19 17:31:21,955] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 12, \n",
      "    \"train_batch_size\": 72, \n",
      "    \"train_micro_batch_size_per_gpu\": 6, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "2720it [15:46,  2.87it/s]\n",
      "Epoch: 1 cost time: 946.0945961475372\n",
      "3204it [08:44,  6.11it/s]\n",
      "3192it [08:40,  6.13it/s]\n",
      "Epoch: 1 | Train Loss: 0.7990270 Vali Loss: 0.5247439 Test Loss: 0.6784378 MAE Loss: 0.5595884\n",
      "lr = 0.0000000064\n",
      "Updating learning rate to 6.353085799523562e-09\n",
      "2720it [15:41,  2.89it/s]\n",
      "Epoch: 2 cost time: 941.1734478473663\n",
      "3204it [08:43,  6.12it/s]\n",
      "3192it [08:38,  6.15it/s]\n",
      "Epoch: 2 | Train Loss: 0.6710997 Vali Loss: 0.5246066 Test Loss: 0.6782700 MAE Loss: 0.5594953\n",
      "Updating learning rate to 3.176542899761781e-09\n",
      "2720it [15:47,  2.87it/s]\n",
      "Epoch: 3 cost time: 947.1261622905731\n",
      "3204it [08:41,  6.14it/s]\n",
      "3192it [08:40,  6.13it/s]\n",
      "Epoch: 3 | Train Loss: 0.6686619 Vali Loss: 0.5243272 Test Loss: 0.6782718 MAE Loss: 0.5594830\n",
      "Updating learning rate to 1.5882714498808905e-09\n",
      "2720it [15:42,  2.88it/s]\n",
      "Epoch: 4 cost time: 942.8947217464447\n",
      "3204it [08:41,  6.14it/s]\n",
      "3192it [08:39,  6.15it/s]\n",
      "Epoch: 4 | Train Loss: 0.6680728 Vali Loss: 0.5243337 Test Loss: 0.6782324 MAE Loss: 0.5594588\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.941357249404453e-10\n",
      "2500it [14:26,  2.71it/s]^C\n",
      "2500it [14:27,  2.88it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main_copy_copy.py\", line 208, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1976, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2051, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Total time: 157.27620307207107 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=6\n",
    "d_model=8\n",
    "d_ff=32\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main_copy_copy.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch accumulation only in zero script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 16323\n",
      "val 19227\n",
      "test 19155\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.33s/it]\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-19 20:10:12,135] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-19 20:10:13,035] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-19 20:10:13,035] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-19 20:10:13,035] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-19 20:10:13,968] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-05-19 20:10:13,968] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-19 20:10:25,650] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-19 20:10:25,652] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-19 20:10:25,652] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-19 20:10:25,655] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-19 20:10:25,655] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-19 20:10:25,655] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-19 20:10:25,655] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-19 20:10:25,655] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-19 20:10:25,655] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-19 20:10:25,655] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-19 20:10:26,037] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-19 20:10:26,038] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.63 GB         CA 12.63 GB         Max_CA 13 GB \n",
      "[2024-05-19 20:10:26,038] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 176.96 GB, percent = 23.5%\n",
      "[2024-05-19 20:10:26,214] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-19 20:10:26,215] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.7 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-19 20:10:26,216] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.68 GB, percent = 23.7%\n",
      "[2024-05-19 20:10:26,216] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-19 20:10:26,383] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-19 20:10:26,384] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.77 GB         Max_CA 13 GB \n",
      "[2024-05-19 20:10:26,384] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.15 GB, percent = 23.9%\n",
      "[2024-05-19 20:10:26,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-19 20:10:26,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-19 20:10:26,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-19 20:10:26,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-19 20:10:26,387] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-19 20:10:26,387] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-19 20:10:26,387] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d74cebc90>\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-19 20:10:26,388] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 12\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-19 20:10:26,389] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   train_batch_size ............. 72\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  6\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-19 20:10:26,390] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 12, \n",
      "    \"train_batch_size\": 72, \n",
      "    \"train_micro_batch_size_per_gpu\": 6, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:36,  2.72it/s]\titers: 100, epoch: 1 | loss: 0.8050584\n",
      "\tspeed: 0.5212s/iter; left time: 28300.5571s\n",
      "199it [01:09,  3.14it/s]\titers: 200, epoch: 1 | loss: 1.1475379\n",
      "\tspeed: 0.3285s/iter; left time: 17807.4217s\n",
      "299it [01:46,  2.69it/s]\titers: 300, epoch: 1 | loss: 0.8868881\n",
      "\tspeed: 0.3667s/iter; left time: 19837.6809s\n",
      "399it [02:20,  3.10it/s]\titers: 400, epoch: 1 | loss: 1.6485856\n",
      "\tspeed: 0.3353s/iter; left time: 18104.7161s\n",
      "499it [02:56,  2.66it/s]\titers: 500, epoch: 1 | loss: 0.8336833\n",
      "\tspeed: 0.3655s/iter; left time: 19699.5475s\n",
      "599it [03:30,  3.15it/s]\titers: 600, epoch: 1 | loss: 0.6093600\n",
      "\tspeed: 0.3354s/iter; left time: 18045.4004s\n",
      "699it [04:06,  2.70it/s]\titers: 700, epoch: 1 | loss: 1.8790340\n",
      "\tspeed: 0.3630s/iter; left time: 19492.4877s\n",
      "799it [04:40,  3.16it/s]\titers: 800, epoch: 1 | loss: 0.8916640\n",
      "\tspeed: 0.3368s/iter; left time: 18052.5524s\n",
      "899it [05:16,  2.70it/s]\titers: 900, epoch: 1 | loss: 0.9307731\n",
      "\tspeed: 0.3615s/iter; left time: 19338.6451s\n",
      "999it [05:50,  3.15it/s]\titers: 1000, epoch: 1 | loss: 0.9908145\n",
      "\tspeed: 0.3400s/iter; left time: 18155.3838s\n",
      "1099it [06:25,  2.66it/s]\titers: 1100, epoch: 1 | loss: 1.1385700\n",
      "\tspeed: 0.3528s/iter; left time: 18806.7830s\n",
      "1199it [07:00,  3.14it/s]\titers: 1200, epoch: 1 | loss: 0.6653945\n",
      "\tspeed: 0.3467s/iter; left time: 18446.1339s\n",
      "1299it [07:35,  2.64it/s]\titers: 1300, epoch: 1 | loss: 1.3656404\n",
      "\tspeed: 0.3508s/iter; left time: 18627.8473s\n",
      "1399it [08:10,  3.16it/s]\titers: 1400, epoch: 1 | loss: 2.1376164\n",
      "\tspeed: 0.3487s/iter; left time: 18478.9111s\n",
      "1499it [08:44,  2.71it/s]\titers: 1500, epoch: 1 | loss: 0.3603545\n",
      "\tspeed: 0.3453s/iter; left time: 18265.1916s\n",
      "1599it [09:20,  3.13it/s]\titers: 1600, epoch: 1 | loss: 1.2387872\n",
      "\tspeed: 0.3531s/iter; left time: 18646.6261s\n",
      "1699it [09:54,  2.69it/s]\titers: 1700, epoch: 1 | loss: 0.3630705\n",
      "\tspeed: 0.3443s/iter; left time: 18145.1836s\n",
      "1799it [10:29,  3.13it/s]\titers: 1800, epoch: 1 | loss: 0.6337727\n",
      "\tspeed: 0.3547s/iter; left time: 18658.4645s\n",
      "1899it [11:04,  2.71it/s]\titers: 1900, epoch: 1 | loss: 1.1487328\n",
      "\tspeed: 0.3414s/iter; left time: 17926.3539s\n",
      "1999it [11:39,  3.15it/s]\titers: 2000, epoch: 1 | loss: 0.5663310\n",
      "\tspeed: 0.3588s/iter; left time: 18801.8472s\n",
      "2099it [12:13,  2.70it/s]\titers: 2100, epoch: 1 | loss: 0.6323719\n",
      "\tspeed: 0.3376s/iter; left time: 17657.5280s\n",
      "2199it [12:49,  3.09it/s]\titers: 2200, epoch: 1 | loss: 0.7556834\n",
      "\tspeed: 0.3613s/iter; left time: 18859.0177s\n",
      "2299it [13:23,  2.70it/s]\titers: 2300, epoch: 1 | loss: 0.6515235\n",
      "\tspeed: 0.3378s/iter; left time: 17597.8467s\n",
      "2399it [13:59,  3.16it/s]\titers: 2400, epoch: 1 | loss: 0.4892376\n",
      "\tspeed: 0.3603s/iter; left time: 18737.5389s\n",
      "2499it [14:33,  2.70it/s]\titers: 2500, epoch: 1 | loss: 0.9829432\n",
      "\tspeed: 0.3345s/iter; left time: 17358.6820s\n",
      "2599it [15:09,  3.10it/s]\titers: 2600, epoch: 1 | loss: 0.9602507\n",
      "\tspeed: 0.3628s/iter; left time: 18793.9185s\n",
      "2699it [15:43,  2.70it/s]\titers: 2700, epoch: 1 | loss: 0.5522569\n",
      "\tspeed: 0.3377s/iter; left time: 17457.9619s\n",
      "2720it [15:51,  2.86it/s]\n",
      "Epoch: 1 cost time: 951.0345602035522\n",
      "3204it [08:40,  6.16it/s]\n",
      "3192it [08:40,  6.13it/s]\n",
      "Epoch: 1 | Train Loss: 1.0360728 Vali Loss: 0.6319737 Test Loss: 0.8146519 MAE Loss: 0.6482812\n",
      "Validation loss decreased (inf --> 0.631974).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:32,  2.74it/s]\titers: 100, epoch: 2 | loss: 0.6190321\n",
      "\tspeed: 11.7114s/iter; left time: 604086.3900s\n",
      "199it [01:09,  2.68it/s]\titers: 200, epoch: 2 | loss: 0.3381664\n",
      "\tspeed: 0.3734s/iter; left time: 19222.8932s\n",
      "299it [01:41,  2.90it/s]\titers: 300, epoch: 2 | loss: 0.4290878\n",
      "\tspeed: 0.3256s/iter; left time: 16730.0094s\n",
      "399it [02:19,  2.68it/s]\titers: 400, epoch: 2 | loss: 0.8702677\n",
      "\tspeed: 0.3749s/iter; left time: 19225.2181s\n",
      "499it [02:52,  3.13it/s]\titers: 500, epoch: 2 | loss: 1.3481412\n",
      "\tspeed: 0.3302s/iter; left time: 16899.8049s\n",
      "599it [03:29,  2.65it/s]\titers: 600, epoch: 2 | loss: 0.4988285\n",
      "\tspeed: 0.3685s/iter; left time: 18825.5724s\n",
      "699it [04:02,  3.14it/s]\titers: 700, epoch: 2 | loss: 0.4919096\n",
      "\tspeed: 0.3344s/iter; left time: 17048.7344s\n",
      "799it [04:38,  2.69it/s]\titers: 800, epoch: 2 | loss: 0.5587555\n",
      "\tspeed: 0.3614s/iter; left time: 18386.5875s\n",
      "899it [05:12,  3.10it/s]\titers: 900, epoch: 2 | loss: 1.4482762\n",
      "\tspeed: 0.3384s/iter; left time: 17184.0855s\n",
      "999it [05:48,  2.68it/s]\titers: 1000, epoch: 2 | loss: 0.3507511\n",
      "\tspeed: 0.3571s/iter; left time: 18099.4127s\n",
      "1099it [06:22,  3.10it/s]\titers: 1100, epoch: 2 | loss: 0.8050450\n",
      "\tspeed: 0.3432s/iter; left time: 17360.3138s\n",
      "1199it [06:57,  2.60it/s]\titers: 1200, epoch: 2 | loss: 0.7329844\n",
      "\tspeed: 0.3507s/iter; left time: 17702.7874s\n",
      "1299it [07:32,  3.13it/s]\titers: 1300, epoch: 2 | loss: 0.7444612\n",
      "\tspeed: 0.3485s/iter; left time: 17557.9863s\n",
      "1399it [08:07,  2.69it/s]\titers: 1400, epoch: 2 | loss: 0.6439669\n",
      "\tspeed: 0.3459s/iter; left time: 17393.6975s\n",
      "1499it [08:42,  3.14it/s]\titers: 1500, epoch: 2 | loss: 0.7714360\n",
      "\tspeed: 0.3551s/iter; left time: 17820.1943s\n",
      "1599it [09:16,  2.71it/s]\titers: 1600, epoch: 2 | loss: 0.5861552\n",
      "\tspeed: 0.3393s/iter; left time: 16991.0975s\n",
      "1699it [09:52,  3.11it/s]\titers: 1700, epoch: 2 | loss: 0.6138228\n",
      "\tspeed: 0.3625s/iter; left time: 18118.9222s\n",
      "1799it [10:25,  2.71it/s]\titers: 1800, epoch: 2 | loss: 0.5815929\n",
      "\tspeed: 0.3301s/iter; left time: 16463.7339s\n",
      "1899it [11:02,  3.08it/s]\titers: 1900, epoch: 2 | loss: 1.3181134\n",
      "\tspeed: 0.3697s/iter; left time: 18405.0516s\n",
      "1999it [11:35,  2.68it/s]\titers: 2000, epoch: 2 | loss: 0.7618001\n",
      "\tspeed: 0.3265s/iter; left time: 16223.2468s\n",
      "2099it [12:12,  2.68it/s]\titers: 2100, epoch: 2 | loss: 0.7852208\n",
      "\tspeed: 0.3720s/iter; left time: 18442.7905s\n",
      "2199it [12:44,  3.15it/s]\titers: 2200, epoch: 2 | loss: 0.6833733\n",
      "\tspeed: 0.3192s/iter; left time: 15791.8913s\n",
      "2299it [13:21,  2.69it/s]\titers: 2300, epoch: 2 | loss: 0.3906111\n",
      "\tspeed: 0.3686s/iter; left time: 18201.8353s\n",
      "2399it [13:54,  3.16it/s]\titers: 2400, epoch: 2 | loss: 0.5461358\n",
      "\tspeed: 0.3344s/iter; left time: 16478.5796s\n",
      "2499it [14:30,  2.69it/s]\titers: 2500, epoch: 2 | loss: 0.4062229\n",
      "\tspeed: 0.3580s/iter; left time: 17608.2419s\n",
      "2599it [15:05,  3.12it/s]\titers: 2600, epoch: 2 | loss: 0.8395727\n",
      "\tspeed: 0.3425s/iter; left time: 16810.5569s\n",
      "2699it [15:40,  2.66it/s]\titers: 2700, epoch: 2 | loss: 0.6855595\n",
      "\tspeed: 0.3529s/iter; left time: 17287.2558s\n",
      "2720it [15:48,  2.87it/s]\n",
      "Epoch: 2 cost time: 948.1718575954437\n",
      "3204it [08:39,  6.17it/s]\n",
      "3192it [08:35,  6.20it/s]\n",
      "Epoch: 2 | Train Loss: 0.6425603 Vali Loss: 0.5475840 Test Loss: 0.7115200 MAE Loss: 0.5791693\n",
      "Validation loss decreased (0.631974 --> 0.547584).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:32,  2.76it/s]\titers: 100, epoch: 3 | loss: 0.6144947\n",
      "\tspeed: 11.5274s/iter; left time: 563241.1899s\n",
      "199it [01:09,  2.70it/s]\titers: 200, epoch: 3 | loss: 0.9419234\n",
      "\tspeed: 0.3710s/iter; left time: 18091.9468s\n",
      "299it [01:42,  3.11it/s]\titers: 300, epoch: 3 | loss: 0.4742942\n",
      "\tspeed: 0.3322s/iter; left time: 16164.1729s\n",
      "399it [02:19,  2.67it/s]\titers: 400, epoch: 3 | loss: 0.3423749\n",
      "\tspeed: 0.3705s/iter; left time: 17991.0332s\n",
      "499it [02:53,  3.09it/s]\titers: 500, epoch: 3 | loss: 0.4458218\n",
      "\tspeed: 0.3377s/iter; left time: 16364.4847s\n",
      "599it [03:29,  2.68it/s]\titers: 600, epoch: 3 | loss: 0.5262709\n",
      "\tspeed: 0.3586s/iter; left time: 17341.9643s\n",
      "699it [04:03,  3.13it/s]\titers: 700, epoch: 3 | loss: 0.5030478\n",
      "\tspeed: 0.3453s/iter; left time: 16662.4953s\n",
      "799it [04:38,  2.68it/s]\titers: 800, epoch: 3 | loss: 0.6623328\n",
      "\tspeed: 0.3482s/iter; left time: 16770.4709s\n",
      "899it [05:14,  3.15it/s]\titers: 900, epoch: 3 | loss: 0.3170228\n",
      "\tspeed: 0.3552s/iter; left time: 17073.4709s\n",
      "999it [05:47,  2.68it/s]\titers: 1000, epoch: 3 | loss: 0.2859673\n",
      "\tspeed: 0.3390s/iter; left time: 16261.1479s\n",
      "1099it [06:24,  3.15it/s]\titers: 1100, epoch: 3 | loss: 0.6016991\n",
      "\tspeed: 0.3648s/iter; left time: 17461.0682s\n",
      "1199it [06:57,  2.71it/s]\titers: 1200, epoch: 3 | loss: 0.4105784\n",
      "\tspeed: 0.3285s/iter; left time: 15691.5433s\n",
      "1299it [07:34,  2.70it/s]\titers: 1300, epoch: 3 | loss: 0.7606825\n",
      "\tspeed: 0.3694s/iter; left time: 17604.4304s\n",
      "1399it [08:06,  3.14it/s]\titers: 1400, epoch: 3 | loss: 0.4757666\n",
      "\tspeed: 0.3237s/iter; left time: 15396.2378s\n",
      "1499it [08:43,  2.70it/s]\titers: 1500, epoch: 3 | loss: 0.3324931\n",
      "\tspeed: 0.3687s/iter; left time: 17501.1006s\n",
      "1599it [09:16,  3.15it/s]\titers: 1600, epoch: 3 | loss: 0.7279596\n",
      "\tspeed: 0.3332s/iter; left time: 15779.2924s\n",
      "1699it [09:52,  2.72it/s]\titers: 1700, epoch: 3 | loss: 0.2525400\n",
      "\tspeed: 0.3590s/iter; left time: 16965.2378s\n",
      "1799it [10:27,  3.13it/s]\titers: 1800, epoch: 3 | loss: 0.4727688\n",
      "\tspeed: 0.3441s/iter; left time: 16226.2782s\n",
      "1899it [11:01,  2.68it/s]\titers: 1900, epoch: 3 | loss: 0.5086299\n",
      "\tspeed: 0.3481s/iter; left time: 16380.9579s\n",
      "1999it [11:37,  3.16it/s]\titers: 2000, epoch: 3 | loss: 0.6979778\n",
      "\tspeed: 0.3513s/iter; left time: 16498.4657s\n",
      "2099it [12:11,  2.70it/s]\titers: 2100, epoch: 3 | loss: 0.9915733\n",
      "\tspeed: 0.3401s/iter; left time: 15939.4180s\n",
      "2199it [12:47,  3.13it/s]\titers: 2200, epoch: 3 | loss: 0.3013663\n",
      "\tspeed: 0.3639s/iter; left time: 17015.1394s\n",
      "2299it [13:20,  2.70it/s]\titers: 2300, epoch: 3 | loss: 0.6838180\n",
      "\tspeed: 0.3279s/iter; left time: 15301.2222s\n",
      "2399it [13:57,  2.71it/s]\titers: 2400, epoch: 3 | loss: 0.4120084\n",
      "\tspeed: 0.3708s/iter; left time: 17265.3155s\n",
      "2499it [14:29,  3.10it/s]\titers: 2500, epoch: 3 | loss: 0.5940225\n",
      "\tspeed: 0.3249s/iter; left time: 15096.2761s\n",
      "2599it [15:06,  2.66it/s]\titers: 2600, epoch: 3 | loss: 0.4922849\n",
      "\tspeed: 0.3697s/iter; left time: 17138.3999s\n",
      "2699it [15:40,  3.13it/s]\titers: 2700, epoch: 3 | loss: 0.6248993\n",
      "\tspeed: 0.3332s/iter; left time: 15412.1949s\n",
      "2720it [15:46,  2.87it/s]\n",
      "Epoch: 3 cost time: 946.9244887828827\n",
      "3204it [08:41,  6.14it/s]\n",
      "3192it [08:36,  6.18it/s]\n",
      "Epoch: 3 | Train Loss: 0.5092859 Vali Loss: 0.5881992 Test Loss: 0.7736666 MAE Loss: 0.6017070\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "99it [00:32,  2.72it/s]\titers: 100, epoch: 4 | loss: 0.3010140\n",
      "\tspeed: 10.7785s/iter; left time: 497329.0759s\n",
      "199it [01:10,  2.66it/s]\titers: 200, epoch: 4 | loss: 0.3380611\n",
      "\tspeed: 0.3717s/iter; left time: 17111.3784s\n",
      "299it [01:42,  2.86it/s]\titers: 300, epoch: 4 | loss: 0.2092494\n",
      "\tspeed: 0.3223s/iter; left time: 14808.3095s\n",
      "399it [02:19,  2.70it/s]\titers: 400, epoch: 4 | loss: 0.2408736\n",
      "\tspeed: 0.3694s/iter; left time: 16932.3971s\n",
      "499it [02:52,  3.16it/s]\titers: 500, epoch: 4 | loss: 0.5282897\n",
      "\tspeed: 0.3307s/iter; left time: 15128.2214s\n",
      "599it [03:28,  2.69it/s]\titers: 600, epoch: 4 | loss: 0.8559238\n",
      "\tspeed: 0.3605s/iter; left time: 16455.0082s\n",
      "699it [04:02,  3.11it/s]\titers: 700, epoch: 4 | loss: 0.5088249\n",
      "\tspeed: 0.3431s/iter; left time: 15625.8933s\n",
      "799it [04:37,  2.74it/s]\titers: 800, epoch: 4 | loss: 0.3616590\n",
      "\tspeed: 0.3474s/iter; left time: 15785.7997s\n",
      "899it [05:13,  3.16it/s]\titers: 900, epoch: 4 | loss: 0.3215808\n",
      "\tspeed: 0.3560s/iter; left time: 16142.9750s\n",
      "999it [05:46,  2.70it/s]\titers: 1000, epoch: 4 | loss: 0.2107311\n",
      "\tspeed: 0.3321s/iter; left time: 15022.6162s\n",
      "1099it [06:23,  2.70it/s]\titers: 1100, epoch: 4 | loss: 0.6553261\n",
      "\tspeed: 0.3691s/iter; left time: 16663.3524s\n",
      "1199it [06:55,  3.13it/s]\titers: 1200, epoch: 4 | loss: 0.2540773\n",
      "\tspeed: 0.3213s/iter; left time: 14471.5182s\n",
      "1299it [07:32,  2.68it/s]\titers: 1300, epoch: 4 | loss: 0.4908604\n",
      "\tspeed: 0.3689s/iter; left time: 16576.7384s\n",
      "1399it [08:05,  3.16it/s]\titers: 1400, epoch: 4 | loss: 0.2672223\n",
      "\tspeed: 0.3336s/iter; left time: 14958.6816s\n",
      "1499it [08:41,  2.73it/s]\titers: 1500, epoch: 4 | loss: 0.4376489\n",
      "\tspeed: 0.3547s/iter; left time: 15871.4661s\n",
      "1599it [09:15,  3.16it/s]\titers: 1600, epoch: 4 | loss: 0.6608563\n",
      "\tspeed: 0.3474s/iter; left time: 15509.1030s\n",
      "1699it [09:50,  2.74it/s]\titers: 1700, epoch: 4 | loss: 0.2466058\n",
      "\tspeed: 0.3435s/iter; left time: 15302.0030s\n",
      "1799it [10:26,  3.14it/s]\titers: 1800, epoch: 4 | loss: 0.3377094\n",
      "\tspeed: 0.3594s/iter; left time: 15973.6491s\n",
      "1899it [10:59,  2.68it/s]\titers: 1900, epoch: 4 | loss: 0.4431231\n",
      "\tspeed: 0.3301s/iter; left time: 14635.6322s\n",
      "1999it [11:36,  2.82it/s]\titers: 2000, epoch: 4 | loss: 0.3103598\n",
      "\tspeed: 0.3705s/iter; left time: 16391.4680s\n",
      "2099it [12:08,  2.87it/s]\titers: 2100, epoch: 4 | loss: 0.2943001\n",
      "\tspeed: 0.3216s/iter; left time: 14196.0983s\n",
      "2199it [12:45,  2.73it/s]\titers: 2200, epoch: 4 | loss: 0.3590674\n",
      "\tspeed: 0.3701s/iter; left time: 16301.2763s\n",
      "2299it [13:18,  3.15it/s]\titers: 2300, epoch: 4 | loss: 0.7414067\n",
      "\tspeed: 0.3313s/iter; left time: 14555.9490s\n",
      "2399it [13:54,  2.71it/s]\titers: 2400, epoch: 4 | loss: 0.6814449\n",
      "\tspeed: 0.3626s/iter; left time: 15897.4913s\n",
      "2499it [14:28,  3.15it/s]\titers: 2500, epoch: 4 | loss: 0.2331382\n",
      "\tspeed: 0.3420s/iter; left time: 14958.4050s\n",
      "2599it [15:04,  2.58it/s]\titers: 2600, epoch: 4 | loss: 0.4847037\n",
      "\tspeed: 0.3517s/iter; left time: 15350.5405s\n",
      "2699it [15:39,  3.11it/s]\titers: 2700, epoch: 4 | loss: 0.1791969\n",
      "\tspeed: 0.3509s/iter; left time: 15276.4159s\n",
      "2720it [15:45,  2.88it/s]\n",
      "Epoch: 4 cost time: 945.9578924179077\n",
      "3204it [08:39,  6.16it/s]\n",
      "3192it [08:38,  6.15it/s]\n",
      "Epoch: 4 | Train Loss: 0.4141383 Vali Loss: 0.5604993 Test Loss: 0.7721985 MAE Loss: 0.5863785\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "99it [00:33,  2.70it/s]\titers: 100, epoch: 5 | loss: 0.2978989\n",
      "\tspeed: 10.7947s/iter; left time: 468715.8621s\n",
      "199it [01:10,  3.14it/s]\titers: 200, epoch: 5 | loss: 0.2954189\n",
      "\tspeed: 0.3645s/iter; left time: 15789.4769s\n",
      "299it [01:43,  2.70it/s]\titers: 300, epoch: 5 | loss: 0.4870250\n",
      "\tspeed: 0.3300s/iter; left time: 14261.4906s\n",
      "399it [02:20,  2.92it/s]\titers: 400, epoch: 5 | loss: 0.5323809\n",
      "\tspeed: 0.3696s/iter; left time: 15938.2999s\n",
      "499it [02:52,  2.74it/s]\titers: 500, epoch: 5 | loss: 0.7121066\n",
      "\tspeed: 0.3237s/iter; left time: 13924.5511s\n",
      "599it [03:29,  2.70it/s]\titers: 600, epoch: 5 | loss: 0.3227259\n",
      "\tspeed: 0.3715s/iter; left time: 15946.5950s\n",
      "699it [04:02,  3.11it/s]\titers: 700, epoch: 5 | loss: 0.5293677\n",
      "\tspeed: 0.3226s/iter; left time: 13814.6890s\n",
      "799it [04:39,  2.69it/s]\titers: 800, epoch: 5 | loss: 0.5515209\n",
      "\tspeed: 0.3720s/iter; left time: 15893.5215s\n",
      "899it [05:12,  3.10it/s]\titers: 900, epoch: 5 | loss: 0.2405053\n",
      "\tspeed: 0.3294s/iter; left time: 14040.1620s\n",
      "999it [05:48,  2.72it/s]\titers: 1000, epoch: 5 | loss: 0.3899322\n",
      "\tspeed: 0.3640s/iter; left time: 15477.4290s\n",
      "1099it [06:22,  3.14it/s]\titers: 1100, epoch: 5 | loss: 0.3912500\n",
      "\tspeed: 0.3347s/iter; left time: 14199.7884s\n",
      "1199it [06:58,  2.69it/s]\titers: 1200, epoch: 5 | loss: 0.1245368\n",
      "\tspeed: 0.3598s/iter; left time: 15228.1772s\n",
      "1299it [07:32,  3.14it/s]\titers: 1300, epoch: 5 | loss: 0.3257240\n",
      "\tspeed: 0.3421s/iter; left time: 14443.2406s\n",
      "1399it [08:07,  2.67it/s]\titers: 1400, epoch: 5 | loss: 0.3612733\n",
      "\tspeed: 0.3521s/iter; left time: 14831.5684s\n",
      "1499it [08:42,  3.14it/s]\titers: 1500, epoch: 5 | loss: 0.4417752\n",
      "\tspeed: 0.3475s/iter; left time: 14602.7902s\n",
      "1599it [09:17,  2.69it/s]\titers: 1600, epoch: 5 | loss: 0.3082868\n",
      "\tspeed: 0.3485s/iter; left time: 14609.0980s\n",
      "1699it [09:52,  3.13it/s]\titers: 1700, epoch: 5 | loss: 0.2036671\n",
      "\tspeed: 0.3520s/iter; left time: 14719.2948s\n",
      "1799it [10:26,  2.70it/s]\titers: 1800, epoch: 5 | loss: 0.3148972\n",
      "\tspeed: 0.3414s/iter; left time: 14241.5004s\n",
      "1899it [11:02,  3.12it/s]\titers: 1900, epoch: 5 | loss: 0.3529622\n",
      "\tspeed: 0.3601s/iter; left time: 14986.7856s\n",
      "1999it [11:35,  2.69it/s]\titers: 2000, epoch: 5 | loss: 0.1649828\n",
      "\tspeed: 0.3336s/iter; left time: 13850.4186s\n",
      "2099it [12:12,  3.12it/s]\titers: 2100, epoch: 5 | loss: 0.3854919\n",
      "\tspeed: 0.3679s/iter; left time: 15238.3994s\n",
      "2199it [12:45,  2.70it/s]\titers: 2200, epoch: 5 | loss: 0.2885742\n",
      "\tspeed: 0.3272s/iter; left time: 13519.8390s\n",
      "2299it [13:22,  2.67it/s]\titers: 2300, epoch: 5 | loss: 0.1934094\n",
      "\tspeed: 0.3725s/iter; left time: 15353.8004s\n",
      "2399it [13:54,  2.78it/s]\titers: 2400, epoch: 5 | loss: 0.2911405\n",
      "\tspeed: 0.3223s/iter; left time: 13251.4430s\n",
      "2499it [14:32,  2.70it/s]\titers: 2500, epoch: 5 | loss: 0.4281849\n",
      "\tspeed: 0.3734s/iter; left time: 15315.7485s\n",
      "2599it [15:04,  3.03it/s]\titers: 2600, epoch: 5 | loss: 0.3598448\n",
      "\tspeed: 0.3247s/iter; left time: 13287.7239s\n",
      "2699it [15:41,  2.73it/s]\titers: 2700, epoch: 5 | loss: 0.2287024\n",
      "\tspeed: 0.3715s/iter; left time: 15163.9044s\n",
      "2720it [15:49,  2.86it/s]\n",
      "Epoch: 5 cost time: 949.5517475605011\n",
      "3204it [08:38,  6.17it/s]\n",
      "3192it [08:39,  6.14it/s]\n",
      "Epoch: 5 | Train Loss: 0.3680857 Vali Loss: 0.5681231 Test Loss: 0.7672769 MAE Loss: 0.5758432\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 296, in <module>\n",
      "    mse, mae = test(args, accelerator, model, test_data, test_loader, criterion, mae_metric, setting)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/utils/tools.py\", line 225, in test\n",
      "    model.load_state_dict(torch.load(checkpoint_path, map_location=accelerator.device))\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 1026, in load\n",
      "    return _load(opened_zipfile,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 1438, in _load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 1408, in persistent_load\n",
      "    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 1382, in load_tensor\n",
      "    wrap_storage=restore_location(storage, location),\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 1311, in restore_location\n",
      "    return default_restore_location(storage, str(map_location))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 391, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/serialization.py\", line 271, in _cuda_deserialize\n",
      "    return obj.cuda(device)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_utils.py\", line 115, in _cuda\n",
      "    untyped_storage = torch.UntypedStorage(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 35.12 MiB is free. Process 31286 has 414.00 MiB memory in use. Process 6952 has 52.58 GiB memory in use. Including non-PyTorch memory, this process has 26.28 GiB memory in use. Of the allocated memory 25.16 GiB is allocated by PyTorch, and 26.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1057, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 673, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/vol/cs-hu/riabchuv/.conda/envs/val/bin/python', './Time-LLM/run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './datasets/', '--data_path', 'FR_data.csv', '--model_id', '1', '--model', 'TimeLLM', '--data', 'FR', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '96', '--factor', '3', '--enc_in', '3', '--dec_in', '3', '--c_out', '3', '--des', 'Exp', '--itr', '1', '--llm_model', 'LLAMA', '--llm_dim', '4096', '--percent', '20', '--d_model', '16', '--d_ff', '64', '--batch_size', '6', '--learning_rate', '0.001', '--llm_layers', '32', '--train_epochs', '20', '--model_comment', 'TimeLLM-FR']' returned non-zero exit status 1.\n",
      "Total time: 170.61084510882696 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=32\n",
    "\n",
    "# num_process=1\n",
    "batch_size=6\n",
    "d_model=16\n",
    "d_ff=64\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --percent 20 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETTH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "192/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 56231\n",
      "val 19495\n",
      "test 19495\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 139, in <module>\n",
      "    model = TimeLLM.Model(args).float()\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 52, in __init__\n",
      "    self.llm_model = LlamaModel.from_pretrained(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 561, in __init__\n",
      "    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 561, in <listcomp>\n",
      "    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 376, in __init__\n",
      "    self.mlp = LlamaMLP(config)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 198, in __init__\n",
      "    self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 103, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 109, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/init.py\", line 459, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound, generator=generator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_epochs=50\n",
    "learning_rate=0.01\n",
    "llama_layers=32\n",
    "\n",
    "#master_port=00097\n",
    "num_process=1\n",
    "batch_size=6\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --main_process_port \"00097\"\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-ETTh1'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "!python -m accelerate.commands.launch  --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"LLAMA\" \\\n",
    "  --llm_dim 4096 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
