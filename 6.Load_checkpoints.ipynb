{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 90363\n",
      "val 19443\n",
      "test 19371\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-16 19:44:11,332] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-16 19:44:12,138] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 19:44:12,139] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-16 19:44:12,139] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 19:44:12,966] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.164, master_port=29500\n",
      "[2024-05-16 19:44:12,966] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 19:44:13,548] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-16 19:44:13,549] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-16 19:44:13,550] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-16 19:44:13,550] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-16 19:44:13,551] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-16 19:44:13,551] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-16 19:44:13,551] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-16 19:44:13,551] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-16 19:44:13,551] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-16 19:44:13,551] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-16 19:44:13,836] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-16 19:44:13,837] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB \n",
      "[2024-05-16 19:44:13,837] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 479.74 GB, percent = 63.6%\n",
      "[2024-05-16 19:44:13,945] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-16 19:44:13,946] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-05-16 19:44:13,946] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 479.69 GB, percent = 63.6%\n",
      "[2024-05-16 19:44:13,946] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-16 19:44:14,057] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-16 19:44:14,057] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-05-16 19:44:14,058] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 479.74 GB, percent = 63.6%\n",
      "[2024-05-16 19:44:14,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-16 19:44:14,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-16 19:44:14,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-16 19:44:14,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe764715a50>\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-16 19:44:14,059] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-16 19:44:14,060] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:14,  7.32it/s]\titers: 100, epoch: 1 | loss: 0.5444238\n",
      "\tspeed: 0.1745s/iter; left time: 6554.3007s\n",
      "199it [00:27,  7.37it/s]\titers: 200, epoch: 1 | loss: 0.4104359\n",
      "\tspeed: 0.1363s/iter; left time: 5106.3481s\n",
      "299it [00:41,  7.34it/s]\titers: 300, epoch: 1 | loss: 0.4074034\n",
      "\tspeed: 0.1357s/iter; left time: 5069.8843s\n",
      "399it [00:54,  7.36it/s]\titers: 400, epoch: 1 | loss: 0.5265598\n",
      "\tspeed: 0.1361s/iter; left time: 5071.4657s\n",
      "499it [01:08,  7.38it/s]\titers: 500, epoch: 1 | loss: 0.2807295\n",
      "\tspeed: 0.1358s/iter; left time: 5043.9680s\n",
      "599it [01:22,  7.39it/s]\titers: 600, epoch: 1 | loss: 0.5387488\n",
      "\tspeed: 0.1359s/iter; left time: 5036.1779s\n",
      "699it [01:35,  7.36it/s]\titers: 700, epoch: 1 | loss: 0.2135512\n",
      "\tspeed: 0.1362s/iter; left time: 5033.3591s\n",
      "799it [01:49,  7.37it/s]\titers: 800, epoch: 1 | loss: 0.2850479\n",
      "\tspeed: 0.1359s/iter; left time: 5008.5004s\n",
      "899it [02:03,  7.43it/s]\titers: 900, epoch: 1 | loss: 0.2934930\n",
      "\tspeed: 0.1363s/iter; left time: 5009.1247s\n",
      "999it [02:16,  7.42it/s]\titers: 1000, epoch: 1 | loss: 0.2978153\n",
      "\tspeed: 0.1348s/iter; left time: 4941.1510s\n",
      "1099it [02:30,  7.40it/s]\titers: 1100, epoch: 1 | loss: 0.2092738\n",
      "\tspeed: 0.1364s/iter; left time: 4985.9823s\n",
      "1199it [02:43,  7.41it/s]\titers: 1200, epoch: 1 | loss: 0.1737960\n",
      "\tspeed: 0.1349s/iter; left time: 4918.5710s\n",
      "1299it [02:57,  7.41it/s]\titers: 1300, epoch: 1 | loss: 0.1932664\n",
      "\tspeed: 0.1348s/iter; left time: 4900.1357s\n",
      "1399it [03:10,  7.42it/s]\titers: 1400, epoch: 1 | loss: 0.2342813\n",
      "\tspeed: 0.1349s/iter; left time: 4891.0240s\n",
      "1499it [03:24,  7.38it/s]\titers: 1500, epoch: 1 | loss: 0.4414093\n",
      "\tspeed: 0.1350s/iter; left time: 4878.7582s\n",
      "1599it [03:37,  7.38it/s]\titers: 1600, epoch: 1 | loss: 0.4585559\n",
      "\tspeed: 0.1364s/iter; left time: 4916.3070s\n",
      "1699it [03:51,  7.36it/s]\titers: 1700, epoch: 1 | loss: 0.2643373\n",
      "\tspeed: 0.1361s/iter; left time: 4891.7086s\n",
      "1799it [04:04,  7.43it/s]\titers: 1800, epoch: 1 | loss: 0.2432564\n",
      "\tspeed: 0.1358s/iter; left time: 4869.0283s\n",
      "1899it [04:18,  7.42it/s]\titers: 1900, epoch: 1 | loss: 0.1711638\n",
      "\tspeed: 0.1347s/iter; left time: 4815.2909s\n",
      "1999it [04:31,  7.36it/s]\titers: 2000, epoch: 1 | loss: 0.1636032\n",
      "\tspeed: 0.1350s/iter; left time: 4811.7229s\n",
      "2099it [04:45,  7.38it/s]\titers: 2100, epoch: 1 | loss: 0.2445004\n",
      "\tspeed: 0.1357s/iter; left time: 4823.9708s\n",
      "2199it [04:59,  7.36it/s]\titers: 2200, epoch: 1 | loss: 0.4539610\n",
      "\tspeed: 0.1363s/iter; left time: 4830.9949s\n",
      "2299it [05:12,  7.34it/s]\titers: 2300, epoch: 1 | loss: 0.2724272\n",
      "\tspeed: 0.1368s/iter; left time: 4837.7720s\n",
      "2399it [05:26,  7.39it/s]\titers: 2400, epoch: 1 | loss: 0.4076100\n",
      "\tspeed: 0.1367s/iter; left time: 4817.4510s\n",
      "2499it [05:40,  7.33it/s]\titers: 2500, epoch: 1 | loss: 0.1925327\n",
      "\tspeed: 0.1360s/iter; left time: 4781.2800s\n",
      "2599it [05:53,  7.37it/s]\titers: 2600, epoch: 1 | loss: 0.3031555\n",
      "\tspeed: 0.1366s/iter; left time: 4788.3093s\n",
      "2699it [06:07,  7.35it/s]\titers: 2700, epoch: 1 | loss: 0.4735817\n",
      "\tspeed: 0.1362s/iter; left time: 4761.3737s\n",
      "2799it [06:20,  7.37it/s]\titers: 2800, epoch: 1 | loss: 0.2143671\n",
      "\tspeed: 0.1363s/iter; left time: 4748.5502s\n",
      "2899it [06:34,  7.21it/s]\titers: 2900, epoch: 1 | loss: 0.3639215\n",
      "\tspeed: 0.1360s/iter; left time: 4726.7645s\n",
      "2999it [06:48,  7.36it/s]\titers: 3000, epoch: 1 | loss: 0.3485069\n",
      "\tspeed: 0.1362s/iter; left time: 4720.3578s\n",
      "3099it [07:01,  7.37it/s]\titers: 3100, epoch: 1 | loss: 0.2981286\n",
      "\tspeed: 0.1357s/iter; left time: 4689.2952s\n",
      "3199it [07:15,  7.38it/s]\titers: 3200, epoch: 1 | loss: 0.4006509\n",
      "\tspeed: 0.1357s/iter; left time: 4675.5226s\n",
      "3299it [07:28,  7.38it/s]\titers: 3300, epoch: 1 | loss: 0.2170447\n",
      "\tspeed: 0.1357s/iter; left time: 4660.0974s\n",
      "3399it [07:42,  7.35it/s]\titers: 3400, epoch: 1 | loss: 0.3905250\n",
      "\tspeed: 0.1368s/iter; left time: 4684.5064s\n",
      "3499it [07:56,  7.35it/s]\titers: 3500, epoch: 1 | loss: 0.2816363\n",
      "\tspeed: 0.1365s/iter; left time: 4662.2885s\n",
      "3599it [08:09,  7.38it/s]\titers: 3600, epoch: 1 | loss: 0.3380325\n",
      "\tspeed: 0.1355s/iter; left time: 4615.1793s\n",
      "3699it [08:23,  7.36it/s]\titers: 3700, epoch: 1 | loss: 0.2030608\n",
      "\tspeed: 0.1359s/iter; left time: 4612.9207s\n",
      "3765it [08:32,  7.35it/s]\n",
      "Epoch: 1 cost time: 512.4030148983002\n",
      "810it [00:54, 14.77it/s]\n",
      "807it [00:54, 14.76it/s]\n",
      "Epoch: 1 | Train Loss: 0.3081911 Vali Loss: 0.3295813 Test Loss: 0.3944334 MAE Loss: 0.3974499\n",
      "Validation loss decreased (inf --> 0.329581).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:13,  7.59it/s]\titers: 100, epoch: 2 | loss: 0.1639574\n",
      "\tspeed: 1.3308s/iter; left time: 44962.5603s\n",
      "199it [00:26,  7.59it/s]\titers: 200, epoch: 2 | loss: 0.5405571\n",
      "\tspeed: 0.1321s/iter; left time: 4451.6021s\n",
      "299it [00:39,  7.22it/s]\titers: 300, epoch: 2 | loss: 0.1843774\n",
      "\tspeed: 0.1328s/iter; left time: 4459.3794s\n",
      "399it [00:53,  7.59it/s]\titers: 400, epoch: 2 | loss: 0.1597055\n",
      "\tspeed: 0.1320s/iter; left time: 4421.7719s\n",
      "499it [01:06,  7.53it/s]\titers: 500, epoch: 2 | loss: 0.2466474\n",
      "\tspeed: 0.1328s/iter; left time: 4434.4015s\n",
      "599it [01:19,  7.57it/s]\titers: 600, epoch: 2 | loss: 0.1386792\n",
      "\tspeed: 0.1323s/iter; left time: 4404.3537s\n",
      "699it [01:32,  7.57it/s]\titers: 700, epoch: 2 | loss: 0.4463665\n",
      "\tspeed: 0.1325s/iter; left time: 4397.6110s\n",
      "799it [01:46,  7.44it/s]\titers: 800, epoch: 2 | loss: 0.2692041\n",
      "\tspeed: 0.1321s/iter; left time: 4370.2985s\n",
      "899it [01:59,  7.59it/s]\titers: 900, epoch: 2 | loss: 0.3635383\n",
      "\tspeed: 0.1324s/iter; left time: 4366.5591s\n",
      "999it [02:12,  7.55it/s]\titers: 1000, epoch: 2 | loss: 0.3119984\n",
      "\tspeed: 0.1320s/iter; left time: 4339.7068s\n",
      "1099it [02:25,  7.47it/s]\titers: 1100, epoch: 2 | loss: 0.6365123\n",
      "\tspeed: 0.1324s/iter; left time: 4339.7744s\n",
      "1199it [02:39,  7.60it/s]\titers: 1200, epoch: 2 | loss: 0.3789161\n",
      "\tspeed: 0.1319s/iter; left time: 4312.3449s\n",
      "1299it [02:52,  7.56it/s]\titers: 1300, epoch: 2 | loss: 0.2570554\n",
      "\tspeed: 0.1322s/iter; left time: 4306.4677s\n",
      "1399it [03:05,  7.58it/s]\titers: 1400, epoch: 2 | loss: 0.1983924\n",
      "\tspeed: 0.1319s/iter; left time: 4283.5818s\n",
      "1499it [03:18,  7.57it/s]\titers: 1500, epoch: 2 | loss: 0.1906707\n",
      "\tspeed: 0.1320s/iter; left time: 4273.7912s\n",
      "1599it [03:31,  7.57it/s]\titers: 1600, epoch: 2 | loss: 0.2400161\n",
      "\tspeed: 0.1321s/iter; left time: 4266.4344s\n",
      "1699it [03:45,  7.62it/s]\titers: 1700, epoch: 2 | loss: 0.3308109\n",
      "\tspeed: 0.1322s/iter; left time: 4255.2711s\n",
      "1799it [03:58,  7.58it/s]\titers: 1800, epoch: 2 | loss: 0.3109717\n",
      "\tspeed: 0.1318s/iter; left time: 4228.7816s\n",
      "1899it [04:11,  7.59it/s]\titers: 1900, epoch: 2 | loss: 0.1874094\n",
      "\tspeed: 0.1322s/iter; left time: 4227.3428s\n",
      "1999it [04:24,  7.55it/s]\titers: 2000, epoch: 2 | loss: 0.2633019\n",
      "\tspeed: 0.1320s/iter; left time: 4209.7489s\n",
      "2099it [04:37,  7.61it/s]\titers: 2100, epoch: 2 | loss: 0.1405029\n",
      "\tspeed: 0.1319s/iter; left time: 4192.4973s\n",
      "2199it [04:51,  7.58it/s]\titers: 2200, epoch: 2 | loss: 0.3681089\n",
      "\tspeed: 0.1320s/iter; left time: 4184.0048s\n",
      "2299it [05:04,  7.34it/s]\titers: 2300, epoch: 2 | loss: 0.1729460\n",
      "\tspeed: 0.1334s/iter; left time: 4214.2706s\n",
      "2399it [05:17,  7.50it/s]\titers: 2400, epoch: 2 | loss: 0.1386145\n",
      "\tspeed: 0.1335s/iter; left time: 4203.9278s\n",
      "2499it [05:31,  7.55it/s]\titers: 2500, epoch: 2 | loss: 0.2711107\n",
      "\tspeed: 0.1329s/iter; left time: 4171.2821s\n",
      "2599it [05:44,  7.52it/s]\titers: 2600, epoch: 2 | loss: 0.2899007\n",
      "\tspeed: 0.1330s/iter; left time: 4160.7598s\n",
      "2699it [05:57,  7.54it/s]\titers: 2700, epoch: 2 | loss: 0.1953906\n",
      "\tspeed: 0.1326s/iter; left time: 4136.6706s\n",
      "2799it [06:10,  7.51it/s]\titers: 2800, epoch: 2 | loss: 0.1765995\n",
      "\tspeed: 0.1332s/iter; left time: 4142.2019s\n",
      "2899it [06:24,  7.52it/s]\titers: 2900, epoch: 2 | loss: 0.2706118\n",
      "\tspeed: 0.1329s/iter; left time: 4117.3060s\n",
      "2999it [06:37,  7.58it/s]\titers: 3000, epoch: 2 | loss: 0.2374129\n",
      "\tspeed: 0.1325s/iter; left time: 4092.8108s\n",
      "3099it [06:50,  7.30it/s]\titers: 3100, epoch: 2 | loss: 0.3832311\n",
      "\tspeed: 0.1341s/iter; left time: 4127.1200s\n",
      "3199it [07:04,  7.55it/s]\titers: 3200, epoch: 2 | loss: 0.4006540\n",
      "\tspeed: 0.1333s/iter; left time: 4090.0951s\n",
      "3299it [07:17,  7.53it/s]\titers: 3300, epoch: 2 | loss: 0.2149494\n",
      "\tspeed: 0.1328s/iter; left time: 4060.6090s\n",
      "3399it [07:30,  7.28it/s]\titers: 3400, epoch: 2 | loss: 0.2486580\n",
      "\tspeed: 0.1327s/iter; left time: 4044.7987s\n",
      "3499it [07:44,  7.54it/s]\titers: 3500, epoch: 2 | loss: 0.2286827\n",
      "\tspeed: 0.1330s/iter; left time: 4041.7779s\n",
      "3599it [07:57,  7.50it/s]\titers: 3600, epoch: 2 | loss: 0.3397874\n",
      "\tspeed: 0.1336s/iter; left time: 4045.8610s\n",
      "3699it [08:10,  7.40it/s]\titers: 3700, epoch: 2 | loss: 0.2991925\n",
      "\tspeed: 0.1330s/iter; left time: 4013.4711s\n",
      "3765it [08:19,  7.54it/s]\n",
      "Epoch: 2 cost time: 499.4950647354126\n",
      "810it [00:52, 15.56it/s]\n",
      "807it [00:51, 15.59it/s]\n",
      "Epoch: 2 | Train Loss: 0.2648553 Vali Loss: 0.3046619 Test Loss: 0.3734396 MAE Loss: 0.3793366\n",
      "Validation loss decreased (0.329581 --> 0.304662).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:13,  7.49it/s]\titers: 100, epoch: 3 | loss: 0.2093413\n",
      "\tspeed: 1.2722s/iter; left time: 38193.9278s\n",
      "199it [00:26,  7.54it/s]\titers: 200, epoch: 3 | loss: 0.2412249\n",
      "\tspeed: 0.1330s/iter; left time: 3979.2402s\n",
      "299it [00:40,  7.54it/s]\titers: 300, epoch: 3 | loss: 0.2878701\n",
      "\tspeed: 0.1325s/iter; left time: 3950.3112s\n",
      "399it [00:53,  7.52it/s]\titers: 400, epoch: 3 | loss: 0.2571405\n",
      "\tspeed: 0.1323s/iter; left time: 3931.7609s\n",
      "499it [01:06,  7.55it/s]\titers: 500, epoch: 3 | loss: 0.1948572\n",
      "\tspeed: 0.1327s/iter; left time: 3929.9438s\n",
      "599it [01:19,  7.55it/s]\titers: 600, epoch: 3 | loss: 0.1865165\n",
      "\tspeed: 0.1327s/iter; left time: 3917.2771s\n",
      "699it [01:33,  7.55it/s]\titers: 700, epoch: 3 | loss: 0.2182564\n",
      "\tspeed: 0.1338s/iter; left time: 3937.6387s\n",
      "799it [01:46,  7.53it/s]\titers: 800, epoch: 3 | loss: 0.1523785\n",
      "\tspeed: 0.1334s/iter; left time: 3912.8294s\n",
      "899it [01:59,  7.58it/s]\titers: 900, epoch: 3 | loss: 0.3450936\n",
      "\tspeed: 0.1327s/iter; left time: 3878.8543s\n",
      "999it [02:13,  7.53it/s]\titers: 1000, epoch: 3 | loss: 0.1820113\n",
      "\tspeed: 0.1327s/iter; left time: 3863.9830s\n",
      "1099it [02:26,  7.56it/s]\titers: 1100, epoch: 3 | loss: 0.2890514\n",
      "\tspeed: 0.1329s/iter; left time: 3857.0475s\n",
      "1199it [02:39,  7.52it/s]\titers: 1200, epoch: 3 | loss: 0.1903125\n",
      "\tspeed: 0.1332s/iter; left time: 3852.9397s\n",
      "1299it [02:52,  7.53it/s]\titers: 1300, epoch: 3 | loss: 0.1512738\n",
      "\tspeed: 0.1326s/iter; left time: 3820.6317s\n",
      "1399it [03:06,  7.54it/s]\titers: 1400, epoch: 3 | loss: 0.1770290\n",
      "\tspeed: 0.1325s/iter; left time: 3804.2849s\n",
      "1499it [03:19,  7.53it/s]\titers: 1500, epoch: 3 | loss: 0.2750645\n",
      "\tspeed: 0.1327s/iter; left time: 3797.5773s\n",
      "1599it [03:32,  7.53it/s]\titers: 1600, epoch: 3 | loss: 0.2504632\n",
      "\tspeed: 0.1327s/iter; left time: 3784.2435s\n",
      "1699it [03:46,  7.58it/s]\titers: 1700, epoch: 3 | loss: 0.2141457\n",
      "\tspeed: 0.1327s/iter; left time: 3770.6069s\n",
      "1799it [03:59,  7.55it/s]\titers: 1800, epoch: 3 | loss: 0.1819759\n",
      "\tspeed: 0.1331s/iter; left time: 3768.2339s\n",
      "1899it [04:12,  7.57it/s]\titers: 1900, epoch: 3 | loss: 0.2926572\n",
      "\tspeed: 0.1326s/iter; left time: 3742.9956s\n",
      "1999it [04:25,  7.54it/s]\titers: 2000, epoch: 3 | loss: 0.2725919\n",
      "\tspeed: 0.1326s/iter; left time: 3729.4863s\n",
      "2099it [04:39,  7.57it/s]\titers: 2100, epoch: 3 | loss: 0.2264288\n",
      "\tspeed: 0.1321s/iter; left time: 3701.1604s\n",
      "2199it [04:52,  7.59it/s]\titers: 2200, epoch: 3 | loss: 0.1622341\n",
      "\tspeed: 0.1329s/iter; left time: 3710.8892s\n",
      "2299it [05:05,  7.50it/s]\titers: 2300, epoch: 3 | loss: 0.2355407\n",
      "\tspeed: 0.1328s/iter; left time: 3695.2239s\n",
      "2399it [05:18,  7.55it/s]\titers: 2400, epoch: 3 | loss: 0.2252967\n",
      "\tspeed: 0.1321s/iter; left time: 3661.5852s\n",
      "2499it [05:32,  7.57it/s]\titers: 2500, epoch: 3 | loss: 0.1823240\n",
      "\tspeed: 0.1322s/iter; left time: 3651.4408s\n",
      "2599it [05:45,  7.59it/s]\titers: 2600, epoch: 3 | loss: 0.1959126\n",
      "\tspeed: 0.1320s/iter; left time: 3633.1091s\n",
      "2699it [05:58,  7.57it/s]\titers: 2700, epoch: 3 | loss: 0.3472180\n",
      "\tspeed: 0.1319s/iter; left time: 3617.7800s\n",
      "2799it [06:11,  7.57it/s]\titers: 2800, epoch: 3 | loss: 0.1436818\n",
      "\tspeed: 0.1321s/iter; left time: 3610.0668s\n",
      "2899it [06:24,  7.57it/s]\titers: 2900, epoch: 3 | loss: 0.2979626\n",
      "\tspeed: 0.1322s/iter; left time: 3598.7545s\n",
      "2999it [06:38,  7.53it/s]\titers: 3000, epoch: 3 | loss: 0.1734074\n",
      "\tspeed: 0.1324s/iter; left time: 3591.6529s\n",
      "3099it [06:51,  7.53it/s]\titers: 3100, epoch: 3 | loss: 0.1960131\n",
      "\tspeed: 0.1329s/iter; left time: 3590.0403s\n",
      "3199it [07:04,  7.57it/s]\titers: 3200, epoch: 3 | loss: 0.2625073\n",
      "\tspeed: 0.1325s/iter; left time: 3567.7611s\n",
      "3299it [07:17,  7.57it/s]\titers: 3300, epoch: 3 | loss: 0.2109915\n",
      "\tspeed: 0.1323s/iter; left time: 3547.2994s\n",
      "3399it [07:31,  7.57it/s]\titers: 3400, epoch: 3 | loss: 0.1227532\n",
      "\tspeed: 0.1321s/iter; left time: 3529.1480s\n",
      "3499it [07:44,  7.52it/s]\titers: 3500, epoch: 3 | loss: 0.3649703\n",
      "\tspeed: 0.1322s/iter; left time: 3519.1234s\n",
      "3599it [07:57,  7.56it/s]\titers: 3600, epoch: 3 | loss: 0.1288858\n",
      "\tspeed: 0.1332s/iter; left time: 3531.4956s\n",
      "3699it [08:10,  7.60it/s]\titers: 3700, epoch: 3 | loss: 0.1547276\n",
      "\tspeed: 0.1320s/iter; left time: 3487.0944s\n",
      "3765it [08:19,  7.54it/s]\n",
      "Epoch: 3 cost time: 499.6046266555786\n",
      "810it [00:51, 15.59it/s]\n",
      "807it [00:52, 15.38it/s]\n",
      "Epoch: 3 | Train Loss: 0.2490945 Vali Loss: 0.2926479 Test Loss: 0.3656700 MAE Loss: 0.3646462\n",
      "Validation loss decreased (0.304662 --> 0.292648).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "99it [00:13,  7.56it/s]\titers: 100, epoch: 4 | loss: 0.2759263\n",
      "\tspeed: 1.2780s/iter; left time: 33555.4243s\n",
      "199it [00:26,  7.59it/s]\titers: 200, epoch: 4 | loss: 0.2688626\n",
      "\tspeed: 0.1323s/iter; left time: 3461.6520s\n",
      "299it [00:39,  7.59it/s]\titers: 300, epoch: 4 | loss: 0.1620458\n",
      "\tspeed: 0.1320s/iter; left time: 3439.2737s\n",
      "399it [00:53,  7.58it/s]\titers: 400, epoch: 4 | loss: 0.3921472\n",
      "\tspeed: 0.1322s/iter; left time: 3431.5047s\n",
      "499it [01:06,  7.58it/s]\titers: 500, epoch: 4 | loss: 0.3646387\n",
      "\tspeed: 0.1322s/iter; left time: 3418.5346s\n",
      "599it [01:19,  7.56it/s]\titers: 600, epoch: 4 | loss: 0.3020435\n",
      "\tspeed: 0.1324s/iter; left time: 3409.0587s\n",
      "699it [01:32,  7.55it/s]\titers: 700, epoch: 4 | loss: 0.2508266\n",
      "\tspeed: 0.1329s/iter; left time: 3410.1556s\n",
      "799it [01:46,  7.55it/s]\titers: 800, epoch: 4 | loss: 0.3295434\n",
      "\tspeed: 0.1327s/iter; left time: 3390.8359s\n",
      "899it [01:59,  7.57it/s]\titers: 900, epoch: 4 | loss: 0.1927819\n",
      "\tspeed: 0.1321s/iter; left time: 3362.3725s\n",
      "999it [02:12,  7.58it/s]\titers: 1000, epoch: 4 | loss: 0.2571589\n",
      "\tspeed: 0.1324s/iter; left time: 3357.3529s\n",
      "1099it [02:25,  7.59it/s]\titers: 1100, epoch: 4 | loss: 0.2878469\n",
      "\tspeed: 0.1325s/iter; left time: 3346.0604s\n",
      "1199it [02:39,  7.60it/s]\titers: 1200, epoch: 4 | loss: 0.3565212\n",
      "\tspeed: 0.1324s/iter; left time: 3329.8223s\n",
      "1299it [02:52,  7.46it/s]\titers: 1300, epoch: 4 | loss: 0.2147350\n",
      "\tspeed: 0.1335s/iter; left time: 3346.1436s\n",
      "1399it [03:05,  7.53it/s]\titers: 1400, epoch: 4 | loss: 0.2387441\n",
      "\tspeed: 0.1333s/iter; left time: 3326.5425s\n",
      "1499it [03:19,  7.57it/s]\titers: 1500, epoch: 4 | loss: 0.4468493\n",
      "\tspeed: 0.1328s/iter; left time: 3301.8544s\n",
      "1599it [03:32,  7.57it/s]\titers: 1600, epoch: 4 | loss: 0.1831624\n",
      "\tspeed: 0.1320s/iter; left time: 3267.2144s\n",
      "1699it [03:45,  7.57it/s]\titers: 1700, epoch: 4 | loss: 0.5170913\n",
      "\tspeed: 0.1322s/iter; left time: 3258.4198s\n",
      "1799it [03:58,  7.54it/s]\titers: 1800, epoch: 4 | loss: 0.2346326\n",
      "\tspeed: 0.1321s/iter; left time: 3244.4253s\n",
      "1899it [04:11,  7.59it/s]\titers: 1900, epoch: 4 | loss: 0.3640767\n",
      "\tspeed: 0.1320s/iter; left time: 3229.3617s\n",
      "1999it [04:25,  7.56it/s]\titers: 2000, epoch: 4 | loss: 0.3537697\n",
      "\tspeed: 0.1321s/iter; left time: 3216.8289s\n",
      "2099it [04:38,  7.56it/s]\titers: 2100, epoch: 4 | loss: 0.2310031\n",
      "\tspeed: 0.1321s/iter; left time: 3205.3738s\n",
      "2199it [04:51,  7.55it/s]\titers: 2200, epoch: 4 | loss: 0.3086650\n",
      "\tspeed: 0.1326s/iter; left time: 3202.0230s\n",
      "2299it [05:04,  7.58it/s]\titers: 2300, epoch: 4 | loss: 0.3222348\n",
      "\tspeed: 0.1324s/iter; left time: 3185.2207s\n",
      "2399it [05:18,  7.56it/s]\titers: 2400, epoch: 4 | loss: 0.2270203\n",
      "\tspeed: 0.1323s/iter; left time: 3168.5461s\n",
      "2499it [05:31,  7.51it/s]\titers: 2500, epoch: 4 | loss: 0.1147346\n",
      "\tspeed: 0.1341s/iter; left time: 3199.6179s\n",
      "2599it [05:44,  7.55it/s]\titers: 2600, epoch: 4 | loss: 0.2836791\n",
      "\tspeed: 0.1322s/iter; left time: 3140.1287s\n",
      "2699it [05:57,  7.56it/s]\titers: 2700, epoch: 4 | loss: 0.1636396\n",
      "\tspeed: 0.1321s/iter; left time: 3125.3402s\n",
      "2799it [06:11,  7.57it/s]\titers: 2800, epoch: 4 | loss: 0.2994059\n",
      "\tspeed: 0.1322s/iter; left time: 3114.6640s\n",
      "2899it [06:24,  7.54it/s]\titers: 2900, epoch: 4 | loss: 0.3039326\n",
      "\tspeed: 0.1325s/iter; left time: 3108.2394s\n",
      "2999it [06:37,  7.35it/s]\titers: 3000, epoch: 4 | loss: 0.2165833\n",
      "\tspeed: 0.1327s/iter; left time: 3099.0823s\n",
      "3099it [06:50,  7.58it/s]\titers: 3100, epoch: 4 | loss: 0.2040674\n",
      "\tspeed: 0.1330s/iter; left time: 3093.5986s\n",
      "3199it [07:04,  7.56it/s]\titers: 3200, epoch: 4 | loss: 0.1911794\n",
      "\tspeed: 0.1331s/iter; left time: 3081.1424s\n",
      "3299it [07:17,  7.52it/s]\titers: 3300, epoch: 4 | loss: 0.2202309\n",
      "\tspeed: 0.1334s/iter; left time: 3075.5599s\n",
      "3399it [07:30,  7.54it/s]\titers: 3400, epoch: 4 | loss: 0.1280185\n",
      "\tspeed: 0.1326s/iter; left time: 3044.0447s\n",
      "3499it [07:44,  7.54it/s]\titers: 3500, epoch: 4 | loss: 0.2306248\n",
      "\tspeed: 0.1328s/iter; left time: 3034.3671s\n",
      "3599it [07:57,  7.54it/s]\titers: 3600, epoch: 4 | loss: 0.1040642\n",
      "\tspeed: 0.1335s/iter; left time: 3037.8985s\n",
      "3699it [08:10,  7.53it/s]\titers: 3700, epoch: 4 | loss: 0.2552805\n",
      "\tspeed: 0.1321s/iter; left time: 2991.8523s\n",
      "3765it [08:19,  7.54it/s]\n",
      "Epoch: 4 cost time: 499.4989092350006\n",
      "810it [00:51, 15.58it/s]\n",
      "807it [00:52, 15.47it/s]\n",
      "Epoch: 4 | Train Loss: 0.2373823 Vali Loss: 0.2769820 Test Loss: 0.3434146 MAE Loss: 0.3444500\n",
      "Validation loss decreased (0.292648 --> 0.276982).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "learning_rate 0.001\n",
      "lr 3.9999999999999996e-05\n",
      "60it [00:08,  7.52it/s]^C\n",
      "60it [00:08,  7.17it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./Time-LLM/run_main.py\", line 234, in <module>\n",
      "    \n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1852, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 242, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 280, in forecast\n",
      "    prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2577, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2663, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2854, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 701, in get_input_ids\n",
      "    return self.convert_tokens_to_ids(tokens)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 579, in convert_tokens_to_ids\n",
      "    ids.append(self._convert_token_to_id_with_added_voc(token))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 582, in _convert_token_to_id_with_added_voc\n",
      "    def _convert_token_to_id_with_added_voc(self, token):\n",
      "\n",
      "KeyboardInterrupt\n",
      "Total time: 41.04080427090327 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=10\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-05-18 14:56:19,301] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:56:19,304] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:56:19,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:56:20,277] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:56:20,277] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:56:20,277] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:56:20,277] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 695\n",
      "val 195\n",
      "test 75\n",
      "train 695\n",
      "val 195\n",
      "test 75\n",
      "train 695\n",
      "val 195\n",
      "test 75\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-18 14:56:21,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-18 14:56:22,679] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-18 14:56:22,679] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-18 14:56:22,679] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-18 14:56:22,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-18 14:56:22,680] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-18 14:56:22,680] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-18 14:56:22,680] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-18 14:56:22,680] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-18 14:56:22,680] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-18 14:56:22,680] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "0it [00:00, ?it/s][2024-05-18 14:56:22,955] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-18 14:56:22,956] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.36 GB         CA 0.38 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:56:22,956] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.15 GB, percent = 7.8%\n",
      "[2024-05-18 14:56:23,142] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-18 14:56:23,143] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.39 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:56:23,143] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.38 GB, percent = 7.9%\n",
      "[2024-05-18 14:56:23,143] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-18 14:56:23,245] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-18 14:56:23,245] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.33 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:56:23,245] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.62 GB, percent = 7.9%\n",
      "[2024-05-18 14:56:23,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-18 14:56:23,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-18 14:56:23,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-18 14:56:23,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-18 14:56:23,246] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-18 14:56:23,246] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-18 14:56:23,246] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-18 14:56:23,246] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-18 14:56:23,246] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa49db48b50>\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-18 14:56:23,247] [INFO] [config.py:1000:print]   train_batch_size ............. 72\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   world_size ................... 3\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-18 14:56:23,248] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 72, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "9it [00:02,  3.14it/s]\n",
      "9it [00:02,  3.14it/s]\n",
      "9it [00:02,  3.72it/s]\n",
      "Epoch: 1 cost time: 2.4226653575897217\n",
      "2it [00:00,  3.23it/s]\n",
      "2it [00:00,  3.24it/s]\n",
      "2it [00:00,  3.23it/s]\n",
      "1it [00:00,  2.18it/s]\n",
      "1it [00:00,  2.17it/s]\n",
      "1it [00:00,  2.19it/s]\n",
      "Epoch: 1 | Train Loss: 1.7797860 Vali Loss: 0.5421565 Test Loss: 0.3647018 MAE Loss: 0.4002779\n",
      "Validation loss decreased (inf --> 0.542156).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "9it [00:02,  4.19it/s]\n",
      "9it [00:02,  4.19it/s]\n",
      "9it [00:02,  4.19it/s]\n",
      "Epoch: 2 cost time: 2.1477303504943848\n",
      "2it [00:00,  3.50it/s]\n",
      "\n",
      "2it [00:00,  3.53it/s]\n",
      "1it [00:00,  2.11it/s]\n",
      "\n",
      "1it [00:00,  2.11it/s]\n",
      "Epoch: 2 | Train Loss: 1.3189792 Vali Loss: 0.4916477 Test Loss: 0.3679875 MAE Loss: 0.3957626\n",
      "Validation loss decreased (0.542156 --> 0.491648).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "1it [00:00,  1.12it/s]\n",
      "1it [00:00,  1.12it/s]\n",
      "1it [00:00,  1.12it/s]\n",
      "mse:0.3679874837398529, mae:0.39576256275177\n",
      "train_losses mse:0.3679874837398529, mae:0.39576256275177\n",
      "[1.6690247986051772, 1.3587378329700894]train_losses\n",
      " val_losses [0.542156457901001, 0.4916476905345917]\n",
      "[1.5889913770887587, 1.4698483877711825]\n",
      "val_losses [0.542156457901001, 0.4916476905345917]\n",
      "mse:0.3679874837398529, mae:0.39576256275177\n",
      "train_losses [1.77978601720598, 1.3189791838328044]\n",
      "val_losses [0.542156457901001, 0.4916476905345917]\n",
      "Total time: 0.6070495168368022 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level for the `accelerate` library (or its submodules) to a higher level like `WARNING` or `ERROR`. This will suppress informational messages like the ones you're seeing.\n",
    "\n",
    "#logging.getLogger('accelerator').setLevel(logging.WARNING)  # Adjust level as needed\n",
    "logging.getLogger('DeepSpeed').setLevel(logging.WARNING)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=2\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='GB_data_small'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --multi_gpu --num_processes=3 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data_small.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 20 \\\n",
    "  --label_len 10 \\\n",
    "  --pred_len 10 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-05-18 14:57:07,194] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:57:07,197] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:57:07,223] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 14:57:07,967] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:57:08,010] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:57:08,019] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 14:57:08,019] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 695\n",
      "val 195\n",
      "test 75\n",
      "train 695\n",
      "train 695\n",
      "val 195\n",
      "test 75\n",
      "val 195\n",
      "test 75\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-18 14:57:09,624] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-18 14:57:10,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-18 14:57:10,252] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-18 14:57:10,252] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-18 14:57:10,253] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-18 14:57:10,253] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-18 14:57:10,253] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-18 14:57:10,253] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-18 14:57:10,253] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-18 14:57:10,253] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-18 14:57:10,253] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "0it [00:00, ?it/s][2024-05-18 14:57:10,616] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-18 14:57:10,617] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.36 GB         CA 0.38 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:57:10,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.21 GB, percent = 7.8%\n",
      "[2024-05-18 14:57:10,767] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-18 14:57:10,768] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.39 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:57:10,768] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.41 GB, percent = 7.9%\n",
      "[2024-05-18 14:57:10,768] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-18 14:57:10,909] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-18 14:57:10,910] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.33 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 14:57:10,910] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.78 GB, percent = 7.9%\n",
      "[2024-05-18 14:57:10,910] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-18 14:57:10,911] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-18 14:57:10,911] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-18 14:57:10,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb410fb5550>\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-18 14:57:10,911] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   train_batch_size ............. 72\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   world_size ................... 3\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-18 14:57:10,912] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-18 14:57:10,913] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 72, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "9it [00:03,  2.97it/s]\n",
      "9it [00:03,  2.98it/s]\n",
      "9it [00:02,  3.45it/s]\n",
      "Epoch: 1 cost time: 2.6094071865081787\n",
      "2it [00:00,  2.82it/s]\n",
      "2it [00:00,  2.82it/s]\n",
      "2it [00:00,  2.82it/s]\n",
      "1it [00:00,  1.95it/s]\n",
      "1it [00:00,  1.95it/s]\n",
      "1it [00:00,  1.95it/s]\n",
      "Epoch: 1 | Train Loss: 1.7797860 Vali Loss: 0.5421565 Test Loss: 0.3647018 MAE Loss: 0.4002779\n",
      "Validation loss decreased (inf --> 0.542156).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "9it [00:02,  4.22it/s]\n",
      "9it [00:02,  4.22it/s]\n",
      "9it [00:02,  4.22it/s]\n",
      "Epoch: 2 cost time: 2.1349241733551025\n",
      "2it [00:00,  3.64it/s]\n",
      "2it [00:00,  3.62it/s]\n",
      "2it [00:00,  3.64it/s]\n",
      "1it [00:00,  2.10it/s]\n",
      "1it [00:00,  2.10it/s]\n",
      "1it [00:00,  2.09it/s]\n",
      "Epoch: 2 | Train Loss: 1.3189792 Vali Loss: 0.4916477 Test Loss: 0.3679875 MAE Loss: 0.3957626\n",
      "Validation loss decreased (0.542156 --> 0.491648).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "1it [00:00,  1.17it/s]\n",
      "1it [00:00,  1.15it/s]\n",
      "1it [00:00,  1.15it/s]\n",
      "mse:0.3679874837398529, mae:0.39576256275177\n",
      "train_losses [1.6690247986051772, 1.3587378329700894]\n",
      "val_losses [0.542156457901001, 0.4916476905345917]\n",
      "mse:0.3679874837398529, mae:0.39576256275177\n",
      "train_losses [1.5889913770887587, 1.4698483877711825]\n",
      "val_losses [0.542156457901001, 0.4916476905345917]\n",
      "mse:0.3679874837398529, mae:0.39576256275177\n",
      "train_losses [1.77978601720598, 1.3189791838328044]\n",
      "val_losses [0.542156457901001, 0.4916476905345917]\n",
      "Total time: 0.5747854193051656 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level for the `accelerate` library (or its submodules) to a higher level like `WARNING` or `ERROR`. This will suppress informational messages like the ones you're seeing.\n",
    "\n",
    "#logging.getLogger('accelerator').setLevel(logging.WARNING)  # Adjust level as needed\n",
    "logging.getLogger('DeepSpeed').setLevel(logging.WARNING)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=2\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='GB_data_small'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2\"\n",
    "\n",
    "\n",
    "!accelerate launch --mixed_precision bf16 --multi_gpu --num_processes=3 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path GB_data_small.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data GB \\\n",
    "  --features M \\\n",
    "  --seq_len 20 \\\n",
    "  --label_len 10 \\\n",
    "  --pred_len 10 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-05-18 15:09:09,116] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 15:09:09,115] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 15:09:09,137] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-18 15:09:10,101] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 15:09:10,102] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 15:09:10,102] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-18 15:09:10,102] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 89115\n",
      "val 19443\n",
      "test 19371\n",
      "train 89115\n",
      "train 89115\n",
      "val 19443\n",
      "val 19443\n",
      "test 19371\n",
      "test 19371\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-05-18 15:09:11,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-18 15:09:12,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-18 15:09:12,479] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-18 15:09:12,479] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-18 15:09:12,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-18 15:09:12,480] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-18 15:09:12,480] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-18 15:09:12,480] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-18 15:09:12,480] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-18 15:09:12,480] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-18 15:09:12,480] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "0it [00:00, ?it/s][2024-05-18 15:09:12,726] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-18 15:09:12,727] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.36 GB         CA 0.38 GB         Max_CA 0 GB \n",
      "[2024-05-18 15:09:12,727] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.27 GB, percent = 7.9%\n",
      "[2024-05-18 15:09:12,855] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-18 15:09:12,856] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.39 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 15:09:12,856] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.34 GB, percent = 7.9%\n",
      "[2024-05-18 15:09:12,856] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-18 15:09:12,963] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-18 15:09:12,964] [INFO] [utils.py:801:see_memory_usage] MA 0.33 GB         Max_MA 0.33 GB         CA 0.44 GB         Max_CA 0 GB \n",
      "[2024-05-18 15:09:12,964] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.57 GB, percent = 7.9%\n",
      "[2024-05-18 15:09:12,964] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-18 15:09:12,964] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-18 15:09:12,965] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-18 15:09:12,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0eb0eca610>\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-18 15:09:12,965] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   train_batch_size ............. 72\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   world_size ................... 3\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-18 15:09:12,966] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 72, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:23,  4.28it/s]\titers: 100, epoch: 1 | loss: 0.2338368\n",
      "\tspeed: 0.2516s/iter; left time: 9316.4275s\n",
      "199it [00:47,  4.28it/s]\titers: 200, epoch: 1 | loss: 0.3919589\n",
      "\tspeed: 0.2336s/iter; left time: 8627.7709s\n",
      "299it [01:10,  4.27it/s]\titers: 300, epoch: 1 | loss: 0.3580763\n",
      "\tspeed: 0.2336s/iter; left time: 8602.7467s\n",
      "399it [01:34,  4.28it/s]\titers: 400, epoch: 1 | loss: 0.4503109\n",
      "\tspeed: 0.2336s/iter; left time: 8579.9874s\n",
      "499it [01:57,  4.28it/s]\titers: 500, epoch: 1 | loss: 0.2813678\n",
      "\tspeed: 0.2337s/iter; left time: 8559.3640s\n",
      "599it [02:20,  4.28it/s]\titers: 600, epoch: 1 | loss: 0.5090407\n",
      "\tspeed: 0.2336s/iter; left time: 8533.9745s\n",
      "699it [02:44,  4.27it/s]\titers: 700, epoch: 1 | loss: 0.3414985\n",
      "\tspeed: 0.2336s/iter; left time: 8510.2458s\n",
      "799it [03:07,  4.28it/s]\titers: 800, epoch: 1 | loss: 0.2279195\n",
      "\tspeed: 0.2343s/iter; left time: 8512.3415s\n",
      "899it [03:30,  4.27it/s]\titers: 900, epoch: 1 | loss: 0.3915574\n",
      "\tspeed: 0.2339s/iter; left time: 8475.9589s\n",
      "999it [03:54,  4.27it/s]\titers: 1000, epoch: 1 | loss: 0.1924698\n",
      "\tspeed: 0.2339s/iter; left time: 8452.6986s\n",
      "1099it [04:17,  4.28it/s]\titers: 1100, epoch: 1 | loss: 0.2865834\n",
      "\tspeed: 0.2338s/iter; left time: 8425.2877s\n",
      "1199it [04:41,  4.29it/s]\titers: 1200, epoch: 1 | loss: 0.1711770\n",
      "\tspeed: 0.2337s/iter; left time: 8397.2956s\n",
      "1237it [04:50,  4.26it/s]\n",
      "1237it [04:50,  4.26it/s]\n",
      "1237it [04:50,  4.26it/s]\n",
      "Epoch: 1 cost time: 290.0600655078888\n",
      "270it [00:27,  9.78it/s]\n",
      "270it [00:27,  9.78it/s]\n",
      "270it [00:27,  9.78it/s]\n",
      "269it [00:27,  9.81it/s]\n",
      "269it [00:27,  9.81it/s]\n",
      "269it [00:27,  9.81it/s]\n",
      "Epoch: 1 | Train Loss: 0.3235069 Vali Loss: 0.3234685 Test Loss: 0.3985661 MAE Loss: 0.4217681\n",
      "Validation loss decreased (inf --> 0.323468).  Saving model ...\n",
      "0it [00:00, ?it/s]lr = 0.0000400000\n",
      "99it [00:23,  4.30it/s]\titers: 100, epoch: 2 | loss: 0.2064619\n",
      "\tspeed: 0.9192s/iter; left time: 30626.6488s\n",
      "199it [00:46,  4.30it/s]\titers: 200, epoch: 2 | loss: 0.3930522\n",
      "\tspeed: 0.2326s/iter; left time: 7728.1486s\n",
      "299it [01:09,  4.29it/s]\titers: 300, epoch: 2 | loss: 0.2777818\n",
      "\tspeed: 0.2326s/iter; left time: 7703.6996s\n",
      "399it [01:33,  4.30it/s]\titers: 400, epoch: 2 | loss: 0.1422874\n",
      "\tspeed: 0.2323s/iter; left time: 7671.7075s\n",
      "499it [01:56,  4.31it/s]\titers: 500, epoch: 2 | loss: 0.3869125\n",
      "\tspeed: 0.2324s/iter; left time: 7649.5992s\n",
      "599it [02:19,  4.31it/s]\titers: 600, epoch: 2 | loss: 0.2200215\n",
      "\tspeed: 0.2328s/iter; left time: 7638.4915s\n",
      "699it [02:42,  4.30it/s]\titers: 700, epoch: 2 | loss: 0.1777192\n",
      "\tspeed: 0.2326s/iter; left time: 7611.3290s\n",
      "799it [03:06,  4.31it/s]\titers: 800, epoch: 2 | loss: 0.3462526\n",
      "\tspeed: 0.2323s/iter; left time: 7578.7797s\n",
      "899it [03:29,  4.30it/s]\titers: 900, epoch: 2 | loss: 0.1723523\n",
      "\tspeed: 0.2324s/iter; left time: 7557.9241s\n",
      "999it [03:52,  4.31it/s]\titers: 1000, epoch: 2 | loss: 0.2949506\n",
      "\tspeed: 0.2323s/iter; left time: 7530.8460s\n",
      "1099it [04:15,  4.30it/s]\titers: 1100, epoch: 2 | loss: 0.2365257\n",
      "\tspeed: 0.2327s/iter; left time: 7520.2890s\n",
      "1199it [04:39,  4.30it/s]\titers: 1200, epoch: 2 | loss: 0.2073333\n",
      "\tspeed: 0.2324s/iter; left time: 7488.6712s\n",
      "1237it [04:48,  4.29it/s]\n",
      "\n",
      "Epoch: 2 cost time: 288.11002707481384\n",
      "1237it [04:48,  4.29it/s]\n",
      "270it [00:27,  9.92it/s]\n",
      "\n",
      "270it [00:27,  9.92it/s]\n",
      "269it [00:26, 10.13it/s]\n",
      "269it [00:26, 10.13it/s]\n",
      "269it [00:26, 10.13it/s]\n",
      "Epoch: 2 | Train Loss: 0.2557515 Vali Loss: 0.3034147 Test Loss: 0.3629858 MAE Loss: 0.3847603\n",
      "Validation loss decreased (0.323468 --> 0.303415).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.32it/s]\titers: 100, epoch: 3 | loss: 0.3321282\n",
      "\tspeed: 0.9121s/iter; left time: 27004.0295s\n",
      "199it [00:46,  4.31it/s]\titers: 200, epoch: 3 | loss: 0.2496302\n",
      "\tspeed: 0.2318s/iter; left time: 6837.8741s\n",
      "299it [01:09,  4.32it/s]\titers: 300, epoch: 3 | loss: 0.2208373\n",
      "\tspeed: 0.2315s/iter; left time: 6807.1565s\n",
      "399it [01:32,  4.31it/s]\titers: 400, epoch: 3 | loss: 0.1461284\n",
      "\tspeed: 0.2319s/iter; left time: 6796.4685s\n",
      "499it [01:56,  4.31it/s]\titers: 500, epoch: 3 | loss: 0.1255829\n",
      "\tspeed: 0.2318s/iter; left time: 6768.4559s\n",
      "599it [02:19,  4.31it/s]\titers: 600, epoch: 3 | loss: 0.2365111\n",
      "\tspeed: 0.2317s/iter; left time: 6743.0481s\n",
      "699it [02:42,  4.32it/s]\titers: 700, epoch: 3 | loss: 0.1926436\n",
      "\tspeed: 0.2316s/iter; left time: 6718.2119s\n",
      "799it [03:05,  4.32it/s]\titers: 800, epoch: 3 | loss: 0.3557851\n",
      "\tspeed: 0.2317s/iter; left time: 6696.1195s\n",
      "899it [03:28,  4.32it/s]\titers: 900, epoch: 3 | loss: 0.3506252\n",
      "\tspeed: 0.2316s/iter; left time: 6671.0159s\n",
      "999it [03:51,  4.32it/s]\titers: 1000, epoch: 3 | loss: 0.1919847\n",
      "\tspeed: 0.2317s/iter; left time: 6650.3406s\n",
      "1099it [04:15,  4.32it/s]\titers: 1100, epoch: 3 | loss: 0.2017919\n",
      "\tspeed: 0.2316s/iter; left time: 6624.0773s\n",
      "1199it [04:38,  4.32it/s]\titers: 1200, epoch: 3 | loss: 0.1624073\n",
      "\tspeed: 0.2315s/iter; left time: 6597.5569s\n",
      "1237it [04:47,  4.31it/s]\n",
      "1237it [04:47,  4.31it/s]\n",
      "1237it [04:47,  4.31it/s]\n",
      "Epoch: 3 cost time: 287.1023545265198\n",
      "270it [00:27,  9.97it/s]\n",
      "270it [00:27,  9.97it/s]\n",
      "270it [00:27,  9.97it/s]\n",
      "269it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "Epoch: 3 | Train Loss: 0.2425380 Vali Loss: 0.2877412 Test Loss: 0.3531875 MAE Loss: 0.3706547\n",
      "Validation loss decreased (0.303415 --> 0.287741).  Saving model ...\n",
      "0it [00:00, ?it/s]lr = 0.0000400000\n",
      "99it [00:23,  4.31it/s]\titers: 100, epoch: 4 | loss: 0.2046693\n",
      "\tspeed: 0.9416s/iter; left time: 24378.9870s\n",
      "199it [00:46,  4.33it/s]\titers: 200, epoch: 4 | loss: 0.2139112\n",
      "\tspeed: 0.2312s/iter; left time: 5964.1049s\n",
      "299it [01:09,  4.33it/s]\titers: 300, epoch: 4 | loss: 0.1839541\n",
      "\tspeed: 0.2310s/iter; left time: 5933.8560s\n",
      "399it [01:32,  4.32it/s]\titers: 400, epoch: 4 | loss: 0.3666812\n",
      "\tspeed: 0.2317s/iter; left time: 5930.8244s\n",
      "499it [01:55,  4.32it/s]\titers: 500, epoch: 4 | loss: 0.1913617\n",
      "\tspeed: 0.2312s/iter; left time: 5893.1401s\n",
      "599it [02:18,  4.33it/s]\titers: 600, epoch: 4 | loss: 0.2976087\n",
      "\tspeed: 0.2311s/iter; left time: 5868.2515s\n",
      "699it [02:41,  4.32it/s]\titers: 700, epoch: 4 | loss: 0.2763425\n",
      "\tspeed: 0.2312s/iter; left time: 5846.3691s\n",
      "799it [03:05,  4.34it/s]\titers: 800, epoch: 4 | loss: 0.1731579\n",
      "\tspeed: 0.2310s/iter; left time: 5819.8060s\n",
      "899it [03:28,  4.33it/s]\titers: 900, epoch: 4 | loss: 0.3909900\n",
      "\tspeed: 0.2316s/iter; left time: 5811.0626s\n",
      "999it [03:51,  4.33it/s]\titers: 1000, epoch: 4 | loss: 0.3055318\n",
      "\tspeed: 0.2311s/iter; left time: 5776.3210s\n",
      "1099it [04:14,  4.32it/s]\titers: 1100, epoch: 4 | loss: 0.2751352\n",
      "\tspeed: 0.2313s/iter; left time: 5757.9857s\n",
      "1199it [04:37,  4.33it/s]\titers: 1200, epoch: 4 | loss: 0.1747742\n",
      "\tspeed: 0.2311s/iter; left time: 5729.3488s\n",
      "1237it [04:46,  4.32it/s]\n",
      "1237it [04:47,  4.30it/s]\n",
      "1237it [04:46,  4.32it/s]\n",
      "Epoch: 4 cost time: 286.4650192260742\n",
      "270it [00:26, 10.05it/s]\n",
      "270it [00:26, 10.05it/s]\n",
      "270it [00:26, 10.05it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "Epoch: 4 | Train Loss: 0.2442119 Vali Loss: 0.2818128 Test Loss: 0.3530701 MAE Loss: 0.3787316\n",
      "Validation loss decreased (0.287741 --> 0.281813).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:24,  4.32it/s]\titers: 100, epoch: 5 | loss: 0.2269319\n",
      "\tspeed: 0.9035s/iter; left time: 20037.8303s\n",
      "199it [00:47,  4.33it/s]\titers: 200, epoch: 5 | loss: 0.4186458\n",
      "\tspeed: 0.2309s/iter; left time: 5098.8453s\n",
      "299it [01:10,  4.33it/s]\titers: 300, epoch: 5 | loss: 0.1994685\n",
      "\tspeed: 0.2310s/iter; left time: 5077.5441s\n",
      "399it [01:33,  4.33it/s]\titers: 400, epoch: 5 | loss: 0.3148949\n",
      "\tspeed: 0.2306s/iter; left time: 5045.8950s\n",
      "499it [01:56,  4.34it/s]\titers: 500, epoch: 5 | loss: 0.1404067\n",
      "\tspeed: 0.2308s/iter; left time: 5026.9481s\n",
      "599it [02:19,  4.33it/s]\titers: 600, epoch: 5 | loss: 0.2081823\n",
      "\tspeed: 0.2306s/iter; left time: 4999.7443s\n",
      "699it [02:42,  4.33it/s]\titers: 700, epoch: 5 | loss: 0.1925576\n",
      "\tspeed: 0.2308s/iter; left time: 4980.2572s\n",
      "799it [03:05,  4.34it/s]\titers: 800, epoch: 5 | loss: 0.2641938\n",
      "\tspeed: 0.2308s/iter; left time: 4957.4389s\n",
      "899it [03:28,  4.33it/s]\titers: 900, epoch: 5 | loss: 0.3422894\n",
      "\tspeed: 0.2309s/iter; left time: 4935.9754s\n",
      "999it [03:51,  4.33it/s]\titers: 1000, epoch: 5 | loss: 0.1672980\n",
      "\tspeed: 0.2309s/iter; left time: 4912.3724s\n",
      "1099it [04:15,  4.34it/s]\titers: 1100, epoch: 5 | loss: 0.2438947\n",
      "\tspeed: 0.2305s/iter; left time: 4882.8007s\n",
      "1199it [04:38,  4.33it/s]\titers: 1200, epoch: 5 | loss: 0.3016025\n",
      "\tspeed: 0.2306s/iter; left time: 4861.0523s\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:46,  4.31it/s]\n",
      "Epoch: 5 cost time: 286.94688391685486\n",
      "1237it [04:45,  4.33it/s]\n",
      "270it [00:26, 10.13it/s]\n",
      "270it [00:26, 10.13it/s]\n",
      "270it [00:26, 10.13it/s]\n",
      "269it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.18it/s]\n",
      "Epoch: 5 | Train Loss: 0.2309175 Vali Loss: 0.2840051 Test Loss: 0.3589971 MAE Loss: 0.3795014\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.34it/s]\titers: 100, epoch: 6 | loss: 0.1849926\n",
      "\tspeed: 0.8510s/iter; left time: 15713.8132s\n",
      "199it [00:46,  4.34it/s]\titers: 200, epoch: 6 | loss: 0.1626025\n",
      "\tspeed: 0.2304s/iter; left time: 4232.3390s\n",
      "299it [01:09,  4.34it/s]\titers: 300, epoch: 6 | loss: 0.3379232\n",
      "\tspeed: 0.2308s/iter; left time: 4215.1353s\n",
      "399it [01:32,  4.33it/s]\titers: 400, epoch: 6 | loss: 0.2419637\n",
      "\tspeed: 0.2308s/iter; left time: 4193.3765s\n",
      "499it [01:55,  4.34it/s]\titers: 500, epoch: 6 | loss: 0.3440466\n",
      "\tspeed: 0.2308s/iter; left time: 4169.5047s\n",
      "599it [02:18,  4.33it/s]\titers: 600, epoch: 6 | loss: 0.2128929\n",
      "\tspeed: 0.2309s/iter; left time: 4147.8071s\n",
      "699it [02:41,  4.34it/s]\titers: 700, epoch: 6 | loss: 0.2033142\n",
      "\tspeed: 0.2307s/iter; left time: 4121.6122s\n",
      "799it [03:04,  4.34it/s]\titers: 800, epoch: 6 | loss: 0.1967149\n",
      "\tspeed: 0.2311s/iter; left time: 4105.0322s\n",
      "899it [03:27,  4.34it/s]\titers: 900, epoch: 6 | loss: 0.2492412\n",
      "\tspeed: 0.2305s/iter; left time: 4072.4001s\n",
      "999it [03:50,  4.34it/s]\titers: 1000, epoch: 6 | loss: 0.2377310\n",
      "\tspeed: 0.2308s/iter; left time: 4054.2311s\n",
      "1099it [04:13,  4.34it/s]\titers: 1100, epoch: 6 | loss: 0.3103164\n",
      "\tspeed: 0.2306s/iter; left time: 4027.8150s\n",
      "1199it [04:36,  4.34it/s]\titers: 1200, epoch: 6 | loss: 0.3374685\n",
      "\tspeed: 0.2304s/iter; left time: 4001.9158s\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "Epoch: 6 cost time: 285.81378078460693\n",
      "270it [00:26, 10.08it/s]\n",
      "270it [00:26, 10.08it/s]\n",
      "\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "Epoch: 6 | Train Loss: 0.2322039 Vali Loss: 0.2804268 Test Loss: 0.3538222 MAE Loss: 0.3631575\n",
      "Validation loss decreased (0.281813 --> 0.280427).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.34it/s]\titers: 100, epoch: 7 | loss: 0.1809644\n",
      "\tspeed: 0.9075s/iter; left time: 13388.3167s\n",
      "199it [00:46,  4.34it/s]\titers: 200, epoch: 7 | loss: 0.1849745\n",
      "\tspeed: 0.2304s/iter; left time: 3376.5525s\n",
      "299it [01:09,  4.34it/s]\titers: 300, epoch: 7 | loss: 0.1153273\n",
      "\tspeed: 0.2302s/iter; left time: 3349.9577s\n",
      "399it [01:32,  4.30it/s]\titers: 400, epoch: 7 | loss: 0.2128937\n",
      "\tspeed: 0.2306s/iter; left time: 3333.3892s\n",
      "499it [01:55,  4.35it/s]\titers: 500, epoch: 7 | loss: 0.2025683\n",
      "\tspeed: 0.2303s/iter; left time: 3305.3703s\n",
      "599it [02:18,  4.34it/s]\titers: 600, epoch: 7 | loss: 0.1687654\n",
      "\tspeed: 0.2303s/iter; left time: 3283.1313s\n",
      "699it [02:41,  4.35it/s]\titers: 700, epoch: 7 | loss: 0.1129322\n",
      "\tspeed: 0.2302s/iter; left time: 3258.0942s\n",
      "799it [03:04,  4.34it/s]\titers: 800, epoch: 7 | loss: 0.1632805\n",
      "\tspeed: 0.2301s/iter; left time: 3234.0867s\n",
      "899it [03:27,  4.34it/s]\titers: 900, epoch: 7 | loss: 0.1306383\n",
      "\tspeed: 0.2307s/iter; left time: 3218.8777s\n",
      "999it [03:50,  4.36it/s]\titers: 1000, epoch: 7 | loss: 0.1985820\n",
      "\tspeed: 0.2302s/iter; left time: 3189.4289s\n",
      "1099it [04:13,  4.31it/s]\titers: 1100, epoch: 7 | loss: 0.2904322\n",
      "\tspeed: 0.2304s/iter; left time: 3169.1220s\n",
      "1199it [04:36,  4.35it/s]\titers: 1200, epoch: 7 | loss: 0.1280741\n",
      "\tspeed: 0.2303s/iter; left time: 3144.1302s\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "Epoch: 7 cost time: 285.4704911708832\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.23it/s]\n",
      "269it [00:26, 10.23it/s]\n",
      "269it [00:26, 10.23it/s]\n",
      "Epoch: 7 | Train Loss: 0.2239995 Vali Loss: 0.2736918 Test Loss: 0.3400456 MAE Loss: 0.3501729\n",
      "Validation loss decreased (0.280427 --> 0.273692).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.35it/s]\titers: 100, epoch: 8 | loss: 0.1949926\n",
      "\tspeed: 0.8710s/iter; left time: 9615.4047s\n",
      "199it [00:46,  4.34it/s]\titers: 200, epoch: 8 | loss: 0.3954399\n",
      "\tspeed: 0.2305s/iter; left time: 2521.2736s\n",
      "299it [01:09,  4.33it/s]\titers: 300, epoch: 8 | loss: 0.1586261\n",
      "\tspeed: 0.2305s/iter; left time: 2498.4505s\n",
      "399it [01:32,  4.34it/s]\titers: 400, epoch: 8 | loss: 0.2319518\n",
      "\tspeed: 0.2307s/iter; left time: 2477.6788s\n",
      "499it [01:55,  4.34it/s]\titers: 500, epoch: 8 | loss: 0.2683760\n",
      "\tspeed: 0.2305s/iter; left time: 2452.8810s\n",
      "599it [02:18,  4.34it/s]\titers: 600, epoch: 8 | loss: 0.1780945\n",
      "\tspeed: 0.2300s/iter; left time: 2424.7233s\n",
      "699it [02:41,  4.34it/s]\titers: 700, epoch: 8 | loss: 0.2552770\n",
      "\tspeed: 0.2303s/iter; left time: 2404.0348s\n",
      "799it [03:04,  4.35it/s]\titers: 800, epoch: 8 | loss: 0.1875731\n",
      "\tspeed: 0.2300s/iter; left time: 2378.6544s\n",
      "899it [03:27,  4.35it/s]\titers: 900, epoch: 8 | loss: 0.1449085\n",
      "\tspeed: 0.2301s/iter; left time: 2356.4458s\n",
      "999it [03:50,  4.34it/s]\titers: 1000, epoch: 8 | loss: 0.2664477\n",
      "\tspeed: 0.2305s/iter; left time: 2337.5616s\n",
      "1099it [04:13,  4.34it/s]\titers: 1100, epoch: 8 | loss: 0.1972092\n",
      "\tspeed: 0.2302s/iter; left time: 2311.0662s\n",
      "1199it [04:36,  4.35it/s]\titers: 1200, epoch: 8 | loss: 0.1942407\n",
      "\tspeed: 0.2302s/iter; left time: 2287.9356s\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "Epoch: 8 cost time: 285.4523983001709\n",
      "1237it [04:45,  4.33it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.17it/s]\n",
      "269it [00:26, 10.17it/s]\n",
      "269it [00:26, 10.17it/s]\n",
      "Epoch: 8 | Train Loss: 0.2189498 Vali Loss: 0.2741332 Test Loss: 0.3485263 MAE Loss: 0.3568822\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.34it/s]\titers: 100, epoch: 9 | loss: 0.1457879\n",
      "\tspeed: 0.8497s/iter; left time: 6225.3898s\n",
      "199it [00:46,  4.35it/s]\titers: 200, epoch: 9 | loss: 0.1678416\n",
      "\tspeed: 0.2301s/iter; left time: 1662.9707s\n",
      "299it [01:09,  4.35it/s]\titers: 300, epoch: 9 | loss: 0.1173880\n",
      "\tspeed: 0.2302s/iter; left time: 1640.6771s\n",
      "399it [01:32,  4.35it/s]\titers: 400, epoch: 9 | loss: 0.1617281\n",
      "\tspeed: 0.2301s/iter; left time: 1617.0327s\n",
      "499it [01:55,  4.35it/s]\titers: 500, epoch: 9 | loss: 0.1292903\n",
      "\tspeed: 0.2301s/iter; left time: 1593.9991s\n",
      "599it [02:18,  4.35it/s]\titers: 600, epoch: 9 | loss: 0.1421006\n",
      "\tspeed: 0.2304s/iter; left time: 1572.8793s\n",
      "699it [02:41,  4.34it/s]\titers: 700, epoch: 9 | loss: 0.2650892\n",
      "\tspeed: 0.2302s/iter; left time: 1548.2501s\n",
      "799it [03:04,  4.35it/s]\titers: 800, epoch: 9 | loss: 0.1760664\n",
      "\tspeed: 0.2303s/iter; left time: 1526.4078s\n",
      "899it [03:27,  4.35it/s]\titers: 900, epoch: 9 | loss: 0.1561299\n",
      "\tspeed: 0.2300s/iter; left time: 1501.4146s\n",
      "999it [03:50,  4.35it/s]\titers: 1000, epoch: 9 | loss: 0.3723768\n",
      "\tspeed: 0.2301s/iter; left time: 1478.9295s\n",
      "1099it [04:13,  4.35it/s]\titers: 1100, epoch: 9 | loss: 0.2011068\n",
      "\tspeed: 0.2302s/iter; left time: 1456.2956s\n",
      "1199it [04:36,  4.35it/s]\titers: 1200, epoch: 9 | loss: 0.3204172\n",
      "\tspeed: 0.2302s/iter; left time: 1433.5864s\n",
      "1237it [04:45,  4.34it/s]\n",
      "1237it [04:45,  4.34it/s]\n",
      "1237it [04:45,  4.34it/s]\n",
      "Epoch: 9 cost time: 285.1458604335785\n",
      "270it [00:26, 10.11it/s]\n",
      "270it [00:26, 10.10it/s]\n",
      "270it [00:26, 10.11it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "269it [00:26, 10.19it/s]\n",
      "Epoch: 9 | Train Loss: 0.2164538 Vali Loss: 0.2914817 Test Loss: 0.3604188 MAE Loss: 0.3713610\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "99it [00:23,  4.35it/s]\titers: 100, epoch: 10 | loss: 0.1985932\n",
      "\tspeed: 0.8512s/iter; left time: 3076.0695s\n",
      "199it [00:46,  4.33it/s]\titers: 200, epoch: 10 | loss: 0.3308183\n",
      "\tspeed: 0.2301s/iter; left time: 808.6111s\n",
      "299it [01:09,  4.34it/s]\titers: 300, epoch: 10 | loss: 0.3448433\n",
      "\tspeed: 0.2305s/iter; left time: 786.9166s\n",
      "399it [01:32,  4.31it/s]\titers: 400, epoch: 10 | loss: 0.1603817\n",
      "\tspeed: 0.2309s/iter; left time: 765.3263s\n",
      "499it [01:55,  4.35it/s]\titers: 500, epoch: 10 | loss: 0.2873846\n",
      "\tspeed: 0.2302s/iter; left time: 739.7242s\n",
      "599it [02:18,  4.33it/s]\titers: 600, epoch: 10 | loss: 0.2127608\n",
      "\tspeed: 0.2302s/iter; left time: 716.7667s\n",
      "699it [02:41,  4.35it/s]\titers: 700, epoch: 10 | loss: 0.1733853\n",
      "\tspeed: 0.2300s/iter; left time: 693.1601s\n",
      "799it [03:04,  4.34it/s]\titers: 800, epoch: 10 | loss: 0.1946474\n",
      "\tspeed: 0.2301s/iter; left time: 670.6172s\n",
      "899it [03:27,  4.35it/s]\titers: 900, epoch: 10 | loss: 0.1825336\n",
      "\tspeed: 0.2307s/iter; left time: 649.0744s\n",
      "999it [03:50,  4.35it/s]\titers: 1000, epoch: 10 | loss: 0.1496710\n",
      "\tspeed: 0.2305s/iter; left time: 625.6396s\n",
      "1099it [04:13,  4.35it/s]\titers: 1100, epoch: 10 | loss: 0.2088047\n",
      "\tspeed: 0.2301s/iter; left time: 601.4608s\n",
      "1199it [04:36,  4.34it/s]\titers: 1200, epoch: 10 | loss: 0.1797712\n",
      "\tspeed: 0.2302s/iter; left time: 578.6114s\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "1237it [04:45,  4.33it/s]\n",
      "Epoch: 10 cost time: 285.3630356788635\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "270it [00:26, 10.18it/s]\n",
      "269it [00:26, 10.22it/s]\n",
      "269it [00:26, 10.22it/s]\n",
      "\n",
      "Epoch: 10 | Train Loss: 0.2117101 Vali Loss: 0.2838833 Test Loss: 0.3494291 MAE Loss: 0.3531988\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "269it [00:27,  9.94it/s]\n",
      "269it [00:27,  9.94it/s]\n",
      "269it [00:27,  9.94it/s]\n",
      "mse:0.34004558852032435, mae:0.35017291351211116\n",
      "train_losses [0.32350687513981696, 0.2557515266773772, 0.24253799052564193, 0.24421186510868004, 0.23091746639631983, 0.23220392276827329, 0.22399954274356992, 0.21894978406020682, 0.21645375480913018]\n",
      "val_losses [0.32346846124640216, 0.3034147344805576, 0.28774122435737537, 0.2818128319802108, 0.2840051320415956, 0.2804268256381706, 0.27369178577705666, 0.2741332018540965, 0.291481670792456]\n",
      "mse:0.34004558852032435, mae:0.35017291351211116\n",
      "train_losses mse:0.34004558852032435, mae:0.35017291351211116\n",
      "train_losses [0.32281718625955846, 0.2541058050798329, 0.2517441824476085, 0.2404350770717199, 0.23657918699151967, 0.2238810020246714, 0.22657794106025897, 0.2215172321555975, 0.21469677686269667]\n",
      "val_losses [0.32346846124640216, 0.3034147344805576, 0.28774122435737537, 0.2818128319802108, 0.2840051320415956, 0.2804268256381706, 0.27369178577705666, 0.2741332018540965, 0.291481670792456]\n",
      "[0.3288075468392264, 0.2594376241271787, 0.2469417753606047, 0.23899303758539495, 0.23484861405028773, 0.23065665359446957, 0.22126869229101423, 0.22200633893835997, 0.21724575995892986]\n",
      "val_losses [0.32346846124640216, 0.3034147344805576, 0.28774122435737537, 0.2818128319802108, 0.2840051320415956, 0.2804268256381706, 0.27369178577705666, 0.2741332018540965, 0.291481670792456]\n",
      "Total time: 57.93768241405487 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=10\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2\"\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --multi_gpu --mixed_precision bf16 --num_processes=3 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA version:\", cuda_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_17:16:49_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.103\n",
      "Build cuda_12.3.r12.3/compiler.33492891_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 18 16:24:21 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             25W /  250W |      18MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           Off |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             23W /  250W |       9MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Quadro RTX 6000                Off |   00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   26C    P8             15W /  250W |       8MiB /  23040MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6987      G   /usr/bin/X                                      4MiB |\n",
      "|    1   N/A  N/A      6987      G   /usr/bin/X                                      4MiB |\n",
      "|    2   N/A  N/A      6987      G   /usr/bin/X                                      4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "\n",
    "args = dotdict()\n",
    "\n",
    "args.task_name = \"long_term_forecast\"\n",
    "args.is_training = 1\n",
    "args.model_id = 1\n",
    "args.model_comment = \"TimeLLM-FR\"\n",
    "args.model = \"TimeLLM\"\n",
    "args.seed = 2021\n",
    "args.data = \"FR\"\n",
    "args.root_path = \"./datasets/\"\n",
    "args.data_path = \"FR_data.csv\"\n",
    "args.features = \"M\"\n",
    "args.target = \"OT\"\n",
    "args.loader = \"modal\"\n",
    "args.freq = \"h\"\n",
    "args.checkpoints = \"./checkpoints/\"\n",
    "args.seq_len = 96\n",
    "args.label_len = 48\n",
    "args.pred_len = 24\n",
    "args.seasonal_patterns = \"Monthly\"\n",
    "args.enc_in = 3\n",
    "args.dec_in = 3\n",
    "args.c_out = 3\n",
    "args.d_model = 32\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 128\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.dropout = 0.1\n",
    "args.embed = \"timeF\"\n",
    "args.activation = \"gelu\"\n",
    "args.output_attention = False\n",
    "args.patch_len = 16\n",
    "args.stride = 8\n",
    "args.prompt_domain = 0\n",
    "args.llm_model = \"GPT2\"\n",
    "args.llm_dim = 768\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 20\n",
    "args.align_epochs = 10\n",
    "args.batch_size = 24\n",
    "args.eval_batch_size = 8\n",
    "args.patience = 3\n",
    "args.learning_rate = 0.001\n",
    "args.des = \"Exp\"\n",
    "args.loss = \"MSE\"\n",
    "args.lradj = \"type1\"\n",
    "args.pct_start = 0.2\n",
    "args.use_amp = False\n",
    "args.llm_layers = 6\n",
    "args.percent = 100\n",
    "args.content = \"Hourly data detailing load, solar generation, and wind generation in France. Wind generation is onshore wind eneration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n"
     ]
    }
   ],
   "source": [
    "from models.TimeLLM import Model\n",
    "model = Model(configs=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings\n",
      "llm_model.wte.weight\n",
      "llm_model.wpe.weight\n",
      "llm_model.h.0.ln_1.weight\n",
      "llm_model.h.0.ln_1.bias\n",
      "llm_model.h.0.attn.c_attn.weight\n",
      "llm_model.h.0.attn.c_attn.bias\n",
      "llm_model.h.0.attn.c_proj.weight\n",
      "llm_model.h.0.attn.c_proj.bias\n",
      "llm_model.h.0.ln_2.weight\n",
      "llm_model.h.0.ln_2.bias\n",
      "llm_model.h.0.mlp.c_fc.weight\n",
      "llm_model.h.0.mlp.c_fc.bias\n",
      "llm_model.h.0.mlp.c_proj.weight\n",
      "llm_model.h.0.mlp.c_proj.bias\n",
      "llm_model.h.1.ln_1.weight\n",
      "llm_model.h.1.ln_1.bias\n",
      "llm_model.h.1.attn.c_attn.weight\n",
      "llm_model.h.1.attn.c_attn.bias\n",
      "llm_model.h.1.attn.c_proj.weight\n",
      "llm_model.h.1.attn.c_proj.bias\n",
      "llm_model.h.1.ln_2.weight\n",
      "llm_model.h.1.ln_2.bias\n",
      "llm_model.h.1.mlp.c_fc.weight\n",
      "llm_model.h.1.mlp.c_fc.bias\n",
      "llm_model.h.1.mlp.c_proj.weight\n",
      "llm_model.h.1.mlp.c_proj.bias\n",
      "llm_model.h.2.ln_1.weight\n",
      "llm_model.h.2.ln_1.bias\n",
      "llm_model.h.2.attn.c_attn.weight\n",
      "llm_model.h.2.attn.c_attn.bias\n",
      "llm_model.h.2.attn.c_proj.weight\n",
      "llm_model.h.2.attn.c_proj.bias\n",
      "llm_model.h.2.ln_2.weight\n",
      "llm_model.h.2.ln_2.bias\n",
      "llm_model.h.2.mlp.c_fc.weight\n",
      "llm_model.h.2.mlp.c_fc.bias\n",
      "llm_model.h.2.mlp.c_proj.weight\n",
      "llm_model.h.2.mlp.c_proj.bias\n",
      "llm_model.h.3.ln_1.weight\n",
      "llm_model.h.3.ln_1.bias\n",
      "llm_model.h.3.attn.c_attn.weight\n",
      "llm_model.h.3.attn.c_attn.bias\n",
      "llm_model.h.3.attn.c_proj.weight\n",
      "llm_model.h.3.attn.c_proj.bias\n",
      "llm_model.h.3.ln_2.weight\n",
      "llm_model.h.3.ln_2.bias\n",
      "llm_model.h.3.mlp.c_fc.weight\n",
      "llm_model.h.3.mlp.c_fc.bias\n",
      "llm_model.h.3.mlp.c_proj.weight\n",
      "llm_model.h.3.mlp.c_proj.bias\n",
      "llm_model.h.4.ln_1.weight\n",
      "llm_model.h.4.ln_1.bias\n",
      "llm_model.h.4.attn.c_attn.weight\n",
      "llm_model.h.4.attn.c_attn.bias\n",
      "llm_model.h.4.attn.c_proj.weight\n",
      "llm_model.h.4.attn.c_proj.bias\n",
      "llm_model.h.4.ln_2.weight\n",
      "llm_model.h.4.ln_2.bias\n",
      "llm_model.h.4.mlp.c_fc.weight\n",
      "llm_model.h.4.mlp.c_fc.bias\n",
      "llm_model.h.4.mlp.c_proj.weight\n",
      "llm_model.h.4.mlp.c_proj.bias\n",
      "llm_model.h.5.ln_1.weight\n",
      "llm_model.h.5.ln_1.bias\n",
      "llm_model.h.5.attn.c_attn.weight\n",
      "llm_model.h.5.attn.c_attn.bias\n",
      "llm_model.h.5.attn.c_proj.weight\n",
      "llm_model.h.5.attn.c_proj.bias\n",
      "llm_model.h.5.ln_2.weight\n",
      "llm_model.h.5.ln_2.bias\n",
      "llm_model.h.5.mlp.c_fc.weight\n",
      "llm_model.h.5.mlp.c_fc.bias\n",
      "llm_model.h.5.mlp.c_proj.weight\n",
      "llm_model.h.5.mlp.c_proj.bias\n",
      "llm_model.ln_f.weight\n",
      "llm_model.ln_f.bias\n",
      "patch_embedding.value_embedding.tokenConv.weight\n",
      "mapping_layer.weight\n",
      "mapping_layer.bias\n",
      "reprogramming_layer.query_projection.weight\n",
      "reprogramming_layer.query_projection.bias\n",
      "reprogramming_layer.key_projection.weight\n",
      "reprogramming_layer.key_projection.bias\n",
      "reprogramming_layer.value_projection.weight\n",
      "reprogramming_layer.value_projection.bias\n",
      "reprogramming_layer.out_projection.weight\n",
      "reprogramming_layer.out_projection.bias\n",
      "output_projection.linear.weight\n",
      "output_projection.linear.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('/vol/cs-hu/riabchuv/my_work/checkpoints/long_term_forecast_1_TimeLLM_GB_ftM_sl20_ll10_pl10_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp_0-GB_data_small/checkpoint.pth')\n",
    "\n",
    "# Print the keys in the checkpoint\n",
    "for key in checkpoint.keys():\n",
    "    print(key)\n",
    "\n",
    "\n",
    "# Access the model's state dictionary using the key 'llm_model'\n",
    "#model_state_dict = checkpoint['llm_model']\n",
    "\n",
    "# Load the model's state dictionary into your model\n",
    "#model.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings\n",
      "llm_model.wte.weight\n",
      "llm_model.wpe.weight\n",
      "llm_model.h.0.ln_1.weight\n",
      "llm_model.h.0.ln_1.bias\n",
      "llm_model.h.0.attn.c_attn.weight\n",
      "llm_model.h.0.attn.c_attn.bias\n",
      "llm_model.h.0.attn.c_proj.weight\n",
      "llm_model.h.0.attn.c_proj.bias\n",
      "llm_model.h.0.ln_2.weight\n",
      "llm_model.h.0.ln_2.bias\n",
      "llm_model.h.0.mlp.c_fc.weight\n",
      "llm_model.h.0.mlp.c_fc.bias\n",
      "llm_model.h.0.mlp.c_proj.weight\n",
      "llm_model.h.0.mlp.c_proj.bias\n",
      "llm_model.h.1.ln_1.weight\n",
      "llm_model.h.1.ln_1.bias\n",
      "llm_model.h.1.attn.c_attn.weight\n",
      "llm_model.h.1.attn.c_attn.bias\n",
      "llm_model.h.1.attn.c_proj.weight\n",
      "llm_model.h.1.attn.c_proj.bias\n",
      "llm_model.h.1.ln_2.weight\n",
      "llm_model.h.1.ln_2.bias\n",
      "llm_model.h.1.mlp.c_fc.weight\n",
      "llm_model.h.1.mlp.c_fc.bias\n",
      "llm_model.h.1.mlp.c_proj.weight\n",
      "llm_model.h.1.mlp.c_proj.bias\n",
      "llm_model.h.2.ln_1.weight\n",
      "llm_model.h.2.ln_1.bias\n",
      "llm_model.h.2.attn.c_attn.weight\n",
      "llm_model.h.2.attn.c_attn.bias\n",
      "llm_model.h.2.attn.c_proj.weight\n",
      "llm_model.h.2.attn.c_proj.bias\n",
      "llm_model.h.2.ln_2.weight\n",
      "llm_model.h.2.ln_2.bias\n",
      "llm_model.h.2.mlp.c_fc.weight\n",
      "llm_model.h.2.mlp.c_fc.bias\n",
      "llm_model.h.2.mlp.c_proj.weight\n",
      "llm_model.h.2.mlp.c_proj.bias\n",
      "llm_model.h.3.ln_1.weight\n",
      "llm_model.h.3.ln_1.bias\n",
      "llm_model.h.3.attn.c_attn.weight\n",
      "llm_model.h.3.attn.c_attn.bias\n",
      "llm_model.h.3.attn.c_proj.weight\n",
      "llm_model.h.3.attn.c_proj.bias\n",
      "llm_model.h.3.ln_2.weight\n",
      "llm_model.h.3.ln_2.bias\n",
      "llm_model.h.3.mlp.c_fc.weight\n",
      "llm_model.h.3.mlp.c_fc.bias\n",
      "llm_model.h.3.mlp.c_proj.weight\n",
      "llm_model.h.3.mlp.c_proj.bias\n",
      "llm_model.h.4.ln_1.weight\n",
      "llm_model.h.4.ln_1.bias\n",
      "llm_model.h.4.attn.c_attn.weight\n",
      "llm_model.h.4.attn.c_attn.bias\n",
      "llm_model.h.4.attn.c_proj.weight\n",
      "llm_model.h.4.attn.c_proj.bias\n",
      "llm_model.h.4.ln_2.weight\n",
      "llm_model.h.4.ln_2.bias\n",
      "llm_model.h.4.mlp.c_fc.weight\n",
      "llm_model.h.4.mlp.c_fc.bias\n",
      "llm_model.h.4.mlp.c_proj.weight\n",
      "llm_model.h.4.mlp.c_proj.bias\n",
      "llm_model.h.5.ln_1.weight\n",
      "llm_model.h.5.ln_1.bias\n",
      "llm_model.h.5.attn.c_attn.weight\n",
      "llm_model.h.5.attn.c_attn.bias\n",
      "llm_model.h.5.attn.c_proj.weight\n",
      "llm_model.h.5.attn.c_proj.bias\n",
      "llm_model.h.5.ln_2.weight\n",
      "llm_model.h.5.ln_2.bias\n",
      "llm_model.h.5.mlp.c_fc.weight\n",
      "llm_model.h.5.mlp.c_fc.bias\n",
      "llm_model.h.5.mlp.c_proj.weight\n",
      "llm_model.h.5.mlp.c_proj.bias\n",
      "llm_model.ln_f.weight\n",
      "llm_model.ln_f.bias\n",
      "patch_embedding.value_embedding.tokenConv.weight\n",
      "mapping_layer.weight\n",
      "mapping_layer.bias\n",
      "reprogramming_layer.query_projection.weight\n",
      "reprogramming_layer.query_projection.bias\n",
      "reprogramming_layer.key_projection.weight\n",
      "reprogramming_layer.key_projection.bias\n",
      "reprogramming_layer.value_projection.weight\n",
      "reprogramming_layer.value_projection.bias\n",
      "reprogramming_layer.out_projection.weight\n",
      "reprogramming_layer.out_projection.bias\n",
      "output_projection.linear.weight\n",
      "output_projection.linear.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('/vol/cs-hu/riabchuv/my_work/checkpoints/long_term_forecast_1_TimeLLM_FR_ftM_sl96_ll48_pl24_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp_0-TimeLLM-FR/checkpoint.pth')\n",
    "\n",
    "# Print the keys in the checkpoint\n",
    "for key in checkpoint.keys():\n",
    "    print(key)\n",
    "\n",
    "\n",
    "# Access the model's state dictionary using the key 'llm_model'\n",
    "#model_state_dict = checkpoint['llm_model']\n",
    "\n",
    "# Load the model's state dictionary into your model\n",
    "#model.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "0.68 < np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/vol/cs-hu/riabchuv/my_work/checkpoints/long_term_forecast_1_TimeLLM_FR_ftM_sl96_ll48_pl24_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp_0-TimeLLM-FR/long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['enc_embedding.value_embedding.tokenConv.weight', 'enc_embedding.position_embedding.pe', 'enc_embedding.temporal_embedding.embed.weight', 'dec_embedding.value_embedding.tokenConv.weight', 'dec_embedding.position_embedding.pe', 'dec_embedding.temporal_embedding.embed.weight', 'encoder.attn_layers.0.attention.query_projection.weight', 'encoder.attn_layers.0.attention.query_projection.bias', 'encoder.attn_layers.0.attention.key_projection.weight', 'encoder.attn_layers.0.attention.key_projection.bias', 'encoder.attn_layers.0.attention.value_projection.weight', 'encoder.attn_layers.0.attention.value_projection.bias', 'encoder.attn_layers.0.attention.out_projection.weight', 'encoder.attn_layers.0.attention.out_projection.bias', 'encoder.attn_layers.0.conv1.weight', 'encoder.attn_layers.0.conv1.bias', 'encoder.attn_layers.0.conv2.weight', 'encoder.attn_layers.0.conv2.bias', 'encoder.attn_layers.0.norm1.weight', 'encoder.attn_layers.0.norm1.bias', 'encoder.attn_layers.0.norm2.weight', 'encoder.attn_layers.0.norm2.bias', 'encoder.attn_layers.1.attention.query_projection.weight', 'encoder.attn_layers.1.attention.query_projection.bias', 'encoder.attn_layers.1.attention.key_projection.weight', 'encoder.attn_layers.1.attention.key_projection.bias', 'encoder.attn_layers.1.attention.value_projection.weight', 'encoder.attn_layers.1.attention.value_projection.bias', 'encoder.attn_layers.1.attention.out_projection.weight', 'encoder.attn_layers.1.attention.out_projection.bias', 'encoder.attn_layers.1.conv1.weight', 'encoder.attn_layers.1.conv1.bias', 'encoder.attn_layers.1.conv2.weight', 'encoder.attn_layers.1.conv2.bias', 'encoder.attn_layers.1.norm1.weight', 'encoder.attn_layers.1.norm1.bias', 'encoder.attn_layers.1.norm2.weight', 'encoder.attn_layers.1.norm2.bias', 'encoder.conv_layers.0.downConv.weight', 'encoder.conv_layers.0.downConv.bias', 'encoder.conv_layers.0.norm.weight', 'encoder.conv_layers.0.norm.bias', 'encoder.conv_layers.0.norm.running_mean', 'encoder.conv_layers.0.norm.running_var', 'encoder.conv_layers.0.norm.num_batches_tracked', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.layers.0.self_attention.query_projection.weight', 'decoder.layers.0.self_attention.query_projection.bias', 'decoder.layers.0.self_attention.key_projection.weight', 'decoder.layers.0.self_attention.key_projection.bias', 'decoder.layers.0.self_attention.value_projection.weight', 'decoder.layers.0.self_attention.value_projection.bias', 'decoder.layers.0.self_attention.out_projection.weight', 'decoder.layers.0.self_attention.out_projection.bias', 'decoder.layers.0.cross_attention.query_projection.weight', 'decoder.layers.0.cross_attention.query_projection.bias', 'decoder.layers.0.cross_attention.key_projection.weight', 'decoder.layers.0.cross_attention.key_projection.bias', 'decoder.layers.0.cross_attention.value_projection.weight', 'decoder.layers.0.cross_attention.value_projection.bias', 'decoder.layers.0.cross_attention.out_projection.weight', 'decoder.layers.0.cross_attention.out_projection.bias', 'decoder.layers.0.conv1.weight', 'decoder.layers.0.conv1.bias', 'decoder.layers.0.conv2.weight', 'decoder.layers.0.conv2.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.1.self_attention.query_projection.weight', 'decoder.layers.1.self_attention.query_projection.bias', 'decoder.layers.1.self_attention.key_projection.weight', 'decoder.layers.1.self_attention.key_projection.bias', 'decoder.layers.1.self_attention.value_projection.weight', 'decoder.layers.1.self_attention.value_projection.bias', 'decoder.layers.1.self_attention.out_projection.weight', 'decoder.layers.1.self_attention.out_projection.bias', 'decoder.layers.1.cross_attention.query_projection.weight', 'decoder.layers.1.cross_attention.query_projection.bias', 'decoder.layers.1.cross_attention.key_projection.weight', 'decoder.layers.1.cross_attention.key_projection.bias', 'decoder.layers.1.cross_attention.value_projection.weight', 'decoder.layers.1.cross_attention.value_projection.bias', 'decoder.layers.1.cross_attention.out_projection.weight', 'decoder.layers.1.cross_attention.out_projection.bias', 'decoder.layers.1.conv1.weight', 'decoder.layers.1.conv1.bias', 'decoder.layers.1.conv2.weight', 'decoder.layers.1.conv2.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.1.norm1.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.norm3.bias', 'decoder.layers.2.self_attention.query_projection.weight', 'decoder.layers.2.self_attention.query_projection.bias', 'decoder.layers.2.self_attention.key_projection.weight', 'decoder.layers.2.self_attention.key_projection.bias', 'decoder.layers.2.self_attention.value_projection.weight', 'decoder.layers.2.self_attention.value_projection.bias', 'decoder.layers.2.self_attention.out_projection.weight', 'decoder.layers.2.self_attention.out_projection.bias', 'decoder.layers.2.cross_attention.query_projection.weight', 'decoder.layers.2.cross_attention.query_projection.bias', 'decoder.layers.2.cross_attention.key_projection.weight', 'decoder.layers.2.cross_attention.key_projection.bias', 'decoder.layers.2.cross_attention.value_projection.weight', 'decoder.layers.2.cross_attention.value_projection.bias', 'decoder.layers.2.cross_attention.out_projection.weight', 'decoder.layers.2.cross_attention.out_projection.bias', 'decoder.layers.2.conv1.weight', 'decoder.layers.2.conv1.bias', 'decoder.layers.2.conv2.weight', 'decoder.layers.2.conv2.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.2.norm1.bias', 'decoder.layers.2.norm2.weight', 'decoder.layers.2.norm2.bias', 'decoder.layers.2.norm3.weight', 'decoder.layers.2.norm3.bias', 'decoder.layers.3.self_attention.query_projection.weight', 'decoder.layers.3.self_attention.query_projection.bias', 'decoder.layers.3.self_attention.key_projection.weight', 'decoder.layers.3.self_attention.key_projection.bias', 'decoder.layers.3.self_attention.value_projection.weight', 'decoder.layers.3.self_attention.value_projection.bias', 'decoder.layers.3.self_attention.out_projection.weight', 'decoder.layers.3.self_attention.out_projection.bias', 'decoder.layers.3.cross_attention.query_projection.weight', 'decoder.layers.3.cross_attention.query_projection.bias', 'decoder.layers.3.cross_attention.key_projection.weight', 'decoder.layers.3.cross_attention.key_projection.bias', 'decoder.layers.3.cross_attention.value_projection.weight', 'decoder.layers.3.cross_attention.value_projection.bias', 'decoder.layers.3.cross_attention.out_projection.weight', 'decoder.layers.3.cross_attention.out_projection.bias', 'decoder.layers.3.conv1.weight', 'decoder.layers.3.conv1.bias', 'decoder.layers.3.conv2.weight', 'decoder.layers.3.conv2.bias', 'decoder.layers.3.norm1.weight', 'decoder.layers.3.norm1.bias', 'decoder.layers.3.norm2.weight', 'decoder.layers.3.norm2.bias', 'decoder.layers.3.norm3.weight', 'decoder.layers.3.norm3.bias', 'decoder.layers.4.self_attention.query_projection.weight', 'decoder.layers.4.self_attention.query_projection.bias', 'decoder.layers.4.self_attention.key_projection.weight', 'decoder.layers.4.self_attention.key_projection.bias', 'decoder.layers.4.self_attention.value_projection.weight', 'decoder.layers.4.self_attention.value_projection.bias', 'decoder.layers.4.self_attention.out_projection.weight', 'decoder.layers.4.self_attention.out_projection.bias', 'decoder.layers.4.cross_attention.query_projection.weight', 'decoder.layers.4.cross_attention.query_projection.bias', 'decoder.layers.4.cross_attention.key_projection.weight', 'decoder.layers.4.cross_attention.key_projection.bias', 'decoder.layers.4.cross_attention.value_projection.weight', 'decoder.layers.4.cross_attention.value_projection.bias', 'decoder.layers.4.cross_attention.out_projection.weight', 'decoder.layers.4.cross_attention.out_projection.bias', 'decoder.layers.4.conv1.weight', 'decoder.layers.4.conv1.bias', 'decoder.layers.4.conv2.weight', 'decoder.layers.4.conv2.bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.4.norm2.weight', 'decoder.layers.4.norm2.bias', 'decoder.layers.4.norm3.weight', 'decoder.layers.4.norm3.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.projection.weight', 'decoder.projection.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "# Assign values to its attributes\n",
    "args.task_name = \"long_term_forecast\"\n",
    "args.is_training = 1\n",
    "args.model_id = 1\n",
    "args.model_comment = \"TimeLLM-GB\"\n",
    "args.model = \"TimeLLM\"\n",
    "args.seed = 2021\n",
    "args.data = \"GB\"\n",
    "args.root_path = \"./datasets/\"\n",
    "args.data_path = \"GB_data.csv\"\n",
    "args.features = \"M\"\n",
    "args.target = \"OT\"\n",
    "args.loader = \"modal\"\n",
    "args.freq = \"h\"\n",
    "args.checkpoints = \"./checkpoints/\"\n",
    "args.seq_len = 512\n",
    "args.label_len = 48\n",
    "args.pred_len = 96\n",
    "args.seasonal_patterns = \"Monthly\"\n",
    "args.enc_in = 5\n",
    "args.dec_in = 5\n",
    "args.c_out = 5\n",
    "args.d_model = 16\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 64\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.dropout = 0.1\n",
    "args.embed = \"timeF\"\n",
    "args.activation = \"gelu\"\n",
    "args.output_attention = False\n",
    "args.patch_len = 16\n",
    "args.stride = 8\n",
    "args.prompt_domain = 0\n",
    "args.llm_model = \"GPT2\"\n",
    "args.llm_dim = 768\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 20\n",
    "args.align_epochs = 10\n",
    "args.batch_size = 128\n",
    "args.eval_batch_size = 8\n",
    "args.patience = 3\n",
    "args.learning_rate = 4.0000110639950106e-05\n",
    "args.des = \"Exp\"\n",
    "args.loss = \"MSE\"\n",
    "args.lradj = \"type1\"\n",
    "args.pct_start = 0.2\n",
    "args.use_amp = False\n",
    "args.llm_layers = 12\n",
    "args.percent = 100\n",
    "args.content = \"Hourly data detailing load, solar generation, and wind generation in Great Britain. Wind generation is segmented into offshore, onshore, and total generation, with the latter being the combined sum of onshore and offshore wind generation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.TimeLLM import Model\n",
    "model = Model(configs=args)\n",
    "checkpoint = torch.load('/vol/cs-hu/riabchuv/my_work/checkpoints/long_term_forecast_1_TimeLLM_GB_ftM_sl512_ll48_pl96_dm16_nh8_el2_dl1_df64_fc3_ebtimeF_Exp_0-TimeLLM-GB/checkpoint.pth')\n",
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
