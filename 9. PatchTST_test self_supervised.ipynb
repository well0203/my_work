{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. Pretrain](#1-pretrain)\n",
    "- [2. Finetune](#2-finetune)\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "This notebook demonstrates experiments on 5 countries with self-supervised learning. \n",
    "All parameters are taken from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pretrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "\n",
    "for country in countries:\n",
    "    for pred_len in pred_lens:\n",
    "        # Just use the same parameters as from the paper\n",
    "        params = {\n",
    "            \"--dset\": country,\n",
    "            \"--mask_ratio\": 0.4,\n",
    "            \"--scaler_type\": \"minmax\",\n",
    "            \"--n_epochs_pretrain\": 100,\n",
    "            \"--target_points\": pred_len\n",
    "        }\n",
    "\n",
    "        # Build the command string\n",
    "        command = \"python PatchTST-main/PatchTST_self_supervised/patchtst_pretrain.py \"\n",
    "\n",
    "        # Add parameters to the command\n",
    "        for key, value in params.items():\n",
    "            if value is not None:\n",
    "                command += f\"{key} {value} \"\n",
    "            else:\n",
    "                command += f\"{key} \"  # Add flags with no value\n",
    "\n",
    "        # Execute the command\n",
    "        print(f\"==========Running command for {country}, pred_len {pred_len}:\\n{command}==========\")\n",
    "        !{command}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finetune\n",
    "\n",
    "Linear probing + full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you just want finetune, please uncomment below\n",
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "\n",
    "for country in countries:\n",
    "    for pred_len in pred_lens:\n",
    "        # Define the parameters for each country and pred_len\n",
    "        params = {\n",
    "            \"--dset\": country,\n",
    "            \"--linear_prob_finetune\": 1,\n",
    "            \"--pretrained_model\": f\"saved_models/{country}/masked_patchtst/based_model/patchtst_pretrained_cw512_patch12_stride12_epochs-pretrain30_mask0.4_model1.pth\",\n",
    "            \"--scaler_type\": \"minmax\",\n",
    "            \"--n_epochs_finetune\": 10,\n",
    "            \"--n_epochs_linear_probe\": 20,\n",
    "            \"--target_points\": pred_len  \n",
    "        }\n",
    "\n",
    "        # Build the command string\n",
    "        command = \"python PatchTST-main/PatchTST_self_supervised/patchtst_finetune.py \"\n",
    "\n",
    "        # Add parameters to the command\n",
    "        for key, value in params.items():\n",
    "            if value is not None:\n",
    "                command += f\"{key} {value} \"\n",
    "            else:\n",
    "                command += f\"{key} \"  # Add flags with no value\n",
    "\n",
    "        print(f\"==========Running command for {country}, pred_len {pred_len}:\\n{command}==========\")\n",
    "\n",
    "        # Run the command\n",
    "        !{command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on bugs for Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(dset_pretrain='IT', context_points=512, target_points=96, batch_size=64, num_workers=0, features='M', patch_len=12, stride=12, revin=1, overlapping_windows=True, scaler_type='minmax', if_relu=False, n_layers=3, n_heads=16, d_model=128, d_ff=512, dropout=0.2, head_dropout=0.2, mask_ratio=0.4, n_epochs_pretrain=100, lr=0.0001, pretrained_model_id=1, model_type='based_model')\n",
      "scaler_type: minmax overlapping_windows: True\n",
      "number of patches: 42\n",
      "number of model params 603404\n",
      "suggested_lr 0.0002477076355991711\n",
      "scaler_type: minmax overlapping_windows: True\n",
      "number of patches: 42\n",
      "number of model params 603404\n",
      "          epoch     train_loss     valid_loss           time\n",
      "Better model found at epoch 0 with valid_loss value: 0.8967356272884459.\n",
      "              0       0.938521       0.896736          00:09\n",
      "Better model found at epoch 1 with valid_loss value: 0.8881698471826688.\n",
      "              1       0.896467       0.888170          00:09\n",
      "Better model found at epoch 2 with valid_loss value: 0.8823258079485354.\n",
      "              2       0.885393       0.882326          00:09\n",
      "Better model found at epoch 3 with valid_loss value: 0.8821932687906835.\n",
      "              3       0.878312       0.882193          00:09\n",
      "Better model found at epoch 4 with valid_loss value: 0.8765498658716436.\n",
      "              4       0.876024       0.876550          00:09\n",
      "              5       0.872625       0.877340          00:09\n",
      "Better model found at epoch 6 with valid_loss value: 0.8757813294599268.\n",
      "              6       0.871166       0.875781          00:09\n",
      "              7       0.869507       0.877671          00:09\n",
      "Better model found at epoch 8 with valid_loss value: 0.873351206519528.\n",
      "              8       0.867975       0.873351          00:09\n",
      "              9       0.867239       0.875410          00:09\n",
      "             10       0.867303       0.873736          00:09\n",
      "Better model found at epoch 11 with valid_loss value: 0.495188105484642.\n",
      "             11       0.608950       0.495188          00:09\n",
      "Better model found at epoch 12 with valid_loss value: 0.4847974089707079.\n",
      "             12       0.503978       0.484797          00:09\n",
      "Better model found at epoch 13 with valid_loss value: 0.4774695747940399.\n",
      "             13       0.495159       0.477470          00:09\n",
      "Better model found at epoch 14 with valid_loss value: 0.4762754510145443.\n",
      "             14       0.490194       0.476275          00:09\n",
      "             15       0.486599       0.476790          00:09\n",
      "Better model found at epoch 16 with valid_loss value: 0.4751617804109032.\n",
      "             16       0.484648       0.475162          00:09\n",
      "Better model found at epoch 17 with valid_loss value: 0.47410210282750204.\n",
      "             17       0.482739       0.474102          00:09\n",
      "Better model found at epoch 18 with valid_loss value: 0.4726005883212978.\n",
      "             18       0.481069       0.472601          00:09\n",
      "Better model found at epoch 19 with valid_loss value: 0.4454305774511798.\n",
      "             19       0.473361       0.445431          00:09\n",
      "Better model found at epoch 20 with valid_loss value: 0.318727651895596.\n",
      "             20       0.419336       0.318728          00:09\n",
      "Better model found at epoch 21 with valid_loss value: 0.2427597703925956.\n",
      "             21       0.313724       0.242760          00:09\n",
      "Better model found at epoch 22 with valid_loss value: 0.22882416789564686.\n",
      "             22       0.260966       0.228824          00:09\n",
      "Better model found at epoch 23 with valid_loss value: 0.21324038773265866.\n",
      "             23       0.242250       0.213240          00:09\n",
      "Better model found at epoch 24 with valid_loss value: 0.180197354620118.\n",
      "             24       0.222249       0.180197          00:09\n",
      "Better model found at epoch 25 with valid_loss value: 0.1609063417762154.\n",
      "             25       0.196233       0.160906          00:09\n",
      "Better model found at epoch 26 with valid_loss value: 0.15310471785369204.\n",
      "             26       0.180621       0.153105          00:09\n",
      "Better model found at epoch 27 with valid_loss value: 0.148449061419345.\n",
      "             27       0.170199       0.148449          00:09\n",
      "Better model found at epoch 28 with valid_loss value: 0.14366076650223758.\n",
      "             28       0.162535       0.143661          00:09\n",
      "Better model found at epoch 29 with valid_loss value: 0.1396192061794396.\n",
      "             29       0.156692       0.139619          00:09\n",
      "Better model found at epoch 30 with valid_loss value: 0.13863732979683685.\n",
      "             30       0.151614       0.138637          00:09\n",
      "Better model found at epoch 31 with valid_loss value: 0.1341101203341131.\n",
      "             31       0.148787       0.134110          00:09\n",
      "Better model found at epoch 32 with valid_loss value: 0.13148661179693857.\n",
      "             32       0.145196       0.131487          00:09\n",
      "             33       0.143299       0.133147          00:09\n",
      "             34       0.140370       0.132577          00:09\n",
      "Better model found at epoch 35 with valid_loss value: 0.13096277222970912.\n",
      "             35       0.138300       0.130963          00:09\n",
      "Better model found at epoch 36 with valid_loss value: 0.12906480235137816.\n",
      "             36       0.136646       0.129065          00:09\n",
      "Better model found at epoch 37 with valid_loss value: 0.12854402199132933.\n",
      "             37       0.134263       0.128544          00:09\n",
      "Better model found at epoch 38 with valid_loss value: 0.1268503733821959.\n",
      "             38       0.132729       0.126850          00:09\n",
      "             39       0.131227       0.128180          00:09\n",
      "             40       0.129391       0.126982          00:09\n",
      "Better model found at epoch 41 with valid_loss value: 0.1263340030480828.\n",
      "             41       0.128036       0.126334          00:09\n",
      "             42       0.127176       0.129643          00:09\n",
      "             43       0.126021       0.128046          00:08\n",
      "Better model found at epoch 44 with valid_loss value: 0.12491932830934703.\n",
      "             44       0.124882       0.124919          00:09\n",
      "             45       0.123773       0.129891          00:09\n",
      "             46       0.122938       0.125227          00:09\n",
      "             47       0.122092       0.128650          00:09\n",
      "             48       0.120755       0.129128          00:09\n",
      "             49       0.119283       0.132512          00:09\n",
      "No improvement since epoch 44: early stopping\n",
      "pretraining completed\n",
      "Model saved in: saved_models/IT/masked_patchtst/based_model/patchtst_pretrained_cw512_patch12_stride12_epochs-pretrain100_mask0.4_model1\n",
      "Metrics saved in: saved_models/IT/masked_patchtst/based_model/patchtst_pretrained_cw512_patch12_stride12_epochs-pretrain100_mask0.4_model1_losses.csv\n"
     ]
    }
   ],
   "source": [
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "\n",
    "for country in countries:\n",
    "    for pred_len in pred_lens:\n",
    "        # Just use the same parameters as from the paper\n",
    "        params = {\n",
    "            \"--dset\": country,\n",
    "            \"--mask_ratio\": 0.4,\n",
    "            \"--scaler_type\": \"minmax\",\n",
    "            \"--n_epochs_pretrain\": 100,\n",
    "            \"--target_points\": pred_len\n",
    "            }\n",
    "\n",
    "        # Build the command string\n",
    "        command = \"python PatchTST-main/PatchTST_self_supervised/patchtst_pretrain.py \"\n",
    "\n",
    "        # Add parameters to the command\n",
    "        for key, value in params.items():\n",
    "            if value is not None:\n",
    "                command += f\"{key} {value} \"\n",
    "            else:\n",
    "                command += f\"{key} \"  # Add flags with no value\n",
    "\n",
    "        # Complete command\n",
    "        !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(linear_prob_finetune=1, is_finetune=0, is_linear_probe=0, n_epochs_linear_probe=20, dset_finetune='IT', context_points=512, target_points=96, batch_size=64, num_workers=0, scaler='standard', features='M', patch_len=12, stride=12, revin=1, overlapping_windows=True, scaler_type='minmax', if_relu=False, n_layers=3, n_heads=16, d_model=128, d_ff=256, dropout=0.2, head_dropout=0.2, n_epochs_finetune=10, lr=0.0001, pretrained_model='saved_models/IT/masked_patchtst/based_model/patchtst_pretrained_cw512_patch12_stride12_epochs-pretrain30_mask0.4_model1.pth', finetuned_model_id=1, model_type='based_model')\n",
      "scaler_type: minmax overlapping_windows: True\n",
      "number of patches: 42\n",
      "number of model params 920672\n",
      "check unmatched_layers: ['backbone.encoder.layers.0.ff.0.weight', 'backbone.encoder.layers.0.ff.0.bias', 'backbone.encoder.layers.0.ff.3.weight', 'backbone.encoder.layers.1.ff.0.weight', 'backbone.encoder.layers.1.ff.0.bias', 'backbone.encoder.layers.1.ff.3.weight', 'backbone.encoder.layers.2.ff.0.weight', 'backbone.encoder.layers.2.ff.0.bias', 'backbone.encoder.layers.2.ff.3.weight']\n",
      "suggested_lr 6.73415065775082e-05\n",
      "Performing linear probing followed by end-to-end fine-tuning\n",
      "scaler_type: minmax overlapping_windows: True\n",
      "number of patches: 42\n",
      "number of model params 920672\n",
      "check unmatched_layers: ['backbone.encoder.layers.0.ff.0.weight', 'backbone.encoder.layers.0.ff.0.bias', 'backbone.encoder.layers.0.ff.3.weight', 'backbone.encoder.layers.1.ff.0.weight', 'backbone.encoder.layers.1.ff.0.bias', 'backbone.encoder.layers.1.ff.3.weight', 'backbone.encoder.layers.2.ff.0.weight', 'backbone.encoder.layers.2.ff.0.bias', 'backbone.encoder.layers.2.ff.3.weight']\n",
      "Starting linear probing for 10 epochs\n",
      "Finetune the head\n",
      "          epoch     train_loss     valid_loss           time\n",
      "Better model found at epoch 0 with valid_loss value: 0.10813757056041497.\n",
      "              0       0.161137       0.108138          00:04\n",
      "Better model found at epoch 1 with valid_loss value: 0.09656939712164361.\n",
      "              1       0.132584       0.096569          00:04\n",
      "Better model found at epoch 2 with valid_loss value: 0.09087136550294955.\n",
      "              2       0.123537       0.090871          00:04\n",
      "Better model found at epoch 3 with valid_loss value: 0.08758014526863812.\n",
      "              3       0.114970       0.087580          00:04\n",
      "Better model found at epoch 4 with valid_loss value: 0.08586278780385476.\n",
      "              4       0.106743       0.085863          00:04\n",
      "Better model found at epoch 5 with valid_loss value: 0.0844781985798922.\n",
      "              5       0.099737       0.084478          00:04\n",
      "Better model found at epoch 6 with valid_loss value: 0.0842069819848454.\n",
      "              6       0.094818       0.084207          00:04\n",
      "Better model found at epoch 7 with valid_loss value: 0.08405468710270037.\n",
      "              7       0.092057       0.084055          00:04\n",
      "Better model found at epoch 8 with valid_loss value: 0.08382456122228438.\n",
      "              8       0.090704       0.083825          00:04\n",
      "              9       0.090003       0.083871          00:04\n",
      "Better model found at epoch 10 with valid_loss value: 0.08365624523240439.\n",
      "             10       0.089537       0.083656          00:04\n",
      "Better model found at epoch 11 with valid_loss value: 0.08359747468406734.\n",
      "             11       0.089301       0.083597          00:04\n",
      "Better model found at epoch 12 with valid_loss value: 0.08344239870448536.\n",
      "             12       0.089119       0.083442          00:04\n",
      "Better model found at epoch 13 with valid_loss value: 0.08318924434404242.\n",
      "             13       0.088892       0.083189          00:04\n",
      "Better model found at epoch 14 with valid_loss value: 0.08296695519890408.\n",
      "             14       0.088728       0.082967          00:04\n",
      "             15       0.088581       0.082979          00:04\n",
      "Better model found at epoch 16 with valid_loss value: 0.08290457419014442.\n",
      "             16       0.088482       0.082905          00:04\n",
      "Better model found at epoch 17 with valid_loss value: 0.08284981140186382.\n",
      "             17       0.088394       0.082850          00:04\n",
      "Better model found at epoch 18 with valid_loss value: 0.08275175785222996.\n",
      "             18       0.088338       0.082752          00:04\n",
      "             19       0.088268       0.082760          00:04\n",
      "Starting full fine-tuning for 20 epochs\n",
      "Finetune the head\n",
      "          epoch     train_loss     valid_loss           time\n",
      "              0       0.088347       0.083214          00:04\n",
      "              1       0.088748       0.083694          00:04\n",
      "              2       0.089289       0.083948          00:04\n",
      "              3       0.089499       0.084088          00:04\n",
      "              4       0.089419       0.083947          00:04\n",
      "              5       0.089231       0.083624          00:04\n",
      "              6       0.088961       0.083192          00:04\n",
      "              7       0.088721       0.082865          00:04\n",
      "              8       0.088478       0.082801          00:04\n",
      "              9       0.088414       0.082753          00:04\n",
      "Finetune the entire network\n",
      "          epoch     train_loss     valid_loss           time\n",
      "Better model found at epoch 0 with valid_loss value: 0.081913217245029.\n",
      "              0       0.087893       0.081913          00:08\n",
      "Better model found at epoch 1 with valid_loss value: 0.08021011290460994.\n",
      "              1       0.085796       0.080210          00:08\n",
      "Better model found at epoch 2 with valid_loss value: 0.07895167624688518.\n",
      "              2       0.082999       0.078952          00:07\n",
      "Better model found at epoch 3 with valid_loss value: 0.0781030342964936.\n",
      "              3       0.081195       0.078103          00:08\n",
      "Better model found at epoch 4 with valid_loss value: 0.07764475910134001.\n",
      "              4       0.080039       0.077645          00:08\n",
      "Better model found at epoch 5 with valid_loss value: 0.07738752248715927.\n",
      "              5       0.079188       0.077388          00:08\n",
      "Better model found at epoch 6 with valid_loss value: 0.07722825003019478.\n",
      "              6       0.078624       0.077228          00:08\n",
      "Better model found at epoch 7 with valid_loss value: 0.0770519830813924.\n",
      "              7       0.078195       0.077052          00:08\n",
      "              8       0.077899       0.077059          00:08\n",
      "Better model found at epoch 9 with valid_loss value: 0.07698487420698993.\n",
      "              9       0.077870       0.076985          00:08\n",
      "Linear probing and finetune completed\n",
      "scaler_type: minmax overlapping_windows: True\n",
      "number of patches: 42\n",
      "number of model params 920672\n",
      "scores: mse: 0.018168202 rmse: 0.13478947 mae: 0.081972055\n",
      "----------- Complete! -----------\n"
     ]
    }
   ],
   "source": [
    "# IT IS BETTER!\n",
    "# Define parameters in a dictionary\n",
    "params = {\n",
    "    \"--dset\": \"IT\",\n",
    "    \"--linear_prob_finetune\": 1,\n",
    "    \"--pretrained_model\": \"saved_models/IT/masked_patchtst/based_model/patchtst_pretrained_cw512_patch12_stride12_epochs-pretrain30_mask0.4_model1.pth\",    \n",
    "    \"--scaler_type\": \"minmax\",\n",
    "    \"--n_epochs_finetune\": 10,\n",
    "    \"--n_epochs_linear_probe\": 20\n",
    "}\n",
    "\n",
    "# Build the command string\n",
    "command = \"python PatchTST-main/PatchTST_self_supervised/patchtst_finetune.py \"\n",
    "\n",
    "# Add parameters to the command\n",
    "for key, value in params.items():\n",
    "    if value is not None:\n",
    "        command += f\"{key} {value} \"\n",
    "    else:\n",
    "        command += f\"{key} \"  # Add flags with no value\n",
    "\n",
    "# Run the command with !\n",
    "!{command}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
