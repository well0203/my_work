{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to CUDA Grünau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I tried out Google Colab, but it did not work out.\n",
    "Even after fixing all not working things (that work everywhere else except colab), its capacity is too limited and they actually do not write after how many hours they interrupt process (\"because everytime it is different\").\n",
    "\n",
    "So, if you are not going to use CUDA Grünau, skip this part.\n",
    "\n",
    "If you are interested, which code is working in Colab, it is in Appendix. It runs only on small datasets (that processes are not interrupted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# For gruenau: For CUDA making it available this works:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of available GPUs:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100 80GB PCIe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Nov__3_17:16:49_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.103\n",
      "Build cuda_12.3.r12.3/compiler.33492891_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the GPU you want to use (e.g., 0, 1, 2, etc.)\n",
    "# Choose that one that is not used by other processes\n",
    "gpu_index = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder named \"datasets\" if it doesn't exist\n",
    "folder_name = \"datasets\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "df = pd.read_csv(\"./datasets/top_5_countries.csv\", index_col=0, parse_dates=True)\n",
    "# Reset index for Data Loader\n",
    "df.reset_index(inplace=True)\n",
    "df = df.iloc[:,:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "\n",
    "# Split and save the datasets\n",
    "for country_prefix in top_5_countries:\n",
    "    # Filter columns with the specified prefix\n",
    "    country_columns = [col for col in df.columns if col.startswith(country_prefix)]\n",
    "    \n",
    "    # Insert the date column at the beginning of every dataset\n",
    "    country_columns.insert(0,\"date\")\n",
    "    country_df = df[country_columns]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"./datasets/{country_prefix}_data.csv\"\n",
    "    country_df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=28321\n",
    "val=6577\n",
    "test=8713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "def split_scale_dataset(data, train_size, val_size, test_size=None):\n",
    "\n",
    "    \"\"\"\n",
    "    data (pd.DataFrame): Dataframe with time series data.\n",
    "    train_size, test_size, val_size (int): number of days in train, \n",
    "                                            test and validation datasets.\n",
    "\n",
    "    return: Scaled datasets\n",
    "   \n",
    "    \n",
    "    num_train = train_size*24\n",
    "    if test_size is not None:\n",
    "        num_test = test_size*24\n",
    "    num_vali = val_size*24\n",
    "\"\"\"\n",
    "    num_train = train_size\n",
    "    num_vali = val_size\n",
    "    train_data = data.iloc[:num_train] # 0, a-1\n",
    "    vali_data = data.iloc[num_train: num_train + num_vali] # a, a+b-1\n",
    "    test_data = data.iloc[num_train + num_vali:] # a+b\n",
    "\n",
    "    assert(len(data) == len(train_data) + len(test_data) + len(vali_data))\n",
    "\n",
    "    print(f'{len(train_data)} observations in the train dataset.\\n {len(test_data)} observations in the test dataset.\\n {len(vali_data)} observations in the validation dataset.')\n",
    "\n",
    "    # initialize scaler object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scale data\n",
    "    train_data_sc = scaler.fit_transform(train_data)\n",
    "    vali_data_sc = scaler.transform(vali_data)\n",
    "    test_data_sc = scaler.transform(test_data)\n",
    "\n",
    "    train_data_sc = pd.DataFrame(train_data_sc, columns=train_data.columns, index=train_data.index)\n",
    "    vali_data_sc = pd.DataFrame(vali_data_sc, columns=vali_data.columns, index=vali_data.index)\n",
    "    test_data_sc = pd.DataFrame(test_data_sc, columns=test_data.columns, index=test_data.index)\n",
    "\n",
    "    return train_data_sc, vali_data_sc, test_data_sc\n",
    "\n",
    "#time_series = pd.read_csv(\"./datasets/df_most_important_columns.csv\", index_col=0, parse_dates=True)\n",
    "#train, vali, test = split_scale_dataset(data=time_series, train_size=train, val_size=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(train[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28321.00</td>\n",
       "      <td>28321.00</td>\n",
       "      <td>28321.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.43</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.16</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                            28321.00                    28321.00   \n",
       "mean                                 0.00                       -0.00   \n",
       "std                                  1.00                        1.00   \n",
       "min                                 -2.43                       -0.64   \n",
       "25%                                 -0.84                       -0.64   \n",
       "50%                                 -0.03                       -0.63   \n",
       "75%                                  0.89                        0.37   \n",
       "max                                  2.16                        3.94   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                   28321.00  \n",
       "mean                       -0.00  \n",
       "std                         1.00  \n",
       "min                        -1.24  \n",
       "25%                        -0.77  \n",
       "50%                        -0.29  \n",
       "75%                         0.52  \n",
       "max                         3.99  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGxCAYAAACEFXd4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA290lEQVR4nO3de3RU5b3/8U8CuZJMIIQkoEyCiISggCLCYC0XI5FSKoXT4w0NilZpQCFKKT1IBIu48AJeAhaXgLZSPNraAlIUQkELATEUyyWhguBQSIIDJcMl5Pr8/vAwP0cSLsmEmey8X2vttZi9n/nu784G83HPs2cHGWOMAAAALCrY3w0AAAA0JsIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwtJb+biAQ1NTU6PDhw4qOjlZQUJC/2wEAABfBGKMTJ06oQ4cOCg6u+/oNYUfS4cOH1bFjR3+3AQAA6uHgwYO68sor69xO2JEUHR0t6dsfls1m83M3AADgYrjdbnXs2NHze7wuhB3J89GVzWYj7AAA0MRcaAoKE5QBAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAICltfR3AwAAnOV0OuVyuXxWLy4uTna73Wf10DQRdgAAAcHpdColpZvKyk77rGZERKQKCwsIPM0cYQcAEBBcLpfKyk6r74PZsrVPbnA9d9EBbVk0Qy6Xi7DTzBF2AAABxdY+WbH2rv5uAxbi1wnKTz/9tIKCgryWlJQUz/YzZ84oMzNTbdu2VVRUlEaNGqWSkhKvGk6nU8OGDVNkZKTi4+M1efJkVVVVXe5DAQAAAcrvV3a6d++utWvXel63bPn/W5o0aZI+/PBDvffee4qJidH48eM1cuRIbdy4UZJUXV2tYcOGKTExUZs2bVJRUZHuv/9+hYSE6Nlnn73sxwIAzZGvJhUXFBT4oBvgXH4POy1btlRiYuI560tLS/Xmm29q6dKlGjx4sCRp8eLF6tatmzZv3qx+/frp448/1u7du7V27VolJCSoV69eeuaZZzRlyhQ9/fTTCg0NvdyHAwDNSmNMKq4sr/BZLUAKgLDz5ZdfqkOHDgoPD5fD4dDs2bNlt9uVn5+vyspKpaWlecampKTIbrcrLy9P/fr1U15enq677jolJCR4xqSnp2vcuHHatWuXrr/+en8cEgA0G76cVFy0I087ly9kKgJ8zq9hp2/fvlqyZIm6du2qoqIizZgxQ7fccot27typ4uJihYaGqnXr1l7vSUhIUHFxsSSpuLjYK+ic3X52W13Ky8tVXl7uee12u310RADQPPliUrG76IBvmgG+x69hZ+jQoZ4/9+jRQ3379lVSUpL+93//VxEREY2239mzZ2vGjBmNVh8AAASOgHpcROvWrXXNNddo7969SkxMVEVFhY4fP+41pqSkxDPHJzEx8Zy7s86+rm0e0FlTp05VaWmpZzl48KBvDwQAAASMgAo7J0+e1L59+9S+fXv17t1bISEhys3N9Wzfs2ePnE6nHA6HJMnhcGjHjh06cuSIZ8yaNWtks9mUmppa537CwsJks9m8FgAAYE1+/RjrySef1PDhw5WUlKTDhw8rOztbLVq00N13362YmBiNHTtWWVlZio2Nlc1m04QJE+RwONSvXz9J0pAhQ5Samqr77rtPc+bMUXFxsaZNm6bMzEyFhYX589AAAECA8GvY+fe//627775bR48eVbt27fSDH/xAmzdvVrt27SRJc+fOVXBwsEaNGqXy8nKlp6dr/vz5nve3aNFCK1eu1Lhx4+RwONSqVStlZGRo5syZ/jokAAAQYPwadpYtW3be7eHh4crJyVFOTk6dY5KSkrRq1SpftwYAACwioObsAAAA+BphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWJpfv2cHgOR0OuVyuXxWLy4uTna73Wf1AKCpI+wAfuR0OpWS0k1lZad9VjMiIlKFhQUEHgD4P4QdwI9cLpfKyk6r74PZsrVPbnA9d9EBbVk0Qy6Xi7ADAP+HsAMEAFv7ZMXau/q7DQCwJCYoAwAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS+NuLKAefPVFgAUFBT7oBgBwPoQd4BI1xhcBVpZX+KwWAMAbYQe4RL78IsCiHXnauXyhqqqqfNMcAOAchB2gnnzxRYDuogO+aQYAUCcmKAMAAEvjyg6aBV8+WZxJxQDQtBB2YHmNMaFYYlIxADQVhB1Ynq+fLM6kYgBoWgg7aDZ89WRxJhUDQNPCBGUAAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBpLf3dAADg8nI6nXK5XD6pVVBQ4JM6QGMi7ABAM+J0OpWS0k1lZad9WreyvMKn9QBfIuwAQDPicrlUVnZafR/Mlq19coPrFe3I087lC1VVVdXw5oBGQtgBgGbI1j5ZsfauDa7jLjrQ8GaARsYEZQAAYGmEHQAAYGmEHQAAYGnM2QEsyJe3A8fFxclut/usHgBcboQdwELKSo9KCtLo0aN9VjMiIlKFhQUEHgBNFmEHsJDK0yckGfW6Z4radUppcD130QFtWTRDLpeLsAOgySLsABYUFW/3yW3FAGAFTFAGAACWFjBh57nnnlNQUJAmTpzoWXfmzBllZmaqbdu2ioqK0qhRo1RSUuL1PqfTqWHDhikyMlLx8fGaPHky3+QJAAA8AiLsbN26Vb/97W/Vo0cPr/WTJk3SihUr9N5772nDhg06fPiwRo4c6dleXV2tYcOGqaKiQps2bdJbb72lJUuWaPr06Zf7EAAAQIDye9g5efKk7r33Xr3xxhtq06aNZ31paanefPNNvfTSSxo8eLB69+6txYsXa9OmTdq8ebMk6eOPP9bu3bv1+9//Xr169dLQoUP1zDPPKCcnRxUVPJQOAAAEQNjJzMzUsGHDlJaW5rU+Pz9flZWVXutTUlJkt9uVl5cnScrLy9N1112nhIQEz5j09HS53W7t2rXr8hwAAAAIaH69G2vZsmXatm2btm7des624uJihYaGqnXr1l7rExISVFxc7Bnz3aBzdvvZbXUpLy9XeXm557Xb7a7vIQAAgADntys7Bw8e1OOPP6533nlH4eHhl3Xfs2fPVkxMjGfp2LHjZd0/AAC4fPx2ZSc/P19HjhzRDTfc4FlXXV2tTz75RK+99po++ugjVVRU6Pjx415Xd0pKSpSYmChJSkxM1GeffeZV9+zdWmfH1Gbq1KnKysryvHa73QQeALAoHp8Cv4WdW2+9VTt27PBa98ADDyglJUVTpkxRx44dFRISotzcXI0aNUqStGfPHjmdTjkcDkmSw+HQrFmzdOTIEcXHx0uS1qxZI5vNptTU1Dr3HRYWprCwsEY6MgBAIODxKTjLb2EnOjpa1157rde6Vq1aqW3btp71Y8eOVVZWlmJjY2Wz2TRhwgQ5HA7169dPkjRkyBClpqbqvvvu05w5c1RcXKxp06YpMzOTMAMAzRyPT8FZAf24iLlz5yo4OFijRo1SeXm50tPTNX/+fM/2Fi1aaOXKlRo3bpwcDodatWqljIwMzZw5049dAwACCY9PQUCFnfXr13u9Dg8PV05OjnJycup8T1JSklatWtXInQEAgKbK79+zAwAA0JgIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNJa+rsBAMCFOZ1OuVyuBtcpKCjwQTdA00LYAYAA53Q6lZLSTWVlp31Ws7K8wme1gEBH2AGAAOdyuVRWdlp9H8yWrX1yg2oV7cjTzuULVVVV5ZvmgCaAsAMATYStfbJi7V0bVMNddMA3zQBNCGEHwAX5ap5HXFyc7Ha7T2oBwMUi7ACoU1npUUlBGj16tE/qRUREqrCwgMAD4LIi7ACoU+XpE5KMet0zRe06pTSolrvogLYsmiGXy0XYAXBZEXYAXFBUvL3Bc0UAwF/4UkEAAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBpfg07CxYsUI8ePWSz2WSz2eRwOPTXv/7Vs/3MmTPKzMxU27ZtFRUVpVGjRqmkpMSrhtPp1LBhwxQZGan4+HhNnjxZVVVVl/tQAABAgPJr2Lnyyiv13HPPKT8/X59//rkGDx6sO+64Q7t27ZIkTZo0SStWrNB7772nDRs26PDhwxo5cqTn/dXV1Ro2bJgqKiq0adMmvfXWW1qyZImmT5/ur0MCAAABpqU/dz58+HCv17NmzdKCBQu0efNmXXnllXrzzTe1dOlSDR48WJK0ePFidevWTZs3b1a/fv308ccfa/fu3Vq7dq0SEhLUq1cvPfPMM5oyZYqefvpphYaG+uOwAABAAAmYOTvV1dVatmyZTp06JYfDofz8fFVWViotLc0zJiUlRXa7XXl5eZKkvLw8XXfddUpISPCMSU9Pl9vt9lwdqk15ebncbrfXAgAArMnvYWfHjh2KiopSWFiYHn30UX3wwQdKTU1VcXGxQkND1bp1a6/xCQkJKi4uliQVFxd7BZ2z289uq8vs2bMVExPjWTp27OjbgwIAAAHD72Gna9eu2r59u7Zs2aJx48YpIyNDu3fvbtR9Tp06VaWlpZ7l4MGDjbo/AADgP36dsyNJoaGhuvrqqyVJvXv31tatW/Xyyy/rzjvvVEVFhY4fP+51daekpESJiYmSpMTERH322Wde9c7erXV2TG3CwsIUFhbm4yOBrzmdTrlcrgbXKSgo8EE3AICmql5h56qrrtLWrVvVtm1br/XHjx/XDTfcoK+++qreDdXU1Ki8vFy9e/dWSEiIcnNzNWrUKEnSnj175HQ65XA4JEkOh0OzZs3SkSNHFB8fL0las2aNbDabUlNT690D/M/pdColpZvKyk77rGZleYXPagEAmo56hZ0DBw6ourr6nPXl5eU6dOjQRdeZOnWqhg4dKrvdrhMnTmjp0qVav369PvroI8XExGjs2LHKyspSbGysbDabJkyYIIfDoX79+kmShgwZotTUVN13332aM2eOiouLNW3aNGVmZnLlpolzuVwqKzutvg9my9Y+uUG1inbkaefyhXz/kgX56uqfJMXFxclut/ukFoDAcklhZ/ny5Z4/nw0kZ1VXVys3N1fJyckXXe/IkSO6//77VVRUpJiYGPXo0UMfffSRbrvtNknS3LlzFRwcrFGjRqm8vFzp6emaP3++5/0tWrTQypUrNW7cODkcDrVq1UoZGRmaOXPmpRwWApitfbJi7V0bVMNddMA3zSCg+PrqX0REpAoLCwg8gAVdUtgZMWKEJCkoKEgZGRle20JCQpScnKwXX3zxouu9+eab590eHh6unJwc5eTk1DkmKSlJq1atuuh9ArAGX179cxcd0JZFM+RyuQg7gAVdUtipqamRJHXq1Elbt25VXFxcozQFABfLF1f/AFhbvebs7N+/39d9AAAANIp633qem5ur3NxcHTlyxHPF56xFixY1uDEAAABfqFfYmTFjhmbOnKkbb7xR7du3V1BQkK/7AgAA8Il6hZ3XX39dS5Ys0X333efrfgAAAHyqXo+LqKioUP/+/X3dCwAAgM/VK+w89NBDWrp0qa97AQAA8Ll6fYx15swZLVy4UGvXrlWPHj0UEhLitf2ll17ySXMAAAANVa+w889//lO9evWSJO3cudNrG5OVAQBAIKlX2Pnb3/7m6z4AAAAaRb3m7AAAADQV9bqyM2jQoPN+XLVu3bp6NwQAAOBL9Qo7Z+frnFVZWant27dr586d5zwgFAAAwJ/qFXbmzp1b6/qnn35aJ0+ebFBDAKytoKAgoOoAsL56PxurNqNHj9ZNN92kF154wZdlAVhAWelRSUEaPXq0T+tWllf4tB4A6/Fp2MnLy1N4eLgvSwKwiMrTJyQZ9bpnitp1SmlwvaIdedq5fKGqqqoa3hwAS6tX2Bk5cqTXa2OMioqK9Pnnn+upp57ySWMArCkq3q5Ye9cG13EXHWh4MwCahXqFnZiYGK/XwcHB6tq1q2bOnKkhQ4b4pDEAAABfqFfYWbx4sa/7AAAAaBQNmrOTn5/vuSOie/fuuv76633SFAAAgK/UK+wcOXJEd911l9avX6/WrVtLko4fP65BgwZp2bJlateunS97BAAAqLd6PS5iwoQJOnHihHbt2qVjx47p2LFj2rlzp9xutx577DFf9wgAAFBv9bqys3r1aq1du1bdunXzrEtNTVVOTg4TlAEAQECp15WdmpoahYSEnLM+JCRENTU1DW4KAADAV+oVdgYPHqzHH39chw8f9qw7dOiQJk2apFtvvdVnzQEAADRUvcLOa6+9JrfbreTkZHXu3FmdO3dWp06d5Ha79eqrr/q6RwAAgHqr15ydjh07atu2bVq7dq0KCwslSd26dVNaWppPmwMAAGioS7qys27dOqWmpsrtdisoKEi33XabJkyYoAkTJqhPnz7q3r27Pv3008bqFQAA4JJdUtiZN2+eHn74YdlstnO2xcTE6JFHHtFLL73ks+YAAAAa6pLCzhdffKHbb7+9zu1DhgxRfn5+g5sCAADwlUuas1NSUlLrLeeeYi1b6ptvvmlwUwDgD2cff+MLcXFxstvtPqsHoP4uKexcccUV2rlzp66++upat//zn/9U+/btfdIYAFwuZaVHJQVp9OjRPqsZERGpwsICAg8QAC4p7PzoRz/SU089pdtvv13h4eFe28rKypSdna0f//jHPm0QABpb5ekTkox63TNF7TqlNLieu+iAtiyaIZfLRdgBAsAlhZ1p06bpT3/6k6655hqNHz9eXbt2lSQVFhYqJydH1dXV+p//+Z9GaRQAGltUvF2x9q7+bgOAj11S2ElISNCmTZs0btw4TZ06VcYYSVJQUJDS09OVk5OjhISERmkUAACgPi75SwWTkpK0atUq/ec//9HevXtljFGXLl3Upk2bxugPAACgQer1DcqS1KZNG/Xp08eXvQAAAPhcvZ6NBQAA0FQQdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKXV+3ERAAA0RwUFBT6pExcXJ7vd7pNaOD/CDgAAF6Gs9KikII0ePdon9SIiIlVYWEDguQwIOwAAXITK0yckGfW6Z4radUppUC130QFtWTRDLpeLsHMZEHYAALgEUfF2xdq7+rsNXAImKAMAAEsj7AAAAEvjYywAaCS+umvHV3WA5oqwAwA+5uu7ds6qLK/waT2guSDsAICP+fKuHUkq2pGnncsXqqqqquHNAc0QYQcAGomv7tpxFx1oeDNAM8YEZQAAYGl+DTuzZ89Wnz59FB0drfj4eI0YMUJ79uzxGnPmzBllZmaqbdu2ioqK0qhRo1RSUuI1xul0atiwYYqMjFR8fLwmT57M5V4AACDJz2Fnw4YNyszM1ObNm7VmzRpVVlZqyJAhOnXqlGfMpEmTtGLFCr333nvasGGDDh8+rJEjR3q2V1dXa9iwYaqoqNCmTZv01ltvacmSJZo+fbo/DgkAAAQYv87ZWb16tdfrJUuWKD4+Xvn5+frhD3+o0tJSvfnmm1q6dKkGDx4sSVq8eLG6deumzZs3q1+/fvr444+1e/durV27VgkJCerVq5eeeeYZTZkyRU8//bRCQ0P9cWgAACBABNScndLSUklSbGysJCk/P1+VlZVKS0vzjElJSZHdbldeXp4kKS8vT9ddd50SEhI8Y9LT0+V2u7Vr165a91NeXi632+21AAAAawqYsFNTU6OJEyfq5ptv1rXXXitJKi4uVmhoqFq3bu01NiEhQcXFxZ4x3w06Z7ef3Vab2bNnKyYmxrN07NjRx0cDAAACRcCEnczMTO3cuVPLli1r9H1NnTpVpaWlnuXgwYONvk8AAOAfAfE9O+PHj9fKlSv1ySef6Morr/SsT0xMVEVFhY4fP+51daekpESJiYmeMZ999plXvbN3a50d831hYWEKCwvz8VEAAIBA5NcrO8YYjR8/Xh988IHWrVunTp06eW3v3bu3QkJClJub61m3Z88eOZ1OORwOSZLD4dCOHTt05MgRz5g1a9bIZrMpNTX18hwIAAAIWH69spOZmamlS5fqL3/5i6Kjoz1zbGJiYhQREaGYmBiNHTtWWVlZio2Nlc1m04QJE+RwONSvXz9J0pAhQ5Samqr77rtPc+bMUXFxsaZNm6bMzEyu3gAAAP+GnQULFkiSBg4c6LV+8eLFGjNmjCRp7ty5Cg4O1qhRo1ReXq709HTNnz/fM7ZFixZauXKlxo0bJ4fDoVatWikjI0MzZ868XIcBAAACmF/DjjHmgmPCw8OVk5OjnJycOsckJSVp1apVvmwNAABYRMDcjQUAANAYCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSAuLZWLAGp9Mpl8vlk1oFBQU+qQMAAGEHPuF0OpWS0k1lZad9WreyvMKn9QAAzQ9hBz7hcrlUVnZafR/Mlq19coPrFe3I087lC1VVVdXw5gAAzRphBz5la5+sWHvXBtdxFx1oeDMAAIgJygAAwOIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNJa+rsB+I/T6ZTL5fJJrYKCAp/UAQDA1wg7zZTT6VRKSjeVlZ32ad3K8gqf1gMAoKEIO82Uy+VSWdlp9X0wW7b2yQ2uV7QjTzuXL1RVVVXDmwMAwIcIO82crX2yYu1dG1zHXXSg4c0AANAImKAMAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsje/ZAQDAT3z5qJ24uDjZ7Xaf1bMSwg4AAJdZWelRSUEaPXq0z2pGRESqsLCAwFMLwg4AAJdZ5ekTkox63TNF7TqlNLieu+iAtiyaIZfLRdipBWEHAAA/iYq3++SRPTg/JigDAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLa+nPnX/yySd6/vnnlZ+fr6KiIn3wwQcaMWKEZ7sxRtnZ2XrjjTd0/Phx3XzzzVqwYIG6dOniGXPs2DFNmDBBK1asUHBwsEaNGqWXX35ZUVFRfjiixud0OuVyuRpcp6CgwAfdAAAQ+Pwadk6dOqWePXvqwQcf1MiRI8/ZPmfOHL3yyit666231KlTJz311FNKT0/X7t27FR4eLkm69957VVRUpDVr1qiyslIPPPCAfv7zn2vp0qWX+3AandPpVEpKN5WVnfZZzcryCp/VAgAgEPk17AwdOlRDhw6tdZsxRvPmzdO0adN0xx13SJLefvttJSQk6M9//rPuuusuFRQUaPXq1dq6datuvPFGSdKrr76qH/3oR3rhhRfUoUOHy3Ysl4PL5VJZ2Wn1fTBbtvbJDapVtCNPO5cvVFVVlW+aAwAgQPk17JzP/v37VVxcrLS0NM+6mJgY9e3bV3l5ebrrrruUl5en1q1be4KOJKWlpSk4OFhbtmzRT3/6U3+03uhs7ZMVa+/aoBruogO+aQYAgAAXsGGnuLhYkpSQkOC1PiEhwbOtuLhY8fHxXttbtmyp2NhYz5jalJeXq7y83PPa7Xb7qm0AABBgmuXdWLNnz1ZMTIxn6dixo79bAgAAjSRgw05iYqIkqaSkxGt9SUmJZ1tiYqKOHDnitb2qqkrHjh3zjKnN1KlTVVpa6lkOHjzo4+4BAECgCNiw06lTJyUmJio3N9ezzu12a8uWLXI4HJIkh8Oh48ePKz8/3zNm3bp1qqmpUd++feusHRYWJpvN5rUAAABr8uucnZMnT2rv3r2e1/v379f27dsVGxsru92uiRMn6je/+Y26dOniufW8Q4cOnu/i6datm26//XY9/PDDev3111VZWanx48frrrvustydWAAAoH78GnY+//xzDRo0yPM6KytLkpSRkaElS5bol7/8pU6dOqWf//znOn78uH7wgx9o9erVnu/YkaR33nlH48eP16233ur5UsFXXnnlsh8LAAAITH4NOwMHDpQxps7tQUFBmjlzpmbOnFnnmNjYWEt+gSAAAPCNgL31HAAAXBpfPgooLi5OdrvdZ/X8ibADAEATV1Z6VFKQRo8e7bOaERGRKiwssETgIewAANDEVZ4+Icmo1z1T1K5TSoPruYsOaMuiGXK5XIQdAAAQOKLi7Q1+nJAVBez37AAAAPgCYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFhaS383YHVOp1Mul8sntQoKCnxSBwCA5oSw04icTqdSUrqprOy0T+tWllf4tB4AAFZG2GlELpdLZWWn1ffBbNnaJze4XtGOPO1cvlBVVVUNbw4AgGaCsHMZ2NonK9betcF13EUHGt4MAADNDBOUAQCApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApbX0dwMAACAwFRQU+KROXFyc7Ha7T2rVB2EHAAB4KSs9KilIo0eP9km9iIhIFRYW+C3wEHYAAICXytMnJBn1umeK2nVKaVAtd9EBbVk0Qy6Xi7ADAAACS1S8XbH2rv5uo8GYoAwAACzNMmEnJydHycnJCg8PV9++ffXZZ5/5uyUAABAALBF23n33XWVlZSk7O1vbtm1Tz549lZ6eriNHjvi7NQAA4GeWCDsvvfSSHn74YT3wwANKTU3V66+/rsjISC1atMjfrQEAAD9r8mGnoqJC+fn5SktL86wLDg5WWlqa8vLy/NgZAAAIBE3+biyXy6Xq6molJCR4rU9ISFBhYWGt7ykvL1d5ebnndWlpqSTJ7Xb7tLeTJ09Kko59vUdV5WUNrucu+lqSVHroS4W0DAqYWoFej94Cox69BUY9eguMeoHcm6/ruYudkr79nejr37Nn6xljzj/QNHGHDh0yksymTZu81k+ePNncdNNNtb4nOzvbSGJhYWFhYWGxwHLw4MHzZoUmf2UnLi5OLVq0UElJidf6kpISJSYm1vqeqVOnKisry/O6pqZGx44dU9u2bRUU1PBEfCFut1sdO3bUwYMHZbPZGn1/aDjOWdPC+Wp6OGdNTyCcM2OMTpw4oQ4dOpx3XJMPO6Ghoerdu7dyc3M1YsQISd+Gl9zcXI0fP77W94SFhSksLMxrXevWrRu503PZbDb+UTcxnLOmhfPV9HDOmh5/n7OYmJgLjmnyYUeSsrKylJGRoRtvvFE33XST5s2bp1OnTumBBx7wd2sAAMDPLBF27rzzTn3zzTeaPn26iouL1atXL61evfqcScsAAKD5sUTYkaTx48fX+bFVoAkLC1N2dvY5H6UhcHHOmhbOV9PDOWt6mtI5CzLmQvdrAQAANF1N/ksFAQAAzoewAwAALI2wAwAALI2w40cHDhzQ2LFj1alTJ0VERKhz587Kzs5WRUWFv1vDecyaNUv9+/dXZGSkX76fCReWk5Oj5ORkhYeHq2/fvvrss8/83RLq8Mknn2j48OHq0KGDgoKC9Oc//9nfLeECZs+erT59+ig6Olrx8fEaMWKE9uzZ4++2zouw40eFhYWqqanRb3/7W+3atUtz587V66+/rl//+tf+bg3nUVFRoZ/97GcaN26cv1tBLd59911lZWUpOztb27ZtU8+ePZWenq4jR474uzXU4tSpU+rZs6dycnL83Qou0oYNG5SZmanNmzdrzZo1qqys1JAhQ3Tq1Cl/t1Yn7sYKMM8//7wWLFigr776yt+t4AKWLFmiiRMn6vjx4/5uBd/Rt29f9enTR6+99pqkb79RvWPHjpowYYJ+9atf+bk7nE9QUJA++OADz7fho2n45ptvFB8frw0bNuiHP/yhv9upFVd2AkxpaaliY2P93QbQJFVUVCg/P19paWmedcHBwUpLS1NeXp4fOwOsq7S0VJIC+ncXYSeA7N27V6+++qoeeeQRf7cCNEkul0vV1dXnfHt6QkKCiouL/dQVYF01NTWaOHGibr75Zl177bX+bqdOhJ1G8Ktf/UpBQUHnXQoLC73ec+jQId1+++362c9+pocffthPnTdf9TlnANDcZWZmaufOnVq2bJm/WzkvyzwuIpA88cQTGjNmzHnHXHXVVZ4/Hz58WIMGDVL//v21cOHCRu4OtbnUc4bAFBcXpxYtWqikpMRrfUlJiRITE/3UFWBN48eP18qVK/XJJ5/oyiuv9Hc750XYaQTt2rVTu3btLmrsoUOHNGjQIPXu3VuLFy9WcDAX2/zhUs4ZAldoaKh69+6t3NxczyTXmpoa5ebmNpln5wGBzhijCRMm6IMPPtD69evVqVMnf7d0QYQdPzp06JAGDhyopKQkvfDCC/rmm2882/i/0MDldDp17NgxOZ1OVVdXa/v27ZKkq6++WlFRUf5tDsrKylJGRoZuvPFG3XTTTZo3b55OnTqlBx54wN+toRYnT57U3r17Pa/379+v7du3KzY2Vna73Y+doS6ZmZlaunSp/vKXvyg6OtozHy4mJkYRERF+7q523HruR0uWLKnzP8CclsA1ZswYvfXWW+es/9vf/qaBAwde/oZwjtdee03PP/+8iouL1atXL73yyivq27evv9tCLdavX69Bgwadsz4jI0NLliy5/A3hgoKCgmpdv3jx4gtOB/AXwg4AALA0JogAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAPrB+/XoFBQXp+PHjlthPY7PKcQBoGgg7sLwxY8YoKChIQUFBCgkJUUJCgm677TYtWrRINTU1nnHJycmecd9dnnvuOT927z8HDhxQUFCQ59lfTVkgHsvTTz+tXr16+bsNoFngQaBoFm6//XYtXrxY1dXVKikp0erVq/X444/r/fff1/Lly9Wy5bf/FGbOnKmHH37Y673R0dH+aBmQJFVWViokJMTfbdSLMUbV1dWef1+Av3BlB81CWFiYEhMTdcUVV+iGG27Qr3/9a/3lL3/RX//6V6+HDUZHRysxMdFradWqVb32+cc//lHdu3dXWFiYkpOT9eKLL3pt/93vfqcbb7zRs8977rlHR44c8RqzatUqXXPNNYqIiNCgQYN04MCBi97/0aNHdffdd+uKK65QZGSkrrvuOv3hD3/wGlNTU6M5c+bo6quvVlhYmOx2u2bNmiVJ6tSpkyTp+uuvV1BQkOchpwMHDtTEiRO96owYMcLrAYAXc2yX4u9//7tuueUWRUREqGPHjnrsscd06tQpz/bk5GQ9++yzevDBBxUdHS273a6FCxd6ttd1LOvXr9dNN92kVq1aqXXr1rr55pv19ddfe963YMECde7cWaGhoeratat+97vfefV1/PhxPfTQQ2rXrp1sNpsGDx6sL7744oLHs2TJEs2YMUNffPGF5wri2b+HQUFBWrBggX7yk5+oVatWmjVrlqqrqzV27Fh16tRJERER6tq1q15++WWvmmPGjNGIESP0wgsvqH379mrbtq0yMzNVWVnpGTN//nx16dJF4eHhSkhI0H/91395tg0cOFDjx4/X+PHjFRMTo7i4OD311FNeDyW+0Hk9+/HkX//6V/Xu3VthYWH6+9//rpqaGs2ePdvTf8+ePfX++++f877c3FzdeOONioyMVP/+/bVnzx6vY1yxYoX69Omj8PBwxcXF6ac//amkb/8n5dprrz3n59yrVy899dRTFzwfaAYMYHEZGRnmjjvuqHVbz549zdChQ40xxiQlJZm5c+fWax9/+9vfjCTzn//8xxhjzOeff26Cg4PNzJkzzZ49e8zixYtNRESEWbx4sec9b775plm1apXZt2+fycvLMw6Hw9OLMcY4nU4TFhZmsrKyTGFhofn9739vEhISvPZzPv/+97/N888/b/7xj3+Yffv2mVdeecW0aNHCbNmyxTPml7/8pWnTpo1ZsmSJ2bt3r/n000/NG2+8YYwx5rPPPjOSzNq1a01RUZE5evSoMcaYAQMGmMcff9xrX3fccYfJyMi46GP7/s/rfPbu3WtatWpl5s6da/71r3+ZjRs3muuvv96MGTPGMyYpKcnExsaanJwc8+WXX5rZs2eb4OBgU1hYWOexVFZWmpiYGPPkk0+avXv3mt27d5slS5aYr7/+2hhjzJ/+9CcTEhJicnJyzJ49e8yLL75oWrRoYdatW+fZb1pamhk+fLjZunWr+de//mWeeOIJ07ZtW8/Pqi6nT582TzzxhOnevbspKioyRUVF5vTp08YYYySZ+Ph4s2jRIrNv3z7z9ddfm4qKCjN9+nSzdetW89VXX5nf//73JjIy0rz77ruemhkZGcZms5lHH33UFBQUmBUrVpjIyEizcOFCY4wxW7duNS1atDBLly41Bw4cMNu2bTMvv/yy5/0DBgwwUVFR5vHHH/f8ffvu+y/lvPbo0cN8/PHHZu/evebo0aPmN7/5jUlJSTGrV682+/btM4sXLzZhYWFm/fr1Xu/r27evWb9+vdm1a5e55ZZbTP/+/T21V65caVq0aGGmT59udu/ebbZv326effZZY4wxBw8eNMHBweazzz7zjN+2bZsJCgoy+/btO++5QPNA2IHlnS/s3HnnnaZbt27GmG9/YYaGhppWrVp5LZ988skF9/H9X9733HOPue2227zGTJ482aSmptZZY+vWrUaSOXHihDHGmKlTp54zfsqUKRcdEmozbNgw88QTTxhjjHG73SYsLMwTbr5v//79RpL5xz/+4bX+YsLO933/2C4l7IwdO9b8/Oc/91r36aefmuDgYFNWVmaM+fbcjR492rO9pqbGxMfHmwULFtR5LEePHjWSPL9wv69///7m4Ycf9lr3s5/9zPzoRz/y9GCz2cyZM2e8xnTu3Nn89re/veBxZWdnm549e56zXpKZOHHiBd+fmZlpRo0a5XmdkZFhkpKSTFVVlVe/d955pzHGmD/+8Y/GZrMZt9tda70BAwaYbt26mZqaGs+6KVOmeP591Kau8/rnP//ZM+bMmTMmMjLSbNq0yeu9Y8eONXfffbfX+9auXevZ/uGHHxpJnnPscDjMvffeW2cvQ4cONePGjfO8njBhghk4cGCd49G88DEWmjVjjIKCgjyvJ0+erO3bt3stN9544yXXLSgo0M033+y17uabb9aXX36p6upqSVJ+fr6GDx8uu92u6OhoDRgwQJLkdDo9Nfr27etVw+FwXHQP1dXVeuaZZ3TdddcpNjZWUVFR+uijj7zql5eX69Zbb73k47uQCx3bpfjiiy+0ZMkSRUVFeZb09HTV1NRo//79nnE9evTw/DkoKEiJiYnn/egsNjZWY8aMUXp6uoYPH66XX35ZRUVFnu11ncOCggJPXydPnlTbtm29etu/f7/27dt3ycf5XbX9ncvJyVHv3r3Vrl07RUVFaeHChef8PLt3764WLVp4Xrdv397zM7jtttuUlJSkq666Svfdd5/eeecdnT592uv9/fr18/r34HA4LunvbG397927V6dPn9Ztt93m9XN6++23z/k5ffcctm/fXpI8/W/fvv28f1cffvhh/eEPf9CZM2dUUVGhpUuX6sEHH6xzPJoXZo2hWSsoKPDM55CkuLg4XX311Y2+31OnTik9PV3p6el655131K5dOzmdTqWnp6uiosIn+3j++ef18ssva968ebruuuvUqlUrTZw40VM/IiKiXnWDg4O95nFI8poX4utjO3nypB555BE99thj52yz2+2eP39/Em9QUJDX3Xa1Wbx4sR577DGtXr1a7777rqZNm6Y1a9aoX79+F9VX+/bttX79+nO2tW7d+oLvP5/vzxNbtmyZnnzySb344otyOByKjo7W888/ry1btniNO9/PIDo6Wtu2bdP69ev18ccfa/r06Xr66ae1devWi+r3Us7rd/s/efKkJOnDDz/UFVdc4TUuLCyszv7Phq6z/V/o7+vw4cMVFhamDz74QKGhoaqsrPSak4TmjbCDZmvdunXasWOHJk2a5PPa3bp108aNG73Wbdy4Uddcc41atGihwsJCHT16VM8995w6duwoSfr888/PqbF8+XKvdZs3b77oHjZu3Kg77rhDo0ePlvTtL41//etfSk1NlSR16dJFERERys3N1UMPPXTO+0NDQyXJ83/1Z7Vr187rCkh1dbV27typQYMGSdJFHduluOGGG7R79+4GhdC6jkX6dtLy9ddfr6lTp8rhcGjp0qXq16+f5xxmZGR4xm7cuNHz87vhhhtUXFysli1bKjk5uV491dZPbTZu3Kj+/fvrF7/4hWddfa4etWzZUmlpaUpLS1N2drZat26tdevWaeTIkZJ0TnjavHmzunTpctF/Z2uTmpqqsLAwOZ1Oz5Wg+ujRo4dyc3P1wAMP1HlsGRkZWrx4sUJDQ3XXXXfVO9DDegg7aBbKy8tVXFzsdev57Nmz9eMf/1j333+/Z9yJEydUXFzs9d7IyEjZbLZL2t8TTzyhPn366JlnntGdd96pvLw8vfbaa5o/f76kb69IhIaG6tVXX9Wjjz6qnTt36plnnvGq8eijj+rFF1/U5MmT9dBDDyk/P9/rzrEL6dKli95//31t2rRJbdq00UsvvaSSkhLPL+vw8HBNmTJFv/zlLxUaGqqbb75Z33zzjXbt2qWxY8cqPj5eERERWr16ta688kqFh4crJiZGgwcPVlZWlj788EN17txZL730kteXA17MsV2KKVOmqF+/fho/frweeughtWrVSrt379aaNWv02muvXVSN2o7l2LFjWrhwoX7yk5+oQ4cO2rNnj7788kvP34fJkyfrv//7v3X99dcrLS1NK1as0J/+9CetXbtWkpSWliaHw6ERI0Zozpw5uuaaa3T48GF9+OGH+ulPf3rBjz+Tk5O1f/9+bd++XVdeeaWio6PPudJxVpcuXfT222/ro48+UqdOnfS73/1OW7du9boqeSErV67UV199pR/+8Idq06aNVq1apZqaGnXt2tUzxul0KisrS4888oi2bdumV1991XMXYX3Pa3R0tJ588klNmjRJNTU1+sEPfqDS0lJt3LhRNpvNK0yeT3Z2tm699VZ17txZd911l6qqqrRq1SpNmTLFM+ahhx5St27dJOmc/9lAM+fvSUNAY8vIyDCSjCTTsmVL065dO5OWlmYWLVpkqqurPeOSkpI84767PPLIIxfcR20Tbt9//32TmppqQkJCjN1uN88//7zXe5YuXWqSk5NNWFiYcTgcZvny5edMol2xYoW5+uqrTVhYmLnlllvMokWLLnpi79GjR80dd9xhoqKiTHx8vJk2bZq5//77vSZrV1dXm9/85jcmKSnJ0+fZO1yMMeaNN94wHTt2NMHBwWbAgAHGGGMqKirMuHHjTGxsrImPjzezZ88+Z4LyhY7tUiYoG/Pt3VS33XabiYqKMq1atTI9evQws2bN8myv7U66nj17muzs7DqPpbi42IwYMcK0b9/ehIaGmqSkJDN9+nSvvxPz5883V111lQkJCTHXXHONefvtt7324Xa7zYQJE0yHDh1MSEiI6dixo7n33nuN0+m84DGdOXPGjBo1yrRu3dpI8typJ8l88MEH54wdM2aMiYmJMa1btzbjxo0zv/rVr7wmONc2Ef/xxx/3nLdPP/3UDBgwwLRp08ZERESYHj16eN3NNWDAAPOLX/zCPProo8Zms5k2bdqYX//6114Tlut7Xmtqasy8efNM165dTUhIiGnXrp1JT083GzZsqPN9//jHP4wks3//fs+6P/7xj6ZXr14mNDTUxMXFmZEjR57zc73llltM9+7d6/7Bo1kKMuZ7H74DAJqdgQMHqlevXpo3b56/W6k3Y4y6dOmiX/ziF8rKyvJ3OwggfIwFAGjyvvnmGy1btkzFxcV1zutB88Wt58BFePTRR71um/3u8uijj/qlp6FDh9bZ07PPPuuXnurDKsfxfd27d6/zuN555x1/t2c58fHxmjlzphYuXKg2bdr4ux0EGD7GAi7CkSNH5Ha7a91ms9kUHx9/mTuSDh06pLKyslq3xcbGKjY29jJ3VD9WOY7v+/rrr71uyf+uhIQEnrkGXEaEHQAAYGl8jAUAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACzt/wHqg5tM9nzypgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(vali[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6577.00</td>\n",
       "      <td>6577.00</td>\n",
       "      <td>6577.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.05</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.82</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.09</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                             6577.00                     6577.00   \n",
       "mean                                 0.03                        0.27   \n",
       "std                                  0.97                        1.29   \n",
       "min                                 -2.05                       -0.64   \n",
       "25%                                 -0.82                       -0.64   \n",
       "50%                                 -0.01                       -0.56   \n",
       "75%                                  0.91                        1.05   \n",
       "max                                  2.09                        4.16   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                    6577.00  \n",
       "mean                        0.14  \n",
       "std                         1.03  \n",
       "min                        -1.21  \n",
       "25%                        -0.64  \n",
       "50%                        -0.14  \n",
       "75%                         0.65  \n",
       "max                         4.24  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(test[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8895.00</td>\n",
       "      <td>8895.00</td>\n",
       "      <td>8895.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.98</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.03</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                             8895.00                     8895.00   \n",
       "mean                                 0.01                        0.14   \n",
       "std                                  0.98                        1.20   \n",
       "min                                 -2.23                       -0.64   \n",
       "25%                                 -0.81                       -0.64   \n",
       "50%                                 -0.05                       -0.62   \n",
       "75%                                  0.87                        0.59   \n",
       "max                                  2.03                        4.34   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                    8895.00  \n",
       "mean                        0.49  \n",
       "std                         1.22  \n",
       "min                        -1.20  \n",
       "25%                        -0.46  \n",
       "50%                         0.18  \n",
       "75%                         1.14  \n",
       "max                         4.30  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution ETTH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8521 observations in the train dataset.\n",
      " 6042 observations in the test dataset.\n",
      " 2857 observations in the validation dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "time_series = pd.read_csv(\"./datasets/ETTh1.csv\", index_col=0, parse_dates=True)\n",
    "train, vali, test = split_scale_dataset(data=time_series, train_size=8521, val_size=2857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 17420 entries, 2016-07-01 00:00:00 to 2018-06-26 19:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   HUFL    17420 non-null  float64\n",
      " 1   HULL    17420 non-null  float64\n",
      " 2   MUFL    17420 non-null  float64\n",
      " 3   MULL    17420 non-null  float64\n",
      " 4   LUFL    17420 non-null  float64\n",
      " 5   LULL    17420 non-null  float64\n",
      " 6   OT      17420 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "time_series.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14235"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8521 + 2857 + 2857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8569\n",
      "val 4345\n",
      "test 4321\n",
      "\titers: 100, epoch: 1 | loss: 0.4733695\n",
      "\tspeed: 0.0626s/iter; left time: 160.8995s\n",
      "\titers: 200, epoch: 1 | loss: 0.3275512\n",
      "\tspeed: 0.0413s/iter; left time: 102.0419s\n",
      "Epoch: 1 running time: 0.20149732033411663 min.\n",
      "Epoch: 1, Steps: 267 | Train Loss: 0.4435941 Vali Loss: 0.6720330 Test Loss: 0.7471600\n",
      "Validation loss decreased (inf --> 0.672033).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3207772\n",
      "\tspeed: 0.1169s/iter; left time: 269.3744s\n",
      "\titers: 200, epoch: 2 | loss: 0.3147012\n",
      "\tspeed: 0.0416s/iter; left time: 91.7787s\n",
      "Epoch: 2 running time: 0.18970996936162313 min.\n",
      "Epoch: 2, Steps: 267 | Train Loss: 0.3283183 Vali Loss: 0.6654594 Test Loss: 0.8112167\n",
      "Validation loss decreased (0.672033 --> 0.665459).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2579902\n",
      "\tspeed: 0.1220s/iter; left time: 248.5572s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# https://colab.research.google.com/drive/1rv2rKwQqgoHDNjXtRoAEWZ2ATz0gGAKu?usp=sharing#scrollTo=yu6zzic9t_Cz\n",
    "# Popen: https://colab.research.google.com/github/aviadr1/learn-python/blob/master/content/13_multiprocessing/notebooks/os_system_subprocess.ipynb\n",
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"./TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output  \n",
    "\n",
    "    return output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"/content/my_work/TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Define command and options wanted\n",
    "        command = \"python\"\n",
    "        options = \"-u\"\n",
    "        # Run the shell command directly in Colab\n",
    "        output = !{command} {options} {path_to_run_file} {model_arguments}\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output\n",
    "\n",
    "    return output\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script using the %run magic command\n",
    "        output = %run -i {path_to_run_file} {model_arguments}\n",
    "\n",
    "    except Exception as e:\n",
    "        output = str(e)\n",
    "\n",
    "    return output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to work in colab\n",
    "import subprocess\n",
    "\n",
    "path_to_run_file = \"./TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Construct the command to execute the script with required and model arguments\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        # Execute the script and capture the output\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        # Check if there's any error in the process\n",
    "        if process.returncode != 0:\n",
    "            output = stderr.decode(\"utf-8\")\n",
    "        else:\n",
    "            output = stdout.decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        output = str(e)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_most_important_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m script_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--is_training\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--itr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[0;32m---> 29\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_run_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\u001b[39;00m\n\u001b[1;32m     32\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/cs-hu/riabchuv/hu-home/my_work/results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInformer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mrun_output\u001b[0;34m(path_to_run_file, model_arguments)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the script and capture the output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m---> 12\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there's any error in the process\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "script_arguments = [\n",
    "    \"--task_name\", \"long_term_forecast\",\n",
    "    \"--is_training\", \"1\",\n",
    "    \"--root_path\", current_path,\n",
    "    \"--data_path\", dataset,\n",
    "    \"--model_id\", \"1\",\n",
    "    \"--model\", \"Informer\",\n",
    "    \"--data\", \"custom\",\n",
    "    \"--features\", \"M\",\n",
    "    \"--seq_len\", \"96\",\n",
    "    \"--label_len\", \"48\",\n",
    "    \"--pred_len\", \"24\",\n",
    "    \"--e_layers\", \"2\",\n",
    "    \"--d_layers\", \"5\",\n",
    "    \"--factor\", \"5\",\n",
    "    \"--enc_in\", \"3\",\n",
    "    \"--dec_in\", \"3\",\n",
    "    \"--c_out\", \"3\",\n",
    "    \"--des\", \"Exp\",\n",
    "    \"--itr\", \"2\"\n",
    "]\n",
    "\n",
    "model_output = run_output(path_to_run_file, script_arguments)\n",
    "\n",
    "#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "folder_path = f'./results/{\"Informer\"}/'\n",
    "\n",
    "# Write model output into txt file\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output_chr.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "print(model_output)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 07:00:00</td>\n",
       "      <td>41133.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 08:00:00</td>\n",
       "      <td>42963.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>10029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 09:00:00</td>\n",
       "      <td>45088.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>10550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 10:00:00</td>\n",
       "      <td>47013.0</td>\n",
       "      <td>3364.0</td>\n",
       "      <td>11390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 11:00:00</td>\n",
       "      <td>48159.0</td>\n",
       "      <td>4198.0</td>\n",
       "      <td>12103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43788</th>\n",
       "      <td>2019-12-30 19:00:00</td>\n",
       "      <td>53959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43789</th>\n",
       "      <td>2019-12-30 20:00:00</td>\n",
       "      <td>51937.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43790</th>\n",
       "      <td>2019-12-30 21:00:00</td>\n",
       "      <td>50574.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43791</th>\n",
       "      <td>2019-12-30 22:00:00</td>\n",
       "      <td>47382.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43792</th>\n",
       "      <td>2019-12-30 23:00:00</td>\n",
       "      <td>44018.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29928.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43793 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  DE_load_actual_entsoe_transparency  \\\n",
       "0      2015-01-01 07:00:00                             41133.0   \n",
       "1      2015-01-01 08:00:00                             42963.0   \n",
       "2      2015-01-01 09:00:00                             45088.0   \n",
       "3      2015-01-01 10:00:00                             47013.0   \n",
       "4      2015-01-01 11:00:00                             48159.0   \n",
       "...                    ...                                 ...   \n",
       "43788  2019-12-30 19:00:00                             53959.0   \n",
       "43789  2019-12-30 20:00:00                             51937.0   \n",
       "43790  2019-12-30 21:00:00                             50574.0   \n",
       "43791  2019-12-30 22:00:00                             47382.0   \n",
       "43792  2019-12-30 23:00:00                             44018.0   \n",
       "\n",
       "       DE_solar_generation_actual  DE_wind_generation_actual  \n",
       "0                            71.0                    10208.0  \n",
       "1                           773.0                    10029.0  \n",
       "2                          2117.0                    10550.0  \n",
       "3                          3364.0                    11390.0  \n",
       "4                          4198.0                    12103.0  \n",
       "...                           ...                        ...  \n",
       "43788                         0.0                    32323.0  \n",
       "43789                         0.0                    32395.0  \n",
       "43790                         0.0                    31439.0  \n",
       "43791                         0.0                    30727.0  \n",
       "43792                         0.0                    29928.0  \n",
       "\n",
       "[43793 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./datasets/df_most_important_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43611"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28321 + 6577 + 8713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28321\n",
      "val 6577\n",
      "test 8713\n",
      "\titers: 100, epoch: 1 | loss: 0.4881925\n",
      "\tspeed: 0.0612s/iter; left time: 535.5396s\n",
      "\titers: 200, epoch: 1 | loss: 0.4415741\n",
      "\tspeed: 0.0419s/iter; left time: 362.6325s\n",
      "\titers: 300, epoch: 1 | loss: 0.4410460\n",
      "\tspeed: 0.0412s/iter; left time: 351.9689s\n",
      "\titers: 400, epoch: 1 | loss: 0.2902627\n",
      "\tspeed: 0.0415s/iter; left time: 350.4010s\n",
      "\titers: 500, epoch: 1 | loss: 0.3088942\n",
      "\tspeed: 0.0412s/iter; left time: 343.8548s\n",
      "\titers: 600, epoch: 1 | loss: 0.3044106\n",
      "\tspeed: 0.0414s/iter; left time: 341.2761s\n",
      "\titers: 700, epoch: 1 | loss: 0.2735647\n",
      "\tspeed: 0.0421s/iter; left time: 342.8331s\n",
      "\titers: 800, epoch: 1 | loss: 0.2792752\n",
      "\tspeed: 0.0417s/iter; left time: 335.5625s\n",
      "Epoch: 1 running time: 0.6292555173238118 min.\n",
      "Epoch: 1, Steps: 885 | Train Loss: 0.3454897 Vali Loss: 0.2995675 Test Loss: 0.3626922\n",
      "Validation loss decreased (inf --> 0.299567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2089891\n",
      "\tspeed: 0.1524s/iter; left time: 1198.9570s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 160, in train\n",
      "    model_optim.step()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 166, in step\n",
      "    adam(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 316, in adam\n",
      "    func(params,\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 583, in _multi_tensor_adam\n",
      "    torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt, step_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 4321\n",
      "test 8641\n",
      "\titers: 100, epoch: 1 | loss: 0.7090327\n",
      "\tspeed: 0.0694s/iter; left time: 653.2046s\n",
      "\titers: 200, epoch: 1 | loss: 0.6357286\n",
      "\tspeed: 0.0500s/iter; left time: 465.5139s\n",
      "\titers: 300, epoch: 1 | loss: 0.5953805\n",
      "\tspeed: 0.0505s/iter; left time: 464.7455s\n",
      "\titers: 400, epoch: 1 | loss: 0.4663185\n",
      "\tspeed: 0.0504s/iter; left time: 459.0093s\n",
      "\titers: 500, epoch: 1 | loss: 0.4046238\n",
      "\tspeed: 0.0505s/iter; left time: 455.3592s\n",
      "\titers: 600, epoch: 1 | loss: 0.3477176\n",
      "\tspeed: 0.0494s/iter; left time: 440.2458s\n",
      "\titers: 700, epoch: 1 | loss: 0.4344813\n",
      "\tspeed: 0.0500s/iter; left time: 440.9754s\n",
      "\titers: 800, epoch: 1 | loss: 0.3226214\n",
      "\tspeed: 0.0484s/iter; left time: 421.2105s\n",
      "\titers: 900, epoch: 1 | loss: 0.3889300\n",
      "\tspeed: 0.0482s/iter; left time: 415.0947s\n",
      "Epoch: 1 running time: 0.8037162542343139 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.5144386 Vali Loss: 0.4448476 Test Loss: 0.6013156\n",
      "Validation loss decreased (inf --> 0.444848).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3872515\n",
      "\tspeed: 0.1517s/iter; left time: 1283.7868s\n",
      "\titers: 200, epoch: 2 | loss: 0.2599374\n",
      "\tspeed: 0.0506s/iter; left time: 422.8650s\n",
      "\titers: 300, epoch: 2 | loss: 0.4126107\n",
      "\tspeed: 0.0495s/iter; left time: 408.7092s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  #--d_ff 256 \\\n",
    "  #--d_model 256 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28249\n",
      "val 6505\n",
      "test 8641\n",
      "\titers: 100, epoch: 1 | loss: 0.7768582\n",
      "\tspeed: 0.0982s/iter; left time: 856.2301s\n",
      "\titers: 200, epoch: 1 | loss: 0.6496214\n",
      "\tspeed: 0.0791s/iter; left time: 681.5313s\n",
      "\titers: 300, epoch: 1 | loss: 0.5861947\n",
      "\tspeed: 0.0791s/iter; left time: 674.2545s\n",
      "\titers: 400, epoch: 1 | loss: 0.4726883\n",
      "\tspeed: 0.0785s/iter; left time: 660.8224s\n",
      "\titers: 500, epoch: 1 | loss: 0.4668243\n",
      "\tspeed: 0.0783s/iter; left time: 651.3847s\n",
      "\titers: 600, epoch: 1 | loss: 0.3922010\n",
      "\tspeed: 0.0784s/iter; left time: 644.8716s\n",
      "\titers: 700, epoch: 1 | loss: 0.4009480\n",
      "\tspeed: 0.0790s/iter; left time: 641.2551s\n",
      "\titers: 800, epoch: 1 | loss: 0.3644775\n",
      "\tspeed: 0.0792s/iter; left time: 635.1636s\n",
      "Epoch: 1 running time: 1.1743187705675762 min.\n",
      "Epoch: 1, Steps: 882 | Train Loss: 0.5210655 Vali Loss: 0.4104438 Test Loss: 0.5408772\n",
      "Validation loss decreased (inf --> 0.410444).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3693374\n",
      "\tspeed: 0.3058s/iter; left time: 2397.3544s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/well0203/my_work.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r /content/my_work/TSLibrary/requirements.txt\n",
    "#!pip install sktime\n",
    "#!pip install reformer-pytorch==1.4.4\n",
    "# Drive python version 3.10.6, therefore torch==1.7.1 does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5979588\n",
      "\tspeed: 0.1078s/iter; left time: 1003.5939s\n",
      "\titers: 200, epoch: 1 | loss: 0.3988746\n",
      "\tspeed: 0.0981s/iter; left time: 903.5919s\n",
      "\titers: 300, epoch: 1 | loss: 0.4951838\n",
      "\tspeed: 0.0980s/iter; left time: 892.9673s\n",
      "\titers: 400, epoch: 1 | loss: 0.3251971\n",
      "\tspeed: 0.0979s/iter; left time: 882.0710s\n",
      "\titers: 500, epoch: 1 | loss: 0.2972392\n",
      "\tspeed: 0.0979s/iter; left time: 872.1553s\n",
      "\titers: 600, epoch: 1 | loss: 0.3141234\n",
      "\tspeed: 0.0983s/iter; left time: 865.7961s\n",
      "\titers: 700, epoch: 1 | loss: 0.3582392\n",
      "\tspeed: 0.0980s/iter; left time: 853.8956s\n",
      "\titers: 800, epoch: 1 | loss: 0.3090230\n",
      "\tspeed: 0.0979s/iter; left time: 842.9842s\n",
      "\titers: 900, epoch: 1 | loss: 0.2600375\n",
      "\tspeed: 0.0980s/iter; left time: 833.7338s\n",
      "Epoch: 1 running time: 1.5546314120292664 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3885667 Vali Loss: 0.3599150 Test Loss: 0.5413693\n",
      "Validation loss decreased (inf --> 0.359915).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2532829\n",
      "\tspeed: 0.2840s/iter; left time: 2377.0272s\n",
      "\titers: 200, epoch: 2 | loss: 0.2295953\n",
      "\tspeed: 0.0978s/iter; left time: 809.0253s\n",
      "\titers: 300, epoch: 2 | loss: 0.1922673\n",
      "\tspeed: 0.0976s/iter; left time: 797.2087s\n",
      "\titers: 400, epoch: 2 | loss: 0.4123895\n",
      "\tspeed: 0.0978s/iter; left time: 788.9432s\n",
      "\titers: 500, epoch: 2 | loss: 0.2371316\n",
      "\tspeed: 0.0983s/iter; left time: 783.2757s\n",
      "\titers: 600, epoch: 2 | loss: 0.3011464\n",
      "\tspeed: 0.0982s/iter; left time: 772.5010s\n",
      "\titers: 700, epoch: 2 | loss: 0.2392187\n",
      "\tspeed: 0.0980s/iter; left time: 761.5209s\n",
      "\titers: 800, epoch: 2 | loss: 0.2716683\n",
      "\tspeed: 0.0976s/iter; left time: 748.2816s\n",
      "\titers: 900, epoch: 2 | loss: 0.2080009\n",
      "\tspeed: 0.0977s/iter; left time: 739.3383s\n",
      "Epoch: 2 running time: 1.5396055936813355 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2747000 Vali Loss: 0.3615359 Test Loss: 0.5790962\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3574236\n",
      "\tspeed: 0.2801s/iter; left time: 2081.2302s\n",
      "\titers: 200, epoch: 3 | loss: 0.2763731\n",
      "\tspeed: 0.0978s/iter; left time: 716.7625s\n",
      "\titers: 300, epoch: 3 | loss: 0.2299239\n",
      "\tspeed: 0.0976s/iter; left time: 705.4228s\n",
      "\titers: 400, epoch: 3 | loss: 0.2516459\n",
      "\tspeed: 0.0977s/iter; left time: 696.3497s\n",
      "\titers: 500, epoch: 3 | loss: 0.2625080\n",
      "\tspeed: 0.0978s/iter; left time: 687.0868s\n",
      "\titers: 600, epoch: 3 | loss: 0.2179623\n",
      "\tspeed: 0.0977s/iter; left time: 676.7548s\n",
      "\titers: 700, epoch: 3 | loss: 0.2282525\n",
      "\tspeed: 0.0975s/iter; left time: 665.8238s\n",
      "\titers: 800, epoch: 3 | loss: 0.3034237\n",
      "\tspeed: 0.0975s/iter; left time: 656.3987s\n",
      "\titers: 900, epoch: 3 | loss: 0.3019572\n",
      "\tspeed: 0.0977s/iter; left time: 647.6157s\n",
      "Epoch: 3 running time: 1.5372791488965352 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2638785 Vali Loss: 0.3499245 Test Loss: 0.5233747\n",
      "Validation loss decreased (0.359915 --> 0.349925).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2404048\n",
      "\tspeed: 0.2853s/iter; left time: 1850.7819s\n",
      "\titers: 200, epoch: 4 | loss: 0.2120794\n",
      "\tspeed: 0.0979s/iter; left time: 625.5544s\n",
      "\titers: 300, epoch: 4 | loss: 0.2520547\n",
      "\tspeed: 0.0976s/iter; left time: 613.7269s\n",
      "\titers: 400, epoch: 4 | loss: 0.2569040\n",
      "\tspeed: 0.0978s/iter; left time: 605.3048s\n",
      "\titers: 500, epoch: 4 | loss: 0.1980895\n",
      "\tspeed: 0.0977s/iter; left time: 595.0931s\n",
      "\titers: 600, epoch: 4 | loss: 0.2096463\n",
      "\tspeed: 0.0975s/iter; left time: 584.0573s\n",
      "\titers: 700, epoch: 4 | loss: 0.2016285\n",
      "\tspeed: 0.0977s/iter; left time: 575.0260s\n",
      "\titers: 800, epoch: 4 | loss: 0.1970884\n",
      "\tspeed: 0.0975s/iter; left time: 564.5929s\n",
      "\titers: 900, epoch: 4 | loss: 0.2034923\n",
      "\tspeed: 0.0975s/iter; left time: 554.6836s\n",
      "Epoch: 4 running time: 1.5379542350769042 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2285615 Vali Loss: 0.3327911 Test Loss: 0.5450918\n",
      "Validation loss decreased (0.349925 --> 0.332791).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1831390\n",
      "\tspeed: 0.2850s/iter; left time: 1580.6327s\n",
      "\titers: 200, epoch: 5 | loss: 0.2395813\n",
      "\tspeed: 0.0973s/iter; left time: 530.2214s\n",
      "\titers: 300, epoch: 5 | loss: 0.1956781\n",
      "\tspeed: 0.0973s/iter; left time: 520.5293s\n",
      "\titers: 400, epoch: 5 | loss: 0.2888322\n",
      "\tspeed: 0.0975s/iter; left time: 511.7390s\n",
      "\titers: 500, epoch: 5 | loss: 0.1966490\n",
      "\tspeed: 0.0974s/iter; left time: 501.3349s\n",
      "\titers: 600, epoch: 5 | loss: 0.1773667\n",
      "\tspeed: 0.0976s/iter; left time: 492.3736s\n",
      "\titers: 700, epoch: 5 | loss: 0.1866304\n",
      "\tspeed: 0.0975s/iter; left time: 482.3317s\n",
      "\titers: 800, epoch: 5 | loss: 0.1474601\n",
      "\tspeed: 0.0973s/iter; left time: 471.8175s\n",
      "\titers: 900, epoch: 5 | loss: 0.2766321\n",
      "\tspeed: 0.0974s/iter; left time: 462.2471s\n",
      "Epoch: 5 running time: 1.533496336142222 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2052569 Vali Loss: 0.3558107 Test Loss: 0.5533416\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1901380\n",
      "\tspeed: 0.2782s/iter; left time: 1281.4157s\n",
      "\titers: 200, epoch: 6 | loss: 0.1638627\n",
      "\tspeed: 0.0975s/iter; left time: 439.1547s\n",
      "\titers: 300, epoch: 6 | loss: 0.2496205\n",
      "\tspeed: 0.0974s/iter; left time: 428.9254s\n",
      "\titers: 400, epoch: 6 | loss: 0.2062164\n",
      "\tspeed: 0.0975s/iter; left time: 420.0256s\n",
      "\titers: 500, epoch: 6 | loss: 0.2275229\n",
      "\tspeed: 0.0977s/iter; left time: 410.7234s\n",
      "\titers: 600, epoch: 6 | loss: 0.1896032\n",
      "\tspeed: 0.0975s/iter; left time: 400.2688s\n",
      "\titers: 700, epoch: 6 | loss: 0.1997263\n",
      "\tspeed: 0.0975s/iter; left time: 390.4182s\n",
      "\titers: 800, epoch: 6 | loss: 0.2681848\n",
      "\tspeed: 0.0975s/iter; left time: 380.8619s\n",
      "\titers: 900, epoch: 6 | loss: 0.2032136\n",
      "\tspeed: 0.0973s/iter; left time: 370.5132s\n",
      "Epoch: 6 running time: 1.5333383083343506 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2041673 Vali Loss: 0.3364544 Test Loss: 0.5421493\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1747950\n",
      "\tspeed: 0.2787s/iter; left time: 1021.2745s\n",
      "\titers: 200, epoch: 7 | loss: 0.2300793\n",
      "\tspeed: 0.0976s/iter; left time: 347.9793s\n",
      "\titers: 300, epoch: 7 | loss: 0.1680039\n",
      "\tspeed: 0.0976s/iter; left time: 338.2595s\n",
      "\titers: 400, epoch: 7 | loss: 0.2118735\n",
      "\tspeed: 0.0978s/iter; left time: 329.1586s\n",
      "\titers: 500, epoch: 7 | loss: 0.1732029\n",
      "\tspeed: 0.0978s/iter; left time: 319.2355s\n",
      "\titers: 600, epoch: 7 | loss: 0.1689785\n",
      "\tspeed: 0.0977s/iter; left time: 309.1045s\n",
      "\titers: 700, epoch: 7 | loss: 0.2123212\n",
      "\tspeed: 0.0974s/iter; left time: 298.5726s\n",
      "\titers: 800, epoch: 7 | loss: 0.2141715\n",
      "\tspeed: 0.0975s/iter; left time: 289.2073s\n",
      "\titers: 900, epoch: 7 | loss: 0.1998224\n",
      "\tspeed: 0.0976s/iter; left time: 279.6204s\n",
      "Epoch: 7 running time: 1.5368539373079935 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2040884 Vali Loss: 0.3359562 Test Loss: 0.5423290\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.542351484298706, mae:0.49271532893180847\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5885839\n",
      "\tspeed: 0.0911s/iter; left time: 848.3252s\n",
      "\titers: 200, epoch: 1 | loss: 0.4808174\n",
      "\tspeed: 0.0983s/iter; left time: 905.3794s\n",
      "\titers: 300, epoch: 1 | loss: 0.3567823\n",
      "\tspeed: 0.0987s/iter; left time: 899.4436s\n",
      "\titers: 400, epoch: 1 | loss: 0.3922084\n",
      "\tspeed: 0.0992s/iter; left time: 894.3320s\n",
      "\titers: 500, epoch: 1 | loss: 0.3549460\n",
      "\tspeed: 0.0982s/iter; left time: 874.8667s\n",
      "\titers: 600, epoch: 1 | loss: 0.4167266\n",
      "\tspeed: 0.0980s/iter; left time: 863.8790s\n",
      "\titers: 700, epoch: 1 | loss: 0.3898646\n",
      "\tspeed: 0.0958s/iter; left time: 834.3015s\n",
      "\titers: 800, epoch: 1 | loss: 0.2914270\n",
      "\tspeed: 0.0983s/iter; left time: 846.5094s\n",
      "\titers: 900, epoch: 1 | loss: 0.2078680\n",
      "\tspeed: 0.0986s/iter; left time: 838.8358s\n",
      "Epoch: 1 running time: 1.52977428038915 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3939400 Vali Loss: 0.3607067 Test Loss: 0.5616335\n",
      "Validation loss decreased (inf --> 0.360707).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2776674\n",
      "\tspeed: 0.2878s/iter; left time: 2409.1260s\n",
      "\titers: 200, epoch: 2 | loss: 0.2421905\n",
      "\tspeed: 0.0982s/iter; left time: 811.8457s\n",
      "\titers: 300, epoch: 2 | loss: 0.3501032\n",
      "\tspeed: 0.0979s/iter; left time: 799.7434s\n",
      "\titers: 400, epoch: 2 | loss: 0.2221153\n",
      "\tspeed: 0.0980s/iter; left time: 791.1889s\n",
      "\titers: 500, epoch: 2 | loss: 0.3621227\n",
      "\tspeed: 0.0978s/iter; left time: 779.6867s\n",
      "\titers: 600, epoch: 2 | loss: 0.3243517\n",
      "\tspeed: 0.0979s/iter; left time: 770.5618s\n",
      "\titers: 700, epoch: 2 | loss: 0.3431254\n",
      "\tspeed: 0.0978s/iter; left time: 759.9195s\n",
      "\titers: 800, epoch: 2 | loss: 0.2447850\n",
      "\tspeed: 0.0979s/iter; left time: 751.0887s\n",
      "\titers: 900, epoch: 2 | loss: 0.2354745\n",
      "\tspeed: 0.0979s/iter; left time: 740.9177s\n",
      "Epoch: 2 running time: 1.5421267906824747 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2753468 Vali Loss: 0.3552379 Test Loss: 0.5603691\n",
      "Validation loss decreased (0.360707 --> 0.355238).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2077665\n",
      "\tspeed: 0.2863s/iter; left time: 2126.7015s\n",
      "\titers: 200, epoch: 3 | loss: 0.2024092\n",
      "\tspeed: 0.0977s/iter; left time: 716.3017s\n",
      "\titers: 300, epoch: 3 | loss: 0.2525340\n",
      "\tspeed: 0.0977s/iter; left time: 706.2460s\n",
      "\titers: 400, epoch: 3 | loss: 0.2007055\n",
      "\tspeed: 0.0982s/iter; left time: 699.9823s\n",
      "\titers: 500, epoch: 3 | loss: 0.2679331\n",
      "\tspeed: 0.0989s/iter; left time: 695.2980s\n",
      "\titers: 600, epoch: 3 | loss: 0.1978059\n",
      "\tspeed: 0.0991s/iter; left time: 686.6898s\n",
      "\titers: 700, epoch: 3 | loss: 0.1983410\n",
      "\tspeed: 0.0987s/iter; left time: 674.1259s\n",
      "\titers: 800, epoch: 3 | loss: 0.2749475\n",
      "\tspeed: 0.0981s/iter; left time: 660.0698s\n",
      "\titers: 900, epoch: 3 | loss: 0.2155552\n",
      "\tspeed: 0.0982s/iter; left time: 650.6816s\n",
      "Epoch: 3 running time: 1.5467728972434998 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2296516 Vali Loss: 0.3449344 Test Loss: 0.5333540\n",
      "Validation loss decreased (0.355238 --> 0.344934).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1944146\n",
      "\tspeed: 0.2881s/iter; left time: 1869.2917s\n",
      "\titers: 200, epoch: 4 | loss: 0.2291458\n",
      "\tspeed: 0.0977s/iter; left time: 624.0008s\n",
      "\titers: 300, epoch: 4 | loss: 0.1660754\n",
      "\tspeed: 0.0978s/iter; left time: 614.6755s\n",
      "\titers: 400, epoch: 4 | loss: 0.1500978\n",
      "\tspeed: 0.0977s/iter; left time: 604.4314s\n",
      "\titers: 500, epoch: 4 | loss: 0.2157296\n",
      "\tspeed: 0.0976s/iter; left time: 593.9782s\n",
      "\titers: 600, epoch: 4 | loss: 0.1963040\n",
      "\tspeed: 0.0978s/iter; left time: 585.5368s\n",
      "\titers: 700, epoch: 4 | loss: 0.2547735\n",
      "\tspeed: 0.0977s/iter; left time: 575.1413s\n",
      "\titers: 800, epoch: 4 | loss: 0.1726060\n",
      "\tspeed: 0.0977s/iter; left time: 565.3300s\n",
      "\titers: 900, epoch: 4 | loss: 0.1798732\n",
      "\tspeed: 0.0975s/iter; left time: 554.5946s\n",
      "Epoch: 4 running time: 1.5375561753908793 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1954855 Vali Loss: 0.3462040 Test Loss: 0.5407835\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1592964\n",
      "\tspeed: 0.2801s/iter; left time: 1553.5432s\n",
      "\titers: 200, epoch: 5 | loss: 0.1947924\n",
      "\tspeed: 0.0982s/iter; left time: 535.0782s\n",
      "\titers: 300, epoch: 5 | loss: 0.1617831\n",
      "\tspeed: 0.0981s/iter; left time: 524.6174s\n",
      "\titers: 400, epoch: 5 | loss: 0.2011821\n",
      "\tspeed: 0.0980s/iter; left time: 514.0995s\n",
      "\titers: 500, epoch: 5 | loss: 0.2041656\n",
      "\tspeed: 0.0975s/iter; left time: 501.5775s\n",
      "\titers: 600, epoch: 5 | loss: 0.1855498\n",
      "\tspeed: 0.0976s/iter; left time: 492.3413s\n",
      "\titers: 700, epoch: 5 | loss: 0.1450996\n",
      "\tspeed: 0.0975s/iter; left time: 482.4622s\n",
      "\titers: 800, epoch: 5 | loss: 0.1926186\n",
      "\tspeed: 0.0979s/iter; left time: 474.4031s\n",
      "\titers: 900, epoch: 5 | loss: 0.2197612\n",
      "\tspeed: 0.0980s/iter; left time: 465.3897s\n",
      "Epoch: 5 running time: 1.5406445781389873 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1928436 Vali Loss: 0.3556490 Test Loss: 0.5593730\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2385069\n",
      "\tspeed: 0.2796s/iter; left time: 1287.8641s\n",
      "\titers: 200, epoch: 6 | loss: 0.2327145\n",
      "\tspeed: 0.0978s/iter; left time: 440.4635s\n",
      "\titers: 300, epoch: 6 | loss: 0.2079315\n",
      "\tspeed: 0.0978s/iter; left time: 430.9414s\n",
      "\titers: 400, epoch: 6 | loss: 0.1807522\n",
      "\tspeed: 0.0976s/iter; left time: 420.3452s\n",
      "\titers: 500, epoch: 6 | loss: 0.2193317\n",
      "\tspeed: 0.0978s/iter; left time: 411.4552s\n",
      "\titers: 600, epoch: 6 | loss: 0.1836175\n",
      "\tspeed: 0.0977s/iter; left time: 401.0777s\n",
      "\titers: 700, epoch: 6 | loss: 0.2200144\n",
      "\tspeed: 0.0976s/iter; left time: 390.8376s\n",
      "\titers: 800, epoch: 6 | loss: 0.2055064\n",
      "\tspeed: 0.0976s/iter; left time: 381.2572s\n",
      "\titers: 900, epoch: 6 | loss: 0.2023076\n",
      "\tspeed: 0.0978s/iter; left time: 372.2206s\n",
      "Epoch: 6 running time: 1.5379039565722148 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1931179 Vali Loss: 0.3452641 Test Loss: 0.5490345\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.5488144755363464, mae:0.4969179332256317\n",
      "\n",
      "Time intermediate for DE dataset: 23.553507006168367 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4583763\n",
      "\tspeed: 0.1078s/iter; left time: 1003.8090s\n",
      "\titers: 200, epoch: 1 | loss: 0.3878897\n",
      "\tspeed: 0.0979s/iter; left time: 901.8856s\n",
      "\titers: 300, epoch: 1 | loss: 0.3311236\n",
      "\tspeed: 0.0977s/iter; left time: 890.4963s\n",
      "\titers: 400, epoch: 1 | loss: 0.3112401\n",
      "\tspeed: 0.0979s/iter; left time: 881.7811s\n",
      "\titers: 500, epoch: 1 | loss: 0.2713794\n",
      "\tspeed: 0.0980s/iter; left time: 873.5798s\n",
      "\titers: 600, epoch: 1 | loss: 0.3417489\n",
      "\tspeed: 0.0981s/iter; left time: 864.7337s\n",
      "\titers: 700, epoch: 1 | loss: 0.4003856\n",
      "\tspeed: 0.0980s/iter; left time: 853.8352s\n",
      "\titers: 800, epoch: 1 | loss: 0.2808123\n",
      "\tspeed: 0.0981s/iter; left time: 844.3268s\n",
      "\titers: 900, epoch: 1 | loss: 0.2576330\n",
      "\tspeed: 0.0980s/iter; left time: 833.8802s\n",
      "Epoch: 1 running time: 1.5542073210080465 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4064409 Vali Loss: 0.4122250 Test Loss: 0.7646125\n",
      "Validation loss decreased (inf --> 0.412225).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3067522\n",
      "\tspeed: 0.2851s/iter; left time: 2385.9309s\n",
      "\titers: 200, epoch: 2 | loss: 0.2744507\n",
      "\tspeed: 0.0981s/iter; left time: 810.9219s\n",
      "\titers: 300, epoch: 2 | loss: 0.3486772\n",
      "\tspeed: 0.0976s/iter; left time: 797.1353s\n",
      "\titers: 400, epoch: 2 | loss: 0.3093699\n",
      "\tspeed: 0.0979s/iter; left time: 790.0474s\n",
      "\titers: 500, epoch: 2 | loss: 0.2064186\n",
      "\tspeed: 0.0979s/iter; left time: 780.3782s\n",
      "\titers: 600, epoch: 2 | loss: 0.2748952\n",
      "\tspeed: 0.0976s/iter; left time: 768.4677s\n",
      "\titers: 700, epoch: 2 | loss: 0.2839590\n",
      "\tspeed: 0.0978s/iter; left time: 760.1446s\n",
      "\titers: 800, epoch: 2 | loss: 0.2919414\n",
      "\tspeed: 0.0979s/iter; left time: 750.7189s\n",
      "\titers: 900, epoch: 2 | loss: 0.2487071\n",
      "\tspeed: 0.0976s/iter; left time: 738.7067s\n",
      "Epoch: 2 running time: 1.5410709222157797 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3065783 Vali Loss: 0.3944587 Test Loss: 0.6807294\n",
      "Validation loss decreased (0.412225 --> 0.394459).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2702128\n",
      "\tspeed: 0.2877s/iter; left time: 2137.5545s\n",
      "\titers: 200, epoch: 3 | loss: 0.3190711\n",
      "\tspeed: 0.0979s/iter; left time: 717.7133s\n",
      "\titers: 300, epoch: 3 | loss: 0.2532526\n",
      "\tspeed: 0.0982s/iter; left time: 709.7745s\n",
      "\titers: 400, epoch: 3 | loss: 0.2061189\n",
      "\tspeed: 0.0976s/iter; left time: 696.1377s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977401\n",
      "\tspeed: 0.0977s/iter; left time: 686.5519s\n",
      "\titers: 600, epoch: 3 | loss: 0.2680047\n",
      "\tspeed: 0.0978s/iter; left time: 677.4644s\n",
      "\titers: 700, epoch: 3 | loss: 0.2145547\n",
      "\tspeed: 0.0874s/iter; left time: 597.0928s\n",
      "\titers: 800, epoch: 3 | loss: 0.2101406\n",
      "\tspeed: 0.0809s/iter; left time: 544.3271s\n",
      "\titers: 900, epoch: 3 | loss: 0.3406821\n",
      "\tspeed: 0.0809s/iter; left time: 536.2867s\n",
      "Epoch: 3 running time: 1.4540736556053162 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2630870 Vali Loss: 0.4008739 Test Loss: 0.7279691\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2422981\n",
      "\tspeed: 0.2724s/iter; left time: 1767.1854s\n",
      "\titers: 200, epoch: 4 | loss: 0.1884126\n",
      "\tspeed: 0.0978s/iter; left time: 624.4336s\n",
      "\titers: 300, epoch: 4 | loss: 0.2599521\n",
      "\tspeed: 0.0975s/iter; left time: 613.1612s\n",
      "\titers: 400, epoch: 4 | loss: 0.2694383\n",
      "\tspeed: 0.0977s/iter; left time: 604.5321s\n",
      "\titers: 500, epoch: 4 | loss: 0.2439736\n",
      "\tspeed: 0.0974s/iter; left time: 593.0381s\n",
      "\titers: 600, epoch: 4 | loss: 0.2632573\n",
      "\tspeed: 0.0974s/iter; left time: 583.4135s\n",
      "\titers: 700, epoch: 4 | loss: 0.2553389\n",
      "\tspeed: 0.0975s/iter; left time: 574.0162s\n",
      "\titers: 800, epoch: 4 | loss: 0.2767370\n",
      "\tspeed: 0.0975s/iter; left time: 564.1970s\n",
      "\titers: 900, epoch: 4 | loss: 0.2517391\n",
      "\tspeed: 0.0977s/iter; left time: 555.8763s\n",
      "Epoch: 4 running time: 1.5366331458091735 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2589726 Vali Loss: 0.3914911 Test Loss: 0.6960634\n",
      "Validation loss decreased (0.394459 --> 0.391491).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2741575\n",
      "\tspeed: 0.2861s/iter; left time: 1586.9365s\n",
      "\titers: 200, epoch: 5 | loss: 0.2336923\n",
      "\tspeed: 0.0975s/iter; left time: 531.2220s\n",
      "\titers: 300, epoch: 5 | loss: 0.2015684\n",
      "\tspeed: 0.0974s/iter; left time: 521.0125s\n",
      "\titers: 400, epoch: 5 | loss: 0.2283741\n",
      "\tspeed: 0.0975s/iter; left time: 511.6482s\n",
      "\titers: 500, epoch: 5 | loss: 0.2496907\n",
      "\tspeed: 0.0976s/iter; left time: 502.3256s\n",
      "\titers: 600, epoch: 5 | loss: 0.2165726\n",
      "\tspeed: 0.0978s/iter; left time: 493.7778s\n",
      "\titers: 700, epoch: 5 | loss: 0.2022953\n",
      "\tspeed: 0.0978s/iter; left time: 483.6539s\n",
      "\titers: 800, epoch: 5 | loss: 0.2490409\n",
      "\tspeed: 0.0978s/iter; left time: 474.0028s\n",
      "\titers: 900, epoch: 5 | loss: 0.2817516\n",
      "\tspeed: 0.0975s/iter; left time: 462.9715s\n",
      "Epoch: 5 running time: 1.5363385915756225 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2380383 Vali Loss: 0.3972078 Test Loss: 0.7027150\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1886821\n",
      "\tspeed: 0.2781s/iter; left time: 1281.1298s\n",
      "\titers: 200, epoch: 6 | loss: 0.2433284\n",
      "\tspeed: 0.0976s/iter; left time: 439.8831s\n",
      "\titers: 300, epoch: 6 | loss: 0.2376238\n",
      "\tspeed: 0.0975s/iter; left time: 429.6387s\n",
      "\titers: 400, epoch: 6 | loss: 0.2259449\n",
      "\tspeed: 0.0977s/iter; left time: 420.5388s\n",
      "\titers: 500, epoch: 6 | loss: 0.2131033\n",
      "\tspeed: 0.0979s/iter; left time: 411.7965s\n",
      "\titers: 600, epoch: 6 | loss: 0.2139467\n",
      "\tspeed: 0.0979s/iter; left time: 401.8017s\n",
      "\titers: 700, epoch: 6 | loss: 0.2462851\n",
      "\tspeed: 0.0978s/iter; left time: 391.9464s\n",
      "\titers: 800, epoch: 6 | loss: 0.2430781\n",
      "\tspeed: 0.0980s/iter; left time: 382.7134s\n",
      "\titers: 900, epoch: 6 | loss: 0.1916809\n",
      "\tspeed: 0.0979s/iter; left time: 372.7594s\n",
      "Epoch: 6 running time: 1.5383511741956075 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2370810 Vali Loss: 0.4012877 Test Loss: 0.7130616\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2536033\n",
      "\tspeed: 0.2795s/iter; left time: 1024.3367s\n",
      "\titers: 200, epoch: 7 | loss: 0.2663393\n",
      "\tspeed: 0.0976s/iter; left time: 347.8432s\n",
      "\titers: 300, epoch: 7 | loss: 0.2347079\n",
      "\tspeed: 0.0978s/iter; left time: 338.7855s\n",
      "\titers: 400, epoch: 7 | loss: 0.2837873\n",
      "\tspeed: 0.0977s/iter; left time: 328.8627s\n",
      "\titers: 500, epoch: 7 | loss: 0.1881218\n",
      "\tspeed: 0.0978s/iter; left time: 319.2112s\n",
      "\titers: 600, epoch: 7 | loss: 0.2129756\n",
      "\tspeed: 0.0976s/iter; left time: 308.9672s\n",
      "\titers: 700, epoch: 7 | loss: 0.2155947\n",
      "\tspeed: 0.0975s/iter; left time: 298.7364s\n",
      "\titers: 800, epoch: 7 | loss: 0.2029046\n",
      "\tspeed: 0.0974s/iter; left time: 288.8096s\n",
      "\titers: 900, epoch: 7 | loss: 0.2018593\n",
      "\tspeed: 0.0977s/iter; left time: 279.7914s\n",
      "Epoch: 7 running time: 1.5372508883476257 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2374023 Vali Loss: 0.3983790 Test Loss: 0.7232236\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.7234761118888855, mae:0.5841315388679504\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5943394\n",
      "\tspeed: 0.1022s/iter; left time: 951.5721s\n",
      "\titers: 200, epoch: 1 | loss: 0.4951172\n",
      "\tspeed: 0.0989s/iter; left time: 911.3668s\n",
      "\titers: 300, epoch: 1 | loss: 0.4118982\n",
      "\tspeed: 0.0991s/iter; left time: 902.8581s\n",
      "\titers: 400, epoch: 1 | loss: 0.3952385\n",
      "\tspeed: 0.0993s/iter; left time: 894.4173s\n",
      "\titers: 500, epoch: 1 | loss: 0.3432862\n",
      "\tspeed: 0.0991s/iter; left time: 883.3927s\n",
      "\titers: 600, epoch: 1 | loss: 0.3879800\n",
      "\tspeed: 0.0986s/iter; left time: 868.4864s\n",
      "\titers: 700, epoch: 1 | loss: 0.5058795\n",
      "\tspeed: 0.0994s/iter; left time: 866.2208s\n",
      "\titers: 800, epoch: 1 | loss: 0.2738970\n",
      "\tspeed: 0.0992s/iter; left time: 854.0643s\n",
      "\titers: 900, epoch: 1 | loss: 0.2377400\n",
      "\tspeed: 0.0987s/iter; left time: 839.8687s\n",
      "Epoch: 1 running time: 1.560698135693868 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4166347 Vali Loss: 0.4075450 Test Loss: 0.6658818\n",
      "Validation loss decreased (inf --> 0.407545).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2869742\n",
      "\tspeed: 0.2890s/iter; left time: 2418.7386s\n",
      "\titers: 200, epoch: 2 | loss: 0.2624056\n",
      "\tspeed: 0.0983s/iter; left time: 812.5547s\n",
      "\titers: 300, epoch: 2 | loss: 0.3053213\n",
      "\tspeed: 0.0981s/iter; left time: 801.7064s\n",
      "\titers: 400, epoch: 2 | loss: 0.2667679\n",
      "\tspeed: 0.0976s/iter; left time: 787.6466s\n",
      "\titers: 500, epoch: 2 | loss: 0.3287040\n",
      "\tspeed: 0.0979s/iter; left time: 780.6480s\n",
      "\titers: 600, epoch: 2 | loss: 0.3694022\n",
      "\tspeed: 0.0978s/iter; left time: 769.3081s\n",
      "\titers: 700, epoch: 2 | loss: 0.2779715\n",
      "\tspeed: 0.0978s/iter; left time: 759.9436s\n",
      "\titers: 800, epoch: 2 | loss: 0.2777001\n",
      "\tspeed: 0.0978s/iter; left time: 749.8622s\n",
      "\titers: 900, epoch: 2 | loss: 0.2957426\n",
      "\tspeed: 0.0981s/iter; left time: 742.3073s\n",
      "Epoch: 2 running time: 1.5421652674674988 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3094313 Vali Loss: 0.4175729 Test Loss: 0.6661352\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2607539\n",
      "\tspeed: 0.2653s/iter; left time: 1971.0595s\n",
      "\titers: 200, epoch: 3 | loss: 0.3359609\n",
      "\tspeed: 0.0898s/iter; left time: 658.3817s\n",
      "\titers: 300, epoch: 3 | loss: 0.3067381\n",
      "\tspeed: 0.0984s/iter; left time: 711.6286s\n",
      "\titers: 400, epoch: 3 | loss: 0.3878072\n",
      "\tspeed: 0.0981s/iter; left time: 699.5963s\n",
      "\titers: 500, epoch: 3 | loss: 0.2739624\n",
      "\tspeed: 0.0984s/iter; left time: 691.4417s\n",
      "\titers: 600, epoch: 3 | loss: 0.2888962\n",
      "\tspeed: 0.0910s/iter; left time: 630.6627s\n",
      "\titers: 700, epoch: 3 | loss: 0.3805921\n",
      "\tspeed: 0.0809s/iter; left time: 552.2846s\n",
      "\titers: 800, epoch: 3 | loss: 0.2942919\n",
      "\tspeed: 0.0810s/iter; left time: 544.9244s\n",
      "\titers: 900, epoch: 3 | loss: 0.2940906\n",
      "\tspeed: 0.0967s/iter; left time: 640.8422s\n",
      "Epoch: 3 running time: 1.4228740453720092 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.3010982 Vali Loss: 0.4082417 Test Loss: 0.7074066\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3284783\n",
      "\tspeed: 0.2752s/iter; left time: 1785.5537s\n",
      "\titers: 200, epoch: 4 | loss: 0.2637954\n",
      "\tspeed: 0.0978s/iter; left time: 624.7798s\n",
      "\titers: 300, epoch: 4 | loss: 0.3258612\n",
      "\tspeed: 0.0978s/iter; left time: 615.0151s\n",
      "\titers: 400, epoch: 4 | loss: 0.2646027\n",
      "\tspeed: 0.0982s/iter; left time: 607.9364s\n",
      "\titers: 500, epoch: 4 | loss: 0.3703732\n",
      "\tspeed: 0.0977s/iter; left time: 594.5776s\n",
      "\titers: 600, epoch: 4 | loss: 0.3313794\n",
      "\tspeed: 0.0978s/iter; left time: 585.5359s\n",
      "\titers: 700, epoch: 4 | loss: 0.3470367\n",
      "\tspeed: 0.0979s/iter; left time: 576.4887s\n",
      "\titers: 800, epoch: 4 | loss: 0.2665585\n",
      "\tspeed: 0.0980s/iter; left time: 567.3158s\n",
      "\titers: 900, epoch: 4 | loss: 0.2471944\n",
      "\tspeed: 0.0983s/iter; left time: 558.8526s\n",
      "Epoch: 4 running time: 1.542477059364319 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2977920 Vali Loss: 0.3883834 Test Loss: 0.6649781\n",
      "Validation loss decreased (0.407545 --> 0.388383).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2147296\n",
      "\tspeed: 0.2881s/iter; left time: 1597.8585s\n",
      "\titers: 200, epoch: 5 | loss: 0.2421745\n",
      "\tspeed: 0.0985s/iter; left time: 536.2651s\n",
      "\titers: 300, epoch: 5 | loss: 0.2301781\n",
      "\tspeed: 0.0986s/iter; left time: 527.1485s\n",
      "\titers: 400, epoch: 5 | loss: 0.2338233\n",
      "\tspeed: 0.0987s/iter; left time: 518.0345s\n",
      "\titers: 500, epoch: 5 | loss: 0.2931637\n",
      "\tspeed: 0.0985s/iter; left time: 506.8242s\n",
      "\titers: 600, epoch: 5 | loss: 0.3693731\n",
      "\tspeed: 0.0987s/iter; left time: 497.9164s\n",
      "\titers: 700, epoch: 5 | loss: 0.2314854\n",
      "\tspeed: 0.0984s/iter; left time: 486.5726s\n",
      "\titers: 800, epoch: 5 | loss: 0.3689868\n",
      "\tspeed: 0.0985s/iter; left time: 477.3464s\n",
      "\titers: 900, epoch: 5 | loss: 0.2607145\n",
      "\tspeed: 0.0982s/iter; left time: 466.3733s\n",
      "Epoch: 5 running time: 1.551684522628784 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2765266 Vali Loss: 0.3792059 Test Loss: 0.6425338\n",
      "Validation loss decreased (0.388383 --> 0.379206).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1965883\n",
      "\tspeed: 0.2883s/iter; left time: 1327.8715s\n",
      "\titers: 200, epoch: 6 | loss: 0.2588012\n",
      "\tspeed: 0.0979s/iter; left time: 441.2202s\n",
      "\titers: 300, epoch: 6 | loss: 0.2251538\n",
      "\tspeed: 0.0978s/iter; left time: 430.9883s\n",
      "\titers: 400, epoch: 6 | loss: 0.2345210\n",
      "\tspeed: 0.0982s/iter; left time: 422.9801s\n",
      "\titers: 500, epoch: 6 | loss: 0.2129091\n",
      "\tspeed: 0.0981s/iter; left time: 412.5801s\n",
      "\titers: 600, epoch: 6 | loss: 0.2210598\n",
      "\tspeed: 0.0981s/iter; left time: 402.7707s\n",
      "\titers: 700, epoch: 6 | loss: 0.2509040\n",
      "\tspeed: 0.0980s/iter; left time: 392.6660s\n",
      "\titers: 800, epoch: 6 | loss: 0.2515513\n",
      "\tspeed: 0.0980s/iter; left time: 382.8037s\n",
      "\titers: 900, epoch: 6 | loss: 0.3001803\n",
      "\tspeed: 0.0984s/iter; left time: 374.4913s\n",
      "Epoch: 6 running time: 1.5439664721488953 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2621217 Vali Loss: 0.3838913 Test Loss: 0.6670998\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2747995\n",
      "\tspeed: 0.2812s/iter; left time: 1030.4218s\n",
      "\titers: 200, epoch: 7 | loss: 0.3177167\n",
      "\tspeed: 0.0982s/iter; left time: 350.1239s\n",
      "\titers: 300, epoch: 7 | loss: 0.2049908\n",
      "\tspeed: 0.0982s/iter; left time: 340.3733s\n",
      "\titers: 400, epoch: 7 | loss: 0.2936099\n",
      "\tspeed: 0.0983s/iter; left time: 330.9056s\n",
      "\titers: 500, epoch: 7 | loss: 0.2532768\n",
      "\tspeed: 0.0980s/iter; left time: 320.1151s\n",
      "\titers: 600, epoch: 7 | loss: 0.2556626\n",
      "\tspeed: 0.0981s/iter; left time: 310.4543s\n",
      "\titers: 700, epoch: 7 | loss: 0.2591571\n",
      "\tspeed: 0.0984s/iter; left time: 301.6442s\n",
      "\titers: 800, epoch: 7 | loss: 0.2360556\n",
      "\tspeed: 0.0983s/iter; left time: 291.3749s\n",
      "\titers: 900, epoch: 7 | loss: 0.2521653\n",
      "\tspeed: 0.0984s/iter; left time: 281.9666s\n",
      "Epoch: 7 running time: 1.5469970226287841 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2620220 Vali Loss: 0.3817948 Test Loss: 0.6543524\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2943222\n",
      "\tspeed: 0.2819s/iter; left time: 767.9045s\n",
      "\titers: 200, epoch: 8 | loss: 0.2702481\n",
      "\tspeed: 0.0989s/iter; left time: 259.4857s\n",
      "\titers: 300, epoch: 8 | loss: 0.2870710\n",
      "\tspeed: 0.0987s/iter; left time: 248.9951s\n",
      "\titers: 400, epoch: 8 | loss: 0.2909333\n",
      "\tspeed: 0.0980s/iter; left time: 237.6063s\n",
      "\titers: 500, epoch: 8 | loss: 0.3105505\n",
      "\tspeed: 0.0983s/iter; left time: 228.3404s\n",
      "\titers: 600, epoch: 8 | loss: 0.2014252\n",
      "\tspeed: 0.0981s/iter; left time: 218.2501s\n",
      "\titers: 700, epoch: 8 | loss: 0.3274187\n",
      "\tspeed: 0.0982s/iter; left time: 208.5026s\n",
      "\titers: 800, epoch: 8 | loss: 0.2913635\n",
      "\tspeed: 0.0980s/iter; left time: 198.2944s\n",
      "\titers: 900, epoch: 8 | loss: 0.2628719\n",
      "\tspeed: 0.0978s/iter; left time: 188.2189s\n",
      "Epoch: 8 running time: 1.5476595918337503 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.2621921 Vali Loss: 0.3789827 Test Loss: 0.6548730\n",
      "Validation loss decreased (0.379206 --> 0.378983).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2330881\n",
      "\tspeed: 0.2707s/iter; left time: 482.5948s\n",
      "\titers: 200, epoch: 9 | loss: 0.3100449\n",
      "\tspeed: 0.0926s/iter; left time: 155.8765s\n",
      "\titers: 300, epoch: 9 | loss: 0.3230477\n",
      "\tspeed: 0.0983s/iter; left time: 155.5689s\n",
      "\titers: 400, epoch: 9 | loss: 0.2439216\n",
      "\tspeed: 0.0983s/iter; left time: 145.7318s\n",
      "\titers: 500, epoch: 9 | loss: 0.1571486\n",
      "\tspeed: 0.0983s/iter; left time: 136.0101s\n",
      "\titers: 600, epoch: 9 | loss: 0.2126698\n",
      "\tspeed: 0.0982s/iter; left time: 126.0013s\n",
      "\titers: 700, epoch: 9 | loss: 0.2150868\n",
      "\tspeed: 0.0981s/iter; left time: 116.0861s\n",
      "\titers: 800, epoch: 9 | loss: 0.2607985\n",
      "\tspeed: 0.0882s/iter; left time: 95.4774s\n",
      "\titers: 900, epoch: 9 | loss: 0.2161590\n",
      "\tspeed: 0.0812s/iter; left time: 79.8587s\n",
      "Epoch: 9 running time: 1.4526338895161948 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.2588452 Vali Loss: 0.3799818 Test Loss: 0.6566387\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.3090481\n",
      "\tspeed: 0.2756s/iter; left time: 232.0299s\n",
      "\titers: 200, epoch: 10 | loss: 0.2841766\n",
      "\tspeed: 0.0981s/iter; left time: 72.8058s\n",
      "\titers: 300, epoch: 10 | loss: 0.2535132\n",
      "\tspeed: 0.0982s/iter; left time: 63.0232s\n",
      "\titers: 400, epoch: 10 | loss: 0.2925140\n",
      "\tspeed: 0.0981s/iter; left time: 53.1847s\n",
      "\titers: 500, epoch: 10 | loss: 0.2265480\n",
      "\tspeed: 0.0983s/iter; left time: 43.4432s\n",
      "\titers: 600, epoch: 10 | loss: 0.2891370\n",
      "\tspeed: 0.0983s/iter; left time: 33.6099s\n",
      "\titers: 700, epoch: 10 | loss: 0.2668970\n",
      "\tspeed: 0.0982s/iter; left time: 23.7603s\n",
      "\titers: 800, epoch: 10 | loss: 0.2983802\n",
      "\tspeed: 0.0982s/iter; left time: 13.9401s\n",
      "\titers: 900, epoch: 10 | loss: 0.2642350\n",
      "\tspeed: 0.0982s/iter; left time: 4.1245s\n",
      "Epoch: 10 running time: 1.5469300587972006 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.2590863 Vali Loss: 0.3810705 Test Loss: 0.6588838\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.6564114093780518, mae:0.5621141195297241\n",
      "\n",
      "Time intermediate for GB dataset: 30.468563119570415 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4183913\n",
      "\tspeed: 0.1072s/iter; left time: 997.7440s\n",
      "\titers: 200, epoch: 1 | loss: 0.3176747\n",
      "\tspeed: 0.0978s/iter; left time: 900.8986s\n",
      "\titers: 300, epoch: 1 | loss: 0.2389105\n",
      "\tspeed: 0.0982s/iter; left time: 894.4722s\n",
      "\titers: 400, epoch: 1 | loss: 0.2031928\n",
      "\tspeed: 0.0982s/iter; left time: 885.2418s\n",
      "\titers: 500, epoch: 1 | loss: 0.3285361\n",
      "\tspeed: 0.0981s/iter; left time: 874.1403s\n",
      "\titers: 600, epoch: 1 | loss: 0.2553721\n",
      "\tspeed: 0.0977s/iter; left time: 860.4383s\n",
      "\titers: 700, epoch: 1 | loss: 0.1943906\n",
      "\tspeed: 0.0980s/iter; left time: 854.0919s\n",
      "\titers: 800, epoch: 1 | loss: 0.1912926\n",
      "\tspeed: 0.0978s/iter; left time: 842.5617s\n",
      "\titers: 900, epoch: 1 | loss: 0.2323324\n",
      "\tspeed: 0.0977s/iter; left time: 831.6367s\n",
      "Epoch: 1 running time: 1.5527711788813272 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3024487 Vali Loss: 0.1919765 Test Loss: 0.3718269\n",
      "Validation loss decreased (inf --> 0.191977).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2539674\n",
      "\tspeed: 0.2869s/iter; left time: 2401.2293s\n",
      "\titers: 200, epoch: 2 | loss: 0.2502235\n",
      "\tspeed: 0.0978s/iter; left time: 808.7327s\n",
      "\titers: 300, epoch: 2 | loss: 0.1820616\n",
      "\tspeed: 0.0978s/iter; left time: 799.0027s\n",
      "\titers: 400, epoch: 2 | loss: 0.1761179\n",
      "\tspeed: 0.0978s/iter; left time: 789.2069s\n",
      "\titers: 500, epoch: 2 | loss: 0.2026360\n",
      "\tspeed: 0.0985s/iter; left time: 784.7859s\n",
      "\titers: 600, epoch: 2 | loss: 0.1865350\n",
      "\tspeed: 0.0982s/iter; left time: 772.8608s\n",
      "\titers: 700, epoch: 2 | loss: 0.1284744\n",
      "\tspeed: 0.0983s/iter; left time: 763.4893s\n",
      "\titers: 800, epoch: 2 | loss: 0.1330113\n",
      "\tspeed: 0.0979s/iter; left time: 750.7078s\n",
      "\titers: 900, epoch: 2 | loss: 0.1531144\n",
      "\tspeed: 0.0978s/iter; left time: 740.2682s\n",
      "Epoch: 2 running time: 1.5425472497940063 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1868798 Vali Loss: 0.1877339 Test Loss: 0.4226499\n",
      "Validation loss decreased (0.191977 --> 0.187734).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1913545\n",
      "\tspeed: 0.2875s/iter; left time: 2135.7922s\n",
      "\titers: 200, epoch: 3 | loss: 0.1383831\n",
      "\tspeed: 0.0984s/iter; left time: 721.3151s\n",
      "\titers: 300, epoch: 3 | loss: 0.1247306\n",
      "\tspeed: 0.0984s/iter; left time: 711.3806s\n",
      "\titers: 400, epoch: 3 | loss: 0.1372115\n",
      "\tspeed: 0.0978s/iter; left time: 697.1526s\n",
      "\titers: 500, epoch: 3 | loss: 0.1255482\n",
      "\tspeed: 0.0976s/iter; left time: 685.7565s\n",
      "\titers: 600, epoch: 3 | loss: 0.1160520\n",
      "\tspeed: 0.0976s/iter; left time: 676.5837s\n",
      "\titers: 700, epoch: 3 | loss: 0.1767505\n",
      "\tspeed: 0.0976s/iter; left time: 666.4980s\n",
      "\titers: 800, epoch: 3 | loss: 0.1513874\n",
      "\tspeed: 0.0976s/iter; left time: 656.8225s\n",
      "\titers: 900, epoch: 3 | loss: 0.1642675\n",
      "\tspeed: 0.0976s/iter; left time: 647.1322s\n",
      "Epoch: 3 running time: 1.5413131872812906 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1582189 Vali Loss: 0.1863528 Test Loss: 0.4277900\n",
      "Validation loss decreased (0.187734 --> 0.186353).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1476804\n",
      "\tspeed: 0.2858s/iter; left time: 1854.4157s\n",
      "\titers: 200, epoch: 4 | loss: 0.1471415\n",
      "\tspeed: 0.0978s/iter; left time: 624.6770s\n",
      "\titers: 300, epoch: 4 | loss: 0.1413845\n",
      "\tspeed: 0.0978s/iter; left time: 614.6736s\n",
      "\titers: 400, epoch: 4 | loss: 0.1490542\n",
      "\tspeed: 0.0981s/iter; left time: 606.8956s\n",
      "\titers: 500, epoch: 4 | loss: 0.1359551\n",
      "\tspeed: 0.0983s/iter; left time: 598.2066s\n",
      "\titers: 600, epoch: 4 | loss: 0.1420367\n",
      "\tspeed: 0.0976s/iter; left time: 584.6019s\n",
      "\titers: 700, epoch: 4 | loss: 0.1456454\n",
      "\tspeed: 0.0975s/iter; left time: 574.0613s\n",
      "\titers: 800, epoch: 4 | loss: 0.1749017\n",
      "\tspeed: 0.0975s/iter; left time: 564.4226s\n",
      "\titers: 900, epoch: 4 | loss: 0.1734580\n",
      "\tspeed: 0.0978s/iter; left time: 556.1529s\n",
      "Epoch: 4 running time: 1.539186914761861 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1428258 Vali Loss: 0.1726202 Test Loss: 0.3963655\n",
      "Validation loss decreased (0.186353 --> 0.172620).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1538869\n",
      "\tspeed: 0.2862s/iter; left time: 1587.6994s\n",
      "\titers: 200, epoch: 5 | loss: 0.1421568\n",
      "\tspeed: 0.0976s/iter; left time: 531.4726s\n",
      "\titers: 300, epoch: 5 | loss: 0.1535484\n",
      "\tspeed: 0.0976s/iter; left time: 521.8218s\n",
      "\titers: 400, epoch: 5 | loss: 0.0991839\n",
      "\tspeed: 0.0974s/iter; left time: 510.9178s\n",
      "\titers: 500, epoch: 5 | loss: 0.1206037\n",
      "\tspeed: 0.0976s/iter; left time: 502.4609s\n",
      "\titers: 600, epoch: 5 | loss: 0.1114162\n",
      "\tspeed: 0.0975s/iter; left time: 491.8379s\n",
      "\titers: 700, epoch: 5 | loss: 0.1209668\n",
      "\tspeed: 0.0974s/iter; left time: 481.6596s\n",
      "\titers: 800, epoch: 5 | loss: 0.1229157\n",
      "\tspeed: 0.0975s/iter; left time: 472.7072s\n",
      "\titers: 900, epoch: 5 | loss: 0.1002003\n",
      "\tspeed: 0.0981s/iter; left time: 465.8038s\n",
      "Epoch: 5 running time: 1.537100859483083 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1325181 Vali Loss: 0.1755163 Test Loss: 0.4312892\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1051527\n",
      "\tspeed: 0.2787s/iter; left time: 1283.5629s\n",
      "\titers: 200, epoch: 6 | loss: 0.1434750\n",
      "\tspeed: 0.0975s/iter; left time: 439.2067s\n",
      "\titers: 300, epoch: 6 | loss: 0.1429069\n",
      "\tspeed: 0.0975s/iter; left time: 429.7380s\n",
      "\titers: 400, epoch: 6 | loss: 0.1395658\n",
      "\tspeed: 0.0973s/iter; left time: 419.0939s\n",
      "\titers: 500, epoch: 6 | loss: 0.1126466\n",
      "\tspeed: 0.0975s/iter; left time: 410.1389s\n",
      "\titers: 600, epoch: 6 | loss: 0.0956490\n",
      "\tspeed: 0.0976s/iter; left time: 400.6408s\n",
      "\titers: 700, epoch: 6 | loss: 0.1260535\n",
      "\tspeed: 0.0975s/iter; left time: 390.5135s\n",
      "\titers: 800, epoch: 6 | loss: 0.1255859\n",
      "\tspeed: 0.0972s/iter; left time: 379.5172s\n",
      "\titers: 900, epoch: 6 | loss: 0.1606035\n",
      "\tspeed: 0.0974s/iter; left time: 370.5261s\n",
      "Epoch: 6 running time: 1.5332700729370117 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1315052 Vali Loss: 0.1704435 Test Loss: 0.4157898\n",
      "Validation loss decreased (0.172620 --> 0.170444).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1184536\n",
      "\tspeed: 0.2840s/iter; left time: 1040.9181s\n",
      "\titers: 200, epoch: 7 | loss: 0.1058492\n",
      "\tspeed: 0.0980s/iter; left time: 349.2116s\n",
      "\titers: 300, epoch: 7 | loss: 0.1113770\n",
      "\tspeed: 0.0981s/iter; left time: 339.7681s\n",
      "\titers: 400, epoch: 7 | loss: 0.1360071\n",
      "\tspeed: 0.0978s/iter; left time: 328.9867s\n",
      "\titers: 500, epoch: 7 | loss: 0.1233190\n",
      "\tspeed: 0.0977s/iter; left time: 318.8432s\n",
      "\titers: 600, epoch: 7 | loss: 0.1102490\n",
      "\tspeed: 0.0975s/iter; left time: 308.5809s\n",
      "\titers: 700, epoch: 7 | loss: 0.1098698\n",
      "\tspeed: 0.0976s/iter; left time: 299.1463s\n",
      "\titers: 800, epoch: 7 | loss: 0.0954850\n",
      "\tspeed: 0.0976s/iter; left time: 289.5034s\n",
      "\titers: 900, epoch: 7 | loss: 0.1111772\n",
      "\tspeed: 0.0977s/iter; left time: 279.8117s\n",
      "Epoch: 7 running time: 1.5388555566469828 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1274667 Vali Loss: 0.1747580 Test Loss: 0.4195948\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1401287\n",
      "\tspeed: 0.2791s/iter; left time: 760.1574s\n",
      "\titers: 200, epoch: 8 | loss: 0.1006209\n",
      "\tspeed: 0.0980s/iter; left time: 257.1423s\n",
      "\titers: 300, epoch: 8 | loss: 0.1409823\n",
      "\tspeed: 0.0980s/iter; left time: 247.3329s\n",
      "\titers: 400, epoch: 8 | loss: 0.1357944\n",
      "\tspeed: 0.0980s/iter; left time: 237.6313s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975027\n",
      "\tspeed: 0.0982s/iter; left time: 228.1614s\n",
      "\titers: 600, epoch: 8 | loss: 0.1578860\n",
      "\tspeed: 0.0977s/iter; left time: 217.2455s\n",
      "\titers: 700, epoch: 8 | loss: 0.1083857\n",
      "\tspeed: 0.0976s/iter; left time: 207.3591s\n",
      "\titers: 800, epoch: 8 | loss: 0.1733156\n",
      "\tspeed: 0.0977s/iter; left time: 197.7911s\n",
      "\titers: 900, epoch: 8 | loss: 0.1274520\n",
      "\tspeed: 0.0980s/iter; left time: 188.5411s\n",
      "Epoch: 8 running time: 1.5404897371927897 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1269825 Vali Loss: 0.1702255 Test Loss: 0.4147992\n",
      "Validation loss decreased (0.170444 --> 0.170226).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1236518\n",
      "\tspeed: 0.2837s/iter; left time: 505.8556s\n",
      "\titers: 200, epoch: 9 | loss: 0.1474202\n",
      "\tspeed: 0.0977s/iter; left time: 164.4516s\n",
      "\titers: 300, epoch: 9 | loss: 0.1326301\n",
      "\tspeed: 0.0977s/iter; left time: 154.7009s\n",
      "\titers: 400, epoch: 9 | loss: 0.1410056\n",
      "\tspeed: 0.0977s/iter; left time: 144.8837s\n",
      "\titers: 500, epoch: 9 | loss: 0.1886584\n",
      "\tspeed: 0.0977s/iter; left time: 135.1679s\n",
      "\titers: 600, epoch: 9 | loss: 0.1180006\n",
      "\tspeed: 0.0978s/iter; left time: 125.4362s\n",
      "\titers: 700, epoch: 9 | loss: 0.1217441\n",
      "\tspeed: 0.0978s/iter; left time: 115.6728s\n",
      "\titers: 800, epoch: 9 | loss: 0.1094521\n",
      "\tspeed: 0.0978s/iter; left time: 105.9416s\n",
      "\titers: 900, epoch: 9 | loss: 0.1150863\n",
      "\tspeed: 0.0977s/iter; left time: 96.0801s\n",
      "Epoch: 9 running time: 1.5383431275685628 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1260302 Vali Loss: 0.1710989 Test Loss: 0.4079327\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0950374\n",
      "\tspeed: 0.2780s/iter; left time: 234.0744s\n",
      "\titers: 200, epoch: 10 | loss: 0.1361429\n",
      "\tspeed: 0.0977s/iter; left time: 72.4586s\n",
      "\titers: 300, epoch: 10 | loss: 0.1148399\n",
      "\tspeed: 0.0975s/iter; left time: 62.5860s\n",
      "\titers: 400, epoch: 10 | loss: 0.1340534\n",
      "\tspeed: 0.0854s/iter; left time: 46.2878s\n",
      "\titers: 500, epoch: 10 | loss: 0.1267132\n",
      "\tspeed: 0.0809s/iter; left time: 35.7678s\n",
      "\titers: 600, epoch: 10 | loss: 0.1358077\n",
      "\tspeed: 0.0925s/iter; left time: 31.6383s\n",
      "\titers: 700, epoch: 10 | loss: 0.1198425\n",
      "\tspeed: 0.0810s/iter; left time: 19.6036s\n",
      "\titers: 800, epoch: 10 | loss: 0.2074506\n",
      "\tspeed: 0.0809s/iter; left time: 11.4930s\n",
      "\titers: 900, epoch: 10 | loss: 0.1105775\n",
      "\tspeed: 0.0809s/iter; left time: 3.3985s\n",
      "Epoch: 10 running time: 1.3840091983477274 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1258493 Vali Loss: 0.1716152 Test Loss: 0.4135497\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.4149816930294037, mae:0.4109726548194885\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.3302637\n",
      "\tspeed: 0.1006s/iter; left time: 936.4050s\n",
      "\titers: 200, epoch: 1 | loss: 0.3151070\n",
      "\tspeed: 0.0982s/iter; left time: 904.5968s\n",
      "\titers: 300, epoch: 1 | loss: 0.2623212\n",
      "\tspeed: 0.0982s/iter; left time: 894.3453s\n",
      "\titers: 400, epoch: 1 | loss: 0.2294310\n",
      "\tspeed: 0.0980s/iter; left time: 883.0982s\n",
      "\titers: 500, epoch: 1 | loss: 0.2054772\n",
      "\tspeed: 0.0980s/iter; left time: 873.2886s\n",
      "\titers: 600, epoch: 1 | loss: 0.2432472\n",
      "\tspeed: 0.0981s/iter; left time: 864.3074s\n",
      "\titers: 700, epoch: 1 | loss: 0.2112736\n",
      "\tspeed: 0.0981s/iter; left time: 854.8301s\n",
      "\titers: 800, epoch: 1 | loss: 0.2934595\n",
      "\tspeed: 0.0984s/iter; left time: 847.4383s\n",
      "\titers: 900, epoch: 1 | loss: 0.2331795\n",
      "\tspeed: 0.0982s/iter; left time: 835.8786s\n",
      "Epoch: 1 running time: 1.5455976327260335 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3063375 Vali Loss: 0.2056014 Test Loss: 0.3703719\n",
      "Validation loss decreased (inf --> 0.205601).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2075216\n",
      "\tspeed: 0.2866s/iter; left time: 2398.4506s\n",
      "\titers: 200, epoch: 2 | loss: 0.1935526\n",
      "\tspeed: 0.0977s/iter; left time: 807.7090s\n",
      "\titers: 300, epoch: 2 | loss: 0.1708436\n",
      "\tspeed: 0.0975s/iter; left time: 796.9012s\n",
      "\titers: 400, epoch: 2 | loss: 0.1765831\n",
      "\tspeed: 0.0976s/iter; left time: 787.6118s\n",
      "\titers: 500, epoch: 2 | loss: 0.2278909\n",
      "\tspeed: 0.0976s/iter; left time: 778.0649s\n",
      "\titers: 600, epoch: 2 | loss: 0.1893847\n",
      "\tspeed: 0.0977s/iter; left time: 768.6612s\n",
      "\titers: 700, epoch: 2 | loss: 0.1558357\n",
      "\tspeed: 0.0978s/iter; left time: 759.6271s\n",
      "\titers: 800, epoch: 2 | loss: 0.1449818\n",
      "\tspeed: 0.0978s/iter; left time: 749.8155s\n",
      "\titers: 900, epoch: 2 | loss: 0.1376279\n",
      "\tspeed: 0.0976s/iter; left time: 739.0294s\n",
      "Epoch: 2 running time: 1.5377209385236104 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1848781 Vali Loss: 0.1817326 Test Loss: 0.3549186\n",
      "Validation loss decreased (0.205601 --> 0.181733).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1394800\n",
      "\tspeed: 0.2872s/iter; left time: 2133.8337s\n",
      "\titers: 200, epoch: 3 | loss: 0.1271416\n",
      "\tspeed: 0.0979s/iter; left time: 717.8706s\n",
      "\titers: 300, epoch: 3 | loss: 0.1655629\n",
      "\tspeed: 0.0977s/iter; left time: 706.4635s\n",
      "\titers: 400, epoch: 3 | loss: 0.1368292\n",
      "\tspeed: 0.0980s/iter; left time: 698.5540s\n",
      "\titers: 500, epoch: 3 | loss: 0.1455152\n",
      "\tspeed: 0.0974s/iter; left time: 684.4711s\n",
      "\titers: 600, epoch: 3 | loss: 0.1904377\n",
      "\tspeed: 0.0975s/iter; left time: 675.7959s\n",
      "\titers: 700, epoch: 3 | loss: 0.2081547\n",
      "\tspeed: 0.0976s/iter; left time: 666.3756s\n",
      "\titers: 800, epoch: 3 | loss: 0.1410057\n",
      "\tspeed: 0.0976s/iter; left time: 656.6885s\n",
      "\titers: 900, epoch: 3 | loss: 0.1790302\n",
      "\tspeed: 0.0976s/iter; left time: 646.7100s\n",
      "Epoch: 3 running time: 1.5385660449663798 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1583684 Vali Loss: 0.1737744 Test Loss: 0.3850651\n",
      "Validation loss decreased (0.181733 --> 0.173774).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1106178\n",
      "\tspeed: 0.2862s/iter; left time: 1857.0656s\n",
      "\titers: 200, epoch: 4 | loss: 0.1032562\n",
      "\tspeed: 0.0980s/iter; left time: 626.1459s\n",
      "\titers: 300, epoch: 4 | loss: 0.1630179\n",
      "\tspeed: 0.0976s/iter; left time: 613.5937s\n",
      "\titers: 400, epoch: 4 | loss: 0.1381390\n",
      "\tspeed: 0.0977s/iter; left time: 604.7695s\n",
      "\titers: 500, epoch: 4 | loss: 0.1481630\n",
      "\tspeed: 0.0978s/iter; left time: 595.3543s\n",
      "\titers: 600, epoch: 4 | loss: 0.1578198\n",
      "\tspeed: 0.0977s/iter; left time: 584.7553s\n",
      "\titers: 700, epoch: 4 | loss: 0.1212454\n",
      "\tspeed: 0.0977s/iter; left time: 575.2874s\n",
      "\titers: 800, epoch: 4 | loss: 0.1387872\n",
      "\tspeed: 0.0976s/iter; left time: 565.0239s\n",
      "\titers: 900, epoch: 4 | loss: 0.0989111\n",
      "\tspeed: 0.0976s/iter; left time: 555.3796s\n",
      "Epoch: 4 running time: 1.5385752638181052 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1416687 Vali Loss: 0.1775134 Test Loss: 0.4505916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1332261\n",
      "\tspeed: 0.2798s/iter; left time: 1551.8333s\n",
      "\titers: 200, epoch: 5 | loss: 0.1178803\n",
      "\tspeed: 0.0977s/iter; left time: 532.1037s\n",
      "\titers: 300, epoch: 5 | loss: 0.1324545\n",
      "\tspeed: 0.0977s/iter; left time: 522.6032s\n",
      "\titers: 400, epoch: 5 | loss: 0.1655561\n",
      "\tspeed: 0.0980s/iter; left time: 514.4260s\n",
      "\titers: 500, epoch: 5 | loss: 0.1961126\n",
      "\tspeed: 0.0980s/iter; left time: 504.5302s\n",
      "\titers: 600, epoch: 5 | loss: 0.1407264\n",
      "\tspeed: 0.0981s/iter; left time: 494.8663s\n",
      "\titers: 700, epoch: 5 | loss: 0.1143082\n",
      "\tspeed: 0.0983s/iter; left time: 486.0738s\n",
      "\titers: 800, epoch: 5 | loss: 0.1553129\n",
      "\tspeed: 0.0982s/iter; left time: 475.8228s\n",
      "\titers: 900, epoch: 5 | loss: 0.1202482\n",
      "\tspeed: 0.0981s/iter; left time: 465.5710s\n",
      "Epoch: 5 running time: 1.534405815601349 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1385942 Vali Loss: 0.1720676 Test Loss: 0.4219375\n",
      "Validation loss decreased (0.173774 --> 0.172068).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1157490\n",
      "\tspeed: 0.2818s/iter; left time: 1298.0999s\n",
      "\titers: 200, epoch: 6 | loss: 0.1406169\n",
      "\tspeed: 0.0975s/iter; left time: 439.4475s\n",
      "\titers: 300, epoch: 6 | loss: 0.1372459\n",
      "\tspeed: 0.0975s/iter; left time: 429.3841s\n",
      "\titers: 400, epoch: 6 | loss: 0.1087425\n",
      "\tspeed: 0.0979s/iter; left time: 421.6321s\n",
      "\titers: 500, epoch: 6 | loss: 0.1378059\n",
      "\tspeed: 0.0979s/iter; left time: 411.8105s\n",
      "\titers: 600, epoch: 6 | loss: 0.1528922\n",
      "\tspeed: 0.0980s/iter; left time: 402.2005s\n",
      "\titers: 700, epoch: 6 | loss: 0.1253271\n",
      "\tspeed: 0.0981s/iter; left time: 392.8542s\n",
      "\titers: 800, epoch: 6 | loss: 0.1130694\n",
      "\tspeed: 0.0980s/iter; left time: 382.8259s\n",
      "\titers: 900, epoch: 6 | loss: 0.1053836\n",
      "\tspeed: 0.0977s/iter; left time: 371.7387s\n",
      "Epoch: 6 running time: 1.5394508878389994 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1311712 Vali Loss: 0.1755019 Test Loss: 0.4397382\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1171806\n",
      "\tspeed: 0.2819s/iter; left time: 1033.3229s\n",
      "\titers: 200, epoch: 7 | loss: 0.1571493\n",
      "\tspeed: 0.0986s/iter; left time: 351.6177s\n",
      "\titers: 300, epoch: 7 | loss: 0.1066693\n",
      "\tspeed: 0.0985s/iter; left time: 341.1681s\n",
      "\titers: 400, epoch: 7 | loss: 0.1229019\n",
      "\tspeed: 0.0985s/iter; left time: 331.5692s\n",
      "\titers: 500, epoch: 7 | loss: 0.1200267\n",
      "\tspeed: 0.0983s/iter; left time: 320.7892s\n",
      "\titers: 600, epoch: 7 | loss: 0.1354899\n",
      "\tspeed: 0.0977s/iter; left time: 309.3551s\n",
      "\titers: 700, epoch: 7 | loss: 0.1273839\n",
      "\tspeed: 0.0977s/iter; left time: 299.3756s\n",
      "\titers: 800, epoch: 7 | loss: 0.1198170\n",
      "\tspeed: 0.0977s/iter; left time: 289.8079s\n",
      "\titers: 900, epoch: 7 | loss: 0.1362952\n",
      "\tspeed: 0.0979s/iter; left time: 280.5158s\n",
      "Epoch: 7 running time: 1.5471929033597311 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1305958 Vali Loss: 0.1727828 Test Loss: 0.4353591\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1495225\n",
      "\tspeed: 0.2801s/iter; left time: 762.9073s\n",
      "\titers: 200, epoch: 8 | loss: 0.1441906\n",
      "\tspeed: 0.0978s/iter; left time: 256.5227s\n",
      "\titers: 300, epoch: 8 | loss: 0.1668217\n",
      "\tspeed: 0.0977s/iter; left time: 246.6713s\n",
      "\titers: 400, epoch: 8 | loss: 0.1325599\n",
      "\tspeed: 0.0977s/iter; left time: 236.7391s\n",
      "\titers: 500, epoch: 8 | loss: 0.1169434\n",
      "\tspeed: 0.0977s/iter; left time: 227.1326s\n",
      "\titers: 600, epoch: 8 | loss: 0.1312203\n",
      "\tspeed: 0.0973s/iter; left time: 216.3205s\n",
      "\titers: 700, epoch: 8 | loss: 0.1789077\n",
      "\tspeed: 0.0976s/iter; left time: 207.4015s\n",
      "\titers: 800, epoch: 8 | loss: 0.1233529\n",
      "\tspeed: 0.0979s/iter; left time: 198.1780s\n",
      "\titers: 900, epoch: 8 | loss: 0.1030344\n",
      "\tspeed: 0.0977s/iter; left time: 187.8853s\n",
      "Epoch: 8 running time: 1.5382991472880045 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1308838 Vali Loss: 0.1731599 Test Loss: 0.4241140\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.423806756734848, mae:0.4119410812854767\n",
      "\n",
      "Time intermediate for ES dataset: 32.3044050971667 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.3871442\n",
      "\tspeed: 0.1069s/iter; left time: 995.4311s\n",
      "\titers: 200, epoch: 1 | loss: 0.2673644\n",
      "\tspeed: 0.0890s/iter; left time: 819.9429s\n",
      "\titers: 300, epoch: 1 | loss: 0.2006789\n",
      "\tspeed: 0.0837s/iter; left time: 762.3904s\n",
      "\titers: 400, epoch: 1 | loss: 0.1515370\n",
      "\tspeed: 0.0981s/iter; left time: 883.9953s\n",
      "\titers: 500, epoch: 1 | loss: 0.2455160\n",
      "\tspeed: 0.0981s/iter; left time: 873.8359s\n",
      "\titers: 600, epoch: 1 | loss: 0.2780952\n",
      "\tspeed: 0.0978s/iter; left time: 861.7338s\n",
      "\titers: 700, epoch: 1 | loss: 0.2736951\n",
      "\tspeed: 0.0979s/iter; left time: 853.0327s\n",
      "\titers: 800, epoch: 1 | loss: 0.2647220\n",
      "\tspeed: 0.0979s/iter; left time: 842.8808s\n",
      "\titers: 900, epoch: 1 | loss: 0.1626280\n",
      "\tspeed: 0.0979s/iter; left time: 833.1405s\n",
      "Epoch: 1 running time: 1.5138357957204183 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2831952 Vali Loss: 0.2616880 Test Loss: 0.4787354\n",
      "Validation loss decreased (inf --> 0.261688).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1852047\n",
      "\tspeed: 0.2854s/iter; left time: 2389.1435s\n",
      "\titers: 200, epoch: 2 | loss: 0.1698483\n",
      "\tspeed: 0.0973s/iter; left time: 804.8258s\n",
      "\titers: 300, epoch: 2 | loss: 0.2079634\n",
      "\tspeed: 0.0974s/iter; left time: 795.3713s\n",
      "\titers: 400, epoch: 2 | loss: 0.1993060\n",
      "\tspeed: 0.0974s/iter; left time: 786.0162s\n",
      "\titers: 500, epoch: 2 | loss: 0.2143207\n",
      "\tspeed: 0.0974s/iter; left time: 776.5000s\n",
      "\titers: 600, epoch: 2 | loss: 0.1892886\n",
      "\tspeed: 0.0974s/iter; left time: 766.3956s\n",
      "\titers: 700, epoch: 2 | loss: 0.1499426\n",
      "\tspeed: 0.0974s/iter; left time: 756.9036s\n",
      "\titers: 800, epoch: 2 | loss: 0.1230438\n",
      "\tspeed: 0.0974s/iter; left time: 746.8013s\n",
      "\titers: 900, epoch: 2 | loss: 0.2164897\n",
      "\tspeed: 0.0974s/iter; left time: 737.3784s\n",
      "Epoch: 2 running time: 1.5335353374481202 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1828118 Vali Loss: 0.2568518 Test Loss: 0.4131241\n",
      "Validation loss decreased (0.261688 --> 0.256852).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1794982\n",
      "\tspeed: 0.2868s/iter; left time: 2130.9892s\n",
      "\titers: 200, epoch: 3 | loss: 0.1258878\n",
      "\tspeed: 0.0983s/iter; left time: 720.4389s\n",
      "\titers: 300, epoch: 3 | loss: 0.1473013\n",
      "\tspeed: 0.0980s/iter; left time: 708.3873s\n",
      "\titers: 400, epoch: 3 | loss: 0.1278618\n",
      "\tspeed: 0.0981s/iter; left time: 699.1794s\n",
      "\titers: 500, epoch: 3 | loss: 0.1277567\n",
      "\tspeed: 0.0981s/iter; left time: 689.2351s\n",
      "\titers: 600, epoch: 3 | loss: 0.1196522\n",
      "\tspeed: 0.0981s/iter; left time: 679.5763s\n",
      "\titers: 700, epoch: 3 | loss: 0.1161006\n",
      "\tspeed: 0.0978s/iter; left time: 667.7730s\n",
      "\titers: 800, epoch: 3 | loss: 0.2125789\n",
      "\tspeed: 0.0977s/iter; left time: 657.7221s\n",
      "\titers: 900, epoch: 3 | loss: 0.1116768\n",
      "\tspeed: 0.0978s/iter; left time: 648.4570s\n",
      "Epoch: 3 running time: 1.5419822017351785 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1522176 Vali Loss: 0.2416414 Test Loss: 0.3996641\n",
      "Validation loss decreased (0.256852 --> 0.241641).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1334140\n",
      "\tspeed: 0.2861s/iter; left time: 1856.2962s\n",
      "\titers: 200, epoch: 4 | loss: 0.1624571\n",
      "\tspeed: 0.0980s/iter; left time: 626.0158s\n",
      "\titers: 300, epoch: 4 | loss: 0.1250404\n",
      "\tspeed: 0.0979s/iter; left time: 615.6574s\n",
      "\titers: 400, epoch: 4 | loss: 0.1235920\n",
      "\tspeed: 0.0979s/iter; left time: 605.8320s\n",
      "\titers: 500, epoch: 4 | loss: 0.1108047\n",
      "\tspeed: 0.0980s/iter; left time: 596.4108s\n",
      "\titers: 600, epoch: 4 | loss: 0.1781396\n",
      "\tspeed: 0.0978s/iter; left time: 585.7715s\n",
      "\titers: 700, epoch: 4 | loss: 0.1164244\n",
      "\tspeed: 0.0979s/iter; left time: 576.5792s\n",
      "\titers: 800, epoch: 4 | loss: 0.1635794\n",
      "\tspeed: 0.0980s/iter; left time: 567.1916s\n",
      "\titers: 900, epoch: 4 | loss: 0.0831978\n",
      "\tspeed: 0.0953s/iter; left time: 542.1088s\n",
      "Epoch: 4 running time: 1.531578799088796 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1316964 Vali Loss: 0.2272363 Test Loss: 0.4058791\n",
      "Validation loss decreased (0.241641 --> 0.227236).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1401690\n",
      "\tspeed: 0.2901s/iter; left time: 1609.3889s\n",
      "\titers: 200, epoch: 5 | loss: 0.2141778\n",
      "\tspeed: 0.0982s/iter; left time: 534.6809s\n",
      "\titers: 300, epoch: 5 | loss: 0.0976685\n",
      "\tspeed: 0.0981s/iter; left time: 524.3869s\n",
      "\titers: 400, epoch: 5 | loss: 0.1276522\n",
      "\tspeed: 0.0980s/iter; left time: 514.1479s\n",
      "\titers: 500, epoch: 5 | loss: 0.0873264\n",
      "\tspeed: 0.0979s/iter; left time: 504.1038s\n",
      "\titers: 600, epoch: 5 | loss: 0.0939021\n",
      "\tspeed: 0.0982s/iter; left time: 495.5769s\n",
      "\titers: 700, epoch: 5 | loss: 0.1109263\n",
      "\tspeed: 0.0981s/iter; left time: 485.2249s\n",
      "\titers: 800, epoch: 5 | loss: 0.1041656\n",
      "\tspeed: 0.0978s/iter; left time: 473.8629s\n",
      "\titers: 900, epoch: 5 | loss: 0.1287480\n",
      "\tspeed: 0.0983s/iter; left time: 466.8228s\n",
      "Epoch: 5 running time: 1.5440135836601256 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1180394 Vali Loss: 0.2291849 Test Loss: 0.4162515\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0720225\n",
      "\tspeed: 0.2793s/iter; left time: 1286.4110s\n",
      "\titers: 200, epoch: 6 | loss: 0.1116268\n",
      "\tspeed: 0.0979s/iter; left time: 440.9537s\n",
      "\titers: 300, epoch: 6 | loss: 0.1110815\n",
      "\tspeed: 0.0938s/iter; left time: 413.1271s\n",
      "\titers: 400, epoch: 6 | loss: 0.0973093\n",
      "\tspeed: 0.0976s/iter; left time: 420.2059s\n",
      "\titers: 500, epoch: 6 | loss: 0.0811179\n",
      "\tspeed: 0.0977s/iter; left time: 411.0745s\n",
      "\titers: 600, epoch: 6 | loss: 0.1062097\n",
      "\tspeed: 0.0979s/iter; left time: 401.8598s\n",
      "\titers: 700, epoch: 6 | loss: 0.1420697\n",
      "\tspeed: 0.0981s/iter; left time: 392.8887s\n",
      "\titers: 800, epoch: 6 | loss: 0.1018597\n",
      "\tspeed: 0.0980s/iter; left time: 382.7114s\n",
      "\titers: 900, epoch: 6 | loss: 0.1084752\n",
      "\tspeed: 0.0978s/iter; left time: 372.1815s\n",
      "Epoch: 6 running time: 1.5343475143114726 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1172928 Vali Loss: 0.2305413 Test Loss: 0.4107639\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0814674\n",
      "\tspeed: 0.2799s/iter; left time: 1025.9080s\n",
      "\titers: 200, epoch: 7 | loss: 0.0873558\n",
      "\tspeed: 0.0980s/iter; left time: 349.4500s\n",
      "\titers: 300, epoch: 7 | loss: 0.0967285\n",
      "\tspeed: 0.0979s/iter; left time: 339.3697s\n",
      "\titers: 400, epoch: 7 | loss: 0.1214434\n",
      "\tspeed: 0.0976s/iter; left time: 328.4567s\n",
      "\titers: 500, epoch: 7 | loss: 0.1131551\n",
      "\tspeed: 0.0975s/iter; left time: 318.1933s\n",
      "\titers: 600, epoch: 7 | loss: 0.1573701\n",
      "\tspeed: 0.0980s/iter; left time: 310.0173s\n",
      "\titers: 700, epoch: 7 | loss: 0.1066053\n",
      "\tspeed: 0.0978s/iter; left time: 299.7031s\n",
      "\titers: 800, epoch: 7 | loss: 0.1340960\n",
      "\tspeed: 0.0977s/iter; left time: 289.7749s\n",
      "\titers: 900, epoch: 7 | loss: 0.1279623\n",
      "\tspeed: 0.0975s/iter; left time: 279.4537s\n",
      "Epoch: 7 running time: 1.5399282574653625 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1167402 Vali Loss: 0.2287699 Test Loss: 0.4122698\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.40965697169303894, mae:0.35750654339790344\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4186364\n",
      "\tspeed: 0.1010s/iter; left time: 940.0655s\n",
      "\titers: 200, epoch: 1 | loss: 0.3263134\n",
      "\tspeed: 0.0982s/iter; left time: 904.9285s\n",
      "\titers: 300, epoch: 1 | loss: 0.2293927\n",
      "\tspeed: 0.0981s/iter; left time: 894.0635s\n",
      "\titers: 400, epoch: 1 | loss: 0.1484141\n",
      "\tspeed: 0.0984s/iter; left time: 886.4984s\n",
      "\titers: 500, epoch: 1 | loss: 0.1641806\n",
      "\tspeed: 0.0984s/iter; left time: 876.7438s\n",
      "\titers: 600, epoch: 1 | loss: 0.1693609\n",
      "\tspeed: 0.0981s/iter; left time: 864.5007s\n",
      "\titers: 700, epoch: 1 | loss: 0.1987397\n",
      "\tspeed: 0.0986s/iter; left time: 858.7678s\n",
      "\titers: 800, epoch: 1 | loss: 0.1767388\n",
      "\tspeed: 0.0981s/iter; left time: 844.9249s\n",
      "\titers: 900, epoch: 1 | loss: 0.2046778\n",
      "\tspeed: 0.0982s/iter; left time: 835.8932s\n",
      "Epoch: 1 running time: 1.5474209944407145 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2850537 Vali Loss: 0.2336705 Test Loss: 0.4041315\n",
      "Validation loss decreased (inf --> 0.233671).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1910070\n",
      "\tspeed: 0.2886s/iter; left time: 2415.7236s\n",
      "\titers: 200, epoch: 2 | loss: 0.2027248\n",
      "\tspeed: 0.0978s/iter; left time: 809.1651s\n",
      "\titers: 300, epoch: 2 | loss: 0.1684840\n",
      "\tspeed: 0.0978s/iter; left time: 798.7959s\n",
      "\titers: 400, epoch: 2 | loss: 0.1411078\n",
      "\tspeed: 0.0977s/iter; left time: 788.2662s\n",
      "\titers: 500, epoch: 2 | loss: 0.1941990\n",
      "\tspeed: 0.0976s/iter; left time: 778.2225s\n",
      "\titers: 600, epoch: 2 | loss: 0.1813835\n",
      "\tspeed: 0.0978s/iter; left time: 769.3898s\n",
      "\titers: 700, epoch: 2 | loss: 0.1679413\n",
      "\tspeed: 0.0978s/iter; left time: 760.2324s\n",
      "\titers: 800, epoch: 2 | loss: 0.1534651\n",
      "\tspeed: 0.0977s/iter; left time: 749.5613s\n",
      "\titers: 900, epoch: 2 | loss: 0.2049218\n",
      "\tspeed: 0.0976s/iter; left time: 739.1731s\n",
      "Epoch: 2 running time: 1.5386431614557903 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1826428 Vali Loss: 0.2532248 Test Loss: 0.4527559\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2628095\n",
      "\tspeed: 0.2792s/iter; left time: 2074.0601s\n",
      "\titers: 200, epoch: 3 | loss: 0.2365825\n",
      "\tspeed: 0.0976s/iter; left time: 715.6503s\n",
      "\titers: 300, epoch: 3 | loss: 0.1906161\n",
      "\tspeed: 0.0979s/iter; left time: 707.6222s\n",
      "\titers: 400, epoch: 3 | loss: 0.1980529\n",
      "\tspeed: 0.0979s/iter; left time: 697.9331s\n",
      "\titers: 500, epoch: 3 | loss: 0.1365095\n",
      "\tspeed: 0.0980s/iter; left time: 688.8696s\n",
      "\titers: 600, epoch: 3 | loss: 0.1684517\n",
      "\tspeed: 0.0980s/iter; left time: 679.3249s\n",
      "\titers: 700, epoch: 3 | loss: 0.3296774\n",
      "\tspeed: 0.0982s/iter; left time: 670.5298s\n",
      "\titers: 800, epoch: 3 | loss: 0.1282694\n",
      "\tspeed: 0.0981s/iter; left time: 660.0982s\n",
      "\titers: 900, epoch: 3 | loss: 0.1457012\n",
      "\tspeed: 0.0981s/iter; left time: 650.3731s\n",
      "Epoch: 3 running time: 1.5414589126904805 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1762556 Vali Loss: 0.2430529 Test Loss: 0.3964816\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1606507\n",
      "\tspeed: 0.2662s/iter; left time: 1726.8307s\n",
      "\titers: 200, epoch: 4 | loss: 0.1244453\n",
      "\tspeed: 0.0978s/iter; left time: 624.9634s\n",
      "\titers: 300, epoch: 4 | loss: 0.2576856\n",
      "\tspeed: 0.0978s/iter; left time: 615.0768s\n",
      "\titers: 400, epoch: 4 | loss: 0.2086918\n",
      "\tspeed: 0.0978s/iter; left time: 605.3667s\n",
      "\titers: 500, epoch: 4 | loss: 0.1658135\n",
      "\tspeed: 0.0978s/iter; left time: 595.4880s\n",
      "\titers: 600, epoch: 4 | loss: 0.1352087\n",
      "\tspeed: 0.0976s/iter; left time: 584.5615s\n",
      "\titers: 700, epoch: 4 | loss: 0.1375851\n",
      "\tspeed: 0.0977s/iter; left time: 575.1545s\n",
      "\titers: 800, epoch: 4 | loss: 0.1459120\n",
      "\tspeed: 0.0977s/iter; left time: 565.6086s\n",
      "\titers: 900, epoch: 4 | loss: 0.1659870\n",
      "\tspeed: 0.0976s/iter; left time: 555.1267s\n",
      "Epoch: 4 running time: 1.5165399193763733 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1710265 Vali Loss: 0.2365344 Test Loss: 0.3860765\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.3855854570865631, mae:0.3636378049850464\n",
      "\n",
      "Time intermediate for FR dataset: 19.977133317788443 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4839092\n",
      "\tspeed: 0.1066s/iter; left time: 992.9850s\n",
      "\titers: 200, epoch: 1 | loss: 0.2879023\n",
      "\tspeed: 0.0973s/iter; left time: 896.5701s\n",
      "\titers: 300, epoch: 1 | loss: 0.1979710\n",
      "\tspeed: 0.0976s/iter; left time: 888.8603s\n",
      "\titers: 400, epoch: 1 | loss: 0.2898935\n",
      "\tspeed: 0.0974s/iter; left time: 878.1138s\n",
      "\titers: 500, epoch: 1 | loss: 0.2302311\n",
      "\tspeed: 0.0975s/iter; left time: 868.8615s\n",
      "\titers: 600, epoch: 1 | loss: 0.1686959\n",
      "\tspeed: 0.0975s/iter; left time: 858.6793s\n",
      "\titers: 700, epoch: 1 | loss: 0.1522209\n",
      "\tspeed: 0.0974s/iter; left time: 848.2631s\n",
      "\titers: 800, epoch: 1 | loss: 0.1437175\n",
      "\tspeed: 0.0976s/iter; left time: 840.4306s\n",
      "\titers: 900, epoch: 1 | loss: 0.1686486\n",
      "\tspeed: 0.0975s/iter; left time: 829.4021s\n",
      "Epoch: 1 running time: 1.5453044414520263 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2949341 Vali Loss: 0.1825299 Test Loss: 0.2419658\n",
      "Validation loss decreased (inf --> 0.182530).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2636757\n",
      "\tspeed: 0.2838s/iter; left time: 2375.5902s\n",
      "\titers: 200, epoch: 2 | loss: 0.1816243\n",
      "\tspeed: 0.0976s/iter; left time: 807.4244s\n",
      "\titers: 300, epoch: 2 | loss: 0.1623272\n",
      "\tspeed: 0.0975s/iter; left time: 796.4880s\n",
      "\titers: 400, epoch: 2 | loss: 0.1740752\n",
      "\tspeed: 0.0974s/iter; left time: 786.0349s\n",
      "\titers: 500, epoch: 2 | loss: 0.1692941\n",
      "\tspeed: 0.0976s/iter; left time: 778.0605s\n",
      "\titers: 600, epoch: 2 | loss: 0.1518331\n",
      "\tspeed: 0.0975s/iter; left time: 767.6252s\n",
      "\titers: 700, epoch: 2 | loss: 0.1923162\n",
      "\tspeed: 0.0974s/iter; left time: 756.9518s\n",
      "\titers: 800, epoch: 2 | loss: 0.1279732\n",
      "\tspeed: 0.0974s/iter; left time: 746.6892s\n",
      "\titers: 900, epoch: 2 | loss: 0.1602855\n",
      "\tspeed: 0.0973s/iter; left time: 736.8447s\n",
      "Epoch: 2 running time: 1.5341623226801555 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1825944 Vali Loss: 0.1623728 Test Loss: 0.2211493\n",
      "Validation loss decreased (0.182530 --> 0.162373).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2162754\n",
      "\tspeed: 0.2838s/iter; left time: 2108.5827s\n",
      "\titers: 200, epoch: 3 | loss: 0.1780417\n",
      "\tspeed: 0.0974s/iter; left time: 714.0807s\n",
      "\titers: 300, epoch: 3 | loss: 0.1844170\n",
      "\tspeed: 0.0974s/iter; left time: 704.1188s\n",
      "\titers: 400, epoch: 3 | loss: 0.2321048\n",
      "\tspeed: 0.0974s/iter; left time: 694.1767s\n",
      "\titers: 500, epoch: 3 | loss: 0.1619159\n",
      "\tspeed: 0.0974s/iter; left time: 684.3971s\n",
      "\titers: 600, epoch: 3 | loss: 0.2035481\n",
      "\tspeed: 0.0973s/iter; left time: 674.3872s\n",
      "\titers: 700, epoch: 3 | loss: 0.1415086\n",
      "\tspeed: 0.0974s/iter; left time: 665.2122s\n",
      "\titers: 800, epoch: 3 | loss: 0.1461832\n",
      "\tspeed: 0.0975s/iter; left time: 656.0179s\n",
      "\titers: 900, epoch: 3 | loss: 0.1757724\n",
      "\tspeed: 0.0975s/iter; left time: 646.6506s\n",
      "Epoch: 3 running time: 1.5335398634274802 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1551666 Vali Loss: 0.1509909 Test Loss: 0.2212254\n",
      "Validation loss decreased (0.162373 --> 0.150991).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1292468\n",
      "\tspeed: 0.2858s/iter; left time: 1854.2312s\n",
      "\titers: 200, epoch: 4 | loss: 0.1385312\n",
      "\tspeed: 0.0979s/iter; left time: 625.6877s\n",
      "\titers: 300, epoch: 4 | loss: 0.1528608\n",
      "\tspeed: 0.0977s/iter; left time: 614.1243s\n",
      "\titers: 400, epoch: 4 | loss: 0.1385029\n",
      "\tspeed: 0.0976s/iter; left time: 603.7465s\n",
      "\titers: 500, epoch: 4 | loss: 0.1575158\n",
      "\tspeed: 0.0976s/iter; left time: 594.1274s\n",
      "\titers: 600, epoch: 4 | loss: 0.1237735\n",
      "\tspeed: 0.0977s/iter; left time: 585.1954s\n",
      "\titers: 700, epoch: 4 | loss: 0.1452523\n",
      "\tspeed: 0.0978s/iter; left time: 575.6677s\n",
      "\titers: 800, epoch: 4 | loss: 0.1854530\n",
      "\tspeed: 0.0977s/iter; left time: 565.4985s\n",
      "\titers: 900, epoch: 4 | loss: 0.1564882\n",
      "\tspeed: 0.0976s/iter; left time: 555.1507s\n",
      "Epoch: 4 running time: 1.5377938310305277 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1399588 Vali Loss: 0.1468418 Test Loss: 0.2187929\n",
      "Validation loss decreased (0.150991 --> 0.146842).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1530052\n",
      "\tspeed: 0.2846s/iter; left time: 1578.4834s\n",
      "\titers: 200, epoch: 5 | loss: 0.1308514\n",
      "\tspeed: 0.0972s/iter; left time: 529.5762s\n",
      "\titers: 300, epoch: 5 | loss: 0.1466258\n",
      "\tspeed: 0.0973s/iter; left time: 520.4438s\n",
      "\titers: 400, epoch: 5 | loss: 0.1345804\n",
      "\tspeed: 0.0975s/iter; left time: 511.4472s\n",
      "\titers: 500, epoch: 5 | loss: 0.1079054\n",
      "\tspeed: 0.0974s/iter; left time: 501.4257s\n",
      "\titers: 600, epoch: 5 | loss: 0.1301066\n",
      "\tspeed: 0.0973s/iter; left time: 491.1329s\n",
      "\titers: 700, epoch: 5 | loss: 0.1477804\n",
      "\tspeed: 0.0972s/iter; left time: 481.0689s\n",
      "\titers: 800, epoch: 5 | loss: 0.1133912\n",
      "\tspeed: 0.0976s/iter; left time: 472.8656s\n",
      "\titers: 900, epoch: 5 | loss: 0.1090341\n",
      "\tspeed: 0.0977s/iter; left time: 463.8174s\n",
      "Epoch: 5 running time: 1.5345877250035604 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1297619 Vali Loss: 0.1458476 Test Loss: 0.2184652\n",
      "Validation loss decreased (0.146842 --> 0.145848).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1200157\n",
      "\tspeed: 0.2862s/iter; left time: 1318.1616s\n",
      "\titers: 200, epoch: 6 | loss: 0.1132532\n",
      "\tspeed: 0.0979s/iter; left time: 441.1880s\n",
      "\titers: 300, epoch: 6 | loss: 0.1247416\n",
      "\tspeed: 0.0975s/iter; left time: 429.7224s\n",
      "\titers: 400, epoch: 6 | loss: 0.1308755\n",
      "\tspeed: 0.0974s/iter; left time: 419.2142s\n",
      "\titers: 500, epoch: 6 | loss: 0.1172737\n",
      "\tspeed: 0.0976s/iter; left time: 410.4727s\n",
      "\titers: 600, epoch: 6 | loss: 0.1246800\n",
      "\tspeed: 0.0973s/iter; left time: 399.3930s\n",
      "\titers: 700, epoch: 6 | loss: 0.1159766\n",
      "\tspeed: 0.0976s/iter; left time: 390.8258s\n",
      "\titers: 800, epoch: 6 | loss: 0.1150436\n",
      "\tspeed: 0.0980s/iter; left time: 382.7629s\n",
      "\titers: 900, epoch: 6 | loss: 0.1527046\n",
      "\tspeed: 0.0977s/iter; left time: 371.9833s\n",
      "Epoch: 6 running time: 1.537830646832784 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1229240 Vali Loss: 0.1457663 Test Loss: 0.2212498\n",
      "Validation loss decreased (0.145848 --> 0.145766).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1354768\n",
      "\tspeed: 0.2848s/iter; left time: 1043.8545s\n",
      "\titers: 200, epoch: 7 | loss: 0.1053292\n",
      "\tspeed: 0.0971s/iter; left time: 346.3194s\n",
      "\titers: 300, epoch: 7 | loss: 0.1470087\n",
      "\tspeed: 0.0975s/iter; left time: 337.7739s\n",
      "\titers: 400, epoch: 7 | loss: 0.1179281\n",
      "\tspeed: 0.0974s/iter; left time: 327.8276s\n",
      "\titers: 500, epoch: 7 | loss: 0.1284748\n",
      "\tspeed: 0.0976s/iter; left time: 318.7656s\n",
      "\titers: 600, epoch: 7 | loss: 0.0976006\n",
      "\tspeed: 0.0974s/iter; left time: 308.2349s\n",
      "\titers: 700, epoch: 7 | loss: 0.1113579\n",
      "\tspeed: 0.0973s/iter; left time: 298.0942s\n",
      "\titers: 800, epoch: 7 | loss: 0.1444205\n",
      "\tspeed: 0.0976s/iter; left time: 289.2410s\n",
      "\titers: 900, epoch: 7 | loss: 0.1121703\n",
      "\tspeed: 0.0977s/iter; left time: 279.9088s\n",
      "Epoch: 7 running time: 1.5341106096903483 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1188808 Vali Loss: 0.1469098 Test Loss: 0.2276276\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1236797\n",
      "\tspeed: 0.2797s/iter; left time: 761.8005s\n",
      "\titers: 200, epoch: 8 | loss: 0.1208407\n",
      "\tspeed: 0.0977s/iter; left time: 256.3200s\n",
      "\titers: 300, epoch: 8 | loss: 0.0918566\n",
      "\tspeed: 0.0976s/iter; left time: 246.2813s\n",
      "\titers: 400, epoch: 8 | loss: 0.1157172\n",
      "\tspeed: 0.0975s/iter; left time: 236.3369s\n",
      "\titers: 500, epoch: 8 | loss: 0.1351758\n",
      "\tspeed: 0.0973s/iter; left time: 226.0756s\n",
      "\titers: 600, epoch: 8 | loss: 0.1347238\n",
      "\tspeed: 0.0976s/iter; left time: 216.9753s\n",
      "\titers: 700, epoch: 8 | loss: 0.1363690\n",
      "\tspeed: 0.0973s/iter; left time: 206.6648s\n",
      "\titers: 800, epoch: 8 | loss: 0.1204014\n",
      "\tspeed: 0.0973s/iter; left time: 196.8978s\n",
      "\titers: 900, epoch: 8 | loss: 0.1477749\n",
      "\tspeed: 0.0973s/iter; left time: 187.2952s\n",
      "Epoch: 8 running time: 1.5349057833353679 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1187568 Vali Loss: 0.1480086 Test Loss: 0.2246380\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1125018\n",
      "\tspeed: 0.2789s/iter; left time: 497.2608s\n",
      "\titers: 200, epoch: 9 | loss: 0.1158750\n",
      "\tspeed: 0.0973s/iter; left time: 163.8118s\n",
      "\titers: 300, epoch: 9 | loss: 0.0744595\n",
      "\tspeed: 0.0971s/iter; left time: 153.6721s\n",
      "\titers: 400, epoch: 9 | loss: 0.1645487\n",
      "\tspeed: 0.0971s/iter; left time: 144.0459s\n",
      "\titers: 500, epoch: 9 | loss: 0.1351148\n",
      "\tspeed: 0.0972s/iter; left time: 134.4204s\n",
      "\titers: 600, epoch: 9 | loss: 0.1294514\n",
      "\tspeed: 0.0974s/iter; left time: 124.9993s\n",
      "\titers: 700, epoch: 9 | loss: 0.1100625\n",
      "\tspeed: 0.0978s/iter; left time: 115.7038s\n",
      "\titers: 800, epoch: 9 | loss: 0.0923413\n",
      "\tspeed: 0.0973s/iter; left time: 105.4079s\n",
      "\titers: 900, epoch: 9 | loss: 0.1044033\n",
      "\tspeed: 0.0976s/iter; left time: 95.9670s\n",
      "Epoch: 9 running time: 1.5329768737157186 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1184515 Vali Loss: 0.1479403 Test Loss: 0.2238555\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.22542472183704376, mae:0.28379637002944946\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5664268\n",
      "\tspeed: 0.1015s/iter; left time: 945.3234s\n",
      "\titers: 200, epoch: 1 | loss: 0.2526940\n",
      "\tspeed: 0.0982s/iter; left time: 904.5285s\n",
      "\titers: 300, epoch: 1 | loss: 0.2292914\n",
      "\tspeed: 0.0979s/iter; left time: 892.0578s\n",
      "\titers: 400, epoch: 1 | loss: 0.2790347\n",
      "\tspeed: 0.0977s/iter; left time: 880.2624s\n",
      "\titers: 500, epoch: 1 | loss: 0.2430350\n",
      "\tspeed: 0.0978s/iter; left time: 871.8664s\n",
      "\titers: 600, epoch: 1 | loss: 0.1929865\n",
      "\tspeed: 0.0979s/iter; left time: 862.9822s\n",
      "\titers: 700, epoch: 1 | loss: 0.2642112\n",
      "\tspeed: 0.0978s/iter; left time: 851.8589s\n",
      "\titers: 800, epoch: 1 | loss: 0.2145494\n",
      "\tspeed: 0.0988s/iter; left time: 850.7168s\n",
      "\titers: 900, epoch: 1 | loss: 0.2589278\n",
      "\tspeed: 0.0979s/iter; left time: 833.5124s\n",
      "Epoch: 1 running time: 1.54476105372111 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2910090 Vali Loss: 0.1831906 Test Loss: 0.2378062\n",
      "Validation loss decreased (inf --> 0.183191).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1635738\n",
      "\tspeed: 0.2865s/iter; left time: 2397.9939s\n",
      "\titers: 200, epoch: 2 | loss: 0.1695694\n",
      "\tspeed: 0.0980s/iter; left time: 810.1318s\n",
      "\titers: 300, epoch: 2 | loss: 0.2019649\n",
      "\tspeed: 0.0977s/iter; left time: 798.4613s\n",
      "\titers: 400, epoch: 2 | loss: 0.1792056\n",
      "\tspeed: 0.0976s/iter; left time: 787.4753s\n",
      "\titers: 500, epoch: 2 | loss: 0.1652687\n",
      "\tspeed: 0.0977s/iter; left time: 778.5259s\n",
      "\titers: 600, epoch: 2 | loss: 0.2338050\n",
      "\tspeed: 0.0977s/iter; left time: 768.5250s\n",
      "\titers: 700, epoch: 2 | loss: 0.1457106\n",
      "\tspeed: 0.0978s/iter; left time: 760.1506s\n",
      "\titers: 800, epoch: 2 | loss: 0.2114065\n",
      "\tspeed: 0.0978s/iter; left time: 750.1098s\n",
      "\titers: 900, epoch: 2 | loss: 0.1232173\n",
      "\tspeed: 0.0976s/iter; left time: 738.8120s\n",
      "Epoch: 2 running time: 1.53813587029775 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1803521 Vali Loss: 0.1595277 Test Loss: 0.2245688\n",
      "Validation loss decreased (0.183191 --> 0.159528).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1655260\n",
      "\tspeed: 0.2837s/iter; left time: 2107.4144s\n",
      "\titers: 200, epoch: 3 | loss: 0.1260740\n",
      "\tspeed: 0.0977s/iter; left time: 715.9353s\n",
      "\titers: 300, epoch: 3 | loss: 0.1393616\n",
      "\tspeed: 0.0975s/iter; left time: 705.0829s\n",
      "\titers: 400, epoch: 3 | loss: 0.2075959\n",
      "\tspeed: 0.0978s/iter; left time: 697.2631s\n",
      "\titers: 500, epoch: 3 | loss: 0.1714134\n",
      "\tspeed: 0.0975s/iter; left time: 685.3577s\n",
      "\titers: 600, epoch: 3 | loss: 0.1982437\n",
      "\tspeed: 0.0977s/iter; left time: 676.8993s\n",
      "\titers: 700, epoch: 3 | loss: 0.1601679\n",
      "\tspeed: 0.0976s/iter; left time: 666.7277s\n",
      "\titers: 800, epoch: 3 | loss: 0.1601529\n",
      "\tspeed: 0.0981s/iter; left time: 659.8929s\n",
      "\titers: 900, epoch: 3 | loss: 0.1533434\n",
      "\tspeed: 0.0979s/iter; left time: 648.9327s\n",
      "Epoch: 3 running time: 1.5359195987383525 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1536249 Vali Loss: 0.1533358 Test Loss: 0.2164915\n",
      "Validation loss decreased (0.159528 --> 0.153336).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1581352\n",
      "\tspeed: 0.2850s/iter; left time: 1849.0780s\n",
      "\titers: 200, epoch: 4 | loss: 0.1934823\n",
      "\tspeed: 0.0979s/iter; left time: 625.3818s\n",
      "\titers: 300, epoch: 4 | loss: 0.1746560\n",
      "\tspeed: 0.0977s/iter; left time: 614.4730s\n",
      "\titers: 400, epoch: 4 | loss: 0.1177440\n",
      "\tspeed: 0.0978s/iter; left time: 605.3739s\n",
      "\titers: 500, epoch: 4 | loss: 0.1525689\n",
      "\tspeed: 0.0979s/iter; left time: 595.9054s\n",
      "\titers: 600, epoch: 4 | loss: 0.1305330\n",
      "\tspeed: 0.0981s/iter; left time: 587.4081s\n",
      "\titers: 700, epoch: 4 | loss: 0.1843030\n",
      "\tspeed: 0.0990s/iter; left time: 583.0748s\n",
      "\titers: 800, epoch: 4 | loss: 0.0997227\n",
      "\tspeed: 0.0983s/iter; left time: 569.1900s\n",
      "\titers: 900, epoch: 4 | loss: 0.1070071\n",
      "\tspeed: 0.0976s/iter; left time: 555.1758s\n",
      "Epoch: 4 running time: 1.542645013332367 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1371581 Vali Loss: 0.1579345 Test Loss: 0.2233839\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2039691\n",
      "\tspeed: 0.2791s/iter; left time: 1548.3095s\n",
      "\titers: 200, epoch: 5 | loss: 0.1556967\n",
      "\tspeed: 0.0974s/iter; left time: 530.3643s\n",
      "\titers: 300, epoch: 5 | loss: 0.0973489\n",
      "\tspeed: 0.0979s/iter; left time: 523.2324s\n",
      "\titers: 400, epoch: 5 | loss: 0.1531475\n",
      "\tspeed: 0.0978s/iter; left time: 513.4027s\n",
      "\titers: 500, epoch: 5 | loss: 0.1251926\n",
      "\tspeed: 0.0978s/iter; left time: 503.6290s\n",
      "\titers: 600, epoch: 5 | loss: 0.1437106\n",
      "\tspeed: 0.0979s/iter; left time: 493.9730s\n",
      "\titers: 700, epoch: 5 | loss: 0.1226156\n",
      "\tspeed: 0.0982s/iter; left time: 485.8401s\n",
      "\titers: 800, epoch: 5 | loss: 0.1796097\n",
      "\tspeed: 0.0983s/iter; left time: 476.2434s\n",
      "\titers: 900, epoch: 5 | loss: 0.1031976\n",
      "\tspeed: 0.0982s/iter; left time: 466.3618s\n",
      "Epoch: 5 running time: 1.5422874291737874 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1345451 Vali Loss: 0.1518466 Test Loss: 0.2202574\n",
      "Validation loss decreased (0.153336 --> 0.151847).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1445282\n",
      "\tspeed: 0.2845s/iter; left time: 1310.2221s\n",
      "\titers: 200, epoch: 6 | loss: 0.1143389\n",
      "\tspeed: 0.0977s/iter; left time: 440.1757s\n",
      "\titers: 300, epoch: 6 | loss: 0.1190768\n",
      "\tspeed: 0.0976s/iter; left time: 430.0741s\n",
      "\titers: 400, epoch: 6 | loss: 0.1352780\n",
      "\tspeed: 0.0976s/iter; left time: 420.4648s\n",
      "\titers: 500, epoch: 6 | loss: 0.1314371\n",
      "\tspeed: 0.0987s/iter; left time: 415.2133s\n",
      "\titers: 600, epoch: 6 | loss: 0.1066968\n",
      "\tspeed: 0.0980s/iter; left time: 402.2130s\n",
      "\titers: 700, epoch: 6 | loss: 0.1378544\n",
      "\tspeed: 0.0980s/iter; left time: 392.4402s\n",
      "\titers: 800, epoch: 6 | loss: 0.0896610\n",
      "\tspeed: 0.0980s/iter; left time: 382.8143s\n",
      "\titers: 900, epoch: 6 | loss: 0.1284886\n",
      "\tspeed: 0.0978s/iter; left time: 372.4146s\n",
      "Epoch: 6 running time: 1.542008380095164 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1270434 Vali Loss: 0.1534718 Test Loss: 0.2216068\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1149424\n",
      "\tspeed: 0.2793s/iter; left time: 1023.6423s\n",
      "\titers: 200, epoch: 7 | loss: 0.1310254\n",
      "\tspeed: 0.0971s/iter; left time: 346.2150s\n",
      "\titers: 300, epoch: 7 | loss: 0.1674199\n",
      "\tspeed: 0.0974s/iter; left time: 337.3509s\n",
      "\titers: 400, epoch: 7 | loss: 0.1332274\n",
      "\tspeed: 0.0974s/iter; left time: 327.8453s\n",
      "\titers: 500, epoch: 7 | loss: 0.0768914\n",
      "\tspeed: 0.0974s/iter; left time: 317.9397s\n",
      "\titers: 600, epoch: 7 | loss: 0.1472603\n",
      "\tspeed: 0.0971s/iter; left time: 307.4626s\n",
      "\titers: 700, epoch: 7 | loss: 0.1242801\n",
      "\tspeed: 0.0971s/iter; left time: 297.7126s\n",
      "\titers: 800, epoch: 7 | loss: 0.1110313\n",
      "\tspeed: 0.0971s/iter; left time: 287.8902s\n",
      "\titers: 900, epoch: 7 | loss: 0.1239939\n",
      "\tspeed: 0.0971s/iter; left time: 278.0831s\n",
      "Epoch: 7 running time: 1.5312985539436341 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1262259 Vali Loss: 0.1533842 Test Loss: 0.2228643\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1185199\n",
      "\tspeed: 0.2770s/iter; left time: 754.5685s\n",
      "\titers: 200, epoch: 8 | loss: 0.1375734\n",
      "\tspeed: 0.0974s/iter; left time: 255.5432s\n",
      "\titers: 300, epoch: 8 | loss: 0.1466773\n",
      "\tspeed: 0.0973s/iter; left time: 245.7113s\n",
      "\titers: 400, epoch: 8 | loss: 0.1268446\n",
      "\tspeed: 0.0973s/iter; left time: 235.9252s\n",
      "\titers: 500, epoch: 8 | loss: 0.1191575\n",
      "\tspeed: 0.0974s/iter; left time: 226.2573s\n",
      "\titers: 600, epoch: 8 | loss: 0.1166207\n",
      "\tspeed: 0.0973s/iter; left time: 216.3031s\n",
      "\titers: 700, epoch: 8 | loss: 0.1127195\n",
      "\tspeed: 0.0975s/iter; left time: 207.0986s\n",
      "\titers: 800, epoch: 8 | loss: 0.1257366\n",
      "\tspeed: 0.0974s/iter; left time: 197.2305s\n",
      "\titers: 900, epoch: 8 | loss: 0.1132852\n",
      "\tspeed: 0.0976s/iter; left time: 187.7518s\n",
      "Epoch: 8 running time: 1.5325612545013427 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1262816 Vali Loss: 0.1506053 Test Loss: 0.2187933\n",
      "Validation loss decreased (0.151847 --> 0.150605).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1213355\n",
      "\tspeed: 0.2844s/iter; left time: 507.1089s\n",
      "\titers: 200, epoch: 9 | loss: 0.1153753\n",
      "\tspeed: 0.0977s/iter; left time: 164.4755s\n",
      "\titers: 300, epoch: 9 | loss: 0.1769007\n",
      "\tspeed: 0.0976s/iter; left time: 154.5795s\n",
      "\titers: 400, epoch: 9 | loss: 0.0931088\n",
      "\tspeed: 0.0972s/iter; left time: 144.2051s\n",
      "\titers: 500, epoch: 9 | loss: 0.1505617\n",
      "\tspeed: 0.0975s/iter; left time: 134.8129s\n",
      "\titers: 600, epoch: 9 | loss: 0.0891185\n",
      "\tspeed: 0.0975s/iter; left time: 125.0980s\n",
      "\titers: 700, epoch: 9 | loss: 0.1516637\n",
      "\tspeed: 0.0977s/iter; left time: 115.5484s\n",
      "\titers: 800, epoch: 9 | loss: 0.1149239\n",
      "\tspeed: 0.0976s/iter; left time: 105.6957s\n",
      "\titers: 900, epoch: 9 | loss: 0.1068046\n",
      "\tspeed: 0.0977s/iter; left time: 96.0086s\n",
      "Epoch: 9 running time: 1.5352671146392822 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1247482 Vali Loss: 0.1516108 Test Loss: 0.2206672\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1710355\n",
      "\tspeed: 0.2782s/iter; left time: 234.2491s\n",
      "\titers: 200, epoch: 10 | loss: 0.0977202\n",
      "\tspeed: 0.0977s/iter; left time: 72.5232s\n",
      "\titers: 300, epoch: 10 | loss: 0.1131416\n",
      "\tspeed: 0.0978s/iter; left time: 62.7592s\n",
      "\titers: 400, epoch: 10 | loss: 0.0992893\n",
      "\tspeed: 0.0978s/iter; left time: 52.9885s\n",
      "\titers: 500, epoch: 10 | loss: 0.1500777\n",
      "\tspeed: 0.0976s/iter; left time: 43.1232s\n",
      "\titers: 600, epoch: 10 | loss: 0.1034379\n",
      "\tspeed: 0.0977s/iter; left time: 33.4155s\n",
      "\titers: 700, epoch: 10 | loss: 0.1288431\n",
      "\tspeed: 0.0977s/iter; left time: 23.6381s\n",
      "\titers: 800, epoch: 10 | loss: 0.0905820\n",
      "\tspeed: 0.0976s/iter; left time: 13.8616s\n",
      "\titers: 900, epoch: 10 | loss: 0.1386539\n",
      "\tspeed: 0.0976s/iter; left time: 4.1007s\n",
      "Epoch: 10 running time: 1.5367359081904093 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1242712 Vali Loss: 0.1508001 Test Loss: 0.2190831\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.21892176568508148, mae:0.2853127717971802\n",
      "\n",
      "Time intermediate for IT dataset: 34.16802665392558 min.\n",
      "Total time: 140.47173579136532 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A100 80 GB GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5892869\n",
      "\tspeed: 1.7740s/iter; left time: 16517.8096s\n",
      "\titers: 200, epoch: 1 | loss: 0.3933146\n",
      "\tspeed: 1.7834s/iter; left time: 16426.8272s\n",
      "\titers: 300, epoch: 1 | loss: 0.4908104\n",
      "\tspeed: 1.7627s/iter; left time: 16060.3907s\n",
      "\titers: 400, epoch: 1 | loss: 0.3265559\n",
      "\tspeed: 1.7188s/iter; left time: 15488.1032s\n",
      "\titers: 500, epoch: 1 | loss: 0.2909752\n",
      "\tspeed: 1.7761s/iter; left time: 15826.7083s\n",
      "\titers: 600, epoch: 1 | loss: 0.3200079\n",
      "\tspeed: 1.7850s/iter; left time: 15727.7167s\n",
      "\titers: 700, epoch: 1 | loss: 0.3656467\n",
      "\tspeed: 1.7830s/iter; left time: 15531.9936s\n",
      "\titers: 800, epoch: 1 | loss: 0.3098063\n",
      "\tspeed: 1.7816s/iter; left time: 15341.5524s\n",
      "\titers: 900, epoch: 1 | loss: 0.2568061\n",
      "\tspeed: 1.6853s/iter; left time: 14343.6843s\n",
      "Epoch: 1 running time: 27.634515269597372 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3881650 Vali Loss: 0.4417571 Test Loss: 0.5049031\n",
      "Validation loss decreased (inf --> 0.441757).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3426694\n",
      "\tspeed: 6.4784s/iter; left time: 54224.2063s\n",
      "\titers: 200, epoch: 2 | loss: 0.2399278\n",
      "\tspeed: 1.7739s/iter; left time: 14670.3334s\n",
      "\titers: 300, epoch: 2 | loss: 0.2567746\n",
      "\tspeed: 1.7714s/iter; left time: 14472.3325s\n",
      "\titers: 400, epoch: 2 | loss: 0.2278990\n",
      "\tspeed: 1.7610s/iter; left time: 14211.3144s\n",
      "\titers: 500, epoch: 2 | loss: 0.3125402\n",
      "\tspeed: 1.7496s/iter; left time: 13944.5195s\n",
      "\titers: 600, epoch: 2 | loss: 0.2258390\n",
      "\tspeed: 1.7046s/iter; left time: 13415.2120s\n",
      "\titers: 700, epoch: 2 | loss: 0.2849320\n",
      "\tspeed: 1.7012s/iter; left time: 13218.6742s\n",
      "\titers: 800, epoch: 2 | loss: 0.3336966\n",
      "\tspeed: 1.6323s/iter; left time: 12519.4820s\n",
      "\titers: 900, epoch: 2 | loss: 0.2731778\n",
      "\tspeed: 1.6584s/iter; left time: 12553.8093s\n",
      "Epoch: 2 running time: 26.984792792797087 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2710038 Vali Loss: 0.4523767 Test Loss: 0.5282342\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2522012\n",
      "\tspeed: 4.8176s/iter; left time: 35789.6374s\n",
      "\titers: 200, epoch: 3 | loss: 0.2502043\n",
      "\tspeed: 1.3412s/iter; left time: 9829.7210s\n",
      "\titers: 300, epoch: 3 | loss: 0.3234225\n",
      "\tspeed: 1.7360s/iter; left time: 12549.5617s\n",
      "\titers: 400, epoch: 3 | loss: 0.2362487\n",
      "\tspeed: 1.7909s/iter; left time: 12767.2286s\n",
      "\titers: 500, epoch: 3 | loss: 0.2309934\n",
      "\tspeed: 1.7932s/iter; left time: 12604.6409s\n",
      "\titers: 600, epoch: 3 | loss: 0.2739768\n",
      "\tspeed: 1.7936s/iter; left time: 12427.9019s\n",
      "\titers: 700, epoch: 3 | loss: 0.3025040\n",
      "\tspeed: 1.7927s/iter; left time: 12242.5609s\n",
      "\titers: 800, epoch: 3 | loss: 0.1998511\n",
      "\tspeed: 1.7792s/iter; left time: 11972.4464s\n",
      "\titers: 900, epoch: 3 | loss: 0.1695538\n",
      "\tspeed: 1.7912s/iter; left time: 11874.1508s\n",
      "Epoch: 3 running time: 26.104775579770408 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2621829 Vali Loss: 0.4351654 Test Loss: 0.5090449\n",
      "Validation loss decreased (0.441757 --> 0.435165).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2559014\n",
      "\tspeed: 6.4509s/iter; left time: 41853.6480s\n",
      "\titers: 200, epoch: 4 | loss: 0.3135229\n",
      "\tspeed: 1.7900s/iter; left time: 11434.6898s\n",
      "\titers: 300, epoch: 4 | loss: 0.2431349\n",
      "\tspeed: 1.7104s/iter; left time: 10755.2828s\n",
      "\titers: 400, epoch: 4 | loss: 0.1893651\n",
      "\tspeed: 1.7813s/iter; left time: 11022.4670s\n",
      "\titers: 500, epoch: 4 | loss: 0.2253976\n",
      "\tspeed: 1.7824s/iter; left time: 10851.3085s\n",
      "\titers: 600, epoch: 4 | loss: 0.1855248\n",
      "\tspeed: 1.7802s/iter; left time: 10659.6153s\n",
      "\titers: 700, epoch: 4 | loss: 0.2409668\n",
      "\tspeed: 1.7594s/iter; left time: 10359.2135s\n",
      "\titers: 800, epoch: 4 | loss: 0.1979460\n",
      "\tspeed: 1.7811s/iter; left time: 10308.8789s\n",
      "\titers: 900, epoch: 4 | loss: 0.2517775\n",
      "\tspeed: 1.7798s/iter; left time: 10123.7314s\n",
      "Epoch: 4 running time: 27.813729711373647 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2252403 Vali Loss: 0.4279542 Test Loss: 0.5073299\n",
      "Validation loss decreased (0.435165 --> 0.427954).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1416561\n",
      "\tspeed: 6.4838s/iter; left time: 35965.8712s\n",
      "\titers: 200, epoch: 5 | loss: 0.1728254\n",
      "\tspeed: 1.7797s/iter; left time: 9694.2236s\n",
      "\titers: 300, epoch: 5 | loss: 0.1700590\n",
      "\tspeed: 1.7675s/iter; left time: 9450.8093s\n",
      "\titers: 400, epoch: 5 | loss: 0.2203045\n",
      "\tspeed: 1.6914s/iter; left time: 8874.6419s\n",
      "\titers: 500, epoch: 5 | loss: 0.1987383\n",
      "\tspeed: 1.7258s/iter; left time: 8882.7904s\n",
      "\titers: 600, epoch: 5 | loss: 0.1874608\n",
      "\tspeed: 1.7476s/iter; left time: 8819.9565s\n",
      "\titers: 700, epoch: 5 | loss: 0.1910253\n",
      "\tspeed: 1.7768s/iter; left time: 8789.9132s\n",
      "\titers: 800, epoch: 5 | loss: 0.1339178\n",
      "\tspeed: 1.7646s/iter; left time: 8552.9812s\n",
      "\titers: 900, epoch: 5 | loss: 0.2224116\n",
      "\tspeed: 1.7729s/iter; left time: 8416.1086s\n",
      "Epoch: 5 running time: 27.31057822306951 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2015247 Vali Loss: 0.4473854 Test Loss: 0.5301394\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1897485\n",
      "\tspeed: 5.1661s/iter; left time: 23795.2181s\n",
      "\titers: 200, epoch: 6 | loss: 0.1521612\n",
      "\tspeed: 1.4302s/iter; left time: 6444.4513s\n",
      "\titers: 300, epoch: 6 | loss: 0.2534637\n",
      "\tspeed: 1.4333s/iter; left time: 6315.1067s\n",
      "\titers: 400, epoch: 6 | loss: 0.2329123\n",
      "\tspeed: 1.4297s/iter; left time: 6156.2153s\n",
      "\titers: 500, epoch: 6 | loss: 0.1705337\n",
      "\tspeed: 1.4257s/iter; left time: 5996.5011s\n",
      "\titers: 600, epoch: 6 | loss: 0.1758052\n",
      "\tspeed: 1.3979s/iter; left time: 5739.8332s\n",
      "\titers: 700, epoch: 6 | loss: 0.1472269\n",
      "\tspeed: 1.4176s/iter; left time: 5678.9607s\n",
      "\titers: 800, epoch: 6 | loss: 0.2156595\n",
      "\tspeed: 1.2984s/iter; left time: 5071.4561s\n",
      "\titers: 900, epoch: 6 | loss: 0.2027543\n",
      "\tspeed: 1.3351s/iter; left time: 5081.4661s\n",
      "Epoch: 6 running time: 21.879121748606362 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2005814 Vali Loss: 0.4429339 Test Loss: 0.5146396\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2896483\n",
      "\tspeed: 4.6987s/iter; left time: 17220.7879s\n",
      "\titers: 200, epoch: 7 | loss: 0.1951245\n",
      "\tspeed: 0.9930s/iter; left time: 3540.0434s\n",
      "\titers: 300, epoch: 7 | loss: 0.1956486\n",
      "\tspeed: 0.7265s/iter; left time: 2517.2155s\n",
      "\titers: 400, epoch: 7 | loss: 0.1279040\n",
      "\tspeed: 0.6539s/iter; left time: 2200.4047s\n",
      "\titers: 500, epoch: 7 | loss: 0.1855619\n",
      "\tspeed: 0.8751s/iter; left time: 2857.0585s\n",
      "\titers: 600, epoch: 7 | loss: 0.2258058\n",
      "\tspeed: 1.2685s/iter; left time: 4014.7927s\n",
      "\titers: 700, epoch: 7 | loss: 0.1987671\n",
      "\tspeed: 1.4336s/iter; left time: 4393.9192s\n",
      "\titers: 800, epoch: 7 | loss: 0.1772510\n",
      "\tspeed: 1.4381s/iter; left time: 4263.9319s\n",
      "\titers: 900, epoch: 7 | loss: 0.1790513\n",
      "\tspeed: 1.4330s/iter; left time: 4105.4028s\n",
      "Epoch: 7 running time: 17.652518121401467 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2002954 Vali Loss: 0.4401115 Test Loss: 0.5113256\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5103916525840759, mae:0.4799240231513977\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4982714\n",
      "\tspeed: 1.4374s/iter; left time: 13383.4126s\n",
      "\titers: 200, epoch: 1 | loss: 0.3455804\n",
      "\tspeed: 1.4423s/iter; left time: 13284.7779s\n",
      "\titers: 300, epoch: 1 | loss: 0.3005768\n",
      "\tspeed: 1.4395s/iter; left time: 13115.6823s\n",
      "\titers: 400, epoch: 1 | loss: 0.3961580\n",
      "\tspeed: 1.4217s/iter; left time: 12810.5044s\n",
      "\titers: 500, epoch: 1 | loss: 0.2840077\n",
      "\tspeed: 1.4397s/iter; left time: 12829.3174s\n",
      "\titers: 600, epoch: 1 | loss: 0.3224835\n",
      "\tspeed: 1.4440s/iter; left time: 12723.0401s\n",
      "\titers: 700, epoch: 1 | loss: 0.2960292\n",
      "\tspeed: 1.4416s/iter; left time: 12557.8911s\n",
      "\titers: 800, epoch: 1 | loss: 0.3862472\n",
      "\tspeed: 1.4420s/iter; left time: 12417.0894s\n",
      "\titers: 900, epoch: 1 | loss: 0.2340263\n",
      "\tspeed: 1.4331s/iter; left time: 12196.7282s\n",
      "Epoch: 1 running time: 22.5532741467158 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3891936 Vali Loss: 0.4420241 Test Loss: 0.5089546\n",
      "Validation loss decreased (inf --> 0.442024).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2936978\n",
      "\tspeed: 5.1865s/iter; left time: 43411.0637s\n",
      "\titers: 200, epoch: 2 | loss: 0.3230321\n",
      "\tspeed: 1.4344s/iter; left time: 11862.6389s\n",
      "\titers: 300, epoch: 2 | loss: 0.3572863\n",
      "\tspeed: 1.4069s/iter; left time: 11494.4504s\n",
      "\titers: 400, epoch: 2 | loss: 0.2790028\n",
      "\tspeed: 1.3553s/iter; left time: 10937.2989s\n",
      "\titers: 500, epoch: 2 | loss: 0.2723592\n",
      "\tspeed: 1.4308s/iter; left time: 11403.2557s\n",
      "\titers: 600, epoch: 2 | loss: 0.2180605\n",
      "\tspeed: 1.4314s/iter; left time: 11264.9379s\n",
      "\titers: 700, epoch: 2 | loss: 0.3322076\n",
      "\tspeed: 1.4272s/iter; left time: 11089.5330s\n",
      "\titers: 800, epoch: 2 | loss: 0.2515522\n",
      "\tspeed: 1.4287s/iter; left time: 10958.1257s\n",
      "\titers: 900, epoch: 2 | loss: 0.2368346\n",
      "\tspeed: 1.4276s/iter; left time: 10807.0779s\n",
      "Epoch: 2 running time: 22.11847771803538 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2732664 Vali Loss: 0.4254902 Test Loss: 0.5132075\n",
      "Validation loss decreased (0.442024 --> 0.425490).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2119966\n",
      "\tspeed: 5.1525s/iter; left time: 38277.9960s\n",
      "\titers: 200, epoch: 3 | loss: 0.2830262\n",
      "\tspeed: 1.4251s/iter; left time: 10444.4019s\n",
      "\titers: 300, epoch: 3 | loss: 0.2171073\n",
      "\tspeed: 1.4268s/iter; left time: 10314.1490s\n",
      "\titers: 400, epoch: 3 | loss: 0.2491532\n",
      "\tspeed: 1.4281s/iter; left time: 10181.1824s\n",
      "\titers: 500, epoch: 3 | loss: 0.1985783\n",
      "\tspeed: 1.4274s/iter; left time: 10032.9357s\n",
      "\titers: 600, epoch: 3 | loss: 0.2014756\n",
      "\tspeed: 1.4308s/iter; left time: 9913.9760s\n",
      "\titers: 700, epoch: 3 | loss: 0.1760274\n",
      "\tspeed: 1.3566s/iter; left time: 9264.1872s\n",
      "\titers: 800, epoch: 3 | loss: 0.2013480\n",
      "\tspeed: 1.3355s/iter; left time: 8986.8703s\n",
      "\titers: 900, epoch: 3 | loss: 0.1793382\n",
      "\tspeed: 1.4094s/iter; left time: 9342.8138s\n",
      "Epoch: 3 running time: 22.066988996664683 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2239663 Vali Loss: 0.4408260 Test Loss: 0.5401742\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2301260\n",
      "\tspeed: 4.9796s/iter; left time: 32307.3978s\n",
      "\titers: 200, epoch: 4 | loss: 0.2951562\n",
      "\tspeed: 1.3245s/iter; left time: 8460.6221s\n",
      "\titers: 300, epoch: 4 | loss: 0.1593555\n",
      "\tspeed: 1.1704s/iter; left time: 7359.4767s\n",
      "\titers: 400, epoch: 4 | loss: 0.1884202\n",
      "\tspeed: 1.0732s/iter; left time: 6640.6912s\n",
      "\titers: 500, epoch: 4 | loss: 0.2468180\n",
      "\tspeed: 0.8089s/iter; left time: 4924.8729s\n",
      "\titers: 600, epoch: 4 | loss: 0.1581286\n",
      "\tspeed: 0.6471s/iter; left time: 3874.9299s\n",
      "\titers: 700, epoch: 4 | loss: 0.2391508\n",
      "\tspeed: 0.7650s/iter; left time: 4504.4719s\n",
      "\titers: 800, epoch: 4 | loss: 0.2061963\n",
      "\tspeed: 1.1186s/iter; left time: 6474.4536s\n",
      "\titers: 900, epoch: 4 | loss: 0.1761994\n",
      "\tspeed: 1.4355s/iter; left time: 8165.3420s\n",
      "Epoch: 4 running time: 17.1251859386762 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2176665 Vali Loss: 0.4362066 Test Loss: 0.5301881\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2694066\n",
      "\tspeed: 5.0994s/iter; left time: 28286.4630s\n",
      "\titers: 200, epoch: 5 | loss: 0.2159820\n",
      "\tspeed: 1.4300s/iter; left time: 7788.9937s\n",
      "\titers: 300, epoch: 5 | loss: 0.1976472\n",
      "\tspeed: 1.4326s/iter; left time: 7659.9370s\n",
      "\titers: 400, epoch: 5 | loss: 0.2887886\n",
      "\tspeed: 1.3876s/iter; left time: 7280.8094s\n",
      "\titers: 500, epoch: 5 | loss: 0.1834570\n",
      "\tspeed: 1.4207s/iter; left time: 7312.3769s\n",
      "\titers: 600, epoch: 5 | loss: 0.2080595\n",
      "\tspeed: 1.4274s/iter; left time: 7204.2006s\n",
      "\titers: 700, epoch: 5 | loss: 0.1690192\n",
      "\tspeed: 1.4278s/iter; left time: 7063.3950s\n",
      "\titers: 800, epoch: 5 | loss: 0.2367482\n",
      "\tspeed: 1.4282s/iter; left time: 6922.4804s\n",
      "\titers: 900, epoch: 5 | loss: 0.2473151\n",
      "\tspeed: 1.4323s/iter; left time: 6799.0906s\n",
      "Epoch: 5 running time: 22.21115117073059 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2170444 Vali Loss: 0.4308408 Test Loss: 0.5248260\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5258011817932129, mae:0.4902995228767395\n",
      "\n",
      "Time intermediate for DE dataset: 353.4190151294072 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4796472\n",
      "\tspeed: 1.3850s/iter; left time: 12895.7260s\n",
      "\titers: 200, epoch: 1 | loss: 0.3964097\n",
      "\tspeed: 1.3697s/iter; left time: 12616.1297s\n",
      "\titers: 300, epoch: 1 | loss: 0.3231561\n",
      "\tspeed: 1.3550s/iter; left time: 12345.3918s\n",
      "\titers: 400, epoch: 1 | loss: 0.3265139\n",
      "\tspeed: 1.2937s/iter; left time: 11657.9029s\n",
      "\titers: 500, epoch: 1 | loss: 0.2750012\n",
      "\tspeed: 1.3655s/iter; left time: 12167.8918s\n",
      "\titers: 600, epoch: 1 | loss: 0.3435578\n",
      "\tspeed: 1.3661s/iter; left time: 12036.9963s\n",
      "\titers: 700, epoch: 1 | loss: 0.4015486\n",
      "\tspeed: 1.3264s/iter; left time: 11554.4076s\n",
      "\titers: 800, epoch: 1 | loss: 0.2962549\n",
      "\tspeed: 1.3483s/iter; left time: 11610.1210s\n",
      "\titers: 900, epoch: 1 | loss: 0.2510418\n",
      "\tspeed: 1.3622s/iter; left time: 11593.7777s\n",
      "Epoch: 1 running time: 21.2065079053243 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4066807 Vali Loss: 0.4919056 Test Loss: 0.8280761\n",
      "Validation loss decreased (inf --> 0.491906).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2909266\n",
      "\tspeed: 5.0090s/iter; left time: 41925.1790s\n",
      "\titers: 200, epoch: 2 | loss: 0.3427289\n",
      "\tspeed: 1.3748s/iter; left time: 11369.3495s\n",
      "\titers: 300, epoch: 2 | loss: 0.3370485\n",
      "\tspeed: 1.3707s/iter; left time: 11198.6578s\n",
      "\titers: 400, epoch: 2 | loss: 0.3099507\n",
      "\tspeed: 1.3399s/iter; left time: 10813.0219s\n",
      "\titers: 500, epoch: 2 | loss: 0.2469848\n",
      "\tspeed: 1.3649s/iter; left time: 10878.1935s\n",
      "\titers: 600, epoch: 2 | loss: 0.2515435\n",
      "\tspeed: 1.3614s/iter; left time: 10714.1964s\n",
      "\titers: 700, epoch: 2 | loss: 0.2755833\n",
      "\tspeed: 1.3706s/iter; left time: 10649.5267s\n",
      "\titers: 800, epoch: 2 | loss: 0.2752435\n",
      "\tspeed: 1.2579s/iter; left time: 9648.1905s\n",
      "\titers: 900, epoch: 2 | loss: 0.2883866\n",
      "\tspeed: 1.3547s/iter; left time: 10254.8205s\n",
      "Epoch: 2 running time: 21.195697434743245 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3072687 Vali Loss: 0.4500027 Test Loss: 0.6647689\n",
      "Validation loss decreased (0.491906 --> 0.450003).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2362531\n",
      "\tspeed: 4.8455s/iter; left time: 35997.1274s\n",
      "\titers: 200, epoch: 3 | loss: 0.2749321\n",
      "\tspeed: 1.2853s/iter; left time: 9420.2433s\n",
      "\titers: 300, epoch: 3 | loss: 0.2563096\n",
      "\tspeed: 1.2758s/iter; left time: 9222.9034s\n",
      "\titers: 400, epoch: 3 | loss: 0.2283548\n",
      "\tspeed: 1.2296s/iter; left time: 8765.7035s\n",
      "\titers: 500, epoch: 3 | loss: 0.2578490\n",
      "\tspeed: 1.1192s/iter; left time: 7867.2008s\n",
      "\titers: 600, epoch: 3 | loss: 0.2866107\n",
      "\tspeed: 0.8677s/iter; left time: 6012.2398s\n",
      "\titers: 700, epoch: 3 | loss: 0.3263125\n",
      "\tspeed: 0.7238s/iter; left time: 4942.9240s\n",
      "\titers: 800, epoch: 3 | loss: 0.2137966\n",
      "\tspeed: 0.5863s/iter; left time: 3944.9171s\n",
      "\titers: 900, epoch: 3 | loss: 0.2546224\n",
      "\tspeed: 0.5636s/iter; left time: 3736.2682s\n",
      "Epoch: 3 running time: 15.401446159680685 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2680396 Vali Loss: 0.4645892 Test Loss: 0.6988297\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2277489\n",
      "\tspeed: 4.3137s/iter; left time: 27987.2562s\n",
      "\titers: 200, epoch: 4 | loss: 0.2136873\n",
      "\tspeed: 1.3829s/iter; left time: 8833.6807s\n",
      "\titers: 300, epoch: 4 | loss: 0.2673800\n",
      "\tspeed: 1.3828s/iter; left time: 8694.8421s\n",
      "\titers: 400, epoch: 4 | loss: 0.2411096\n",
      "\tspeed: 1.3816s/iter; left time: 8549.1769s\n",
      "\titers: 500, epoch: 4 | loss: 0.1858426\n",
      "\tspeed: 1.3775s/iter; left time: 8386.0030s\n",
      "\titers: 600, epoch: 4 | loss: 0.2128242\n",
      "\tspeed: 1.3820s/iter; left time: 8275.6342s\n",
      "\titers: 700, epoch: 4 | loss: 0.2819637\n",
      "\tspeed: 1.3834s/iter; left time: 8145.7501s\n",
      "\titers: 800, epoch: 4 | loss: 0.2102221\n",
      "\tspeed: 1.3786s/iter; left time: 7979.5622s\n",
      "\titers: 900, epoch: 4 | loss: 0.3691365\n",
      "\tspeed: 1.3702s/iter; left time: 7793.6622s\n",
      "Epoch: 4 running time: 21.51249199708303 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2621755 Vali Loss: 0.4662929 Test Loss: 0.7555156\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2223944\n",
      "\tspeed: 5.0266s/iter; left time: 27882.2882s\n",
      "\titers: 200, epoch: 5 | loss: 0.2555063\n",
      "\tspeed: 1.3755s/iter; left time: 7492.5291s\n",
      "\titers: 300, epoch: 5 | loss: 0.2554726\n",
      "\tspeed: 1.3860s/iter; left time: 7410.9994s\n",
      "\titers: 400, epoch: 5 | loss: 0.2304695\n",
      "\tspeed: 1.3777s/iter; left time: 7228.6914s\n",
      "\titers: 500, epoch: 5 | loss: 0.2484536\n",
      "\tspeed: 1.2556s/iter; left time: 6462.6218s\n",
      "\titers: 600, epoch: 5 | loss: 0.2365081\n",
      "\tspeed: 1.3428s/iter; left time: 6777.0384s\n",
      "\titers: 700, epoch: 5 | loss: 0.2740030\n",
      "\tspeed: 1.3757s/iter; left time: 6805.6870s\n",
      "\titers: 800, epoch: 5 | loss: 0.1907829\n",
      "\tspeed: 1.3714s/iter; left time: 6647.0590s\n",
      "\titers: 900, epoch: 5 | loss: 0.2470180\n",
      "\tspeed: 1.3584s/iter; left time: 6448.3293s\n",
      "Epoch: 5 running time: 21.294666449228924 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2611465 Vali Loss: 0.4532668 Test Loss: 0.7080034\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.7063973546028137, mae:0.5872204899787903\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5946396\n",
      "\tspeed: 1.3254s/iter; left time: 12341.0649s\n",
      "\titers: 200, epoch: 1 | loss: 0.3767517\n",
      "\tspeed: 1.3709s/iter; left time: 12627.2231s\n",
      "\titers: 300, epoch: 1 | loss: 0.2703782\n",
      "\tspeed: 1.3655s/iter; left time: 12440.9661s\n",
      "\titers: 400, epoch: 1 | loss: 0.4213276\n",
      "\tspeed: 1.3664s/iter; left time: 12312.9954s\n",
      "\titers: 500, epoch: 1 | loss: 0.3396932\n",
      "\tspeed: 1.3602s/iter; left time: 12120.4149s\n",
      "\titers: 600, epoch: 1 | loss: 0.3294225\n",
      "\tspeed: 1.3675s/iter; left time: 12048.6527s\n",
      "\titers: 700, epoch: 1 | loss: 0.3020035\n",
      "\tspeed: 1.3654s/iter; left time: 11893.8196s\n",
      "\titers: 800, epoch: 1 | loss: 0.3186351\n",
      "\tspeed: 1.3129s/iter; left time: 11305.5845s\n",
      "\titers: 900, epoch: 1 | loss: 0.3404082\n",
      "\tspeed: 1.3120s/iter; left time: 11166.5978s\n",
      "Epoch: 1 running time: 21.128375180562337 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4035210 Vali Loss: 0.4791251 Test Loss: 0.7368550\n",
      "Validation loss decreased (inf --> 0.479125).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2643960\n",
      "\tspeed: 4.8237s/iter; left time: 40373.9827s\n",
      "\titers: 200, epoch: 2 | loss: 0.3099206\n",
      "\tspeed: 1.2782s/iter; left time: 10570.8677s\n",
      "\titers: 300, epoch: 2 | loss: 0.3049880\n",
      "\tspeed: 1.2688s/iter; left time: 10366.2147s\n",
      "\titers: 400, epoch: 2 | loss: 0.2149809\n",
      "\tspeed: 1.2489s/iter; left time: 10078.6189s\n",
      "\titers: 500, epoch: 2 | loss: 0.3037942\n",
      "\tspeed: 1.2314s/iter; left time: 9814.3295s\n",
      "\titers: 600, epoch: 2 | loss: 0.2450974\n",
      "\tspeed: 1.1355s/iter; left time: 8936.2553s\n",
      "\titers: 700, epoch: 2 | loss: 0.2411249\n",
      "\tspeed: 1.1555s/iter; left time: 8978.2480s\n",
      "\titers: 800, epoch: 2 | loss: 0.2300154\n",
      "\tspeed: 1.0474s/iter; left time: 8033.7250s\n",
      "\titers: 900, epoch: 2 | loss: 0.2866468\n",
      "\tspeed: 0.7826s/iter; left time: 5924.1696s\n",
      "Epoch: 2 running time: 17.882394417126974 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3074028 Vali Loss: 0.4672675 Test Loss: 0.7471255\n",
      "Validation loss decreased (0.479125 --> 0.467268).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3160414\n",
      "\tspeed: 1.8532s/iter; left time: 13767.5335s\n",
      "\titers: 200, epoch: 3 | loss: 0.2468834\n",
      "\tspeed: 0.3005s/iter; left time: 2202.5809s\n",
      "\titers: 300, epoch: 3 | loss: 0.3383032\n",
      "\tspeed: 0.6520s/iter; left time: 4713.5842s\n",
      "\titers: 400, epoch: 3 | loss: 0.3059641\n",
      "\tspeed: 1.1035s/iter; left time: 7866.8764s\n",
      "\titers: 500, epoch: 3 | loss: 0.2890663\n",
      "\tspeed: 1.2825s/iter; left time: 9014.5395s\n",
      "\titers: 600, epoch: 3 | loss: 0.2565822\n",
      "\tspeed: 1.3146s/iter; left time: 9108.9164s\n",
      "\titers: 700, epoch: 3 | loss: 0.3086628\n",
      "\tspeed: 1.3144s/iter; left time: 8976.2766s\n",
      "\titers: 800, epoch: 3 | loss: 0.3262424\n",
      "\tspeed: 1.3154s/iter; left time: 8851.5350s\n",
      "\titers: 900, epoch: 3 | loss: 0.3235765\n",
      "\tspeed: 1.3168s/iter; left time: 8729.3705s\n",
      "Epoch: 3 running time: 15.939021106561025 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2682118 Vali Loss: 0.4870894 Test Loss: 0.8450049\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2983608\n",
      "\tspeed: 4.7959s/iter; left time: 31115.8646s\n",
      "\titers: 200, epoch: 4 | loss: 0.3798157\n",
      "\tspeed: 1.3158s/iter; left time: 8405.0878s\n",
      "\titers: 300, epoch: 4 | loss: 0.3646375\n",
      "\tspeed: 1.3164s/iter; left time: 8277.4508s\n",
      "\titers: 400, epoch: 4 | loss: 0.2232091\n",
      "\tspeed: 1.3138s/iter; left time: 8129.5373s\n",
      "\titers: 500, epoch: 4 | loss: 0.2237715\n",
      "\tspeed: 1.3118s/iter; left time: 7986.0461s\n",
      "\titers: 600, epoch: 4 | loss: 0.2513711\n",
      "\tspeed: 1.3124s/iter; left time: 7858.3846s\n",
      "\titers: 700, epoch: 4 | loss: 0.2766150\n",
      "\tspeed: 1.1828s/iter; left time: 6964.3137s\n",
      "\titers: 800, epoch: 4 | loss: 0.2482956\n",
      "\tspeed: 1.2756s/iter; left time: 7383.3370s\n",
      "\titers: 900, epoch: 4 | loss: 0.2322919\n",
      "\tspeed: 1.3148s/iter; left time: 7478.6640s\n",
      "Epoch: 4 running time: 20.341920590400697 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2625625 Vali Loss: 0.4763106 Test Loss: 0.8177143\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2406502\n",
      "\tspeed: 4.8073s/iter; left time: 26666.0044s\n",
      "\titers: 200, epoch: 5 | loss: 0.4271190\n",
      "\tspeed: 1.3183s/iter; left time: 7180.9562s\n",
      "\titers: 300, epoch: 5 | loss: 0.2466582\n",
      "\tspeed: 1.3101s/iter; left time: 7005.2303s\n",
      "\titers: 400, epoch: 5 | loss: 0.2363973\n",
      "\tspeed: 1.2632s/iter; left time: 6628.2263s\n",
      "\titers: 500, epoch: 5 | loss: 0.2911821\n",
      "\tspeed: 1.3385s/iter; left time: 6889.1402s\n",
      "\titers: 600, epoch: 5 | loss: 0.2240661\n",
      "\tspeed: 1.3298s/iter; left time: 6711.4679s\n",
      "\titers: 700, epoch: 5 | loss: 0.2569813\n",
      "\tspeed: 1.3120s/iter; left time: 6490.6207s\n",
      "\titers: 800, epoch: 5 | loss: 0.2432252\n",
      "\tspeed: 1.3067s/iter; left time: 6333.6801s\n",
      "\titers: 900, epoch: 5 | loss: 0.2644678\n",
      "\tspeed: 1.3071s/iter; left time: 6204.8726s\n",
      "Epoch: 5 running time: 20.55677490234375 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2632069 Vali Loss: 0.4643446 Test Loss: 0.7614479\n",
      "Validation loss decreased (0.467268 --> 0.464345).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2581221\n",
      "\tspeed: 4.6743s/iter; left time: 21529.6402s\n",
      "\titers: 200, epoch: 6 | loss: 0.2635585\n",
      "\tspeed: 1.2880s/iter; left time: 5803.5701s\n",
      "\titers: 300, epoch: 6 | loss: 0.2201428\n",
      "\tspeed: 1.3104s/iter; left time: 5773.6709s\n",
      "\titers: 400, epoch: 6 | loss: 0.2015900\n",
      "\tspeed: 1.3062s/iter; left time: 5624.6506s\n",
      "\titers: 500, epoch: 6 | loss: 0.2559985\n",
      "\tspeed: 1.3046s/iter; left time: 5486.9547s\n",
      "\titers: 600, epoch: 6 | loss: 0.2385457\n",
      "\tspeed: 1.3030s/iter; left time: 5350.3084s\n",
      "\titers: 700, epoch: 6 | loss: 0.2262333\n",
      "\tspeed: 1.3022s/iter; left time: 5216.7013s\n",
      "\titers: 800, epoch: 6 | loss: 0.2220343\n",
      "\tspeed: 1.2885s/iter; left time: 5033.0233s\n",
      "\titers: 900, epoch: 6 | loss: 0.2669659\n",
      "\tspeed: 1.2725s/iter; left time: 4843.0334s\n",
      "Epoch: 6 running time: 20.273427005608877 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2484723 Vali Loss: 0.4801593 Test Loss: 0.8024629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2513306\n",
      "\tspeed: 4.4382s/iter; left time: 16266.1361s\n",
      "\titers: 200, epoch: 7 | loss: 0.2128576\n",
      "\tspeed: 1.0449s/iter; left time: 3725.1048s\n",
      "\titers: 300, epoch: 7 | loss: 0.1917190\n",
      "\tspeed: 0.8012s/iter; left time: 2776.2266s\n",
      "\titers: 400, epoch: 7 | loss: 0.2231624\n",
      "\tspeed: 0.5754s/iter; left time: 1936.3859s\n",
      "\titers: 500, epoch: 7 | loss: 0.2302730\n",
      "\tspeed: 0.2980s/iter; left time: 972.9092s\n",
      "\titers: 600, epoch: 7 | loss: 0.2364664\n",
      "\tspeed: 0.3832s/iter; left time: 1212.7526s\n",
      "\titers: 700, epoch: 7 | loss: 0.2781757\n",
      "\tspeed: 0.4962s/iter; left time: 1520.7114s\n",
      "\titers: 800, epoch: 7 | loss: 0.3141681\n",
      "\tspeed: 0.8149s/iter; left time: 2416.1909s\n",
      "\titers: 900, epoch: 7 | loss: 0.2893362\n",
      "\tspeed: 0.9295s/iter; left time: 2662.9514s\n",
      "Epoch: 7 running time: 11.501748581727346 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2479377 Vali Loss: 0.4753176 Test Loss: 0.7996016\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2381537\n",
      "\tspeed: 4.6481s/iter; left time: 12661.3142s\n",
      "\titers: 200, epoch: 8 | loss: 0.2409409\n",
      "\tspeed: 1.3186s/iter; left time: 3460.1084s\n",
      "\titers: 300, epoch: 8 | loss: 0.2478120\n",
      "\tspeed: 1.3241s/iter; left time: 3341.9916s\n",
      "\titers: 400, epoch: 8 | loss: 0.2283510\n",
      "\tspeed: 1.3163s/iter; left time: 3190.7831s\n",
      "\titers: 500, epoch: 8 | loss: 0.2755316\n",
      "\tspeed: 1.3161s/iter; left time: 3058.6641s\n",
      "\titers: 600, epoch: 8 | loss: 0.2858851\n",
      "\tspeed: 1.3153s/iter; left time: 2925.1764s\n",
      "\titers: 700, epoch: 8 | loss: 0.2509952\n",
      "\tspeed: 1.3124s/iter; left time: 2787.5540s\n",
      "\titers: 800, epoch: 8 | loss: 0.2344993\n",
      "\tspeed: 1.3167s/iter; left time: 2665.0554s\n",
      "\titers: 900, epoch: 8 | loss: 0.2150075\n",
      "\tspeed: 1.3123s/iter; left time: 2524.8438s\n",
      "Epoch: 8 running time: 20.60582449833552 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.2487196 Vali Loss: 0.4689870 Test Loss: 0.7732055\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.774609386920929, mae:0.616533637046814\n",
      "\n",
      "Time intermediate for GB dataset: 314.8255021095276 min.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     12\u001b[0m model_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--is_training\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--itr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m         ]\n\u001b[1;32m     35\u001b[0m int_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 37\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_run_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mrun_output\u001b[0;34m(path_to_run_file, model_arguments)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the script and capture the output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m---> 12\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there's any error in the process\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     13\u001b[0m model_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--is_training\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--itr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m         ]\n\u001b[1;32m     36\u001b[0m int_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_run_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\u001b[39;00m\n\u001b[1;32m     41\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mrun_output\u001b[0;34m(path_to_run_file, model_arguments)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the script and capture the output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m---> 12\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there's any error in the process\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"96\"\n",
    "model = \"Informer\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"96\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"512\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "140/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['FR_data.csv']\n",
    "num_cols = [\"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp='GB_data_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vol/cs-hu/riabchuv/hu-home/my_work/datasets/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GB_UKM_load_actual_entsoe_transparency</th>\n",
       "      <th>GB_UKM_solar_generation_actual</th>\n",
       "      <th>GB_UKM_wind_generation_actual</th>\n",
       "      <th>GB_UKM_wind_offshore_generation_actual</th>\n",
       "      <th>GB_UKM_wind_onshore_generation_actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-10-26 00:00:00</th>\n",
       "      <td>30680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>3463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 01:00:00</th>\n",
       "      <td>29218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5194.0</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>3383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 02:00:00</th>\n",
       "      <td>28016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4389.0</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>2633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 03:00:00</th>\n",
       "      <td>27402.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5104.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>3417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 04:00:00</th>\n",
       "      <td>27490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5206.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>3456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 19:00:00</th>\n",
       "      <td>49595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 20:00:00</th>\n",
       "      <td>46550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 21:00:00</th>\n",
       "      <td>42752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 22:00:00</th>\n",
       "      <td>38984.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 23:00:00</th>\n",
       "      <td>34184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     GB_UKM_load_actual_entsoe_transparency  \\\n",
       "date                                                          \n",
       "2015-10-26 00:00:00                                 30680.0   \n",
       "2015-10-26 01:00:00                                 29218.0   \n",
       "2015-10-26 02:00:00                                 28016.0   \n",
       "2015-10-26 03:00:00                                 27402.0   \n",
       "2015-10-26 04:00:00                                 27490.0   \n",
       "...                                                     ...   \n",
       "2015-11-04 19:00:00                                 49595.0   \n",
       "2015-11-04 20:00:00                                 46550.0   \n",
       "2015-11-04 21:00:00                                 42752.0   \n",
       "2015-11-04 22:00:00                                 38984.0   \n",
       "2015-11-04 23:00:00                                 34184.0   \n",
       "\n",
       "                     GB_UKM_solar_generation_actual  \\\n",
       "date                                                  \n",
       "2015-10-26 00:00:00                             0.0   \n",
       "2015-10-26 01:00:00                             0.0   \n",
       "2015-10-26 02:00:00                             0.0   \n",
       "2015-10-26 03:00:00                             0.0   \n",
       "2015-10-26 04:00:00                             0.0   \n",
       "...                                             ...   \n",
       "2015-11-04 19:00:00                             0.0   \n",
       "2015-11-04 20:00:00                             0.0   \n",
       "2015-11-04 21:00:00                             0.0   \n",
       "2015-11-04 22:00:00                             0.0   \n",
       "2015-11-04 23:00:00                             0.0   \n",
       "\n",
       "                     GB_UKM_wind_generation_actual  \\\n",
       "date                                                 \n",
       "2015-10-26 00:00:00                         5348.0   \n",
       "2015-10-26 01:00:00                         5194.0   \n",
       "2015-10-26 02:00:00                         4389.0   \n",
       "2015-10-26 03:00:00                         5104.0   \n",
       "2015-10-26 04:00:00                         5206.0   \n",
       "...                                            ...   \n",
       "2015-11-04 19:00:00                         1012.0   \n",
       "2015-11-04 20:00:00                         1246.0   \n",
       "2015-11-04 21:00:00                         1441.0   \n",
       "2015-11-04 22:00:00                         1430.0   \n",
       "2015-11-04 23:00:00                         1400.0   \n",
       "\n",
       "                     GB_UKM_wind_offshore_generation_actual  \\\n",
       "date                                                          \n",
       "2015-10-26 00:00:00                                  1885.0   \n",
       "2015-10-26 01:00:00                                  1810.0   \n",
       "2015-10-26 02:00:00                                  1756.0   \n",
       "2015-10-26 03:00:00                                  1687.0   \n",
       "2015-10-26 04:00:00                                  1749.0   \n",
       "...                                                     ...   \n",
       "2015-11-04 19:00:00                                   584.0   \n",
       "2015-11-04 20:00:00                                   792.0   \n",
       "2015-11-04 21:00:00                                   915.0   \n",
       "2015-11-04 22:00:00                                   912.0   \n",
       "2015-11-04 23:00:00                                   875.0   \n",
       "\n",
       "                     GB_UKM_wind_onshore_generation_actual  \n",
       "date                                                        \n",
       "2015-10-26 00:00:00                                 3463.0  \n",
       "2015-10-26 01:00:00                                 3383.0  \n",
       "2015-10-26 02:00:00                                 2633.0  \n",
       "2015-10-26 03:00:00                                 3417.0  \n",
       "2015-10-26 04:00:00                                 3456.0  \n",
       "...                                                    ...  \n",
       "2015-11-04 19:00:00                                  429.0  \n",
       "2015-11-04 20:00:00                                  455.0  \n",
       "2015-11-04 21:00:00                                  526.0  \n",
       "2015-11-04 22:00:00                                  518.0  \n",
       "2015-11-04 23:00:00                                  525.0  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(current_path, dp), index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.039925122261047365 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0024538 Vali Loss: nan Test Loss: 1.2172809\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.01469890276590983 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6911948 Vali Loss: nan Test Loss: 1.4431949\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.014795935153961182 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5788186 Vali Loss: nan Test Loss: 1.0701528\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.015081985791524252 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5235551 Vali Loss: nan Test Loss: 0.9722701\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.014554961522420248 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.5137975 Vali Loss: nan Test Loss: 0.9586014\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.01539984146753947 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4784191 Vali Loss: nan Test Loss: 0.9611190\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.013930594921112061 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4814504 Vali Loss: nan Test Loss: 1.0199240\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014625775814056396 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4706157 Vali Loss: nan Test Loss: 0.9661508\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.015251608689626057 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4747935 Vali Loss: nan Test Loss: 0.9857954\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014180195331573487 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4762641 Vali Loss: nan Test Loss: 1.0232698\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.985942542552948, mae:0.7991647124290466\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.014746411641438802 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0295902 Vali Loss: nan Test Loss: 1.0363106\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.015388592084248861 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6777777 Vali Loss: nan Test Loss: 1.3939192\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.014550344149271647 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5698880 Vali Loss: nan Test Loss: 0.9854409\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.014660712083180745 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5290706 Vali Loss: nan Test Loss: 0.8179453\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.015316299597422282 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.4899310 Vali Loss: nan Test Loss: 0.8191302\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.014206993579864501 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4454456 Vali Loss: nan Test Loss: 0.8206318\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.014676713943481445 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4726178 Vali Loss: nan Test Loss: 0.8455051\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014798065026601156 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4536124 Vali Loss: nan Test Loss: 0.8689371\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014613501230875651 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4576709 Vali Loss: nan Test Loss: 0.9006981\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014256656169891357 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4339121 Vali Loss: nan Test Loss: 0.8823724\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.8921732306480408, mae:0.7604851722717285\n",
      "\n",
      "Time intermediate for GB dataset: 0.9230153759320577 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.0379722277323405 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0133409 Vali Loss: nan Test Loss: 1.1593016\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.014988048871358236 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6685318 Vali Loss: nan Test Loss: 1.5033898\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.015134803454081218 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5853137 Vali Loss: nan Test Loss: 1.0170285\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.015229304631551107 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5230797 Vali Loss: nan Test Loss: 0.9165366\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.01482845942179362 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.5061147 Vali Loss: nan Test Loss: 0.9382831\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.014434333642323811 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4742088 Vali Loss: nan Test Loss: 0.9203134\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.014867182572682698 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4816395 Vali Loss: nan Test Loss: 0.9207461\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014206230640411377 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4701661 Vali Loss: nan Test Loss: 0.9398845\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014715186754862468 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4746773 Vali Loss: nan Test Loss: 0.9466402\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014221092065175375 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4755139 Vali Loss: nan Test Loss: 0.9503284\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.9485630989074707, mae:0.7849948406219482\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.014317051569620768 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0192896 Vali Loss: nan Test Loss: 1.0006683\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.016005825996398926 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6856293 Vali Loss: nan Test Loss: 1.3917358\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.015333712100982666 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5695837 Vali Loss: nan Test Loss: 1.0196233\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.014633981386820476 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5196934 Vali Loss: nan Test Loss: 0.8472352\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.015336012840270996 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.4939889 Vali Loss: nan Test Loss: 0.8318195\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.01478735605875651 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4583812 Vali Loss: nan Test Loss: 0.8314679\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.01449210246404012 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4641255 Vali Loss: nan Test Loss: 0.8144280\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014517529805501302 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4530064 Vali Loss: nan Test Loss: 0.8637968\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014745950698852539 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4684971 Vali Loss: nan Test Loss: 0.8901826\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.015699354807535808 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4408530 Vali Loss: nan Test Loss: 0.9042748\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.944560706615448, mae:0.7830029129981995\n",
      "\n",
      "Time intermediate for GB dataset: 0.935037616888682 min.\n",
      "Total time: 1.8580724199612935 min.\n"
     ]
    }
   ],
   "source": [
    "# 5.413828869660695 min. without test\n",
    "# 6.30 min with test\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "datasets = ['GB_data_small.csv', 'GB_data_small.csv']\n",
    "num_cols = [\"5\", \"5\"]\n",
    "pred_len = \"10\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "            \"--task_name\", \"long_term_forecast\",\n",
    "            \"--is_training\", \"1\", #True\n",
    "            \"--root_path\", current_path, #\"/content/my_work/datasets/\",\n",
    "            \"--data_path\", dataset,\n",
    "            #\"--train_epochs\", \"1\",\n",
    "            \"--model_id\", model_id,\n",
    "            \"--model\", model,\n",
    "            \"--data\", \"custom\", # This ensures a 70%,10%,20% train,val,test split see data_provider/data_loader.py\n",
    "            \"--features\", \"M\", # Multivariate\n",
    "            \"--seq_len\", \"10\",\n",
    "            \"--label_len\", \"5\",\n",
    "            \"--pred_len\", pred_len,\n",
    "            \"--e_layers\", \"3\", # Hyperparameters as in original model\n",
    "            \"--d_layers\", \"2\",\n",
    "            \"--factor\", \"3\",\n",
    "            #\"--gpu\", gpu_index,\n",
    "            # '--use_multi_gpu', \n",
    "            \"--enc_in\", num_cols[i],\n",
    "            \"--dec_in\", num_cols[i],\n",
    "            \"--c_out\", num_cols[i],\n",
    "            \"--des\", \"Exp\",\n",
    "            \"--itr\", \"2\",\n",
    "        ]\n",
    "    int_start = time.time()\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    # folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.86847895,   1.1777946 ,   1.0852624 ,   3.2318454 ,\n",
       "       769.5208    ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics\n",
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/metrics.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 10, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds\n",
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/pred.npy\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
