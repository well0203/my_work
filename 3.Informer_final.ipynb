{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connecting to CUDA Grünau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on GPU, because running it on CPU will cost a lot of time (during my experiments, it exeeds 8 hours).\n",
    "\n",
    "\n",
    "I do not recommend to run it in Google Colab, because it interrupts training process.\n",
    "\n",
    "If you are not going to use remote servers with multiple GPUs (e. g. Grünau servers), skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# For gruenau: For CUDA making it available this works:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of available GPUs:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the GPU you want to use (e.g., 0, 1, 2, etc.)\n",
    "# Choose that one that is not used by other processes\n",
    "gpu_index = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='test_20_10', model='Informer', data='custom', root_path='/vol/cs-hu/riabchuv/my_work/datasets/', data_path='GB_data_small.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=20, label_len=5, pred_len=10, fc_dropout=0.2, head_dropout=0.0, patch_len=3, stride=2, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=128, n_heads=16, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=3, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=1, batch_size=2, patience=1, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 139\n",
      "val 39\n",
      "test 15\n",
      "Epoch: 1 cost time: 3.1731815338134766\n",
      "Epoch: 1, Steps: 69 | Train Loss: 1.3197709 Vali Loss: 1.9088759 Test Loss: 2.2190468\n",
      "Validation loss decreased (inf --> 1.908876).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 15\n",
      "mse:2.2094132900238037, mae:1.2662930488586426, rse:1.3595283031463623\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to your script\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "# Define the command with arguments (without redirection to a log file)\n",
    "command = f\"\"\"\n",
    "python {script_path} \\\n",
    "  --random_seed 2021 \\\n",
    "  --is_training 1 \\\n",
    "  --root_path \"/vol/cs-hu/riabchuv/my_work/datasets/\" \\\n",
    "  --data_path \"GB_data_small.csv\" \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --model_id test_20_10 \\\n",
    "  --data \"custom\" \\\n",
    "  --features M \\\n",
    "  --seq_len 20 \\\n",
    "  --label_len 5 \\\n",
    "  --pred_len 10 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --train_epochs 1 \\\n",
    "  --patience 1 \\\n",
    "  --patch_len 3 \\\n",
    "  --stride 2 \\\n",
    "  --n_heads 16 \\\n",
    "  --d_model 128 \\\n",
    "  --d_ff 256 \\\n",
    "  --dropout 0.2 \\\n",
    "  --fc_dropout 0.2 \\\n",
    "  --head_dropout 0 \\\n",
    "  --itr 1 --batch_size 2 --learning_rate 0.0001\n",
    "\"\"\"\n",
    "\n",
    "# Execute the command and display output in the notebook\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='test_20_10', model='Informer', data='custom', root_path='/vol/cs-hu/riabchuv/my_work/datasets/', data_path='GB_data_small.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=20, label_len=5, pred_len=10, fc_dropout=0.2, head_dropout=0.0, patch_len=3, stride=2, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=128, n_heads=16, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=3, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=1, batch_size=2, patience=1, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 139\n",
      "val 39\n",
      "test 15\n",
      "Epoch: 1 cost time: 3.300485610961914\n",
      "Epoch: 1, Steps: 69 | Train Loss: 1.3197709 Vali Loss: 1.9088759 Test Loss: 2.2190468\n",
      "Validation loss decreased (inf --> 1.908876).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 15\n",
      "mse:2.2094132900238037, mae:1.2662930488586426, rse:1.3595283031463623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define dynamic variables\n",
    "pred_len = 10\n",
    "seq_len = 20\n",
    "model_id_name = \"test\"\n",
    "\n",
    "# Define the path to your script\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "# Define the log file path (dynamically use the variables)\n",
    "log_file = f\"logs/LongForecasting/Informer_{model_id_name}_{seq_len}_{pred_len}.log\"\n",
    "\n",
    "# Construct the command using the variables\n",
    "command = f\"\"\"\n",
    "python {script_path} \\\n",
    "  --random_seed 2021 \\\n",
    "  --is_training 1 \\\n",
    "  --root_path \"/vol/cs-hu/riabchuv/my_work/datasets/\" \\\n",
    "  --data_path \"GB_data_small.csv\" \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --model_id {model_id_name}_{seq_len}_{pred_len} \\\n",
    "  --data \"custom\" \\\n",
    "  --features M \\\n",
    "  --seq_len {seq_len} \\\n",
    "  --label_len 5 \\\n",
    "  --pred_len {pred_len} \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --train_epochs 1 \\\n",
    "  --patience 1 \\\n",
    "  --patch_len 3 \\\n",
    "  --stride 2 \\\n",
    "  --n_heads 16 \\\n",
    "  --d_model 128 \\\n",
    "  --d_ff 256 \\\n",
    "  --dropout 0.2 \\\n",
    "  --fc_dropout 0.2 \\\n",
    "  --head_dropout 0 \\\n",
    "  --itr 1 --batch_size 2 --learning_rate 0.0001 > {log_file}\n",
    "\"\"\"\n",
    "\n",
    "# Run the command\n",
    "os.system(command)\n",
    "\n",
    "# Optionally, display the content of the log file\n",
    "with open(log_file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4864095297137339"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(2.2094132900238037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'GB_data_small.csv'\n",
    "data = \"custom\"\n",
    "random_seed=2021\n",
    "model_name=\"Informer\"\n",
    "seq_len=20\n",
    "model_id_name=\"test\"\n",
    "pred_len=10\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "!python -u ./PatchTST-main/PatchTST_supervised/run_longExp.py \\\n",
    "  --random_seed 2021 \\\n",
    "  --is_training 1 \\\n",
    "  --root_path \"/vol/cs-hu/riabchuv/my_work/datasets/\" \\\n",
    "  --data_path \"GB_data_small.csv\" \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --model_id $model_id_name_$seq_len'_'$pred_len \\\n",
    "  --data \"custom\" \\\n",
    "  --features M \\\n",
    "  --seq_len 20 \\\n",
    "  --label_len 5 \\\n",
    "  --pred_len 10 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --train_epochs 1\\\n",
    "  --patience 1\\\n",
    "  --patch_len 3\\\n",
    "  --stride 2\\\n",
    "  --n_heads 16 \\\n",
    "  --d_model 128 \\\n",
    "  --d_ff 256 \\\n",
    "  --dropout 0.2\\\n",
    "  --fc_dropout 0.2\\\n",
    "  --head_dropout 0\\\n",
    "  --itr 1 --batch_size 2 --learning_rate 0.0001 >logs/LongForecasting/$model_name'_'$model_id_name'_'$seq_len'_'$pred_len.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'GB_data_small.csv'\n",
    "data = \"custom\"\n",
    "random_seed=2021\n",
    "model_name=\"PatchTST\"\n",
    "seq_len=20\n",
    "model_id_name=\"test\"\n",
    "pred_len=10\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "!python -u ./PatchTST-main/PatchTST_supervised/run_longExp.py \\\n",
    "  --random_seed 2021 \\\n",
    "  --is_training 1 \\\n",
    "  --root_path \"/vol/cs-hu/riabchuv/my_work/datasets/\" \\\n",
    "  --data_path \"GB_data_small.csv\" \\\n",
    "  --model_id 1 \\\n",
    "  --model \"PatchTST\" \\\n",
    "  --model_id $model_id_name_$seq_len'_'$pred_len \\\n",
    "  --data \"custom\" \\\n",
    "  --features M \\\n",
    "  --seq_len 20 \\\n",
    "  --label_len 5 \\\n",
    "  --pred_len 10 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --train_epochs 1\\\n",
    "  --patience 1\\\n",
    "  --patch_len 3\\\n",
    "  --stride 2\\\n",
    "  --n_heads 16 \\\n",
    "  --d_model 128 \\\n",
    "  --d_ff 256 \\\n",
    "  --dropout 0.2\\\n",
    "  --fc_dropout 0.2\\\n",
    "  --head_dropout 0\\\n",
    "  --itr 1 --batch_size 2 --learning_rate 0.0001 >logs/LongForecasting/$model_name'_'$model_id_name'_'$seq_len'_'$pred_len.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_longExp.py [-h] [--random_seed RANDOM_SEED] --is_training\n",
      "                      IS_TRAINING --model_id MODEL_ID --model MODEL --data\n",
      "                      DATA [--root_path ROOT_PATH] [--data_path DATA_PATH]\n",
      "                      [--features FEATURES] [--target TARGET] [--freq FREQ]\n",
      "                      [--checkpoints CHECKPOINTS] [--seq_len SEQ_LEN]\n",
      "                      [--label_len LABEL_LEN] [--pred_len PRED_LEN]\n",
      "                      [--fc_dropout FC_DROPOUT] [--head_dropout HEAD_DROPOUT]\n",
      "                      [--patch_len PATCH_LEN] [--stride STRIDE]\n",
      "                      [--padding_patch PADDING_PATCH] [--revin REVIN]\n",
      "                      [--affine AFFINE] [--subtract_last SUBTRACT_LAST]\n",
      "                      [--decomposition DECOMPOSITION]\n",
      "                      [--kernel_size KERNEL_SIZE] [--individual INDIVIDUAL]\n",
      "                      [--embed_type EMBED_TYPE] [--enc_in ENC_IN]\n",
      "                      [--dec_in DEC_IN] [--c_out C_OUT] [--d_model D_MODEL]\n",
      "                      [--n_heads N_HEADS] [--e_layers E_LAYERS]\n",
      "                      [--d_layers D_LAYERS] [--d_ff D_FF]\n",
      "                      [--moving_avg MOVING_AVG] [--factor FACTOR] [--distil]\n",
      "                      [--dropout DROPOUT] [--embed EMBED]\n",
      "                      [--activation ACTIVATION] [--output_attention]\n",
      "                      [--do_predict] [--num_workers NUM_WORKERS] [--itr ITR]\n",
      "                      [--train_epochs TRAIN_EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                      [--patience PATIENCE] [--learning_rate LEARNING_RATE]\n",
      "                      [--des DES] [--loss LOSS] [--lradj LRADJ]\n",
      "                      [--pct_start PCT_START] [--use_amp] [--use_gpu USE_GPU]\n",
      "                      [--gpu GPU] [--use_multi_gpu] [--devices DEVICES]\n",
      "                      [--test_flop]\n",
      "run_longExp.py: error: unrecognized arguments: --task_name long_term_forecast\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'GB_data_small.csv'\n",
    "\n",
    "!python -u ./PatchTST-main/PatchTST_supervised/run_longExp.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 5 \\\n",
    "  --dec_in 5 \\\n",
    "  --c_out 5 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder named \"datasets\" if it doesn't exist\n",
    "folder_name = \"datasets\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "df = pd.read_csv(\"./datasets/top_5_countries.csv\", index_col=0, parse_dates=True)\n",
    "# Reset index for Data Loader\n",
    "df.reset_index(inplace=True)\n",
    "df = df.iloc[:,:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "\n",
    "# Split and save the datasets\n",
    "for country_prefix in top_5_countries:\n",
    "    # Filter columns with the specified prefix\n",
    "    country_columns = [col for col in df.columns if col.startswith(country_prefix)]\n",
    "    \n",
    "    # Insert the date column at the beginning of every dataset\n",
    "    country_columns.insert(0,\"date\")\n",
    "    country_df = df[country_columns]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"./datasets/{country_prefix}_data.csv\"\n",
    "    country_df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=28321\n",
    "val=6577\n",
    "test=8713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "def split_scale_dataset(data, train_size, val_size, test_size=None):\n",
    "\n",
    "    \"\"\"\n",
    "    data (pd.DataFrame): Dataframe with time series data.\n",
    "    train_size, test_size, val_size (int): number of days in train, \n",
    "                                            test and validation datasets.\n",
    "\n",
    "    return: Scaled datasets\n",
    "   \n",
    "    \n",
    "    num_train = train_size*24\n",
    "    if test_size is not None:\n",
    "        num_test = test_size*24\n",
    "    num_vali = val_size*24\n",
    "\"\"\"\n",
    "    num_train = train_size\n",
    "    num_vali = val_size\n",
    "    train_data = data.iloc[:num_train] # 0, a-1\n",
    "    vali_data = data.iloc[num_train: num_train + num_vali] # a, a+b-1\n",
    "    test_data = data.iloc[num_train + num_vali:] # a+b\n",
    "\n",
    "    assert(len(data) == len(train_data) + len(test_data) + len(vali_data))\n",
    "\n",
    "    print(f'{len(train_data)} observations in the train dataset.\\n {len(test_data)} observations in the test dataset.\\n {len(vali_data)} observations in the validation dataset.')\n",
    "\n",
    "    # initialize scaler object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scale data\n",
    "    train_data_sc = scaler.fit_transform(train_data)\n",
    "    vali_data_sc = scaler.transform(vali_data)\n",
    "    test_data_sc = scaler.transform(test_data)\n",
    "\n",
    "    train_data_sc = pd.DataFrame(train_data_sc, columns=train_data.columns, index=train_data.index)\n",
    "    vali_data_sc = pd.DataFrame(vali_data_sc, columns=vali_data.columns, index=vali_data.index)\n",
    "    test_data_sc = pd.DataFrame(test_data_sc, columns=test_data.columns, index=test_data.index)\n",
    "\n",
    "    return train_data_sc, vali_data_sc, test_data_sc\n",
    "\n",
    "#time_series = pd.read_csv(\"./datasets/df_most_important_columns.csv\", index_col=0, parse_dates=True)\n",
    "#train, vali, test = split_scale_dataset(data=time_series, train_size=train, val_size=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(train[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28321.00</td>\n",
       "      <td>28321.00</td>\n",
       "      <td>28321.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.43</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.16</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                            28321.00                    28321.00   \n",
       "mean                                 0.00                       -0.00   \n",
       "std                                  1.00                        1.00   \n",
       "min                                 -2.43                       -0.64   \n",
       "25%                                 -0.84                       -0.64   \n",
       "50%                                 -0.03                       -0.63   \n",
       "75%                                  0.89                        0.37   \n",
       "max                                  2.16                        3.94   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                   28321.00  \n",
       "mean                       -0.00  \n",
       "std                         1.00  \n",
       "min                        -1.24  \n",
       "25%                        -0.77  \n",
       "50%                        -0.29  \n",
       "75%                         0.52  \n",
       "max                         3.99  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(vali[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6577.00</td>\n",
       "      <td>6577.00</td>\n",
       "      <td>6577.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.05</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.82</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.91</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.09</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                             6577.00                     6577.00   \n",
       "mean                                 0.03                        0.27   \n",
       "std                                  0.97                        1.29   \n",
       "min                                 -2.05                       -0.64   \n",
       "25%                                 -0.82                       -0.64   \n",
       "50%                                 -0.01                       -0.56   \n",
       "75%                                  0.91                        1.05   \n",
       "max                                  2.09                        4.16   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                    6577.00  \n",
       "mean                        0.14  \n",
       "std                         1.03  \n",
       "min                        -1.21  \n",
       "25%                        -0.64  \n",
       "50%                        -0.14  \n",
       "75%                         0.65  \n",
       "max                         4.24  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='DE_load_actual_entsoe_transparency', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGxCAYAAACEFXd4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA35klEQVR4nO3de3RU5b3/8U8CuUEygZgrmkkQkSQoBAHJYC2ggUgplcrp8YYGRao0oBillP6QAIpxaRVvAUuXQG2leNRqFSkKUdRCQASxXBIqFhyLSXDgwAAJSUie3x8eZjlyzWRgJjvv11qzlnvv53n2d88O5pM9z94TYowxAgAAsKjQQBcAAABwLhF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApbUPdAHBoKmpSd98841iYmIUEhIS6HIAAMBZMMbo0KFD6tKli0JDT339hrAj6ZtvvlFqamqgywAAAD74+uuvddFFF51yO2FHUkxMjKTv3iybzRbgagAAwNlwu91KTU31/B4/FcKO5PnoymazEXYAAGhlzjQFhQnKAADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0toHugCgLXI6nXK5XC0aIz4+Xna73U8VAYB1EXaA88zpdCojI1O1tTUtGiciIlKvv/6aUlJSfOpPWALQVhB2gPPM5XKptrZGA+4ski0l3acxvv3ic23+n2f005/+1Oc6oqI6qKKinMADwPIIO0CA2FLSFWfv4VNfd+VuSUbZt0xVQtcMn/qvXzhLLpeLsAPA8gg7QCsWnWj3OTABQFvB3VgAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDS2ge6AACAtTidTrlcLp/7x8fHy263+7EitHWEHQCA3zidTmVkZKq2tsbnMaKiOqiiopzAA78JaNiZOXOmZs2a5bWuR48eqqiokCQdPXpUDzzwgJYuXaq6ujrl5eVp3rx5SkpK8rR3Op2aMGGCPvjgA0VHRys/P1/FxcVq354cB5xJeXl5i/rzFzh+yOVyqba2RgPuLJItJb3Z/d2Vu7V+4Sy5XC5+tuA3AU8EPXv21KpVqzzL3w8p999/v9555x29+uqrio2N1cSJE3XDDTdozZo1kqTGxkaNGDFCycnJWrt2rSorK3X77bcrLCxMjz766Hk/FqC1qD24T1KIxowZ06Jx+Ascp2JLSVecvUegywAkBUHYad++vZKTk09Yf/DgQb344otasmSJrrnmGknSokWLlJmZqXXr1iknJ0fvvfeetm/frlWrVikpKUnZ2dl6+OGHNXXqVM2cOVPh4eHn+3CAVqGh5pAko+xbpiqha4ZPY/AXOIDWIuBh54svvlCXLl0UGRkph8Oh4uJi2e12bdy4UQ0NDcrNzfW0zcjIkN1uV1lZmXJyclRWVqbLL7/c62OtvLw8TZgwQdu2bVOfPn1Ous+6ujrV1dV5lt1u97k7QCCIRSfa+esbgOUFNOwMGDBAixcvVo8ePVRZWalZs2bp6quv1tatW1VVVaXw8HB16tTJq09SUpKqqqokSVVVVV5B5/j249tOpbi4+IS5QgCA4MF8MvhTQMPO8OHDPf/dq1cvDRgwQGlpafqf//kfRUVFnbP9Tps2TYWFhZ5lt9ut1NTUc7Y/AMDZYT4ZzoWAf4z1fZ06ddKll16qnTt3aujQoaqvr9eBAwe8ru5UV1d75vgkJyfrk08+8Rqjurras+1UIiIiFBER4f8DAAC0CPPJcC4EVdg5fPiwvvzyS912223q27evwsLCVFpaqtGjR0uSduzYIafTKYfDIUlyOByaM2eO9u7dq8TEREnSypUrZbPZlJWVFbDjAAC0DPPJ4E8BDTsPPvigRo4cqbS0NH3zzTcqKipSu3btdPPNNys2Nlbjxo1TYWGh4uLiZLPZNGnSJDkcDuXk5EiShg0bpqysLN122216/PHHVVVVpenTp6ugoIArNwAAQFKAw85//vMf3Xzzzdq3b58SEhL0ox/9SOvWrVNCQoIkae7cuQoNDdXo0aO9Hip4XLt27bRs2TJNmDBBDodDHTt2VH5+vmbPnh2oQwIAAEEmoGFn6dKlp90eGRmpkpISlZSUnLJNWlqali9f7u/SAACARfCt5wAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNKC6ruxAADwl/Ly8hb1j4+P54tELYKwAwCwlNqD+ySFaMyYMS0aJyqqgyoqygk8FkDYAQBYSkPNIUlG2bdMVULXDJ/GcFfu1vqFs+RyuQg7FkDYAQBYUnSiXXH2HoEuA0GACcoAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSuBsLaCan0ymXy+Vz/5Y+6AwA0DyEHaAZnE6nMjIyVVtb0+KxGurq/VARAOBMCDtAM7hcLtXW1mjAnUWypaT7NEblljJtfWuBjh075t/iAD/gyiWsiLAD+MCWku7zw8rclbv9WwzgJ1y5hFURdgAAkrhyCesi7AAAvHDlElbDrecAAMDSCDsAAMDS+BgLQIu09O6b+Ph42e12P1UDACci7ADwSe3BfZJCNGbMmBaNExXVQRUV5QQeAOcMYQeATxpqDkkyyr5lqhK6Zvg0hrtyt9YvnCWXy0XYAXDOEHYAtEh0ot3nO3cA4HxggjIAALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA07sYCEHAteTAhDyUEcCaEHQAB448HE/JQQgBnQtgBEDAtfTAhDyUEcDYIOwACjgcTAjiXmKAMAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsLWjCzmOPPaaQkBBNnjzZs+7o0aMqKCjQBRdcoOjoaI0ePVrV1dVe/ZxOp0aMGKEOHTooMTFRU6ZM0bFjx85z9QAAIFi1D3QBkrRhwwb9/ve/V69evbzW33///XrnnXf06quvKjY2VhMnTtQNN9ygNWvWSJIaGxs1YsQIJScna+3ataqsrNTtt9+usLAwPfroo4E4FAAIGKfTKZfL5XP/8vJyP1YDBI+Ah53Dhw/r1ltv1R/+8Ac98sgjnvUHDx7Uiy++qCVLluiaa66RJC1atEiZmZlat26dcnJy9N5772n79u1atWqVkpKSlJ2drYcfflhTp07VzJkzFR4eHqjDAoDzyul0KiMjU7W1NS0eq6Gu3g8VAcEj4GGnoKBAI0aMUG5urlfY2bhxoxoaGpSbm+tZl5GRIbvdrrKyMuXk5KisrEyXX365kpKSPG3y8vI0YcIEbdu2TX369DmvxwIAgeJyuVRbW6MBdxbJlpLu0xiVW8q09a0FTAWA5QQ07CxdulSbNm3Shg0bTthWVVWl8PBwderUyWt9UlKSqqqqPG2+H3SObz++7VTq6upUV1fnWXa73b4eAgAEFVtKuuLsPXzq667c7d9igCARsAnKX3/9te677z69/PLLioyMPK/7Li4uVmxsrOeVmpp6XvcPAADOn4Bd2dm4caP27t2rK664wrOusbFRH330kZ5//nm9++67qq+v14EDB7yu7lRXVys5OVmSlJycrE8++cRr3ON3ax1vczLTpk1TYWGhZ9ntdhN4AAAnaMmk7fj4eNntdj9WA18FLOxce+212rJli9e6O+64QxkZGZo6dapSU1MVFham0tJSjR49WpK0Y8cOOZ1OORwOSZLD4dCcOXO0d+9eJSYmSpJWrlwpm82mrKysU+47IiJCERER5+jIAACtXe3BfZJCNGbMGJ/HiIrqoIqKcgJPEAhY2ImJidFll13mta5jx4664IILPOvHjRunwsJCxcXFyWazadKkSXI4HMrJyZEkDRs2TFlZWbrtttv0+OOPq6qqStOnT1dBQQFhBgDgs4aaQ5KMsm+ZqoSuGc3u767crfULZ8nlchF2gkDA78Y6nblz5yo0NFSjR49WXV2d8vLyNG/ePM/2du3aadmyZZowYYIcDoc6duyo/Px8zZ49O4BVAwCsIjrR7vOEbwSPoAo7q1ev9lqOjIxUSUmJSkpKTtknLS1Ny5cvP8eVAQCA1ipovi4CAADgXCDsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASwuqLwIFAF+Ul5e3qH98fLzsdrufqgEQbAg7AFqt2oP7JIVozJgxLRonKqqDKirKCTyARRF2ALRaDTWHJBll3zJVCV0zfBrDXblb6xfOksvlIuwAFkXYAdDqRSfaFWfvEegyAAQpJigDAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABL47uxAEBSeXl5i/rHx8fzRaJAkCLsoE1xOp1yuVw+92/pL0QEn9qD+ySFaMyYMS0aJyqqgyoqygk8QBAi7KDNcDqdysjIVG1tTYvHaqir90NFCAYNNYckGWXfMlUJXTN8GsNduVvrF86Sy+Ui7ABBiLCDNsPlcqm2tkYD7iySLSXdpzEqt5Rp61sLdOzYMf8Wh4CLTrQrzt4j0GUAOAcIO2hzbCnpPv9Sc1fu9m8xAIBzjruxAACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApfGt5wDgJ+Xl5T73jY+Pl91u92M1AI4j7ABAC9Ue3CcpRGPGjPF5jKioDqqoKCfwAOcAYQcAWqih5pAko+xbpiqha0az+7srd2v9wllyuVyEHeAcIOwAgJ9EJ9oVZ+8R6DIA/AATlAEAgKURdgAAgKURdgAAgKX5FHYuvvhi7du374T1Bw4c0MUXX9ziogAAAPzFp7Cze/duNTY2nrC+rq5Oe/bsaXFRAAAA/tKsu7Heeustz3+/++67io2N9Sw3NjaqtLRU6enpfisOANqSljyUsCV9AatrVtgZNWqUJCkkJET5+fle28LCwpSenq4nn3zyrMebP3++5s+fr927d0uSevbsqRkzZmj48OGSpKNHj+qBBx7Q0qVLVVdXp7y8PM2bN09JSUmeMZxOpyZMmKAPPvhA0dHRys/PV3Fxsdq35656AK2DPx5KeFxDXX3LCwIsplmJoKmpSZLUtWtXbdiwQfHx8S3a+UUXXaTHHntM3bt3lzFGf/zjH3X99dfrs88+U8+ePXX//ffrnXfe0auvvqrY2FhNnDhRN9xwg9asWSPpu6tJI0aMUHJystauXavKykrdfvvtCgsL06OPPtqi2gDgfGnpQwklqXJLmba+tUDHjh3zb3GABfh0+WPXrl1+2fnIkSO9lufMmaP58+dr3bp1uuiii/Tiiy9qyZIluuaaayRJixYtUmZmptatW6ecnBy999572r59u1atWqWkpCRlZ2fr4Ycf1tSpUzVz5kyFh4f7pU4AOB9a8lBCd+Vu/xYDWIjPn/WUlpaqtLRUe/fu9VzxOW7hwoXNHq+xsVGvvvqqjhw5IofDoY0bN6qhoUG5ubmeNhkZGbLb7SorK1NOTo7Kysp0+eWXe32slZeXpwkTJmjbtm3q06fPSfdVV1enuro6z7Lb7W52vQAAoHXwKezMmjVLs2fPVr9+/ZSSkqKQkBCfC9iyZYscDoeOHj2q6OhovfHGG8rKytLmzZsVHh6uTp06ebVPSkpSVVWVJKmqqsor6BzffnzbqRQXF2vWrFk+14zAcTqdcrlcPvVlAicAtE0+hZ0XXnhBixcv1m233dbiAnr06KHNmzfr4MGDeu2115Sfn68PP/ywxeOezrRp01RYWOhZdrvdSk1NPaf7RMs5nU5lZGSqtramReMwgRMA2hafwk59fb0GDhzolwLCw8N1ySWXSJL69u2rDRs26JlnntGNN96o+vp6HThwwOvqTnV1tZKTkyVJycnJ+uSTT7zGq66u9mw7lYiICEVERPilfpw/LpdLtbU1GnBnkWwp6c3uzwROAGibfAo7d911l5YsWaKHHnrI3/WoqalJdXV16tu3r8LCwlRaWqrRo0dLknbs2CGn0ymHwyFJcjgcmjNnjvbu3avExERJ0sqVK2Wz2ZSVleX32hAcbCnpPk3iZAInALRNPoWdo0ePasGCBVq1apV69eqlsLAwr+1PPfXUWY0zbdo0DR8+XHa7XYcOHdKSJUu0evVqzwMLx40bp8LCQsXFxclms2nSpElyOBzKycmRJA0bNkxZWVm67bbb9Pjjj6uqqkrTp09XQUEBV24AAIAkH8POP//5T2VnZ0uStm7d6rWtOZOV9+7dq9tvv12VlZWKjY1Vr1699O6772ro0KGSpLlz5yo0NFSjR4/2eqjgce3atdOyZcs0YcIEORwOdezYUfn5+Zo9e7YvhwUAACzIp7DzwQcf+GXnL7744mm3R0ZGqqSkRCUlJadsk5aWpuXLl/ulHgAAYD18pwIAAOdISx95ER8fL7vd7qdq2i6fws6QIUNO+3HV+++/73NBAAC0dv76vrOoqA6qqCgn8LSQT2Hn+Hyd4xoaGrR582Zt3br1hC8IBQCgrfHH9525K3dr/cJZcrlchJ0W8inszJ0796TrZ86cqcOHD7eoIAAArKIl33cG/wn152Bjxozx6XuxAAAAzhW/hp2ysjJFRkb6c0gAAIAW8eljrBtuuMFr2RijyspKffrpp+fkqcoAAAC+8insxMbGei2HhoaqR48emj17toYNG+aXwgAAAPzBp7CzaNEif9cBAABwTrTooYIbN270PDCpZ8+e6tOnj1+KAgAA8Befws7evXt10003afXq1erUqZMk6cCBAxoyZIiWLl2qhIQEf9YIAADgM5/uxpo0aZIOHTqkbdu2af/+/dq/f7+2bt0qt9ute++91981AgAA+MynKzsrVqzQqlWrlJmZ6VmXlZWlkpISJigDAICg4tOVnaamJoWFhZ2wPiwsTE1NTS0uCgAAwF98CjvXXHON7rvvPn3zzTeedXv27NH999+va6+91m/FAQAAtJRPYef555+X2+1Wenq6unXrpm7duqlr165yu9167rnn/F0jAACAz3yas5OamqpNmzZp1apVqqiokCRlZmYqNzfXr8UBAAC0VLOu7Lz//vvKysqS2+1WSEiIhg4dqkmTJmnSpEnq37+/evbsqY8//vhc1QoAANBszQo7Tz/9tMaPHy+bzXbCttjYWN1999166qmn/FYcAABASzUr7Hz++ee67rrrTrl92LBh2rhxY4uLAgAA8JdmhZ3q6uqT3nJ+XPv27fXtt9+2uCgAAAB/aVbYufDCC7V169ZTbv/nP/+plJSUFhcFAADgL80KOz/5yU/00EMP6ejRoydsq62tVVFRkX7605/6rTgAAICWatat59OnT9df//pXXXrppZo4caJ69OghSaqoqFBJSYkaGxv1//7f/zsnhQIAAPiiWWEnKSlJa9eu1YQJEzRt2jQZYyRJISEhysvLU0lJiZKSks5JoQAAAL5o9kMF09LStHz5cv3v//6vdu7cKWOMunfvrs6dO5+L+gAAAFrEpycoS1Lnzp3Vv39/f9YCAADgdz59NxYAAEBrQdgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACW1j7QBaDtcDqdcrlcPvcvLy/3YzUAgLaCsIPzwul0KiMjU7W1NS0eq6Gu3g8VAQDaCsIOzguXy6Xa2hoNuLNItpR0n8ao3FKmrW8t0LFjx/xbHADA0gg7OK9sKemKs/fwqa+7crd/iwEAtAlMUAYAAJbGlR0AAIJYS2/OiI+Pl91u91M1rRNhBwCAIFR7cJ+kEI0ZM6ZF40RFdVBFRXmbDjyEHQAAglBDzSFJRtm3TFVC1wyfxnBX7tb6hbPkcrkIOwAAIDhFJ9p9vrED3wnoBOXi4mL1799fMTExSkxM1KhRo7Rjxw6vNkePHlVBQYEuuOACRUdHa/To0aqurvZq43Q6NWLECHXo0EGJiYmaMmUKtycDAABJAQ47H374oQoKCrRu3TqtXLlSDQ0NGjZsmI4cOeJpc//99+vtt9/Wq6++qg8//FDffPONbrjhBs/2xsZGjRgxQvX19Vq7dq3++Mc/avHixZoxY0YgDgkAAASZgH6MtWLFCq/lxYsXKzExURs3btSPf/xjHTx4UC+++KKWLFmia665RpK0aNEiZWZmat26dcrJydF7772n7du3a9WqVUpKSlJ2drYefvhhTZ06VTNnzlR4eHggDg0AAASJoHrOzsGDByVJcXFxkqSNGzeqoaFBubm5njYZGRmy2+0qKyuTJJWVlenyyy9XUlKSp01eXp7cbre2bdt2HqsHAADBKGgmKDc1NWny5Mm66qqrdNlll0mSqqqqFB4erk6dOnm1TUpKUlVVlafN94PO8e3Ht51MXV2d6urqPMtut9tfhwEAAIJM0FzZKSgo0NatW7V06dJzvq/i4mLFxsZ6Xqmpqed8nwAAIDCCIuxMnDhRy5Yt0wcffKCLLrrIsz45OVn19fU6cOCAV/vq6molJyd72vzw7qzjy8fb/NC0adN08OBBz+vrr7/249EAAIBgEtCwY4zRxIkT9cYbb+j9999X165dvbb37dtXYWFhKi0t9azbsWOHnE6nHA6HJMnhcGjLli3au3evp83KlStls9mUlZV10v1GRETIZrN5vQAAgDUFdM5OQUGBlixZor/97W+KiYnxzLGJjY1VVFSUYmNjNW7cOBUWFiouLk42m02TJk2Sw+FQTk6OJGnYsGHKysrSbbfdpscff1xVVVWaPn26CgoKFBEREcjDAwAAQSCgYWf+/PmSpMGDB3utX7RokcaOHStJmjt3rkJDQzV69GjV1dUpLy9P8+bN87Rt166dli1bpgkTJsjhcKhjx47Kz8/X7Nmzz9dhAACAIBbQsGOMOWObyMhIlZSUqKSk5JRt0tLStHz5cn+WBgAALCIoJigDAACcK4QdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgae0DXQBaB6fTKZfL5XP/8vJyP1YDAMDZI+zgjJxOpzIyMlVbW9PisRrq6v1QEQAAZ4+wgzNyuVyqra3RgDuLZEtJ92mMyi1l2vrWAh07dsy/xQEAcAaEHZw1W0q64uw9fOrrrtzt32IAADhLTFAGAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACWRtgBAACW1j7QBQAAgHOrvLzc577x8fGy2+1+rOb8I+wAAGBRtQf3SQrRmDFjfB4jKqqDKirKW3XgIewAAGBRDTWHJBll3zJVCV0zmt3fXblb6xfOksvlIuwAAIDgFZ1oV5y9R6DLCBgmKAMAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsLaNj56KOPNHLkSHXp0kUhISF68803vbYbYzRjxgylpKQoKipKubm5+uKLL7za7N+/X7feeqtsNps6deqkcePG6fDhw+fxKAAAQDALaNg5cuSIevfurZKSkpNuf/zxx/Xss8/qhRde0Pr169WxY0fl5eXp6NGjnja33nqrtm3bppUrV2rZsmX66KOP9Mtf/vJ8HQIAAAhy7QO58+HDh2v48OEn3WaM0dNPP63p06fr+uuvlyS99NJLSkpK0ptvvqmbbrpJ5eXlWrFihTZs2KB+/fpJkp577jn95Cc/0e9+9zt16dLlvB1LsHM6nXK5XD71LS8v93M1AACcPwENO6eza9cuVVVVKTc317MuNjZWAwYMUFlZmW666SaVlZWpU6dOnqAjSbm5uQoNDdX69ev185///KRj19XVqa6uzrPsdrvP3YEEAafTqYyMTNXW1rRonIa6ej9VBADA+RO0YaeqqkqSlJSU5LU+KSnJs62qqkqJiYle29u3b6+4uDhPm5MpLi7WrFmz/Fxx8HK5XKqtrdGAO4tkS0lvdv/KLWXa+tYCHTt2zP/FAQBwjgVt2DmXpk2bpsLCQs+y2+1WampqACs6P2wp6Yqz92h2P3flbv8XAwDAeRK0t54nJydLkqqrq73WV1dXe7YlJydr7969XtuPHTum/fv3e9qcTEREhGw2m9cLAABYU9CGna5duyo5OVmlpaWedW63W+vXr5fD4ZAkORwOHThwQBs3bvS0ef/999XU1KQBAwac95oBAEDwCejHWIcPH9bOnTs9y7t27dLmzZsVFxcnu92uyZMn65FHHlH37t3VtWtXPfTQQ+rSpYtGjRolScrMzNR1112n8ePH64UXXlBDQ4MmTpyom266iTuxAACApACHnU8//VRDhgzxLB+fR5Ofn6/Fixfr17/+tY4cOaJf/vKXOnDggH70ox9pxYoVioyM9PR5+eWXNXHiRF177bUKDQ3V6NGj9eyzz573YwEAAMEpoGFn8ODBMsaccntISIhmz56t2bNnn7JNXFyclixZci7KAwAAFhC0c3YAAAD8gbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsrX2gC8CZOZ1OuVwun/uXl5f7sRoAAFoXwk6QczqdysjIVG1tTYvHaqir90NFAAC0LoSdIOdyuVRbW6MBdxbJlpLu0xiVW8q09a0FOnbsmH+LAwCgFSDstBK2lHTF2Xv41Nddudu/xQAA0IowQRkAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaT1AGAACn1dIvlI6Pj5fdbvdTNc1H2AEAACdVe3CfpBCNGTOmReNERXVQRUV5wAIPYQcAAJxUQ80hSUbZt0xVQtcMn8ZwV+7W+oWz5HK5CDsAACA4RSfaff4y6mDABGUAAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBp7QNdgNU5nU65XC6f+5eXl/uxGgAA2h7CzjnkdDqVkZGp2tqaFo/VUFfvh4oAAGh7CDvnkMvlUm1tjQbcWSRbSrpPY1RuKdPWtxbo2LFj/i0OAIA2grBzHthS0hVn7+FTX3flbv8WAwBAG8MEZQAAYGmWCTslJSVKT09XZGSkBgwYoE8++STQJQEAgCBgibDzyiuvqLCwUEVFRdq0aZN69+6tvLw87d27N9ClAQCAALNE2Hnqqac0fvx43XHHHcrKytILL7ygDh06aOHChYEuDQAABFirDzv19fXauHGjcnNzPetCQ0OVm5ursrKyAFYGAACCQau/G8vlcqmxsVFJSUle65OSklRRUXHSPnV1daqrq/MsHzx4UJLkdrv9Wtvhw4clSfu/2qFjdbU+jeGu/EqSdHDPFwprHxKQMYKhBn+MEQw1BMsYwVCDP8YIhhr8MUYw1BAsYwRDDf4YIxhq8McYfqmhyinpu9+J/v49e3w8Y8zpG5pWbs+ePUaSWbt2rdf6KVOmmCuvvPKkfYqKiowkXrx48eLFi5cFXl9//fVps0Krv7ITHx+vdu3aqbq62mt9dXW1kpOTT9pn2rRpKiws9Cw3NTVp//79uuCCCxQS4ltyDRS3263U1FR9/fXXstlsgS4HJ8E5ah04T60D56l1OF/nyRijQ4cOqUuXLqdt1+rDTnh4uPr27avS0lKNGjVK0nfhpbS0VBMnTjxpn4iICEVERHit69Sp0zmu9Nyy2Wz8ww9ynKPWgfPUOnCeWofzcZ5iY2PP2KbVhx1JKiwsVH5+vvr166crr7xSTz/9tI4cOaI77rgj0KUBAIAAs0TYufHGG/Xtt99qxowZqqqqUnZ2tlasWHHCpGUAAND2WCLsSNLEiRNP+bGVlUVERKioqOiEj+UQPDhHrQPnqXXgPLUOwXaeQow50/1aAAAArVerf6ggAADA6RB2AACApRF2AACApRF2LGL37t0aN26cunbtqqioKHXr1k1FRUWqr68PdGn4gTlz5mjgwIHq0KFDq3++k5WUlJQoPT1dkZGRGjBggD755JNAl4Tv+eijjzRy5Eh16dJFISEhevPNNwNdEn6guLhY/fv3V0xMjBITEzVq1Cjt2LEj0GVJIuxYRkVFhZqamvT73/9e27Zt09y5c/XCCy/ot7/9baBLww/U19frF7/4hSZMmBDoUvB/XnnlFRUWFqqoqEibNm1S7969lZeXp7179wa6NPyfI0eOqHfv3iopKQl0KTiFDz/8UAUFBVq3bp1WrlyphoYGDRs2TEeOHAl0adyNZWVPPPGE5s+fr3//+9+BLgUnsXjxYk2ePFkHDhwIdClt3oABA9S/f389//zzkr57CntqaqomTZqk3/zmNwGuDj8UEhKiN954w/PUfASnb7/9VomJifrwww/14x//OKC1cGXHwg4ePKi4uLhAlwEEtfr6em3cuFG5ubmedaGhocrNzVVZWVkAKwNat4MHD0pSUPweIuxY1M6dO/Xcc8/p7rvvDnQpQFBzuVxqbGw84YnrSUlJqqqqClBVQOvW1NSkyZMn66qrrtJll10W6HIIO8HuN7/5jUJCQk77qqio8OqzZ88eXXfddfrFL36h8ePHB6jytsWX8wQAVlVQUKCtW7dq6dKlgS5FkoW+LsKqHnjgAY0dO/a0bS6++GLPf3/zzTcaMmSIBg4cqAULFpzj6nBcc88Tgkd8fLzatWun6upqr/XV1dVKTk4OUFVA6zVx4kQtW7ZMH330kS666KJAlyOJsBP0EhISlJCQcFZt9+zZoyFDhqhv375atGiRQkO5cHe+NOc8IbiEh4erb9++Ki0t9Ux4bWpqUmlpaZv8vj3AV8YYTZo0SW+88YZWr16trl27BrokD8KORezZs0eDBw9WWlqafve73+nbb7/1bOOv0+DidDq1f/9+OZ1ONTY2avPmzZKkSy65RNHR0YEtro0qLCxUfn6++vXrpyuvvFJPP/20jhw5ojvuuCPQpeH/HD58WDt37vQs79q1S5s3b1ZcXJzsdnsAK8NxBQUFWrJkif72t78pJibGM+ctNjZWUVFRAa2NW88tYvHixaf8HzOnOLiMHTtWf/zjH09Y/8EHH2jw4MHnvyBIkp5//nk98cQTqqqqUnZ2tp599lkNGDAg0GXh/6xevVpDhgw5YX1+fr4WL158/gvCCUJCQk66ftGiRWf8mP9cI+wAAABLY1IHAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIO4AerV69WSEiIDhw4YIn9nGtWOQ4ArQNhB5Y3duxYhYSEKCQkRGFhYUpKStLQoUO1cOFCNTU1edqlp6d72n3/9dhjjwWw+sDZvXu3QkJCPN/d1ZoF47HMnDlT2dnZgS4DaBP4IlC0Cdddd50WLVqkxsZGVVdXa8WKFbrvvvv02muv6a233lL79t/9U5g9e7bGjx/v1TcmJiYQJQOSpIaGBoWFhQW6DJ8YY9TY2Oj59wUECld20CZEREQoOTlZF154oa644gr99re/1d/+9jf9/e9/9/oSwZiYGCUnJ3u9Onbs6NM+X3/9dfXs2VMRERFKT0/Xk08+6bX9T3/6k/r16+fZ5y233KK9e/d6tVm+fLkuvfRSRUVFaciQIdq9e/dZ73/fvn26+eabdeGFF6pDhw66/PLL9Ze//MWrTVNTkx5//HFdcsklioiIkN1u15w5cyRJXbt2lST16dNHISEhni8pHTx4sCZPnuw1zqhRo7y+6O9sjq05/vGPf+jqq69WVFSUUlNTde+99+rIkSOe7enp6Xr00Ud15513KiYmRna7XQsWLPBsP9WxrF69WldeeaU6duyoTp066aqrrtJXX33l6Td//nx169ZN4eHh6tGjh/70pz951XXgwAHdddddSkhIkM1m0zXXXKPPP//8jMezePFizZo1S59//rnnCuLxn8OQkBDNnz9fP/vZz9SxY0fNmTNHjY2NGjdunLp27aqoqCj16NFDzzzzjNeYY8eO1ahRo/S73/1OKSkpuuCCC1RQUKCGhgZPm3nz5ql79+6KjIxUUlKS/uu//suzbfDgwZo4caImTpyo2NhYxcfH66GHHvL6IuEzndfjH0/+/e9/V9++fRUREaF//OMfampqUnFxsaf+3r1767XXXjuhX2lpqfr166cOHTpo4MCB2rFjh9cxvv322+rfv78iIyMVHx+vn//855K++yPlsssuO+F9zs7O1kMPPXTG84E2wAAWl5+fb66//vqTbuvdu7cZPny4McaYtLQ0M3fuXJ/28cEHHxhJ5n//93+NMcZ8+umnJjQ01MyePdvs2LHDLFq0yERFRZlFixZ5+rz44otm+fLl5ssvvzRlZWXG4XB4ajHGGKfTaSIiIkxhYaGpqKgwf/7zn01SUpLXfk7nP//5j3niiSfMZ599Zr788kvz7LPPmnbt2pn169d72vz61782nTt3NosXLzY7d+40H3/8sfnDH/5gjDHmk08+MZLMqlWrTGVlpdm3b58xxphBgwaZ++67z2tf119/vcnPzz/rY/vh+3U6O3fuNB07djRz5841//rXv8yaNWtMnz59zNixYz1t0tLSTFxcnCkpKTFffPGFKS4uNqGhoaaiouKUx9LQ0GBiY2PNgw8+aHbu3Gm2b99uFi9ebL766itjjDF//etfTVhYmCkpKTE7duwwTz75pGnXrp15//33PfvNzc01I0eONBs2bDD/+te/zAMPPGAuuOACz3t1KjU1NeaBBx4wPXv2NJWVlaaystLU1NQYY4yRZBITE83ChQvNl19+ab766itTX19vZsyYYTZs2GD+/e9/mz//+c+mQ4cO5pVXXvGMmZ+fb2w2m7nnnntMeXm5efvtt02HDh3MggULjDHGbNiwwbRr184sWbLE7N6922zatMk888wznv6DBg0y0dHR5r777vP8vH2/f3POa69evcx7771ndu7cafbt22ceeeQRk5GRYVasWGG+/PJLs2jRIhMREWFWr17t1W/AgAFm9erVZtu2bebqq682AwcO9Iy9bNky065dOzNjxgyzfft2s3nzZvPoo48aY4z5+uuvTWhoqPnkk0887Tdt2mRCQkLMl19+edpzgbaBsAPLO13YufHGG01mZqYx5rtfmOHh4aZjx45er48++uiM+/jhL+9bbrnFDB061KvNlClTTFZW1inH2LBhg5FkDh06ZIwxZtq0aSe0nzp16lmHhJMZMWKEeeCBB4wxxrjdbhMREeEJNz+0a9cuI8l89tlnXuvPJuz80A+PrTlhZ9y4ceaXv/yl17qPP/7YhIaGmtraWmPMd+duzJgxnu1NTU0mMTHRzJ8//5THsm/fPiPJ8wv3hwYOHGjGjx/vte4Xv/iF+clPfuKpwWazmaNHj3q16datm/n9739/xuMqKioyvXv3PmG9JDN58uQz9i8oKDCjR4/2LOfn55u0tDRz7Ngxr3pvvPFGY4wxr7/+urHZbMbtdp90vEGDBpnMzEzT1NTkWTd16lTPv4+TOdV5ffPNNz1tjh49ajp06GDWrl3r1XfcuHHm5ptv9uq3atUqz/Z33nnHSPKcY4fDYW699dZT1jJ8+HAzYcIEz/KkSZPM4MGDT9kebQsfY6FNM8YoJCTEszxlyhRt3rzZ69WvX79mj1teXq6rrrrKa91VV12lL774Qo2NjZKkjRs3auTIkbLb7YqJidGgQYMkSU6n0zPGgAEDvMZwOBxnXUNjY6MefvhhXX755YqLi1N0dLTeffddr/Hr6up07bXXNvv4zuRMx9Ycn3/+uRYvXqzo6GjPKy8vT01NTdq1a5enXa9evTz/HRISouTk5NN+dBYXF6exY8cqLy9PI0eO1DPPPKPKykrP9lOdw/Lyck9dhw8f1gUXXOBV265du/Tll182+zi/72Q/cyUlJerbt68SEhIUHR2tBQsWnPB+9uzZU+3atfMsp6SkeN6DoUOHKi0tTRdffLFuu+02vfzyy6qpqfHqn5OT4/XvweFwNOtn9mT179y5UzU1NRo6dKjX+/TSSy+d8D59/xympKRIkqf+zZs3n/Zndfz48frLX/6io0ePqr6+XkuWLNGdd955yvZoW5g1hjatvLzcM59DkuLj43XJJZec8/0eOXJEeXl5ysvL08svv6yEhAQ5nU7l5eWpvr7eL/t44okn9Mwzz+jpp5/W5Zdfro4dO2ry5Mme8aOionwaNzQ01GsehySveSH+PrbDhw/r7rvv1r333nvCNrvd7vnvH07iDQkJ8brb7mQWLVqke++9VytWrNArr7yi6dOna+XKlcrJyTmrulJSUrR69eoTtnXq1OmM/U/nh/PEli5dqgcffFBPPvmkHA6HYmJi9MQTT2j9+vVe7U73HsTExGjTpk1avXq13nvvPc2YMUMzZ87Uhg0bzqre5pzX79d/+PBhSdI777yjCy+80KtdRETEKes/HrqO13+mn9eRI0cqIiJCb7zxhsLDw9XQ0OA1JwltG2EHbdb777+vLVu26P777/f72JmZmVqzZo3XujVr1ujSSy9Vu3btVFFRoX379umxxx5TamqqJOnTTz89YYy33nrLa926devOuoY1a9bo+uuv15gxYyR990vjX//6l7KysiRJ3bt3V1RUlEpLS3XXXXed0D88PFySPH/VH5eQkOB1BaSxsVFbt27VkCFDJOmsjq05rrjiCm3fvr1FIfRUxyJ9N2m5T58+mjZtmhwOh5YsWaKcnBzPOczPz/e0XbNmjef9u+KKK1RVVaX27dsrPT3dp5pOVs/JrFmzRgMHDtSvfvUrzzpfrh61b99eubm5ys3NVVFRkTp16qT3339fN9xwgySdEJ7WrVun7t27n/XP7MlkZWUpIiJCTqfTcyXIF7169VJpaanuuOOOUx5bfn6+Fi1apPDwcN10000+B3pYD2EHbUJdXZ2qqqq8bj0vLi7WT3/6U91+++2edocOHVJVVZVX3w4dOshmszVrfw888ID69++vhx9+WDfeeKPKysr0/PPPa968eZK+uyIRHh6u5557Tvfcc4+2bt2qhx9+2GuMe+65R08++aSmTJmiu+66Sxs3bvS6c+xMunfvrtdee01r165V586d9dRTT6m6utrzyzoyMlJTp07Vr3/9a4WHh+uqq67St99+q23btmncuHFKTExUVFSUVqxYoYsuukiRkZGKjY3VNddco8LCQr3zzjvq1q2bnnrqKa+HA57NsTXH1KlTlZOTo4kTJ+quu+5Sx44dtX37dq1cuVLPP//8WY1xsmPZv3+/FixYoJ/97Gfq0qWLduzYoS+++MLz8zBlyhT993//t/r06aPc3Fy9/fbb+utf/6pVq1ZJknJzc+VwODRq1Cg9/vjjuvTSS/XNN9/onXfe0c9//vMzfvyZnp6uXbt2afPmzbrooosUExNzwpWO47p3766XXnpJ7777rrp27ao//elP2rBhg9dVyTNZtmyZ/v3vf+vHP/6xOnfurOXLl6upqUk9evTwtHE6nSosLNTdd9+tTZs26bnnnvPcRejreY2JidGDDz6o+++/X01NTfrRj36kgwcPas2aNbLZbF5h8nSKiop07bXXqlu3brrpppt07NgxLV++XFOnTvW0ueuuu5SZmSlJJ/yxgTYu0JOGgHMtPz/fSDKSTPv27U1CQoLJzc01CxcuNI2NjZ52aWlpnnbff919991n3MfJJty+9tprJisry4SFhRm73W6eeOIJrz5Lliwx6enpJiIiwjgcDvPWW2+dMIn27bffNpdccomJiIgwV199tVm4cOFZT+zdt2+fuf766010dLRJTEw006dPN7fffrvXZO3GxkbzyCOPmLS0NE+dx+9wMcaYP/zhDyY1NdWEhoaaQYMGGWOMqa+vNxMmTDBxcXEmMTHRFBcXnzBB+UzH1pwJysZ8dzfV0KFDTXR0tOnYsaPp1auXmTNnjmf7ye6k6927tykqKjrlsVRVVZlRo0aZlJQUEx4ebtLS0syMGTO8fibmzZtnLr74YhMWFmYuvfRS89JLL3ntw+12m0mTJpkuXbqYsLAwk5qaam699VbjdDrPeExHjx41o0ePNp06dTKSPHfqSTJvvPHGCW3Hjh1rYmNjTadOncyECRPMb37zG68JziebiH/fffd5ztvHH39sBg0aZDp37myioqJMr169vO7mGjRokPnVr35l7rnnHmOz2Uznzp3Nb3/7W68Jy76e16amJvP000+bHj16mLCwMJOQkGDy8vLMhx9+eMp+n332mZFkdu3a5Vn3+uuvm+zsbBMeHm7i4+PNDTfccML7evXVV5uePXue+o1HmxRizA8+fAcAtDmDBw9Wdna2nn766UCX4jNjjLp3765f/epXKiwsDHQ5CCJ8jAUAaPW+/fZbLV26VFVVVaec14O2i1vPgbNwzz33eN02+/3XPffcE5Cahg8ffsqaHn300YDU5AurHMcP9ezZ85TH9fLLLwe6PMtJTEzU7NmztWDBAnXu3DnQ5SDI8DEWcBb27t0rt9t90m02m02JiYnnuSJpz549qq2tPem2uLg4xcXFneeKfGOV4/ihr776yuuW/O9LSkriO9eA84iwAwAALI2PsQAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKX9f/lK3ZA5FiLKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(test[\"DE_load_actual_entsoe_transparency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8895.00</td>\n",
       "      <td>8895.00</td>\n",
       "      <td>8895.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.98</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.03</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DE_load_actual_entsoe_transparency  DE_solar_generation_actual  \\\n",
       "count                             8895.00                     8895.00   \n",
       "mean                                 0.01                        0.14   \n",
       "std                                  0.98                        1.20   \n",
       "min                                 -2.23                       -0.64   \n",
       "25%                                 -0.81                       -0.64   \n",
       "50%                                 -0.05                       -0.62   \n",
       "75%                                  0.87                        0.59   \n",
       "max                                  2.03                        4.34   \n",
       "\n",
       "       DE_wind_generation_actual  \n",
       "count                    8895.00  \n",
       "mean                        0.49  \n",
       "std                         1.22  \n",
       "min                        -1.20  \n",
       "25%                        -0.46  \n",
       "50%                         0.18  \n",
       "75%                         1.14  \n",
       "max                         4.30  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution ETTH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8521 observations in the train dataset.\n",
      " 6042 observations in the test dataset.\n",
      " 2857 observations in the validation dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "time_series = pd.read_csv(\"./datasets/ETTh1.csv\", index_col=0, parse_dates=True)\n",
    "train, vali, test = split_scale_dataset(data=time_series, train_size=8521, val_size=2857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 17420 entries, 2016-07-01 00:00:00 to 2018-06-26 19:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   HUFL    17420 non-null  float64\n",
      " 1   HULL    17420 non-null  float64\n",
      " 2   MUFL    17420 non-null  float64\n",
      " 3   MULL    17420 non-null  float64\n",
      " 4   LUFL    17420 non-null  float64\n",
      " 5   LULL    17420 non-null  float64\n",
      " 6   OT      17420 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "time_series.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14235"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8521 + 2857 + 2857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8569\n",
      "val 4345\n",
      "test 4321\n",
      "\titers: 100, epoch: 1 | loss: 0.4733695\n",
      "\tspeed: 0.0626s/iter; left time: 160.8995s\n",
      "\titers: 200, epoch: 1 | loss: 0.3275512\n",
      "\tspeed: 0.0413s/iter; left time: 102.0419s\n",
      "Epoch: 1 running time: 0.20149732033411663 min.\n",
      "Epoch: 1, Steps: 267 | Train Loss: 0.4435941 Vali Loss: 0.6720330 Test Loss: 0.7471600\n",
      "Validation loss decreased (inf --> 0.672033).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3207772\n",
      "\tspeed: 0.1169s/iter; left time: 269.3744s\n",
      "\titers: 200, epoch: 2 | loss: 0.3147012\n",
      "\tspeed: 0.0416s/iter; left time: 91.7787s\n",
      "Epoch: 2 running time: 0.18970996936162313 min.\n",
      "Epoch: 2, Steps: 267 | Train Loss: 0.3283183 Vali Loss: 0.6654594 Test Loss: 0.8112167\n",
      "Validation loss decreased (0.672033 --> 0.665459).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2579902\n",
      "\tspeed: 0.1220s/iter; left time: 248.5572s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'ETTh1.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# https://colab.research.google.com/drive/1rv2rKwQqgoHDNjXtRoAEWZ2ATz0gGAKu?usp=sharing#scrollTo=yu6zzic9t_Cz\n",
    "# Popen: https://colab.research.google.com/github/aviadr1/learn-python/blob/master/content/13_multiprocessing/notebooks/os_system_subprocess.ipynb\n",
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"./TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output  \n",
    "\n",
    "    return output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"/content/my_work/TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Define command and options wanted\n",
    "        command = \"python\"\n",
    "        options = \"-u\"\n",
    "        # Run the shell command directly in Colab\n",
    "        output = !{command} {options} {path_to_run_file} {model_arguments}\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output\n",
    "\n",
    "    return output\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script using the %run magic command\n",
    "        output = %run -i {path_to_run_file} {model_arguments}\n",
    "\n",
    "    except Exception as e:\n",
    "        output = str(e)\n",
    "\n",
    "    return output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to work in colab\n",
    "import subprocess\n",
    "\n",
    "path_to_run_file = \"./TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Construct the command to execute the script with required and model arguments\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        # Execute the script and capture the output\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        # Check if there's any error in the process\n",
    "        if process.returncode != 0:\n",
    "            output = stderr.decode(\"utf-8\")\n",
    "        else:\n",
    "            output = stdout.decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        output = str(e)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_most_important_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m script_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--is_training\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--itr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[0;32m---> 29\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_run_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\u001b[39;00m\n\u001b[1;32m     32\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/cs-hu/riabchuv/hu-home/my_work/results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInformer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mrun_output\u001b[0;34m(path_to_run_file, model_arguments)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the script and capture the output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m---> 12\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there's any error in the process\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "script_arguments = [\n",
    "    \"--task_name\", \"long_term_forecast\",\n",
    "    \"--is_training\", \"1\",\n",
    "    \"--root_path\", current_path,\n",
    "    \"--data_path\", dataset,\n",
    "    \"--model_id\", \"1\",\n",
    "    \"--model\", \"Informer\",\n",
    "    \"--data\", \"custom\",\n",
    "    \"--features\", \"M\",\n",
    "    \"--seq_len\", \"96\",\n",
    "    \"--label_len\", \"48\",\n",
    "    \"--pred_len\", \"24\",\n",
    "    \"--e_layers\", \"2\",\n",
    "    \"--d_layers\", \"5\",\n",
    "    \"--factor\", \"5\",\n",
    "    \"--enc_in\", \"3\",\n",
    "    \"--dec_in\", \"3\",\n",
    "    \"--c_out\", \"3\",\n",
    "    \"--des\", \"Exp\",\n",
    "    \"--itr\", \"2\"\n",
    "]\n",
    "\n",
    "model_output = run_output(path_to_run_file, script_arguments)\n",
    "\n",
    "#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "folder_path = f'./results/{\"Informer\"}/'\n",
    "\n",
    "# Write model output into txt file\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output_chr.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "print(model_output)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DE_load_actual_entsoe_transparency</th>\n",
       "      <th>DE_solar_generation_actual</th>\n",
       "      <th>DE_wind_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 07:00:00</td>\n",
       "      <td>41133.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 08:00:00</td>\n",
       "      <td>42963.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>10029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 09:00:00</td>\n",
       "      <td>45088.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>10550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 10:00:00</td>\n",
       "      <td>47013.0</td>\n",
       "      <td>3364.0</td>\n",
       "      <td>11390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 11:00:00</td>\n",
       "      <td>48159.0</td>\n",
       "      <td>4198.0</td>\n",
       "      <td>12103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43788</th>\n",
       "      <td>2019-12-30 19:00:00</td>\n",
       "      <td>53959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43789</th>\n",
       "      <td>2019-12-30 20:00:00</td>\n",
       "      <td>51937.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43790</th>\n",
       "      <td>2019-12-30 21:00:00</td>\n",
       "      <td>50574.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43791</th>\n",
       "      <td>2019-12-30 22:00:00</td>\n",
       "      <td>47382.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43792</th>\n",
       "      <td>2019-12-30 23:00:00</td>\n",
       "      <td>44018.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29928.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43793 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  DE_load_actual_entsoe_transparency  \\\n",
       "0      2015-01-01 07:00:00                             41133.0   \n",
       "1      2015-01-01 08:00:00                             42963.0   \n",
       "2      2015-01-01 09:00:00                             45088.0   \n",
       "3      2015-01-01 10:00:00                             47013.0   \n",
       "4      2015-01-01 11:00:00                             48159.0   \n",
       "...                    ...                                 ...   \n",
       "43788  2019-12-30 19:00:00                             53959.0   \n",
       "43789  2019-12-30 20:00:00                             51937.0   \n",
       "43790  2019-12-30 21:00:00                             50574.0   \n",
       "43791  2019-12-30 22:00:00                             47382.0   \n",
       "43792  2019-12-30 23:00:00                             44018.0   \n",
       "\n",
       "       DE_solar_generation_actual  DE_wind_generation_actual  \n",
       "0                            71.0                    10208.0  \n",
       "1                           773.0                    10029.0  \n",
       "2                          2117.0                    10550.0  \n",
       "3                          3364.0                    11390.0  \n",
       "4                          4198.0                    12103.0  \n",
       "...                           ...                        ...  \n",
       "43788                         0.0                    32323.0  \n",
       "43789                         0.0                    32395.0  \n",
       "43790                         0.0                    31439.0  \n",
       "43791                         0.0                    30727.0  \n",
       "43792                         0.0                    29928.0  \n",
       "\n",
       "[43793 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./datasets/df_most_important_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43611"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28321 + 6577 + 8713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28321\n",
      "val 6577\n",
      "test 8713\n",
      "\titers: 100, epoch: 1 | loss: 0.4881925\n",
      "\tspeed: 0.0612s/iter; left time: 535.5396s\n",
      "\titers: 200, epoch: 1 | loss: 0.4415741\n",
      "\tspeed: 0.0419s/iter; left time: 362.6325s\n",
      "\titers: 300, epoch: 1 | loss: 0.4410460\n",
      "\tspeed: 0.0412s/iter; left time: 351.9689s\n",
      "\titers: 400, epoch: 1 | loss: 0.2902627\n",
      "\tspeed: 0.0415s/iter; left time: 350.4010s\n",
      "\titers: 500, epoch: 1 | loss: 0.3088942\n",
      "\tspeed: 0.0412s/iter; left time: 343.8548s\n",
      "\titers: 600, epoch: 1 | loss: 0.3044106\n",
      "\tspeed: 0.0414s/iter; left time: 341.2761s\n",
      "\titers: 700, epoch: 1 | loss: 0.2735647\n",
      "\tspeed: 0.0421s/iter; left time: 342.8331s\n",
      "\titers: 800, epoch: 1 | loss: 0.2792752\n",
      "\tspeed: 0.0417s/iter; left time: 335.5625s\n",
      "Epoch: 1 running time: 0.6292555173238118 min.\n",
      "Epoch: 1, Steps: 885 | Train Loss: 0.3454897 Vali Loss: 0.2995675 Test Loss: 0.3626922\n",
      "Validation loss decreased (inf --> 0.299567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2089891\n",
      "\tspeed: 0.1524s/iter; left time: 1198.9570s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 160, in train\n",
      "    model_optim.step()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 166, in step\n",
      "    adam(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 316, in adam\n",
      "    func(params,\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/optim/adam.py\", line 583, in _multi_tensor_adam\n",
      "    torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt, step_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 4321\n",
      "test 8641\n",
      "\titers: 100, epoch: 1 | loss: 0.7090327\n",
      "\tspeed: 0.0694s/iter; left time: 653.2046s\n",
      "\titers: 200, epoch: 1 | loss: 0.6357286\n",
      "\tspeed: 0.0500s/iter; left time: 465.5139s\n",
      "\titers: 300, epoch: 1 | loss: 0.5953805\n",
      "\tspeed: 0.0505s/iter; left time: 464.7455s\n",
      "\titers: 400, epoch: 1 | loss: 0.4663185\n",
      "\tspeed: 0.0504s/iter; left time: 459.0093s\n",
      "\titers: 500, epoch: 1 | loss: 0.4046238\n",
      "\tspeed: 0.0505s/iter; left time: 455.3592s\n",
      "\titers: 600, epoch: 1 | loss: 0.3477176\n",
      "\tspeed: 0.0494s/iter; left time: 440.2458s\n",
      "\titers: 700, epoch: 1 | loss: 0.4344813\n",
      "\tspeed: 0.0500s/iter; left time: 440.9754s\n",
      "\titers: 800, epoch: 1 | loss: 0.3226214\n",
      "\tspeed: 0.0484s/iter; left time: 421.2105s\n",
      "\titers: 900, epoch: 1 | loss: 0.3889300\n",
      "\tspeed: 0.0482s/iter; left time: 415.0947s\n",
      "Epoch: 1 running time: 0.8037162542343139 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.5144386 Vali Loss: 0.4448476 Test Loss: 0.6013156\n",
      "Validation loss decreased (inf --> 0.444848).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3872515\n",
      "\tspeed: 0.1517s/iter; left time: 1283.7868s\n",
      "\titers: 200, epoch: 2 | loss: 0.2599374\n",
      "\tspeed: 0.0506s/iter; left time: 422.8650s\n",
      "\titers: 300, epoch: 2 | loss: 0.4126107\n",
      "\tspeed: 0.0495s/iter; left time: 408.7092s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/run.py\", line 156, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 159, in train\n",
      "    loss.backward()\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  #--d_ff 256 \\\n",
    "  #--d_model 256 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enc_layers = 2, d_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.7141522\n",
      "\tspeed: 0.0572s/iter; left time: 538.7338s\n",
      "\titers: 200, epoch: 1 | loss: 0.6334165\n",
      "\tspeed: 0.0384s/iter; left time: 357.9389s\n",
      "\titers: 300, epoch: 1 | loss: 0.5902387\n",
      "\tspeed: 0.0393s/iter; left time: 361.8272s\n",
      "\titers: 400, epoch: 1 | loss: 0.4588140\n",
      "\tspeed: 0.0348s/iter; left time: 316.8726s\n",
      "\titers: 500, epoch: 1 | loss: 0.4049430\n",
      "\tspeed: 0.0388s/iter; left time: 349.9006s\n",
      "\titers: 600, epoch: 1 | loss: 0.3436919\n",
      "\tspeed: 0.0382s/iter; left time: 340.4654s\n",
      "\titers: 700, epoch: 1 | loss: 0.4383478\n",
      "\tspeed: 0.0384s/iter; left time: 338.2228s\n",
      "\titers: 800, epoch: 1 | loss: 0.3165578\n",
      "\tspeed: 0.0383s/iter; left time: 333.2159s\n",
      "\titers: 900, epoch: 1 | loss: 0.3756514\n",
      "\tspeed: 0.0384s/iter; left time: 330.3723s\n",
      "Epoch: 1 running time: 0.6205641269683838 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.5120596 Vali Loss: 0.6239172 Test Loss: 0.4709466\n",
      "Validation loss decreased (inf --> 0.623917).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3611738\n",
      "\tspeed: 0.1105s/iter; left time: 935.1631s\n",
      "\titers: 200, epoch: 2 | loss: 0.3394974\n",
      "\tspeed: 0.0383s/iter; left time: 319.7919s\n",
      "\titers: 300, epoch: 2 | loss: 0.3734077\n",
      "\tspeed: 0.0399s/iter; left time: 329.7170s\n",
      "\titers: 400, epoch: 2 | loss: 0.2974185\n",
      "\tspeed: 0.0402s/iter; left time: 328.4005s\n",
      "\titers: 500, epoch: 2 | loss: 0.2851786\n",
      "\tspeed: 0.0409s/iter; left time: 329.4732s\n",
      "\titers: 600, epoch: 2 | loss: 0.2759520\n",
      "\tspeed: 0.0384s/iter; left time: 305.9789s\n",
      "\titers: 700, epoch: 2 | loss: 0.2996328\n",
      "\tspeed: 0.0390s/iter; left time: 306.3728s\n",
      "\titers: 800, epoch: 2 | loss: 0.4047164\n",
      "\tspeed: 0.0382s/iter; left time: 296.7280s\n",
      "\titers: 900, epoch: 2 | loss: 0.3827966\n",
      "\tspeed: 0.0391s/iter; left time: 299.2556s\n",
      "Epoch: 2 running time: 0.624190107981364 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3368625 Vali Loss: 0.5615881 Test Loss: 0.4295469\n",
      "Validation loss decreased (0.623917 --> 0.561588).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2764411\n",
      "\tspeed: 0.1067s/iter; left time: 801.4424s\n",
      "\titers: 200, epoch: 3 | loss: 0.3008927\n",
      "\tspeed: 0.0364s/iter; left time: 269.3789s\n",
      "\titers: 300, epoch: 3 | loss: 0.3424858\n",
      "\tspeed: 0.0332s/iter; left time: 242.5320s\n",
      "\titers: 400, epoch: 3 | loss: 0.3750836\n",
      "\tspeed: 0.0299s/iter; left time: 215.7507s\n",
      "\titers: 500, epoch: 3 | loss: 0.3410933\n",
      "\tspeed: 0.0391s/iter; left time: 278.0072s\n",
      "\titers: 600, epoch: 3 | loss: 0.2627601\n",
      "\tspeed: 0.0732s/iter; left time: 513.3012s\n",
      "\titers: 700, epoch: 3 | loss: 0.3647805\n",
      "\tspeed: 0.1552s/iter; left time: 1072.3409s\n",
      "\titers: 800, epoch: 3 | loss: 0.3069986\n",
      "\tspeed: 0.1048s/iter; left time: 713.6192s\n",
      "\titers: 900, epoch: 3 | loss: 0.2798613\n",
      "\tspeed: 0.1291s/iter; left time: 866.2063s\n",
      "Epoch: 3 running time: 1.1808760285377502 min.\n",
      "Epoch: 3, Steps: 951 | Train Loss: 0.2991357 Vali Loss: 0.5515444 Test Loss: 0.4388487\n",
      "Validation loss decreased (0.561588 --> 0.551544).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2746516\n",
      "\tspeed: 0.3748s/iter; left time: 2458.1990s\n",
      "\titers: 200, epoch: 4 | loss: 0.3386393\n",
      "\tspeed: 0.1296s/iter; left time: 837.0068s\n",
      "\titers: 300, epoch: 4 | loss: 0.2837962\n",
      "\tspeed: 0.1226s/iter; left time: 779.6436s\n",
      "\titers: 400, epoch: 4 | loss: 0.2993825\n",
      "\tspeed: 0.1303s/iter; left time: 815.6540s\n",
      "\titers: 500, epoch: 4 | loss: 0.3678939\n",
      "\tspeed: 0.1293s/iter; left time: 796.5225s\n",
      "\titers: 600, epoch: 4 | loss: 0.2832958\n",
      "\tspeed: 0.1269s/iter; left time: 768.7856s\n",
      "\titers: 700, epoch: 4 | loss: 0.2086661\n",
      "\tspeed: 0.1262s/iter; left time: 752.0278s\n",
      "\titers: 800, epoch: 4 | loss: 0.2271242\n",
      "\tspeed: 0.1282s/iter; left time: 751.2870s\n",
      "\titers: 900, epoch: 4 | loss: 0.2186353\n",
      "\tspeed: 0.1284s/iter; left time: 739.6054s\n",
      "Epoch: 4 running time: 2.027537763118744 min.\n",
      "Epoch: 4, Steps: 951 | Train Loss: 0.2752065 Vali Loss: 0.5582085 Test Loss: 0.4290419\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2826999\n",
      "\tspeed: 0.4435s/iter; left time: 2486.6521s\n",
      "\titers: 200, epoch: 5 | loss: 0.3473910\n",
      "\tspeed: 0.1328s/iter; left time: 731.0702s\n",
      "\titers: 300, epoch: 5 | loss: 0.3006740\n",
      "\tspeed: 0.1299s/iter; left time: 702.2622s\n",
      "\titers: 400, epoch: 5 | loss: 0.2491039\n",
      "\tspeed: 0.1285s/iter; left time: 682.0910s\n",
      "\titers: 500, epoch: 5 | loss: 0.2718999\n",
      "\tspeed: 0.1287s/iter; left time: 669.9253s\n",
      "\titers: 600, epoch: 5 | loss: 0.2556385\n",
      "\tspeed: 0.1270s/iter; left time: 648.8306s\n",
      "\titers: 700, epoch: 5 | loss: 0.2236764\n",
      "\tspeed: 0.1265s/iter; left time: 633.6129s\n",
      "\titers: 800, epoch: 5 | loss: 0.2606227\n",
      "\tspeed: 0.1272s/iter; left time: 624.1430s\n",
      "\titers: 900, epoch: 5 | loss: 0.2403338\n",
      "\tspeed: 0.1291s/iter; left time: 620.4193s\n",
      "Epoch: 5 running time: 2.0442325115203857 min.\n",
      "Epoch: 5, Steps: 951 | Train Loss: 0.2731212 Vali Loss: 0.5569632 Test Loss: 0.4376281\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2709441\n",
      "\tspeed: 0.4935s/iter; left time: 2297.6529s\n",
      "\titers: 200, epoch: 6 | loss: 0.2830582\n",
      "\tspeed: 0.1287s/iter; left time: 586.2541s\n",
      "\titers: 300, epoch: 6 | loss: 0.2441400\n",
      "\tspeed: 0.1281s/iter; left time: 570.8664s\n",
      "\titers: 400, epoch: 6 | loss: 0.2625414\n",
      "\tspeed: 0.1318s/iter; left time: 574.1891s\n",
      "\titers: 500, epoch: 6 | loss: 0.2472361\n",
      "\tspeed: 0.1294s/iter; left time: 550.7394s\n",
      "\titers: 600, epoch: 6 | loss: 0.2639813\n",
      "\tspeed: 0.1277s/iter; left time: 530.8347s\n",
      "\titers: 700, epoch: 6 | loss: 0.2222858\n",
      "\tspeed: 0.1300s/iter; left time: 527.1481s\n",
      "\titers: 800, epoch: 6 | loss: 0.2225119\n",
      "\tspeed: 0.1330s/iter; left time: 526.2023s\n",
      "\titers: 900, epoch: 6 | loss: 0.2692212\n",
      "\tspeed: 0.1287s/iter; left time: 496.1340s\n",
      "Epoch: 6 running time: 2.0480775674184164 min.\n",
      "Epoch: 6, Steps: 951 | Train Loss: 0.2728180 Vali Loss: 0.5797018 Test Loss: 0.4556924\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.4559608995914459, mae:0.42249640822410583\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enc_layers = 2, d_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5454894\n",
      "\tspeed: 0.2071s/iter; left time: 1949.4369s\n",
      "\titers: 200, epoch: 1 | loss: 0.4153013\n",
      "\tspeed: 0.1876s/iter; left time: 1746.8217s\n",
      "\titers: 300, epoch: 1 | loss: 0.3637479\n",
      "\tspeed: 0.1845s/iter; left time: 1699.0011s\n",
      "\titers: 400, epoch: 1 | loss: 0.3130654\n",
      "\tspeed: 0.1870s/iter; left time: 1703.5923s\n",
      "\titers: 500, epoch: 1 | loss: 0.3671271\n",
      "\tspeed: 0.1853s/iter; left time: 1669.6942s\n",
      "\titers: 600, epoch: 1 | loss: 0.3828981\n",
      "\tspeed: 0.1905s/iter; left time: 1697.6272s\n",
      "\titers: 700, epoch: 1 | loss: 0.2786609\n",
      "\tspeed: 0.1832s/iter; left time: 1614.5077s\n",
      "\titers: 800, epoch: 1 | loss: 0.4051742\n",
      "\tspeed: 0.1803s/iter; left time: 1570.2727s\n",
      "\titers: 900, epoch: 1 | loss: 0.3166372\n",
      "\tspeed: 0.1838s/iter; left time: 1582.4113s\n",
      "Epoch: 1 running time: 2.956809914112091 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.4210682 Vali Loss: 0.5960458 Test Loss: 0.4425991\n",
      "Validation loss decreased (inf --> 0.596046).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3554033\n",
      "\tspeed: 0.6426s/iter; left time: 5436.1286s\n",
      "\titers: 200, epoch: 2 | loss: 0.2777174\n",
      "\tspeed: 0.2088s/iter; left time: 1745.2965s\n",
      "\titers: 300, epoch: 2 | loss: 0.2990886\n",
      "\tspeed: 0.1981s/iter; left time: 1636.3021s\n",
      "\titers: 400, epoch: 2 | loss: 0.2444989\n",
      "\tspeed: 0.1843s/iter; left time: 1503.7316s\n",
      "\titers: 500, epoch: 2 | loss: 0.2642674\n",
      "\tspeed: 0.1905s/iter; left time: 1535.6160s\n",
      "\titers: 600, epoch: 2 | loss: 0.3096100\n",
      "\tspeed: 0.1818s/iter; left time: 1447.2285s\n",
      "\titers: 700, epoch: 2 | loss: 0.2612270\n",
      "\tspeed: 0.1858s/iter; left time: 1460.2724s\n",
      "\titers: 800, epoch: 2 | loss: 0.2501468\n",
      "\tspeed: 0.1891s/iter; left time: 1467.2437s\n",
      "\titers: 900, epoch: 2 | loss: 0.3030309\n",
      "\tspeed: 0.1825s/iter; left time: 1397.9793s\n",
      "Epoch: 2 running time: 3.011246955394745 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3139220 Vali Loss: 0.5717895 Test Loss: 0.4166458\n",
      "Validation loss decreased (0.596046 --> 0.571790).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2359918\n",
      "\tspeed: 0.6454s/iter; left time: 4846.4168s\n",
      "\titers: 200, epoch: 3 | loss: 0.2424805\n",
      "\tspeed: 0.1858s/iter; left time: 1376.3532s\n",
      "\titers: 300, epoch: 3 | loss: 0.2929650\n",
      "\tspeed: 0.1847s/iter; left time: 1350.1094s\n",
      "\titers: 400, epoch: 3 | loss: 0.3429941\n",
      "\tspeed: 0.1874s/iter; left time: 1350.8279s\n",
      "\titers: 500, epoch: 3 | loss: 0.2726091\n",
      "\tspeed: 0.1889s/iter; left time: 1343.0250s\n",
      "\titers: 600, epoch: 3 | loss: 0.2681721\n",
      "\tspeed: 0.1891s/iter; left time: 1325.3565s\n",
      "\titers: 700, epoch: 3 | loss: 0.3254008\n",
      "\tspeed: 0.1813s/iter; left time: 1252.3694s\n",
      "\titers: 800, epoch: 3 | loss: 0.2703008\n",
      "\tspeed: 0.1848s/iter; left time: 1258.6357s\n",
      "\titers: 900, epoch: 3 | loss: 0.2300956\n",
      "\tspeed: 0.2086s/iter; left time: 1399.2511s\n",
      "Epoch: 3 running time: 3.0217740138371787 min.\n",
      "Epoch: 3, Steps: 951 | Train Loss: 0.2796335 Vali Loss: 0.6095524 Test Loss: 0.4656214\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3412687\n",
      "\tspeed: 0.6218s/iter; left time: 4077.4879s\n",
      "\titers: 200, epoch: 4 | loss: 0.2064609\n",
      "\tspeed: 0.1897s/iter; left time: 1224.9097s\n",
      "\titers: 300, epoch: 4 | loss: 0.3333808\n",
      "\tspeed: 0.1845s/iter; left time: 1172.7958s\n",
      "\titers: 400, epoch: 4 | loss: 0.3362628\n",
      "\tspeed: 0.1903s/iter; left time: 1191.1262s\n",
      "\titers: 500, epoch: 4 | loss: 0.3032527\n",
      "\tspeed: 0.1844s/iter; left time: 1135.8034s\n",
      "\titers: 600, epoch: 4 | loss: 0.2457445\n",
      "\tspeed: 0.1893s/iter; left time: 1146.7858s\n",
      "\titers: 700, epoch: 4 | loss: 0.2105109\n",
      "\tspeed: 0.1395s/iter; left time: 831.0531s\n",
      "\titers: 800, epoch: 4 | loss: 0.3005113\n",
      "\tspeed: 0.1855s/iter; left time: 1086.4778s\n",
      "\titers: 900, epoch: 4 | loss: 0.2480656\n",
      "\tspeed: 0.1562s/iter; left time: 899.4006s\n",
      "Epoch: 4 running time: 2.7920525153477986 min.\n",
      "Epoch: 4, Steps: 951 | Train Loss: 0.2759286 Vali Loss: 0.5859023 Test Loss: 0.4506322\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2228749\n",
      "\tspeed: 0.4686s/iter; left time: 2627.4017s\n",
      "\titers: 200, epoch: 5 | loss: 0.2523571\n",
      "\tspeed: 0.1345s/iter; left time: 740.4186s\n",
      "\titers: 300, epoch: 5 | loss: 0.2830021\n",
      "\tspeed: 0.1854s/iter; left time: 1002.3437s\n",
      "\titers: 400, epoch: 5 | loss: 0.2671471\n",
      "\tspeed: 0.1848s/iter; left time: 980.9264s\n",
      "\titers: 500, epoch: 5 | loss: 0.3003709\n",
      "\tspeed: 0.1805s/iter; left time: 940.0000s\n",
      "\titers: 600, epoch: 5 | loss: 0.2270721\n",
      "\tspeed: 0.1910s/iter; left time: 975.2245s\n",
      "\titers: 700, epoch: 5 | loss: 0.2827327\n",
      "\tspeed: 0.2276s/iter; left time: 1139.7601s\n",
      "\titers: 800, epoch: 5 | loss: 0.2605267\n",
      "\tspeed: 0.1802s/iter; left time: 884.1519s\n",
      "\titers: 900, epoch: 5 | loss: 0.2766968\n",
      "\tspeed: 0.1813s/iter; left time: 871.4463s\n",
      "Epoch: 5 running time: 2.8687805056571962 min.\n",
      "Epoch: 5, Steps: 951 | Train Loss: 0.2755826 Vali Loss: 0.5693536 Test Loss: 0.4508579\n",
      "Validation loss decreased (0.571790 --> 0.569354).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2449235\n",
      "\tspeed: 0.6438s/iter; left time: 2997.3951s\n",
      "\titers: 200, epoch: 6 | loss: 0.3221987\n",
      "\tspeed: 0.1844s/iter; left time: 840.0540s\n",
      "\titers: 300, epoch: 6 | loss: 0.3097484\n",
      "\tspeed: 0.1819s/iter; left time: 810.6772s\n",
      "\titers: 400, epoch: 6 | loss: 0.2539929\n",
      "\tspeed: 0.1880s/iter; left time: 818.8981s\n",
      "\titers: 500, epoch: 6 | loss: 0.2426753\n",
      "\tspeed: 0.1854s/iter; left time: 789.2689s\n",
      "\titers: 600, epoch: 6 | loss: 0.2053958\n",
      "\tspeed: 0.1883s/iter; left time: 782.6653s\n",
      "\titers: 700, epoch: 6 | loss: 0.2433844\n",
      "\tspeed: 0.1905s/iter; left time: 772.5811s\n",
      "\titers: 800, epoch: 6 | loss: 0.2400787\n",
      "\tspeed: 0.1865s/iter; left time: 737.8775s\n",
      "\titers: 900, epoch: 6 | loss: 0.2652541\n",
      "\tspeed: 0.1905s/iter; left time: 734.4916s\n",
      "Epoch: 6 running time: 2.972999894618988 min.\n",
      "Epoch: 6, Steps: 951 | Train Loss: 0.2648981 Vali Loss: 0.5657748 Test Loss: 0.4404665\n",
      "Validation loss decreased (0.569354 --> 0.565775).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2735194\n",
      "\tspeed: 0.6383s/iter; left time: 2364.9650s\n",
      "\titers: 200, epoch: 7 | loss: 0.2643351\n",
      "\tspeed: 0.2072s/iter; left time: 746.8322s\n",
      "\titers: 300, epoch: 7 | loss: 0.1916679\n",
      "\tspeed: 0.1931s/iter; left time: 676.8039s\n",
      "\titers: 400, epoch: 7 | loss: 0.2479863\n",
      "\tspeed: 0.1901s/iter; left time: 647.1266s\n",
      "\titers: 500, epoch: 7 | loss: 0.2218800\n",
      "\tspeed: 0.1862s/iter; left time: 615.4911s\n",
      "\titers: 600, epoch: 7 | loss: 0.2408008\n",
      "\tspeed: 0.1878s/iter; left time: 601.9122s\n",
      "\titers: 700, epoch: 7 | loss: 0.2242043\n",
      "\tspeed: 0.1888s/iter; left time: 586.3248s\n",
      "\titers: 800, epoch: 7 | loss: 0.3246762\n",
      "\tspeed: 0.1863s/iter; left time: 559.7344s\n",
      "\titers: 900, epoch: 7 | loss: 0.2430054\n",
      "\tspeed: 0.1869s/iter; left time: 542.8945s\n",
      "Epoch: 7 running time: 3.004595987002055 min.\n",
      "Epoch: 7, Steps: 951 | Train Loss: 0.2592509 Vali Loss: 0.5687160 Test Loss: 0.4400822\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2366101\n",
      "\tspeed: 0.6211s/iter; left time: 1710.5510s\n",
      "\titers: 200, epoch: 8 | loss: 0.2885689\n",
      "\tspeed: 0.1862s/iter; left time: 494.0728s\n",
      "\titers: 300, epoch: 8 | loss: 0.3270811\n",
      "\tspeed: 0.1820s/iter; left time: 464.7346s\n",
      "\titers: 400, epoch: 8 | loss: 0.2077899\n",
      "\tspeed: 0.1849s/iter; left time: 453.8286s\n",
      "\titers: 500, epoch: 8 | loss: 0.2357884\n",
      "\tspeed: 0.1837s/iter; left time: 432.4428s\n",
      "\titers: 600, epoch: 8 | loss: 0.2465918\n",
      "\tspeed: 0.1827s/iter; left time: 411.8056s\n",
      "\titers: 700, epoch: 8 | loss: 0.2836409\n",
      "\tspeed: 0.1864s/iter; left time: 401.5478s\n",
      "\titers: 800, epoch: 8 | loss: 0.2226415\n",
      "\tspeed: 0.1842s/iter; left time: 378.3808s\n",
      "\titers: 900, epoch: 8 | loss: 0.2481663\n",
      "\tspeed: 0.2086s/iter; left time: 407.6435s\n",
      "Epoch: 8 running time: 2.998710870742798 min.\n",
      "Epoch: 8, Steps: 951 | Train Loss: 0.2591326 Vali Loss: 0.5725902 Test Loss: 0.4390641\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2282997\n",
      "\tspeed: 0.6321s/iter; left time: 1139.6039s\n",
      "\titers: 200, epoch: 9 | loss: 0.2097238\n",
      "\tspeed: 0.1872s/iter; left time: 318.7854s\n",
      "\titers: 300, epoch: 9 | loss: 0.3651353\n",
      "\tspeed: 0.1865s/iter; left time: 298.9983s\n",
      "\titers: 400, epoch: 9 | loss: 0.2293399\n",
      "\tspeed: 0.1853s/iter; left time: 278.5717s\n",
      "\titers: 500, epoch: 9 | loss: 0.3315751\n",
      "\tspeed: 0.1886s/iter; left time: 264.5909s\n",
      "\titers: 600, epoch: 9 | loss: 0.2984387\n",
      "\tspeed: 0.1851s/iter; left time: 241.1345s\n",
      "\titers: 700, epoch: 9 | loss: 0.1925654\n",
      "\tspeed: 0.1905s/iter; left time: 229.1691s\n",
      "\titers: 800, epoch: 9 | loss: 0.2590722\n",
      "\tspeed: 0.1853s/iter; left time: 204.4069s\n",
      "\titers: 900, epoch: 9 | loss: 0.2495753\n",
      "\tspeed: 0.1859s/iter; left time: 186.4211s\n",
      "Epoch: 9 running time: 2.9654166221618654 min.\n",
      "Epoch: 9, Steps: 951 | Train Loss: 0.2589385 Vali Loss: 0.5716400 Test Loss: 0.4400660\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.440328985452652, mae:0.40719130635261536\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 2 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5629588\n",
      "\tspeed: 0.2633s/iter; left time: 2477.8476s\n",
      "\titers: 200, epoch: 1 | loss: 0.4396147\n",
      "\tspeed: 0.2566s/iter; left time: 2389.3623s\n",
      "\titers: 300, epoch: 1 | loss: 0.4173067\n",
      "\tspeed: 0.2585s/iter; left time: 2380.9574s\n",
      "\titers: 400, epoch: 1 | loss: 0.3548193\n",
      "\tspeed: 0.2377s/iter; left time: 2165.8850s\n",
      "\titers: 500, epoch: 1 | loss: 0.3554465\n",
      "\tspeed: 0.2500s/iter; left time: 2253.1584s\n",
      "\titers: 600, epoch: 1 | loss: 0.3692197\n",
      "\tspeed: 0.2545s/iter; left time: 2268.0729s\n",
      "\titers: 700, epoch: 1 | loss: 0.3172928\n",
      "\tspeed: 0.2442s/iter; left time: 2151.5909s\n",
      "\titers: 800, epoch: 1 | loss: 0.2884246\n",
      "\tspeed: 0.2504s/iter; left time: 2181.0234s\n",
      "\titers: 900, epoch: 1 | loss: 0.3082856\n",
      "\tspeed: 0.2439s/iter; left time: 2100.1119s\n",
      "Epoch: 1 running time: 3.967679738998413 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.4181503 Vali Loss: 0.5173140 Test Loss: 0.4150916\n",
      "Validation loss decreased (inf --> 0.517314).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2968092\n",
      "\tspeed: 0.7440s/iter; left time: 6294.0378s\n",
      "\titers: 200, epoch: 2 | loss: 0.3257899\n",
      "\tspeed: 0.2411s/iter; left time: 2015.7419s\n",
      "\titers: 300, epoch: 2 | loss: 0.2617401\n",
      "\tspeed: 0.2505s/iter; left time: 2069.0788s\n",
      "\titers: 400, epoch: 2 | loss: 0.2497798\n",
      "\tspeed: 0.2420s/iter; left time: 1974.4273s\n",
      "\titers: 500, epoch: 2 | loss: 0.2695893\n",
      "\tspeed: 0.2676s/iter; left time: 2156.8834s\n",
      "\titers: 600, epoch: 2 | loss: 0.3152319\n",
      "\tspeed: 0.2468s/iter; left time: 1964.7322s\n",
      "\titers: 700, epoch: 2 | loss: 0.4157215\n",
      "\tspeed: 0.2420s/iter; left time: 1902.1108s\n",
      "\titers: 800, epoch: 2 | loss: 0.2834806\n",
      "\tspeed: 0.2491s/iter; left time: 1932.7549s\n",
      "\titers: 900, epoch: 2 | loss: 0.3974758\n",
      "\tspeed: 0.2428s/iter; left time: 1860.2247s\n",
      "Epoch: 2 running time: 3.919164856274923 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3139247 Vali Loss: 0.5604933 Test Loss: 0.4216607\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2770340\n",
      "\tspeed: 0.8263s/iter; left time: 6205.0291s\n",
      "\titers: 200, epoch: 3 | loss: 0.3480883\n",
      "\tspeed: 0.2456s/iter; left time: 1819.5547s\n",
      "\titers: 300, epoch: 3 | loss: 0.3721225\n",
      "\tspeed: 0.2454s/iter; left time: 1793.9582s\n",
      "\titers: 400, epoch: 3 | loss: 0.2977997\n",
      "\tspeed: 0.2502s/iter; left time: 1803.6899s\n",
      "\titers: 500, epoch: 3 | loss: 0.2860091\n",
      "\tspeed: 0.2418s/iter; left time: 1718.8038s\n",
      "\titers: 600, epoch: 3 | loss: 0.2995236\n",
      "\tspeed: 0.2413s/iter; left time: 1690.9583s\n",
      "\titers: 700, epoch: 3 | loss: 0.3559653\n",
      "\tspeed: 0.2480s/iter; left time: 1713.5807s\n",
      "\titers: 800, epoch: 3 | loss: 0.2810177\n",
      "\tspeed: 0.2926s/iter; left time: 1992.2126s\n",
      "\titers: 900, epoch: 3 | loss: 0.3318444\n",
      "\tspeed: 0.2298s/iter; left time: 1541.6969s\n",
      "Epoch: 3 running time: 3.957081727186839 min.\n",
      "Epoch: 3, Steps: 951 | Train Loss: 0.3080926 Vali Loss: 0.5581094 Test Loss: 0.4483782\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3100637\n",
      "\tspeed: 0.8201s/iter; left time: 5377.9341s\n",
      "\titers: 200, epoch: 4 | loss: 0.3671353\n",
      "\tspeed: 0.2433s/iter; left time: 1571.3639s\n",
      "\titers: 300, epoch: 4 | loss: 0.2673966\n",
      "\tspeed: 0.2425s/iter; left time: 1541.5019s\n",
      "\titers: 400, epoch: 4 | loss: 0.2323629\n",
      "\tspeed: 0.2499s/iter; left time: 1563.5922s\n",
      "\titers: 500, epoch: 4 | loss: 0.3563589\n",
      "\tspeed: 0.2505s/iter; left time: 1542.4580s\n",
      "\titers: 600, epoch: 4 | loss: 0.3342963\n",
      "\tspeed: 0.2507s/iter; left time: 1518.8572s\n",
      "\titers: 700, epoch: 4 | loss: 0.3279363\n",
      "\tspeed: 0.2479s/iter; left time: 1477.2112s\n",
      "\titers: 800, epoch: 4 | loss: 0.3420931\n",
      "\tspeed: 0.2469s/iter; left time: 1446.1535s\n",
      "\titers: 900, epoch: 4 | loss: 0.3330595\n",
      "\tspeed: 0.2472s/iter; left time: 1423.2031s\n",
      "Epoch: 4 running time: 3.9275894204775494 min.\n",
      "Epoch: 4, Steps: 951 | Train Loss: 0.3042413 Vali Loss: 0.5342554 Test Loss: 0.4215220\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.4214131236076355, mae:0.4039125442504883\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 3 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl4_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6925252\n",
      "\tspeed: 0.3040s/iter; left time: 2860.7458s\n",
      "\titers: 200, epoch: 1 | loss: 0.5013836\n",
      "\tspeed: 0.2943s/iter; left time: 2739.8099s\n",
      "\titers: 300, epoch: 1 | loss: 0.3475897\n",
      "\tspeed: 0.2994s/iter; left time: 2757.7811s\n",
      "\titers: 400, epoch: 1 | loss: 0.3775092\n",
      "\tspeed: 0.2996s/iter; left time: 2730.1056s\n",
      "\titers: 500, epoch: 1 | loss: 0.4223146\n",
      "\tspeed: 0.2955s/iter; left time: 2662.6196s\n",
      "\titers: 600, epoch: 1 | loss: 0.3139987\n",
      "\tspeed: 0.3032s/iter; left time: 2702.1949s\n",
      "\titers: 700, epoch: 1 | loss: 0.3323351\n",
      "\tspeed: 0.2917s/iter; left time: 2570.1443s\n",
      "\titers: 800, epoch: 1 | loss: 0.2748648\n",
      "\tspeed: 0.2911s/iter; left time: 2535.6203s\n",
      "\titers: 900, epoch: 1 | loss: 0.3644424\n",
      "\tspeed: 0.3168s/iter; left time: 2727.8254s\n",
      "Epoch: 1 running time: 4.775242531299591 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.4296448 Vali Loss: 0.5413219 Test Loss: 0.4233636\n",
      "Validation loss decreased (inf --> 0.541322).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3696244\n",
      "\tspeed: 0.9799s/iter; left time: 8290.3404s\n",
      "\titers: 200, epoch: 2 | loss: 0.2765972\n",
      "\tspeed: 0.3053s/iter; left time: 2552.6139s\n",
      "\titers: 300, epoch: 2 | loss: 0.4066612\n",
      "\tspeed: 0.3008s/iter; left time: 2484.3143s\n",
      "\titers: 400, epoch: 2 | loss: 0.3500443\n",
      "\tspeed: 0.3009s/iter; left time: 2455.4894s\n",
      "\titers: 500, epoch: 2 | loss: 0.3691807\n",
      "\tspeed: 0.3041s/iter; left time: 2451.1996s\n",
      "\titers: 600, epoch: 2 | loss: 0.2600079\n",
      "\tspeed: 0.3096s/iter; left time: 2464.0749s\n",
      "\titers: 700, epoch: 2 | loss: 0.3343633\n",
      "\tspeed: 0.2508s/iter; left time: 1971.2231s\n",
      "\titers: 800, epoch: 2 | loss: 0.3432780\n",
      "\tspeed: 0.3048s/iter; left time: 2365.1516s\n",
      "\titers: 900, epoch: 2 | loss: 0.3436725\n",
      "\tspeed: 0.3038s/iter; left time: 2327.1612s\n",
      "Epoch: 2 running time: 4.784613180160522 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3143326 Vali Loss: 0.5575294 Test Loss: 0.4381422\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3366106\n",
      "\tspeed: 1.0686s/iter; left time: 8023.7461s\n",
      "\titers: 200, epoch: 3 | loss: 0.2571192\n",
      "\tspeed: 0.3042s/iter; left time: 2253.8719s\n",
      "\titers: 300, epoch: 3 | loss: 0.2949219\n",
      "\tspeed: 0.3014s/iter; left time: 2202.8529s\n",
      "\titers: 400, epoch: 3 | loss: 0.3542461\n",
      "\tspeed: 0.3067s/iter; left time: 2211.3303s\n",
      "\titers: 500, epoch: 3 | loss: 0.3337331\n",
      "\tspeed: 0.3054s/iter; left time: 2171.4040s\n",
      "\titers: 600, epoch: 3 | loss: 0.2462891\n",
      "\tspeed: 0.3067s/iter; left time: 2149.4347s\n",
      "\titers: 700, epoch: 3 | loss: 0.2825884\n",
      "\tspeed: 0.3049s/iter; left time: 2106.2919s\n",
      "\titers: 800, epoch: 3 | loss: 0.3316242\n",
      "\tspeed: 0.3003s/iter; left time: 2044.4219s\n",
      "\titers: 900, epoch: 3 | loss: 0.3085484\n",
      "\tspeed: 0.3085s/iter; left time: 2069.4244s\n",
      "Epoch: 3 running time: 4.906721206506093 min.\n",
      "Epoch: 3, Steps: 951 | Train Loss: 0.3080895 Vali Loss: 0.5655278 Test Loss: 0.4538541\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3088266\n",
      "\tspeed: 1.0398s/iter; left time: 6819.2167s\n",
      "\titers: 200, epoch: 4 | loss: 0.3841794\n",
      "\tspeed: 0.3013s/iter; left time: 1945.5644s\n",
      "\titers: 300, epoch: 4 | loss: 0.3426464\n",
      "\tspeed: 0.3055s/iter; left time: 1942.2230s\n",
      "\titers: 400, epoch: 4 | loss: 0.2988502\n",
      "\tspeed: 0.2953s/iter; left time: 1847.7622s\n",
      "\titers: 500, epoch: 4 | loss: 0.2877802\n",
      "\tspeed: 0.2625s/iter; left time: 1616.2555s\n",
      "\titers: 600, epoch: 4 | loss: 0.3586464\n",
      "\tspeed: 0.2419s/iter; left time: 1465.6963s\n",
      "\titers: 700, epoch: 4 | loss: 0.2752313\n",
      "\tspeed: 0.2731s/iter; left time: 1627.2340s\n",
      "\titers: 800, epoch: 4 | loss: 0.2525577\n",
      "\tspeed: 0.2722s/iter; left time: 1594.8267s\n",
      "\titers: 900, epoch: 4 | loss: 0.3981183\n",
      "\tspeed: 0.3028s/iter; left time: 1743.5247s\n",
      "Epoch: 4 running time: 4.536727464199066 min.\n",
      "Epoch: 4, Steps: 951 | Train Loss: 0.3046938 Vali Loss: 0.5818819 Test Loss: 0.4401085\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl4_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.439877450466156, mae:0.4154130220413208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 4 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6451275\n",
      "\tspeed: 0.3685s/iter; left time: 3467.8304s\n",
      "\titers: 200, epoch: 1 | loss: 0.4659448\n",
      "\tspeed: 0.3537s/iter; left time: 3293.6577s\n",
      "\titers: 300, epoch: 1 | loss: 0.4373887\n",
      "\tspeed: 0.3559s/iter; left time: 3278.6291s\n",
      "\titers: 400, epoch: 1 | loss: 0.4091117\n",
      "\tspeed: 0.3554s/iter; left time: 3237.9175s\n",
      "\titers: 500, epoch: 1 | loss: 0.4036097\n",
      "\tspeed: 0.3617s/iter; left time: 3259.2343s\n",
      "\titers: 600, epoch: 1 | loss: 0.2723704\n",
      "\tspeed: 0.3481s/iter; left time: 3101.9438s\n",
      "\titers: 700, epoch: 1 | loss: 0.3614541\n",
      "\tspeed: 0.3942s/iter; left time: 3472.9975s\n",
      "\titers: 800, epoch: 1 | loss: 0.3330092\n",
      "\tspeed: 0.3308s/iter; left time: 2881.4987s\n",
      "\titers: 900, epoch: 1 | loss: 0.3676375\n",
      "\tspeed: 0.3349s/iter; left time: 2884.1667s\n",
      "Epoch: 1 running time: 5.624558067321777 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.4209583 Vali Loss: 0.5518361 Test Loss: 0.4462260\n",
      "Validation loss decreased (inf --> 0.551836).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2718028\n",
      "\tspeed: 1.2239s/iter; left time: 10354.1009s\n",
      "\titers: 200, epoch: 2 | loss: 0.4156291\n",
      "\tspeed: 0.3569s/iter; left time: 2983.8622s\n",
      "\titers: 300, epoch: 2 | loss: 0.3505756\n",
      "\tspeed: 0.3526s/iter; left time: 2912.6693s\n",
      "\titers: 400, epoch: 2 | loss: 0.2953941\n",
      "\tspeed: 0.3570s/iter; left time: 2913.3543s\n",
      "\titers: 500, epoch: 2 | loss: 0.3201417\n",
      "\tspeed: 0.3078s/iter; left time: 2480.4679s\n",
      "\titers: 600, epoch: 2 | loss: 0.2982628\n",
      "\tspeed: 0.3850s/iter; left time: 3064.7259s\n",
      "\titers: 700, epoch: 2 | loss: 0.2886133\n",
      "\tspeed: 0.3442s/iter; left time: 2705.6079s\n",
      "\titers: 800, epoch: 2 | loss: 0.3132242\n",
      "\tspeed: 0.3612s/iter; left time: 2802.9765s\n",
      "\titers: 900, epoch: 2 | loss: 0.2783976\n",
      "\tspeed: 0.3586s/iter; left time: 2746.9360s\n",
      "Epoch: 2 running time: 5.617387545108795 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3152328 Vali Loss: 0.5902461 Test Loss: 0.4580575\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3025784\n",
      "\tspeed: 1.2287s/iter; left time: 9226.1979s\n",
      "\titers: 200, epoch: 3 | loss: 0.3488313\n",
      "\tspeed: 0.3606s/iter; left time: 2671.7811s\n",
      "\titers: 300, epoch: 3 | loss: 0.3036665\n",
      "\tspeed: 0.3636s/iter; left time: 2657.8157s\n",
      "\titers: 400, epoch: 3 | loss: 0.3199162\n",
      "\tspeed: 0.3899s/iter; left time: 2810.9103s\n",
      "\titers: 500, epoch: 3 | loss: 0.3405753\n",
      "\tspeed: 0.3557s/iter; left time: 2528.4808s\n",
      "\titers: 600, epoch: 3 | loss: 0.2831150\n",
      "\tspeed: 0.3596s/iter; left time: 2520.1852s\n",
      "\titers: 700, epoch: 3 | loss: 0.2757951\n",
      "\tspeed: 0.3617s/iter; left time: 2499.3146s\n",
      "\titers: 800, epoch: 3 | loss: 0.2828690\n",
      "\tspeed: 0.3611s/iter; left time: 2458.9731s\n",
      "\titers: 900, epoch: 3 | loss: 0.3142776\n",
      "\tspeed: 0.3625s/iter; left time: 2431.8176s\n",
      "Epoch: 3 running time: 5.769182733694712 min.\n",
      "Epoch: 3, Steps: 951 | Train Loss: 0.3089763 Vali Loss: 0.5499101 Test Loss: 0.4359493\n",
      "Validation loss decreased (0.551836 --> 0.549910).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2608531\n",
      "\tspeed: 1.2319s/iter; left time: 8078.8502s\n",
      "\titers: 200, epoch: 4 | loss: 0.2666556\n",
      "\tspeed: 0.3840s/iter; left time: 2480.0399s\n",
      "\titers: 300, epoch: 4 | loss: 0.2931675\n",
      "\tspeed: 0.3504s/iter; left time: 2227.8986s\n",
      "\titers: 400, epoch: 4 | loss: 0.3723831\n",
      "\tspeed: 0.3115s/iter; left time: 1949.4641s\n",
      "\titers: 500, epoch: 4 | loss: 0.2617837\n",
      "\tspeed: 0.3532s/iter; left time: 2175.0528s\n",
      "\titers: 600, epoch: 4 | loss: 0.2166860\n",
      "\tspeed: 0.3558s/iter; left time: 2155.6900s\n",
      "\titers: 700, epoch: 4 | loss: 0.3119849\n",
      "\tspeed: 0.3544s/iter; left time: 2111.3947s\n",
      "\titers: 800, epoch: 4 | loss: 0.2894043\n",
      "\tspeed: 0.3564s/iter; left time: 2087.7449s\n",
      "\titers: 900, epoch: 4 | loss: 0.2394595\n",
      "\tspeed: 0.3553s/iter; left time: 2045.8839s\n",
      "Epoch: 4 running time: 5.612051343917846 min.\n",
      "Epoch: 4, Steps: 951 | Train Loss: 0.2820764 Vali Loss: 0.5628141 Test Loss: 0.4420143\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3239182\n",
      "\tspeed: 1.2655s/iter; left time: 7095.5837s\n",
      "\titers: 200, epoch: 5 | loss: 0.2451817\n",
      "\tspeed: 0.3547s/iter; left time: 1953.2228s\n",
      "\titers: 300, epoch: 5 | loss: 0.2499569\n",
      "\tspeed: 0.3518s/iter; left time: 1902.1126s\n",
      "\titers: 400, epoch: 5 | loss: 0.3074337\n",
      "\tspeed: 0.3613s/iter; left time: 1917.4689s\n",
      "\titers: 500, epoch: 5 | loss: 0.2894968\n",
      "\tspeed: 0.3607s/iter; left time: 1878.1320s\n",
      "\titers: 600, epoch: 5 | loss: 0.2498654\n",
      "\tspeed: 0.3582s/iter; left time: 1829.3900s\n",
      "\titers: 700, epoch: 5 | loss: 0.2703929\n",
      "\tspeed: 0.3538s/iter; left time: 1771.7245s\n",
      "\titers: 800, epoch: 5 | loss: 0.2333638\n",
      "\tspeed: 0.3553s/iter; left time: 1743.3144s\n",
      "\titers: 900, epoch: 5 | loss: 0.2918534\n",
      "\tspeed: 0.3704s/iter; left time: 1780.5249s\n",
      "Epoch: 5 running time: 5.725687285264333 min.\n",
      "Epoch: 5, Steps: 951 | Train Loss: 0.2796692 Vali Loss: 0.5545157 Test Loss: 0.4380550\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.3109682\n",
      "\tspeed: 1.2077s/iter; left time: 5623.0752s\n",
      "\titers: 200, epoch: 6 | loss: 0.2884267\n",
      "\tspeed: 0.3616s/iter; left time: 1647.4133s\n",
      "\titers: 300, epoch: 6 | loss: 0.2665873\n",
      "\tspeed: 0.3498s/iter; left time: 1558.5411s\n",
      "\titers: 400, epoch: 6 | loss: 0.2689838\n",
      "\tspeed: 0.3610s/iter; left time: 1572.3749s\n",
      "\titers: 500, epoch: 6 | loss: 0.2711958\n",
      "\tspeed: 0.3533s/iter; left time: 1503.5292s\n",
      "\titers: 600, epoch: 6 | loss: 0.2605055\n",
      "\tspeed: 0.3542s/iter; left time: 1472.2449s\n",
      "\titers: 700, epoch: 6 | loss: 0.2603697\n",
      "\tspeed: 0.3593s/iter; left time: 1457.1395s\n",
      "\titers: 800, epoch: 6 | loss: 0.3003718\n",
      "\tspeed: 0.3733s/iter; left time: 1476.9348s\n",
      "\titers: 900, epoch: 6 | loss: 0.3573194\n",
      "\tspeed: 0.3615s/iter; left time: 1393.8867s\n",
      "Epoch: 6 running time: 5.696836535135905 min.\n",
      "Epoch: 6, Steps: 951 | Train Loss: 0.2803092 Vali Loss: 0.5648609 Test Loss: 0.4420541\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.44193562865257263, mae:0.40831807255744934\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 5 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.5846890\n",
      "\tspeed: 0.0769s/iter; left time: 714.5033s\n",
      "\titers: 200, epoch: 1 | loss: 0.3475232\n",
      "\tspeed: 0.0576s/iter; left time: 529.3475s\n",
      "\titers: 300, epoch: 1 | loss: 0.3225615\n",
      "\tspeed: 0.0546s/iter; left time: 496.3886s\n",
      "\titers: 400, epoch: 1 | loss: 0.4322594\n",
      "\tspeed: 0.0581s/iter; left time: 522.2346s\n",
      "\titers: 500, epoch: 1 | loss: 0.3482488\n",
      "\tspeed: 0.0578s/iter; left time: 513.8622s\n",
      "\titers: 600, epoch: 1 | loss: 0.3050084\n",
      "\tspeed: 0.0547s/iter; left time: 481.0005s\n",
      "\titers: 700, epoch: 1 | loss: 0.3001569\n",
      "\tspeed: 0.0547s/iter; left time: 475.3532s\n",
      "\titers: 800, epoch: 1 | loss: 0.2735440\n",
      "\tspeed: 0.0528s/iter; left time: 453.7545s\n",
      "\titers: 900, epoch: 1 | loss: 0.3970126\n",
      "\tspeed: 0.0536s/iter; left time: 454.9399s\n",
      "Epoch: 1 running time: 0.8815101385116577 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3927087 Vali Loss: 0.4839968 Test Loss: 0.6839352\n",
      "Validation loss decreased (inf --> 0.483997).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3589596\n",
      "\tspeed: 0.1598s/iter; left time: 1335.0521s\n",
      "\titers: 200, epoch: 2 | loss: 0.2795996\n",
      "\tspeed: 0.0557s/iter; left time: 459.9739s\n",
      "\titers: 300, epoch: 2 | loss: 0.2859835\n",
      "\tspeed: 0.0519s/iter; left time: 422.9833s\n",
      "\titers: 400, epoch: 2 | loss: 0.2958144\n",
      "\tspeed: 0.0546s/iter; left time: 439.6922s\n",
      "\titers: 500, epoch: 2 | loss: 0.2654315\n",
      "\tspeed: 0.0577s/iter; left time: 459.1994s\n",
      "\titers: 600, epoch: 2 | loss: 0.2606258\n",
      "\tspeed: 0.0594s/iter; left time: 466.1443s\n",
      "\titers: 700, epoch: 2 | loss: 0.2221496\n",
      "\tspeed: 0.0631s/iter; left time: 488.9850s\n",
      "\titers: 800, epoch: 2 | loss: 0.2582953\n",
      "\tspeed: 0.0576s/iter; left time: 440.9536s\n",
      "\titers: 900, epoch: 2 | loss: 0.2963994\n",
      "\tspeed: 0.0556s/iter; left time: 419.6177s\n",
      "Epoch: 2 running time: 0.8964163661003113 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2954035 Vali Loss: 0.5165864 Test Loss: 0.6999782\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3393297\n",
      "\tspeed: 0.1620s/iter; left time: 1200.9244s\n",
      "\titers: 200, epoch: 3 | loss: 0.2465124\n",
      "\tspeed: 0.0520s/iter; left time: 380.4156s\n",
      "\titers: 300, epoch: 3 | loss: 0.3947915\n",
      "\tspeed: 0.0564s/iter; left time: 406.8392s\n",
      "\titers: 400, epoch: 3 | loss: 0.4065081\n",
      "\tspeed: 0.0544s/iter; left time: 386.9349s\n",
      "\titers: 500, epoch: 3 | loss: 0.3108299\n",
      "\tspeed: 0.0581s/iter; left time: 407.4384s\n",
      "\titers: 600, epoch: 3 | loss: 0.3866495\n",
      "\tspeed: 0.0545s/iter; left time: 376.5240s\n",
      "\titers: 700, epoch: 3 | loss: 0.2280141\n",
      "\tspeed: 0.0560s/iter; left time: 381.7683s\n",
      "\titers: 800, epoch: 3 | loss: 0.2385044\n",
      "\tspeed: 0.0501s/iter; left time: 336.0036s\n",
      "\titers: 900, epoch: 3 | loss: 0.2765742\n",
      "\tspeed: 0.0557s/iter; left time: 368.4481s\n",
      "Epoch: 3 running time: 0.8696665008862813 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2870860 Vali Loss: 0.5012509 Test Loss: 0.6538504\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2698189\n",
      "\tspeed: 0.1663s/iter; left time: 1076.7163s\n",
      "\titers: 200, epoch: 4 | loss: 0.2044309\n",
      "\tspeed: 0.0555s/iter; left time: 354.0007s\n",
      "\titers: 300, epoch: 4 | loss: 0.3196722\n",
      "\tspeed: 0.0540s/iter; left time: 338.9470s\n",
      "\titers: 400, epoch: 4 | loss: 0.2950669\n",
      "\tspeed: 0.0558s/iter; left time: 344.4489s\n",
      "\titers: 500, epoch: 4 | loss: 0.2445747\n",
      "\tspeed: 0.0563s/iter; left time: 341.9149s\n",
      "\titers: 600, epoch: 4 | loss: 0.2638303\n",
      "\tspeed: 0.0555s/iter; left time: 331.2886s\n",
      "\titers: 700, epoch: 4 | loss: 0.2904097\n",
      "\tspeed: 0.0600s/iter; left time: 352.2434s\n",
      "\titers: 800, epoch: 4 | loss: 0.2889928\n",
      "\tspeed: 0.0589s/iter; left time: 339.9621s\n",
      "\titers: 900, epoch: 4 | loss: 0.2502265\n",
      "\tspeed: 0.0507s/iter; left time: 287.4868s\n",
      "Epoch: 4 running time: 0.8901489973068237 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2846830 Vali Loss: 0.4924122 Test Loss: 0.6661170\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6669511795043945, mae:0.49454358220100403\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 3 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6903490\n",
      "\tspeed: 0.1461s/iter; left time: 1357.1336s\n",
      "\titers: 200, epoch: 1 | loss: 0.4752216\n",
      "\tspeed: 0.0985s/iter; left time: 905.6919s\n",
      "\titers: 300, epoch: 1 | loss: 0.3777922\n",
      "\tspeed: 0.1132s/iter; left time: 1029.4739s\n",
      "\titers: 400, epoch: 1 | loss: 0.3623431\n",
      "\tspeed: 0.0874s/iter; left time: 785.9209s\n",
      "\titers: 500, epoch: 1 | loss: 0.3333850\n",
      "\tspeed: 0.0939s/iter; left time: 835.3008s\n",
      "\titers: 600, epoch: 1 | loss: 0.2815011\n",
      "\tspeed: 0.1108s/iter; left time: 974.3129s\n",
      "\titers: 700, epoch: 1 | loss: 0.3888963\n",
      "\tspeed: 0.0886s/iter; left time: 770.0692s\n",
      "\titers: 800, epoch: 1 | loss: 0.3795986\n",
      "\tspeed: 0.1013s/iter; left time: 870.1836s\n",
      "\titers: 900, epoch: 1 | loss: 0.3074558\n",
      "\tspeed: 0.1013s/iter; left time: 859.9714s\n",
      "Epoch: 1 running time: 1.6102526942888895 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4071100 Vali Loss: 0.5395414 Test Loss: 0.7552096\n",
      "Validation loss decreased (inf --> 0.539541).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2898625\n",
      "\tspeed: 0.2773s/iter; left time: 2315.6617s\n",
      "\titers: 200, epoch: 2 | loss: 0.3112490\n",
      "\tspeed: 0.0948s/iter; left time: 782.3039s\n",
      "\titers: 300, epoch: 2 | loss: 0.3047902\n",
      "\tspeed: 0.1055s/iter; left time: 860.3920s\n",
      "\titers: 400, epoch: 2 | loss: 0.3030248\n",
      "\tspeed: 0.0921s/iter; left time: 741.8123s\n",
      "\titers: 500, epoch: 2 | loss: 0.2426786\n",
      "\tspeed: 0.1030s/iter; left time: 818.7083s\n",
      "\titers: 600, epoch: 2 | loss: 0.3847619\n",
      "\tspeed: 0.1015s/iter; left time: 797.2493s\n",
      "\titers: 700, epoch: 2 | loss: 0.3298605\n",
      "\tspeed: 0.0870s/iter; left time: 674.6311s\n",
      "\titers: 800, epoch: 2 | loss: 0.2715181\n",
      "\tspeed: 0.0991s/iter; left time: 758.5295s\n",
      "\titers: 900, epoch: 2 | loss: 0.2437565\n",
      "\tspeed: 0.1020s/iter; left time: 770.1985s\n",
      "Epoch: 2 running time: 1.5216052651405334 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2977419 Vali Loss: 0.4900926 Test Loss: 0.6335091\n",
      "Validation loss decreased (0.539541 --> 0.490093).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2197417\n",
      "\tspeed: 0.2914s/iter; left time: 2159.9690s\n",
      "\titers: 200, epoch: 3 | loss: 0.2878434\n",
      "\tspeed: 0.0947s/iter; left time: 692.5822s\n",
      "\titers: 300, epoch: 3 | loss: 0.2280357\n",
      "\tspeed: 0.0977s/iter; left time: 704.3879s\n",
      "\titers: 400, epoch: 3 | loss: 0.2238168\n",
      "\tspeed: 0.1046s/iter; left time: 743.9953s\n",
      "\titers: 500, epoch: 3 | loss: 0.2070825\n",
      "\tspeed: 0.0952s/iter; left time: 667.4854s\n",
      "\titers: 600, epoch: 3 | loss: 0.2217783\n",
      "\tspeed: 0.1014s/iter; left time: 700.8524s\n",
      "\titers: 700, epoch: 3 | loss: 0.3131665\n",
      "\tspeed: 0.1063s/iter; left time: 724.4788s\n",
      "\titers: 800, epoch: 3 | loss: 0.2172683\n",
      "\tspeed: 0.0912s/iter; left time: 612.1235s\n",
      "\titers: 900, epoch: 3 | loss: 0.2130647\n",
      "\tspeed: 0.1108s/iter; left time: 732.6214s\n",
      "Epoch: 3 running time: 1.5888168136278789 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2557704 Vali Loss: 0.5351897 Test Loss: 0.7142332\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2594420\n",
      "\tspeed: 0.2885s/iter; left time: 1867.4847s\n",
      "\titers: 200, epoch: 4 | loss: 0.2542996\n",
      "\tspeed: 0.0963s/iter; left time: 614.1112s\n",
      "\titers: 300, epoch: 4 | loss: 0.2453306\n",
      "\tspeed: 0.0910s/iter; left time: 571.2066s\n",
      "\titers: 400, epoch: 4 | loss: 0.3137099\n",
      "\tspeed: 0.1048s/iter; left time: 647.3319s\n",
      "\titers: 500, epoch: 4 | loss: 0.2254761\n",
      "\tspeed: 0.0986s/iter; left time: 598.9162s\n",
      "\titers: 600, epoch: 4 | loss: 0.3074834\n",
      "\tspeed: 0.0892s/iter; left time: 532.5897s\n",
      "\titers: 700, epoch: 4 | loss: 0.2208290\n",
      "\tspeed: 0.1121s/iter; left time: 658.6675s\n",
      "\titers: 800, epoch: 4 | loss: 0.2173367\n",
      "\tspeed: 0.0983s/iter; left time: 567.5831s\n",
      "\titers: 900, epoch: 4 | loss: 0.2305159\n",
      "\tspeed: 0.0969s/iter; left time: 549.8332s\n",
      "Epoch: 4 running time: 1.5521041750907898 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2500553 Vali Loss: 0.5453894 Test Loss: 0.7202582\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3051056\n",
      "\tspeed: 0.2877s/iter; left time: 1592.3626s\n",
      "\titers: 200, epoch: 5 | loss: 0.2863479\n",
      "\tspeed: 0.1156s/iter; left time: 628.1382s\n",
      "\titers: 300, epoch: 5 | loss: 0.3400606\n",
      "\tspeed: 0.0954s/iter; left time: 508.8319s\n",
      "\titers: 400, epoch: 5 | loss: 0.3653413\n",
      "\tspeed: 0.0918s/iter; left time: 480.6791s\n",
      "\titers: 500, epoch: 5 | loss: 0.2944063\n",
      "\tspeed: 0.1083s/iter; left time: 555.9213s\n",
      "\titers: 600, epoch: 5 | loss: 0.3015046\n",
      "\tspeed: 0.0861s/iter; left time: 433.6654s\n",
      "\titers: 700, epoch: 5 | loss: 0.2344951\n",
      "\tspeed: 0.1005s/iter; left time: 495.9380s\n",
      "\titers: 800, epoch: 5 | loss: 0.3019577\n",
      "\tspeed: 0.1039s/iter; left time: 502.3814s\n",
      "\titers: 900, epoch: 5 | loss: 0.2409422\n",
      "\tspeed: 0.0952s/iter; left time: 450.7459s\n",
      "Epoch: 5 running time: 1.5488781770070394 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2507698 Vali Loss: 0.5322880 Test Loss: 0.6943022\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6940097808837891, mae:0.5008580088615417\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 5 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el3_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.4479353\n",
      "\tspeed: 0.1135s/iter; left time: 1054.9520s\n",
      "\titers: 200, epoch: 1 | loss: 0.3741404\n",
      "\tspeed: 0.0950s/iter; left time: 873.2347s\n",
      "\titers: 300, epoch: 1 | loss: 0.3645598\n",
      "\tspeed: 0.0867s/iter; left time: 788.3400s\n",
      "\titers: 400, epoch: 1 | loss: 0.3582897\n",
      "\tspeed: 0.0967s/iter; left time: 869.4632s\n",
      "\titers: 500, epoch: 1 | loss: 0.2958503\n",
      "\tspeed: 0.0882s/iter; left time: 784.2466s\n",
      "\titers: 600, epoch: 1 | loss: 0.2719029\n",
      "\tspeed: 0.0906s/iter; left time: 796.9037s\n",
      "\titers: 700, epoch: 1 | loss: 0.2582170\n",
      "\tspeed: 0.0889s/iter; left time: 772.2398s\n",
      "\titers: 800, epoch: 1 | loss: 0.2941581\n",
      "\tspeed: 0.0905s/iter; left time: 777.8923s\n",
      "\titers: 900, epoch: 1 | loss: 0.2963971\n",
      "\tspeed: 0.0938s/iter; left time: 796.6215s\n",
      "Epoch: 1 running time: 1.4416791081428528 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3748130 Vali Loss: 0.5303699 Test Loss: 0.6891758\n",
      "Validation loss decreased (inf --> 0.530370).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3219392\n",
      "\tspeed: 0.1975s/iter; left time: 1649.3420s\n",
      "\titers: 200, epoch: 2 | loss: 0.2555420\n",
      "\tspeed: 0.0812s/iter; left time: 670.3853s\n",
      "\titers: 300, epoch: 2 | loss: 0.2879037\n",
      "\tspeed: 0.0960s/iter; left time: 782.1952s\n",
      "\titers: 400, epoch: 2 | loss: 0.3013706\n",
      "\tspeed: 0.0878s/iter; left time: 707.2605s\n",
      "\titers: 500, epoch: 2 | loss: 0.3088601\n",
      "\tspeed: 0.0920s/iter; left time: 731.2243s\n",
      "\titers: 600, epoch: 2 | loss: 0.2756523\n",
      "\tspeed: 0.0884s/iter; left time: 694.4556s\n",
      "\titers: 700, epoch: 2 | loss: 0.2667270\n",
      "\tspeed: 0.0886s/iter; left time: 686.5724s\n",
      "\titers: 800, epoch: 2 | loss: 0.2477957\n",
      "\tspeed: 0.0964s/iter; left time: 737.6124s\n",
      "\titers: 900, epoch: 2 | loss: 0.2779084\n",
      "\tspeed: 0.0876s/iter; left time: 661.8372s\n",
      "Epoch: 2 running time: 1.3897498965263366 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2815387 Vali Loss: 0.5756713 Test Loss: 0.7324072\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2321064\n",
      "\tspeed: 0.2072s/iter; left time: 1535.6267s\n",
      "\titers: 200, epoch: 3 | loss: 0.2339470\n",
      "\tspeed: 0.0970s/iter; left time: 709.2602s\n",
      "\titers: 300, epoch: 3 | loss: 0.2508437\n",
      "\tspeed: 0.0897s/iter; left time: 646.6639s\n",
      "\titers: 400, epoch: 3 | loss: 0.3268474\n",
      "\tspeed: 0.0880s/iter; left time: 625.6691s\n",
      "\titers: 500, epoch: 3 | loss: 0.2443049\n",
      "\tspeed: 0.0880s/iter; left time: 617.3745s\n",
      "\titers: 600, epoch: 3 | loss: 0.2528357\n",
      "\tspeed: 0.0970s/iter; left time: 670.2941s\n",
      "\titers: 700, epoch: 3 | loss: 0.2972489\n",
      "\tspeed: 0.0881s/iter; left time: 600.4563s\n",
      "\titers: 800, epoch: 3 | loss: 0.2260821\n",
      "\tspeed: 0.0903s/iter; left time: 606.3383s\n",
      "\titers: 900, epoch: 3 | loss: 0.2970836\n",
      "\tspeed: 0.0874s/iter; left time: 578.0153s\n",
      "Epoch: 3 running time: 1.4152067184448243 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2723303 Vali Loss: 0.5180216 Test Loss: 0.7012784\n",
      "Validation loss decreased (0.530370 --> 0.518022).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1754325\n",
      "\tspeed: 0.2171s/iter; left time: 1405.6161s\n",
      "\titers: 200, epoch: 4 | loss: 0.3159214\n",
      "\tspeed: 0.0910s/iter; left time: 580.1943s\n",
      "\titers: 300, epoch: 4 | loss: 0.2058543\n",
      "\tspeed: 0.0868s/iter; left time: 544.4062s\n",
      "\titers: 400, epoch: 4 | loss: 0.1598521\n",
      "\tspeed: 0.0858s/iter; left time: 529.4636s\n",
      "\titers: 500, epoch: 4 | loss: 0.2912533\n",
      "\tspeed: 0.0877s/iter; left time: 532.8469s\n",
      "\titers: 600, epoch: 4 | loss: 0.2281765\n",
      "\tspeed: 0.0951s/iter; left time: 567.9021s\n",
      "\titers: 700, epoch: 4 | loss: 0.2369843\n",
      "\tspeed: 0.0890s/iter; left time: 523.0053s\n",
      "\titers: 800, epoch: 4 | loss: 0.2223687\n",
      "\tspeed: 0.0881s/iter; left time: 508.6559s\n",
      "\titers: 900, epoch: 4 | loss: 0.2088963\n",
      "\tspeed: 0.0917s/iter; left time: 520.4781s\n",
      "Epoch: 4 running time: 1.4004667321840922 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2316216 Vali Loss: 0.5576540 Test Loss: 0.6897295\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2513526\n",
      "\tspeed: 0.2197s/iter; left time: 1216.2547s\n",
      "\titers: 200, epoch: 5 | loss: 0.1831914\n",
      "\tspeed: 0.0917s/iter; left time: 498.6302s\n",
      "\titers: 300, epoch: 5 | loss: 0.2311211\n",
      "\tspeed: 0.0884s/iter; left time: 471.4072s\n",
      "\titers: 400, epoch: 5 | loss: 0.2319108\n",
      "\tspeed: 0.0882s/iter; left time: 461.7314s\n",
      "\titers: 500, epoch: 5 | loss: 0.2647802\n",
      "\tspeed: 0.0884s/iter; left time: 453.9936s\n",
      "\titers: 600, epoch: 5 | loss: 0.1641247\n",
      "\tspeed: 0.0939s/iter; left time: 473.0266s\n",
      "\titers: 700, epoch: 5 | loss: 0.2555897\n",
      "\tspeed: 0.0866s/iter; left time: 427.2748s\n",
      "\titers: 800, epoch: 5 | loss: 0.2059496\n",
      "\tspeed: 0.0862s/iter; left time: 416.6103s\n",
      "\titers: 900, epoch: 5 | loss: 0.2166138\n",
      "\tspeed: 0.0906s/iter; left time: 428.8311s\n",
      "Epoch: 5 running time: 1.411656371752421 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2290578 Vali Loss: 0.5412562 Test Loss: 0.6965559\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2225716\n",
      "\tspeed: 0.2145s/iter; left time: 985.6494s\n",
      "\titers: 200, epoch: 6 | loss: 0.1699059\n",
      "\tspeed: 0.0882s/iter; left time: 396.5305s\n",
      "\titers: 300, epoch: 6 | loss: 0.2183810\n",
      "\tspeed: 0.0869s/iter; left time: 381.9317s\n",
      "\titers: 400, epoch: 6 | loss: 0.3054489\n",
      "\tspeed: 0.0878s/iter; left time: 377.3393s\n",
      "\titers: 500, epoch: 6 | loss: 0.2406327\n",
      "\tspeed: 0.0930s/iter; left time: 390.1318s\n",
      "\titers: 600, epoch: 6 | loss: 0.2381927\n",
      "\tspeed: 0.0869s/iter; left time: 355.8469s\n",
      "\titers: 700, epoch: 6 | loss: 0.1565809\n",
      "\tspeed: 0.0878s/iter; left time: 350.7311s\n",
      "\titers: 800, epoch: 6 | loss: 0.2006986\n",
      "\tspeed: 0.0883s/iter; left time: 344.1505s\n",
      "\titers: 900, epoch: 6 | loss: 0.2632939\n",
      "\tspeed: 0.0907s/iter; left time: 344.3594s\n",
      "Epoch: 6 running time: 1.4017518003781637 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2313061 Vali Loss: 0.5281875 Test Loss: 0.6948586\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el3_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.69614177942276, mae:0.4940055012702942\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 3 \\\n",
    "  --d_layers 3 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.4420350\n",
      "\tspeed: 0.1172s/iter; left time: 1088.8234s\n",
      "\titers: 200, epoch: 1 | loss: 0.2913979\n",
      "\tspeed: 0.0851s/iter; left time: 782.4764s\n",
      "\titers: 300, epoch: 1 | loss: 0.3384283\n",
      "\tspeed: 0.0751s/iter; left time: 682.5596s\n",
      "\titers: 400, epoch: 1 | loss: 0.3608872\n",
      "\tspeed: 0.0922s/iter; left time: 828.9005s\n",
      "\titers: 500, epoch: 1 | loss: 0.3856245\n",
      "\tspeed: 0.0812s/iter; left time: 722.2347s\n",
      "\titers: 600, epoch: 1 | loss: 0.3024383\n",
      "\tspeed: 0.0784s/iter; left time: 689.4299s\n",
      "\titers: 700, epoch: 1 | loss: 0.3283021\n",
      "\tspeed: 0.0901s/iter; left time: 783.0923s\n",
      "\titers: 800, epoch: 1 | loss: 0.2953873\n",
      "\tspeed: 0.0834s/iter; left time: 716.6688s\n",
      "\titers: 900, epoch: 1 | loss: 0.4737537\n",
      "\tspeed: 0.0771s/iter; left time: 654.9533s\n",
      "Epoch: 1 running time: 1.3363105138142903 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3607818 Vali Loss: 0.5203818 Test Loss: 0.6823179\n",
      "Validation loss decreased (inf --> 0.520382).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3754005\n",
      "\tspeed: 0.2359s/iter; left time: 1970.2681s\n",
      "\titers: 200, epoch: 2 | loss: 0.3439583\n",
      "\tspeed: 0.0876s/iter; left time: 723.2269s\n",
      "\titers: 300, epoch: 2 | loss: 0.2859876\n",
      "\tspeed: 0.0843s/iter; left time: 687.3612s\n",
      "\titers: 400, epoch: 2 | loss: 0.2801615\n",
      "\tspeed: 0.0790s/iter; left time: 636.0375s\n",
      "\titers: 500, epoch: 2 | loss: 0.2596623\n",
      "\tspeed: 0.0875s/iter; left time: 695.7919s\n",
      "\titers: 600, epoch: 2 | loss: 0.2079262\n",
      "\tspeed: 0.0864s/iter; left time: 678.3325s\n",
      "\titers: 700, epoch: 2 | loss: 0.2804331\n",
      "\tspeed: 0.0783s/iter; left time: 606.6836s\n",
      "\titers: 800, epoch: 2 | loss: 0.2833836\n",
      "\tspeed: 0.0867s/iter; left time: 663.2206s\n",
      "\titers: 900, epoch: 2 | loss: 0.2230399\n",
      "\tspeed: 0.0849s/iter; left time: 641.1574s\n",
      "Epoch: 2 running time: 1.3099960724512736 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2767171 Vali Loss: 0.5437708 Test Loss: 0.7159970\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2632788\n",
      "\tspeed: 0.2419s/iter; left time: 1792.9364s\n",
      "\titers: 200, epoch: 3 | loss: 0.3014497\n",
      "\tspeed: 0.0772s/iter; left time: 564.2145s\n",
      "\titers: 300, epoch: 3 | loss: 0.2670193\n",
      "\tspeed: 0.0763s/iter; left time: 550.6839s\n",
      "\titers: 400, epoch: 3 | loss: 0.3535962\n",
      "\tspeed: 0.0935s/iter; left time: 664.9300s\n",
      "\titers: 500, epoch: 3 | loss: 0.2383446\n",
      "\tspeed: 0.0816s/iter; left time: 572.4977s\n",
      "\titers: 600, epoch: 3 | loss: 0.2560691\n",
      "\tspeed: 0.0747s/iter; left time: 516.6235s\n",
      "\titers: 700, epoch: 3 | loss: 0.2272770\n",
      "\tspeed: 0.0898s/iter; left time: 611.7847s\n",
      "\titers: 800, epoch: 3 | loss: 0.1869066\n",
      "\tspeed: 0.0805s/iter; left time: 540.2276s\n",
      "\titers: 900, epoch: 3 | loss: 0.2212516\n",
      "\tspeed: 0.0773s/iter; left time: 511.4355s\n",
      "Epoch: 3 running time: 1.3009437322616577 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2634108 Vali Loss: 0.5444070 Test Loss: 0.7031043\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2876417\n",
      "\tspeed: 0.2405s/iter; left time: 1557.0036s\n",
      "\titers: 200, epoch: 4 | loss: 0.2483470\n",
      "\tspeed: 0.0835s/iter; left time: 532.2865s\n",
      "\titers: 300, epoch: 4 | loss: 0.2695302\n",
      "\tspeed: 0.0888s/iter; left time: 557.3234s\n",
      "\titers: 400, epoch: 4 | loss: 0.2990396\n",
      "\tspeed: 0.0723s/iter; left time: 446.5065s\n",
      "\titers: 500, epoch: 4 | loss: 0.2231608\n",
      "\tspeed: 0.0838s/iter; left time: 509.1254s\n",
      "\titers: 600, epoch: 4 | loss: 0.2468629\n",
      "\tspeed: 0.0903s/iter; left time: 539.4348s\n",
      "\titers: 700, epoch: 4 | loss: 0.2366886\n",
      "\tspeed: 0.0798s/iter; left time: 468.9954s\n",
      "\titers: 800, epoch: 4 | loss: 0.2121254\n",
      "\tspeed: 0.0804s/iter; left time: 464.1056s\n",
      "\titers: 900, epoch: 4 | loss: 0.2714798\n",
      "\tspeed: 0.0969s/iter; left time: 549.8570s\n",
      "Epoch: 4 running time: 1.3157716393470764 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2595688 Vali Loss: 0.5414866 Test Loss: 0.6875662\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6872633695602417, mae:0.49675434827804565\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 4 \\\n",
    "  --d_layers 3 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.5640328\n",
      "\tspeed: 0.0895s/iter; left time: 831.6055s\n",
      "\titers: 200, epoch: 1 | loss: 0.4044257\n",
      "\tspeed: 0.0789s/iter; left time: 725.5688s\n",
      "\titers: 300, epoch: 1 | loss: 0.2757474\n",
      "\tspeed: 0.0623s/iter; left time: 566.6315s\n",
      "\titers: 400, epoch: 1 | loss: 0.4992739\n",
      "\tspeed: 0.0682s/iter; left time: 613.1427s\n",
      "\titers: 500, epoch: 1 | loss: 0.3228981\n",
      "\tspeed: 0.0715s/iter; left time: 635.8237s\n",
      "\titers: 600, epoch: 1 | loss: 0.2940255\n",
      "\tspeed: 0.0767s/iter; left time: 674.4515s\n",
      "\titers: 700, epoch: 1 | loss: 0.2463365\n",
      "\tspeed: 0.0663s/iter; left time: 576.0224s\n",
      "\titers: 800, epoch: 1 | loss: 0.3332577\n",
      "\tspeed: 0.0643s/iter; left time: 552.4396s\n",
      "\titers: 900, epoch: 1 | loss: 0.3320244\n",
      "\tspeed: 0.0716s/iter; left time: 607.7356s\n",
      "Epoch: 1 running time: 1.1198751091957093 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3564506 Vali Loss: 0.5391698 Test Loss: 0.7027764\n",
      "Validation loss decreased (inf --> 0.539170).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3107933\n",
      "\tspeed: 0.2012s/iter; left time: 1680.5597s\n",
      "\titers: 200, epoch: 2 | loss: 0.2367918\n",
      "\tspeed: 0.0796s/iter; left time: 656.4725s\n",
      "\titers: 300, epoch: 2 | loss: 0.2536191\n",
      "\tspeed: 0.0691s/iter; left time: 563.0304s\n",
      "\titers: 400, epoch: 2 | loss: 0.2263752\n",
      "\tspeed: 0.0649s/iter; left time: 522.8652s\n",
      "\titers: 500, epoch: 2 | loss: 0.2547895\n",
      "\tspeed: 0.0714s/iter; left time: 567.7936s\n",
      "\titers: 600, epoch: 2 | loss: 0.2062609\n",
      "\tspeed: 0.0793s/iter; left time: 622.5137s\n",
      "\titers: 700, epoch: 2 | loss: 0.2528280\n",
      "\tspeed: 0.0632s/iter; left time: 489.9849s\n",
      "\titers: 800, epoch: 2 | loss: 0.2543326\n",
      "\tspeed: 0.0615s/iter; left time: 470.5356s\n",
      "\titers: 900, epoch: 2 | loss: 0.2570190\n",
      "\tspeed: 0.0722s/iter; left time: 545.2996s\n",
      "Epoch: 2 running time: 1.1050272941589356 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2742101 Vali Loss: 0.5483174 Test Loss: 0.7114063\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2494182\n",
      "\tspeed: 0.1920s/iter; left time: 1423.4562s\n",
      "\titers: 200, epoch: 3 | loss: 0.3396971\n",
      "\tspeed: 0.0759s/iter; left time: 554.8465s\n",
      "\titers: 300, epoch: 3 | loss: 0.2406197\n",
      "\tspeed: 0.0674s/iter; left time: 485.9226s\n",
      "\titers: 400, epoch: 3 | loss: 0.2355893\n",
      "\tspeed: 0.0623s/iter; left time: 443.2039s\n",
      "\titers: 500, epoch: 3 | loss: 0.2623782\n",
      "\tspeed: 0.0684s/iter; left time: 479.5361s\n",
      "\titers: 600, epoch: 3 | loss: 0.2246383\n",
      "\tspeed: 0.0756s/iter; left time: 522.6075s\n",
      "\titers: 700, epoch: 3 | loss: 0.2314495\n",
      "\tspeed: 0.0694s/iter; left time: 472.6329s\n",
      "\titers: 800, epoch: 3 | loss: 0.2894511\n",
      "\tspeed: 0.0607s/iter; left time: 407.3242s\n",
      "\titers: 900, epoch: 3 | loss: 0.1787766\n",
      "\tspeed: 0.0660s/iter; left time: 436.6643s\n",
      "Epoch: 3 running time: 1.0760322610537212 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2596544 Vali Loss: 0.5167230 Test Loss: 0.6811156\n",
      "Validation loss decreased (0.539170 --> 0.516723).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2786396\n",
      "\tspeed: 0.2012s/iter; left time: 1302.4971s\n",
      "\titers: 200, epoch: 4 | loss: 0.1996232\n",
      "\tspeed: 0.0690s/iter; left time: 440.0569s\n",
      "\titers: 300, epoch: 4 | loss: 0.1568614\n",
      "\tspeed: 0.0765s/iter; left time: 479.9673s\n",
      "\titers: 400, epoch: 4 | loss: 0.2030590\n",
      "\tspeed: 0.0594s/iter; left time: 367.0278s\n",
      "\titers: 500, epoch: 4 | loss: 0.2128104\n",
      "\tspeed: 0.0674s/iter; left time: 409.4072s\n",
      "\titers: 600, epoch: 4 | loss: 0.2434413\n",
      "\tspeed: 0.0722s/iter; left time: 431.2996s\n",
      "\titers: 700, epoch: 4 | loss: 0.2099820\n",
      "\tspeed: 0.0782s/iter; left time: 459.0995s\n",
      "\titers: 800, epoch: 4 | loss: 0.2256811\n",
      "\tspeed: 0.0613s/iter; left time: 354.1555s\n",
      "\titers: 900, epoch: 4 | loss: 0.1926836\n",
      "\tspeed: 0.0597s/iter; left time: 338.5584s\n",
      "Epoch: 4 running time: 1.0578638990720113 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2084396 Vali Loss: 0.5428152 Test Loss: 0.7152766\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2258551\n",
      "\tspeed: 0.1967s/iter; left time: 1088.8129s\n",
      "\titers: 200, epoch: 5 | loss: 0.2013590\n",
      "\tspeed: 0.0653s/iter; left time: 354.7518s\n",
      "\titers: 300, epoch: 5 | loss: 0.2095477\n",
      "\tspeed: 0.0759s/iter; left time: 404.9615s\n",
      "\titers: 400, epoch: 5 | loss: 0.1570509\n",
      "\tspeed: 0.0639s/iter; left time: 334.3679s\n",
      "\titers: 500, epoch: 5 | loss: 0.1594193\n",
      "\tspeed: 0.0637s/iter; left time: 327.0130s\n",
      "\titers: 600, epoch: 5 | loss: 0.1507765\n",
      "\tspeed: 0.0651s/iter; left time: 327.6499s\n",
      "\titers: 700, epoch: 5 | loss: 0.1795943\n",
      "\tspeed: 0.0771s/iter; left time: 380.3030s\n",
      "\titers: 800, epoch: 5 | loss: 0.1627337\n",
      "\tspeed: 0.0643s/iter; left time: 310.9285s\n",
      "\titers: 900, epoch: 5 | loss: 0.1598172\n",
      "\tspeed: 0.0632s/iter; left time: 299.4047s\n",
      "Epoch: 5 running time: 1.0484286348025005 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2067277 Vali Loss: 0.5333205 Test Loss: 0.7073026\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2340366\n",
      "\tspeed: 0.1986s/iter; left time: 912.6898s\n",
      "\titers: 200, epoch: 6 | loss: 0.2263941\n",
      "\tspeed: 0.0687s/iter; left time: 308.8203s\n",
      "\titers: 300, epoch: 6 | loss: 0.2230766\n",
      "\tspeed: 0.0727s/iter; left time: 319.7427s\n",
      "\titers: 400, epoch: 6 | loss: 0.1875015\n",
      "\tspeed: 0.0730s/iter; left time: 313.5103s\n",
      "\titers: 500, epoch: 6 | loss: 0.2048168\n",
      "\tspeed: 0.0673s/iter; left time: 282.5580s\n",
      "\titers: 600, epoch: 6 | loss: 0.1985764\n",
      "\tspeed: 0.0620s/iter; left time: 254.0472s\n",
      "\titers: 700, epoch: 6 | loss: 0.1866997\n",
      "\tspeed: 0.0720s/iter; left time: 287.7628s\n",
      "\titers: 800, epoch: 6 | loss: 0.1889175\n",
      "\tspeed: 0.0768s/iter; left time: 299.0715s\n",
      "\titers: 900, epoch: 6 | loss: 0.2099251\n",
      "\tspeed: 0.0650s/iter; left time: 246.7236s\n",
      "Epoch: 6 running time: 1.0757810592651367 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2085836 Vali Loss: 0.5334154 Test Loss: 0.7062182\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.7067193388938904, mae:0.486599326133728\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 4 \\\n",
    "  --d_layers 2 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el5_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.2724705\n",
      "\tspeed: 0.1007s/iter; left time: 935.3776s\n",
      "\titers: 200, epoch: 1 | loss: 0.3360111\n",
      "\tspeed: 0.0826s/iter; left time: 759.2508s\n",
      "\titers: 300, epoch: 1 | loss: 0.3031200\n",
      "\tspeed: 0.0640s/iter; left time: 581.9662s\n",
      "\titers: 400, epoch: 1 | loss: 0.3241749\n",
      "\tspeed: 0.0747s/iter; left time: 672.0739s\n",
      "\titers: 500, epoch: 1 | loss: 0.4436989\n",
      "\tspeed: 0.0818s/iter; left time: 727.1866s\n",
      "\titers: 600, epoch: 1 | loss: 0.2832341\n",
      "\tspeed: 0.0750s/iter; left time: 658.9216s\n",
      "\titers: 700, epoch: 1 | loss: 0.2883096\n",
      "\tspeed: 0.0759s/iter; left time: 659.3510s\n",
      "\titers: 800, epoch: 1 | loss: 0.2843474\n",
      "\tspeed: 0.0754s/iter; left time: 647.4826s\n",
      "\titers: 900, epoch: 1 | loss: 0.2853507\n",
      "\tspeed: 0.0876s/iter; left time: 743.8912s\n",
      "Epoch: 1 running time: 1.2261707663536072 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3514542 Vali Loss: 0.5110533 Test Loss: 0.7006608\n",
      "Validation loss decreased (inf --> 0.511053).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2838375\n",
      "\tspeed: 0.2202s/iter; left time: 1839.3361s\n",
      "\titers: 200, epoch: 2 | loss: 0.3174154\n",
      "\tspeed: 0.0805s/iter; left time: 663.9494s\n",
      "\titers: 300, epoch: 2 | loss: 0.2704579\n",
      "\tspeed: 0.0724s/iter; left time: 590.2162s\n",
      "\titers: 400, epoch: 2 | loss: 0.2998215\n",
      "\tspeed: 0.0748s/iter; left time: 602.2133s\n",
      "\titers: 500, epoch: 2 | loss: 0.2826991\n",
      "\tspeed: 0.0714s/iter; left time: 567.9892s\n",
      "\titers: 600, epoch: 2 | loss: 0.2354458\n",
      "\tspeed: 0.0794s/iter; left time: 623.4227s\n",
      "\titers: 700, epoch: 2 | loss: 0.2185039\n",
      "\tspeed: 0.0727s/iter; left time: 563.9199s\n",
      "\titers: 800, epoch: 2 | loss: 0.2907267\n",
      "\tspeed: 0.0696s/iter; left time: 532.3915s\n",
      "\titers: 900, epoch: 2 | loss: 0.2060656\n",
      "\tspeed: 0.0766s/iter; left time: 578.7363s\n",
      "Epoch: 2 running time: 1.1805304050445558 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2660816 Vali Loss: 0.5596452 Test Loss: 0.7050695\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2320471\n",
      "\tspeed: 0.2032s/iter; left time: 1506.5012s\n",
      "\titers: 200, epoch: 3 | loss: 0.2426718\n",
      "\tspeed: 0.0730s/iter; left time: 533.5564s\n",
      "\titers: 300, epoch: 3 | loss: 0.2341955\n",
      "\tspeed: 0.0813s/iter; left time: 586.4915s\n",
      "\titers: 400, epoch: 3 | loss: 0.2900559\n",
      "\tspeed: 0.0693s/iter; left time: 493.1464s\n",
      "\titers: 500, epoch: 3 | loss: 0.2482189\n",
      "\tspeed: 0.0679s/iter; left time: 476.0777s\n",
      "\titers: 600, epoch: 3 | loss: 0.1943938\n",
      "\tspeed: 0.0721s/iter; left time: 498.2364s\n",
      "\titers: 700, epoch: 3 | loss: 0.2334485\n",
      "\tspeed: 0.0803s/iter; left time: 547.0177s\n",
      "\titers: 800, epoch: 3 | loss: 0.2198879\n",
      "\tspeed: 0.0707s/iter; left time: 474.6715s\n",
      "\titers: 900, epoch: 3 | loss: 0.2073019\n",
      "\tspeed: 0.0669s/iter; left time: 442.1287s\n",
      "Epoch: 3 running time: 1.1402729074160258 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2492289 Vali Loss: 0.5502313 Test Loss: 0.7162092\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2484804\n",
      "\tspeed: 0.2110s/iter; left time: 1365.7834s\n",
      "\titers: 200, epoch: 4 | loss: 0.2935393\n",
      "\tspeed: 0.0694s/iter; left time: 442.5148s\n",
      "\titers: 300, epoch: 4 | loss: 0.2457228\n",
      "\tspeed: 0.0831s/iter; left time: 521.5130s\n",
      "\titers: 400, epoch: 4 | loss: 0.2819709\n",
      "\tspeed: 0.0722s/iter; left time: 445.8109s\n",
      "\titers: 500, epoch: 4 | loss: 0.2787814\n",
      "\tspeed: 0.0698s/iter; left time: 423.9807s\n",
      "\titers: 600, epoch: 4 | loss: 0.2240943\n",
      "\tspeed: 0.0751s/iter; left time: 448.6325s\n",
      "\titers: 700, epoch: 4 | loss: 0.1947944\n",
      "\tspeed: 0.0796s/iter; left time: 467.6339s\n",
      "\titers: 800, epoch: 4 | loss: 0.2129528\n",
      "\tspeed: 0.0685s/iter; left time: 395.7617s\n",
      "\titers: 900, epoch: 4 | loss: 0.2525314\n",
      "\tspeed: 0.0730s/iter; left time: 414.0848s\n",
      "Epoch: 4 running time: 1.1513005097707112 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2447866 Vali Loss: 0.5440360 Test Loss: 0.6981807\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el5_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6974019408226013, mae:0.48650822043418884\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 5 \\\n",
    "  --d_layers 2 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'FR_data.csv'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 4 \\\n",
    "  --d_layers 3 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 5 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30433\n",
      "val 6505\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6451275\n",
      "\tspeed: 0.1165s/iter; left time: 1096.0194s\n",
      "\titers: 200, epoch: 1 | loss: 0.4659448\n",
      "\tspeed: 0.0810s/iter; left time: 754.2153s\n",
      "\titers: 300, epoch: 1 | loss: 0.4373887\n",
      "\tspeed: 0.0835s/iter; left time: 769.0859s\n",
      "\titers: 400, epoch: 1 | loss: 0.4091117\n",
      "\tspeed: 0.0808s/iter; left time: 736.2644s\n",
      "\titers: 500, epoch: 1 | loss: 0.4036097\n",
      "\tspeed: 0.0810s/iter; left time: 730.1276s\n",
      "\titers: 600, epoch: 1 | loss: 0.2723704\n",
      "\tspeed: 0.0809s/iter; left time: 721.2786s\n",
      "\titers: 700, epoch: 1 | loss: 0.3614541\n",
      "\tspeed: 0.0805s/iter; left time: 709.2631s\n",
      "\titers: 800, epoch: 1 | loss: 0.3330092\n",
      "\tspeed: 0.0845s/iter; left time: 735.9239s\n",
      "\titers: 900, epoch: 1 | loss: 0.3676375\n",
      "\tspeed: 0.0790s/iter; left time: 680.1017s\n",
      "Epoch: 1 running time: 1.315728747844696 min.\n",
      "Epoch: 1, Steps: 951 | Train Loss: 0.4209583 Vali Loss: 0.5518361 Test Loss: 0.4462260\n",
      "Validation loss decreased (inf --> 0.551836).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2718028\n",
      "\tspeed: 0.2304s/iter; left time: 1949.2051s\n",
      "\titers: 200, epoch: 2 | loss: 0.4156291\n",
      "\tspeed: 0.0806s/iter; left time: 673.5139s\n",
      "\titers: 300, epoch: 2 | loss: 0.3505756\n",
      "\tspeed: 0.0809s/iter; left time: 668.5692s\n",
      "\titers: 400, epoch: 2 | loss: 0.2953941\n",
      "\tspeed: 0.0810s/iter; left time: 660.7926s\n",
      "\titers: 500, epoch: 2 | loss: 0.3201417\n",
      "\tspeed: 0.0806s/iter; left time: 649.9939s\n",
      "\titers: 600, epoch: 2 | loss: 0.2982628\n",
      "\tspeed: 0.0799s/iter; left time: 636.1100s\n",
      "\titers: 700, epoch: 2 | loss: 0.2886133\n",
      "\tspeed: 0.0830s/iter; left time: 652.2518s\n",
      "\titers: 800, epoch: 2 | loss: 0.3132242\n",
      "\tspeed: 0.0809s/iter; left time: 628.0762s\n",
      "\titers: 900, epoch: 2 | loss: 0.2783976\n",
      "\tspeed: 0.0809s/iter; left time: 619.8360s\n",
      "Epoch: 2 running time: 1.2925765593846639 min.\n",
      "Epoch: 2, Steps: 951 | Train Loss: 0.3152328 Vali Loss: 0.5902461 Test Loss: 0.4580575\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3025784\n",
      "\tspeed: 0.2287s/iter; left time: 1717.1781s\n",
      "\titers: 200, epoch: 3 | loss: 0.3488313\n",
      "\tspeed: 0.0835s/iter; left time: 618.5175s\n",
      "\titers: 300, epoch: 3 | loss: 0.3036665\n",
      "\tspeed: 0.0808s/iter; left time: 590.4024s\n",
      "\titers: 400, epoch: 3 | loss: 0.3199162\n",
      "\tspeed: 0.0809s/iter; left time: 583.2490s\n",
      "\titers: 500, epoch: 3 | loss: 0.3405753\n",
      "\tspeed: 0.0809s/iter; left time: 575.4169s\n",
      "\titers: 600, epoch: 3 | loss: 0.2831150\n",
      "\tspeed: 0.0807s/iter; left time: 565.7890s\n",
      "\titers: 700, epoch: 3 | loss: 0.2757951\n",
      "\tspeed: 0.0830s/iter; left time: 573.6742s\n",
      "\titers: 800, epoch: 3 | loss: 0.2828690\n",
      "\tspeed: 0.0805s/iter; left time: 548.3554s\n",
      "\titers: 900, epoch: 3 | loss: 0.3142776\n",
      "\tspeed: 0.0803s/iter; left time: 538.8550s\n",
      "Epoch: 3 running time: 1.2886242667833965 min.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/./TSLibrary/run.py\", line 158, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 164, in train\n",
      "    vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/exp/exp_long_term_forecasting.py\", line 64, in vali\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/models/Informer.py\", line 133, in forward\n",
      "    dec_out = self.long_forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/models/Informer.py\", line 80, in long_forecast\n",
      "    enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/layers/Transformer_EncDec.py\", line 70, in forward\n",
      "    x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/layers/Transformer_EncDec.py\", line 40, in forward\n",
      "    new_x, attn = self.attention(\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/layers/SelfAttention_Family.py\", line 203, in forward\n",
      "    out, attn = self.inner_attention(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/layers/SelfAttention_Family.py\", line 163, in forward\n",
      "    scores_top, index = self._prob_QK(\n",
      "                        ^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/my_work/TSLibrary/layers/SelfAttention_Family.py\", line 99, in _prob_QK\n",
      "    Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
      "    ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "dataset = 'df_most_important_columns.csv'\n",
    "\n",
    "!python -u ./TSLibrary/run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path $current_path \\\n",
    "  --data_path $dataset \\\n",
    "  --model_id 1 \\\n",
    "  --model \"Informer\" \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 5 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/well0203/my_work.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r /content/my_work/TSLibrary/requirements.txt\n",
    "#!pip install sktime\n",
    "#!pip install reformer-pytorch==1.4.4\n",
    "# Drive python version 3.10.6, therefore torch==1.7.1 does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5979588\n",
      "\tspeed: 0.1078s/iter; left time: 1003.5939s\n",
      "\titers: 200, epoch: 1 | loss: 0.3988746\n",
      "\tspeed: 0.0981s/iter; left time: 903.5919s\n",
      "\titers: 300, epoch: 1 | loss: 0.4951838\n",
      "\tspeed: 0.0980s/iter; left time: 892.9673s\n",
      "\titers: 400, epoch: 1 | loss: 0.3251971\n",
      "\tspeed: 0.0979s/iter; left time: 882.0710s\n",
      "\titers: 500, epoch: 1 | loss: 0.2972392\n",
      "\tspeed: 0.0979s/iter; left time: 872.1553s\n",
      "\titers: 600, epoch: 1 | loss: 0.3141234\n",
      "\tspeed: 0.0983s/iter; left time: 865.7961s\n",
      "\titers: 700, epoch: 1 | loss: 0.3582392\n",
      "\tspeed: 0.0980s/iter; left time: 853.8956s\n",
      "\titers: 800, epoch: 1 | loss: 0.3090230\n",
      "\tspeed: 0.0979s/iter; left time: 842.9842s\n",
      "\titers: 900, epoch: 1 | loss: 0.2600375\n",
      "\tspeed: 0.0980s/iter; left time: 833.7338s\n",
      "Epoch: 1 running time: 1.5546314120292664 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3885667 Vali Loss: 0.3599150 Test Loss: 0.5413693\n",
      "Validation loss decreased (inf --> 0.359915).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2532829\n",
      "\tspeed: 0.2840s/iter; left time: 2377.0272s\n",
      "\titers: 200, epoch: 2 | loss: 0.2295953\n",
      "\tspeed: 0.0978s/iter; left time: 809.0253s\n",
      "\titers: 300, epoch: 2 | loss: 0.1922673\n",
      "\tspeed: 0.0976s/iter; left time: 797.2087s\n",
      "\titers: 400, epoch: 2 | loss: 0.4123895\n",
      "\tspeed: 0.0978s/iter; left time: 788.9432s\n",
      "\titers: 500, epoch: 2 | loss: 0.2371316\n",
      "\tspeed: 0.0983s/iter; left time: 783.2757s\n",
      "\titers: 600, epoch: 2 | loss: 0.3011464\n",
      "\tspeed: 0.0982s/iter; left time: 772.5010s\n",
      "\titers: 700, epoch: 2 | loss: 0.2392187\n",
      "\tspeed: 0.0980s/iter; left time: 761.5209s\n",
      "\titers: 800, epoch: 2 | loss: 0.2716683\n",
      "\tspeed: 0.0976s/iter; left time: 748.2816s\n",
      "\titers: 900, epoch: 2 | loss: 0.2080009\n",
      "\tspeed: 0.0977s/iter; left time: 739.3383s\n",
      "Epoch: 2 running time: 1.5396055936813355 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2747000 Vali Loss: 0.3615359 Test Loss: 0.5790962\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3574236\n",
      "\tspeed: 0.2801s/iter; left time: 2081.2302s\n",
      "\titers: 200, epoch: 3 | loss: 0.2763731\n",
      "\tspeed: 0.0978s/iter; left time: 716.7625s\n",
      "\titers: 300, epoch: 3 | loss: 0.2299239\n",
      "\tspeed: 0.0976s/iter; left time: 705.4228s\n",
      "\titers: 400, epoch: 3 | loss: 0.2516459\n",
      "\tspeed: 0.0977s/iter; left time: 696.3497s\n",
      "\titers: 500, epoch: 3 | loss: 0.2625080\n",
      "\tspeed: 0.0978s/iter; left time: 687.0868s\n",
      "\titers: 600, epoch: 3 | loss: 0.2179623\n",
      "\tspeed: 0.0977s/iter; left time: 676.7548s\n",
      "\titers: 700, epoch: 3 | loss: 0.2282525\n",
      "\tspeed: 0.0975s/iter; left time: 665.8238s\n",
      "\titers: 800, epoch: 3 | loss: 0.3034237\n",
      "\tspeed: 0.0975s/iter; left time: 656.3987s\n",
      "\titers: 900, epoch: 3 | loss: 0.3019572\n",
      "\tspeed: 0.0977s/iter; left time: 647.6157s\n",
      "Epoch: 3 running time: 1.5372791488965352 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2638785 Vali Loss: 0.3499245 Test Loss: 0.5233747\n",
      "Validation loss decreased (0.359915 --> 0.349925).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2404048\n",
      "\tspeed: 0.2853s/iter; left time: 1850.7819s\n",
      "\titers: 200, epoch: 4 | loss: 0.2120794\n",
      "\tspeed: 0.0979s/iter; left time: 625.5544s\n",
      "\titers: 300, epoch: 4 | loss: 0.2520547\n",
      "\tspeed: 0.0976s/iter; left time: 613.7269s\n",
      "\titers: 400, epoch: 4 | loss: 0.2569040\n",
      "\tspeed: 0.0978s/iter; left time: 605.3048s\n",
      "\titers: 500, epoch: 4 | loss: 0.1980895\n",
      "\tspeed: 0.0977s/iter; left time: 595.0931s\n",
      "\titers: 600, epoch: 4 | loss: 0.2096463\n",
      "\tspeed: 0.0975s/iter; left time: 584.0573s\n",
      "\titers: 700, epoch: 4 | loss: 0.2016285\n",
      "\tspeed: 0.0977s/iter; left time: 575.0260s\n",
      "\titers: 800, epoch: 4 | loss: 0.1970884\n",
      "\tspeed: 0.0975s/iter; left time: 564.5929s\n",
      "\titers: 900, epoch: 4 | loss: 0.2034923\n",
      "\tspeed: 0.0975s/iter; left time: 554.6836s\n",
      "Epoch: 4 running time: 1.5379542350769042 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2285615 Vali Loss: 0.3327911 Test Loss: 0.5450918\n",
      "Validation loss decreased (0.349925 --> 0.332791).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1831390\n",
      "\tspeed: 0.2850s/iter; left time: 1580.6327s\n",
      "\titers: 200, epoch: 5 | loss: 0.2395813\n",
      "\tspeed: 0.0973s/iter; left time: 530.2214s\n",
      "\titers: 300, epoch: 5 | loss: 0.1956781\n",
      "\tspeed: 0.0973s/iter; left time: 520.5293s\n",
      "\titers: 400, epoch: 5 | loss: 0.2888322\n",
      "\tspeed: 0.0975s/iter; left time: 511.7390s\n",
      "\titers: 500, epoch: 5 | loss: 0.1966490\n",
      "\tspeed: 0.0974s/iter; left time: 501.3349s\n",
      "\titers: 600, epoch: 5 | loss: 0.1773667\n",
      "\tspeed: 0.0976s/iter; left time: 492.3736s\n",
      "\titers: 700, epoch: 5 | loss: 0.1866304\n",
      "\tspeed: 0.0975s/iter; left time: 482.3317s\n",
      "\titers: 800, epoch: 5 | loss: 0.1474601\n",
      "\tspeed: 0.0973s/iter; left time: 471.8175s\n",
      "\titers: 900, epoch: 5 | loss: 0.2766321\n",
      "\tspeed: 0.0974s/iter; left time: 462.2471s\n",
      "Epoch: 5 running time: 1.533496336142222 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2052569 Vali Loss: 0.3558107 Test Loss: 0.5533416\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1901380\n",
      "\tspeed: 0.2782s/iter; left time: 1281.4157s\n",
      "\titers: 200, epoch: 6 | loss: 0.1638627\n",
      "\tspeed: 0.0975s/iter; left time: 439.1547s\n",
      "\titers: 300, epoch: 6 | loss: 0.2496205\n",
      "\tspeed: 0.0974s/iter; left time: 428.9254s\n",
      "\titers: 400, epoch: 6 | loss: 0.2062164\n",
      "\tspeed: 0.0975s/iter; left time: 420.0256s\n",
      "\titers: 500, epoch: 6 | loss: 0.2275229\n",
      "\tspeed: 0.0977s/iter; left time: 410.7234s\n",
      "\titers: 600, epoch: 6 | loss: 0.1896032\n",
      "\tspeed: 0.0975s/iter; left time: 400.2688s\n",
      "\titers: 700, epoch: 6 | loss: 0.1997263\n",
      "\tspeed: 0.0975s/iter; left time: 390.4182s\n",
      "\titers: 800, epoch: 6 | loss: 0.2681848\n",
      "\tspeed: 0.0975s/iter; left time: 380.8619s\n",
      "\titers: 900, epoch: 6 | loss: 0.2032136\n",
      "\tspeed: 0.0973s/iter; left time: 370.5132s\n",
      "Epoch: 6 running time: 1.5333383083343506 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2041673 Vali Loss: 0.3364544 Test Loss: 0.5421493\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1747950\n",
      "\tspeed: 0.2787s/iter; left time: 1021.2745s\n",
      "\titers: 200, epoch: 7 | loss: 0.2300793\n",
      "\tspeed: 0.0976s/iter; left time: 347.9793s\n",
      "\titers: 300, epoch: 7 | loss: 0.1680039\n",
      "\tspeed: 0.0976s/iter; left time: 338.2595s\n",
      "\titers: 400, epoch: 7 | loss: 0.2118735\n",
      "\tspeed: 0.0978s/iter; left time: 329.1586s\n",
      "\titers: 500, epoch: 7 | loss: 0.1732029\n",
      "\tspeed: 0.0978s/iter; left time: 319.2355s\n",
      "\titers: 600, epoch: 7 | loss: 0.1689785\n",
      "\tspeed: 0.0977s/iter; left time: 309.1045s\n",
      "\titers: 700, epoch: 7 | loss: 0.2123212\n",
      "\tspeed: 0.0974s/iter; left time: 298.5726s\n",
      "\titers: 800, epoch: 7 | loss: 0.2141715\n",
      "\tspeed: 0.0975s/iter; left time: 289.2073s\n",
      "\titers: 900, epoch: 7 | loss: 0.1998224\n",
      "\tspeed: 0.0976s/iter; left time: 279.6204s\n",
      "Epoch: 7 running time: 1.5368539373079935 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2040884 Vali Loss: 0.3359562 Test Loss: 0.5423290\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.542351484298706, mae:0.49271532893180847\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5885839\n",
      "\tspeed: 0.0911s/iter; left time: 848.3252s\n",
      "\titers: 200, epoch: 1 | loss: 0.4808174\n",
      "\tspeed: 0.0983s/iter; left time: 905.3794s\n",
      "\titers: 300, epoch: 1 | loss: 0.3567823\n",
      "\tspeed: 0.0987s/iter; left time: 899.4436s\n",
      "\titers: 400, epoch: 1 | loss: 0.3922084\n",
      "\tspeed: 0.0992s/iter; left time: 894.3320s\n",
      "\titers: 500, epoch: 1 | loss: 0.3549460\n",
      "\tspeed: 0.0982s/iter; left time: 874.8667s\n",
      "\titers: 600, epoch: 1 | loss: 0.4167266\n",
      "\tspeed: 0.0980s/iter; left time: 863.8790s\n",
      "\titers: 700, epoch: 1 | loss: 0.3898646\n",
      "\tspeed: 0.0958s/iter; left time: 834.3015s\n",
      "\titers: 800, epoch: 1 | loss: 0.2914270\n",
      "\tspeed: 0.0983s/iter; left time: 846.5094s\n",
      "\titers: 900, epoch: 1 | loss: 0.2078680\n",
      "\tspeed: 0.0986s/iter; left time: 838.8358s\n",
      "Epoch: 1 running time: 1.52977428038915 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3939400 Vali Loss: 0.3607067 Test Loss: 0.5616335\n",
      "Validation loss decreased (inf --> 0.360707).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2776674\n",
      "\tspeed: 0.2878s/iter; left time: 2409.1260s\n",
      "\titers: 200, epoch: 2 | loss: 0.2421905\n",
      "\tspeed: 0.0982s/iter; left time: 811.8457s\n",
      "\titers: 300, epoch: 2 | loss: 0.3501032\n",
      "\tspeed: 0.0979s/iter; left time: 799.7434s\n",
      "\titers: 400, epoch: 2 | loss: 0.2221153\n",
      "\tspeed: 0.0980s/iter; left time: 791.1889s\n",
      "\titers: 500, epoch: 2 | loss: 0.3621227\n",
      "\tspeed: 0.0978s/iter; left time: 779.6867s\n",
      "\titers: 600, epoch: 2 | loss: 0.3243517\n",
      "\tspeed: 0.0979s/iter; left time: 770.5618s\n",
      "\titers: 700, epoch: 2 | loss: 0.3431254\n",
      "\tspeed: 0.0978s/iter; left time: 759.9195s\n",
      "\titers: 800, epoch: 2 | loss: 0.2447850\n",
      "\tspeed: 0.0979s/iter; left time: 751.0887s\n",
      "\titers: 900, epoch: 2 | loss: 0.2354745\n",
      "\tspeed: 0.0979s/iter; left time: 740.9177s\n",
      "Epoch: 2 running time: 1.5421267906824747 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2753468 Vali Loss: 0.3552379 Test Loss: 0.5603691\n",
      "Validation loss decreased (0.360707 --> 0.355238).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2077665\n",
      "\tspeed: 0.2863s/iter; left time: 2126.7015s\n",
      "\titers: 200, epoch: 3 | loss: 0.2024092\n",
      "\tspeed: 0.0977s/iter; left time: 716.3017s\n",
      "\titers: 300, epoch: 3 | loss: 0.2525340\n",
      "\tspeed: 0.0977s/iter; left time: 706.2460s\n",
      "\titers: 400, epoch: 3 | loss: 0.2007055\n",
      "\tspeed: 0.0982s/iter; left time: 699.9823s\n",
      "\titers: 500, epoch: 3 | loss: 0.2679331\n",
      "\tspeed: 0.0989s/iter; left time: 695.2980s\n",
      "\titers: 600, epoch: 3 | loss: 0.1978059\n",
      "\tspeed: 0.0991s/iter; left time: 686.6898s\n",
      "\titers: 700, epoch: 3 | loss: 0.1983410\n",
      "\tspeed: 0.0987s/iter; left time: 674.1259s\n",
      "\titers: 800, epoch: 3 | loss: 0.2749475\n",
      "\tspeed: 0.0981s/iter; left time: 660.0698s\n",
      "\titers: 900, epoch: 3 | loss: 0.2155552\n",
      "\tspeed: 0.0982s/iter; left time: 650.6816s\n",
      "Epoch: 3 running time: 1.5467728972434998 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2296516 Vali Loss: 0.3449344 Test Loss: 0.5333540\n",
      "Validation loss decreased (0.355238 --> 0.344934).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1944146\n",
      "\tspeed: 0.2881s/iter; left time: 1869.2917s\n",
      "\titers: 200, epoch: 4 | loss: 0.2291458\n",
      "\tspeed: 0.0977s/iter; left time: 624.0008s\n",
      "\titers: 300, epoch: 4 | loss: 0.1660754\n",
      "\tspeed: 0.0978s/iter; left time: 614.6755s\n",
      "\titers: 400, epoch: 4 | loss: 0.1500978\n",
      "\tspeed: 0.0977s/iter; left time: 604.4314s\n",
      "\titers: 500, epoch: 4 | loss: 0.2157296\n",
      "\tspeed: 0.0976s/iter; left time: 593.9782s\n",
      "\titers: 600, epoch: 4 | loss: 0.1963040\n",
      "\tspeed: 0.0978s/iter; left time: 585.5368s\n",
      "\titers: 700, epoch: 4 | loss: 0.2547735\n",
      "\tspeed: 0.0977s/iter; left time: 575.1413s\n",
      "\titers: 800, epoch: 4 | loss: 0.1726060\n",
      "\tspeed: 0.0977s/iter; left time: 565.3300s\n",
      "\titers: 900, epoch: 4 | loss: 0.1798732\n",
      "\tspeed: 0.0975s/iter; left time: 554.5946s\n",
      "Epoch: 4 running time: 1.5375561753908793 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1954855 Vali Loss: 0.3462040 Test Loss: 0.5407835\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1592964\n",
      "\tspeed: 0.2801s/iter; left time: 1553.5432s\n",
      "\titers: 200, epoch: 5 | loss: 0.1947924\n",
      "\tspeed: 0.0982s/iter; left time: 535.0782s\n",
      "\titers: 300, epoch: 5 | loss: 0.1617831\n",
      "\tspeed: 0.0981s/iter; left time: 524.6174s\n",
      "\titers: 400, epoch: 5 | loss: 0.2011821\n",
      "\tspeed: 0.0980s/iter; left time: 514.0995s\n",
      "\titers: 500, epoch: 5 | loss: 0.2041656\n",
      "\tspeed: 0.0975s/iter; left time: 501.5775s\n",
      "\titers: 600, epoch: 5 | loss: 0.1855498\n",
      "\tspeed: 0.0976s/iter; left time: 492.3413s\n",
      "\titers: 700, epoch: 5 | loss: 0.1450996\n",
      "\tspeed: 0.0975s/iter; left time: 482.4622s\n",
      "\titers: 800, epoch: 5 | loss: 0.1926186\n",
      "\tspeed: 0.0979s/iter; left time: 474.4031s\n",
      "\titers: 900, epoch: 5 | loss: 0.2197612\n",
      "\tspeed: 0.0980s/iter; left time: 465.3897s\n",
      "Epoch: 5 running time: 1.5406445781389873 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1928436 Vali Loss: 0.3556490 Test Loss: 0.5593730\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2385069\n",
      "\tspeed: 0.2796s/iter; left time: 1287.8641s\n",
      "\titers: 200, epoch: 6 | loss: 0.2327145\n",
      "\tspeed: 0.0978s/iter; left time: 440.4635s\n",
      "\titers: 300, epoch: 6 | loss: 0.2079315\n",
      "\tspeed: 0.0978s/iter; left time: 430.9414s\n",
      "\titers: 400, epoch: 6 | loss: 0.1807522\n",
      "\tspeed: 0.0976s/iter; left time: 420.3452s\n",
      "\titers: 500, epoch: 6 | loss: 0.2193317\n",
      "\tspeed: 0.0978s/iter; left time: 411.4552s\n",
      "\titers: 600, epoch: 6 | loss: 0.1836175\n",
      "\tspeed: 0.0977s/iter; left time: 401.0777s\n",
      "\titers: 700, epoch: 6 | loss: 0.2200144\n",
      "\tspeed: 0.0976s/iter; left time: 390.8376s\n",
      "\titers: 800, epoch: 6 | loss: 0.2055064\n",
      "\tspeed: 0.0976s/iter; left time: 381.2572s\n",
      "\titers: 900, epoch: 6 | loss: 0.2023076\n",
      "\tspeed: 0.0978s/iter; left time: 372.2206s\n",
      "Epoch: 6 running time: 1.5379039565722148 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1931179 Vali Loss: 0.3452641 Test Loss: 0.5490345\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.5488144755363464, mae:0.4969179332256317\n",
      "\n",
      "Time intermediate for DE dataset: 23.553507006168367 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4583763\n",
      "\tspeed: 0.1078s/iter; left time: 1003.8090s\n",
      "\titers: 200, epoch: 1 | loss: 0.3878897\n",
      "\tspeed: 0.0979s/iter; left time: 901.8856s\n",
      "\titers: 300, epoch: 1 | loss: 0.3311236\n",
      "\tspeed: 0.0977s/iter; left time: 890.4963s\n",
      "\titers: 400, epoch: 1 | loss: 0.3112401\n",
      "\tspeed: 0.0979s/iter; left time: 881.7811s\n",
      "\titers: 500, epoch: 1 | loss: 0.2713794\n",
      "\tspeed: 0.0980s/iter; left time: 873.5798s\n",
      "\titers: 600, epoch: 1 | loss: 0.3417489\n",
      "\tspeed: 0.0981s/iter; left time: 864.7337s\n",
      "\titers: 700, epoch: 1 | loss: 0.4003856\n",
      "\tspeed: 0.0980s/iter; left time: 853.8352s\n",
      "\titers: 800, epoch: 1 | loss: 0.2808123\n",
      "\tspeed: 0.0981s/iter; left time: 844.3268s\n",
      "\titers: 900, epoch: 1 | loss: 0.2576330\n",
      "\tspeed: 0.0980s/iter; left time: 833.8802s\n",
      "Epoch: 1 running time: 1.5542073210080465 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4064409 Vali Loss: 0.4122250 Test Loss: 0.7646125\n",
      "Validation loss decreased (inf --> 0.412225).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3067522\n",
      "\tspeed: 0.2851s/iter; left time: 2385.9309s\n",
      "\titers: 200, epoch: 2 | loss: 0.2744507\n",
      "\tspeed: 0.0981s/iter; left time: 810.9219s\n",
      "\titers: 300, epoch: 2 | loss: 0.3486772\n",
      "\tspeed: 0.0976s/iter; left time: 797.1353s\n",
      "\titers: 400, epoch: 2 | loss: 0.3093699\n",
      "\tspeed: 0.0979s/iter; left time: 790.0474s\n",
      "\titers: 500, epoch: 2 | loss: 0.2064186\n",
      "\tspeed: 0.0979s/iter; left time: 780.3782s\n",
      "\titers: 600, epoch: 2 | loss: 0.2748952\n",
      "\tspeed: 0.0976s/iter; left time: 768.4677s\n",
      "\titers: 700, epoch: 2 | loss: 0.2839590\n",
      "\tspeed: 0.0978s/iter; left time: 760.1446s\n",
      "\titers: 800, epoch: 2 | loss: 0.2919414\n",
      "\tspeed: 0.0979s/iter; left time: 750.7189s\n",
      "\titers: 900, epoch: 2 | loss: 0.2487071\n",
      "\tspeed: 0.0976s/iter; left time: 738.7067s\n",
      "Epoch: 2 running time: 1.5410709222157797 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3065783 Vali Loss: 0.3944587 Test Loss: 0.6807294\n",
      "Validation loss decreased (0.412225 --> 0.394459).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2702128\n",
      "\tspeed: 0.2877s/iter; left time: 2137.5545s\n",
      "\titers: 200, epoch: 3 | loss: 0.3190711\n",
      "\tspeed: 0.0979s/iter; left time: 717.7133s\n",
      "\titers: 300, epoch: 3 | loss: 0.2532526\n",
      "\tspeed: 0.0982s/iter; left time: 709.7745s\n",
      "\titers: 400, epoch: 3 | loss: 0.2061189\n",
      "\tspeed: 0.0976s/iter; left time: 696.1377s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977401\n",
      "\tspeed: 0.0977s/iter; left time: 686.5519s\n",
      "\titers: 600, epoch: 3 | loss: 0.2680047\n",
      "\tspeed: 0.0978s/iter; left time: 677.4644s\n",
      "\titers: 700, epoch: 3 | loss: 0.2145547\n",
      "\tspeed: 0.0874s/iter; left time: 597.0928s\n",
      "\titers: 800, epoch: 3 | loss: 0.2101406\n",
      "\tspeed: 0.0809s/iter; left time: 544.3271s\n",
      "\titers: 900, epoch: 3 | loss: 0.3406821\n",
      "\tspeed: 0.0809s/iter; left time: 536.2867s\n",
      "Epoch: 3 running time: 1.4540736556053162 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2630870 Vali Loss: 0.4008739 Test Loss: 0.7279691\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2422981\n",
      "\tspeed: 0.2724s/iter; left time: 1767.1854s\n",
      "\titers: 200, epoch: 4 | loss: 0.1884126\n",
      "\tspeed: 0.0978s/iter; left time: 624.4336s\n",
      "\titers: 300, epoch: 4 | loss: 0.2599521\n",
      "\tspeed: 0.0975s/iter; left time: 613.1612s\n",
      "\titers: 400, epoch: 4 | loss: 0.2694383\n",
      "\tspeed: 0.0977s/iter; left time: 604.5321s\n",
      "\titers: 500, epoch: 4 | loss: 0.2439736\n",
      "\tspeed: 0.0974s/iter; left time: 593.0381s\n",
      "\titers: 600, epoch: 4 | loss: 0.2632573\n",
      "\tspeed: 0.0974s/iter; left time: 583.4135s\n",
      "\titers: 700, epoch: 4 | loss: 0.2553389\n",
      "\tspeed: 0.0975s/iter; left time: 574.0162s\n",
      "\titers: 800, epoch: 4 | loss: 0.2767370\n",
      "\tspeed: 0.0975s/iter; left time: 564.1970s\n",
      "\titers: 900, epoch: 4 | loss: 0.2517391\n",
      "\tspeed: 0.0977s/iter; left time: 555.8763s\n",
      "Epoch: 4 running time: 1.5366331458091735 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2589726 Vali Loss: 0.3914911 Test Loss: 0.6960634\n",
      "Validation loss decreased (0.394459 --> 0.391491).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2741575\n",
      "\tspeed: 0.2861s/iter; left time: 1586.9365s\n",
      "\titers: 200, epoch: 5 | loss: 0.2336923\n",
      "\tspeed: 0.0975s/iter; left time: 531.2220s\n",
      "\titers: 300, epoch: 5 | loss: 0.2015684\n",
      "\tspeed: 0.0974s/iter; left time: 521.0125s\n",
      "\titers: 400, epoch: 5 | loss: 0.2283741\n",
      "\tspeed: 0.0975s/iter; left time: 511.6482s\n",
      "\titers: 500, epoch: 5 | loss: 0.2496907\n",
      "\tspeed: 0.0976s/iter; left time: 502.3256s\n",
      "\titers: 600, epoch: 5 | loss: 0.2165726\n",
      "\tspeed: 0.0978s/iter; left time: 493.7778s\n",
      "\titers: 700, epoch: 5 | loss: 0.2022953\n",
      "\tspeed: 0.0978s/iter; left time: 483.6539s\n",
      "\titers: 800, epoch: 5 | loss: 0.2490409\n",
      "\tspeed: 0.0978s/iter; left time: 474.0028s\n",
      "\titers: 900, epoch: 5 | loss: 0.2817516\n",
      "\tspeed: 0.0975s/iter; left time: 462.9715s\n",
      "Epoch: 5 running time: 1.5363385915756225 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2380383 Vali Loss: 0.3972078 Test Loss: 0.7027150\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1886821\n",
      "\tspeed: 0.2781s/iter; left time: 1281.1298s\n",
      "\titers: 200, epoch: 6 | loss: 0.2433284\n",
      "\tspeed: 0.0976s/iter; left time: 439.8831s\n",
      "\titers: 300, epoch: 6 | loss: 0.2376238\n",
      "\tspeed: 0.0975s/iter; left time: 429.6387s\n",
      "\titers: 400, epoch: 6 | loss: 0.2259449\n",
      "\tspeed: 0.0977s/iter; left time: 420.5388s\n",
      "\titers: 500, epoch: 6 | loss: 0.2131033\n",
      "\tspeed: 0.0979s/iter; left time: 411.7965s\n",
      "\titers: 600, epoch: 6 | loss: 0.2139467\n",
      "\tspeed: 0.0979s/iter; left time: 401.8017s\n",
      "\titers: 700, epoch: 6 | loss: 0.2462851\n",
      "\tspeed: 0.0978s/iter; left time: 391.9464s\n",
      "\titers: 800, epoch: 6 | loss: 0.2430781\n",
      "\tspeed: 0.0980s/iter; left time: 382.7134s\n",
      "\titers: 900, epoch: 6 | loss: 0.1916809\n",
      "\tspeed: 0.0979s/iter; left time: 372.7594s\n",
      "Epoch: 6 running time: 1.5383511741956075 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2370810 Vali Loss: 0.4012877 Test Loss: 0.7130616\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2536033\n",
      "\tspeed: 0.2795s/iter; left time: 1024.3367s\n",
      "\titers: 200, epoch: 7 | loss: 0.2663393\n",
      "\tspeed: 0.0976s/iter; left time: 347.8432s\n",
      "\titers: 300, epoch: 7 | loss: 0.2347079\n",
      "\tspeed: 0.0978s/iter; left time: 338.7855s\n",
      "\titers: 400, epoch: 7 | loss: 0.2837873\n",
      "\tspeed: 0.0977s/iter; left time: 328.8627s\n",
      "\titers: 500, epoch: 7 | loss: 0.1881218\n",
      "\tspeed: 0.0978s/iter; left time: 319.2112s\n",
      "\titers: 600, epoch: 7 | loss: 0.2129756\n",
      "\tspeed: 0.0976s/iter; left time: 308.9672s\n",
      "\titers: 700, epoch: 7 | loss: 0.2155947\n",
      "\tspeed: 0.0975s/iter; left time: 298.7364s\n",
      "\titers: 800, epoch: 7 | loss: 0.2029046\n",
      "\tspeed: 0.0974s/iter; left time: 288.8096s\n",
      "\titers: 900, epoch: 7 | loss: 0.2018593\n",
      "\tspeed: 0.0977s/iter; left time: 279.7914s\n",
      "Epoch: 7 running time: 1.5372508883476257 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2374023 Vali Loss: 0.3983790 Test Loss: 0.7232236\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.7234761118888855, mae:0.5841315388679504\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5943394\n",
      "\tspeed: 0.1022s/iter; left time: 951.5721s\n",
      "\titers: 200, epoch: 1 | loss: 0.4951172\n",
      "\tspeed: 0.0989s/iter; left time: 911.3668s\n",
      "\titers: 300, epoch: 1 | loss: 0.4118982\n",
      "\tspeed: 0.0991s/iter; left time: 902.8581s\n",
      "\titers: 400, epoch: 1 | loss: 0.3952385\n",
      "\tspeed: 0.0993s/iter; left time: 894.4173s\n",
      "\titers: 500, epoch: 1 | loss: 0.3432862\n",
      "\tspeed: 0.0991s/iter; left time: 883.3927s\n",
      "\titers: 600, epoch: 1 | loss: 0.3879800\n",
      "\tspeed: 0.0986s/iter; left time: 868.4864s\n",
      "\titers: 700, epoch: 1 | loss: 0.5058795\n",
      "\tspeed: 0.0994s/iter; left time: 866.2208s\n",
      "\titers: 800, epoch: 1 | loss: 0.2738970\n",
      "\tspeed: 0.0992s/iter; left time: 854.0643s\n",
      "\titers: 900, epoch: 1 | loss: 0.2377400\n",
      "\tspeed: 0.0987s/iter; left time: 839.8687s\n",
      "Epoch: 1 running time: 1.560698135693868 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4166347 Vali Loss: 0.4075450 Test Loss: 0.6658818\n",
      "Validation loss decreased (inf --> 0.407545).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2869742\n",
      "\tspeed: 0.2890s/iter; left time: 2418.7386s\n",
      "\titers: 200, epoch: 2 | loss: 0.2624056\n",
      "\tspeed: 0.0983s/iter; left time: 812.5547s\n",
      "\titers: 300, epoch: 2 | loss: 0.3053213\n",
      "\tspeed: 0.0981s/iter; left time: 801.7064s\n",
      "\titers: 400, epoch: 2 | loss: 0.2667679\n",
      "\tspeed: 0.0976s/iter; left time: 787.6466s\n",
      "\titers: 500, epoch: 2 | loss: 0.3287040\n",
      "\tspeed: 0.0979s/iter; left time: 780.6480s\n",
      "\titers: 600, epoch: 2 | loss: 0.3694022\n",
      "\tspeed: 0.0978s/iter; left time: 769.3081s\n",
      "\titers: 700, epoch: 2 | loss: 0.2779715\n",
      "\tspeed: 0.0978s/iter; left time: 759.9436s\n",
      "\titers: 800, epoch: 2 | loss: 0.2777001\n",
      "\tspeed: 0.0978s/iter; left time: 749.8622s\n",
      "\titers: 900, epoch: 2 | loss: 0.2957426\n",
      "\tspeed: 0.0981s/iter; left time: 742.3073s\n",
      "Epoch: 2 running time: 1.5421652674674988 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3094313 Vali Loss: 0.4175729 Test Loss: 0.6661352\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2607539\n",
      "\tspeed: 0.2653s/iter; left time: 1971.0595s\n",
      "\titers: 200, epoch: 3 | loss: 0.3359609\n",
      "\tspeed: 0.0898s/iter; left time: 658.3817s\n",
      "\titers: 300, epoch: 3 | loss: 0.3067381\n",
      "\tspeed: 0.0984s/iter; left time: 711.6286s\n",
      "\titers: 400, epoch: 3 | loss: 0.3878072\n",
      "\tspeed: 0.0981s/iter; left time: 699.5963s\n",
      "\titers: 500, epoch: 3 | loss: 0.2739624\n",
      "\tspeed: 0.0984s/iter; left time: 691.4417s\n",
      "\titers: 600, epoch: 3 | loss: 0.2888962\n",
      "\tspeed: 0.0910s/iter; left time: 630.6627s\n",
      "\titers: 700, epoch: 3 | loss: 0.3805921\n",
      "\tspeed: 0.0809s/iter; left time: 552.2846s\n",
      "\titers: 800, epoch: 3 | loss: 0.2942919\n",
      "\tspeed: 0.0810s/iter; left time: 544.9244s\n",
      "\titers: 900, epoch: 3 | loss: 0.2940906\n",
      "\tspeed: 0.0967s/iter; left time: 640.8422s\n",
      "Epoch: 3 running time: 1.4228740453720092 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.3010982 Vali Loss: 0.4082417 Test Loss: 0.7074066\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3284783\n",
      "\tspeed: 0.2752s/iter; left time: 1785.5537s\n",
      "\titers: 200, epoch: 4 | loss: 0.2637954\n",
      "\tspeed: 0.0978s/iter; left time: 624.7798s\n",
      "\titers: 300, epoch: 4 | loss: 0.3258612\n",
      "\tspeed: 0.0978s/iter; left time: 615.0151s\n",
      "\titers: 400, epoch: 4 | loss: 0.2646027\n",
      "\tspeed: 0.0982s/iter; left time: 607.9364s\n",
      "\titers: 500, epoch: 4 | loss: 0.3703732\n",
      "\tspeed: 0.0977s/iter; left time: 594.5776s\n",
      "\titers: 600, epoch: 4 | loss: 0.3313794\n",
      "\tspeed: 0.0978s/iter; left time: 585.5359s\n",
      "\titers: 700, epoch: 4 | loss: 0.3470367\n",
      "\tspeed: 0.0979s/iter; left time: 576.4887s\n",
      "\titers: 800, epoch: 4 | loss: 0.2665585\n",
      "\tspeed: 0.0980s/iter; left time: 567.3158s\n",
      "\titers: 900, epoch: 4 | loss: 0.2471944\n",
      "\tspeed: 0.0983s/iter; left time: 558.8526s\n",
      "Epoch: 4 running time: 1.542477059364319 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2977920 Vali Loss: 0.3883834 Test Loss: 0.6649781\n",
      "Validation loss decreased (0.407545 --> 0.388383).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2147296\n",
      "\tspeed: 0.2881s/iter; left time: 1597.8585s\n",
      "\titers: 200, epoch: 5 | loss: 0.2421745\n",
      "\tspeed: 0.0985s/iter; left time: 536.2651s\n",
      "\titers: 300, epoch: 5 | loss: 0.2301781\n",
      "\tspeed: 0.0986s/iter; left time: 527.1485s\n",
      "\titers: 400, epoch: 5 | loss: 0.2338233\n",
      "\tspeed: 0.0987s/iter; left time: 518.0345s\n",
      "\titers: 500, epoch: 5 | loss: 0.2931637\n",
      "\tspeed: 0.0985s/iter; left time: 506.8242s\n",
      "\titers: 600, epoch: 5 | loss: 0.3693731\n",
      "\tspeed: 0.0987s/iter; left time: 497.9164s\n",
      "\titers: 700, epoch: 5 | loss: 0.2314854\n",
      "\tspeed: 0.0984s/iter; left time: 486.5726s\n",
      "\titers: 800, epoch: 5 | loss: 0.3689868\n",
      "\tspeed: 0.0985s/iter; left time: 477.3464s\n",
      "\titers: 900, epoch: 5 | loss: 0.2607145\n",
      "\tspeed: 0.0982s/iter; left time: 466.3733s\n",
      "Epoch: 5 running time: 1.551684522628784 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2765266 Vali Loss: 0.3792059 Test Loss: 0.6425338\n",
      "Validation loss decreased (0.388383 --> 0.379206).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1965883\n",
      "\tspeed: 0.2883s/iter; left time: 1327.8715s\n",
      "\titers: 200, epoch: 6 | loss: 0.2588012\n",
      "\tspeed: 0.0979s/iter; left time: 441.2202s\n",
      "\titers: 300, epoch: 6 | loss: 0.2251538\n",
      "\tspeed: 0.0978s/iter; left time: 430.9883s\n",
      "\titers: 400, epoch: 6 | loss: 0.2345210\n",
      "\tspeed: 0.0982s/iter; left time: 422.9801s\n",
      "\titers: 500, epoch: 6 | loss: 0.2129091\n",
      "\tspeed: 0.0981s/iter; left time: 412.5801s\n",
      "\titers: 600, epoch: 6 | loss: 0.2210598\n",
      "\tspeed: 0.0981s/iter; left time: 402.7707s\n",
      "\titers: 700, epoch: 6 | loss: 0.2509040\n",
      "\tspeed: 0.0980s/iter; left time: 392.6660s\n",
      "\titers: 800, epoch: 6 | loss: 0.2515513\n",
      "\tspeed: 0.0980s/iter; left time: 382.8037s\n",
      "\titers: 900, epoch: 6 | loss: 0.3001803\n",
      "\tspeed: 0.0984s/iter; left time: 374.4913s\n",
      "Epoch: 6 running time: 1.5439664721488953 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2621217 Vali Loss: 0.3838913 Test Loss: 0.6670998\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2747995\n",
      "\tspeed: 0.2812s/iter; left time: 1030.4218s\n",
      "\titers: 200, epoch: 7 | loss: 0.3177167\n",
      "\tspeed: 0.0982s/iter; left time: 350.1239s\n",
      "\titers: 300, epoch: 7 | loss: 0.2049908\n",
      "\tspeed: 0.0982s/iter; left time: 340.3733s\n",
      "\titers: 400, epoch: 7 | loss: 0.2936099\n",
      "\tspeed: 0.0983s/iter; left time: 330.9056s\n",
      "\titers: 500, epoch: 7 | loss: 0.2532768\n",
      "\tspeed: 0.0980s/iter; left time: 320.1151s\n",
      "\titers: 600, epoch: 7 | loss: 0.2556626\n",
      "\tspeed: 0.0981s/iter; left time: 310.4543s\n",
      "\titers: 700, epoch: 7 | loss: 0.2591571\n",
      "\tspeed: 0.0984s/iter; left time: 301.6442s\n",
      "\titers: 800, epoch: 7 | loss: 0.2360556\n",
      "\tspeed: 0.0983s/iter; left time: 291.3749s\n",
      "\titers: 900, epoch: 7 | loss: 0.2521653\n",
      "\tspeed: 0.0984s/iter; left time: 281.9666s\n",
      "Epoch: 7 running time: 1.5469970226287841 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2620220 Vali Loss: 0.3817948 Test Loss: 0.6543524\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2943222\n",
      "\tspeed: 0.2819s/iter; left time: 767.9045s\n",
      "\titers: 200, epoch: 8 | loss: 0.2702481\n",
      "\tspeed: 0.0989s/iter; left time: 259.4857s\n",
      "\titers: 300, epoch: 8 | loss: 0.2870710\n",
      "\tspeed: 0.0987s/iter; left time: 248.9951s\n",
      "\titers: 400, epoch: 8 | loss: 0.2909333\n",
      "\tspeed: 0.0980s/iter; left time: 237.6063s\n",
      "\titers: 500, epoch: 8 | loss: 0.3105505\n",
      "\tspeed: 0.0983s/iter; left time: 228.3404s\n",
      "\titers: 600, epoch: 8 | loss: 0.2014252\n",
      "\tspeed: 0.0981s/iter; left time: 218.2501s\n",
      "\titers: 700, epoch: 8 | loss: 0.3274187\n",
      "\tspeed: 0.0982s/iter; left time: 208.5026s\n",
      "\titers: 800, epoch: 8 | loss: 0.2913635\n",
      "\tspeed: 0.0980s/iter; left time: 198.2944s\n",
      "\titers: 900, epoch: 8 | loss: 0.2628719\n",
      "\tspeed: 0.0978s/iter; left time: 188.2189s\n",
      "Epoch: 8 running time: 1.5476595918337503 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.2621921 Vali Loss: 0.3789827 Test Loss: 0.6548730\n",
      "Validation loss decreased (0.379206 --> 0.378983).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2330881\n",
      "\tspeed: 0.2707s/iter; left time: 482.5948s\n",
      "\titers: 200, epoch: 9 | loss: 0.3100449\n",
      "\tspeed: 0.0926s/iter; left time: 155.8765s\n",
      "\titers: 300, epoch: 9 | loss: 0.3230477\n",
      "\tspeed: 0.0983s/iter; left time: 155.5689s\n",
      "\titers: 400, epoch: 9 | loss: 0.2439216\n",
      "\tspeed: 0.0983s/iter; left time: 145.7318s\n",
      "\titers: 500, epoch: 9 | loss: 0.1571486\n",
      "\tspeed: 0.0983s/iter; left time: 136.0101s\n",
      "\titers: 600, epoch: 9 | loss: 0.2126698\n",
      "\tspeed: 0.0982s/iter; left time: 126.0013s\n",
      "\titers: 700, epoch: 9 | loss: 0.2150868\n",
      "\tspeed: 0.0981s/iter; left time: 116.0861s\n",
      "\titers: 800, epoch: 9 | loss: 0.2607985\n",
      "\tspeed: 0.0882s/iter; left time: 95.4774s\n",
      "\titers: 900, epoch: 9 | loss: 0.2161590\n",
      "\tspeed: 0.0812s/iter; left time: 79.8587s\n",
      "Epoch: 9 running time: 1.4526338895161948 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.2588452 Vali Loss: 0.3799818 Test Loss: 0.6566387\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.3090481\n",
      "\tspeed: 0.2756s/iter; left time: 232.0299s\n",
      "\titers: 200, epoch: 10 | loss: 0.2841766\n",
      "\tspeed: 0.0981s/iter; left time: 72.8058s\n",
      "\titers: 300, epoch: 10 | loss: 0.2535132\n",
      "\tspeed: 0.0982s/iter; left time: 63.0232s\n",
      "\titers: 400, epoch: 10 | loss: 0.2925140\n",
      "\tspeed: 0.0981s/iter; left time: 53.1847s\n",
      "\titers: 500, epoch: 10 | loss: 0.2265480\n",
      "\tspeed: 0.0983s/iter; left time: 43.4432s\n",
      "\titers: 600, epoch: 10 | loss: 0.2891370\n",
      "\tspeed: 0.0983s/iter; left time: 33.6099s\n",
      "\titers: 700, epoch: 10 | loss: 0.2668970\n",
      "\tspeed: 0.0982s/iter; left time: 23.7603s\n",
      "\titers: 800, epoch: 10 | loss: 0.2983802\n",
      "\tspeed: 0.0982s/iter; left time: 13.9401s\n",
      "\titers: 900, epoch: 10 | loss: 0.2642350\n",
      "\tspeed: 0.0982s/iter; left time: 4.1245s\n",
      "Epoch: 10 running time: 1.5469300587972006 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.2590863 Vali Loss: 0.3810705 Test Loss: 0.6588838\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 5) (269, 32, 24, 5)\n",
      "test shape: (8608, 24, 5) (8608, 24, 5)\n",
      "mse:0.6564114093780518, mae:0.5621141195297241\n",
      "\n",
      "Time intermediate for GB dataset: 30.468563119570415 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4183913\n",
      "\tspeed: 0.1072s/iter; left time: 997.7440s\n",
      "\titers: 200, epoch: 1 | loss: 0.3176747\n",
      "\tspeed: 0.0978s/iter; left time: 900.8986s\n",
      "\titers: 300, epoch: 1 | loss: 0.2389105\n",
      "\tspeed: 0.0982s/iter; left time: 894.4722s\n",
      "\titers: 400, epoch: 1 | loss: 0.2031928\n",
      "\tspeed: 0.0982s/iter; left time: 885.2418s\n",
      "\titers: 500, epoch: 1 | loss: 0.3285361\n",
      "\tspeed: 0.0981s/iter; left time: 874.1403s\n",
      "\titers: 600, epoch: 1 | loss: 0.2553721\n",
      "\tspeed: 0.0977s/iter; left time: 860.4383s\n",
      "\titers: 700, epoch: 1 | loss: 0.1943906\n",
      "\tspeed: 0.0980s/iter; left time: 854.0919s\n",
      "\titers: 800, epoch: 1 | loss: 0.1912926\n",
      "\tspeed: 0.0978s/iter; left time: 842.5617s\n",
      "\titers: 900, epoch: 1 | loss: 0.2323324\n",
      "\tspeed: 0.0977s/iter; left time: 831.6367s\n",
      "Epoch: 1 running time: 1.5527711788813272 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3024487 Vali Loss: 0.1919765 Test Loss: 0.3718269\n",
      "Validation loss decreased (inf --> 0.191977).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2539674\n",
      "\tspeed: 0.2869s/iter; left time: 2401.2293s\n",
      "\titers: 200, epoch: 2 | loss: 0.2502235\n",
      "\tspeed: 0.0978s/iter; left time: 808.7327s\n",
      "\titers: 300, epoch: 2 | loss: 0.1820616\n",
      "\tspeed: 0.0978s/iter; left time: 799.0027s\n",
      "\titers: 400, epoch: 2 | loss: 0.1761179\n",
      "\tspeed: 0.0978s/iter; left time: 789.2069s\n",
      "\titers: 500, epoch: 2 | loss: 0.2026360\n",
      "\tspeed: 0.0985s/iter; left time: 784.7859s\n",
      "\titers: 600, epoch: 2 | loss: 0.1865350\n",
      "\tspeed: 0.0982s/iter; left time: 772.8608s\n",
      "\titers: 700, epoch: 2 | loss: 0.1284744\n",
      "\tspeed: 0.0983s/iter; left time: 763.4893s\n",
      "\titers: 800, epoch: 2 | loss: 0.1330113\n",
      "\tspeed: 0.0979s/iter; left time: 750.7078s\n",
      "\titers: 900, epoch: 2 | loss: 0.1531144\n",
      "\tspeed: 0.0978s/iter; left time: 740.2682s\n",
      "Epoch: 2 running time: 1.5425472497940063 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1868798 Vali Loss: 0.1877339 Test Loss: 0.4226499\n",
      "Validation loss decreased (0.191977 --> 0.187734).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1913545\n",
      "\tspeed: 0.2875s/iter; left time: 2135.7922s\n",
      "\titers: 200, epoch: 3 | loss: 0.1383831\n",
      "\tspeed: 0.0984s/iter; left time: 721.3151s\n",
      "\titers: 300, epoch: 3 | loss: 0.1247306\n",
      "\tspeed: 0.0984s/iter; left time: 711.3806s\n",
      "\titers: 400, epoch: 3 | loss: 0.1372115\n",
      "\tspeed: 0.0978s/iter; left time: 697.1526s\n",
      "\titers: 500, epoch: 3 | loss: 0.1255482\n",
      "\tspeed: 0.0976s/iter; left time: 685.7565s\n",
      "\titers: 600, epoch: 3 | loss: 0.1160520\n",
      "\tspeed: 0.0976s/iter; left time: 676.5837s\n",
      "\titers: 700, epoch: 3 | loss: 0.1767505\n",
      "\tspeed: 0.0976s/iter; left time: 666.4980s\n",
      "\titers: 800, epoch: 3 | loss: 0.1513874\n",
      "\tspeed: 0.0976s/iter; left time: 656.8225s\n",
      "\titers: 900, epoch: 3 | loss: 0.1642675\n",
      "\tspeed: 0.0976s/iter; left time: 647.1322s\n",
      "Epoch: 3 running time: 1.5413131872812906 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1582189 Vali Loss: 0.1863528 Test Loss: 0.4277900\n",
      "Validation loss decreased (0.187734 --> 0.186353).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1476804\n",
      "\tspeed: 0.2858s/iter; left time: 1854.4157s\n",
      "\titers: 200, epoch: 4 | loss: 0.1471415\n",
      "\tspeed: 0.0978s/iter; left time: 624.6770s\n",
      "\titers: 300, epoch: 4 | loss: 0.1413845\n",
      "\tspeed: 0.0978s/iter; left time: 614.6736s\n",
      "\titers: 400, epoch: 4 | loss: 0.1490542\n",
      "\tspeed: 0.0981s/iter; left time: 606.8956s\n",
      "\titers: 500, epoch: 4 | loss: 0.1359551\n",
      "\tspeed: 0.0983s/iter; left time: 598.2066s\n",
      "\titers: 600, epoch: 4 | loss: 0.1420367\n",
      "\tspeed: 0.0976s/iter; left time: 584.6019s\n",
      "\titers: 700, epoch: 4 | loss: 0.1456454\n",
      "\tspeed: 0.0975s/iter; left time: 574.0613s\n",
      "\titers: 800, epoch: 4 | loss: 0.1749017\n",
      "\tspeed: 0.0975s/iter; left time: 564.4226s\n",
      "\titers: 900, epoch: 4 | loss: 0.1734580\n",
      "\tspeed: 0.0978s/iter; left time: 556.1529s\n",
      "Epoch: 4 running time: 1.539186914761861 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1428258 Vali Loss: 0.1726202 Test Loss: 0.3963655\n",
      "Validation loss decreased (0.186353 --> 0.172620).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1538869\n",
      "\tspeed: 0.2862s/iter; left time: 1587.6994s\n",
      "\titers: 200, epoch: 5 | loss: 0.1421568\n",
      "\tspeed: 0.0976s/iter; left time: 531.4726s\n",
      "\titers: 300, epoch: 5 | loss: 0.1535484\n",
      "\tspeed: 0.0976s/iter; left time: 521.8218s\n",
      "\titers: 400, epoch: 5 | loss: 0.0991839\n",
      "\tspeed: 0.0974s/iter; left time: 510.9178s\n",
      "\titers: 500, epoch: 5 | loss: 0.1206037\n",
      "\tspeed: 0.0976s/iter; left time: 502.4609s\n",
      "\titers: 600, epoch: 5 | loss: 0.1114162\n",
      "\tspeed: 0.0975s/iter; left time: 491.8379s\n",
      "\titers: 700, epoch: 5 | loss: 0.1209668\n",
      "\tspeed: 0.0974s/iter; left time: 481.6596s\n",
      "\titers: 800, epoch: 5 | loss: 0.1229157\n",
      "\tspeed: 0.0975s/iter; left time: 472.7072s\n",
      "\titers: 900, epoch: 5 | loss: 0.1002003\n",
      "\tspeed: 0.0981s/iter; left time: 465.8038s\n",
      "Epoch: 5 running time: 1.537100859483083 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1325181 Vali Loss: 0.1755163 Test Loss: 0.4312892\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1051527\n",
      "\tspeed: 0.2787s/iter; left time: 1283.5629s\n",
      "\titers: 200, epoch: 6 | loss: 0.1434750\n",
      "\tspeed: 0.0975s/iter; left time: 439.2067s\n",
      "\titers: 300, epoch: 6 | loss: 0.1429069\n",
      "\tspeed: 0.0975s/iter; left time: 429.7380s\n",
      "\titers: 400, epoch: 6 | loss: 0.1395658\n",
      "\tspeed: 0.0973s/iter; left time: 419.0939s\n",
      "\titers: 500, epoch: 6 | loss: 0.1126466\n",
      "\tspeed: 0.0975s/iter; left time: 410.1389s\n",
      "\titers: 600, epoch: 6 | loss: 0.0956490\n",
      "\tspeed: 0.0976s/iter; left time: 400.6408s\n",
      "\titers: 700, epoch: 6 | loss: 0.1260535\n",
      "\tspeed: 0.0975s/iter; left time: 390.5135s\n",
      "\titers: 800, epoch: 6 | loss: 0.1255859\n",
      "\tspeed: 0.0972s/iter; left time: 379.5172s\n",
      "\titers: 900, epoch: 6 | loss: 0.1606035\n",
      "\tspeed: 0.0974s/iter; left time: 370.5261s\n",
      "Epoch: 6 running time: 1.5332700729370117 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1315052 Vali Loss: 0.1704435 Test Loss: 0.4157898\n",
      "Validation loss decreased (0.172620 --> 0.170444).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1184536\n",
      "\tspeed: 0.2840s/iter; left time: 1040.9181s\n",
      "\titers: 200, epoch: 7 | loss: 0.1058492\n",
      "\tspeed: 0.0980s/iter; left time: 349.2116s\n",
      "\titers: 300, epoch: 7 | loss: 0.1113770\n",
      "\tspeed: 0.0981s/iter; left time: 339.7681s\n",
      "\titers: 400, epoch: 7 | loss: 0.1360071\n",
      "\tspeed: 0.0978s/iter; left time: 328.9867s\n",
      "\titers: 500, epoch: 7 | loss: 0.1233190\n",
      "\tspeed: 0.0977s/iter; left time: 318.8432s\n",
      "\titers: 600, epoch: 7 | loss: 0.1102490\n",
      "\tspeed: 0.0975s/iter; left time: 308.5809s\n",
      "\titers: 700, epoch: 7 | loss: 0.1098698\n",
      "\tspeed: 0.0976s/iter; left time: 299.1463s\n",
      "\titers: 800, epoch: 7 | loss: 0.0954850\n",
      "\tspeed: 0.0976s/iter; left time: 289.5034s\n",
      "\titers: 900, epoch: 7 | loss: 0.1111772\n",
      "\tspeed: 0.0977s/iter; left time: 279.8117s\n",
      "Epoch: 7 running time: 1.5388555566469828 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1274667 Vali Loss: 0.1747580 Test Loss: 0.4195948\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1401287\n",
      "\tspeed: 0.2791s/iter; left time: 760.1574s\n",
      "\titers: 200, epoch: 8 | loss: 0.1006209\n",
      "\tspeed: 0.0980s/iter; left time: 257.1423s\n",
      "\titers: 300, epoch: 8 | loss: 0.1409823\n",
      "\tspeed: 0.0980s/iter; left time: 247.3329s\n",
      "\titers: 400, epoch: 8 | loss: 0.1357944\n",
      "\tspeed: 0.0980s/iter; left time: 237.6313s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975027\n",
      "\tspeed: 0.0982s/iter; left time: 228.1614s\n",
      "\titers: 600, epoch: 8 | loss: 0.1578860\n",
      "\tspeed: 0.0977s/iter; left time: 217.2455s\n",
      "\titers: 700, epoch: 8 | loss: 0.1083857\n",
      "\tspeed: 0.0976s/iter; left time: 207.3591s\n",
      "\titers: 800, epoch: 8 | loss: 0.1733156\n",
      "\tspeed: 0.0977s/iter; left time: 197.7911s\n",
      "\titers: 900, epoch: 8 | loss: 0.1274520\n",
      "\tspeed: 0.0980s/iter; left time: 188.5411s\n",
      "Epoch: 8 running time: 1.5404897371927897 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1269825 Vali Loss: 0.1702255 Test Loss: 0.4147992\n",
      "Validation loss decreased (0.170444 --> 0.170226).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1236518\n",
      "\tspeed: 0.2837s/iter; left time: 505.8556s\n",
      "\titers: 200, epoch: 9 | loss: 0.1474202\n",
      "\tspeed: 0.0977s/iter; left time: 164.4516s\n",
      "\titers: 300, epoch: 9 | loss: 0.1326301\n",
      "\tspeed: 0.0977s/iter; left time: 154.7009s\n",
      "\titers: 400, epoch: 9 | loss: 0.1410056\n",
      "\tspeed: 0.0977s/iter; left time: 144.8837s\n",
      "\titers: 500, epoch: 9 | loss: 0.1886584\n",
      "\tspeed: 0.0977s/iter; left time: 135.1679s\n",
      "\titers: 600, epoch: 9 | loss: 0.1180006\n",
      "\tspeed: 0.0978s/iter; left time: 125.4362s\n",
      "\titers: 700, epoch: 9 | loss: 0.1217441\n",
      "\tspeed: 0.0978s/iter; left time: 115.6728s\n",
      "\titers: 800, epoch: 9 | loss: 0.1094521\n",
      "\tspeed: 0.0978s/iter; left time: 105.9416s\n",
      "\titers: 900, epoch: 9 | loss: 0.1150863\n",
      "\tspeed: 0.0977s/iter; left time: 96.0801s\n",
      "Epoch: 9 running time: 1.5383431275685628 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1260302 Vali Loss: 0.1710989 Test Loss: 0.4079327\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0950374\n",
      "\tspeed: 0.2780s/iter; left time: 234.0744s\n",
      "\titers: 200, epoch: 10 | loss: 0.1361429\n",
      "\tspeed: 0.0977s/iter; left time: 72.4586s\n",
      "\titers: 300, epoch: 10 | loss: 0.1148399\n",
      "\tspeed: 0.0975s/iter; left time: 62.5860s\n",
      "\titers: 400, epoch: 10 | loss: 0.1340534\n",
      "\tspeed: 0.0854s/iter; left time: 46.2878s\n",
      "\titers: 500, epoch: 10 | loss: 0.1267132\n",
      "\tspeed: 0.0809s/iter; left time: 35.7678s\n",
      "\titers: 600, epoch: 10 | loss: 0.1358077\n",
      "\tspeed: 0.0925s/iter; left time: 31.6383s\n",
      "\titers: 700, epoch: 10 | loss: 0.1198425\n",
      "\tspeed: 0.0810s/iter; left time: 19.6036s\n",
      "\titers: 800, epoch: 10 | loss: 0.2074506\n",
      "\tspeed: 0.0809s/iter; left time: 11.4930s\n",
      "\titers: 900, epoch: 10 | loss: 0.1105775\n",
      "\tspeed: 0.0809s/iter; left time: 3.3985s\n",
      "Epoch: 10 running time: 1.3840091983477274 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1258493 Vali Loss: 0.1716152 Test Loss: 0.4135497\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.4149816930294037, mae:0.4109726548194885\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.3302637\n",
      "\tspeed: 0.1006s/iter; left time: 936.4050s\n",
      "\titers: 200, epoch: 1 | loss: 0.3151070\n",
      "\tspeed: 0.0982s/iter; left time: 904.5968s\n",
      "\titers: 300, epoch: 1 | loss: 0.2623212\n",
      "\tspeed: 0.0982s/iter; left time: 894.3453s\n",
      "\titers: 400, epoch: 1 | loss: 0.2294310\n",
      "\tspeed: 0.0980s/iter; left time: 883.0982s\n",
      "\titers: 500, epoch: 1 | loss: 0.2054772\n",
      "\tspeed: 0.0980s/iter; left time: 873.2886s\n",
      "\titers: 600, epoch: 1 | loss: 0.2432472\n",
      "\tspeed: 0.0981s/iter; left time: 864.3074s\n",
      "\titers: 700, epoch: 1 | loss: 0.2112736\n",
      "\tspeed: 0.0981s/iter; left time: 854.8301s\n",
      "\titers: 800, epoch: 1 | loss: 0.2934595\n",
      "\tspeed: 0.0984s/iter; left time: 847.4383s\n",
      "\titers: 900, epoch: 1 | loss: 0.2331795\n",
      "\tspeed: 0.0982s/iter; left time: 835.8786s\n",
      "Epoch: 1 running time: 1.5455976327260335 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3063375 Vali Loss: 0.2056014 Test Loss: 0.3703719\n",
      "Validation loss decreased (inf --> 0.205601).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2075216\n",
      "\tspeed: 0.2866s/iter; left time: 2398.4506s\n",
      "\titers: 200, epoch: 2 | loss: 0.1935526\n",
      "\tspeed: 0.0977s/iter; left time: 807.7090s\n",
      "\titers: 300, epoch: 2 | loss: 0.1708436\n",
      "\tspeed: 0.0975s/iter; left time: 796.9012s\n",
      "\titers: 400, epoch: 2 | loss: 0.1765831\n",
      "\tspeed: 0.0976s/iter; left time: 787.6118s\n",
      "\titers: 500, epoch: 2 | loss: 0.2278909\n",
      "\tspeed: 0.0976s/iter; left time: 778.0649s\n",
      "\titers: 600, epoch: 2 | loss: 0.1893847\n",
      "\tspeed: 0.0977s/iter; left time: 768.6612s\n",
      "\titers: 700, epoch: 2 | loss: 0.1558357\n",
      "\tspeed: 0.0978s/iter; left time: 759.6271s\n",
      "\titers: 800, epoch: 2 | loss: 0.1449818\n",
      "\tspeed: 0.0978s/iter; left time: 749.8155s\n",
      "\titers: 900, epoch: 2 | loss: 0.1376279\n",
      "\tspeed: 0.0976s/iter; left time: 739.0294s\n",
      "Epoch: 2 running time: 1.5377209385236104 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1848781 Vali Loss: 0.1817326 Test Loss: 0.3549186\n",
      "Validation loss decreased (0.205601 --> 0.181733).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1394800\n",
      "\tspeed: 0.2872s/iter; left time: 2133.8337s\n",
      "\titers: 200, epoch: 3 | loss: 0.1271416\n",
      "\tspeed: 0.0979s/iter; left time: 717.8706s\n",
      "\titers: 300, epoch: 3 | loss: 0.1655629\n",
      "\tspeed: 0.0977s/iter; left time: 706.4635s\n",
      "\titers: 400, epoch: 3 | loss: 0.1368292\n",
      "\tspeed: 0.0980s/iter; left time: 698.5540s\n",
      "\titers: 500, epoch: 3 | loss: 0.1455152\n",
      "\tspeed: 0.0974s/iter; left time: 684.4711s\n",
      "\titers: 600, epoch: 3 | loss: 0.1904377\n",
      "\tspeed: 0.0975s/iter; left time: 675.7959s\n",
      "\titers: 700, epoch: 3 | loss: 0.2081547\n",
      "\tspeed: 0.0976s/iter; left time: 666.3756s\n",
      "\titers: 800, epoch: 3 | loss: 0.1410057\n",
      "\tspeed: 0.0976s/iter; left time: 656.6885s\n",
      "\titers: 900, epoch: 3 | loss: 0.1790302\n",
      "\tspeed: 0.0976s/iter; left time: 646.7100s\n",
      "Epoch: 3 running time: 1.5385660449663798 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1583684 Vali Loss: 0.1737744 Test Loss: 0.3850651\n",
      "Validation loss decreased (0.181733 --> 0.173774).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1106178\n",
      "\tspeed: 0.2862s/iter; left time: 1857.0656s\n",
      "\titers: 200, epoch: 4 | loss: 0.1032562\n",
      "\tspeed: 0.0980s/iter; left time: 626.1459s\n",
      "\titers: 300, epoch: 4 | loss: 0.1630179\n",
      "\tspeed: 0.0976s/iter; left time: 613.5937s\n",
      "\titers: 400, epoch: 4 | loss: 0.1381390\n",
      "\tspeed: 0.0977s/iter; left time: 604.7695s\n",
      "\titers: 500, epoch: 4 | loss: 0.1481630\n",
      "\tspeed: 0.0978s/iter; left time: 595.3543s\n",
      "\titers: 600, epoch: 4 | loss: 0.1578198\n",
      "\tspeed: 0.0977s/iter; left time: 584.7553s\n",
      "\titers: 700, epoch: 4 | loss: 0.1212454\n",
      "\tspeed: 0.0977s/iter; left time: 575.2874s\n",
      "\titers: 800, epoch: 4 | loss: 0.1387872\n",
      "\tspeed: 0.0976s/iter; left time: 565.0239s\n",
      "\titers: 900, epoch: 4 | loss: 0.0989111\n",
      "\tspeed: 0.0976s/iter; left time: 555.3796s\n",
      "Epoch: 4 running time: 1.5385752638181052 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1416687 Vali Loss: 0.1775134 Test Loss: 0.4505916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1332261\n",
      "\tspeed: 0.2798s/iter; left time: 1551.8333s\n",
      "\titers: 200, epoch: 5 | loss: 0.1178803\n",
      "\tspeed: 0.0977s/iter; left time: 532.1037s\n",
      "\titers: 300, epoch: 5 | loss: 0.1324545\n",
      "\tspeed: 0.0977s/iter; left time: 522.6032s\n",
      "\titers: 400, epoch: 5 | loss: 0.1655561\n",
      "\tspeed: 0.0980s/iter; left time: 514.4260s\n",
      "\titers: 500, epoch: 5 | loss: 0.1961126\n",
      "\tspeed: 0.0980s/iter; left time: 504.5302s\n",
      "\titers: 600, epoch: 5 | loss: 0.1407264\n",
      "\tspeed: 0.0981s/iter; left time: 494.8663s\n",
      "\titers: 700, epoch: 5 | loss: 0.1143082\n",
      "\tspeed: 0.0983s/iter; left time: 486.0738s\n",
      "\titers: 800, epoch: 5 | loss: 0.1553129\n",
      "\tspeed: 0.0982s/iter; left time: 475.8228s\n",
      "\titers: 900, epoch: 5 | loss: 0.1202482\n",
      "\tspeed: 0.0981s/iter; left time: 465.5710s\n",
      "Epoch: 5 running time: 1.534405815601349 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1385942 Vali Loss: 0.1720676 Test Loss: 0.4219375\n",
      "Validation loss decreased (0.173774 --> 0.172068).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1157490\n",
      "\tspeed: 0.2818s/iter; left time: 1298.0999s\n",
      "\titers: 200, epoch: 6 | loss: 0.1406169\n",
      "\tspeed: 0.0975s/iter; left time: 439.4475s\n",
      "\titers: 300, epoch: 6 | loss: 0.1372459\n",
      "\tspeed: 0.0975s/iter; left time: 429.3841s\n",
      "\titers: 400, epoch: 6 | loss: 0.1087425\n",
      "\tspeed: 0.0979s/iter; left time: 421.6321s\n",
      "\titers: 500, epoch: 6 | loss: 0.1378059\n",
      "\tspeed: 0.0979s/iter; left time: 411.8105s\n",
      "\titers: 600, epoch: 6 | loss: 0.1528922\n",
      "\tspeed: 0.0980s/iter; left time: 402.2005s\n",
      "\titers: 700, epoch: 6 | loss: 0.1253271\n",
      "\tspeed: 0.0981s/iter; left time: 392.8542s\n",
      "\titers: 800, epoch: 6 | loss: 0.1130694\n",
      "\tspeed: 0.0980s/iter; left time: 382.8259s\n",
      "\titers: 900, epoch: 6 | loss: 0.1053836\n",
      "\tspeed: 0.0977s/iter; left time: 371.7387s\n",
      "Epoch: 6 running time: 1.5394508878389994 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1311712 Vali Loss: 0.1755019 Test Loss: 0.4397382\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1171806\n",
      "\tspeed: 0.2819s/iter; left time: 1033.3229s\n",
      "\titers: 200, epoch: 7 | loss: 0.1571493\n",
      "\tspeed: 0.0986s/iter; left time: 351.6177s\n",
      "\titers: 300, epoch: 7 | loss: 0.1066693\n",
      "\tspeed: 0.0985s/iter; left time: 341.1681s\n",
      "\titers: 400, epoch: 7 | loss: 0.1229019\n",
      "\tspeed: 0.0985s/iter; left time: 331.5692s\n",
      "\titers: 500, epoch: 7 | loss: 0.1200267\n",
      "\tspeed: 0.0983s/iter; left time: 320.7892s\n",
      "\titers: 600, epoch: 7 | loss: 0.1354899\n",
      "\tspeed: 0.0977s/iter; left time: 309.3551s\n",
      "\titers: 700, epoch: 7 | loss: 0.1273839\n",
      "\tspeed: 0.0977s/iter; left time: 299.3756s\n",
      "\titers: 800, epoch: 7 | loss: 0.1198170\n",
      "\tspeed: 0.0977s/iter; left time: 289.8079s\n",
      "\titers: 900, epoch: 7 | loss: 0.1362952\n",
      "\tspeed: 0.0979s/iter; left time: 280.5158s\n",
      "Epoch: 7 running time: 1.5471929033597311 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1305958 Vali Loss: 0.1727828 Test Loss: 0.4353591\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1495225\n",
      "\tspeed: 0.2801s/iter; left time: 762.9073s\n",
      "\titers: 200, epoch: 8 | loss: 0.1441906\n",
      "\tspeed: 0.0978s/iter; left time: 256.5227s\n",
      "\titers: 300, epoch: 8 | loss: 0.1668217\n",
      "\tspeed: 0.0977s/iter; left time: 246.6713s\n",
      "\titers: 400, epoch: 8 | loss: 0.1325599\n",
      "\tspeed: 0.0977s/iter; left time: 236.7391s\n",
      "\titers: 500, epoch: 8 | loss: 0.1169434\n",
      "\tspeed: 0.0977s/iter; left time: 227.1326s\n",
      "\titers: 600, epoch: 8 | loss: 0.1312203\n",
      "\tspeed: 0.0973s/iter; left time: 216.3205s\n",
      "\titers: 700, epoch: 8 | loss: 0.1789077\n",
      "\tspeed: 0.0976s/iter; left time: 207.4015s\n",
      "\titers: 800, epoch: 8 | loss: 0.1233529\n",
      "\tspeed: 0.0979s/iter; left time: 198.1780s\n",
      "\titers: 900, epoch: 8 | loss: 0.1030344\n",
      "\tspeed: 0.0977s/iter; left time: 187.8853s\n",
      "Epoch: 8 running time: 1.5382991472880045 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1308838 Vali Loss: 0.1731599 Test Loss: 0.4241140\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.423806756734848, mae:0.4119410812854767\n",
      "\n",
      "Time intermediate for ES dataset: 32.3044050971667 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.3871442\n",
      "\tspeed: 0.1069s/iter; left time: 995.4311s\n",
      "\titers: 200, epoch: 1 | loss: 0.2673644\n",
      "\tspeed: 0.0890s/iter; left time: 819.9429s\n",
      "\titers: 300, epoch: 1 | loss: 0.2006789\n",
      "\tspeed: 0.0837s/iter; left time: 762.3904s\n",
      "\titers: 400, epoch: 1 | loss: 0.1515370\n",
      "\tspeed: 0.0981s/iter; left time: 883.9953s\n",
      "\titers: 500, epoch: 1 | loss: 0.2455160\n",
      "\tspeed: 0.0981s/iter; left time: 873.8359s\n",
      "\titers: 600, epoch: 1 | loss: 0.2780952\n",
      "\tspeed: 0.0978s/iter; left time: 861.7338s\n",
      "\titers: 700, epoch: 1 | loss: 0.2736951\n",
      "\tspeed: 0.0979s/iter; left time: 853.0327s\n",
      "\titers: 800, epoch: 1 | loss: 0.2647220\n",
      "\tspeed: 0.0979s/iter; left time: 842.8808s\n",
      "\titers: 900, epoch: 1 | loss: 0.1626280\n",
      "\tspeed: 0.0979s/iter; left time: 833.1405s\n",
      "Epoch: 1 running time: 1.5138357957204183 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2831952 Vali Loss: 0.2616880 Test Loss: 0.4787354\n",
      "Validation loss decreased (inf --> 0.261688).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1852047\n",
      "\tspeed: 0.2854s/iter; left time: 2389.1435s\n",
      "\titers: 200, epoch: 2 | loss: 0.1698483\n",
      "\tspeed: 0.0973s/iter; left time: 804.8258s\n",
      "\titers: 300, epoch: 2 | loss: 0.2079634\n",
      "\tspeed: 0.0974s/iter; left time: 795.3713s\n",
      "\titers: 400, epoch: 2 | loss: 0.1993060\n",
      "\tspeed: 0.0974s/iter; left time: 786.0162s\n",
      "\titers: 500, epoch: 2 | loss: 0.2143207\n",
      "\tspeed: 0.0974s/iter; left time: 776.5000s\n",
      "\titers: 600, epoch: 2 | loss: 0.1892886\n",
      "\tspeed: 0.0974s/iter; left time: 766.3956s\n",
      "\titers: 700, epoch: 2 | loss: 0.1499426\n",
      "\tspeed: 0.0974s/iter; left time: 756.9036s\n",
      "\titers: 800, epoch: 2 | loss: 0.1230438\n",
      "\tspeed: 0.0974s/iter; left time: 746.8013s\n",
      "\titers: 900, epoch: 2 | loss: 0.2164897\n",
      "\tspeed: 0.0974s/iter; left time: 737.3784s\n",
      "Epoch: 2 running time: 1.5335353374481202 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1828118 Vali Loss: 0.2568518 Test Loss: 0.4131241\n",
      "Validation loss decreased (0.261688 --> 0.256852).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1794982\n",
      "\tspeed: 0.2868s/iter; left time: 2130.9892s\n",
      "\titers: 200, epoch: 3 | loss: 0.1258878\n",
      "\tspeed: 0.0983s/iter; left time: 720.4389s\n",
      "\titers: 300, epoch: 3 | loss: 0.1473013\n",
      "\tspeed: 0.0980s/iter; left time: 708.3873s\n",
      "\titers: 400, epoch: 3 | loss: 0.1278618\n",
      "\tspeed: 0.0981s/iter; left time: 699.1794s\n",
      "\titers: 500, epoch: 3 | loss: 0.1277567\n",
      "\tspeed: 0.0981s/iter; left time: 689.2351s\n",
      "\titers: 600, epoch: 3 | loss: 0.1196522\n",
      "\tspeed: 0.0981s/iter; left time: 679.5763s\n",
      "\titers: 700, epoch: 3 | loss: 0.1161006\n",
      "\tspeed: 0.0978s/iter; left time: 667.7730s\n",
      "\titers: 800, epoch: 3 | loss: 0.2125789\n",
      "\tspeed: 0.0977s/iter; left time: 657.7221s\n",
      "\titers: 900, epoch: 3 | loss: 0.1116768\n",
      "\tspeed: 0.0978s/iter; left time: 648.4570s\n",
      "Epoch: 3 running time: 1.5419822017351785 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1522176 Vali Loss: 0.2416414 Test Loss: 0.3996641\n",
      "Validation loss decreased (0.256852 --> 0.241641).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1334140\n",
      "\tspeed: 0.2861s/iter; left time: 1856.2962s\n",
      "\titers: 200, epoch: 4 | loss: 0.1624571\n",
      "\tspeed: 0.0980s/iter; left time: 626.0158s\n",
      "\titers: 300, epoch: 4 | loss: 0.1250404\n",
      "\tspeed: 0.0979s/iter; left time: 615.6574s\n",
      "\titers: 400, epoch: 4 | loss: 0.1235920\n",
      "\tspeed: 0.0979s/iter; left time: 605.8320s\n",
      "\titers: 500, epoch: 4 | loss: 0.1108047\n",
      "\tspeed: 0.0980s/iter; left time: 596.4108s\n",
      "\titers: 600, epoch: 4 | loss: 0.1781396\n",
      "\tspeed: 0.0978s/iter; left time: 585.7715s\n",
      "\titers: 700, epoch: 4 | loss: 0.1164244\n",
      "\tspeed: 0.0979s/iter; left time: 576.5792s\n",
      "\titers: 800, epoch: 4 | loss: 0.1635794\n",
      "\tspeed: 0.0980s/iter; left time: 567.1916s\n",
      "\titers: 900, epoch: 4 | loss: 0.0831978\n",
      "\tspeed: 0.0953s/iter; left time: 542.1088s\n",
      "Epoch: 4 running time: 1.531578799088796 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1316964 Vali Loss: 0.2272363 Test Loss: 0.4058791\n",
      "Validation loss decreased (0.241641 --> 0.227236).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1401690\n",
      "\tspeed: 0.2901s/iter; left time: 1609.3889s\n",
      "\titers: 200, epoch: 5 | loss: 0.2141778\n",
      "\tspeed: 0.0982s/iter; left time: 534.6809s\n",
      "\titers: 300, epoch: 5 | loss: 0.0976685\n",
      "\tspeed: 0.0981s/iter; left time: 524.3869s\n",
      "\titers: 400, epoch: 5 | loss: 0.1276522\n",
      "\tspeed: 0.0980s/iter; left time: 514.1479s\n",
      "\titers: 500, epoch: 5 | loss: 0.0873264\n",
      "\tspeed: 0.0979s/iter; left time: 504.1038s\n",
      "\titers: 600, epoch: 5 | loss: 0.0939021\n",
      "\tspeed: 0.0982s/iter; left time: 495.5769s\n",
      "\titers: 700, epoch: 5 | loss: 0.1109263\n",
      "\tspeed: 0.0981s/iter; left time: 485.2249s\n",
      "\titers: 800, epoch: 5 | loss: 0.1041656\n",
      "\tspeed: 0.0978s/iter; left time: 473.8629s\n",
      "\titers: 900, epoch: 5 | loss: 0.1287480\n",
      "\tspeed: 0.0983s/iter; left time: 466.8228s\n",
      "Epoch: 5 running time: 1.5440135836601256 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1180394 Vali Loss: 0.2291849 Test Loss: 0.4162515\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0720225\n",
      "\tspeed: 0.2793s/iter; left time: 1286.4110s\n",
      "\titers: 200, epoch: 6 | loss: 0.1116268\n",
      "\tspeed: 0.0979s/iter; left time: 440.9537s\n",
      "\titers: 300, epoch: 6 | loss: 0.1110815\n",
      "\tspeed: 0.0938s/iter; left time: 413.1271s\n",
      "\titers: 400, epoch: 6 | loss: 0.0973093\n",
      "\tspeed: 0.0976s/iter; left time: 420.2059s\n",
      "\titers: 500, epoch: 6 | loss: 0.0811179\n",
      "\tspeed: 0.0977s/iter; left time: 411.0745s\n",
      "\titers: 600, epoch: 6 | loss: 0.1062097\n",
      "\tspeed: 0.0979s/iter; left time: 401.8598s\n",
      "\titers: 700, epoch: 6 | loss: 0.1420697\n",
      "\tspeed: 0.0981s/iter; left time: 392.8887s\n",
      "\titers: 800, epoch: 6 | loss: 0.1018597\n",
      "\tspeed: 0.0980s/iter; left time: 382.7114s\n",
      "\titers: 900, epoch: 6 | loss: 0.1084752\n",
      "\tspeed: 0.0978s/iter; left time: 372.1815s\n",
      "Epoch: 6 running time: 1.5343475143114726 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1172928 Vali Loss: 0.2305413 Test Loss: 0.4107639\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0814674\n",
      "\tspeed: 0.2799s/iter; left time: 1025.9080s\n",
      "\titers: 200, epoch: 7 | loss: 0.0873558\n",
      "\tspeed: 0.0980s/iter; left time: 349.4500s\n",
      "\titers: 300, epoch: 7 | loss: 0.0967285\n",
      "\tspeed: 0.0979s/iter; left time: 339.3697s\n",
      "\titers: 400, epoch: 7 | loss: 0.1214434\n",
      "\tspeed: 0.0976s/iter; left time: 328.4567s\n",
      "\titers: 500, epoch: 7 | loss: 0.1131551\n",
      "\tspeed: 0.0975s/iter; left time: 318.1933s\n",
      "\titers: 600, epoch: 7 | loss: 0.1573701\n",
      "\tspeed: 0.0980s/iter; left time: 310.0173s\n",
      "\titers: 700, epoch: 7 | loss: 0.1066053\n",
      "\tspeed: 0.0978s/iter; left time: 299.7031s\n",
      "\titers: 800, epoch: 7 | loss: 0.1340960\n",
      "\tspeed: 0.0977s/iter; left time: 289.7749s\n",
      "\titers: 900, epoch: 7 | loss: 0.1279623\n",
      "\tspeed: 0.0975s/iter; left time: 279.4537s\n",
      "Epoch: 7 running time: 1.5399282574653625 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1167402 Vali Loss: 0.2287699 Test Loss: 0.4122698\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.40965697169303894, mae:0.35750654339790344\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4186364\n",
      "\tspeed: 0.1010s/iter; left time: 940.0655s\n",
      "\titers: 200, epoch: 1 | loss: 0.3263134\n",
      "\tspeed: 0.0982s/iter; left time: 904.9285s\n",
      "\titers: 300, epoch: 1 | loss: 0.2293927\n",
      "\tspeed: 0.0981s/iter; left time: 894.0635s\n",
      "\titers: 400, epoch: 1 | loss: 0.1484141\n",
      "\tspeed: 0.0984s/iter; left time: 886.4984s\n",
      "\titers: 500, epoch: 1 | loss: 0.1641806\n",
      "\tspeed: 0.0984s/iter; left time: 876.7438s\n",
      "\titers: 600, epoch: 1 | loss: 0.1693609\n",
      "\tspeed: 0.0981s/iter; left time: 864.5007s\n",
      "\titers: 700, epoch: 1 | loss: 0.1987397\n",
      "\tspeed: 0.0986s/iter; left time: 858.7678s\n",
      "\titers: 800, epoch: 1 | loss: 0.1767388\n",
      "\tspeed: 0.0981s/iter; left time: 844.9249s\n",
      "\titers: 900, epoch: 1 | loss: 0.2046778\n",
      "\tspeed: 0.0982s/iter; left time: 835.8932s\n",
      "Epoch: 1 running time: 1.5474209944407145 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2850537 Vali Loss: 0.2336705 Test Loss: 0.4041315\n",
      "Validation loss decreased (inf --> 0.233671).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1910070\n",
      "\tspeed: 0.2886s/iter; left time: 2415.7236s\n",
      "\titers: 200, epoch: 2 | loss: 0.2027248\n",
      "\tspeed: 0.0978s/iter; left time: 809.1651s\n",
      "\titers: 300, epoch: 2 | loss: 0.1684840\n",
      "\tspeed: 0.0978s/iter; left time: 798.7959s\n",
      "\titers: 400, epoch: 2 | loss: 0.1411078\n",
      "\tspeed: 0.0977s/iter; left time: 788.2662s\n",
      "\titers: 500, epoch: 2 | loss: 0.1941990\n",
      "\tspeed: 0.0976s/iter; left time: 778.2225s\n",
      "\titers: 600, epoch: 2 | loss: 0.1813835\n",
      "\tspeed: 0.0978s/iter; left time: 769.3898s\n",
      "\titers: 700, epoch: 2 | loss: 0.1679413\n",
      "\tspeed: 0.0978s/iter; left time: 760.2324s\n",
      "\titers: 800, epoch: 2 | loss: 0.1534651\n",
      "\tspeed: 0.0977s/iter; left time: 749.5613s\n",
      "\titers: 900, epoch: 2 | loss: 0.2049218\n",
      "\tspeed: 0.0976s/iter; left time: 739.1731s\n",
      "Epoch: 2 running time: 1.5386431614557903 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1826428 Vali Loss: 0.2532248 Test Loss: 0.4527559\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2628095\n",
      "\tspeed: 0.2792s/iter; left time: 2074.0601s\n",
      "\titers: 200, epoch: 3 | loss: 0.2365825\n",
      "\tspeed: 0.0976s/iter; left time: 715.6503s\n",
      "\titers: 300, epoch: 3 | loss: 0.1906161\n",
      "\tspeed: 0.0979s/iter; left time: 707.6222s\n",
      "\titers: 400, epoch: 3 | loss: 0.1980529\n",
      "\tspeed: 0.0979s/iter; left time: 697.9331s\n",
      "\titers: 500, epoch: 3 | loss: 0.1365095\n",
      "\tspeed: 0.0980s/iter; left time: 688.8696s\n",
      "\titers: 600, epoch: 3 | loss: 0.1684517\n",
      "\tspeed: 0.0980s/iter; left time: 679.3249s\n",
      "\titers: 700, epoch: 3 | loss: 0.3296774\n",
      "\tspeed: 0.0982s/iter; left time: 670.5298s\n",
      "\titers: 800, epoch: 3 | loss: 0.1282694\n",
      "\tspeed: 0.0981s/iter; left time: 660.0982s\n",
      "\titers: 900, epoch: 3 | loss: 0.1457012\n",
      "\tspeed: 0.0981s/iter; left time: 650.3731s\n",
      "Epoch: 3 running time: 1.5414589126904805 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1762556 Vali Loss: 0.2430529 Test Loss: 0.3964816\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1606507\n",
      "\tspeed: 0.2662s/iter; left time: 1726.8307s\n",
      "\titers: 200, epoch: 4 | loss: 0.1244453\n",
      "\tspeed: 0.0978s/iter; left time: 624.9634s\n",
      "\titers: 300, epoch: 4 | loss: 0.2576856\n",
      "\tspeed: 0.0978s/iter; left time: 615.0768s\n",
      "\titers: 400, epoch: 4 | loss: 0.2086918\n",
      "\tspeed: 0.0978s/iter; left time: 605.3667s\n",
      "\titers: 500, epoch: 4 | loss: 0.1658135\n",
      "\tspeed: 0.0978s/iter; left time: 595.4880s\n",
      "\titers: 600, epoch: 4 | loss: 0.1352087\n",
      "\tspeed: 0.0976s/iter; left time: 584.5615s\n",
      "\titers: 700, epoch: 4 | loss: 0.1375851\n",
      "\tspeed: 0.0977s/iter; left time: 575.1545s\n",
      "\titers: 800, epoch: 4 | loss: 0.1459120\n",
      "\tspeed: 0.0977s/iter; left time: 565.6086s\n",
      "\titers: 900, epoch: 4 | loss: 0.1659870\n",
      "\tspeed: 0.0976s/iter; left time: 555.1267s\n",
      "Epoch: 4 running time: 1.5165399193763733 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1710265 Vali Loss: 0.2365344 Test Loss: 0.3860765\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.3855854570865631, mae:0.3636378049850464\n",
      "\n",
      "Time intermediate for FR dataset: 19.977133317788443 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.4839092\n",
      "\tspeed: 0.1066s/iter; left time: 992.9850s\n",
      "\titers: 200, epoch: 1 | loss: 0.2879023\n",
      "\tspeed: 0.0973s/iter; left time: 896.5701s\n",
      "\titers: 300, epoch: 1 | loss: 0.1979710\n",
      "\tspeed: 0.0976s/iter; left time: 888.8603s\n",
      "\titers: 400, epoch: 1 | loss: 0.2898935\n",
      "\tspeed: 0.0974s/iter; left time: 878.1138s\n",
      "\titers: 500, epoch: 1 | loss: 0.2302311\n",
      "\tspeed: 0.0975s/iter; left time: 868.8615s\n",
      "\titers: 600, epoch: 1 | loss: 0.1686959\n",
      "\tspeed: 0.0975s/iter; left time: 858.6793s\n",
      "\titers: 700, epoch: 1 | loss: 0.1522209\n",
      "\tspeed: 0.0974s/iter; left time: 848.2631s\n",
      "\titers: 800, epoch: 1 | loss: 0.1437175\n",
      "\tspeed: 0.0976s/iter; left time: 840.4306s\n",
      "\titers: 900, epoch: 1 | loss: 0.1686486\n",
      "\tspeed: 0.0975s/iter; left time: 829.4021s\n",
      "Epoch: 1 running time: 1.5453044414520263 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2949341 Vali Loss: 0.1825299 Test Loss: 0.2419658\n",
      "Validation loss decreased (inf --> 0.182530).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2636757\n",
      "\tspeed: 0.2838s/iter; left time: 2375.5902s\n",
      "\titers: 200, epoch: 2 | loss: 0.1816243\n",
      "\tspeed: 0.0976s/iter; left time: 807.4244s\n",
      "\titers: 300, epoch: 2 | loss: 0.1623272\n",
      "\tspeed: 0.0975s/iter; left time: 796.4880s\n",
      "\titers: 400, epoch: 2 | loss: 0.1740752\n",
      "\tspeed: 0.0974s/iter; left time: 786.0349s\n",
      "\titers: 500, epoch: 2 | loss: 0.1692941\n",
      "\tspeed: 0.0976s/iter; left time: 778.0605s\n",
      "\titers: 600, epoch: 2 | loss: 0.1518331\n",
      "\tspeed: 0.0975s/iter; left time: 767.6252s\n",
      "\titers: 700, epoch: 2 | loss: 0.1923162\n",
      "\tspeed: 0.0974s/iter; left time: 756.9518s\n",
      "\titers: 800, epoch: 2 | loss: 0.1279732\n",
      "\tspeed: 0.0974s/iter; left time: 746.6892s\n",
      "\titers: 900, epoch: 2 | loss: 0.1602855\n",
      "\tspeed: 0.0973s/iter; left time: 736.8447s\n",
      "Epoch: 2 running time: 1.5341623226801555 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1825944 Vali Loss: 0.1623728 Test Loss: 0.2211493\n",
      "Validation loss decreased (0.182530 --> 0.162373).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2162754\n",
      "\tspeed: 0.2838s/iter; left time: 2108.5827s\n",
      "\titers: 200, epoch: 3 | loss: 0.1780417\n",
      "\tspeed: 0.0974s/iter; left time: 714.0807s\n",
      "\titers: 300, epoch: 3 | loss: 0.1844170\n",
      "\tspeed: 0.0974s/iter; left time: 704.1188s\n",
      "\titers: 400, epoch: 3 | loss: 0.2321048\n",
      "\tspeed: 0.0974s/iter; left time: 694.1767s\n",
      "\titers: 500, epoch: 3 | loss: 0.1619159\n",
      "\tspeed: 0.0974s/iter; left time: 684.3971s\n",
      "\titers: 600, epoch: 3 | loss: 0.2035481\n",
      "\tspeed: 0.0973s/iter; left time: 674.3872s\n",
      "\titers: 700, epoch: 3 | loss: 0.1415086\n",
      "\tspeed: 0.0974s/iter; left time: 665.2122s\n",
      "\titers: 800, epoch: 3 | loss: 0.1461832\n",
      "\tspeed: 0.0975s/iter; left time: 656.0179s\n",
      "\titers: 900, epoch: 3 | loss: 0.1757724\n",
      "\tspeed: 0.0975s/iter; left time: 646.6506s\n",
      "Epoch: 3 running time: 1.5335398634274802 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1551666 Vali Loss: 0.1509909 Test Loss: 0.2212254\n",
      "Validation loss decreased (0.162373 --> 0.150991).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1292468\n",
      "\tspeed: 0.2858s/iter; left time: 1854.2312s\n",
      "\titers: 200, epoch: 4 | loss: 0.1385312\n",
      "\tspeed: 0.0979s/iter; left time: 625.6877s\n",
      "\titers: 300, epoch: 4 | loss: 0.1528608\n",
      "\tspeed: 0.0977s/iter; left time: 614.1243s\n",
      "\titers: 400, epoch: 4 | loss: 0.1385029\n",
      "\tspeed: 0.0976s/iter; left time: 603.7465s\n",
      "\titers: 500, epoch: 4 | loss: 0.1575158\n",
      "\tspeed: 0.0976s/iter; left time: 594.1274s\n",
      "\titers: 600, epoch: 4 | loss: 0.1237735\n",
      "\tspeed: 0.0977s/iter; left time: 585.1954s\n",
      "\titers: 700, epoch: 4 | loss: 0.1452523\n",
      "\tspeed: 0.0978s/iter; left time: 575.6677s\n",
      "\titers: 800, epoch: 4 | loss: 0.1854530\n",
      "\tspeed: 0.0977s/iter; left time: 565.4985s\n",
      "\titers: 900, epoch: 4 | loss: 0.1564882\n",
      "\tspeed: 0.0976s/iter; left time: 555.1507s\n",
      "Epoch: 4 running time: 1.5377938310305277 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1399588 Vali Loss: 0.1468418 Test Loss: 0.2187929\n",
      "Validation loss decreased (0.150991 --> 0.146842).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1530052\n",
      "\tspeed: 0.2846s/iter; left time: 1578.4834s\n",
      "\titers: 200, epoch: 5 | loss: 0.1308514\n",
      "\tspeed: 0.0972s/iter; left time: 529.5762s\n",
      "\titers: 300, epoch: 5 | loss: 0.1466258\n",
      "\tspeed: 0.0973s/iter; left time: 520.4438s\n",
      "\titers: 400, epoch: 5 | loss: 0.1345804\n",
      "\tspeed: 0.0975s/iter; left time: 511.4472s\n",
      "\titers: 500, epoch: 5 | loss: 0.1079054\n",
      "\tspeed: 0.0974s/iter; left time: 501.4257s\n",
      "\titers: 600, epoch: 5 | loss: 0.1301066\n",
      "\tspeed: 0.0973s/iter; left time: 491.1329s\n",
      "\titers: 700, epoch: 5 | loss: 0.1477804\n",
      "\tspeed: 0.0972s/iter; left time: 481.0689s\n",
      "\titers: 800, epoch: 5 | loss: 0.1133912\n",
      "\tspeed: 0.0976s/iter; left time: 472.8656s\n",
      "\titers: 900, epoch: 5 | loss: 0.1090341\n",
      "\tspeed: 0.0977s/iter; left time: 463.8174s\n",
      "Epoch: 5 running time: 1.5345877250035604 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1297619 Vali Loss: 0.1458476 Test Loss: 0.2184652\n",
      "Validation loss decreased (0.146842 --> 0.145848).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1200157\n",
      "\tspeed: 0.2862s/iter; left time: 1318.1616s\n",
      "\titers: 200, epoch: 6 | loss: 0.1132532\n",
      "\tspeed: 0.0979s/iter; left time: 441.1880s\n",
      "\titers: 300, epoch: 6 | loss: 0.1247416\n",
      "\tspeed: 0.0975s/iter; left time: 429.7224s\n",
      "\titers: 400, epoch: 6 | loss: 0.1308755\n",
      "\tspeed: 0.0974s/iter; left time: 419.2142s\n",
      "\titers: 500, epoch: 6 | loss: 0.1172737\n",
      "\tspeed: 0.0976s/iter; left time: 410.4727s\n",
      "\titers: 600, epoch: 6 | loss: 0.1246800\n",
      "\tspeed: 0.0973s/iter; left time: 399.3930s\n",
      "\titers: 700, epoch: 6 | loss: 0.1159766\n",
      "\tspeed: 0.0976s/iter; left time: 390.8258s\n",
      "\titers: 800, epoch: 6 | loss: 0.1150436\n",
      "\tspeed: 0.0980s/iter; left time: 382.7629s\n",
      "\titers: 900, epoch: 6 | loss: 0.1527046\n",
      "\tspeed: 0.0977s/iter; left time: 371.9833s\n",
      "Epoch: 6 running time: 1.537830646832784 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1229240 Vali Loss: 0.1457663 Test Loss: 0.2212498\n",
      "Validation loss decreased (0.145848 --> 0.145766).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1354768\n",
      "\tspeed: 0.2848s/iter; left time: 1043.8545s\n",
      "\titers: 200, epoch: 7 | loss: 0.1053292\n",
      "\tspeed: 0.0971s/iter; left time: 346.3194s\n",
      "\titers: 300, epoch: 7 | loss: 0.1470087\n",
      "\tspeed: 0.0975s/iter; left time: 337.7739s\n",
      "\titers: 400, epoch: 7 | loss: 0.1179281\n",
      "\tspeed: 0.0974s/iter; left time: 327.8276s\n",
      "\titers: 500, epoch: 7 | loss: 0.1284748\n",
      "\tspeed: 0.0976s/iter; left time: 318.7656s\n",
      "\titers: 600, epoch: 7 | loss: 0.0976006\n",
      "\tspeed: 0.0974s/iter; left time: 308.2349s\n",
      "\titers: 700, epoch: 7 | loss: 0.1113579\n",
      "\tspeed: 0.0973s/iter; left time: 298.0942s\n",
      "\titers: 800, epoch: 7 | loss: 0.1444205\n",
      "\tspeed: 0.0976s/iter; left time: 289.2410s\n",
      "\titers: 900, epoch: 7 | loss: 0.1121703\n",
      "\tspeed: 0.0977s/iter; left time: 279.9088s\n",
      "Epoch: 7 running time: 1.5341106096903483 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1188808 Vali Loss: 0.1469098 Test Loss: 0.2276276\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1236797\n",
      "\tspeed: 0.2797s/iter; left time: 761.8005s\n",
      "\titers: 200, epoch: 8 | loss: 0.1208407\n",
      "\tspeed: 0.0977s/iter; left time: 256.3200s\n",
      "\titers: 300, epoch: 8 | loss: 0.0918566\n",
      "\tspeed: 0.0976s/iter; left time: 246.2813s\n",
      "\titers: 400, epoch: 8 | loss: 0.1157172\n",
      "\tspeed: 0.0975s/iter; left time: 236.3369s\n",
      "\titers: 500, epoch: 8 | loss: 0.1351758\n",
      "\tspeed: 0.0973s/iter; left time: 226.0756s\n",
      "\titers: 600, epoch: 8 | loss: 0.1347238\n",
      "\tspeed: 0.0976s/iter; left time: 216.9753s\n",
      "\titers: 700, epoch: 8 | loss: 0.1363690\n",
      "\tspeed: 0.0973s/iter; left time: 206.6648s\n",
      "\titers: 800, epoch: 8 | loss: 0.1204014\n",
      "\tspeed: 0.0973s/iter; left time: 196.8978s\n",
      "\titers: 900, epoch: 8 | loss: 0.1477749\n",
      "\tspeed: 0.0973s/iter; left time: 187.2952s\n",
      "Epoch: 8 running time: 1.5349057833353679 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1187568 Vali Loss: 0.1480086 Test Loss: 0.2246380\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1125018\n",
      "\tspeed: 0.2789s/iter; left time: 497.2608s\n",
      "\titers: 200, epoch: 9 | loss: 0.1158750\n",
      "\tspeed: 0.0973s/iter; left time: 163.8118s\n",
      "\titers: 300, epoch: 9 | loss: 0.0744595\n",
      "\tspeed: 0.0971s/iter; left time: 153.6721s\n",
      "\titers: 400, epoch: 9 | loss: 0.1645487\n",
      "\tspeed: 0.0971s/iter; left time: 144.0459s\n",
      "\titers: 500, epoch: 9 | loss: 0.1351148\n",
      "\tspeed: 0.0972s/iter; left time: 134.4204s\n",
      "\titers: 600, epoch: 9 | loss: 0.1294514\n",
      "\tspeed: 0.0974s/iter; left time: 124.9993s\n",
      "\titers: 700, epoch: 9 | loss: 0.1100625\n",
      "\tspeed: 0.0978s/iter; left time: 115.7038s\n",
      "\titers: 800, epoch: 9 | loss: 0.0923413\n",
      "\tspeed: 0.0973s/iter; left time: 105.4079s\n",
      "\titers: 900, epoch: 9 | loss: 0.1044033\n",
      "\tspeed: 0.0976s/iter; left time: 95.9670s\n",
      "Epoch: 9 running time: 1.5329768737157186 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1184515 Vali Loss: 0.1479403 Test Loss: 0.2238555\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.22542472183704376, mae:0.28379637002944946\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 4321\n",
      "test 8617\n",
      "\titers: 100, epoch: 1 | loss: 0.5664268\n",
      "\tspeed: 0.1015s/iter; left time: 945.3234s\n",
      "\titers: 200, epoch: 1 | loss: 0.2526940\n",
      "\tspeed: 0.0982s/iter; left time: 904.5285s\n",
      "\titers: 300, epoch: 1 | loss: 0.2292914\n",
      "\tspeed: 0.0979s/iter; left time: 892.0578s\n",
      "\titers: 400, epoch: 1 | loss: 0.2790347\n",
      "\tspeed: 0.0977s/iter; left time: 880.2624s\n",
      "\titers: 500, epoch: 1 | loss: 0.2430350\n",
      "\tspeed: 0.0978s/iter; left time: 871.8664s\n",
      "\titers: 600, epoch: 1 | loss: 0.1929865\n",
      "\tspeed: 0.0979s/iter; left time: 862.9822s\n",
      "\titers: 700, epoch: 1 | loss: 0.2642112\n",
      "\tspeed: 0.0978s/iter; left time: 851.8589s\n",
      "\titers: 800, epoch: 1 | loss: 0.2145494\n",
      "\tspeed: 0.0988s/iter; left time: 850.7168s\n",
      "\titers: 900, epoch: 1 | loss: 0.2589278\n",
      "\tspeed: 0.0979s/iter; left time: 833.5124s\n",
      "Epoch: 1 running time: 1.54476105372111 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2910090 Vali Loss: 0.1831906 Test Loss: 0.2378062\n",
      "Validation loss decreased (inf --> 0.183191).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1635738\n",
      "\tspeed: 0.2865s/iter; left time: 2397.9939s\n",
      "\titers: 200, epoch: 2 | loss: 0.1695694\n",
      "\tspeed: 0.0980s/iter; left time: 810.1318s\n",
      "\titers: 300, epoch: 2 | loss: 0.2019649\n",
      "\tspeed: 0.0977s/iter; left time: 798.4613s\n",
      "\titers: 400, epoch: 2 | loss: 0.1792056\n",
      "\tspeed: 0.0976s/iter; left time: 787.4753s\n",
      "\titers: 500, epoch: 2 | loss: 0.1652687\n",
      "\tspeed: 0.0977s/iter; left time: 778.5259s\n",
      "\titers: 600, epoch: 2 | loss: 0.2338050\n",
      "\tspeed: 0.0977s/iter; left time: 768.5250s\n",
      "\titers: 700, epoch: 2 | loss: 0.1457106\n",
      "\tspeed: 0.0978s/iter; left time: 760.1506s\n",
      "\titers: 800, epoch: 2 | loss: 0.2114065\n",
      "\tspeed: 0.0978s/iter; left time: 750.1098s\n",
      "\titers: 900, epoch: 2 | loss: 0.1232173\n",
      "\tspeed: 0.0976s/iter; left time: 738.8120s\n",
      "Epoch: 2 running time: 1.53813587029775 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1803521 Vali Loss: 0.1595277 Test Loss: 0.2245688\n",
      "Validation loss decreased (0.183191 --> 0.159528).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1655260\n",
      "\tspeed: 0.2837s/iter; left time: 2107.4144s\n",
      "\titers: 200, epoch: 3 | loss: 0.1260740\n",
      "\tspeed: 0.0977s/iter; left time: 715.9353s\n",
      "\titers: 300, epoch: 3 | loss: 0.1393616\n",
      "\tspeed: 0.0975s/iter; left time: 705.0829s\n",
      "\titers: 400, epoch: 3 | loss: 0.2075959\n",
      "\tspeed: 0.0978s/iter; left time: 697.2631s\n",
      "\titers: 500, epoch: 3 | loss: 0.1714134\n",
      "\tspeed: 0.0975s/iter; left time: 685.3577s\n",
      "\titers: 600, epoch: 3 | loss: 0.1982437\n",
      "\tspeed: 0.0977s/iter; left time: 676.8993s\n",
      "\titers: 700, epoch: 3 | loss: 0.1601679\n",
      "\tspeed: 0.0976s/iter; left time: 666.7277s\n",
      "\titers: 800, epoch: 3 | loss: 0.1601529\n",
      "\tspeed: 0.0981s/iter; left time: 659.8929s\n",
      "\titers: 900, epoch: 3 | loss: 0.1533434\n",
      "\tspeed: 0.0979s/iter; left time: 648.9327s\n",
      "Epoch: 3 running time: 1.5359195987383525 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1536249 Vali Loss: 0.1533358 Test Loss: 0.2164915\n",
      "Validation loss decreased (0.159528 --> 0.153336).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1581352\n",
      "\tspeed: 0.2850s/iter; left time: 1849.0780s\n",
      "\titers: 200, epoch: 4 | loss: 0.1934823\n",
      "\tspeed: 0.0979s/iter; left time: 625.3818s\n",
      "\titers: 300, epoch: 4 | loss: 0.1746560\n",
      "\tspeed: 0.0977s/iter; left time: 614.4730s\n",
      "\titers: 400, epoch: 4 | loss: 0.1177440\n",
      "\tspeed: 0.0978s/iter; left time: 605.3739s\n",
      "\titers: 500, epoch: 4 | loss: 0.1525689\n",
      "\tspeed: 0.0979s/iter; left time: 595.9054s\n",
      "\titers: 600, epoch: 4 | loss: 0.1305330\n",
      "\tspeed: 0.0981s/iter; left time: 587.4081s\n",
      "\titers: 700, epoch: 4 | loss: 0.1843030\n",
      "\tspeed: 0.0990s/iter; left time: 583.0748s\n",
      "\titers: 800, epoch: 4 | loss: 0.0997227\n",
      "\tspeed: 0.0983s/iter; left time: 569.1900s\n",
      "\titers: 900, epoch: 4 | loss: 0.1070071\n",
      "\tspeed: 0.0976s/iter; left time: 555.1758s\n",
      "Epoch: 4 running time: 1.542645013332367 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1371581 Vali Loss: 0.1579345 Test Loss: 0.2233839\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2039691\n",
      "\tspeed: 0.2791s/iter; left time: 1548.3095s\n",
      "\titers: 200, epoch: 5 | loss: 0.1556967\n",
      "\tspeed: 0.0974s/iter; left time: 530.3643s\n",
      "\titers: 300, epoch: 5 | loss: 0.0973489\n",
      "\tspeed: 0.0979s/iter; left time: 523.2324s\n",
      "\titers: 400, epoch: 5 | loss: 0.1531475\n",
      "\tspeed: 0.0978s/iter; left time: 513.4027s\n",
      "\titers: 500, epoch: 5 | loss: 0.1251926\n",
      "\tspeed: 0.0978s/iter; left time: 503.6290s\n",
      "\titers: 600, epoch: 5 | loss: 0.1437106\n",
      "\tspeed: 0.0979s/iter; left time: 493.9730s\n",
      "\titers: 700, epoch: 5 | loss: 0.1226156\n",
      "\tspeed: 0.0982s/iter; left time: 485.8401s\n",
      "\titers: 800, epoch: 5 | loss: 0.1796097\n",
      "\tspeed: 0.0983s/iter; left time: 476.2434s\n",
      "\titers: 900, epoch: 5 | loss: 0.1031976\n",
      "\tspeed: 0.0982s/iter; left time: 466.3618s\n",
      "Epoch: 5 running time: 1.5422874291737874 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1345451 Vali Loss: 0.1518466 Test Loss: 0.2202574\n",
      "Validation loss decreased (0.153336 --> 0.151847).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1445282\n",
      "\tspeed: 0.2845s/iter; left time: 1310.2221s\n",
      "\titers: 200, epoch: 6 | loss: 0.1143389\n",
      "\tspeed: 0.0977s/iter; left time: 440.1757s\n",
      "\titers: 300, epoch: 6 | loss: 0.1190768\n",
      "\tspeed: 0.0976s/iter; left time: 430.0741s\n",
      "\titers: 400, epoch: 6 | loss: 0.1352780\n",
      "\tspeed: 0.0976s/iter; left time: 420.4648s\n",
      "\titers: 500, epoch: 6 | loss: 0.1314371\n",
      "\tspeed: 0.0987s/iter; left time: 415.2133s\n",
      "\titers: 600, epoch: 6 | loss: 0.1066968\n",
      "\tspeed: 0.0980s/iter; left time: 402.2130s\n",
      "\titers: 700, epoch: 6 | loss: 0.1378544\n",
      "\tspeed: 0.0980s/iter; left time: 392.4402s\n",
      "\titers: 800, epoch: 6 | loss: 0.0896610\n",
      "\tspeed: 0.0980s/iter; left time: 382.8143s\n",
      "\titers: 900, epoch: 6 | loss: 0.1284886\n",
      "\tspeed: 0.0978s/iter; left time: 372.4146s\n",
      "Epoch: 6 running time: 1.542008380095164 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1270434 Vali Loss: 0.1534718 Test Loss: 0.2216068\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1149424\n",
      "\tspeed: 0.2793s/iter; left time: 1023.6423s\n",
      "\titers: 200, epoch: 7 | loss: 0.1310254\n",
      "\tspeed: 0.0971s/iter; left time: 346.2150s\n",
      "\titers: 300, epoch: 7 | loss: 0.1674199\n",
      "\tspeed: 0.0974s/iter; left time: 337.3509s\n",
      "\titers: 400, epoch: 7 | loss: 0.1332274\n",
      "\tspeed: 0.0974s/iter; left time: 327.8453s\n",
      "\titers: 500, epoch: 7 | loss: 0.0768914\n",
      "\tspeed: 0.0974s/iter; left time: 317.9397s\n",
      "\titers: 600, epoch: 7 | loss: 0.1472603\n",
      "\tspeed: 0.0971s/iter; left time: 307.4626s\n",
      "\titers: 700, epoch: 7 | loss: 0.1242801\n",
      "\tspeed: 0.0971s/iter; left time: 297.7126s\n",
      "\titers: 800, epoch: 7 | loss: 0.1110313\n",
      "\tspeed: 0.0971s/iter; left time: 287.8902s\n",
      "\titers: 900, epoch: 7 | loss: 0.1239939\n",
      "\tspeed: 0.0971s/iter; left time: 278.0831s\n",
      "Epoch: 7 running time: 1.5312985539436341 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1262259 Vali Loss: 0.1533842 Test Loss: 0.2228643\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1185199\n",
      "\tspeed: 0.2770s/iter; left time: 754.5685s\n",
      "\titers: 200, epoch: 8 | loss: 0.1375734\n",
      "\tspeed: 0.0974s/iter; left time: 255.5432s\n",
      "\titers: 300, epoch: 8 | loss: 0.1466773\n",
      "\tspeed: 0.0973s/iter; left time: 245.7113s\n",
      "\titers: 400, epoch: 8 | loss: 0.1268446\n",
      "\tspeed: 0.0973s/iter; left time: 235.9252s\n",
      "\titers: 500, epoch: 8 | loss: 0.1191575\n",
      "\tspeed: 0.0974s/iter; left time: 226.2573s\n",
      "\titers: 600, epoch: 8 | loss: 0.1166207\n",
      "\tspeed: 0.0973s/iter; left time: 216.3031s\n",
      "\titers: 700, epoch: 8 | loss: 0.1127195\n",
      "\tspeed: 0.0975s/iter; left time: 207.0986s\n",
      "\titers: 800, epoch: 8 | loss: 0.1257366\n",
      "\tspeed: 0.0974s/iter; left time: 197.2305s\n",
      "\titers: 900, epoch: 8 | loss: 0.1132852\n",
      "\tspeed: 0.0976s/iter; left time: 187.7518s\n",
      "Epoch: 8 running time: 1.5325612545013427 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1262816 Vali Loss: 0.1506053 Test Loss: 0.2187933\n",
      "Validation loss decreased (0.151847 --> 0.150605).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1213355\n",
      "\tspeed: 0.2844s/iter; left time: 507.1089s\n",
      "\titers: 200, epoch: 9 | loss: 0.1153753\n",
      "\tspeed: 0.0977s/iter; left time: 164.4755s\n",
      "\titers: 300, epoch: 9 | loss: 0.1769007\n",
      "\tspeed: 0.0976s/iter; left time: 154.5795s\n",
      "\titers: 400, epoch: 9 | loss: 0.0931088\n",
      "\tspeed: 0.0972s/iter; left time: 144.2051s\n",
      "\titers: 500, epoch: 9 | loss: 0.1505617\n",
      "\tspeed: 0.0975s/iter; left time: 134.8129s\n",
      "\titers: 600, epoch: 9 | loss: 0.0891185\n",
      "\tspeed: 0.0975s/iter; left time: 125.0980s\n",
      "\titers: 700, epoch: 9 | loss: 0.1516637\n",
      "\tspeed: 0.0977s/iter; left time: 115.5484s\n",
      "\titers: 800, epoch: 9 | loss: 0.1149239\n",
      "\tspeed: 0.0976s/iter; left time: 105.6957s\n",
      "\titers: 900, epoch: 9 | loss: 0.1068046\n",
      "\tspeed: 0.0977s/iter; left time: 96.0086s\n",
      "Epoch: 9 running time: 1.5352671146392822 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1247482 Vali Loss: 0.1516108 Test Loss: 0.2206672\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1710355\n",
      "\tspeed: 0.2782s/iter; left time: 234.2491s\n",
      "\titers: 200, epoch: 10 | loss: 0.0977202\n",
      "\tspeed: 0.0977s/iter; left time: 72.5232s\n",
      "\titers: 300, epoch: 10 | loss: 0.1131416\n",
      "\tspeed: 0.0978s/iter; left time: 62.7592s\n",
      "\titers: 400, epoch: 10 | loss: 0.0992893\n",
      "\tspeed: 0.0978s/iter; left time: 52.9885s\n",
      "\titers: 500, epoch: 10 | loss: 0.1500777\n",
      "\tspeed: 0.0976s/iter; left time: 43.1232s\n",
      "\titers: 600, epoch: 10 | loss: 0.1034379\n",
      "\tspeed: 0.0977s/iter; left time: 33.4155s\n",
      "\titers: 700, epoch: 10 | loss: 0.1288431\n",
      "\tspeed: 0.0977s/iter; left time: 23.6381s\n",
      "\titers: 800, epoch: 10 | loss: 0.0905820\n",
      "\tspeed: 0.0976s/iter; left time: 13.8616s\n",
      "\titers: 900, epoch: 10 | loss: 0.1386539\n",
      "\tspeed: 0.0976s/iter; left time: 4.1007s\n",
      "Epoch: 10 running time: 1.5367359081904093 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1242712 Vali Loss: 0.1508001 Test Loss: 0.2190831\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 8617\n",
      "test shape: (269, 32, 24, 3) (269, 32, 24, 3)\n",
      "test shape: (8608, 24, 3) (8608, 24, 3)\n",
      "mse:0.21892176568508148, mae:0.2853127717971802\n",
      "\n",
      "Time intermediate for IT dataset: 34.16802665392558 min.\n",
      "Total time: 140.47173579136532 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A100 80 GB GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5892869\n",
      "\tspeed: 1.7740s/iter; left time: 16517.8096s\n",
      "\titers: 200, epoch: 1 | loss: 0.3933146\n",
      "\tspeed: 1.7834s/iter; left time: 16426.8272s\n",
      "\titers: 300, epoch: 1 | loss: 0.4908104\n",
      "\tspeed: 1.7627s/iter; left time: 16060.3907s\n",
      "\titers: 400, epoch: 1 | loss: 0.3265559\n",
      "\tspeed: 1.7188s/iter; left time: 15488.1032s\n",
      "\titers: 500, epoch: 1 | loss: 0.2909752\n",
      "\tspeed: 1.7761s/iter; left time: 15826.7083s\n",
      "\titers: 600, epoch: 1 | loss: 0.3200079\n",
      "\tspeed: 1.7850s/iter; left time: 15727.7167s\n",
      "\titers: 700, epoch: 1 | loss: 0.3656467\n",
      "\tspeed: 1.7830s/iter; left time: 15531.9936s\n",
      "\titers: 800, epoch: 1 | loss: 0.3098063\n",
      "\tspeed: 1.7816s/iter; left time: 15341.5524s\n",
      "\titers: 900, epoch: 1 | loss: 0.2568061\n",
      "\tspeed: 1.6853s/iter; left time: 14343.6843s\n",
      "Epoch: 1 running time: 27.634515269597372 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3881650 Vali Loss: 0.4417571 Test Loss: 0.5049031\n",
      "Validation loss decreased (inf --> 0.441757).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3426694\n",
      "\tspeed: 6.4784s/iter; left time: 54224.2063s\n",
      "\titers: 200, epoch: 2 | loss: 0.2399278\n",
      "\tspeed: 1.7739s/iter; left time: 14670.3334s\n",
      "\titers: 300, epoch: 2 | loss: 0.2567746\n",
      "\tspeed: 1.7714s/iter; left time: 14472.3325s\n",
      "\titers: 400, epoch: 2 | loss: 0.2278990\n",
      "\tspeed: 1.7610s/iter; left time: 14211.3144s\n",
      "\titers: 500, epoch: 2 | loss: 0.3125402\n",
      "\tspeed: 1.7496s/iter; left time: 13944.5195s\n",
      "\titers: 600, epoch: 2 | loss: 0.2258390\n",
      "\tspeed: 1.7046s/iter; left time: 13415.2120s\n",
      "\titers: 700, epoch: 2 | loss: 0.2849320\n",
      "\tspeed: 1.7012s/iter; left time: 13218.6742s\n",
      "\titers: 800, epoch: 2 | loss: 0.3336966\n",
      "\tspeed: 1.6323s/iter; left time: 12519.4820s\n",
      "\titers: 900, epoch: 2 | loss: 0.2731778\n",
      "\tspeed: 1.6584s/iter; left time: 12553.8093s\n",
      "Epoch: 2 running time: 26.984792792797087 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2710038 Vali Loss: 0.4523767 Test Loss: 0.5282342\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2522012\n",
      "\tspeed: 4.8176s/iter; left time: 35789.6374s\n",
      "\titers: 200, epoch: 3 | loss: 0.2502043\n",
      "\tspeed: 1.3412s/iter; left time: 9829.7210s\n",
      "\titers: 300, epoch: 3 | loss: 0.3234225\n",
      "\tspeed: 1.7360s/iter; left time: 12549.5617s\n",
      "\titers: 400, epoch: 3 | loss: 0.2362487\n",
      "\tspeed: 1.7909s/iter; left time: 12767.2286s\n",
      "\titers: 500, epoch: 3 | loss: 0.2309934\n",
      "\tspeed: 1.7932s/iter; left time: 12604.6409s\n",
      "\titers: 600, epoch: 3 | loss: 0.2739768\n",
      "\tspeed: 1.7936s/iter; left time: 12427.9019s\n",
      "\titers: 700, epoch: 3 | loss: 0.3025040\n",
      "\tspeed: 1.7927s/iter; left time: 12242.5609s\n",
      "\titers: 800, epoch: 3 | loss: 0.1998511\n",
      "\tspeed: 1.7792s/iter; left time: 11972.4464s\n",
      "\titers: 900, epoch: 3 | loss: 0.1695538\n",
      "\tspeed: 1.7912s/iter; left time: 11874.1508s\n",
      "Epoch: 3 running time: 26.104775579770408 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2621829 Vali Loss: 0.4351654 Test Loss: 0.5090449\n",
      "Validation loss decreased (0.441757 --> 0.435165).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2559014\n",
      "\tspeed: 6.4509s/iter; left time: 41853.6480s\n",
      "\titers: 200, epoch: 4 | loss: 0.3135229\n",
      "\tspeed: 1.7900s/iter; left time: 11434.6898s\n",
      "\titers: 300, epoch: 4 | loss: 0.2431349\n",
      "\tspeed: 1.7104s/iter; left time: 10755.2828s\n",
      "\titers: 400, epoch: 4 | loss: 0.1893651\n",
      "\tspeed: 1.7813s/iter; left time: 11022.4670s\n",
      "\titers: 500, epoch: 4 | loss: 0.2253976\n",
      "\tspeed: 1.7824s/iter; left time: 10851.3085s\n",
      "\titers: 600, epoch: 4 | loss: 0.1855248\n",
      "\tspeed: 1.7802s/iter; left time: 10659.6153s\n",
      "\titers: 700, epoch: 4 | loss: 0.2409668\n",
      "\tspeed: 1.7594s/iter; left time: 10359.2135s\n",
      "\titers: 800, epoch: 4 | loss: 0.1979460\n",
      "\tspeed: 1.7811s/iter; left time: 10308.8789s\n",
      "\titers: 900, epoch: 4 | loss: 0.2517775\n",
      "\tspeed: 1.7798s/iter; left time: 10123.7314s\n",
      "Epoch: 4 running time: 27.813729711373647 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2252403 Vali Loss: 0.4279542 Test Loss: 0.5073299\n",
      "Validation loss decreased (0.435165 --> 0.427954).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1416561\n",
      "\tspeed: 6.4838s/iter; left time: 35965.8712s\n",
      "\titers: 200, epoch: 5 | loss: 0.1728254\n",
      "\tspeed: 1.7797s/iter; left time: 9694.2236s\n",
      "\titers: 300, epoch: 5 | loss: 0.1700590\n",
      "\tspeed: 1.7675s/iter; left time: 9450.8093s\n",
      "\titers: 400, epoch: 5 | loss: 0.2203045\n",
      "\tspeed: 1.6914s/iter; left time: 8874.6419s\n",
      "\titers: 500, epoch: 5 | loss: 0.1987383\n",
      "\tspeed: 1.7258s/iter; left time: 8882.7904s\n",
      "\titers: 600, epoch: 5 | loss: 0.1874608\n",
      "\tspeed: 1.7476s/iter; left time: 8819.9565s\n",
      "\titers: 700, epoch: 5 | loss: 0.1910253\n",
      "\tspeed: 1.7768s/iter; left time: 8789.9132s\n",
      "\titers: 800, epoch: 5 | loss: 0.1339178\n",
      "\tspeed: 1.7646s/iter; left time: 8552.9812s\n",
      "\titers: 900, epoch: 5 | loss: 0.2224116\n",
      "\tspeed: 1.7729s/iter; left time: 8416.1086s\n",
      "Epoch: 5 running time: 27.31057822306951 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2015247 Vali Loss: 0.4473854 Test Loss: 0.5301394\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1897485\n",
      "\tspeed: 5.1661s/iter; left time: 23795.2181s\n",
      "\titers: 200, epoch: 6 | loss: 0.1521612\n",
      "\tspeed: 1.4302s/iter; left time: 6444.4513s\n",
      "\titers: 300, epoch: 6 | loss: 0.2534637\n",
      "\tspeed: 1.4333s/iter; left time: 6315.1067s\n",
      "\titers: 400, epoch: 6 | loss: 0.2329123\n",
      "\tspeed: 1.4297s/iter; left time: 6156.2153s\n",
      "\titers: 500, epoch: 6 | loss: 0.1705337\n",
      "\tspeed: 1.4257s/iter; left time: 5996.5011s\n",
      "\titers: 600, epoch: 6 | loss: 0.1758052\n",
      "\tspeed: 1.3979s/iter; left time: 5739.8332s\n",
      "\titers: 700, epoch: 6 | loss: 0.1472269\n",
      "\tspeed: 1.4176s/iter; left time: 5678.9607s\n",
      "\titers: 800, epoch: 6 | loss: 0.2156595\n",
      "\tspeed: 1.2984s/iter; left time: 5071.4561s\n",
      "\titers: 900, epoch: 6 | loss: 0.2027543\n",
      "\tspeed: 1.3351s/iter; left time: 5081.4661s\n",
      "Epoch: 6 running time: 21.879121748606362 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2005814 Vali Loss: 0.4429339 Test Loss: 0.5146396\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2896483\n",
      "\tspeed: 4.6987s/iter; left time: 17220.7879s\n",
      "\titers: 200, epoch: 7 | loss: 0.1951245\n",
      "\tspeed: 0.9930s/iter; left time: 3540.0434s\n",
      "\titers: 300, epoch: 7 | loss: 0.1956486\n",
      "\tspeed: 0.7265s/iter; left time: 2517.2155s\n",
      "\titers: 400, epoch: 7 | loss: 0.1279040\n",
      "\tspeed: 0.6539s/iter; left time: 2200.4047s\n",
      "\titers: 500, epoch: 7 | loss: 0.1855619\n",
      "\tspeed: 0.8751s/iter; left time: 2857.0585s\n",
      "\titers: 600, epoch: 7 | loss: 0.2258058\n",
      "\tspeed: 1.2685s/iter; left time: 4014.7927s\n",
      "\titers: 700, epoch: 7 | loss: 0.1987671\n",
      "\tspeed: 1.4336s/iter; left time: 4393.9192s\n",
      "\titers: 800, epoch: 7 | loss: 0.1772510\n",
      "\tspeed: 1.4381s/iter; left time: 4263.9319s\n",
      "\titers: 900, epoch: 7 | loss: 0.1790513\n",
      "\tspeed: 1.4330s/iter; left time: 4105.4028s\n",
      "Epoch: 7 running time: 17.652518121401467 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2002954 Vali Loss: 0.4401115 Test Loss: 0.5113256\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5103916525840759, mae:0.4799240231513977\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4982714\n",
      "\tspeed: 1.4374s/iter; left time: 13383.4126s\n",
      "\titers: 200, epoch: 1 | loss: 0.3455804\n",
      "\tspeed: 1.4423s/iter; left time: 13284.7779s\n",
      "\titers: 300, epoch: 1 | loss: 0.3005768\n",
      "\tspeed: 1.4395s/iter; left time: 13115.6823s\n",
      "\titers: 400, epoch: 1 | loss: 0.3961580\n",
      "\tspeed: 1.4217s/iter; left time: 12810.5044s\n",
      "\titers: 500, epoch: 1 | loss: 0.2840077\n",
      "\tspeed: 1.4397s/iter; left time: 12829.3174s\n",
      "\titers: 600, epoch: 1 | loss: 0.3224835\n",
      "\tspeed: 1.4440s/iter; left time: 12723.0401s\n",
      "\titers: 700, epoch: 1 | loss: 0.2960292\n",
      "\tspeed: 1.4416s/iter; left time: 12557.8911s\n",
      "\titers: 800, epoch: 1 | loss: 0.3862472\n",
      "\tspeed: 1.4420s/iter; left time: 12417.0894s\n",
      "\titers: 900, epoch: 1 | loss: 0.2340263\n",
      "\tspeed: 1.4331s/iter; left time: 12196.7282s\n",
      "Epoch: 1 running time: 22.5532741467158 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3891936 Vali Loss: 0.4420241 Test Loss: 0.5089546\n",
      "Validation loss decreased (inf --> 0.442024).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2936978\n",
      "\tspeed: 5.1865s/iter; left time: 43411.0637s\n",
      "\titers: 200, epoch: 2 | loss: 0.3230321\n",
      "\tspeed: 1.4344s/iter; left time: 11862.6389s\n",
      "\titers: 300, epoch: 2 | loss: 0.3572863\n",
      "\tspeed: 1.4069s/iter; left time: 11494.4504s\n",
      "\titers: 400, epoch: 2 | loss: 0.2790028\n",
      "\tspeed: 1.3553s/iter; left time: 10937.2989s\n",
      "\titers: 500, epoch: 2 | loss: 0.2723592\n",
      "\tspeed: 1.4308s/iter; left time: 11403.2557s\n",
      "\titers: 600, epoch: 2 | loss: 0.2180605\n",
      "\tspeed: 1.4314s/iter; left time: 11264.9379s\n",
      "\titers: 700, epoch: 2 | loss: 0.3322076\n",
      "\tspeed: 1.4272s/iter; left time: 11089.5330s\n",
      "\titers: 800, epoch: 2 | loss: 0.2515522\n",
      "\tspeed: 1.4287s/iter; left time: 10958.1257s\n",
      "\titers: 900, epoch: 2 | loss: 0.2368346\n",
      "\tspeed: 1.4276s/iter; left time: 10807.0779s\n",
      "Epoch: 2 running time: 22.11847771803538 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2732664 Vali Loss: 0.4254902 Test Loss: 0.5132075\n",
      "Validation loss decreased (0.442024 --> 0.425490).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2119966\n",
      "\tspeed: 5.1525s/iter; left time: 38277.9960s\n",
      "\titers: 200, epoch: 3 | loss: 0.2830262\n",
      "\tspeed: 1.4251s/iter; left time: 10444.4019s\n",
      "\titers: 300, epoch: 3 | loss: 0.2171073\n",
      "\tspeed: 1.4268s/iter; left time: 10314.1490s\n",
      "\titers: 400, epoch: 3 | loss: 0.2491532\n",
      "\tspeed: 1.4281s/iter; left time: 10181.1824s\n",
      "\titers: 500, epoch: 3 | loss: 0.1985783\n",
      "\tspeed: 1.4274s/iter; left time: 10032.9357s\n",
      "\titers: 600, epoch: 3 | loss: 0.2014756\n",
      "\tspeed: 1.4308s/iter; left time: 9913.9760s\n",
      "\titers: 700, epoch: 3 | loss: 0.1760274\n",
      "\tspeed: 1.3566s/iter; left time: 9264.1872s\n",
      "\titers: 800, epoch: 3 | loss: 0.2013480\n",
      "\tspeed: 1.3355s/iter; left time: 8986.8703s\n",
      "\titers: 900, epoch: 3 | loss: 0.1793382\n",
      "\tspeed: 1.4094s/iter; left time: 9342.8138s\n",
      "Epoch: 3 running time: 22.066988996664683 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2239663 Vali Loss: 0.4408260 Test Loss: 0.5401742\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2301260\n",
      "\tspeed: 4.9796s/iter; left time: 32307.3978s\n",
      "\titers: 200, epoch: 4 | loss: 0.2951562\n",
      "\tspeed: 1.3245s/iter; left time: 8460.6221s\n",
      "\titers: 300, epoch: 4 | loss: 0.1593555\n",
      "\tspeed: 1.1704s/iter; left time: 7359.4767s\n",
      "\titers: 400, epoch: 4 | loss: 0.1884202\n",
      "\tspeed: 1.0732s/iter; left time: 6640.6912s\n",
      "\titers: 500, epoch: 4 | loss: 0.2468180\n",
      "\tspeed: 0.8089s/iter; left time: 4924.8729s\n",
      "\titers: 600, epoch: 4 | loss: 0.1581286\n",
      "\tspeed: 0.6471s/iter; left time: 3874.9299s\n",
      "\titers: 700, epoch: 4 | loss: 0.2391508\n",
      "\tspeed: 0.7650s/iter; left time: 4504.4719s\n",
      "\titers: 800, epoch: 4 | loss: 0.2061963\n",
      "\tspeed: 1.1186s/iter; left time: 6474.4536s\n",
      "\titers: 900, epoch: 4 | loss: 0.1761994\n",
      "\tspeed: 1.4355s/iter; left time: 8165.3420s\n",
      "Epoch: 4 running time: 17.1251859386762 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2176665 Vali Loss: 0.4362066 Test Loss: 0.5301881\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2694066\n",
      "\tspeed: 5.0994s/iter; left time: 28286.4630s\n",
      "\titers: 200, epoch: 5 | loss: 0.2159820\n",
      "\tspeed: 1.4300s/iter; left time: 7788.9937s\n",
      "\titers: 300, epoch: 5 | loss: 0.1976472\n",
      "\tspeed: 1.4326s/iter; left time: 7659.9370s\n",
      "\titers: 400, epoch: 5 | loss: 0.2887886\n",
      "\tspeed: 1.3876s/iter; left time: 7280.8094s\n",
      "\titers: 500, epoch: 5 | loss: 0.1834570\n",
      "\tspeed: 1.4207s/iter; left time: 7312.3769s\n",
      "\titers: 600, epoch: 5 | loss: 0.2080595\n",
      "\tspeed: 1.4274s/iter; left time: 7204.2006s\n",
      "\titers: 700, epoch: 5 | loss: 0.1690192\n",
      "\tspeed: 1.4278s/iter; left time: 7063.3950s\n",
      "\titers: 800, epoch: 5 | loss: 0.2367482\n",
      "\tspeed: 1.4282s/iter; left time: 6922.4804s\n",
      "\titers: 900, epoch: 5 | loss: 0.2473151\n",
      "\tspeed: 1.4323s/iter; left time: 6799.0906s\n",
      "Epoch: 5 running time: 22.21115117073059 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2170444 Vali Loss: 0.4308408 Test Loss: 0.5248260\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5258011817932129, mae:0.4902995228767395\n",
      "\n",
      "Time intermediate for DE dataset: 353.4190151294072 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4796472\n",
      "\tspeed: 1.3850s/iter; left time: 12895.7260s\n",
      "\titers: 200, epoch: 1 | loss: 0.3964097\n",
      "\tspeed: 1.3697s/iter; left time: 12616.1297s\n",
      "\titers: 300, epoch: 1 | loss: 0.3231561\n",
      "\tspeed: 1.3550s/iter; left time: 12345.3918s\n",
      "\titers: 400, epoch: 1 | loss: 0.3265139\n",
      "\tspeed: 1.2937s/iter; left time: 11657.9029s\n",
      "\titers: 500, epoch: 1 | loss: 0.2750012\n",
      "\tspeed: 1.3655s/iter; left time: 12167.8918s\n",
      "\titers: 600, epoch: 1 | loss: 0.3435578\n",
      "\tspeed: 1.3661s/iter; left time: 12036.9963s\n",
      "\titers: 700, epoch: 1 | loss: 0.4015486\n",
      "\tspeed: 1.3264s/iter; left time: 11554.4076s\n",
      "\titers: 800, epoch: 1 | loss: 0.2962549\n",
      "\tspeed: 1.3483s/iter; left time: 11610.1210s\n",
      "\titers: 900, epoch: 1 | loss: 0.2510418\n",
      "\tspeed: 1.3622s/iter; left time: 11593.7777s\n",
      "Epoch: 1 running time: 21.2065079053243 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4066807 Vali Loss: 0.4919056 Test Loss: 0.8280761\n",
      "Validation loss decreased (inf --> 0.491906).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2909266\n",
      "\tspeed: 5.0090s/iter; left time: 41925.1790s\n",
      "\titers: 200, epoch: 2 | loss: 0.3427289\n",
      "\tspeed: 1.3748s/iter; left time: 11369.3495s\n",
      "\titers: 300, epoch: 2 | loss: 0.3370485\n",
      "\tspeed: 1.3707s/iter; left time: 11198.6578s\n",
      "\titers: 400, epoch: 2 | loss: 0.3099507\n",
      "\tspeed: 1.3399s/iter; left time: 10813.0219s\n",
      "\titers: 500, epoch: 2 | loss: 0.2469848\n",
      "\tspeed: 1.3649s/iter; left time: 10878.1935s\n",
      "\titers: 600, epoch: 2 | loss: 0.2515435\n",
      "\tspeed: 1.3614s/iter; left time: 10714.1964s\n",
      "\titers: 700, epoch: 2 | loss: 0.2755833\n",
      "\tspeed: 1.3706s/iter; left time: 10649.5267s\n",
      "\titers: 800, epoch: 2 | loss: 0.2752435\n",
      "\tspeed: 1.2579s/iter; left time: 9648.1905s\n",
      "\titers: 900, epoch: 2 | loss: 0.2883866\n",
      "\tspeed: 1.3547s/iter; left time: 10254.8205s\n",
      "Epoch: 2 running time: 21.195697434743245 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3072687 Vali Loss: 0.4500027 Test Loss: 0.6647689\n",
      "Validation loss decreased (0.491906 --> 0.450003).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2362531\n",
      "\tspeed: 4.8455s/iter; left time: 35997.1274s\n",
      "\titers: 200, epoch: 3 | loss: 0.2749321\n",
      "\tspeed: 1.2853s/iter; left time: 9420.2433s\n",
      "\titers: 300, epoch: 3 | loss: 0.2563096\n",
      "\tspeed: 1.2758s/iter; left time: 9222.9034s\n",
      "\titers: 400, epoch: 3 | loss: 0.2283548\n",
      "\tspeed: 1.2296s/iter; left time: 8765.7035s\n",
      "\titers: 500, epoch: 3 | loss: 0.2578490\n",
      "\tspeed: 1.1192s/iter; left time: 7867.2008s\n",
      "\titers: 600, epoch: 3 | loss: 0.2866107\n",
      "\tspeed: 0.8677s/iter; left time: 6012.2398s\n",
      "\titers: 700, epoch: 3 | loss: 0.3263125\n",
      "\tspeed: 0.7238s/iter; left time: 4942.9240s\n",
      "\titers: 800, epoch: 3 | loss: 0.2137966\n",
      "\tspeed: 0.5863s/iter; left time: 3944.9171s\n",
      "\titers: 900, epoch: 3 | loss: 0.2546224\n",
      "\tspeed: 0.5636s/iter; left time: 3736.2682s\n",
      "Epoch: 3 running time: 15.401446159680685 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2680396 Vali Loss: 0.4645892 Test Loss: 0.6988297\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2277489\n",
      "\tspeed: 4.3137s/iter; left time: 27987.2562s\n",
      "\titers: 200, epoch: 4 | loss: 0.2136873\n",
      "\tspeed: 1.3829s/iter; left time: 8833.6807s\n",
      "\titers: 300, epoch: 4 | loss: 0.2673800\n",
      "\tspeed: 1.3828s/iter; left time: 8694.8421s\n",
      "\titers: 400, epoch: 4 | loss: 0.2411096\n",
      "\tspeed: 1.3816s/iter; left time: 8549.1769s\n",
      "\titers: 500, epoch: 4 | loss: 0.1858426\n",
      "\tspeed: 1.3775s/iter; left time: 8386.0030s\n",
      "\titers: 600, epoch: 4 | loss: 0.2128242\n",
      "\tspeed: 1.3820s/iter; left time: 8275.6342s\n",
      "\titers: 700, epoch: 4 | loss: 0.2819637\n",
      "\tspeed: 1.3834s/iter; left time: 8145.7501s\n",
      "\titers: 800, epoch: 4 | loss: 0.2102221\n",
      "\tspeed: 1.3786s/iter; left time: 7979.5622s\n",
      "\titers: 900, epoch: 4 | loss: 0.3691365\n",
      "\tspeed: 1.3702s/iter; left time: 7793.6622s\n",
      "Epoch: 4 running time: 21.51249199708303 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2621755 Vali Loss: 0.4662929 Test Loss: 0.7555156\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2223944\n",
      "\tspeed: 5.0266s/iter; left time: 27882.2882s\n",
      "\titers: 200, epoch: 5 | loss: 0.2555063\n",
      "\tspeed: 1.3755s/iter; left time: 7492.5291s\n",
      "\titers: 300, epoch: 5 | loss: 0.2554726\n",
      "\tspeed: 1.3860s/iter; left time: 7410.9994s\n",
      "\titers: 400, epoch: 5 | loss: 0.2304695\n",
      "\tspeed: 1.3777s/iter; left time: 7228.6914s\n",
      "\titers: 500, epoch: 5 | loss: 0.2484536\n",
      "\tspeed: 1.2556s/iter; left time: 6462.6218s\n",
      "\titers: 600, epoch: 5 | loss: 0.2365081\n",
      "\tspeed: 1.3428s/iter; left time: 6777.0384s\n",
      "\titers: 700, epoch: 5 | loss: 0.2740030\n",
      "\tspeed: 1.3757s/iter; left time: 6805.6870s\n",
      "\titers: 800, epoch: 5 | loss: 0.1907829\n",
      "\tspeed: 1.3714s/iter; left time: 6647.0590s\n",
      "\titers: 900, epoch: 5 | loss: 0.2470180\n",
      "\tspeed: 1.3584s/iter; left time: 6448.3293s\n",
      "Epoch: 5 running time: 21.294666449228924 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2611465 Vali Loss: 0.4532668 Test Loss: 0.7080034\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.7063973546028137, mae:0.5872204899787903\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5946396\n",
      "\tspeed: 1.3254s/iter; left time: 12341.0649s\n",
      "\titers: 200, epoch: 1 | loss: 0.3767517\n",
      "\tspeed: 1.3709s/iter; left time: 12627.2231s\n",
      "\titers: 300, epoch: 1 | loss: 0.2703782\n",
      "\tspeed: 1.3655s/iter; left time: 12440.9661s\n",
      "\titers: 400, epoch: 1 | loss: 0.4213276\n",
      "\tspeed: 1.3664s/iter; left time: 12312.9954s\n",
      "\titers: 500, epoch: 1 | loss: 0.3396932\n",
      "\tspeed: 1.3602s/iter; left time: 12120.4149s\n",
      "\titers: 600, epoch: 1 | loss: 0.3294225\n",
      "\tspeed: 1.3675s/iter; left time: 12048.6527s\n",
      "\titers: 700, epoch: 1 | loss: 0.3020035\n",
      "\tspeed: 1.3654s/iter; left time: 11893.8196s\n",
      "\titers: 800, epoch: 1 | loss: 0.3186351\n",
      "\tspeed: 1.3129s/iter; left time: 11305.5845s\n",
      "\titers: 900, epoch: 1 | loss: 0.3404082\n",
      "\tspeed: 1.3120s/iter; left time: 11166.5978s\n",
      "Epoch: 1 running time: 21.128375180562337 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4035210 Vali Loss: 0.4791251 Test Loss: 0.7368550\n",
      "Validation loss decreased (inf --> 0.479125).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2643960\n",
      "\tspeed: 4.8237s/iter; left time: 40373.9827s\n",
      "\titers: 200, epoch: 2 | loss: 0.3099206\n",
      "\tspeed: 1.2782s/iter; left time: 10570.8677s\n",
      "\titers: 300, epoch: 2 | loss: 0.3049880\n",
      "\tspeed: 1.2688s/iter; left time: 10366.2147s\n",
      "\titers: 400, epoch: 2 | loss: 0.2149809\n",
      "\tspeed: 1.2489s/iter; left time: 10078.6189s\n",
      "\titers: 500, epoch: 2 | loss: 0.3037942\n",
      "\tspeed: 1.2314s/iter; left time: 9814.3295s\n",
      "\titers: 600, epoch: 2 | loss: 0.2450974\n",
      "\tspeed: 1.1355s/iter; left time: 8936.2553s\n",
      "\titers: 700, epoch: 2 | loss: 0.2411249\n",
      "\tspeed: 1.1555s/iter; left time: 8978.2480s\n",
      "\titers: 800, epoch: 2 | loss: 0.2300154\n",
      "\tspeed: 1.0474s/iter; left time: 8033.7250s\n",
      "\titers: 900, epoch: 2 | loss: 0.2866468\n",
      "\tspeed: 0.7826s/iter; left time: 5924.1696s\n",
      "Epoch: 2 running time: 17.882394417126974 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3074028 Vali Loss: 0.4672675 Test Loss: 0.7471255\n",
      "Validation loss decreased (0.479125 --> 0.467268).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3160414\n",
      "\tspeed: 1.8532s/iter; left time: 13767.5335s\n",
      "\titers: 200, epoch: 3 | loss: 0.2468834\n",
      "\tspeed: 0.3005s/iter; left time: 2202.5809s\n",
      "\titers: 300, epoch: 3 | loss: 0.3383032\n",
      "\tspeed: 0.6520s/iter; left time: 4713.5842s\n",
      "\titers: 400, epoch: 3 | loss: 0.3059641\n",
      "\tspeed: 1.1035s/iter; left time: 7866.8764s\n",
      "\titers: 500, epoch: 3 | loss: 0.2890663\n",
      "\tspeed: 1.2825s/iter; left time: 9014.5395s\n",
      "\titers: 600, epoch: 3 | loss: 0.2565822\n",
      "\tspeed: 1.3146s/iter; left time: 9108.9164s\n",
      "\titers: 700, epoch: 3 | loss: 0.3086628\n",
      "\tspeed: 1.3144s/iter; left time: 8976.2766s\n",
      "\titers: 800, epoch: 3 | loss: 0.3262424\n",
      "\tspeed: 1.3154s/iter; left time: 8851.5350s\n",
      "\titers: 900, epoch: 3 | loss: 0.3235765\n",
      "\tspeed: 1.3168s/iter; left time: 8729.3705s\n",
      "Epoch: 3 running time: 15.939021106561025 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2682118 Vali Loss: 0.4870894 Test Loss: 0.8450049\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2983608\n",
      "\tspeed: 4.7959s/iter; left time: 31115.8646s\n",
      "\titers: 200, epoch: 4 | loss: 0.3798157\n",
      "\tspeed: 1.3158s/iter; left time: 8405.0878s\n",
      "\titers: 300, epoch: 4 | loss: 0.3646375\n",
      "\tspeed: 1.3164s/iter; left time: 8277.4508s\n",
      "\titers: 400, epoch: 4 | loss: 0.2232091\n",
      "\tspeed: 1.3138s/iter; left time: 8129.5373s\n",
      "\titers: 500, epoch: 4 | loss: 0.2237715\n",
      "\tspeed: 1.3118s/iter; left time: 7986.0461s\n",
      "\titers: 600, epoch: 4 | loss: 0.2513711\n",
      "\tspeed: 1.3124s/iter; left time: 7858.3846s\n",
      "\titers: 700, epoch: 4 | loss: 0.2766150\n",
      "\tspeed: 1.1828s/iter; left time: 6964.3137s\n",
      "\titers: 800, epoch: 4 | loss: 0.2482956\n",
      "\tspeed: 1.2756s/iter; left time: 7383.3370s\n",
      "\titers: 900, epoch: 4 | loss: 0.2322919\n",
      "\tspeed: 1.3148s/iter; left time: 7478.6640s\n",
      "Epoch: 4 running time: 20.341920590400697 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2625625 Vali Loss: 0.4763106 Test Loss: 0.8177143\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2406502\n",
      "\tspeed: 4.8073s/iter; left time: 26666.0044s\n",
      "\titers: 200, epoch: 5 | loss: 0.4271190\n",
      "\tspeed: 1.3183s/iter; left time: 7180.9562s\n",
      "\titers: 300, epoch: 5 | loss: 0.2466582\n",
      "\tspeed: 1.3101s/iter; left time: 7005.2303s\n",
      "\titers: 400, epoch: 5 | loss: 0.2363973\n",
      "\tspeed: 1.2632s/iter; left time: 6628.2263s\n",
      "\titers: 500, epoch: 5 | loss: 0.2911821\n",
      "\tspeed: 1.3385s/iter; left time: 6889.1402s\n",
      "\titers: 600, epoch: 5 | loss: 0.2240661\n",
      "\tspeed: 1.3298s/iter; left time: 6711.4679s\n",
      "\titers: 700, epoch: 5 | loss: 0.2569813\n",
      "\tspeed: 1.3120s/iter; left time: 6490.6207s\n",
      "\titers: 800, epoch: 5 | loss: 0.2432252\n",
      "\tspeed: 1.3067s/iter; left time: 6333.6801s\n",
      "\titers: 900, epoch: 5 | loss: 0.2644678\n",
      "\tspeed: 1.3071s/iter; left time: 6204.8726s\n",
      "Epoch: 5 running time: 20.55677490234375 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2632069 Vali Loss: 0.4643446 Test Loss: 0.7614479\n",
      "Validation loss decreased (0.467268 --> 0.464345).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2581221\n",
      "\tspeed: 4.6743s/iter; left time: 21529.6402s\n",
      "\titers: 200, epoch: 6 | loss: 0.2635585\n",
      "\tspeed: 1.2880s/iter; left time: 5803.5701s\n",
      "\titers: 300, epoch: 6 | loss: 0.2201428\n",
      "\tspeed: 1.3104s/iter; left time: 5773.6709s\n",
      "\titers: 400, epoch: 6 | loss: 0.2015900\n",
      "\tspeed: 1.3062s/iter; left time: 5624.6506s\n",
      "\titers: 500, epoch: 6 | loss: 0.2559985\n",
      "\tspeed: 1.3046s/iter; left time: 5486.9547s\n",
      "\titers: 600, epoch: 6 | loss: 0.2385457\n",
      "\tspeed: 1.3030s/iter; left time: 5350.3084s\n",
      "\titers: 700, epoch: 6 | loss: 0.2262333\n",
      "\tspeed: 1.3022s/iter; left time: 5216.7013s\n",
      "\titers: 800, epoch: 6 | loss: 0.2220343\n",
      "\tspeed: 1.2885s/iter; left time: 5033.0233s\n",
      "\titers: 900, epoch: 6 | loss: 0.2669659\n",
      "\tspeed: 1.2725s/iter; left time: 4843.0334s\n",
      "Epoch: 6 running time: 20.273427005608877 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2484723 Vali Loss: 0.4801593 Test Loss: 0.8024629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2513306\n",
      "\tspeed: 4.4382s/iter; left time: 16266.1361s\n",
      "\titers: 200, epoch: 7 | loss: 0.2128576\n",
      "\tspeed: 1.0449s/iter; left time: 3725.1048s\n",
      "\titers: 300, epoch: 7 | loss: 0.1917190\n",
      "\tspeed: 0.8012s/iter; left time: 2776.2266s\n",
      "\titers: 400, epoch: 7 | loss: 0.2231624\n",
      "\tspeed: 0.5754s/iter; left time: 1936.3859s\n",
      "\titers: 500, epoch: 7 | loss: 0.2302730\n",
      "\tspeed: 0.2980s/iter; left time: 972.9092s\n",
      "\titers: 600, epoch: 7 | loss: 0.2364664\n",
      "\tspeed: 0.3832s/iter; left time: 1212.7526s\n",
      "\titers: 700, epoch: 7 | loss: 0.2781757\n",
      "\tspeed: 0.4962s/iter; left time: 1520.7114s\n",
      "\titers: 800, epoch: 7 | loss: 0.3141681\n",
      "\tspeed: 0.8149s/iter; left time: 2416.1909s\n",
      "\titers: 900, epoch: 7 | loss: 0.2893362\n",
      "\tspeed: 0.9295s/iter; left time: 2662.9514s\n",
      "Epoch: 7 running time: 11.501748581727346 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.2479377 Vali Loss: 0.4753176 Test Loss: 0.7996016\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2381537\n",
      "\tspeed: 4.6481s/iter; left time: 12661.3142s\n",
      "\titers: 200, epoch: 8 | loss: 0.2409409\n",
      "\tspeed: 1.3186s/iter; left time: 3460.1084s\n",
      "\titers: 300, epoch: 8 | loss: 0.2478120\n",
      "\tspeed: 1.3241s/iter; left time: 3341.9916s\n",
      "\titers: 400, epoch: 8 | loss: 0.2283510\n",
      "\tspeed: 1.3163s/iter; left time: 3190.7831s\n",
      "\titers: 500, epoch: 8 | loss: 0.2755316\n",
      "\tspeed: 1.3161s/iter; left time: 3058.6641s\n",
      "\titers: 600, epoch: 8 | loss: 0.2858851\n",
      "\tspeed: 1.3153s/iter; left time: 2925.1764s\n",
      "\titers: 700, epoch: 8 | loss: 0.2509952\n",
      "\tspeed: 1.3124s/iter; left time: 2787.5540s\n",
      "\titers: 800, epoch: 8 | loss: 0.2344993\n",
      "\tspeed: 1.3167s/iter; left time: 2665.0554s\n",
      "\titers: 900, epoch: 8 | loss: 0.2150075\n",
      "\tspeed: 1.3123s/iter; left time: 2524.8438s\n",
      "Epoch: 8 running time: 20.60582449833552 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.2487196 Vali Loss: 0.4689870 Test Loss: 0.7732055\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.774609386920929, mae:0.616533637046814\n",
      "\n",
      "Time intermediate for GB dataset: 314.8255021095276 min.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     12\u001b[0m model_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--is_training\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--itr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m         ]\n\u001b[1;32m     35\u001b[0m int_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 37\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_run_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mrun_output\u001b[0;34m(path_to_run_file, model_arguments)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the script and capture the output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m---> 12\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there's any error in the process\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/val/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same but in Grünau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred_len 96 with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7559140\n",
      "\tspeed: 0.1492s/iter; left time: 1386.4834s\n",
      "\titers: 200, epoch: 1 | loss: 0.6601455\n",
      "\tspeed: 0.1277s/iter; left time: 1173.6310s\n",
      "\titers: 300, epoch: 1 | loss: 0.5010030\n",
      "\tspeed: 0.1271s/iter; left time: 1155.7700s\n",
      "\titers: 400, epoch: 1 | loss: 0.4657636\n",
      "\tspeed: 0.1261s/iter; left time: 1133.3460s\n",
      "\titers: 500, epoch: 1 | loss: 0.4198299\n",
      "\tspeed: 0.1291s/iter; left time: 1148.0713s\n",
      "\titers: 600, epoch: 1 | loss: 0.4128707\n",
      "\tspeed: 0.1288s/iter; left time: 1132.1373s\n",
      "\titers: 700, epoch: 1 | loss: 0.4657623\n",
      "\tspeed: 0.1282s/iter; left time: 1114.4254s\n",
      "\titers: 800, epoch: 1 | loss: 0.4991989\n",
      "\tspeed: 0.1250s/iter; left time: 1074.2327s\n",
      "\titers: 900, epoch: 1 | loss: 0.5064697\n",
      "\tspeed: 0.1251s/iter; left time: 1062.3588s\n",
      "Epoch: 1 running time: 2.0119425574938457 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5641891 Vali Loss: 0.7171257 Test Loss: 0.8211090\n",
      "Validation loss decreased (inf --> 0.717126).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5262482\n",
      "\tspeed: 0.3900s/iter; left time: 3257.4528s\n",
      "\titers: 200, epoch: 2 | loss: 0.4040740\n",
      "\tspeed: 0.1291s/iter; left time: 1065.4007s\n",
      "\titers: 300, epoch: 2 | loss: 0.4773462\n",
      "\tspeed: 0.1252s/iter; left time: 1020.8170s\n",
      "\titers: 400, epoch: 2 | loss: 0.6392567\n",
      "\tspeed: 0.1247s/iter; left time: 1004.0049s\n",
      "\titers: 500, epoch: 2 | loss: 0.3605611\n",
      "\tspeed: 0.1276s/iter; left time: 1014.6116s\n",
      "\titers: 600, epoch: 2 | loss: 0.5982773\n",
      "\tspeed: 0.1268s/iter; left time: 995.8242s\n",
      "\titers: 700, epoch: 2 | loss: 0.3476955\n",
      "\tspeed: 0.1253s/iter; left time: 971.4060s\n",
      "\titers: 800, epoch: 2 | loss: 0.3667593\n",
      "\tspeed: 0.1268s/iter; left time: 970.6022s\n",
      "\titers: 900, epoch: 2 | loss: 0.4465778\n",
      "\tspeed: 0.1258s/iter; left time: 950.0438s\n",
      "Epoch: 2 running time: 1.987606986363729 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4277616 Vali Loss: 0.7457622 Test Loss: 0.9326196\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3750877\n",
      "\tspeed: 0.3859s/iter; left time: 2860.8497s\n",
      "\titers: 200, epoch: 3 | loss: 0.4023382\n",
      "\tspeed: 0.1262s/iter; left time: 923.0680s\n",
      "\titers: 300, epoch: 3 | loss: 0.3713675\n",
      "\tspeed: 0.1264s/iter; left time: 911.9096s\n",
      "\titers: 400, epoch: 3 | loss: 0.3791743\n",
      "\tspeed: 0.1258s/iter; left time: 894.9106s\n",
      "\titers: 500, epoch: 3 | loss: 0.3428743\n",
      "\tspeed: 0.1291s/iter; left time: 905.0650s\n",
      "\titers: 600, epoch: 3 | loss: 0.3533941\n",
      "\tspeed: 0.1264s/iter; left time: 874.1252s\n",
      "\titers: 700, epoch: 3 | loss: 0.3791807\n",
      "\tspeed: 0.1264s/iter; left time: 861.1921s\n",
      "\titers: 800, epoch: 3 | loss: 0.2957656\n",
      "\tspeed: 0.1266s/iter; left time: 849.7204s\n",
      "\titers: 900, epoch: 3 | loss: 0.3607214\n",
      "\tspeed: 0.1252s/iter; left time: 828.1062s\n",
      "Epoch: 3 running time: 1.9883628408114116 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.3535496 Vali Loss: 0.7287154 Test Loss: 0.8563452\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3982290\n",
      "\tspeed: 0.3781s/iter; left time: 2447.5183s\n",
      "\titers: 200, epoch: 4 | loss: 0.3396333\n",
      "\tspeed: 0.1253s/iter; left time: 798.4858s\n",
      "\titers: 300, epoch: 4 | loss: 0.3390197\n",
      "\tspeed: 0.1275s/iter; left time: 800.1501s\n",
      "\titers: 400, epoch: 4 | loss: 0.3921471\n",
      "\tspeed: 0.1259s/iter; left time: 777.5382s\n",
      "\titers: 500, epoch: 4 | loss: 0.3249565\n",
      "\tspeed: 0.1267s/iter; left time: 769.6793s\n",
      "\titers: 600, epoch: 4 | loss: 0.3919300\n",
      "\tspeed: 0.1268s/iter; left time: 757.3070s\n",
      "\titers: 700, epoch: 4 | loss: 0.3157945\n",
      "\tspeed: 0.1262s/iter; left time: 741.5554s\n",
      "\titers: 800, epoch: 4 | loss: 0.3575296\n",
      "\tspeed: 0.1285s/iter; left time: 742.2068s\n",
      "\titers: 900, epoch: 4 | loss: 0.3127694\n",
      "\tspeed: 0.1272s/iter; left time: 722.0026s\n",
      "Epoch: 4 running time: 1.9902306199073792 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3488724 Vali Loss: 0.7305930 Test Loss: 0.8885055\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.887506365776062, mae:0.6491989493370056\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.8372460\n",
      "\tspeed: 0.1314s/iter; left time: 1220.9252s\n",
      "\titers: 200, epoch: 1 | loss: 0.6598187\n",
      "\tspeed: 0.1273s/iter; left time: 1170.1092s\n",
      "\titers: 300, epoch: 1 | loss: 0.5175543\n",
      "\tspeed: 0.1265s/iter; left time: 1150.3658s\n",
      "\titers: 400, epoch: 1 | loss: 0.7436482\n",
      "\tspeed: 0.1264s/iter; left time: 1136.5978s\n",
      "\titers: 500, epoch: 1 | loss: 0.5705448\n",
      "\tspeed: 0.1287s/iter; left time: 1144.5367s\n",
      "\titers: 600, epoch: 1 | loss: 0.5614384\n",
      "\tspeed: 0.1256s/iter; left time: 1104.3302s\n",
      "\titers: 700, epoch: 1 | loss: 0.5049872\n",
      "\tspeed: 0.1282s/iter; left time: 1113.8212s\n",
      "\titers: 800, epoch: 1 | loss: 0.3921492\n",
      "\tspeed: 0.1270s/iter; left time: 1090.8086s\n",
      "\titers: 900, epoch: 1 | loss: 0.4384058\n",
      "\tspeed: 0.1291s/iter; left time: 1096.0324s\n",
      "Epoch: 1 running time: 2.0033924341201783 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5718319 Vali Loss: 0.7627536 Test Loss: 0.8485546\n",
      "Validation loss decreased (inf --> 0.762754).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5203069\n",
      "\tspeed: 0.3987s/iter; left time: 3330.1639s\n",
      "\titers: 200, epoch: 2 | loss: 0.4105662\n",
      "\tspeed: 0.1264s/iter; left time: 1043.1099s\n",
      "\titers: 300, epoch: 2 | loss: 0.4338139\n",
      "\tspeed: 0.1286s/iter; left time: 1048.5558s\n",
      "\titers: 400, epoch: 2 | loss: 0.4623082\n",
      "\tspeed: 0.1259s/iter; left time: 1013.5631s\n",
      "\titers: 500, epoch: 2 | loss: 0.3925268\n",
      "\tspeed: 0.1264s/iter; left time: 1005.2824s\n",
      "\titers: 600, epoch: 2 | loss: 0.4094172\n",
      "\tspeed: 0.1240s/iter; left time: 973.4333s\n",
      "\titers: 700, epoch: 2 | loss: 0.4039551\n",
      "\tspeed: 0.1263s/iter; left time: 978.7249s\n",
      "\titers: 800, epoch: 2 | loss: 0.3210678\n",
      "\tspeed: 0.1266s/iter; left time: 968.9346s\n",
      "\titers: 900, epoch: 2 | loss: 0.3939592\n",
      "\tspeed: 0.1251s/iter; left time: 944.9481s\n",
      "Epoch: 2 running time: 1.9883704821268717 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4301373 Vali Loss: 0.6787714 Test Loss: 0.8458909\n",
      "Validation loss decreased (0.762754 --> 0.678771).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2911331\n",
      "\tspeed: 0.3946s/iter; left time: 2925.1690s\n",
      "\titers: 200, epoch: 3 | loss: 0.3410122\n",
      "\tspeed: 0.1271s/iter; left time: 929.2357s\n",
      "\titers: 300, epoch: 3 | loss: 0.2738642\n",
      "\tspeed: 0.1240s/iter; left time: 894.5246s\n",
      "\titers: 400, epoch: 3 | loss: 0.3245182\n",
      "\tspeed: 0.1275s/iter; left time: 906.5706s\n",
      "\titers: 500, epoch: 3 | loss: 0.3625443\n",
      "\tspeed: 0.1266s/iter; left time: 888.1261s\n",
      "\titers: 600, epoch: 3 | loss: 0.3125179\n",
      "\tspeed: 0.1272s/iter; left time: 879.4627s\n",
      "\titers: 700, epoch: 3 | loss: 0.3600514\n",
      "\tspeed: 0.1275s/iter; left time: 868.5463s\n",
      "\titers: 800, epoch: 3 | loss: 0.3778484\n",
      "\tspeed: 0.1284s/iter; left time: 861.7804s\n",
      "\titers: 900, epoch: 3 | loss: 0.2884048\n",
      "\tspeed: 0.1274s/iter; left time: 842.8091s\n",
      "Epoch: 3 running time: 1.9978508949279785 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.3478397 Vali Loss: 0.7336287 Test Loss: 0.8506453\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3081308\n",
      "\tspeed: 0.3805s/iter; left time: 2463.0732s\n",
      "\titers: 200, epoch: 4 | loss: 0.3963213\n",
      "\tspeed: 0.1249s/iter; left time: 795.9377s\n",
      "\titers: 300, epoch: 4 | loss: 0.3623208\n",
      "\tspeed: 0.1266s/iter; left time: 794.5500s\n",
      "\titers: 400, epoch: 4 | loss: 0.3253232\n",
      "\tspeed: 0.1270s/iter; left time: 783.9647s\n",
      "\titers: 500, epoch: 4 | loss: 0.4154426\n",
      "\tspeed: 0.1279s/iter; left time: 776.7806s\n",
      "\titers: 600, epoch: 4 | loss: 0.2958643\n",
      "\tspeed: 0.1283s/iter; left time: 766.6183s\n",
      "\titers: 700, epoch: 4 | loss: 0.3202910\n",
      "\tspeed: 0.1279s/iter; left time: 751.4469s\n",
      "\titers: 800, epoch: 4 | loss: 0.2845508\n",
      "\tspeed: 0.1272s/iter; left time: 734.2735s\n",
      "\titers: 900, epoch: 4 | loss: 0.2924925\n",
      "\tspeed: 0.1276s/iter; left time: 723.8593s\n",
      "Epoch: 4 running time: 1.9958333690961203 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3432350 Vali Loss: 0.7004285 Test Loss: 0.8710135\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3374025\n",
      "\tspeed: 0.3782s/iter; left time: 2093.4315s\n",
      "\titers: 200, epoch: 5 | loss: 0.3468058\n",
      "\tspeed: 0.1264s/iter; left time: 687.0523s\n",
      "\titers: 300, epoch: 5 | loss: 0.4369173\n",
      "\tspeed: 0.1278s/iter; left time: 682.0398s\n",
      "\titers: 400, epoch: 5 | loss: 0.3151688\n",
      "\tspeed: 0.1257s/iter; left time: 658.0620s\n",
      "\titers: 500, epoch: 5 | loss: 0.3519539\n",
      "\tspeed: 0.1259s/iter; left time: 646.2479s\n",
      "\titers: 600, epoch: 5 | loss: 0.3129667\n",
      "\tspeed: 0.1250s/iter; left time: 629.3492s\n",
      "\titers: 700, epoch: 5 | loss: 0.2937752\n",
      "\tspeed: 0.1260s/iter; left time: 621.6421s\n",
      "\titers: 800, epoch: 5 | loss: 0.3253865\n",
      "\tspeed: 0.1245s/iter; left time: 601.7381s\n",
      "\titers: 900, epoch: 5 | loss: 0.3888110\n",
      "\tspeed: 0.1266s/iter; left time: 599.2676s\n",
      "Epoch: 5 running time: 1.9796027461687724 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.3461462 Vali Loss: 0.7217131 Test Loss: 0.8637958\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8654274940490723, mae:0.6541482210159302\n",
      "\n",
      "Time intermediate for DE dataset: 21.604037141799928 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.8083605\n",
      "\tspeed: 0.1462s/iter; left time: 1358.0330s\n",
      "\titers: 200, epoch: 1 | loss: 0.6042727\n",
      "\tspeed: 0.1261s/iter; left time: 1159.0579s\n",
      "\titers: 300, epoch: 1 | loss: 0.5880392\n",
      "\tspeed: 0.1265s/iter; left time: 1150.3364s\n",
      "\titers: 400, epoch: 1 | loss: 0.5434972\n",
      "\tspeed: 0.1266s/iter; left time: 1137.8769s\n",
      "\titers: 500, epoch: 1 | loss: 0.5033150\n",
      "\tspeed: 0.1259s/iter; left time: 1119.0417s\n",
      "\titers: 600, epoch: 1 | loss: 0.4393446\n",
      "\tspeed: 0.1266s/iter; left time: 1113.0018s\n",
      "\titers: 700, epoch: 1 | loss: 0.5264087\n",
      "\tspeed: 0.1265s/iter; left time: 1099.4940s\n",
      "\titers: 800, epoch: 1 | loss: 0.5861061\n",
      "\tspeed: 0.1279s/iter; left time: 1099.1363s\n",
      "\titers: 900, epoch: 1 | loss: 0.4890957\n",
      "\tspeed: 0.1256s/iter; left time: 1066.7729s\n",
      "Epoch: 1 running time: 1.9975093960762025 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5769552 Vali Loss: 0.7088629 Test Loss: 1.0980847\n",
      "Validation loss decreased (inf --> 0.708863).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5545461\n",
      "\tspeed: 0.3960s/iter; left time: 3307.5801s\n",
      "\titers: 200, epoch: 2 | loss: 0.4281227\n",
      "\tspeed: 0.1265s/iter; left time: 1043.5908s\n",
      "\titers: 300, epoch: 2 | loss: 0.4443220\n",
      "\tspeed: 0.1256s/iter; left time: 1023.8438s\n",
      "\titers: 400, epoch: 2 | loss: 0.5764625\n",
      "\tspeed: 0.1278s/iter; left time: 1029.4481s\n",
      "\titers: 500, epoch: 2 | loss: 0.4205113\n",
      "\tspeed: 0.1268s/iter; left time: 1008.1749s\n",
      "\titers: 600, epoch: 2 | loss: 0.4880638\n",
      "\tspeed: 0.1265s/iter; left time: 993.5271s\n",
      "\titers: 700, epoch: 2 | loss: 0.4213868\n",
      "\tspeed: 0.1268s/iter; left time: 983.1635s\n",
      "\titers: 800, epoch: 2 | loss: 0.4026201\n",
      "\tspeed: 0.1258s/iter; left time: 962.7035s\n",
      "\titers: 900, epoch: 2 | loss: 0.4723349\n",
      "\tspeed: 0.1265s/iter; left time: 955.2459s\n",
      "Epoch: 2 running time: 1.9918944875399271 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4428352 Vali Loss: 0.7239694 Test Loss: 1.2943285\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4480673\n",
      "\tspeed: 0.3842s/iter; left time: 2848.0453s\n",
      "\titers: 200, epoch: 3 | loss: 0.4947310\n",
      "\tspeed: 0.1265s/iter; left time: 925.3873s\n",
      "\titers: 300, epoch: 3 | loss: 0.4939314\n",
      "\tspeed: 0.1272s/iter; left time: 917.7620s\n",
      "\titers: 400, epoch: 3 | loss: 0.4132645\n",
      "\tspeed: 0.1274s/iter; left time: 905.9913s\n",
      "\titers: 500, epoch: 3 | loss: 0.4019790\n",
      "\tspeed: 0.1268s/iter; left time: 889.5070s\n",
      "\titers: 600, epoch: 3 | loss: 0.3910426\n",
      "\tspeed: 0.1256s/iter; left time: 867.9940s\n",
      "\titers: 700, epoch: 3 | loss: 0.4234212\n",
      "\tspeed: 0.1268s/iter; left time: 864.1959s\n",
      "\titers: 800, epoch: 3 | loss: 0.4154332\n",
      "\tspeed: 0.1266s/iter; left time: 849.6952s\n",
      "\titers: 900, epoch: 3 | loss: 0.4049415\n",
      "\tspeed: 0.1269s/iter; left time: 839.1047s\n",
      "Epoch: 3 running time: 1.9915094017982482 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.4344580 Vali Loss: 0.7482555 Test Loss: 1.1842282\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.4369820\n",
      "\tspeed: 0.3822s/iter; left time: 2474.1045s\n",
      "\titers: 200, epoch: 4 | loss: 0.4659848\n",
      "\tspeed: 0.1266s/iter; left time: 807.1518s\n",
      "\titers: 300, epoch: 4 | loss: 0.4069278\n",
      "\tspeed: 0.1276s/iter; left time: 800.2751s\n",
      "\titers: 400, epoch: 4 | loss: 0.4918749\n",
      "\tspeed: 0.1265s/iter; left time: 781.1488s\n",
      "\titers: 500, epoch: 4 | loss: 0.4460267\n",
      "\tspeed: 0.1273s/iter; left time: 772.9558s\n",
      "\titers: 600, epoch: 4 | loss: 0.4548974\n",
      "\tspeed: 0.1250s/iter; left time: 746.5060s\n",
      "\titers: 700, epoch: 4 | loss: 0.3654212\n",
      "\tspeed: 0.1292s/iter; left time: 758.6998s\n",
      "\titers: 800, epoch: 4 | loss: 0.3909819\n",
      "\tspeed: 0.1263s/iter; left time: 729.1202s\n",
      "\titers: 900, epoch: 4 | loss: 0.4895321\n",
      "\tspeed: 0.1268s/iter; left time: 719.3560s\n",
      "Epoch: 4 running time: 1.9910707195599875 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.4323684 Vali Loss: 0.7605929 Test Loss: 1.1509625\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:1.151376724243164, mae:0.7609724402427673\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7174057\n",
      "\tspeed: 0.1332s/iter; left time: 1237.2810s\n",
      "\titers: 200, epoch: 1 | loss: 0.5984694\n",
      "\tspeed: 0.1273s/iter; left time: 1169.5639s\n",
      "\titers: 300, epoch: 1 | loss: 0.5788845\n",
      "\tspeed: 0.1269s/iter; left time: 1153.2415s\n",
      "\titers: 400, epoch: 1 | loss: 0.6282368\n",
      "\tspeed: 0.1280s/iter; left time: 1150.7768s\n",
      "\titers: 500, epoch: 1 | loss: 0.5255317\n",
      "\tspeed: 0.1252s/iter; left time: 1113.4211s\n",
      "\titers: 600, epoch: 1 | loss: 0.4614025\n",
      "\tspeed: 0.1278s/iter; left time: 1123.0704s\n",
      "\titers: 700, epoch: 1 | loss: 0.4846911\n",
      "\tspeed: 0.1274s/iter; left time: 1107.4821s\n",
      "\titers: 800, epoch: 1 | loss: 0.4044934\n",
      "\tspeed: 0.1254s/iter; left time: 1077.1423s\n",
      "\titers: 900, epoch: 1 | loss: 0.4881596\n",
      "\tspeed: 0.1277s/iter; left time: 1084.6862s\n",
      "Epoch: 1 running time: 2.0021843473116556 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5708030 Vali Loss: 0.7525572 Test Loss: 1.1455549\n",
      "Validation loss decreased (inf --> 0.752557).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4793997\n",
      "\tspeed: 0.3930s/iter; left time: 3282.1535s\n",
      "\titers: 200, epoch: 2 | loss: 0.4411435\n",
      "\tspeed: 0.1280s/iter; left time: 1056.4404s\n",
      "\titers: 300, epoch: 2 | loss: 0.4005803\n",
      "\tspeed: 0.1274s/iter; left time: 1038.3622s\n",
      "\titers: 400, epoch: 2 | loss: 0.4814230\n",
      "\tspeed: 0.1281s/iter; left time: 1031.1633s\n",
      "\titers: 500, epoch: 2 | loss: 0.3774332\n",
      "\tspeed: 0.1272s/iter; left time: 1011.4161s\n",
      "\titers: 600, epoch: 2 | loss: 0.4242199\n",
      "\tspeed: 0.1270s/iter; left time: 997.5623s\n",
      "\titers: 700, epoch: 2 | loss: 0.4064912\n",
      "\tspeed: 0.1263s/iter; left time: 979.0686s\n",
      "\titers: 800, epoch: 2 | loss: 0.4377633\n",
      "\tspeed: 0.1265s/iter; left time: 967.7488s\n",
      "\titers: 900, epoch: 2 | loss: 0.4441836\n",
      "\tspeed: 0.1263s/iter; left time: 953.4919s\n",
      "Epoch: 2 running time: 1.9958159963289896 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4470905 Vali Loss: 0.7571042 Test Loss: 1.3562176\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4385988\n",
      "\tspeed: 0.3874s/iter; left time: 2871.5610s\n",
      "\titers: 200, epoch: 3 | loss: 0.4950843\n",
      "\tspeed: 0.1268s/iter; left time: 927.6248s\n",
      "\titers: 300, epoch: 3 | loss: 0.3735984\n",
      "\tspeed: 0.1276s/iter; left time: 920.1304s\n",
      "\titers: 400, epoch: 3 | loss: 0.4782867\n",
      "\tspeed: 0.1253s/iter; left time: 891.5770s\n",
      "\titers: 500, epoch: 3 | loss: 0.4981522\n",
      "\tspeed: 0.1267s/iter; left time: 888.2131s\n",
      "\titers: 600, epoch: 3 | loss: 0.4281476\n",
      "\tspeed: 0.1283s/iter; left time: 886.7698s\n",
      "\titers: 700, epoch: 3 | loss: 0.4090459\n",
      "\tspeed: 0.1263s/iter; left time: 860.7896s\n",
      "\titers: 800, epoch: 3 | loss: 0.4218008\n",
      "\tspeed: 0.1267s/iter; left time: 850.6036s\n",
      "\titers: 900, epoch: 3 | loss: 0.4452266\n",
      "\tspeed: 0.1271s/iter; left time: 840.2716s\n",
      "Epoch: 3 running time: 1.9939235687255858 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.4388648 Vali Loss: 0.7479808 Test Loss: 1.2977123\n",
      "Validation loss decreased (0.752557 --> 0.747981).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3496612\n",
      "\tspeed: 0.3955s/iter; left time: 2560.7612s\n",
      "\titers: 200, epoch: 4 | loss: 0.3880953\n",
      "\tspeed: 0.1276s/iter; left time: 813.0040s\n",
      "\titers: 300, epoch: 4 | loss: 0.4699698\n",
      "\tspeed: 0.1292s/iter; left time: 810.7699s\n",
      "\titers: 400, epoch: 4 | loss: 0.4131892\n",
      "\tspeed: 0.1259s/iter; left time: 777.4706s\n",
      "\titers: 500, epoch: 4 | loss: 0.3588521\n",
      "\tspeed: 0.1289s/iter; left time: 782.8093s\n",
      "\titers: 600, epoch: 4 | loss: 0.3679525\n",
      "\tspeed: 0.1291s/iter; left time: 771.3484s\n",
      "\titers: 700, epoch: 4 | loss: 0.4040492\n",
      "\tspeed: 0.1277s/iter; left time: 750.2195s\n",
      "\titers: 800, epoch: 4 | loss: 0.3158428\n",
      "\tspeed: 0.1255s/iter; left time: 724.4276s\n",
      "\titers: 900, epoch: 4 | loss: 0.3549581\n",
      "\tspeed: 0.1270s/iter; left time: 720.5176s\n",
      "Epoch: 4 running time: 2.003739647070567 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3855216 Vali Loss: 0.8231856 Test Loss: 1.4624581\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4242773\n",
      "\tspeed: 0.3895s/iter; left time: 2155.6402s\n",
      "\titers: 200, epoch: 5 | loss: 0.3582258\n",
      "\tspeed: 0.1269s/iter; left time: 689.9527s\n",
      "\titers: 300, epoch: 5 | loss: 0.4846379\n",
      "\tspeed: 0.1273s/iter; left time: 679.0929s\n",
      "\titers: 400, epoch: 5 | loss: 0.3459863\n",
      "\tspeed: 0.1268s/iter; left time: 664.0235s\n",
      "\titers: 500, epoch: 5 | loss: 0.3713422\n",
      "\tspeed: 0.1271s/iter; left time: 652.6475s\n",
      "\titers: 600, epoch: 5 | loss: 0.3948859\n",
      "\tspeed: 0.1266s/iter; left time: 637.1909s\n",
      "\titers: 700, epoch: 5 | loss: 0.3836330\n",
      "\tspeed: 0.1269s/iter; left time: 626.1019s\n",
      "\titers: 800, epoch: 5 | loss: 0.3721221\n",
      "\tspeed: 0.1270s/iter; left time: 614.0005s\n",
      "\titers: 900, epoch: 5 | loss: 0.2794088\n",
      "\tspeed: 0.1266s/iter; left time: 599.6780s\n",
      "Epoch: 5 running time: 1.996334143479665 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.3837905 Vali Loss: 0.7847434 Test Loss: 1.3578115\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.4093121\n",
      "\tspeed: 0.3841s/iter; left time: 1765.2766s\n",
      "\titers: 200, epoch: 6 | loss: 0.3623025\n",
      "\tspeed: 0.1256s/iter; left time: 564.8219s\n",
      "\titers: 300, epoch: 6 | loss: 0.4597489\n",
      "\tspeed: 0.1256s/iter; left time: 552.0163s\n",
      "\titers: 400, epoch: 6 | loss: 0.3277133\n",
      "\tspeed: 0.1269s/iter; left time: 545.3150s\n",
      "\titers: 500, epoch: 6 | loss: 0.4091626\n",
      "\tspeed: 0.1277s/iter; left time: 535.8802s\n",
      "\titers: 600, epoch: 6 | loss: 0.3725473\n",
      "\tspeed: 0.1279s/iter; left time: 524.0640s\n",
      "\titers: 700, epoch: 6 | loss: 0.3676435\n",
      "\tspeed: 0.1273s/iter; left time: 508.6488s\n",
      "\titers: 800, epoch: 6 | loss: 0.3754401\n",
      "\tspeed: 0.1258s/iter; left time: 490.2983s\n",
      "\titers: 900, epoch: 6 | loss: 0.4071269\n",
      "\tspeed: 0.1298s/iter; left time: 492.8995s\n",
      "Epoch: 6 running time: 1.9948103229204814 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.3849790 Vali Loss: 0.7940353 Test Loss: 1.3383511\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:1.339471459388733, mae:0.8141219615936279\n",
      "\n",
      "Time intermediate for GB dataset: 24.022459920247396 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6535346\n",
      "\tspeed: 0.1485s/iter; left time: 1379.6024s\n",
      "\titers: 200, epoch: 1 | loss: 0.4544106\n",
      "\tspeed: 0.1270s/iter; left time: 1167.3953s\n",
      "\titers: 300, epoch: 1 | loss: 0.3685518\n",
      "\tspeed: 0.1263s/iter; left time: 1148.2932s\n",
      "\titers: 400, epoch: 1 | loss: 0.4164305\n",
      "\tspeed: 0.1259s/iter; left time: 1131.6016s\n",
      "\titers: 500, epoch: 1 | loss: 0.3425169\n",
      "\tspeed: 0.1270s/iter; left time: 1128.8789s\n",
      "\titers: 600, epoch: 1 | loss: 0.3266223\n",
      "\tspeed: 0.1273s/iter; left time: 1119.4594s\n",
      "\titers: 700, epoch: 1 | loss: 0.3547635\n",
      "\tspeed: 0.1270s/iter; left time: 1103.6496s\n",
      "\titers: 800, epoch: 1 | loss: 0.3798216\n",
      "\tspeed: 0.1254s/iter; left time: 1077.7014s\n",
      "\titers: 900, epoch: 1 | loss: 0.3751050\n",
      "\tspeed: 0.1272s/iter; left time: 1080.0062s\n",
      "Epoch: 1 running time: 2.0002546628316242 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4262612 Vali Loss: 0.4088405 Test Loss: 0.7468417\n",
      "Validation loss decreased (inf --> 0.408841).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3060112\n",
      "\tspeed: 0.3893s/iter; left time: 3251.8226s\n",
      "\titers: 200, epoch: 2 | loss: 0.3075564\n",
      "\tspeed: 0.1275s/iter; left time: 1051.9450s\n",
      "\titers: 300, epoch: 2 | loss: 0.3118545\n",
      "\tspeed: 0.1276s/iter; left time: 1040.0539s\n",
      "\titers: 400, epoch: 2 | loss: 0.3302422\n",
      "\tspeed: 0.1266s/iter; left time: 1019.2042s\n",
      "\titers: 500, epoch: 2 | loss: 0.2575973\n",
      "\tspeed: 0.1260s/iter; left time: 1001.6273s\n",
      "\titers: 600, epoch: 2 | loss: 0.3559574\n",
      "\tspeed: 0.1252s/iter; left time: 983.2460s\n",
      "\titers: 700, epoch: 2 | loss: 0.3002410\n",
      "\tspeed: 0.1269s/iter; left time: 984.0249s\n",
      "\titers: 800, epoch: 2 | loss: 0.2893221\n",
      "\tspeed: 0.1266s/iter; left time: 968.4949s\n",
      "\titers: 900, epoch: 2 | loss: 0.3215189\n",
      "\tspeed: 0.1282s/iter; left time: 968.2511s\n",
      "Epoch: 2 running time: 1.9908961653709412 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.3083065 Vali Loss: 0.4429979 Test Loss: 0.8032006\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3308239\n",
      "\tspeed: 0.3799s/iter; left time: 2815.9377s\n",
      "\titers: 200, epoch: 3 | loss: 0.2796704\n",
      "\tspeed: 0.1261s/iter; left time: 922.0830s\n",
      "\titers: 300, epoch: 3 | loss: 0.2400482\n",
      "\tspeed: 0.1260s/iter; left time: 908.5829s\n",
      "\titers: 400, epoch: 3 | loss: 0.3041089\n",
      "\tspeed: 0.1261s/iter; left time: 896.6196s\n",
      "\titers: 500, epoch: 3 | loss: 0.2837020\n",
      "\tspeed: 0.1264s/iter; left time: 886.3996s\n",
      "\titers: 600, epoch: 3 | loss: 0.2644754\n",
      "\tspeed: 0.1247s/iter; left time: 861.8565s\n",
      "\titers: 700, epoch: 3 | loss: 0.3089007\n",
      "\tspeed: 0.1265s/iter; left time: 861.9684s\n",
      "\titers: 800, epoch: 3 | loss: 0.2851648\n",
      "\tspeed: 0.1263s/iter; left time: 847.8454s\n",
      "\titers: 900, epoch: 3 | loss: 0.3176130\n",
      "\tspeed: 0.1264s/iter; left time: 836.0363s\n",
      "Epoch: 3 running time: 1.9773066759109497 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.3012448 Vali Loss: 0.4048612 Test Loss: 0.8435549\n",
      "Validation loss decreased (0.408841 --> 0.404861).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3126360\n",
      "\tspeed: 0.3835s/iter; left time: 2482.8402s\n",
      "\titers: 200, epoch: 4 | loss: 0.2761378\n",
      "\tspeed: 0.1278s/iter; left time: 814.7433s\n",
      "\titers: 300, epoch: 4 | loss: 0.2534401\n",
      "\tspeed: 0.1261s/iter; left time: 791.0251s\n",
      "\titers: 400, epoch: 4 | loss: 0.2833184\n",
      "\tspeed: 0.1260s/iter; left time: 777.7014s\n",
      "\titers: 500, epoch: 4 | loss: 0.2574418\n",
      "\tspeed: 0.1260s/iter; left time: 765.3749s\n",
      "\titers: 600, epoch: 4 | loss: 0.3066044\n",
      "\tspeed: 0.1241s/iter; left time: 741.5695s\n",
      "\titers: 700, epoch: 4 | loss: 0.2388677\n",
      "\tspeed: 0.1272s/iter; left time: 747.2247s\n",
      "\titers: 800, epoch: 4 | loss: 0.3113361\n",
      "\tspeed: 0.1268s/iter; left time: 732.0291s\n",
      "\titers: 900, epoch: 4 | loss: 0.2502302\n",
      "\tspeed: 0.1266s/iter; left time: 718.2482s\n",
      "Epoch: 4 running time: 1.9817967375119527 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2669175 Vali Loss: 0.4331120 Test Loss: 1.0065913\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2234550\n",
      "\tspeed: 0.3774s/iter; left time: 2089.0727s\n",
      "\titers: 200, epoch: 5 | loss: 0.2504027\n",
      "\tspeed: 0.1243s/iter; left time: 675.3247s\n",
      "\titers: 300, epoch: 5 | loss: 0.2850033\n",
      "\tspeed: 0.1259s/iter; left time: 671.7752s\n",
      "\titers: 400, epoch: 5 | loss: 0.2300142\n",
      "\tspeed: 0.1262s/iter; left time: 660.6578s\n",
      "\titers: 500, epoch: 5 | loss: 0.3138312\n",
      "\tspeed: 0.1275s/iter; left time: 654.6403s\n",
      "\titers: 600, epoch: 5 | loss: 0.2654854\n",
      "\tspeed: 0.1268s/iter; left time: 638.4683s\n",
      "\titers: 700, epoch: 5 | loss: 0.2302616\n",
      "\tspeed: 0.1277s/iter; left time: 629.9718s\n",
      "\titers: 800, epoch: 5 | loss: 0.2805905\n",
      "\tspeed: 0.1289s/iter; left time: 623.0318s\n",
      "\titers: 900, epoch: 5 | loss: 0.2521963\n",
      "\tspeed: 0.1271s/iter; left time: 601.7727s\n",
      "Epoch: 5 running time: 1.989568305015564 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2646839 Vali Loss: 0.4230137 Test Loss: 0.8850275\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2957079\n",
      "\tspeed: 0.3828s/iter; left time: 1759.4153s\n",
      "\titers: 200, epoch: 6 | loss: 0.3131303\n",
      "\tspeed: 0.1286s/iter; left time: 578.4020s\n",
      "\titers: 300, epoch: 6 | loss: 0.2958193\n",
      "\tspeed: 0.1270s/iter; left time: 558.2925s\n",
      "\titers: 400, epoch: 6 | loss: 0.2801335\n",
      "\tspeed: 0.1260s/iter; left time: 541.2395s\n",
      "\titers: 500, epoch: 6 | loss: 0.3014933\n",
      "\tspeed: 0.1267s/iter; left time: 531.5098s\n",
      "\titers: 600, epoch: 6 | loss: 0.2947007\n",
      "\tspeed: 0.1272s/iter; left time: 520.8884s\n",
      "\titers: 700, epoch: 6 | loss: 0.2537886\n",
      "\tspeed: 0.1271s/iter; left time: 507.9280s\n",
      "\titers: 800, epoch: 6 | loss: 0.2679251\n",
      "\tspeed: 0.1287s/iter; left time: 501.3828s\n",
      "\titers: 900, epoch: 6 | loss: 0.2741717\n",
      "\tspeed: 0.1300s/iter; left time: 493.5151s\n",
      "Epoch: 6 running time: 2.0054595430692035 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2659598 Vali Loss: 0.4145777 Test Loss: 0.8884851\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8879987597465515, mae:0.6414443850517273\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6590257\n",
      "\tspeed: 0.1355s/iter; left time: 1259.3213s\n",
      "\titers: 200, epoch: 1 | loss: 0.3702555\n",
      "\tspeed: 0.1321s/iter; left time: 1214.1525s\n",
      "\titers: 300, epoch: 1 | loss: 0.4171233\n",
      "\tspeed: 0.1308s/iter; left time: 1188.9784s\n",
      "\titers: 400, epoch: 1 | loss: 0.3742107\n",
      "\tspeed: 0.1317s/iter; left time: 1183.7256s\n",
      "\titers: 500, epoch: 1 | loss: 0.3631184\n",
      "\tspeed: 0.1291s/iter; left time: 1148.1089s\n",
      "\titers: 600, epoch: 1 | loss: 0.2829784\n",
      "\tspeed: 0.1278s/iter; left time: 1123.0589s\n",
      "\titers: 700, epoch: 1 | loss: 0.3232366\n",
      "\tspeed: 0.1271s/iter; left time: 1104.6049s\n",
      "\titers: 800, epoch: 1 | loss: 0.3703890\n",
      "\tspeed: 0.1272s/iter; left time: 1092.9412s\n",
      "\titers: 900, epoch: 1 | loss: 0.2851022\n",
      "\tspeed: 0.1285s/iter; left time: 1090.7771s\n",
      "Epoch: 1 running time: 2.0353498021761576 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4290918 Vali Loss: 0.4069749 Test Loss: 0.7548954\n",
      "Validation loss decreased (inf --> 0.406975).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3553617\n",
      "\tspeed: 0.3918s/iter; left time: 3272.7295s\n",
      "\titers: 200, epoch: 2 | loss: 0.2649492\n",
      "\tspeed: 0.1302s/iter; left time: 1074.2200s\n",
      "\titers: 300, epoch: 2 | loss: 0.2963114\n",
      "\tspeed: 0.1273s/iter; left time: 1038.0252s\n",
      "\titers: 400, epoch: 2 | loss: 0.2734796\n",
      "\tspeed: 0.1280s/iter; left time: 1030.2947s\n",
      "\titers: 500, epoch: 2 | loss: 0.3173178\n",
      "\tspeed: 0.1276s/iter; left time: 1014.8633s\n",
      "\titers: 600, epoch: 2 | loss: 0.2838234\n",
      "\tspeed: 0.1248s/iter; left time: 980.0621s\n",
      "\titers: 700, epoch: 2 | loss: 0.2560588\n",
      "\tspeed: 0.1287s/iter; left time: 997.8202s\n",
      "\titers: 800, epoch: 2 | loss: 0.2681250\n",
      "\tspeed: 0.1305s/iter; left time: 998.6619s\n",
      "\titers: 900, epoch: 2 | loss: 0.2787300\n",
      "\tspeed: 0.1292s/iter; left time: 975.9169s\n",
      "Epoch: 2 running time: 2.013636581103007 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.3078535 Vali Loss: 0.3890424 Test Loss: 0.8557831\n",
      "Validation loss decreased (0.406975 --> 0.389042).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2971739\n",
      "\tspeed: 0.3956s/iter; left time: 2932.5769s\n",
      "\titers: 200, epoch: 3 | loss: 0.2232050\n",
      "\tspeed: 0.1264s/iter; left time: 924.6103s\n",
      "\titers: 300, epoch: 3 | loss: 0.2377447\n",
      "\tspeed: 0.1248s/iter; left time: 899.8565s\n",
      "\titers: 400, epoch: 3 | loss: 0.3415096\n",
      "\tspeed: 0.1249s/iter; left time: 888.2788s\n",
      "\titers: 500, epoch: 3 | loss: 0.2730391\n",
      "\tspeed: 0.1274s/iter; left time: 893.3734s\n",
      "\titers: 600, epoch: 3 | loss: 0.2811303\n",
      "\tspeed: 0.1242s/iter; left time: 858.5331s\n",
      "\titers: 700, epoch: 3 | loss: 0.2590662\n",
      "\tspeed: 0.1245s/iter; left time: 848.5305s\n",
      "\titers: 800, epoch: 3 | loss: 0.1946242\n",
      "\tspeed: 0.1261s/iter; left time: 846.1920s\n",
      "\titers: 900, epoch: 3 | loss: 0.2446160\n",
      "\tspeed: 0.1248s/iter; left time: 825.0291s\n",
      "Epoch: 3 running time: 1.9744897564252217 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2659981 Vali Loss: 0.3890681 Test Loss: 0.9074225\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2502083\n",
      "\tspeed: 0.3923s/iter; left time: 2539.9998s\n",
      "\titers: 200, epoch: 4 | loss: 0.2271685\n",
      "\tspeed: 0.1262s/iter; left time: 804.2684s\n",
      "\titers: 300, epoch: 4 | loss: 0.2457022\n",
      "\tspeed: 0.1290s/iter; left time: 809.6422s\n",
      "\titers: 400, epoch: 4 | loss: 0.2382278\n",
      "\tspeed: 0.1328s/iter; left time: 819.8891s\n",
      "\titers: 500, epoch: 4 | loss: 0.2423605\n",
      "\tspeed: 0.1294s/iter; left time: 785.8340s\n",
      "\titers: 600, epoch: 4 | loss: 0.2446430\n",
      "\tspeed: 0.1298s/iter; left time: 775.6136s\n",
      "\titers: 700, epoch: 4 | loss: 0.2296289\n",
      "\tspeed: 0.1247s/iter; left time: 732.7012s\n",
      "\titers: 800, epoch: 4 | loss: 0.2783559\n",
      "\tspeed: 0.1330s/iter; left time: 767.9665s\n",
      "\titers: 900, epoch: 4 | loss: 0.2721460\n",
      "\tspeed: 0.1273s/iter; left time: 722.0189s\n",
      "Epoch: 4 running time: 2.023188070456187 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2610680 Vali Loss: 0.3909471 Test Loss: 0.8500285\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2504730\n",
      "\tspeed: 0.3925s/iter; left time: 2172.6647s\n",
      "\titers: 200, epoch: 5 | loss: 0.3105081\n",
      "\tspeed: 0.1359s/iter; left time: 738.4907s\n",
      "\titers: 300, epoch: 5 | loss: 0.2403706\n",
      "\tspeed: 0.1384s/iter; left time: 738.3890s\n",
      "\titers: 400, epoch: 5 | loss: 0.2308073\n",
      "\tspeed: 0.1374s/iter; left time: 719.4381s\n",
      "\titers: 500, epoch: 5 | loss: 0.2627726\n",
      "\tspeed: 0.1376s/iter; left time: 706.3911s\n",
      "\titers: 600, epoch: 5 | loss: 0.2978262\n",
      "\tspeed: 0.1404s/iter; left time: 706.6841s\n",
      "\titers: 700, epoch: 5 | loss: 0.2063510\n",
      "\tspeed: 0.1427s/iter; left time: 704.3129s\n",
      "\titers: 800, epoch: 5 | loss: 0.2197400\n",
      "\tspeed: 0.1460s/iter; left time: 705.9743s\n",
      "\titers: 900, epoch: 5 | loss: 0.2552235\n",
      "\tspeed: 0.1431s/iter; left time: 677.6830s\n",
      "Epoch: 5 running time: 2.1947686076164246 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2610565 Vali Loss: 0.3817384 Test Loss: 0.8476037\n",
      "Validation loss decreased (0.389042 --> 0.381738).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2394448\n",
      "\tspeed: 0.4524s/iter; left time: 2079.3107s\n",
      "\titers: 200, epoch: 6 | loss: 0.2661604\n",
      "\tspeed: 0.1394s/iter; left time: 626.8968s\n",
      "\titers: 300, epoch: 6 | loss: 0.2508234\n",
      "\tspeed: 0.1438s/iter; left time: 632.0504s\n",
      "\titers: 400, epoch: 6 | loss: 0.2772815\n",
      "\tspeed: 0.1406s/iter; left time: 604.0409s\n",
      "\titers: 500, epoch: 6 | loss: 0.2164235\n",
      "\tspeed: 0.1396s/iter; left time: 585.7184s\n",
      "\titers: 600, epoch: 6 | loss: 0.2276945\n",
      "\tspeed: 0.1391s/iter; left time: 569.8609s\n",
      "\titers: 700, epoch: 6 | loss: 0.2581926\n",
      "\tspeed: 0.1434s/iter; left time: 572.9331s\n",
      "\titers: 800, epoch: 6 | loss: 0.2069966\n",
      "\tspeed: 0.1365s/iter; left time: 531.7613s\n",
      "\titers: 900, epoch: 6 | loss: 0.2439662\n",
      "\tspeed: 0.1379s/iter; left time: 523.4391s\n",
      "Epoch: 6 running time: 2.1982116023699443 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2476775 Vali Loss: 0.3693107 Test Loss: 0.8297454\n",
      "Validation loss decreased (0.381738 --> 0.369311).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2184741\n",
      "\tspeed: 0.4485s/iter; left time: 1640.3100s\n",
      "\titers: 200, epoch: 7 | loss: 0.3154837\n",
      "\tspeed: 0.1390s/iter; left time: 494.2581s\n",
      "\titers: 300, epoch: 7 | loss: 0.2282375\n",
      "\tspeed: 0.1386s/iter; left time: 479.2927s\n",
      "\titers: 400, epoch: 7 | loss: 0.1981492\n",
      "\tspeed: 0.1396s/iter; left time: 468.5280s\n",
      "\titers: 500, epoch: 7 | loss: 0.2087907\n",
      "\tspeed: 0.1421s/iter; left time: 462.7085s\n",
      "\titers: 600, epoch: 7 | loss: 0.2624970\n",
      "\tspeed: 0.1415s/iter; left time: 446.8668s\n",
      "\titers: 700, epoch: 7 | loss: 0.2519007\n",
      "\tspeed: 0.1370s/iter; left time: 418.9112s\n",
      "\titers: 800, epoch: 7 | loss: 0.2490519\n",
      "\tspeed: 0.1409s/iter; left time: 416.6047s\n",
      "\titers: 900, epoch: 7 | loss: 0.2527945\n",
      "\tspeed: 0.1420s/iter; left time: 405.6074s\n",
      "Epoch: 7 running time: 2.2085670828819275 min.\n",
      "Epoch: 7, Steps: 939 | Train Loss: 0.2412477 Vali Loss: 0.3735014 Test Loss: 0.8222224\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2008697\n",
      "\tspeed: 0.4326s/iter; left time: 1175.7260s\n",
      "\titers: 200, epoch: 8 | loss: 0.2527851\n",
      "\tspeed: 0.1404s/iter; left time: 367.5775s\n",
      "\titers: 300, epoch: 8 | loss: 0.2227888\n",
      "\tspeed: 0.1398s/iter; left time: 352.0554s\n",
      "\titers: 400, epoch: 8 | loss: 0.2456555\n",
      "\tspeed: 0.1365s/iter; left time: 330.1324s\n",
      "\titers: 500, epoch: 8 | loss: 0.2695578\n",
      "\tspeed: 0.1397s/iter; left time: 323.7702s\n",
      "\titers: 600, epoch: 8 | loss: 0.2271018\n",
      "\tspeed: 0.1384s/iter; left time: 306.8968s\n",
      "\titers: 700, epoch: 8 | loss: 0.2245621\n",
      "\tspeed: 0.1380s/iter; left time: 292.2947s\n",
      "\titers: 800, epoch: 8 | loss: 0.2599676\n",
      "\tspeed: 0.1377s/iter; left time: 277.8739s\n",
      "\titers: 900, epoch: 8 | loss: 0.2144273\n",
      "\tspeed: 0.1372s/iter; left time: 263.0841s\n",
      "Epoch: 8 running time: 2.177709178129832 min.\n",
      "Epoch: 8, Steps: 939 | Train Loss: 0.2411348 Vali Loss: 0.3763058 Test Loss: 0.8331358\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2465238\n",
      "\tspeed: 0.4347s/iter; left time: 773.3998s\n",
      "\titers: 200, epoch: 9 | loss: 0.2228289\n",
      "\tspeed: 0.1390s/iter; left time: 233.3263s\n",
      "\titers: 300, epoch: 9 | loss: 0.2397258\n",
      "\tspeed: 0.1380s/iter; left time: 217.8390s\n",
      "\titers: 400, epoch: 9 | loss: 0.2728091\n",
      "\tspeed: 0.1371s/iter; left time: 202.7375s\n",
      "\titers: 500, epoch: 9 | loss: 0.2611856\n",
      "\tspeed: 0.1372s/iter; left time: 189.2597s\n",
      "\titers: 600, epoch: 9 | loss: 0.2147598\n",
      "\tspeed: 0.1377s/iter; left time: 176.1760s\n",
      "\titers: 700, epoch: 9 | loss: 0.2725378\n",
      "\tspeed: 0.1363s/iter; left time: 160.6670s\n",
      "\titers: 800, epoch: 9 | loss: 0.2642465\n",
      "\tspeed: 0.1390s/iter; left time: 150.0157s\n",
      "\titers: 900, epoch: 9 | loss: 0.2390002\n",
      "\tspeed: 0.1427s/iter; left time: 139.6597s\n",
      "Epoch: 9 running time: 2.1845043301582336 min.\n",
      "Epoch: 9, Steps: 939 | Train Loss: 0.2408846 Vali Loss: 0.3721567 Test Loss: 0.8406001\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8414180278778076, mae:0.631447970867157\n",
      "\n",
      "Time intermediate for ES dataset: 37.049030566215514 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"96\"\n",
    "model = \"Informer\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7559140\n",
      "\tspeed: 0.1697s/iter; left time: 1577.1307s\n",
      "\titers: 200, epoch: 1 | loss: 0.6601455\n",
      "\tspeed: 0.1413s/iter; left time: 1298.6746s\n",
      "\titers: 300, epoch: 1 | loss: 0.5010030\n",
      "\tspeed: 0.1405s/iter; left time: 1277.6612s\n",
      "\titers: 400, epoch: 1 | loss: 0.4657636\n",
      "\tspeed: 0.1374s/iter; left time: 1235.5857s\n",
      "\titers: 500, epoch: 1 | loss: 0.4198299\n",
      "\tspeed: 0.1405s/iter; left time: 1249.6191s\n",
      "\titers: 600, epoch: 1 | loss: 0.4128707\n",
      "\tspeed: 0.1386s/iter; left time: 1218.1083s\n",
      "\titers: 700, epoch: 1 | loss: 0.4657623\n",
      "\tspeed: 0.1401s/iter; left time: 1217.8301s\n",
      "\titers: 800, epoch: 1 | loss: 0.4991989\n",
      "\tspeed: 0.1407s/iter; left time: 1208.3871s\n",
      "\titers: 900, epoch: 1 | loss: 0.5064697\n",
      "\tspeed: 0.1410s/iter; left time: 1197.1356s\n",
      "Epoch: 1 running time: 2.222497562567393 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5641891 Vali Loss: 0.7171257 Test Loss: 0.8211090\n",
      "Validation loss decreased (inf --> 0.717126).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5262482\n",
      "\tspeed: 0.4475s/iter; left time: 3737.6701s\n",
      "\titers: 200, epoch: 2 | loss: 0.4040740\n",
      "\tspeed: 0.1409s/iter; left time: 1162.9234s\n",
      "\titers: 300, epoch: 2 | loss: 0.4773462\n",
      "\tspeed: 0.1413s/iter; left time: 1151.8564s\n",
      "\titers: 400, epoch: 2 | loss: 0.6392567\n",
      "\tspeed: 0.1388s/iter; left time: 1117.3606s\n",
      "\titers: 500, epoch: 2 | loss: 0.3605611\n",
      "\tspeed: 0.1387s/iter; left time: 1103.0531s\n",
      "\titers: 600, epoch: 2 | loss: 0.5982773\n",
      "\tspeed: 0.1441s/iter; left time: 1131.6860s\n",
      "\titers: 700, epoch: 2 | loss: 0.3476955\n",
      "\tspeed: 0.1428s/iter; left time: 1107.0247s\n",
      "\titers: 800, epoch: 2 | loss: 0.3667593\n",
      "\tspeed: 0.1419s/iter; left time: 1085.4681s\n",
      "\titers: 900, epoch: 2 | loss: 0.4465778\n",
      "\tspeed: 0.1403s/iter; left time: 1059.8167s\n",
      "Epoch: 2 running time: 2.2237186193466187 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4277616 Vali Loss: 0.7457622 Test Loss: 0.9326196\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4095838\n",
      "\tspeed: 0.4417s/iter; left time: 3274.0880s\n",
      "\titers: 200, epoch: 3 | loss: 0.4767677\n",
      "\tspeed: 0.1398s/iter; left time: 1022.1438s\n",
      "\titers: 300, epoch: 3 | loss: 0.4059705\n",
      "\tspeed: 0.1431s/iter; left time: 1032.4363s\n",
      "\titers: 400, epoch: 3 | loss: 0.4408295\n",
      "\tspeed: 0.1448s/iter; left time: 1030.0451s\n",
      "\titers: 500, epoch: 3 | loss: 0.3635677\n",
      "\tspeed: 0.1380s/iter; left time: 968.1367s\n",
      "\titers: 600, epoch: 3 | loss: 0.4532194\n",
      "\tspeed: 0.1416s/iter; left time: 979.1135s\n",
      "\titers: 700, epoch: 3 | loss: 0.4472884\n",
      "\tspeed: 0.1420s/iter; left time: 967.4486s\n",
      "\titers: 800, epoch: 3 | loss: 0.3393967\n",
      "\tspeed: 0.1390s/iter; left time: 932.8111s\n",
      "\titers: 900, epoch: 3 | loss: 0.4051116\n",
      "\tspeed: 0.1492s/iter; left time: 986.6778s\n",
      "Epoch: 3 running time: 2.235738722483317 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.4162776 Vali Loss: 0.7086870 Test Loss: 0.8148070\n",
      "Validation loss decreased (0.717126 --> 0.708687).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.4346139\n",
      "\tspeed: 0.4522s/iter; left time: 2927.7684s\n",
      "\titers: 200, epoch: 4 | loss: 0.3436288\n",
      "\tspeed: 0.1397s/iter; left time: 890.2250s\n",
      "\titers: 300, epoch: 4 | loss: 0.3317654\n",
      "\tspeed: 0.1461s/iter; left time: 916.7987s\n",
      "\titers: 400, epoch: 4 | loss: 0.4210582\n",
      "\tspeed: 0.1400s/iter; left time: 864.3808s\n",
      "\titers: 500, epoch: 4 | loss: 0.3371384\n",
      "\tspeed: 0.1434s/iter; left time: 871.2955s\n",
      "\titers: 600, epoch: 4 | loss: 0.3962014\n",
      "\tspeed: 0.1440s/iter; left time: 860.5139s\n",
      "\titers: 700, epoch: 4 | loss: 0.3111204\n",
      "\tspeed: 0.1418s/iter; left time: 832.6624s\n",
      "\titers: 800, epoch: 4 | loss: 0.3666123\n",
      "\tspeed: 0.1424s/iter; left time: 822.2047s\n",
      "\titers: 900, epoch: 4 | loss: 0.3192907\n",
      "\tspeed: 0.1429s/iter; left time: 810.7932s\n",
      "Epoch: 4 running time: 2.246029837926229 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3593553 Vali Loss: 0.7355791 Test Loss: 0.8664888\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4471202\n",
      "\tspeed: 0.4457s/iter; left time: 2467.1303s\n",
      "\titers: 200, epoch: 5 | loss: 0.3257004\n",
      "\tspeed: 0.1430s/iter; left time: 777.0043s\n",
      "\titers: 300, epoch: 5 | loss: 0.3302858\n",
      "\tspeed: 0.1415s/iter; left time: 755.1070s\n",
      "\titers: 400, epoch: 5 | loss: 0.3556143\n",
      "\tspeed: 0.1441s/iter; left time: 754.2481s\n",
      "\titers: 500, epoch: 5 | loss: 0.3501981\n",
      "\tspeed: 0.1429s/iter; left time: 733.7818s\n",
      "\titers: 600, epoch: 5 | loss: 0.3315580\n",
      "\tspeed: 0.1437s/iter; left time: 723.5594s\n",
      "\titers: 700, epoch: 5 | loss: 0.3834475\n",
      "\tspeed: 0.1394s/iter; left time: 688.0723s\n",
      "\titers: 800, epoch: 5 | loss: 0.3463752\n",
      "\tspeed: 0.1432s/iter; left time: 692.2019s\n",
      "\titers: 900, epoch: 5 | loss: 0.2864018\n",
      "\tspeed: 0.1401s/iter; left time: 663.3613s\n",
      "Epoch: 5 running time: 2.244635566075643 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.3587584 Vali Loss: 0.7178938 Test Loss: 0.8535417\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.4152527\n",
      "\tspeed: 0.4399s/iter; left time: 2021.7767s\n",
      "\titers: 200, epoch: 6 | loss: 0.4054840\n",
      "\tspeed: 0.1453s/iter; left time: 653.1641s\n",
      "\titers: 300, epoch: 6 | loss: 0.3302798\n",
      "\tspeed: 0.1435s/iter; left time: 631.0285s\n",
      "\titers: 400, epoch: 6 | loss: 0.3070555\n",
      "\tspeed: 0.1379s/iter; left time: 592.6007s\n",
      "\titers: 500, epoch: 6 | loss: 0.3973166\n",
      "\tspeed: 0.1432s/iter; left time: 601.0136s\n",
      "\titers: 600, epoch: 6 | loss: 0.4181066\n",
      "\tspeed: 0.1396s/iter; left time: 571.6095s\n",
      "\titers: 700, epoch: 6 | loss: 0.3709582\n",
      "\tspeed: 0.1408s/iter; left time: 562.7144s\n",
      "\titers: 800, epoch: 6 | loss: 0.3426579\n",
      "\tspeed: 0.1381s/iter; left time: 537.9003s\n",
      "\titers: 900, epoch: 6 | loss: 0.3212492\n",
      "\tspeed: 0.1365s/iter; left time: 518.3085s\n",
      "Epoch: 6 running time: 2.209868586063385 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.3594263 Vali Loss: 0.7221369 Test Loss: 0.8450569\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8442292213439941, mae:0.6459956169128418\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7447056\n",
      "\tspeed: 0.1485s/iter; left time: 1379.6285s\n",
      "\titers: 200, epoch: 1 | loss: 0.6593015\n",
      "\tspeed: 0.1438s/iter; left time: 1321.8413s\n",
      "\titers: 300, epoch: 1 | loss: 0.4088263\n",
      "\tspeed: 0.1417s/iter; left time: 1287.7465s\n",
      "\titers: 400, epoch: 1 | loss: 0.5541723\n",
      "\tspeed: 0.1402s/iter; left time: 1260.9021s\n",
      "\titers: 500, epoch: 1 | loss: 0.5033848\n",
      "\tspeed: 0.1422s/iter; left time: 1264.1649s\n",
      "\titers: 600, epoch: 1 | loss: 0.4607519\n",
      "\tspeed: 0.1400s/iter; left time: 1230.5323s\n",
      "\titers: 700, epoch: 1 | loss: 0.4662497\n",
      "\tspeed: 0.1364s/iter; left time: 1185.0691s\n",
      "\titers: 800, epoch: 1 | loss: 0.4591224\n",
      "\tspeed: 0.1420s/iter; left time: 1219.8731s\n",
      "\titers: 900, epoch: 1 | loss: 0.4039500\n",
      "\tspeed: 0.1377s/iter; left time: 1169.0476s\n",
      "Epoch: 1 running time: 2.2160993377367655 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5717848 Vali Loss: 0.7001419 Test Loss: 0.8262067\n",
      "Validation loss decreased (inf --> 0.700142).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4061307\n",
      "\tspeed: 0.4536s/iter; left time: 3788.7552s\n",
      "\titers: 200, epoch: 2 | loss: 0.4506385\n",
      "\tspeed: 0.1435s/iter; left time: 1184.0971s\n",
      "\titers: 300, epoch: 2 | loss: 0.4700473\n",
      "\tspeed: 0.1413s/iter; left time: 1152.2387s\n",
      "\titers: 400, epoch: 2 | loss: 0.4210120\n",
      "\tspeed: 0.1431s/iter; left time: 1152.1514s\n",
      "\titers: 500, epoch: 2 | loss: 0.5275982\n",
      "\tspeed: 0.1493s/iter; left time: 1186.8581s\n",
      "\titers: 600, epoch: 2 | loss: 0.3871483\n",
      "\tspeed: 0.1414s/iter; left time: 1110.1648s\n",
      "\titers: 700, epoch: 2 | loss: 0.3931742\n",
      "\tspeed: 0.1408s/iter; left time: 1091.7922s\n",
      "\titers: 800, epoch: 2 | loss: 0.3450389\n",
      "\tspeed: 0.1408s/iter; left time: 1077.3604s\n",
      "\titers: 900, epoch: 2 | loss: 0.3461212\n",
      "\tspeed: 0.1411s/iter; left time: 1065.3302s\n",
      "Epoch: 2 running time: 2.2447775761286417 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4273554 Vali Loss: 0.6673716 Test Loss: 0.8367274\n",
      "Validation loss decreased (0.700142 --> 0.667372).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3449736\n",
      "\tspeed: 0.4540s/iter; left time: 3365.8061s\n",
      "\titers: 200, epoch: 3 | loss: 0.3258068\n",
      "\tspeed: 0.1382s/iter; left time: 1010.5652s\n",
      "\titers: 300, epoch: 3 | loss: 0.4663257\n",
      "\tspeed: 0.1381s/iter; left time: 996.3314s\n",
      "\titers: 400, epoch: 3 | loss: 0.3481233\n",
      "\tspeed: 0.1397s/iter; left time: 993.9938s\n",
      "\titers: 500, epoch: 3 | loss: 0.3617330\n",
      "\tspeed: 0.1392s/iter; left time: 975.9101s\n",
      "\titers: 600, epoch: 3 | loss: 0.3291736\n",
      "\tspeed: 0.1373s/iter; left time: 949.3970s\n",
      "\titers: 700, epoch: 3 | loss: 0.2632641\n",
      "\tspeed: 0.1391s/iter; left time: 947.5021s\n",
      "\titers: 800, epoch: 3 | loss: 0.3139873\n",
      "\tspeed: 0.1390s/iter; left time: 933.3417s\n",
      "\titers: 900, epoch: 3 | loss: 0.3452283\n",
      "\tspeed: 0.1401s/iter; left time: 926.7752s\n",
      "Epoch: 3 running time: 2.196194016933441 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.3450312 Vali Loss: 0.6789485 Test Loss: 0.9146711\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3476217\n",
      "\tspeed: 0.4441s/iter; left time: 2875.4020s\n",
      "\titers: 200, epoch: 4 | loss: 0.4109974\n",
      "\tspeed: 0.1416s/iter; left time: 902.8536s\n",
      "\titers: 300, epoch: 4 | loss: 0.4098201\n",
      "\tspeed: 0.1406s/iter; left time: 882.3374s\n",
      "\titers: 400, epoch: 4 | loss: 0.2883653\n",
      "\tspeed: 0.1416s/iter; left time: 874.3070s\n",
      "\titers: 500, epoch: 4 | loss: 0.3616478\n",
      "\tspeed: 0.1389s/iter; left time: 843.7064s\n",
      "\titers: 600, epoch: 4 | loss: 0.2977428\n",
      "\tspeed: 0.1399s/iter; left time: 836.0164s\n",
      "\titers: 700, epoch: 4 | loss: 0.3625081\n",
      "\tspeed: 0.1402s/iter; left time: 823.7982s\n",
      "\titers: 800, epoch: 4 | loss: 0.3037106\n",
      "\tspeed: 0.1418s/iter; left time: 818.7681s\n",
      "\titers: 900, epoch: 4 | loss: 0.3495096\n",
      "\tspeed: 0.1406s/iter; left time: 797.6230s\n",
      "Epoch: 4 running time: 2.2155786236127217 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3419181 Vali Loss: 0.6798371 Test Loss: 0.8732088\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3355304\n",
      "\tspeed: 0.4409s/iter; left time: 2440.1861s\n",
      "\titers: 200, epoch: 5 | loss: 0.3756908\n",
      "\tspeed: 0.1405s/iter; left time: 763.6671s\n",
      "\titers: 300, epoch: 5 | loss: 0.3910441\n",
      "\tspeed: 0.1401s/iter; left time: 747.6779s\n",
      "\titers: 400, epoch: 5 | loss: 0.4812200\n",
      "\tspeed: 0.1383s/iter; left time: 723.8599s\n",
      "\titers: 500, epoch: 5 | loss: 0.3561763\n",
      "\tspeed: 0.1427s/iter; left time: 732.8088s\n",
      "\titers: 600, epoch: 5 | loss: 0.3331439\n",
      "\tspeed: 0.1387s/iter; left time: 698.2608s\n",
      "\titers: 700, epoch: 5 | loss: 0.3013994\n",
      "\tspeed: 0.1392s/iter; left time: 687.0521s\n",
      "\titers: 800, epoch: 5 | loss: 0.3873559\n",
      "\tspeed: 0.1405s/iter; left time: 679.3193s\n",
      "\titers: 900, epoch: 5 | loss: 0.2897159\n",
      "\tspeed: 0.1394s/iter; left time: 660.2139s\n",
      "Epoch: 5 running time: 2.204855465888977 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.3433942 Vali Loss: 0.6645554 Test Loss: 0.8553493\n",
      "Validation loss decreased (0.667372 --> 0.664555).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.3726688\n",
      "\tspeed: 0.4510s/iter; left time: 2072.6656s\n",
      "\titers: 200, epoch: 6 | loss: 0.3215174\n",
      "\tspeed: 0.1407s/iter; left time: 632.7621s\n",
      "\titers: 300, epoch: 6 | loss: 0.3263567\n",
      "\tspeed: 0.1372s/iter; left time: 603.2902s\n",
      "\titers: 400, epoch: 6 | loss: 0.3612089\n",
      "\tspeed: 0.1412s/iter; left time: 606.5049s\n",
      "\titers: 500, epoch: 6 | loss: 0.3376253\n",
      "\tspeed: 0.1407s/iter; left time: 590.3374s\n",
      "\titers: 600, epoch: 6 | loss: 0.3098537\n",
      "\tspeed: 0.1427s/iter; left time: 584.6228s\n",
      "\titers: 700, epoch: 6 | loss: 0.3697448\n",
      "\tspeed: 0.1400s/iter; left time: 559.2719s\n",
      "\titers: 800, epoch: 6 | loss: 0.3731429\n",
      "\tspeed: 0.1411s/iter; left time: 549.5880s\n",
      "\titers: 900, epoch: 6 | loss: 0.3212779\n",
      "\tspeed: 0.1414s/iter; left time: 536.8423s\n",
      "Epoch: 6 running time: 2.2140369455019635 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.3231165 Vali Loss: 0.6854773 Test Loss: 0.8901383\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2708530\n",
      "\tspeed: 0.4377s/iter; left time: 1600.6997s\n",
      "\titers: 200, epoch: 7 | loss: 0.3650885\n",
      "\tspeed: 0.1424s/iter; left time: 506.5792s\n",
      "\titers: 300, epoch: 7 | loss: 0.2993652\n",
      "\tspeed: 0.1410s/iter; left time: 487.4130s\n",
      "\titers: 400, epoch: 7 | loss: 0.3053141\n",
      "\tspeed: 0.1456s/iter; left time: 488.8580s\n",
      "\titers: 500, epoch: 7 | loss: 0.2842398\n",
      "\tspeed: 0.1399s/iter; left time: 455.6916s\n",
      "\titers: 600, epoch: 7 | loss: 0.3635543\n",
      "\tspeed: 0.1433s/iter; left time: 452.3473s\n",
      "\titers: 700, epoch: 7 | loss: 0.3592817\n",
      "\tspeed: 0.1395s/iter; left time: 426.3296s\n",
      "\titers: 800, epoch: 7 | loss: 0.3339449\n",
      "\tspeed: 0.1430s/iter; left time: 422.9065s\n",
      "\titers: 900, epoch: 7 | loss: 0.3064209\n",
      "\tspeed: 0.1406s/iter; left time: 401.5687s\n",
      "Epoch: 7 running time: 2.236991306145986 min.\n",
      "Epoch: 7, Steps: 939 | Train Loss: 0.3228807 Vali Loss: 0.6734169 Test Loss: 0.8669579\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2752141\n",
      "\tspeed: 0.4493s/iter; left time: 1221.2706s\n",
      "\titers: 200, epoch: 8 | loss: 0.3803290\n",
      "\tspeed: 0.1380s/iter; left time: 361.3639s\n",
      "\titers: 300, epoch: 8 | loss: 0.3418852\n",
      "\tspeed: 0.1408s/iter; left time: 354.6341s\n",
      "\titers: 400, epoch: 8 | loss: 0.3336903\n",
      "\tspeed: 0.1381s/iter; left time: 333.9304s\n",
      "\titers: 500, epoch: 8 | loss: 0.3313535\n",
      "\tspeed: 0.1418s/iter; left time: 328.6760s\n",
      "\titers: 600, epoch: 8 | loss: 0.2710003\n",
      "\tspeed: 0.1374s/iter; left time: 304.7373s\n",
      "\titers: 700, epoch: 8 | loss: 0.2600959\n",
      "\tspeed: 0.1444s/iter; left time: 305.8473s\n",
      "\titers: 800, epoch: 8 | loss: 0.3119386\n",
      "\tspeed: 0.1401s/iter; left time: 282.6486s\n",
      "\titers: 900, epoch: 8 | loss: 0.3236929\n",
      "\tspeed: 0.1436s/iter; left time: 275.5191s\n",
      "Epoch: 8 running time: 2.2206318457921346 min.\n",
      "Epoch: 8, Steps: 939 | Train Loss: 0.3237316 Vali Loss: 0.6744314 Test Loss: 0.8682292\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_DE_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8667391538619995, mae:0.6471533179283142\n",
      "\n",
      "Time intermediate for DE dataset: 37.49893089532852 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.8083605\n",
      "\tspeed: 0.1663s/iter; left time: 1545.0702s\n",
      "\titers: 200, epoch: 1 | loss: 0.6042727\n",
      "\tspeed: 0.1348s/iter; left time: 1238.9757s\n",
      "\titers: 300, epoch: 1 | loss: 0.5880392\n",
      "\tspeed: 0.1394s/iter; left time: 1267.5648s\n",
      "\titers: 400, epoch: 1 | loss: 0.5434972\n",
      "\tspeed: 0.1431s/iter; left time: 1286.6582s\n",
      "\titers: 500, epoch: 1 | loss: 0.5033150\n",
      "\tspeed: 0.1334s/iter; left time: 1186.4015s\n",
      "\titers: 600, epoch: 1 | loss: 0.4393446\n",
      "\tspeed: 0.1428s/iter; left time: 1255.2664s\n",
      "\titers: 700, epoch: 1 | loss: 0.5264087\n",
      "\tspeed: 0.1357s/iter; left time: 1179.1234s\n",
      "\titers: 800, epoch: 1 | loss: 0.5861061\n",
      "\tspeed: 0.1422s/iter; left time: 1222.0637s\n",
      "\titers: 900, epoch: 1 | loss: 0.4890957\n",
      "\tspeed: 0.1400s/iter; left time: 1189.0148s\n",
      "Epoch: 1 running time: 2.2009796182314556 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5769552 Vali Loss: 0.7088629 Test Loss: 1.0980847\n",
      "Validation loss decreased (inf --> 0.708863).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.5545461\n",
      "\tspeed: 0.4371s/iter; left time: 3650.5840s\n",
      "\titers: 200, epoch: 2 | loss: 0.4281227\n",
      "\tspeed: 0.1418s/iter; left time: 1169.8486s\n",
      "\titers: 300, epoch: 2 | loss: 0.4443220\n",
      "\tspeed: 0.1405s/iter; left time: 1145.1573s\n",
      "\titers: 400, epoch: 2 | loss: 0.5764625\n",
      "\tspeed: 0.1356s/iter; left time: 1092.1453s\n",
      "\titers: 500, epoch: 2 | loss: 0.4205113\n",
      "\tspeed: 0.1424s/iter; left time: 1132.7006s\n",
      "\titers: 600, epoch: 2 | loss: 0.4880638\n",
      "\tspeed: 0.1331s/iter; left time: 1045.1595s\n",
      "\titers: 700, epoch: 2 | loss: 0.4213868\n",
      "\tspeed: 0.1429s/iter; left time: 1108.1428s\n",
      "\titers: 800, epoch: 2 | loss: 0.4026201\n",
      "\tspeed: 0.1401s/iter; left time: 1071.7296s\n",
      "\titers: 900, epoch: 2 | loss: 0.4723349\n",
      "\tspeed: 0.1419s/iter; left time: 1071.5731s\n",
      "Epoch: 2 running time: 2.1962295810381574 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4428352 Vali Loss: 0.7239694 Test Loss: 1.2943285\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4480673\n",
      "\tspeed: 0.4408s/iter; left time: 3267.3063s\n",
      "\titers: 200, epoch: 3 | loss: 0.4947310\n",
      "\tspeed: 0.1413s/iter; left time: 1033.6868s\n",
      "\titers: 300, epoch: 3 | loss: 0.4939314\n",
      "\tspeed: 0.1479s/iter; left time: 1066.9223s\n",
      "\titers: 400, epoch: 3 | loss: 0.4132645\n",
      "\tspeed: 0.1471s/iter; left time: 1045.9972s\n",
      "\titers: 500, epoch: 3 | loss: 0.4019790\n",
      "\tspeed: 0.1422s/iter; left time: 997.4004s\n",
      "\titers: 600, epoch: 3 | loss: 0.3910426\n",
      "\tspeed: 0.1384s/iter; left time: 956.6930s\n",
      "\titers: 700, epoch: 3 | loss: 0.4234212\n",
      "\tspeed: 0.1404s/iter; left time: 956.5800s\n",
      "\titers: 800, epoch: 3 | loss: 0.4154332\n",
      "\tspeed: 0.1389s/iter; left time: 932.7418s\n",
      "\titers: 900, epoch: 3 | loss: 0.4049415\n",
      "\tspeed: 0.1432s/iter; left time: 946.7693s\n",
      "Epoch: 3 running time: 2.235846201578776 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.4344580 Vali Loss: 0.7482555 Test Loss: 1.1842282\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.4369820\n",
      "\tspeed: 0.4360s/iter; left time: 2822.4562s\n",
      "\titers: 200, epoch: 4 | loss: 0.4659848\n",
      "\tspeed: 0.1373s/iter; left time: 875.0506s\n",
      "\titers: 300, epoch: 4 | loss: 0.4069278\n",
      "\tspeed: 0.1402s/iter; left time: 879.3138s\n",
      "\titers: 400, epoch: 4 | loss: 0.4918749\n",
      "\tspeed: 0.1369s/iter; left time: 845.0283s\n",
      "\titers: 500, epoch: 4 | loss: 0.4460267\n",
      "\tspeed: 0.1376s/iter; left time: 835.9737s\n",
      "\titers: 600, epoch: 4 | loss: 0.4548974\n",
      "\tspeed: 0.1424s/iter; left time: 850.5424s\n",
      "\titers: 700, epoch: 4 | loss: 0.3654212\n",
      "\tspeed: 0.1382s/iter; left time: 811.9863s\n",
      "\titers: 800, epoch: 4 | loss: 0.3909819\n",
      "\tspeed: 0.1375s/iter; left time: 793.9217s\n",
      "\titers: 900, epoch: 4 | loss: 0.4895321\n",
      "\tspeed: 0.1385s/iter; left time: 785.5661s\n",
      "Epoch: 4 running time: 2.176387333869934 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.4323684 Vali Loss: 0.7605929 Test Loss: 1.1509625\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:1.151376724243164, mae:0.7609724402427673\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7174057\n",
      "\tspeed: 0.1476s/iter; left time: 1371.2172s\n",
      "\titers: 200, epoch: 1 | loss: 0.5984694\n",
      "\tspeed: 0.1385s/iter; left time: 1272.6982s\n",
      "\titers: 300, epoch: 1 | loss: 0.5788845\n",
      "\tspeed: 0.1431s/iter; left time: 1300.6211s\n",
      "\titers: 400, epoch: 1 | loss: 0.6282368\n",
      "\tspeed: 0.1385s/iter; left time: 1245.3856s\n",
      "\titers: 500, epoch: 1 | loss: 0.5255317\n",
      "\tspeed: 0.1443s/iter; left time: 1283.0313s\n",
      "\titers: 600, epoch: 1 | loss: 0.4614025\n",
      "\tspeed: 0.1325s/iter; left time: 1164.8439s\n",
      "\titers: 700, epoch: 1 | loss: 0.4846911\n",
      "\tspeed: 0.1438s/iter; left time: 1249.9393s\n",
      "\titers: 800, epoch: 1 | loss: 0.4044934\n",
      "\tspeed: 0.1384s/iter; left time: 1189.0376s\n",
      "\titers: 900, epoch: 1 | loss: 0.4881596\n",
      "\tspeed: 0.1414s/iter; left time: 1200.6656s\n",
      "Epoch: 1 running time: 2.204145566622416 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.5708030 Vali Loss: 0.7525572 Test Loss: 1.1455549\n",
      "Validation loss decreased (inf --> 0.752557).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4793997\n",
      "\tspeed: 0.4410s/iter; left time: 3683.2778s\n",
      "\titers: 200, epoch: 2 | loss: 0.4411435\n",
      "\tspeed: 0.1410s/iter; left time: 1163.5484s\n",
      "\titers: 300, epoch: 2 | loss: 0.4005803\n",
      "\tspeed: 0.1401s/iter; left time: 1141.8375s\n",
      "\titers: 400, epoch: 2 | loss: 0.4814230\n",
      "\tspeed: 0.1374s/iter; left time: 1106.2002s\n",
      "\titers: 500, epoch: 2 | loss: 0.3774332\n",
      "\tspeed: 0.1415s/iter; left time: 1125.2094s\n",
      "\titers: 600, epoch: 2 | loss: 0.4242199\n",
      "\tspeed: 0.1352s/iter; left time: 1061.7000s\n",
      "\titers: 700, epoch: 2 | loss: 0.4064912\n",
      "\tspeed: 0.1441s/iter; left time: 1116.8253s\n",
      "\titers: 800, epoch: 2 | loss: 0.4377633\n",
      "\tspeed: 0.1435s/iter; left time: 1098.4013s\n",
      "\titers: 900, epoch: 2 | loss: 0.4441836\n",
      "\tspeed: 0.1365s/iter; left time: 1030.9628s\n",
      "Epoch: 2 running time: 2.19695725440979 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.4470905 Vali Loss: 0.7571042 Test Loss: 1.3562176\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4385988\n",
      "\tspeed: 0.4351s/iter; left time: 3225.1951s\n",
      "\titers: 200, epoch: 3 | loss: 0.4950843\n",
      "\tspeed: 0.1444s/iter; left time: 1055.7211s\n",
      "\titers: 300, epoch: 3 | loss: 0.3735984\n",
      "\tspeed: 0.1350s/iter; left time: 973.9851s\n",
      "\titers: 400, epoch: 3 | loss: 0.4782867\n",
      "\tspeed: 0.1459s/iter; left time: 1038.0703s\n",
      "\titers: 500, epoch: 3 | loss: 0.4981522\n",
      "\tspeed: 0.1388s/iter; left time: 973.6712s\n",
      "\titers: 600, epoch: 3 | loss: 0.4281476\n",
      "\tspeed: 0.1405s/iter; left time: 971.2198s\n",
      "\titers: 700, epoch: 3 | loss: 0.4090459\n",
      "\tspeed: 0.1400s/iter; left time: 953.8539s\n",
      "\titers: 800, epoch: 3 | loss: 0.4218008\n",
      "\tspeed: 0.1455s/iter; left time: 976.9835s\n",
      "\titers: 900, epoch: 3 | loss: 0.4452266\n",
      "\tspeed: 0.1427s/iter; left time: 943.5939s\n",
      "Epoch: 3 running time: 2.223308293024699 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.4388648 Vali Loss: 0.7479808 Test Loss: 1.2977123\n",
      "Validation loss decreased (0.752557 --> 0.747981).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3496612\n",
      "\tspeed: 0.4451s/iter; left time: 2881.4085s\n",
      "\titers: 200, epoch: 4 | loss: 0.3880953\n",
      "\tspeed: 0.1395s/iter; left time: 889.0101s\n",
      "\titers: 300, epoch: 4 | loss: 0.4699698\n",
      "\tspeed: 0.1367s/iter; left time: 857.7777s\n",
      "\titers: 400, epoch: 4 | loss: 0.4131892\n",
      "\tspeed: 0.1414s/iter; left time: 872.8914s\n",
      "\titers: 500, epoch: 4 | loss: 0.3588521\n",
      "\tspeed: 0.1393s/iter; left time: 846.0101s\n",
      "\titers: 600, epoch: 4 | loss: 0.3679525\n",
      "\tspeed: 0.1385s/iter; left time: 827.5881s\n",
      "\titers: 700, epoch: 4 | loss: 0.4040492\n",
      "\tspeed: 0.1421s/iter; left time: 834.9610s\n",
      "\titers: 800, epoch: 4 | loss: 0.3158428\n",
      "\tspeed: 0.1411s/iter; left time: 814.6073s\n",
      "\titers: 900, epoch: 4 | loss: 0.3549581\n",
      "\tspeed: 0.1378s/iter; left time: 782.0366s\n",
      "Epoch: 4 running time: 2.2032761096954347 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.3855216 Vali Loss: 0.8231856 Test Loss: 1.4624581\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4242773\n",
      "\tspeed: 0.4408s/iter; left time: 2440.0455s\n",
      "\titers: 200, epoch: 5 | loss: 0.3582258\n",
      "\tspeed: 0.1407s/iter; left time: 764.4485s\n",
      "\titers: 300, epoch: 5 | loss: 0.4846379\n",
      "\tspeed: 0.1415s/iter; left time: 755.1228s\n",
      "\titers: 400, epoch: 5 | loss: 0.3459863\n",
      "\tspeed: 0.1415s/iter; left time: 740.5587s\n",
      "\titers: 500, epoch: 5 | loss: 0.3713422\n",
      "\tspeed: 0.1393s/iter; left time: 715.2959s\n",
      "\titers: 600, epoch: 5 | loss: 0.3948859\n",
      "\tspeed: 0.1410s/iter; left time: 709.7155s\n",
      "\titers: 700, epoch: 5 | loss: 0.3836330\n",
      "\tspeed: 0.1428s/iter; left time: 704.8322s\n",
      "\titers: 800, epoch: 5 | loss: 0.3721221\n",
      "\tspeed: 0.1411s/iter; left time: 682.1861s\n",
      "\titers: 900, epoch: 5 | loss: 0.2794088\n",
      "\tspeed: 0.1404s/iter; left time: 664.8750s\n",
      "Epoch: 5 running time: 2.223522881666819 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.3837905 Vali Loss: 0.7847434 Test Loss: 1.3578115\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.4093121\n",
      "\tspeed: 0.4492s/iter; left time: 2064.6612s\n",
      "\titers: 200, epoch: 6 | loss: 0.3623025\n",
      "\tspeed: 0.1443s/iter; left time: 648.5532s\n",
      "\titers: 300, epoch: 6 | loss: 0.4597489\n",
      "\tspeed: 0.1483s/iter; left time: 651.9919s\n",
      "\titers: 400, epoch: 6 | loss: 0.3277133\n",
      "\tspeed: 0.1444s/iter; left time: 620.2443s\n",
      "\titers: 500, epoch: 6 | loss: 0.4091626\n",
      "\tspeed: 0.1465s/iter; left time: 614.7795s\n",
      "\titers: 600, epoch: 6 | loss: 0.3725473\n",
      "\tspeed: 0.1428s/iter; left time: 584.8783s\n",
      "\titers: 700, epoch: 6 | loss: 0.3676435\n",
      "\tspeed: 0.1404s/iter; left time: 561.1637s\n",
      "\titers: 800, epoch: 6 | loss: 0.3754401\n",
      "\tspeed: 0.1467s/iter; left time: 571.5348s\n",
      "\titers: 900, epoch: 6 | loss: 0.4071269\n",
      "\tspeed: 0.1442s/iter; left time: 547.5038s\n",
      "Epoch: 6 running time: 2.2740378260612486 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.3849790 Vali Loss: 0.7940353 Test Loss: 1.3383511\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:1.339471459388733, mae:0.8141219615936279\n",
      "\n",
      "Time intermediate for GB dataset: 26.862060554822285 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6535346\n",
      "\tspeed: 0.1683s/iter; left time: 1563.8910s\n",
      "\titers: 200, epoch: 1 | loss: 0.4544106\n",
      "\tspeed: 0.1410s/iter; left time: 1295.8563s\n",
      "\titers: 300, epoch: 1 | loss: 0.3685518\n",
      "\tspeed: 0.1402s/iter; left time: 1274.6814s\n",
      "\titers: 400, epoch: 1 | loss: 0.4164305\n",
      "\tspeed: 0.1403s/iter; left time: 1261.4291s\n",
      "\titers: 500, epoch: 1 | loss: 0.3425169\n",
      "\tspeed: 0.1372s/iter; left time: 1219.5577s\n",
      "\titers: 600, epoch: 1 | loss: 0.3266223\n",
      "\tspeed: 0.1434s/iter; left time: 1260.8429s\n",
      "\titers: 700, epoch: 1 | loss: 0.3547635\n",
      "\tspeed: 0.1399s/iter; left time: 1215.8287s\n",
      "\titers: 800, epoch: 1 | loss: 0.3798216\n",
      "\tspeed: 0.1407s/iter; left time: 1208.5068s\n",
      "\titers: 900, epoch: 1 | loss: 0.3751050\n",
      "\tspeed: 0.1398s/iter; left time: 1187.1484s\n",
      "Epoch: 1 running time: 2.222311882177989 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4262612 Vali Loss: 0.4088405 Test Loss: 0.7468417\n",
      "Validation loss decreased (inf --> 0.408841).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3060112\n",
      "\tspeed: 0.4521s/iter; left time: 3775.9235s\n",
      "\titers: 200, epoch: 2 | loss: 0.3075564\n",
      "\tspeed: 0.1414s/iter; left time: 1166.4730s\n",
      "\titers: 300, epoch: 2 | loss: 0.3118545\n",
      "\tspeed: 0.1404s/iter; left time: 1144.8156s\n",
      "\titers: 400, epoch: 2 | loss: 0.3302422\n",
      "\tspeed: 0.1403s/iter; left time: 1129.3866s\n",
      "\titers: 500, epoch: 2 | loss: 0.2575973\n",
      "\tspeed: 0.1403s/iter; left time: 1116.0293s\n",
      "\titers: 600, epoch: 2 | loss: 0.3559574\n",
      "\tspeed: 0.1398s/iter; left time: 1097.6254s\n",
      "\titers: 700, epoch: 2 | loss: 0.3002410\n",
      "\tspeed: 0.1405s/iter; left time: 1089.0009s\n",
      "\titers: 800, epoch: 2 | loss: 0.2893221\n",
      "\tspeed: 0.1409s/iter; left time: 1078.2982s\n",
      "\titers: 900, epoch: 2 | loss: 0.3215189\n",
      "\tspeed: 0.1449s/iter; left time: 1094.0086s\n",
      "Epoch: 2 running time: 2.22299089829127 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.3083065 Vali Loss: 0.4429979 Test Loss: 0.8032006\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3308239\n",
      "\tspeed: 0.4356s/iter; left time: 3228.7772s\n",
      "\titers: 200, epoch: 3 | loss: 0.2796704\n",
      "\tspeed: 0.1393s/iter; left time: 1018.6471s\n",
      "\titers: 300, epoch: 3 | loss: 0.2400482\n",
      "\tspeed: 0.1407s/iter; left time: 1014.7897s\n",
      "\titers: 400, epoch: 3 | loss: 0.3041089\n",
      "\tspeed: 0.1400s/iter; left time: 995.9157s\n",
      "\titers: 500, epoch: 3 | loss: 0.2837020\n",
      "\tspeed: 0.1401s/iter; left time: 982.4556s\n",
      "\titers: 600, epoch: 3 | loss: 0.2644754\n",
      "\tspeed: 0.1411s/iter; left time: 975.5524s\n",
      "\titers: 700, epoch: 3 | loss: 0.3089007\n",
      "\tspeed: 0.1396s/iter; left time: 950.9849s\n",
      "\titers: 800, epoch: 3 | loss: 0.2851648\n",
      "\tspeed: 0.1382s/iter; left time: 927.5252s\n",
      "\titers: 900, epoch: 3 | loss: 0.3176130\n",
      "\tspeed: 0.1396s/iter; left time: 922.9354s\n",
      "Epoch: 3 running time: 2.2034197449684143 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.3012448 Vali Loss: 0.4048612 Test Loss: 0.8435549\n",
      "Validation loss decreased (0.408841 --> 0.404861).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3126360\n",
      "\tspeed: 0.4488s/iter; left time: 2905.2492s\n",
      "\titers: 200, epoch: 4 | loss: 0.2761378\n",
      "\tspeed: 0.1397s/iter; left time: 890.4685s\n",
      "\titers: 300, epoch: 4 | loss: 0.2534401\n",
      "\tspeed: 0.1393s/iter; left time: 874.2481s\n",
      "\titers: 400, epoch: 4 | loss: 0.2833184\n",
      "\tspeed: 0.1392s/iter; left time: 859.1570s\n",
      "\titers: 500, epoch: 4 | loss: 0.2574418\n",
      "\tspeed: 0.1420s/iter; left time: 862.5534s\n",
      "\titers: 600, epoch: 4 | loss: 0.3066044\n",
      "\tspeed: 0.1409s/iter; left time: 841.6172s\n",
      "\titers: 700, epoch: 4 | loss: 0.2388677\n",
      "\tspeed: 0.1402s/iter; left time: 823.3381s\n",
      "\titers: 800, epoch: 4 | loss: 0.3113361\n",
      "\tspeed: 0.1442s/iter; left time: 832.4139s\n",
      "\titers: 900, epoch: 4 | loss: 0.2502302\n",
      "\tspeed: 0.1397s/iter; left time: 792.5656s\n",
      "Epoch: 4 running time: 2.219559836387634 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2669175 Vali Loss: 0.4331120 Test Loss: 1.0065913\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2234550\n",
      "\tspeed: 0.4376s/iter; left time: 2421.9264s\n",
      "\titers: 200, epoch: 5 | loss: 0.2504027\n",
      "\tspeed: 0.1402s/iter; left time: 761.9013s\n",
      "\titers: 300, epoch: 5 | loss: 0.2850033\n",
      "\tspeed: 0.1402s/iter; left time: 747.9530s\n",
      "\titers: 400, epoch: 5 | loss: 0.2300142\n",
      "\tspeed: 0.1412s/iter; left time: 739.3918s\n",
      "\titers: 500, epoch: 5 | loss: 0.3138312\n",
      "\tspeed: 0.1385s/iter; left time: 711.2291s\n",
      "\titers: 600, epoch: 5 | loss: 0.2654854\n",
      "\tspeed: 0.1407s/iter; left time: 708.5236s\n",
      "\titers: 700, epoch: 5 | loss: 0.2302616\n",
      "\tspeed: 0.1395s/iter; left time: 688.4132s\n",
      "\titers: 800, epoch: 5 | loss: 0.2805905\n",
      "\tspeed: 0.1417s/iter; left time: 685.3270s\n",
      "\titers: 900, epoch: 5 | loss: 0.2521963\n",
      "\tspeed: 0.1404s/iter; left time: 664.9336s\n",
      "Epoch: 5 running time: 2.2065264344215394 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2646839 Vali Loss: 0.4230137 Test Loss: 0.8850275\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2957079\n",
      "\tspeed: 0.4317s/iter; left time: 1984.0570s\n",
      "\titers: 200, epoch: 6 | loss: 0.3131303\n",
      "\tspeed: 0.1399s/iter; left time: 629.1768s\n",
      "\titers: 300, epoch: 6 | loss: 0.2958193\n",
      "\tspeed: 0.1406s/iter; left time: 617.9272s\n",
      "\titers: 400, epoch: 6 | loss: 0.2801335\n",
      "\tspeed: 0.1410s/iter; left time: 605.6425s\n",
      "\titers: 500, epoch: 6 | loss: 0.3014933\n",
      "\tspeed: 0.1430s/iter; left time: 599.9526s\n",
      "\titers: 600, epoch: 6 | loss: 0.2947007\n",
      "\tspeed: 0.1410s/iter; left time: 577.6578s\n",
      "\titers: 700, epoch: 6 | loss: 0.2537886\n",
      "\tspeed: 0.1398s/iter; left time: 558.7022s\n",
      "\titers: 800, epoch: 6 | loss: 0.2679251\n",
      "\tspeed: 0.1432s/iter; left time: 557.7946s\n",
      "\titers: 900, epoch: 6 | loss: 0.2741717\n",
      "\tspeed: 0.1432s/iter; left time: 543.7630s\n",
      "Epoch: 6 running time: 2.222713355223338 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2659598 Vali Loss: 0.4145777 Test Loss: 0.8884851\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8879987597465515, mae:0.6414443850517273\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6590257\n",
      "\tspeed: 0.1540s/iter; left time: 1430.3967s\n",
      "\titers: 200, epoch: 1 | loss: 0.3702555\n",
      "\tspeed: 0.1402s/iter; left time: 1288.6240s\n",
      "\titers: 300, epoch: 1 | loss: 0.4171233\n",
      "\tspeed: 0.1432s/iter; left time: 1301.4684s\n",
      "\titers: 400, epoch: 1 | loss: 0.3742107\n",
      "\tspeed: 0.1471s/iter; left time: 1322.2488s\n",
      "\titers: 500, epoch: 1 | loss: 0.3631184\n",
      "\tspeed: 0.1467s/iter; left time: 1304.4701s\n",
      "\titers: 600, epoch: 1 | loss: 0.2829784\n",
      "\tspeed: 0.1388s/iter; left time: 1220.2653s\n",
      "\titers: 700, epoch: 1 | loss: 0.3232366\n",
      "\tspeed: 0.1394s/iter; left time: 1211.3498s\n",
      "\titers: 800, epoch: 1 | loss: 0.3703890\n",
      "\tspeed: 0.1425s/iter; left time: 1224.0942s\n",
      "\titers: 900, epoch: 1 | loss: 0.2851022\n",
      "\tspeed: 0.1374s/iter; left time: 1166.7489s\n",
      "Epoch: 1 running time: 2.2432622353235883 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4290918 Vali Loss: 0.4069749 Test Loss: 0.7548954\n",
      "Validation loss decreased (inf --> 0.406975).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3553617\n",
      "\tspeed: 0.4588s/iter; left time: 3831.4990s\n",
      "\titers: 200, epoch: 2 | loss: 0.2649492\n",
      "\tspeed: 0.1401s/iter; left time: 1155.9827s\n",
      "\titers: 300, epoch: 2 | loss: 0.2963114\n",
      "\tspeed: 0.1368s/iter; left time: 1115.5835s\n",
      "\titers: 400, epoch: 2 | loss: 0.2734796\n",
      "\tspeed: 0.1466s/iter; left time: 1180.1271s\n",
      "\titers: 500, epoch: 2 | loss: 0.3173178\n",
      "\tspeed: 0.1399s/iter; left time: 1112.7175s\n",
      "\titers: 600, epoch: 2 | loss: 0.2838234\n",
      "\tspeed: 0.1377s/iter; left time: 1081.5540s\n",
      "\titers: 700, epoch: 2 | loss: 0.2560588\n",
      "\tspeed: 0.1445s/iter; left time: 1120.5213s\n",
      "\titers: 800, epoch: 2 | loss: 0.2681250\n",
      "\tspeed: 0.1469s/iter; left time: 1123.8332s\n",
      "\titers: 900, epoch: 2 | loss: 0.2787300\n",
      "\tspeed: 0.1374s/iter; left time: 1037.8378s\n",
      "Epoch: 2 running time: 2.239998916784922 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.3078535 Vali Loss: 0.3890424 Test Loss: 0.8557831\n",
      "Validation loss decreased (0.406975 --> 0.389042).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2971739\n",
      "\tspeed: 0.4558s/iter; left time: 3378.7915s\n",
      "\titers: 200, epoch: 3 | loss: 0.2232050\n",
      "\tspeed: 0.1425s/iter; left time: 1042.1609s\n",
      "\titers: 300, epoch: 3 | loss: 0.2377447\n",
      "\tspeed: 0.1440s/iter; left time: 1038.6305s\n",
      "\titers: 400, epoch: 3 | loss: 0.3415096\n",
      "\tspeed: 0.1350s/iter; left time: 960.1742s\n",
      "\titers: 500, epoch: 3 | loss: 0.2730391\n",
      "\tspeed: 0.1402s/iter; left time: 983.0645s\n",
      "\titers: 600, epoch: 3 | loss: 0.2811303\n",
      "\tspeed: 0.1418s/iter; left time: 979.9586s\n",
      "\titers: 700, epoch: 3 | loss: 0.2590662\n",
      "\tspeed: 0.1360s/iter; left time: 926.3029s\n",
      "\titers: 800, epoch: 3 | loss: 0.1946242\n",
      "\tspeed: 0.1409s/iter; left time: 945.5983s\n",
      "\titers: 900, epoch: 3 | loss: 0.2446160\n",
      "\tspeed: 0.1362s/iter; left time: 900.4667s\n",
      "Epoch: 3 running time: 2.2029984553654987 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2659981 Vali Loss: 0.3890681 Test Loss: 0.9074225\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2502083\n",
      "\tspeed: 0.4430s/iter; left time: 2868.2687s\n",
      "\titers: 200, epoch: 4 | loss: 0.2271685\n",
      "\tspeed: 0.1435s/iter; left time: 914.8559s\n",
      "\titers: 300, epoch: 4 | loss: 0.2457022\n",
      "\tspeed: 0.1431s/iter; left time: 897.7638s\n",
      "\titers: 400, epoch: 4 | loss: 0.2382278\n",
      "\tspeed: 0.1447s/iter; left time: 893.3683s\n",
      "\titers: 500, epoch: 4 | loss: 0.2423605\n",
      "\tspeed: 0.1442s/iter; left time: 875.6035s\n",
      "\titers: 600, epoch: 4 | loss: 0.2446430\n",
      "\tspeed: 0.1435s/iter; left time: 857.5013s\n",
      "\titers: 700, epoch: 4 | loss: 0.2296289\n",
      "\tspeed: 0.1444s/iter; left time: 848.2364s\n",
      "\titers: 800, epoch: 4 | loss: 0.2783559\n",
      "\tspeed: 0.1475s/iter; left time: 851.8250s\n",
      "\titers: 900, epoch: 4 | loss: 0.2721460\n",
      "\tspeed: 0.1400s/iter; left time: 794.0796s\n",
      "Epoch: 4 running time: 2.2713362097740175 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2610680 Vali Loss: 0.3909471 Test Loss: 0.8500285\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2504730\n",
      "\tspeed: 0.4500s/iter; left time: 2490.9330s\n",
      "\titers: 200, epoch: 5 | loss: 0.3105081\n",
      "\tspeed: 0.1407s/iter; left time: 764.5485s\n",
      "\titers: 300, epoch: 5 | loss: 0.2403706\n",
      "\tspeed: 0.1383s/iter; left time: 737.8944s\n",
      "\titers: 400, epoch: 5 | loss: 0.2308073\n",
      "\tspeed: 0.1405s/iter; left time: 735.2826s\n",
      "\titers: 500, epoch: 5 | loss: 0.2627726\n",
      "\tspeed: 0.1435s/iter; left time: 736.6715s\n",
      "\titers: 600, epoch: 5 | loss: 0.2978262\n",
      "\tspeed: 0.1369s/iter; left time: 689.5352s\n",
      "\titers: 700, epoch: 5 | loss: 0.2063510\n",
      "\tspeed: 0.1367s/iter; left time: 674.4606s\n",
      "\titers: 800, epoch: 5 | loss: 0.2197400\n",
      "\tspeed: 0.1365s/iter; left time: 659.7470s\n",
      "\titers: 900, epoch: 5 | loss: 0.2552235\n",
      "\tspeed: 0.1409s/iter; left time: 667.3166s\n",
      "Epoch: 5 running time: 2.1991379578908283 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2610565 Vali Loss: 0.3817384 Test Loss: 0.8476037\n",
      "Validation loss decreased (0.389042 --> 0.381738).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2394448\n",
      "\tspeed: 0.4457s/iter; left time: 2048.3682s\n",
      "\titers: 200, epoch: 6 | loss: 0.2661604\n",
      "\tspeed: 0.1375s/iter; left time: 618.2999s\n",
      "\titers: 300, epoch: 6 | loss: 0.2508234\n",
      "\tspeed: 0.1412s/iter; left time: 620.5033s\n",
      "\titers: 400, epoch: 6 | loss: 0.2772815\n",
      "\tspeed: 0.1421s/iter; left time: 610.6115s\n",
      "\titers: 500, epoch: 6 | loss: 0.2164235\n",
      "\tspeed: 0.1382s/iter; left time: 579.9061s\n",
      "\titers: 600, epoch: 6 | loss: 0.2276945\n",
      "\tspeed: 0.1383s/iter; left time: 566.3795s\n",
      "\titers: 700, epoch: 6 | loss: 0.2581926\n",
      "\tspeed: 0.1408s/iter; left time: 562.7083s\n",
      "\titers: 800, epoch: 6 | loss: 0.2069966\n",
      "\tspeed: 0.1406s/iter; left time: 547.9052s\n",
      "\titers: 900, epoch: 6 | loss: 0.2439662\n",
      "\tspeed: 0.1433s/iter; left time: 543.9857s\n",
      "Epoch: 6 running time: 2.2085657795270284 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2476775 Vali Loss: 0.3693107 Test Loss: 0.8297454\n",
      "Validation loss decreased (0.381738 --> 0.369311).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2184741\n",
      "\tspeed: 0.4467s/iter; left time: 1633.4301s\n",
      "\titers: 200, epoch: 7 | loss: 0.3154837\n",
      "\tspeed: 0.1366s/iter; left time: 485.8426s\n",
      "\titers: 300, epoch: 7 | loss: 0.2282375\n",
      "\tspeed: 0.1372s/iter; left time: 474.1890s\n",
      "\titers: 400, epoch: 7 | loss: 0.1981492\n",
      "\tspeed: 0.1371s/iter; left time: 460.0910s\n",
      "\titers: 500, epoch: 7 | loss: 0.2087907\n",
      "\tspeed: 0.1366s/iter; left time: 444.7809s\n",
      "\titers: 600, epoch: 7 | loss: 0.2624970\n",
      "\tspeed: 0.1365s/iter; left time: 431.0129s\n",
      "\titers: 700, epoch: 7 | loss: 0.2519007\n",
      "\tspeed: 0.1359s/iter; left time: 415.4796s\n",
      "\titers: 800, epoch: 7 | loss: 0.2490519\n",
      "\tspeed: 0.1362s/iter; left time: 402.7780s\n",
      "\titers: 900, epoch: 7 | loss: 0.2527945\n",
      "\tspeed: 0.1364s/iter; left time: 389.5728s\n",
      "Epoch: 7 running time: 2.159076205889384 min.\n",
      "Epoch: 7, Steps: 939 | Train Loss: 0.2412477 Vali Loss: 0.3735014 Test Loss: 0.8222224\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2008697\n",
      "\tspeed: 0.4399s/iter; left time: 1195.5257s\n",
      "\titers: 200, epoch: 8 | loss: 0.2527851\n",
      "\tspeed: 0.1390s/iter; left time: 363.7904s\n",
      "\titers: 300, epoch: 8 | loss: 0.2227888\n",
      "\tspeed: 0.1411s/iter; left time: 355.4098s\n",
      "\titers: 400, epoch: 8 | loss: 0.2456555\n",
      "\tspeed: 0.1400s/iter; left time: 338.4589s\n",
      "\titers: 500, epoch: 8 | loss: 0.2695578\n",
      "\tspeed: 0.1425s/iter; left time: 330.2664s\n",
      "\titers: 600, epoch: 8 | loss: 0.2271018\n",
      "\tspeed: 0.1366s/iter; left time: 302.9049s\n",
      "\titers: 700, epoch: 8 | loss: 0.2245621\n",
      "\tspeed: 0.1393s/iter; left time: 295.0472s\n",
      "\titers: 800, epoch: 8 | loss: 0.2599676\n",
      "\tspeed: 0.1362s/iter; left time: 274.8489s\n",
      "\titers: 900, epoch: 8 | loss: 0.2144273\n",
      "\tspeed: 0.1383s/iter; left time: 265.1671s\n",
      "Epoch: 8 running time: 2.1972790042559307 min.\n",
      "Epoch: 8, Steps: 939 | Train Loss: 0.2411348 Vali Loss: 0.3763058 Test Loss: 0.8331358\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2465238\n",
      "\tspeed: 0.4310s/iter; left time: 766.8084s\n",
      "\titers: 200, epoch: 9 | loss: 0.2228289\n",
      "\tspeed: 0.1389s/iter; left time: 233.1527s\n",
      "\titers: 300, epoch: 9 | loss: 0.2397258\n",
      "\tspeed: 0.1385s/iter; left time: 218.7423s\n",
      "\titers: 400, epoch: 9 | loss: 0.2728091\n",
      "\tspeed: 0.1412s/iter; left time: 208.8150s\n",
      "\titers: 500, epoch: 9 | loss: 0.2611856\n",
      "\tspeed: 0.1383s/iter; left time: 190.7440s\n",
      "\titers: 600, epoch: 9 | loss: 0.2147598\n",
      "\tspeed: 0.1396s/iter; left time: 178.5800s\n",
      "\titers: 700, epoch: 9 | loss: 0.2725378\n",
      "\tspeed: 0.1358s/iter; left time: 160.1405s\n",
      "\titers: 800, epoch: 9 | loss: 0.2642465\n",
      "\tspeed: 0.1361s/iter; left time: 146.8887s\n",
      "\titers: 900, epoch: 9 | loss: 0.2390002\n",
      "\tspeed: 0.1391s/iter; left time: 136.1618s\n",
      "Epoch: 9 running time: 2.1794423898061117 min.\n",
      "Epoch: 9, Steps: 939 | Train Loss: 0.2408846 Vali Loss: 0.3721567 Test Loss: 0.8406001\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_ES_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.8414180278778076, mae:0.631447970867157\n",
      "\n",
      "Time intermediate for ES dataset: 39.92294031381607 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6563620\n",
      "\tspeed: 0.1674s/iter; left time: 1555.0914s\n",
      "\titers: 200, epoch: 1 | loss: 0.4686041\n",
      "\tspeed: 0.1384s/iter; left time: 1271.6980s\n",
      "\titers: 300, epoch: 1 | loss: 0.3708035\n",
      "\tspeed: 0.1358s/iter; left time: 1234.2579s\n",
      "\titers: 400, epoch: 1 | loss: 0.3613045\n",
      "\tspeed: 0.1389s/iter; left time: 1248.5929s\n",
      "\titers: 500, epoch: 1 | loss: 0.3285453\n",
      "\tspeed: 0.1395s/iter; left time: 1239.9522s\n",
      "\titers: 600, epoch: 1 | loss: 0.2799257\n",
      "\tspeed: 0.1395s/iter; left time: 1226.3679s\n",
      "\titers: 700, epoch: 1 | loss: 0.3656779\n",
      "\tspeed: 0.1388s/iter; left time: 1206.3294s\n",
      "\titers: 800, epoch: 1 | loss: 0.3854206\n",
      "\tspeed: 0.1404s/iter; left time: 1205.7575s\n",
      "\titers: 900, epoch: 1 | loss: 0.2973940\n",
      "\tspeed: 0.1401s/iter; left time: 1189.2675s\n",
      "Epoch: 1 running time: 2.206984492142995 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.3997275 Vali Loss: 0.5399036 Test Loss: 0.7632753\n",
      "Validation loss decreased (inf --> 0.539904).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4104810\n",
      "\tspeed: 0.4422s/iter; left time: 3693.4573s\n",
      "\titers: 200, epoch: 2 | loss: 0.2268605\n",
      "\tspeed: 0.1400s/iter; left time: 1155.3717s\n",
      "\titers: 300, epoch: 2 | loss: 0.2820555\n",
      "\tspeed: 0.1395s/iter; left time: 1137.0876s\n",
      "\titers: 400, epoch: 2 | loss: 0.3208773\n",
      "\tspeed: 0.1405s/iter; left time: 1131.5403s\n",
      "\titers: 500, epoch: 2 | loss: 0.3381982\n",
      "\tspeed: 0.1401s/iter; left time: 1113.9469s\n",
      "\titers: 600, epoch: 2 | loss: 0.3214974\n",
      "\tspeed: 0.1408s/iter; left time: 1105.4056s\n",
      "\titers: 700, epoch: 2 | loss: 0.3000483\n",
      "\tspeed: 0.1380s/iter; left time: 1069.9422s\n",
      "\titers: 800, epoch: 2 | loss: 0.2663770\n",
      "\tspeed: 0.1399s/iter; left time: 1070.4070s\n",
      "\titers: 900, epoch: 2 | loss: 0.2478843\n",
      "\tspeed: 0.1402s/iter; left time: 1058.5191s\n",
      "Epoch: 2 running time: 2.2030788103739423 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2903142 Vali Loss: 0.5282452 Test Loss: 0.7457845\n",
      "Validation loss decreased (0.539904 --> 0.528245).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2929942\n",
      "\tspeed: 0.4413s/iter; left time: 3271.4612s\n",
      "\titers: 200, epoch: 3 | loss: 0.2370470\n",
      "\tspeed: 0.1418s/iter; left time: 1036.8606s\n",
      "\titers: 300, epoch: 3 | loss: 0.2487708\n",
      "\tspeed: 0.1406s/iter; left time: 1014.1970s\n",
      "\titers: 400, epoch: 3 | loss: 0.2633350\n",
      "\tspeed: 0.1398s/iter; left time: 994.5472s\n",
      "\titers: 500, epoch: 3 | loss: 0.1461894\n",
      "\tspeed: 0.1399s/iter; left time: 981.1630s\n",
      "\titers: 600, epoch: 3 | loss: 0.2201150\n",
      "\tspeed: 0.1379s/iter; left time: 953.5608s\n",
      "\titers: 700, epoch: 3 | loss: 0.2316150\n",
      "\tspeed: 0.1388s/iter; left time: 945.7094s\n",
      "\titers: 800, epoch: 3 | loss: 0.2849599\n",
      "\tspeed: 0.1389s/iter; left time: 932.1178s\n",
      "\titers: 900, epoch: 3 | loss: 0.2500898\n",
      "\tspeed: 0.1397s/iter; left time: 924.1431s\n",
      "Epoch: 3 running time: 2.2018173813819883 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2413982 Vali Loss: 0.5426540 Test Loss: 0.7224922\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3003728\n",
      "\tspeed: 0.4356s/iter; left time: 2819.9751s\n",
      "\titers: 200, epoch: 4 | loss: 0.2473937\n",
      "\tspeed: 0.1405s/iter; left time: 895.6495s\n",
      "\titers: 300, epoch: 4 | loss: 0.2153327\n",
      "\tspeed: 0.1410s/iter; left time: 884.3825s\n",
      "\titers: 400, epoch: 4 | loss: 0.2618111\n",
      "\tspeed: 0.1391s/iter; left time: 858.6186s\n",
      "\titers: 500, epoch: 4 | loss: 0.2740194\n",
      "\tspeed: 0.1391s/iter; left time: 844.6286s\n",
      "\titers: 600, epoch: 4 | loss: 0.2745269\n",
      "\tspeed: 0.1390s/iter; left time: 830.4832s\n",
      "\titers: 700, epoch: 4 | loss: 0.2463578\n",
      "\tspeed: 0.1376s/iter; left time: 808.3733s\n",
      "\titers: 800, epoch: 4 | loss: 0.2360248\n",
      "\tspeed: 0.1384s/iter; left time: 798.9557s\n",
      "\titers: 900, epoch: 4 | loss: 0.2328671\n",
      "\tspeed: 0.1368s/iter; left time: 776.1307s\n",
      "Epoch: 4 running time: 2.1919012308120727 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2370536 Vali Loss: 0.5345448 Test Loss: 0.6924434\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2533449\n",
      "\tspeed: 0.4374s/iter; left time: 2420.7736s\n",
      "\titers: 200, epoch: 5 | loss: 0.2096524\n",
      "\tspeed: 0.1401s/iter; left time: 761.2294s\n",
      "\titers: 300, epoch: 5 | loss: 0.3028490\n",
      "\tspeed: 0.1401s/iter; left time: 747.2516s\n",
      "\titers: 400, epoch: 5 | loss: 0.2703303\n",
      "\tspeed: 0.1408s/iter; left time: 737.2684s\n",
      "\titers: 500, epoch: 5 | loss: 0.2281400\n",
      "\tspeed: 0.1392s/iter; left time: 714.6591s\n",
      "\titers: 600, epoch: 5 | loss: 0.2631753\n",
      "\tspeed: 0.1387s/iter; left time: 698.4895s\n",
      "\titers: 700, epoch: 5 | loss: 0.2335026\n",
      "\tspeed: 0.1397s/iter; left time: 689.2162s\n",
      "\titers: 800, epoch: 5 | loss: 0.2424679\n",
      "\tspeed: 0.1409s/iter; left time: 681.1555s\n",
      "\titers: 900, epoch: 5 | loss: 0.2382910\n",
      "\tspeed: 0.1368s/iter; left time: 647.8955s\n",
      "Epoch: 5 running time: 2.1999444087346394 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2377859 Vali Loss: 0.5187750 Test Loss: 0.6752720\n",
      "Validation loss decreased (0.528245 --> 0.518775).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2202763\n",
      "\tspeed: 0.4607s/iter; left time: 2117.1892s\n",
      "\titers: 200, epoch: 6 | loss: 0.2050813\n",
      "\tspeed: 0.1411s/iter; left time: 634.3307s\n",
      "\titers: 300, epoch: 6 | loss: 0.2522383\n",
      "\tspeed: 0.1420s/iter; left time: 624.1635s\n",
      "\titers: 400, epoch: 6 | loss: 0.2316654\n",
      "\tspeed: 0.1408s/iter; left time: 604.8102s\n",
      "\titers: 500, epoch: 6 | loss: 0.2543027\n",
      "\tspeed: 0.1411s/iter; left time: 591.8874s\n",
      "\titers: 600, epoch: 6 | loss: 0.2295710\n",
      "\tspeed: 0.1395s/iter; left time: 571.3195s\n",
      "\titers: 700, epoch: 6 | loss: 0.2682272\n",
      "\tspeed: 0.1404s/iter; left time: 560.8702s\n",
      "\titers: 800, epoch: 6 | loss: 0.2612667\n",
      "\tspeed: 0.1389s/iter; left time: 540.9805s\n",
      "\titers: 900, epoch: 6 | loss: 0.2914984\n",
      "\tspeed: 0.1379s/iter; left time: 523.6343s\n",
      "Epoch: 6 running time: 2.2105549216270446 min.\n",
      "Epoch: 6, Steps: 939 | Train Loss: 0.2227965 Vali Loss: 0.5414836 Test Loss: 0.6977964\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2085455\n",
      "\tspeed: 0.4369s/iter; left time: 1597.7668s\n",
      "\titers: 200, epoch: 7 | loss: 0.2083051\n",
      "\tspeed: 0.1390s/iter; left time: 494.3737s\n",
      "\titers: 300, epoch: 7 | loss: 0.1986328\n",
      "\tspeed: 0.1396s/iter; left time: 482.5796s\n",
      "\titers: 400, epoch: 7 | loss: 0.2117108\n",
      "\tspeed: 0.1416s/iter; left time: 475.3209s\n",
      "\titers: 500, epoch: 7 | loss: 0.2330434\n",
      "\tspeed: 0.1414s/iter; left time: 460.4238s\n",
      "\titers: 600, epoch: 7 | loss: 0.2388510\n",
      "\tspeed: 0.1406s/iter; left time: 443.9471s\n",
      "\titers: 700, epoch: 7 | loss: 0.2107410\n",
      "\tspeed: 0.1409s/iter; left time: 430.7160s\n",
      "\titers: 800, epoch: 7 | loss: 0.2191927\n",
      "\tspeed: 0.1377s/iter; left time: 407.2050s\n",
      "\titers: 900, epoch: 7 | loss: 0.2265030\n",
      "\tspeed: 0.1405s/iter; left time: 401.5122s\n",
      "Epoch: 7 running time: 2.208251440525055 min.\n",
      "Epoch: 7, Steps: 939 | Train Loss: 0.2223657 Vali Loss: 0.5449157 Test Loss: 0.7076946\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2023351\n",
      "\tspeed: 0.4311s/iter; left time: 1171.6223s\n",
      "\titers: 200, epoch: 8 | loss: 0.2216889\n",
      "\tspeed: 0.1413s/iter; left time: 369.9353s\n",
      "\titers: 300, epoch: 8 | loss: 0.2495725\n",
      "\tspeed: 0.1380s/iter; left time: 347.5564s\n",
      "\titers: 400, epoch: 8 | loss: 0.2495461\n",
      "\tspeed: 0.1401s/iter; left time: 338.7658s\n",
      "\titers: 500, epoch: 8 | loss: 0.2631519\n",
      "\tspeed: 0.1419s/iter; left time: 328.8136s\n",
      "\titers: 600, epoch: 8 | loss: 0.2328438\n",
      "\tspeed: 0.1412s/iter; left time: 313.1662s\n",
      "\titers: 700, epoch: 8 | loss: 0.2333448\n",
      "\tspeed: 0.1403s/iter; left time: 297.0796s\n",
      "\titers: 800, epoch: 8 | loss: 0.1921285\n",
      "\tspeed: 0.1395s/iter; left time: 281.4139s\n",
      "\titers: 900, epoch: 8 | loss: 0.2323905\n",
      "\tspeed: 0.1389s/iter; left time: 266.4883s\n",
      "Epoch: 8 running time: 2.203486100832621 min.\n",
      "Epoch: 8, Steps: 939 | Train Loss: 0.2228775 Vali Loss: 0.5344476 Test Loss: 0.6950021\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6943550109863281, mae:0.4968421161174774\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.5194973\n",
      "\tspeed: 0.1517s/iter; left time: 1409.3856s\n",
      "\titers: 200, epoch: 1 | loss: 0.3787932\n",
      "\tspeed: 0.1408s/iter; left time: 1294.3423s\n",
      "\titers: 300, epoch: 1 | loss: 0.3483550\n",
      "\tspeed: 0.1370s/iter; left time: 1245.6264s\n",
      "\titers: 400, epoch: 1 | loss: 0.2645572\n",
      "\tspeed: 0.1428s/iter; left time: 1283.6248s\n",
      "\titers: 500, epoch: 1 | loss: 0.3132783\n",
      "\tspeed: 0.1416s/iter; left time: 1258.6913s\n",
      "\titers: 600, epoch: 1 | loss: 0.3135720\n",
      "\tspeed: 0.1403s/iter; left time: 1232.9624s\n",
      "\titers: 700, epoch: 1 | loss: 0.3039061\n",
      "\tspeed: 0.1400s/iter; left time: 1216.9942s\n",
      "\titers: 800, epoch: 1 | loss: 0.2903324\n",
      "\tspeed: 0.1400s/iter; left time: 1202.5081s\n",
      "\titers: 900, epoch: 1 | loss: 0.3246081\n",
      "\tspeed: 0.1385s/iter; left time: 1175.8918s\n",
      "Epoch: 1 running time: 2.214100646972656 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4004633 Vali Loss: 0.4897426 Test Loss: 0.6839262\n",
      "Validation loss decreased (inf --> 0.489743).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2558275\n",
      "\tspeed: 0.4448s/iter; left time: 3714.7527s\n",
      "\titers: 200, epoch: 2 | loss: 0.3506505\n",
      "\tspeed: 0.1381s/iter; left time: 1139.2948s\n",
      "\titers: 300, epoch: 2 | loss: 0.2810801\n",
      "\tspeed: 0.1382s/iter; left time: 1126.3191s\n",
      "\titers: 400, epoch: 2 | loss: 0.2821742\n",
      "\tspeed: 0.1375s/iter; left time: 1106.9653s\n",
      "\titers: 500, epoch: 2 | loss: 0.2878427\n",
      "\tspeed: 0.1348s/iter; left time: 1071.8114s\n",
      "\titers: 600, epoch: 2 | loss: 0.2729373\n",
      "\tspeed: 0.1392s/iter; left time: 1092.8107s\n",
      "\titers: 700, epoch: 2 | loss: 0.2269158\n",
      "\tspeed: 0.1415s/iter; left time: 1096.7202s\n",
      "\titers: 800, epoch: 2 | loss: 0.2657708\n",
      "\tspeed: 0.1378s/iter; left time: 1054.5234s\n",
      "\titers: 900, epoch: 2 | loss: 0.2409639\n",
      "\tspeed: 0.1340s/iter; left time: 1011.6225s\n",
      "Epoch: 2 running time: 2.169920976956685 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2915593 Vali Loss: 0.5546362 Test Loss: 0.8071940\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2414199\n",
      "\tspeed: 0.4344s/iter; left time: 3220.2482s\n",
      "\titers: 200, epoch: 3 | loss: 0.2964270\n",
      "\tspeed: 0.1371s/iter; left time: 1002.9717s\n",
      "\titers: 300, epoch: 3 | loss: 0.2811421\n",
      "\tspeed: 0.1385s/iter; left time: 999.3090s\n",
      "\titers: 400, epoch: 3 | loss: 0.2909580\n",
      "\tspeed: 0.1381s/iter; left time: 982.0182s\n",
      "\titers: 500, epoch: 3 | loss: 0.2352519\n",
      "\tspeed: 0.1412s/iter; left time: 990.4410s\n",
      "\titers: 600, epoch: 3 | loss: 0.2775724\n",
      "\tspeed: 0.1423s/iter; left time: 983.5839s\n",
      "\titers: 700, epoch: 3 | loss: 0.1741922\n",
      "\tspeed: 0.1404s/iter; left time: 956.5445s\n",
      "\titers: 800, epoch: 3 | loss: 0.2339003\n",
      "\tspeed: 0.1403s/iter; left time: 942.1356s\n",
      "\titers: 900, epoch: 3 | loss: 0.1933226\n",
      "\tspeed: 0.1401s/iter; left time: 926.3217s\n",
      "Epoch: 3 running time: 2.1988686164220175 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2799605 Vali Loss: 0.5221055 Test Loss: 0.6822422\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3156842\n",
      "\tspeed: 0.4365s/iter; left time: 2825.6946s\n",
      "\titers: 200, epoch: 4 | loss: 0.4080562\n",
      "\tspeed: 0.1421s/iter; left time: 905.4413s\n",
      "\titers: 300, epoch: 4 | loss: 0.3024582\n",
      "\tspeed: 0.1362s/iter; left time: 854.8051s\n",
      "\titers: 400, epoch: 4 | loss: 0.2646206\n",
      "\tspeed: 0.1404s/iter; left time: 867.0754s\n",
      "\titers: 500, epoch: 4 | loss: 0.2584136\n",
      "\tspeed: 0.1376s/iter; left time: 836.0600s\n",
      "\titers: 600, epoch: 4 | loss: 0.2142642\n",
      "\tspeed: 0.1433s/iter; left time: 856.1142s\n",
      "\titers: 700, epoch: 4 | loss: 0.3037489\n",
      "\tspeed: 0.1412s/iter; left time: 829.1181s\n",
      "\titers: 800, epoch: 4 | loss: 0.2205600\n",
      "\tspeed: 0.1419s/iter; left time: 819.5685s\n",
      "\titers: 900, epoch: 4 | loss: 0.2359709\n",
      "\tspeed: 0.1425s/iter; left time: 808.4580s\n",
      "Epoch: 4 running time: 2.2169278065363565 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2766858 Vali Loss: 0.5307667 Test Loss: 0.6847637\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_FR_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.6852361559867859, mae:0.5024503469467163\n",
      "\n",
      "Time intermediate for FR dataset: 31.90540871222814 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.6227161\n",
      "\tspeed: 0.1714s/iter; left time: 1592.6769s\n",
      "\titers: 200, epoch: 1 | loss: 0.4121668\n",
      "\tspeed: 0.1429s/iter; left time: 1313.6445s\n",
      "\titers: 300, epoch: 1 | loss: 0.4166707\n",
      "\tspeed: 0.1454s/iter; left time: 1322.2688s\n",
      "\titers: 400, epoch: 1 | loss: 0.3719393\n",
      "\tspeed: 0.1419s/iter; left time: 1275.5748s\n",
      "\titers: 500, epoch: 1 | loss: 0.3356986\n",
      "\tspeed: 0.1444s/iter; left time: 1284.0207s\n",
      "\titers: 600, epoch: 1 | loss: 0.3284096\n",
      "\tspeed: 0.1406s/iter; left time: 1236.3403s\n",
      "\titers: 700, epoch: 1 | loss: 0.2966726\n",
      "\tspeed: 0.1384s/iter; left time: 1202.9459s\n",
      "\titers: 800, epoch: 1 | loss: 0.3030324\n",
      "\tspeed: 0.1353s/iter; left time: 1162.4555s\n",
      "\titers: 900, epoch: 1 | loss: 0.3584232\n",
      "\tspeed: 0.1430s/iter; left time: 1214.0607s\n",
      "Epoch: 1 running time: 2.2436476310094196 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4087893 Vali Loss: 0.3208218 Test Loss: 0.3564519\n",
      "Validation loss decreased (inf --> 0.320822).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3594914\n",
      "\tspeed: 0.4400s/iter; left time: 3675.2460s\n",
      "\titers: 200, epoch: 2 | loss: 0.3209587\n",
      "\tspeed: 0.1390s/iter; left time: 1146.8103s\n",
      "\titers: 300, epoch: 2 | loss: 0.2314181\n",
      "\tspeed: 0.1374s/iter; left time: 1119.7428s\n",
      "\titers: 400, epoch: 2 | loss: 0.2764215\n",
      "\tspeed: 0.1396s/iter; left time: 1123.7218s\n",
      "\titers: 500, epoch: 2 | loss: 0.3469344\n",
      "\tspeed: 0.1368s/iter; left time: 1088.0891s\n",
      "\titers: 600, epoch: 2 | loss: 0.3072948\n",
      "\tspeed: 0.1395s/iter; left time: 1095.6213s\n",
      "\titers: 700, epoch: 2 | loss: 0.2812592\n",
      "\tspeed: 0.1408s/iter; left time: 1091.1085s\n",
      "\titers: 800, epoch: 2 | loss: 0.2970227\n",
      "\tspeed: 0.1356s/iter; left time: 1037.6718s\n",
      "\titers: 900, epoch: 2 | loss: 0.2384641\n",
      "\tspeed: 0.1380s/iter; left time: 1042.1270s\n",
      "Epoch: 2 running time: 2.178649445374807 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2926825 Vali Loss: 0.3184617 Test Loss: 0.3801476\n",
      "Validation loss decreased (0.320822 --> 0.318462).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3228781\n",
      "\tspeed: 0.4445s/iter; left time: 3295.3205s\n",
      "\titers: 200, epoch: 3 | loss: 0.2593555\n",
      "\tspeed: 0.1400s/iter; left time: 1023.8349s\n",
      "\titers: 300, epoch: 3 | loss: 0.2294986\n",
      "\tspeed: 0.1382s/iter; left time: 996.7540s\n",
      "\titers: 400, epoch: 3 | loss: 0.2674761\n",
      "\tspeed: 0.1380s/iter; left time: 981.5402s\n",
      "\titers: 500, epoch: 3 | loss: 0.2151335\n",
      "\tspeed: 0.1398s/iter; left time: 980.6495s\n",
      "\titers: 600, epoch: 3 | loss: 0.2301342\n",
      "\tspeed: 0.1421s/iter; left time: 982.6011s\n",
      "\titers: 700, epoch: 3 | loss: 0.2357649\n",
      "\tspeed: 0.1385s/iter; left time: 943.6485s\n",
      "\titers: 800, epoch: 3 | loss: 0.2914589\n",
      "\tspeed: 0.1387s/iter; left time: 931.2701s\n",
      "\titers: 900, epoch: 3 | loss: 0.2712146\n",
      "\tspeed: 0.1397s/iter; left time: 923.5762s\n",
      "Epoch: 3 running time: 2.194086746374766 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2538928 Vali Loss: 0.3415719 Test Loss: 0.3631514\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2648509\n",
      "\tspeed: 0.4263s/iter; left time: 2759.5860s\n",
      "\titers: 200, epoch: 4 | loss: 0.3001632\n",
      "\tspeed: 0.1398s/iter; left time: 890.7821s\n",
      "\titers: 300, epoch: 4 | loss: 0.2633336\n",
      "\tspeed: 0.1376s/iter; left time: 863.5992s\n",
      "\titers: 400, epoch: 4 | loss: 0.2083247\n",
      "\tspeed: 0.1370s/iter; left time: 846.0136s\n",
      "\titers: 500, epoch: 4 | loss: 0.2642303\n",
      "\tspeed: 0.1404s/iter; left time: 852.6747s\n",
      "\titers: 600, epoch: 4 | loss: 0.2964931\n",
      "\tspeed: 0.1403s/iter; left time: 837.8811s\n",
      "\titers: 700, epoch: 4 | loss: 0.2861311\n",
      "\tspeed: 0.1380s/iter; left time: 810.6668s\n",
      "\titers: 800, epoch: 4 | loss: 0.2230942\n",
      "\tspeed: 0.1396s/iter; left time: 805.9496s\n",
      "\titers: 900, epoch: 4 | loss: 0.2731301\n",
      "\tspeed: 0.1388s/iter; left time: 787.4943s\n",
      "Epoch: 4 running time: 2.1875528772672017 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2496940 Vali Loss: 0.3357561 Test Loss: 0.3612220\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2199131\n",
      "\tspeed: 0.4290s/iter; left time: 2374.4517s\n",
      "\titers: 200, epoch: 5 | loss: 0.2645657\n",
      "\tspeed: 0.1356s/iter; left time: 737.1837s\n",
      "\titers: 300, epoch: 5 | loss: 0.2802057\n",
      "\tspeed: 0.1391s/iter; left time: 741.9761s\n",
      "\titers: 400, epoch: 5 | loss: 0.2493426\n",
      "\tspeed: 0.1382s/iter; left time: 723.6955s\n",
      "\titers: 500, epoch: 5 | loss: 0.2088654\n",
      "\tspeed: 0.1373s/iter; left time: 704.8776s\n",
      "\titers: 600, epoch: 5 | loss: 0.2680191\n",
      "\tspeed: 0.1386s/iter; left time: 697.7127s\n",
      "\titers: 700, epoch: 5 | loss: 0.2745305\n",
      "\tspeed: 0.1386s/iter; left time: 683.7683s\n",
      "\titers: 800, epoch: 5 | loss: 0.2202419\n",
      "\tspeed: 0.1385s/iter; left time: 669.8887s\n",
      "\titers: 900, epoch: 5 | loss: 0.2623587\n",
      "\tspeed: 0.1376s/iter; left time: 651.7032s\n",
      "Epoch: 5 running time: 2.1724916815757753 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2499527 Vali Loss: 0.3221219 Test Loss: 0.3645081\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.3639913499355316, mae:0.38471370935440063\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30049\n",
      "val 6409\n",
      "test 6385\n",
      "\titers: 100, epoch: 1 | loss: 0.7966641\n",
      "\tspeed: 0.1484s/iter; left time: 1378.5856s\n",
      "\titers: 200, epoch: 1 | loss: 0.3873826\n",
      "\tspeed: 0.1380s/iter; left time: 1268.3089s\n",
      "\titers: 300, epoch: 1 | loss: 0.4218949\n",
      "\tspeed: 0.1399s/iter; left time: 1271.5682s\n",
      "\titers: 400, epoch: 1 | loss: 0.3321100\n",
      "\tspeed: 0.1426s/iter; left time: 1282.3060s\n",
      "\titers: 500, epoch: 1 | loss: 0.3451025\n",
      "\tspeed: 0.1379s/iter; left time: 1226.0612s\n",
      "\titers: 600, epoch: 1 | loss: 0.3968182\n",
      "\tspeed: 0.1398s/iter; left time: 1228.6496s\n",
      "\titers: 700, epoch: 1 | loss: 0.3482765\n",
      "\tspeed: 0.1416s/iter; left time: 1230.3871s\n",
      "\titers: 800, epoch: 1 | loss: 0.3170562\n",
      "\tspeed: 0.1409s/iter; left time: 1210.5799s\n",
      "\titers: 900, epoch: 1 | loss: 0.2420919\n",
      "\tspeed: 0.1403s/iter; left time: 1191.1086s\n",
      "Epoch: 1 running time: 2.21055801709493 min.\n",
      "Epoch: 1, Steps: 939 | Train Loss: 0.4115887 Vali Loss: 0.3162981 Test Loss: 0.3499801\n",
      "Validation loss decreased (inf --> 0.316298).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3015529\n",
      "\tspeed: 0.4558s/iter; left time: 3807.1432s\n",
      "\titers: 200, epoch: 2 | loss: 0.3497871\n",
      "\tspeed: 0.1373s/iter; left time: 1133.3251s\n",
      "\titers: 300, epoch: 2 | loss: 0.2655376\n",
      "\tspeed: 0.1403s/iter; left time: 1143.3432s\n",
      "\titers: 400, epoch: 2 | loss: 0.2785324\n",
      "\tspeed: 0.1387s/iter; left time: 1116.5246s\n",
      "\titers: 500, epoch: 2 | loss: 0.3417005\n",
      "\tspeed: 0.1391s/iter; left time: 1106.2981s\n",
      "\titers: 600, epoch: 2 | loss: 0.2623331\n",
      "\tspeed: 0.1398s/iter; left time: 1097.8681s\n",
      "\titers: 700, epoch: 2 | loss: 0.2547402\n",
      "\tspeed: 0.1406s/iter; left time: 1089.9397s\n",
      "\titers: 800, epoch: 2 | loss: 0.2743542\n",
      "\tspeed: 0.1386s/iter; left time: 1060.4185s\n",
      "\titers: 900, epoch: 2 | loss: 0.2962853\n",
      "\tspeed: 0.1386s/iter; left time: 1046.6518s\n",
      "Epoch: 2 running time: 2.1918689330418903 min.\n",
      "Epoch: 2, Steps: 939 | Train Loss: 0.2924323 Vali Loss: 0.3136936 Test Loss: 0.3552474\n",
      "Validation loss decreased (0.316298 --> 0.313694).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2595457\n",
      "\tspeed: 0.4480s/iter; left time: 3321.3322s\n",
      "\titers: 200, epoch: 3 | loss: 0.2478217\n",
      "\tspeed: 0.1409s/iter; left time: 1030.5593s\n",
      "\titers: 300, epoch: 3 | loss: 0.2165728\n",
      "\tspeed: 0.1400s/iter; left time: 1009.5229s\n",
      "\titers: 400, epoch: 3 | loss: 0.2474108\n",
      "\tspeed: 0.1409s/iter; left time: 1002.5241s\n",
      "\titers: 500, epoch: 3 | loss: 0.2387710\n",
      "\tspeed: 0.1397s/iter; left time: 979.5362s\n",
      "\titers: 600, epoch: 3 | loss: 0.1994736\n",
      "\tspeed: 0.1377s/iter; left time: 952.0312s\n",
      "\titers: 700, epoch: 3 | loss: 0.2538524\n",
      "\tspeed: 0.1390s/iter; left time: 947.2702s\n",
      "\titers: 800, epoch: 3 | loss: 0.2433887\n",
      "\tspeed: 0.1380s/iter; left time: 926.3635s\n",
      "\titers: 900, epoch: 3 | loss: 0.2416086\n",
      "\tspeed: 0.1403s/iter; left time: 927.6662s\n",
      "Epoch: 3 running time: 2.192528255780538 min.\n",
      "Epoch: 3, Steps: 939 | Train Loss: 0.2537659 Vali Loss: 0.3342690 Test Loss: 0.3709819\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2436643\n",
      "\tspeed: 0.4168s/iter; left time: 2698.0832s\n",
      "\titers: 200, epoch: 4 | loss: 0.2341034\n",
      "\tspeed: 0.1382s/iter; left time: 880.9555s\n",
      "\titers: 300, epoch: 4 | loss: 0.3043119\n",
      "\tspeed: 0.1358s/iter; left time: 851.7415s\n",
      "\titers: 400, epoch: 4 | loss: 0.2789731\n",
      "\tspeed: 0.1370s/iter; left time: 846.1302s\n",
      "\titers: 500, epoch: 4 | loss: 0.2909001\n",
      "\tspeed: 0.1357s/iter; left time: 824.2895s\n",
      "\titers: 600, epoch: 4 | loss: 0.2108449\n",
      "\tspeed: 0.1381s/iter; left time: 824.9227s\n",
      "\titers: 700, epoch: 4 | loss: 0.2683101\n",
      "\tspeed: 0.1352s/iter; left time: 794.4565s\n",
      "\titers: 800, epoch: 4 | loss: 0.2610053\n",
      "\tspeed: 0.1374s/iter; left time: 793.2041s\n",
      "\titers: 900, epoch: 4 | loss: 0.2484051\n",
      "\tspeed: 0.1390s/iter; left time: 788.6212s\n",
      "Epoch: 4 running time: 2.1612834771474203 min.\n",
      "Epoch: 4, Steps: 939 | Train Loss: 0.2505316 Vali Loss: 0.3268104 Test Loss: 0.3715681\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2038951\n",
      "\tspeed: 0.4239s/iter; left time: 2346.3402s\n",
      "\titers: 200, epoch: 5 | loss: 0.2467752\n",
      "\tspeed: 0.1326s/iter; left time: 720.8012s\n",
      "\titers: 300, epoch: 5 | loss: 0.2649160\n",
      "\tspeed: 0.1381s/iter; left time: 736.8723s\n",
      "\titers: 400, epoch: 5 | loss: 0.2298629\n",
      "\tspeed: 0.1375s/iter; left time: 719.7660s\n",
      "\titers: 500, epoch: 5 | loss: 0.2522527\n",
      "\tspeed: 0.1372s/iter; left time: 704.7742s\n",
      "\titers: 600, epoch: 5 | loss: 0.3205259\n",
      "\tspeed: 0.1386s/iter; left time: 697.9614s\n",
      "\titers: 700, epoch: 5 | loss: 0.2131537\n",
      "\tspeed: 0.1333s/iter; left time: 657.7814s\n",
      "\titers: 800, epoch: 5 | loss: 0.2275577\n",
      "\tspeed: 0.1384s/iter; left time: 668.9623s\n",
      "\titers: 900, epoch: 5 | loss: 0.2720839\n",
      "\tspeed: 0.1420s/iter; left time: 672.4355s\n",
      "Epoch: 5 running time: 2.160776138305664 min.\n",
      "Epoch: 5, Steps: 939 | Train Loss: 0.2503408 Vali Loss: 0.3258138 Test Loss: 0.3642627\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__96_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6385\n",
      "mse:0.3640507161617279, mae:0.3810804486274719\n",
      "\n",
      "Time intermediate for IT dataset: 26.532001849015554 min.\n",
      "Total time: 162.72137193282444 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"96\"\n",
    "model = \"Informer\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq len 96, pred len 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.7939370\n",
      "\tspeed: 0.2490s/iter; left time: 2286.3164s\n",
      "\titers: 200, epoch: 1 | loss: 0.5406578\n",
      "\tspeed: 0.2212s/iter; left time: 2008.8977s\n",
      "\titers: 300, epoch: 1 | loss: 0.3586418\n",
      "\tspeed: 0.2264s/iter; left time: 2033.1200s\n",
      "\titers: 400, epoch: 1 | loss: 0.2793235\n",
      "\tspeed: 0.2209s/iter; left time: 1962.1477s\n",
      "\titers: 500, epoch: 1 | loss: 0.3177501\n",
      "\tspeed: 0.2228s/iter; left time: 1956.5983s\n",
      "\titers: 600, epoch: 1 | loss: 0.3329939\n",
      "\tspeed: 0.2186s/iter; left time: 1897.9783s\n",
      "\titers: 700, epoch: 1 | loss: 0.3450849\n",
      "\tspeed: 0.2251s/iter; left time: 1931.8296s\n",
      "\titers: 800, epoch: 1 | loss: 0.2893311\n",
      "\tspeed: 0.2184s/iter; left time: 1852.6362s\n",
      "\titers: 900, epoch: 1 | loss: 0.3893343\n",
      "\tspeed: 0.2257s/iter; left time: 1891.5038s\n",
      "Epoch: 1 running time: 3.4559956192970276 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.4262286 Vali Loss: 0.4615504 Test Loss: 0.5835049\n",
      "Validation loss decreased (inf --> 0.461550).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3135547\n",
      "\tspeed: 0.7168s/iter; left time: 5915.7879s\n",
      "\titers: 200, epoch: 2 | loss: 0.2403071\n",
      "\tspeed: 0.2148s/iter; left time: 1750.9305s\n",
      "\titers: 300, epoch: 2 | loss: 0.2484401\n",
      "\tspeed: 0.2166s/iter; left time: 1744.6187s\n",
      "\titers: 400, epoch: 2 | loss: 0.2386029\n",
      "\tspeed: 0.2079s/iter; left time: 1653.5767s\n",
      "\titers: 500, epoch: 2 | loss: 0.3584791\n",
      "\tspeed: 0.2209s/iter; left time: 1734.7025s\n",
      "\titers: 600, epoch: 2 | loss: 0.2648255\n",
      "\tspeed: 0.2348s/iter; left time: 1820.2813s\n",
      "\titers: 700, epoch: 2 | loss: 0.2036692\n",
      "\tspeed: 0.2253s/iter; left time: 1723.9514s\n",
      "\titers: 800, epoch: 2 | loss: 0.3075930\n",
      "\tspeed: 0.2210s/iter; left time: 1669.2371s\n",
      "\titers: 900, epoch: 2 | loss: 0.2167276\n",
      "\tspeed: 0.2230s/iter; left time: 1661.7383s\n",
      "Epoch: 2 running time: 3.4203436692555744 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.2688619 Vali Loss: 0.4773069 Test Loss: 0.5753458\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2746366\n",
      "\tspeed: 0.7057s/iter; left time: 5168.9192s\n",
      "\titers: 200, epoch: 3 | loss: 0.4012272\n",
      "\tspeed: 0.2054s/iter; left time: 1483.9017s\n",
      "\titers: 300, epoch: 3 | loss: 0.2789852\n",
      "\tspeed: 0.2149s/iter; left time: 1530.9720s\n",
      "\titers: 400, epoch: 3 | loss: 0.2366949\n",
      "\tspeed: 0.2032s/iter; left time: 1427.2657s\n",
      "\titers: 500, epoch: 3 | loss: 0.2477620\n",
      "\tspeed: 0.2067s/iter; left time: 1431.4912s\n",
      "\titers: 600, epoch: 3 | loss: 0.2581277\n",
      "\tspeed: 0.2085s/iter; left time: 1422.7335s\n",
      "\titers: 700, epoch: 3 | loss: 0.2342101\n",
      "\tspeed: 0.2154s/iter; left time: 1448.8822s\n",
      "\titers: 800, epoch: 3 | loss: 0.2976106\n",
      "\tspeed: 0.2110s/iter; left time: 1397.6882s\n",
      "\titers: 900, epoch: 3 | loss: 0.2386472\n",
      "\tspeed: 0.2135s/iter; left time: 1393.1250s\n",
      "Epoch: 3 running time: 3.2477918028831483 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.2576136 Vali Loss: 0.4467855 Test Loss: 0.5544769\n",
      "Validation loss decreased (0.461550 --> 0.446786).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1769555\n",
      "\tspeed: 0.7068s/iter; left time: 4521.7127s\n",
      "\titers: 200, epoch: 4 | loss: 0.2394323\n",
      "\tspeed: 0.2044s/iter; left time: 1286.8638s\n",
      "\titers: 300, epoch: 4 | loss: 0.1900445\n",
      "\tspeed: 0.2021s/iter; left time: 1252.5520s\n",
      "\titers: 400, epoch: 4 | loss: 0.1635077\n",
      "\tspeed: 0.2132s/iter; left time: 1299.6011s\n",
      "\titers: 500, epoch: 4 | loss: 0.1812928\n",
      "\tspeed: 0.2156s/iter; left time: 1293.1660s\n",
      "\titers: 600, epoch: 4 | loss: 0.1848030\n",
      "\tspeed: 0.2160s/iter; left time: 1273.5246s\n",
      "\titers: 700, epoch: 4 | loss: 0.1904898\n",
      "\tspeed: 0.2126s/iter; left time: 1232.5330s\n",
      "\titers: 800, epoch: 4 | loss: 0.1650923\n",
      "\tspeed: 0.2101s/iter; left time: 1197.0318s\n",
      "\titers: 900, epoch: 4 | loss: 0.1956824\n",
      "\tspeed: 0.2121s/iter; left time: 1187.0001s\n",
      "Epoch: 4 running time: 3.2575427373250325 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.2061842 Vali Loss: 0.4843155 Test Loss: 0.6312740\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2535044\n",
      "\tspeed: 0.7069s/iter; left time: 3865.8219s\n",
      "\titers: 200, epoch: 5 | loss: 0.1652980\n",
      "\tspeed: 0.2070s/iter; left time: 1111.5688s\n",
      "\titers: 300, epoch: 5 | loss: 0.1903591\n",
      "\tspeed: 0.2132s/iter; left time: 1123.3500s\n",
      "\titers: 400, epoch: 5 | loss: 0.2188650\n",
      "\tspeed: 0.2050s/iter; left time: 1059.5096s\n",
      "\titers: 500, epoch: 5 | loss: 0.2327570\n",
      "\tspeed: 0.2100s/iter; left time: 1064.3584s\n",
      "\titers: 600, epoch: 5 | loss: 0.2746615\n",
      "\tspeed: 0.2139s/iter; left time: 1062.6930s\n",
      "\titers: 700, epoch: 5 | loss: 0.1564541\n",
      "\tspeed: 0.2127s/iter; left time: 1035.8628s\n",
      "\titers: 800, epoch: 5 | loss: 0.2290102\n",
      "\tspeed: 0.2094s/iter; left time: 998.8336s\n",
      "\titers: 900, epoch: 5 | loss: 0.1458831\n",
      "\tspeed: 0.2115s/iter; left time: 987.4509s\n",
      "Epoch: 5 running time: 3.275063991546631 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.2041485 Vali Loss: 0.5476182 Test Loss: 0.6800209\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2044379\n",
      "\tspeed: 0.6987s/iter; left time: 3172.5896s\n",
      "\titers: 200, epoch: 6 | loss: 0.1881682\n",
      "\tspeed: 0.2181s/iter; left time: 968.5022s\n",
      "\titers: 300, epoch: 6 | loss: 0.1770904\n",
      "\tspeed: 0.2100s/iter; left time: 911.5867s\n",
      "\titers: 400, epoch: 6 | loss: 0.2336290\n",
      "\tspeed: 0.2142s/iter; left time: 908.6178s\n",
      "\titers: 500, epoch: 6 | loss: 0.2398830\n",
      "\tspeed: 0.2207s/iter; left time: 913.7252s\n",
      "\titers: 600, epoch: 6 | loss: 0.1951474\n",
      "\tspeed: 0.2278s/iter; left time: 920.6664s\n",
      "\titers: 700, epoch: 6 | loss: 0.1929741\n",
      "\tspeed: 0.2276s/iter; left time: 897.1235s\n",
      "\titers: 800, epoch: 6 | loss: 0.2230639\n",
      "\tspeed: 0.2011s/iter; left time: 772.3304s\n",
      "\titers: 900, epoch: 6 | loss: 0.1671925\n",
      "\tspeed: 0.2080s/iter; left time: 778.0551s\n",
      "Epoch: 6 running time: 3.34185661872228 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.2060626 Vali Loss: 0.4825986 Test Loss: 0.6071712\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.6053681373596191, mae:0.5268161296844482\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.7768757\n",
      "\tspeed: 0.2167s/iter; left time: 1989.8549s\n",
      "\titers: 200, epoch: 1 | loss: 0.6034181\n",
      "\tspeed: 0.2118s/iter; left time: 1923.6514s\n",
      "\titers: 300, epoch: 1 | loss: 0.3720162\n",
      "\tspeed: 0.2144s/iter; left time: 1925.5286s\n",
      "\titers: 400, epoch: 1 | loss: 0.3129847\n",
      "\tspeed: 0.2075s/iter; left time: 1842.9397s\n",
      "\titers: 500, epoch: 1 | loss: 0.2521520\n",
      "\tspeed: 0.2221s/iter; left time: 1950.6214s\n",
      "\titers: 600, epoch: 1 | loss: 0.2760711\n",
      "\tspeed: 0.2036s/iter; left time: 1767.0955s\n",
      "\titers: 700, epoch: 1 | loss: 0.2956496\n",
      "\tspeed: 0.2166s/iter; left time: 1858.5419s\n",
      "\titers: 800, epoch: 1 | loss: 0.2905807\n",
      "\tspeed: 0.2211s/iter; left time: 1875.4830s\n",
      "\titers: 900, epoch: 1 | loss: 0.3145721\n",
      "\tspeed: 0.2267s/iter; left time: 1899.9329s\n",
      "Epoch: 1 running time: 3.3427194118499757 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.4227242 Vali Loss: 0.4551930 Test Loss: 0.5631264\n",
      "Validation loss decreased (inf --> 0.455193).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2883009\n",
      "\tspeed: 0.7303s/iter; left time: 6027.1618s\n",
      "\titers: 200, epoch: 2 | loss: 0.2464041\n",
      "\tspeed: 0.2194s/iter; left time: 1788.5645s\n",
      "\titers: 300, epoch: 2 | loss: 0.3400281\n",
      "\tspeed: 0.2116s/iter; left time: 1704.0452s\n",
      "\titers: 400, epoch: 2 | loss: 0.2377827\n",
      "\tspeed: 0.2095s/iter; left time: 1665.8080s\n",
      "\titers: 500, epoch: 2 | loss: 0.3333149\n",
      "\tspeed: 0.2094s/iter; left time: 1644.0304s\n",
      "\titers: 600, epoch: 2 | loss: 0.2951936\n",
      "\tspeed: 0.2098s/iter; left time: 1626.9565s\n",
      "\titers: 700, epoch: 2 | loss: 0.2314780\n",
      "\tspeed: 0.2144s/iter; left time: 1641.0287s\n",
      "\titers: 800, epoch: 2 | loss: 0.2566397\n",
      "\tspeed: 0.2177s/iter; left time: 1644.4423s\n",
      "\titers: 900, epoch: 2 | loss: 0.2619588\n",
      "\tspeed: 0.2126s/iter; left time: 1584.5455s\n",
      "Epoch: 2 running time: 3.3271657824516296 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.2650967 Vali Loss: 0.4345545 Test Loss: 0.5785044\n",
      "Validation loss decreased (0.455193 --> 0.434555).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2309944\n",
      "\tspeed: 0.7380s/iter; left time: 5405.7408s\n",
      "\titers: 200, epoch: 3 | loss: 0.1974098\n",
      "\tspeed: 0.2215s/iter; left time: 1600.0276s\n",
      "\titers: 300, epoch: 3 | loss: 0.1951381\n",
      "\tspeed: 0.2114s/iter; left time: 1506.1149s\n",
      "\titers: 400, epoch: 3 | loss: 0.1756040\n",
      "\tspeed: 0.2154s/iter; left time: 1513.3656s\n",
      "\titers: 500, epoch: 3 | loss: 0.2050501\n",
      "\tspeed: 0.2150s/iter; left time: 1488.9955s\n",
      "\titers: 600, epoch: 3 | loss: 0.1718847\n",
      "\tspeed: 0.2146s/iter; left time: 1464.5912s\n",
      "\titers: 700, epoch: 3 | loss: 0.1719801\n",
      "\tspeed: 0.2203s/iter; left time: 1481.7400s\n",
      "\titers: 800, epoch: 3 | loss: 0.1731403\n",
      "\tspeed: 0.2200s/iter; left time: 1457.2685s\n",
      "\titers: 900, epoch: 3 | loss: 0.2141090\n",
      "\tspeed: 0.2243s/iter; left time: 1463.2583s\n",
      "Epoch: 3 running time: 3.3863895893096925 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1991684 Vali Loss: 0.5317937 Test Loss: 0.7108498\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1765164\n",
      "\tspeed: 0.7517s/iter; left time: 4808.8969s\n",
      "\titers: 200, epoch: 4 | loss: 0.1768189\n",
      "\tspeed: 0.2351s/iter; left time: 1480.2467s\n",
      "\titers: 300, epoch: 4 | loss: 0.2159033\n",
      "\tspeed: 0.2323s/iter; left time: 1439.3250s\n",
      "\titers: 400, epoch: 4 | loss: 0.1481172\n",
      "\tspeed: 0.2244s/iter; left time: 1368.2780s\n",
      "\titers: 500, epoch: 4 | loss: 0.1641053\n",
      "\tspeed: 0.2101s/iter; left time: 1259.6935s\n",
      "\titers: 600, epoch: 4 | loss: 0.1782607\n",
      "\tspeed: 0.2128s/iter; left time: 1254.7409s\n",
      "\titers: 700, epoch: 4 | loss: 0.1982099\n",
      "\tspeed: 0.2162s/iter; left time: 1253.1757s\n",
      "\titers: 800, epoch: 4 | loss: 0.1882484\n",
      "\tspeed: 0.2057s/iter; left time: 1171.8910s\n",
      "\titers: 900, epoch: 4 | loss: 0.1616658\n",
      "\tspeed: 0.2215s/iter; left time: 1239.6752s\n",
      "Epoch: 4 running time: 3.43604789574941 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1926734 Vali Loss: 0.5034879 Test Loss: 0.6844848\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1863510\n",
      "\tspeed: 0.7022s/iter; left time: 3840.1422s\n",
      "\titers: 200, epoch: 5 | loss: 0.2081560\n",
      "\tspeed: 0.2107s/iter; left time: 1131.2083s\n",
      "\titers: 300, epoch: 5 | loss: 0.1927193\n",
      "\tspeed: 0.2204s/iter; left time: 1161.0949s\n",
      "\titers: 400, epoch: 5 | loss: 0.1669317\n",
      "\tspeed: 0.2301s/iter; left time: 1189.2551s\n",
      "\titers: 500, epoch: 5 | loss: 0.1950648\n",
      "\tspeed: 0.2290s/iter; left time: 1160.5518s\n",
      "\titers: 600, epoch: 5 | loss: 0.1762444\n",
      "\tspeed: 0.2312s/iter; left time: 1148.7495s\n",
      "\titers: 700, epoch: 5 | loss: 0.2179870\n",
      "\tspeed: 0.2155s/iter; left time: 1049.1409s\n",
      "\titers: 800, epoch: 5 | loss: 0.1861712\n",
      "\tspeed: 0.2218s/iter; left time: 1057.7748s\n",
      "\titers: 900, epoch: 5 | loss: 0.1505106\n",
      "\tspeed: 0.2179s/iter; left time: 1017.3186s\n",
      "Epoch: 5 running time: 3.4209017475446064 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1940724 Vali Loss: 0.4834670 Test Loss: 0.6468741\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.6498473882675171, mae:0.5379905700683594\n",
      "\n",
      "Time intermediate for DE dataset: 45.965057575702666 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.8287822\n",
      "\tspeed: 0.2443s/iter; left time: 2242.6155s\n",
      "\titers: 200, epoch: 1 | loss: 0.7043365\n",
      "\tspeed: 0.2160s/iter; left time: 1961.5976s\n",
      "\titers: 300, epoch: 1 | loss: 0.4867991\n",
      "\tspeed: 0.2145s/iter; left time: 1926.6902s\n",
      "\titers: 400, epoch: 1 | loss: 0.3515458\n",
      "\tspeed: 0.2122s/iter; left time: 1884.2090s\n",
      "\titers: 500, epoch: 1 | loss: 0.4010313\n",
      "\tspeed: 0.2092s/iter; left time: 1836.8440s\n",
      "\titers: 600, epoch: 1 | loss: 0.3184416\n",
      "\tspeed: 0.2210s/iter; left time: 1918.7263s\n",
      "\titers: 700, epoch: 1 | loss: 0.4140712\n",
      "\tspeed: 0.2165s/iter; left time: 1857.8700s\n",
      "\titers: 800, epoch: 1 | loss: 0.3109525\n",
      "\tspeed: 0.2063s/iter; left time: 1749.9139s\n",
      "\titers: 900, epoch: 1 | loss: 0.4002253\n",
      "\tspeed: 0.2173s/iter; left time: 1821.0661s\n",
      "Epoch: 1 running time: 3.3397624770800274 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.4588357 Vali Loss: 0.4996310 Test Loss: 0.7840582\n",
      "Validation loss decreased (inf --> 0.499631).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3212394\n",
      "\tspeed: 0.7223s/iter; left time: 5961.0080s\n",
      "\titers: 200, epoch: 2 | loss: 0.2959928\n",
      "\tspeed: 0.2132s/iter; left time: 1738.3475s\n",
      "\titers: 300, epoch: 2 | loss: 0.3126874\n",
      "\tspeed: 0.2178s/iter; left time: 1754.3317s\n",
      "\titers: 400, epoch: 2 | loss: 0.2971790\n",
      "\tspeed: 0.2157s/iter; left time: 1715.5716s\n",
      "\titers: 500, epoch: 2 | loss: 0.2560361\n",
      "\tspeed: 0.2054s/iter; left time: 1613.1330s\n",
      "\titers: 600, epoch: 2 | loss: 0.2748753\n",
      "\tspeed: 0.2326s/iter; left time: 1803.0398s\n",
      "\titers: 700, epoch: 2 | loss: 0.3117526\n",
      "\tspeed: 0.2060s/iter; left time: 1576.3942s\n",
      "\titers: 800, epoch: 2 | loss: 0.3197159\n",
      "\tspeed: 0.2118s/iter; left time: 1600.0307s\n",
      "\titers: 900, epoch: 2 | loss: 0.2216328\n",
      "\tspeed: 0.2118s/iter; left time: 1578.3537s\n",
      "Epoch: 2 running time: 3.3356568217277527 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.3029322 Vali Loss: 0.5782021 Test Loss: 1.0446669\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3159722\n",
      "\tspeed: 0.7036s/iter; left time: 5153.7269s\n",
      "\titers: 200, epoch: 3 | loss: 0.3718402\n",
      "\tspeed: 0.2164s/iter; left time: 1563.3149s\n",
      "\titers: 300, epoch: 3 | loss: 0.3360743\n",
      "\tspeed: 0.2101s/iter; left time: 1497.2527s\n",
      "\titers: 400, epoch: 3 | loss: 0.2949417\n",
      "\tspeed: 0.2454s/iter; left time: 1723.8858s\n",
      "\titers: 500, epoch: 3 | loss: 0.2426745\n",
      "\tspeed: 0.2304s/iter; left time: 1595.3929s\n",
      "\titers: 600, epoch: 3 | loss: 0.3017188\n",
      "\tspeed: 0.2408s/iter; left time: 1643.1435s\n",
      "\titers: 700, epoch: 3 | loss: 0.2660915\n",
      "\tspeed: 0.2326s/iter; left time: 1564.4389s\n",
      "\titers: 800, epoch: 3 | loss: 0.2739845\n",
      "\tspeed: 0.2366s/iter; left time: 1567.6538s\n",
      "\titers: 900, epoch: 3 | loss: 0.2971630\n",
      "\tspeed: 0.2404s/iter; left time: 1568.8770s\n",
      "Epoch: 3 running time: 3.5586818059285483 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.2848067 Vali Loss: 0.5398618 Test Loss: 0.9207494\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2359476\n",
      "\tspeed: 0.7387s/iter; left time: 4725.5980s\n",
      "\titers: 200, epoch: 4 | loss: 0.3767248\n",
      "\tspeed: 0.2171s/iter; left time: 1366.9753s\n",
      "\titers: 300, epoch: 4 | loss: 0.2389495\n",
      "\tspeed: 0.2126s/iter; left time: 1317.6535s\n",
      "\titers: 400, epoch: 4 | loss: 0.2214884\n",
      "\tspeed: 0.2161s/iter; left time: 1317.6765s\n",
      "\titers: 500, epoch: 4 | loss: 0.3580657\n",
      "\tspeed: 0.2115s/iter; left time: 1268.4155s\n",
      "\titers: 600, epoch: 4 | loss: 0.2561196\n",
      "\tspeed: 0.2226s/iter; left time: 1312.7511s\n",
      "\titers: 700, epoch: 4 | loss: 0.3443271\n",
      "\tspeed: 0.2083s/iter; left time: 1207.6777s\n",
      "\titers: 800, epoch: 4 | loss: 0.3361765\n",
      "\tspeed: 0.2182s/iter; left time: 1243.2852s\n",
      "\titers: 900, epoch: 4 | loss: 0.2652399\n",
      "\tspeed: 0.2171s/iter; left time: 1215.1376s\n",
      "Epoch: 4 running time: 3.33929306268692 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.2818582 Vali Loss: 0.5112154 Test Loss: 0.9322068\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.9258393049240112, mae:0.6708835959434509\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5855842\n",
      "\tspeed: 0.2284s/iter; left time: 2096.6236s\n",
      "\titers: 200, epoch: 1 | loss: 0.4877284\n",
      "\tspeed: 0.2175s/iter; left time: 1975.3621s\n",
      "\titers: 300, epoch: 1 | loss: 0.4147232\n",
      "\tspeed: 0.2128s/iter; left time: 1911.2002s\n",
      "\titers: 400, epoch: 1 | loss: 0.3442721\n",
      "\tspeed: 0.2221s/iter; left time: 1972.3787s\n",
      "\titers: 500, epoch: 1 | loss: 0.4471766\n",
      "\tspeed: 0.2296s/iter; left time: 2015.6842s\n",
      "\titers: 600, epoch: 1 | loss: 0.3561628\n",
      "\tspeed: 0.2286s/iter; left time: 1984.5801s\n",
      "\titers: 700, epoch: 1 | loss: 0.3426434\n",
      "\tspeed: 0.2232s/iter; left time: 1915.6786s\n",
      "\titers: 800, epoch: 1 | loss: 0.3665687\n",
      "\tspeed: 0.2310s/iter; left time: 1959.1514s\n",
      "\titers: 900, epoch: 1 | loss: 0.3928147\n",
      "\tspeed: 0.2327s/iter; left time: 1950.5763s\n",
      "Epoch: 1 running time: 3.4812123735745746 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.4454330 Vali Loss: 0.5174375 Test Loss: 0.8811185\n",
      "Validation loss decreased (inf --> 0.517437).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3241153\n",
      "\tspeed: 0.7180s/iter; left time: 5925.6887s\n",
      "\titers: 200, epoch: 2 | loss: 0.2643043\n",
      "\tspeed: 0.2161s/iter; left time: 1761.8080s\n",
      "\titers: 300, epoch: 2 | loss: 0.3312238\n",
      "\tspeed: 0.2101s/iter; left time: 1691.9035s\n",
      "\titers: 400, epoch: 2 | loss: 0.3907087\n",
      "\tspeed: 0.2223s/iter; left time: 1768.3224s\n",
      "\titers: 500, epoch: 2 | loss: 0.2960263\n",
      "\tspeed: 0.2132s/iter; left time: 1674.2194s\n",
      "\titers: 600, epoch: 2 | loss: 0.2812370\n",
      "\tspeed: 0.2206s/iter; left time: 1709.9971s\n",
      "\titers: 700, epoch: 2 | loss: 0.3093649\n",
      "\tspeed: 0.2228s/iter; left time: 1704.9052s\n",
      "\titers: 800, epoch: 2 | loss: 0.3436110\n",
      "\tspeed: 0.2127s/iter; left time: 1606.5225s\n",
      "\titers: 900, epoch: 2 | loss: 0.2410296\n",
      "\tspeed: 0.2072s/iter; left time: 1544.0415s\n",
      "Epoch: 2 running time: 3.345844582716624 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.3095393 Vali Loss: 0.5217061 Test Loss: 0.9222149\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2847827\n",
      "\tspeed: 0.7366s/iter; left time: 5395.2495s\n",
      "\titers: 200, epoch: 3 | loss: 0.3033452\n",
      "\tspeed: 0.2180s/iter; left time: 1575.4047s\n",
      "\titers: 300, epoch: 3 | loss: 0.2975087\n",
      "\tspeed: 0.2072s/iter; left time: 1476.5856s\n",
      "\titers: 400, epoch: 3 | loss: 0.2571127\n",
      "\tspeed: 0.2154s/iter; left time: 1513.4135s\n",
      "\titers: 500, epoch: 3 | loss: 0.3818382\n",
      "\tspeed: 0.2129s/iter; left time: 1474.1035s\n",
      "\titers: 600, epoch: 3 | loss: 0.3712793\n",
      "\tspeed: 0.2174s/iter; left time: 1483.5849s\n",
      "\titers: 700, epoch: 3 | loss: 0.2585613\n",
      "\tspeed: 0.2121s/iter; left time: 1426.1930s\n",
      "\titers: 800, epoch: 3 | loss: 0.2362047\n",
      "\tspeed: 0.2059s/iter; left time: 1364.1966s\n",
      "\titers: 900, epoch: 3 | loss: 0.2625003\n",
      "\tspeed: 0.2138s/iter; left time: 1395.3333s\n",
      "Epoch: 3 running time: 3.3303054769833884 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.2956861 Vali Loss: 0.5447472 Test Loss: 1.0830812\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3759369\n",
      "\tspeed: 0.7087s/iter; left time: 4533.2742s\n",
      "\titers: 200, epoch: 4 | loss: 0.2823734\n",
      "\tspeed: 0.2090s/iter; left time: 1316.2518s\n",
      "\titers: 300, epoch: 4 | loss: 0.3500485\n",
      "\tspeed: 0.2086s/iter; left time: 1292.6970s\n",
      "\titers: 400, epoch: 4 | loss: 0.2677283\n",
      "\tspeed: 0.2068s/iter; left time: 1260.8784s\n",
      "\titers: 500, epoch: 4 | loss: 0.2489732\n",
      "\tspeed: 0.2078s/iter; left time: 1246.1974s\n",
      "\titers: 600, epoch: 4 | loss: 0.3014233\n",
      "\tspeed: 0.2244s/iter; left time: 1323.1245s\n",
      "\titers: 700, epoch: 4 | loss: 0.2845108\n",
      "\tspeed: 0.2300s/iter; left time: 1333.0389s\n",
      "\titers: 800, epoch: 4 | loss: 0.2861826\n",
      "\tspeed: 0.2119s/iter; left time: 1207.2359s\n",
      "\titers: 900, epoch: 4 | loss: 0.3078247\n",
      "\tspeed: 0.2268s/iter; left time: 1269.1769s\n",
      "Epoch: 4 running time: 3.3378143032391865 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.2902286 Vali Loss: 0.5074262 Test Loss: 1.0133436\n",
      "Validation loss decreased (0.517437 --> 0.507426).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2230290\n",
      "\tspeed: 0.7503s/iter; left time: 4103.5878s\n",
      "\titers: 200, epoch: 5 | loss: 0.2543602\n",
      "\tspeed: 0.2064s/iter; left time: 1107.9276s\n",
      "\titers: 300, epoch: 5 | loss: 0.2318274\n",
      "\tspeed: 0.2066s/iter; left time: 1088.5432s\n",
      "\titers: 400, epoch: 5 | loss: 0.2855835\n",
      "\tspeed: 0.2071s/iter; left time: 1070.3661s\n",
      "\titers: 500, epoch: 5 | loss: 0.2529615\n",
      "\tspeed: 0.2079s/iter; left time: 1054.0157s\n",
      "\titers: 600, epoch: 5 | loss: 0.1925465\n",
      "\tspeed: 0.2125s/iter; left time: 1055.9715s\n",
      "\titers: 700, epoch: 5 | loss: 0.2904137\n",
      "\tspeed: 0.2108s/iter; left time: 1026.3297s\n",
      "\titers: 800, epoch: 5 | loss: 0.2312036\n",
      "\tspeed: 0.2136s/iter; left time: 1018.5287s\n",
      "\titers: 900, epoch: 5 | loss: 0.2401189\n",
      "\tspeed: 0.2085s/iter; left time: 973.3929s\n",
      "Epoch: 5 running time: 3.2568046967188518 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.2541293 Vali Loss: 0.5441358 Test Loss: 1.1684605\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.3069842\n",
      "\tspeed: 0.7274s/iter; left time: 3303.2859s\n",
      "\titers: 200, epoch: 6 | loss: 0.2314566\n",
      "\tspeed: 0.2135s/iter; left time: 948.0483s\n",
      "\titers: 300, epoch: 6 | loss: 0.2352968\n",
      "\tspeed: 0.2091s/iter; left time: 907.8176s\n",
      "\titers: 400, epoch: 6 | loss: 0.2233261\n",
      "\tspeed: 0.2095s/iter; left time: 888.3611s\n",
      "\titers: 500, epoch: 6 | loss: 0.2319467\n",
      "\tspeed: 0.2159s/iter; left time: 893.9944s\n",
      "\titers: 600, epoch: 6 | loss: 0.2252136\n",
      "\tspeed: 0.2046s/iter; left time: 826.6423s\n",
      "\titers: 700, epoch: 6 | loss: 0.2353022\n",
      "\tspeed: 0.2100s/iter; left time: 827.5607s\n",
      "\titers: 800, epoch: 6 | loss: 0.2209006\n",
      "\tspeed: 0.2106s/iter; left time: 808.9410s\n",
      "\titers: 900, epoch: 6 | loss: 0.2230271\n",
      "\tspeed: 0.2152s/iter; left time: 805.0215s\n",
      "Epoch: 6 running time: 3.3036094824473063 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.2545883 Vali Loss: 0.5249553 Test Loss: 1.1006966\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2444387\n",
      "\tspeed: 0.7129s/iter; left time: 2575.5399s\n",
      "\titers: 200, epoch: 7 | loss: 0.2544947\n",
      "\tspeed: 0.2042s/iter; left time: 717.1952s\n",
      "\titers: 300, epoch: 7 | loss: 0.2283785\n",
      "\tspeed: 0.2120s/iter; left time: 723.7228s\n",
      "\titers: 400, epoch: 7 | loss: 0.2768632\n",
      "\tspeed: 0.2074s/iter; left time: 687.1625s\n",
      "\titers: 500, epoch: 7 | loss: 0.3109196\n",
      "\tspeed: 0.2116s/iter; left time: 679.7340s\n",
      "\titers: 600, epoch: 7 | loss: 0.2197296\n",
      "\tspeed: 0.2132s/iter; left time: 663.6013s\n",
      "\titers: 700, epoch: 7 | loss: 0.2374195\n",
      "\tspeed: 0.2065s/iter; left time: 622.0788s\n",
      "\titers: 800, epoch: 7 | loss: 0.2351134\n",
      "\tspeed: 0.2167s/iter; left time: 631.1380s\n",
      "\titers: 900, epoch: 7 | loss: 0.2217243\n",
      "\tspeed: 0.2110s/iter; left time: 593.6088s\n",
      "Epoch: 7 running time: 3.253902518749237 min.\n",
      "Epoch: 7, Steps: 928 | Train Loss: 0.2566992 Vali Loss: 0.5173086 Test Loss: 1.0614542\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:1.0595234632492065, mae:0.720156729221344\n",
      "\n",
      "Time intermediate for GB dataset: 45.97519641319911 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6821507\n",
      "\tspeed: 0.2345s/iter; left time: 2153.3562s\n",
      "\titers: 200, epoch: 1 | loss: 0.3494192\n",
      "\tspeed: 0.2109s/iter; left time: 1915.4591s\n",
      "\titers: 300, epoch: 1 | loss: 0.2723191\n",
      "\tspeed: 0.2116s/iter; left time: 1900.2689s\n",
      "\titers: 400, epoch: 1 | loss: 0.2604924\n",
      "\tspeed: 0.2129s/iter; left time: 1890.7908s\n",
      "\titers: 500, epoch: 1 | loss: 0.2107954\n",
      "\tspeed: 0.2143s/iter; left time: 1882.0736s\n",
      "\titers: 600, epoch: 1 | loss: 0.2287949\n",
      "\tspeed: 0.2210s/iter; left time: 1918.6658s\n",
      "\titers: 700, epoch: 1 | loss: 0.2827240\n",
      "\tspeed: 0.2267s/iter; left time: 1945.1212s\n",
      "\titers: 800, epoch: 1 | loss: 0.1584856\n",
      "\tspeed: 0.2098s/iter; left time: 1779.6932s\n",
      "\titers: 900, epoch: 1 | loss: 0.2253634\n",
      "\tspeed: 0.2080s/iter; left time: 1743.5598s\n",
      "Epoch: 1 running time: 3.3275986035664875 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3408938 Vali Loss: 0.2436228 Test Loss: 0.4506721\n",
      "Validation loss decreased (inf --> 0.243623).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1982307\n",
      "\tspeed: 0.7203s/iter; left time: 5944.9028s\n",
      "\titers: 200, epoch: 2 | loss: 0.1871837\n",
      "\tspeed: 0.2160s/iter; left time: 1761.4135s\n",
      "\titers: 300, epoch: 2 | loss: 0.1729706\n",
      "\tspeed: 0.2080s/iter; left time: 1675.3452s\n",
      "\titers: 400, epoch: 2 | loss: 0.1698892\n",
      "\tspeed: 0.2227s/iter; left time: 1771.0473s\n",
      "\titers: 500, epoch: 2 | loss: 0.1726438\n",
      "\tspeed: 0.2153s/iter; left time: 1690.6192s\n",
      "\titers: 600, epoch: 2 | loss: 0.2229082\n",
      "\tspeed: 0.2191s/iter; left time: 1698.6200s\n",
      "\titers: 700, epoch: 2 | loss: 0.1751979\n",
      "\tspeed: 0.2195s/iter; left time: 1679.8798s\n",
      "\titers: 800, epoch: 2 | loss: 0.1611175\n",
      "\tspeed: 0.2337s/iter; left time: 1765.0382s\n",
      "\titers: 900, epoch: 2 | loss: 0.1865179\n",
      "\tspeed: 0.2264s/iter; left time: 1687.5600s\n",
      "Epoch: 2 running time: 3.423864471912384 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1872602 Vali Loss: 0.2414272 Test Loss: 0.5622443\n",
      "Validation loss decreased (0.243623 --> 0.241427).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1513640\n",
      "\tspeed: 0.6948s/iter; left time: 5089.3756s\n",
      "\titers: 200, epoch: 3 | loss: 0.1778364\n",
      "\tspeed: 0.2089s/iter; left time: 1509.0087s\n",
      "\titers: 300, epoch: 3 | loss: 0.1445066\n",
      "\tspeed: 0.2105s/iter; left time: 1499.8668s\n",
      "\titers: 400, epoch: 3 | loss: 0.1471406\n",
      "\tspeed: 0.2080s/iter; left time: 1460.9346s\n",
      "\titers: 500, epoch: 3 | loss: 0.1470098\n",
      "\tspeed: 0.2165s/iter; left time: 1498.9316s\n",
      "\titers: 600, epoch: 3 | loss: 0.1252215\n",
      "\tspeed: 0.2049s/iter; left time: 1398.4784s\n",
      "\titers: 700, epoch: 3 | loss: 0.1647808\n",
      "\tspeed: 0.2087s/iter; left time: 1403.7261s\n",
      "\titers: 800, epoch: 3 | loss: 0.1431528\n",
      "\tspeed: 0.2111s/iter; left time: 1398.5033s\n",
      "\titers: 900, epoch: 3 | loss: 0.1431089\n",
      "\tspeed: 0.2079s/iter; left time: 1356.3432s\n",
      "Epoch: 3 running time: 3.2338648279507956 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1465302 Vali Loss: 0.2214382 Test Loss: 0.6462561\n",
      "Validation loss decreased (0.241427 --> 0.221438).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1554342\n",
      "\tspeed: 0.6909s/iter; left time: 4419.5641s\n",
      "\titers: 200, epoch: 4 | loss: 0.0954831\n",
      "\tspeed: 0.2204s/iter; left time: 1388.0328s\n",
      "\titers: 300, epoch: 4 | loss: 0.0931890\n",
      "\tspeed: 0.2235s/iter; left time: 1385.2491s\n",
      "\titers: 400, epoch: 4 | loss: 0.1181021\n",
      "\tspeed: 0.2112s/iter; left time: 1287.8341s\n",
      "\titers: 500, epoch: 4 | loss: 0.1219885\n",
      "\tspeed: 0.2096s/iter; left time: 1256.9365s\n",
      "\titers: 600, epoch: 4 | loss: 0.1074922\n",
      "\tspeed: 0.2122s/iter; left time: 1251.2300s\n",
      "\titers: 700, epoch: 4 | loss: 0.0998657\n",
      "\tspeed: 0.2083s/iter; left time: 1207.5419s\n",
      "\titers: 800, epoch: 4 | loss: 0.1292848\n",
      "\tspeed: 0.2091s/iter; left time: 1191.5260s\n",
      "\titers: 900, epoch: 4 | loss: 0.1070315\n",
      "\tspeed: 0.2060s/iter; left time: 1153.1135s\n",
      "Epoch: 4 running time: 3.2945181926091514 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1208702 Vali Loss: 0.2279170 Test Loss: 0.7022680\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1136401\n",
      "\tspeed: 0.6958s/iter; left time: 3805.1821s\n",
      "\titers: 200, epoch: 5 | loss: 0.1289256\n",
      "\tspeed: 0.2083s/iter; left time: 1118.2058s\n",
      "\titers: 300, epoch: 5 | loss: 0.1232128\n",
      "\tspeed: 0.2144s/iter; left time: 1129.7064s\n",
      "\titers: 400, epoch: 5 | loss: 0.1201548\n",
      "\tspeed: 0.2115s/iter; left time: 1093.4297s\n",
      "\titers: 500, epoch: 5 | loss: 0.1218066\n",
      "\tspeed: 0.2115s/iter; left time: 1071.9930s\n",
      "\titers: 600, epoch: 5 | loss: 0.1127867\n",
      "\tspeed: 0.2139s/iter; left time: 1062.6910s\n",
      "\titers: 700, epoch: 5 | loss: 0.1140293\n",
      "\tspeed: 0.2136s/iter; left time: 1039.9234s\n",
      "\titers: 800, epoch: 5 | loss: 0.1222156\n",
      "\tspeed: 0.2063s/iter; left time: 983.9075s\n",
      "\titers: 900, epoch: 5 | loss: 0.1224480\n",
      "\tspeed: 0.2186s/iter; left time: 1020.4760s\n",
      "Epoch: 5 running time: 3.296583942572276 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1186474 Vali Loss: 0.2297494 Test Loss: 0.7457036\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0980260\n",
      "\tspeed: 0.7379s/iter; left time: 3350.6998s\n",
      "\titers: 200, epoch: 6 | loss: 0.1256465\n",
      "\tspeed: 0.2074s/iter; left time: 920.9323s\n",
      "\titers: 300, epoch: 6 | loss: 0.1137154\n",
      "\tspeed: 0.2062s/iter; left time: 895.3248s\n",
      "\titers: 400, epoch: 6 | loss: 0.1119002\n",
      "\tspeed: 0.2077s/iter; left time: 880.7403s\n",
      "\titers: 500, epoch: 6 | loss: 0.1048326\n",
      "\tspeed: 0.2053s/iter; left time: 849.9601s\n",
      "\titers: 600, epoch: 6 | loss: 0.0973411\n",
      "\tspeed: 0.2117s/iter; left time: 855.2924s\n",
      "\titers: 700, epoch: 6 | loss: 0.1188646\n",
      "\tspeed: 0.2069s/iter; left time: 815.4897s\n",
      "\titers: 800, epoch: 6 | loss: 0.0945115\n",
      "\tspeed: 0.2061s/iter; left time: 791.4446s\n",
      "\titers: 900, epoch: 6 | loss: 0.1085715\n",
      "\tspeed: 0.2088s/iter; left time: 781.0483s\n",
      "Epoch: 6 running time: 3.278771495819092 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.1189182 Vali Loss: 0.2207190 Test Loss: 0.6931907\n",
      "Validation loss decreased (0.221438 --> 0.220719).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1368358\n",
      "\tspeed: 0.7108s/iter; left time: 2568.1041s\n",
      "\titers: 200, epoch: 7 | loss: 0.1156221\n",
      "\tspeed: 0.2107s/iter; left time: 740.2957s\n",
      "\titers: 300, epoch: 7 | loss: 0.0979112\n",
      "\tspeed: 0.2112s/iter; left time: 720.9360s\n",
      "\titers: 400, epoch: 7 | loss: 0.1128234\n",
      "\tspeed: 0.2085s/iter; left time: 690.6270s\n",
      "\titers: 500, epoch: 7 | loss: 0.1078582\n",
      "\tspeed: 0.2090s/iter; left time: 671.6636s\n",
      "\titers: 600, epoch: 7 | loss: 0.1278808\n",
      "\tspeed: 0.2098s/iter; left time: 653.0507s\n",
      "\titers: 700, epoch: 7 | loss: 0.1450998\n",
      "\tspeed: 0.2063s/iter; left time: 621.6330s\n",
      "\titers: 800, epoch: 7 | loss: 0.1468092\n",
      "\tspeed: 0.2109s/iter; left time: 614.3748s\n",
      "\titers: 900, epoch: 7 | loss: 0.1182958\n",
      "\tspeed: 0.2148s/iter; left time: 604.2685s\n",
      "Epoch: 7 running time: 3.272748363018036 min.\n",
      "Epoch: 7, Steps: 928 | Train Loss: 0.1118696 Vali Loss: 0.2210953 Test Loss: 0.7269410\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1266328\n",
      "\tspeed: 0.7008s/iter; left time: 1881.7432s\n",
      "\titers: 200, epoch: 8 | loss: 0.0966251\n",
      "\tspeed: 0.2090s/iter; left time: 540.2502s\n",
      "\titers: 300, epoch: 8 | loss: 0.0930746\n",
      "\tspeed: 0.2128s/iter; left time: 528.8381s\n",
      "\titers: 400, epoch: 8 | loss: 0.1178849\n",
      "\tspeed: 0.2146s/iter; left time: 511.7489s\n",
      "\titers: 500, epoch: 8 | loss: 0.1137002\n",
      "\tspeed: 0.2067s/iter; left time: 472.3998s\n",
      "\titers: 600, epoch: 8 | loss: 0.1312456\n",
      "\tspeed: 0.2186s/iter; left time: 477.6688s\n",
      "\titers: 700, epoch: 8 | loss: 0.1071250\n",
      "\tspeed: 0.2224s/iter; left time: 463.6434s\n",
      "\titers: 800, epoch: 8 | loss: 0.1019797\n",
      "\tspeed: 0.2232s/iter; left time: 443.0518s\n",
      "\titers: 900, epoch: 8 | loss: 0.1284935\n",
      "\tspeed: 0.2238s/iter; left time: 421.8680s\n",
      "Epoch: 8 running time: 3.3632206400235494 min.\n",
      "Epoch: 8, Steps: 928 | Train Loss: 0.1118381 Vali Loss: 0.2231853 Test Loss: 0.7276948\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1044541\n",
      "\tspeed: 0.7063s/iter; left time: 1240.9652s\n",
      "\titers: 200, epoch: 9 | loss: 0.1155442\n",
      "\tspeed: 0.2090s/iter; left time: 346.2621s\n",
      "\titers: 300, epoch: 9 | loss: 0.1047138\n",
      "\tspeed: 0.2108s/iter; left time: 328.2820s\n",
      "\titers: 400, epoch: 9 | loss: 0.1081619\n",
      "\tspeed: 0.2134s/iter; left time: 310.8895s\n",
      "\titers: 500, epoch: 9 | loss: 0.1176696\n",
      "\tspeed: 0.2184s/iter; left time: 296.3254s\n",
      "\titers: 600, epoch: 9 | loss: 0.1145592\n",
      "\tspeed: 0.2151s/iter; left time: 270.3600s\n",
      "\titers: 700, epoch: 9 | loss: 0.0844227\n",
      "\tspeed: 0.2156s/iter; left time: 249.4272s\n",
      "\titers: 800, epoch: 9 | loss: 0.0926530\n",
      "\tspeed: 0.2086s/iter; left time: 220.4608s\n",
      "\titers: 900, epoch: 9 | loss: 0.0948408\n",
      "\tspeed: 0.2128s/iter; left time: 203.6472s\n",
      "Epoch: 9 running time: 3.315865631898244 min.\n",
      "Epoch: 9, Steps: 928 | Train Loss: 0.1120570 Vali Loss: 0.2194888 Test Loss: 0.7127830\n",
      "Validation loss decreased (0.220719 --> 0.219489).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0933641\n",
      "\tspeed: 0.7352s/iter; left time: 609.4515s\n",
      "\titers: 200, epoch: 10 | loss: 0.1166504\n",
      "\tspeed: 0.2358s/iter; left time: 171.8862s\n",
      "\titers: 300, epoch: 10 | loss: 0.1104325\n",
      "\tspeed: 0.2307s/iter; left time: 145.1342s\n",
      "\titers: 400, epoch: 10 | loss: 0.1017493\n",
      "\tspeed: 0.2282s/iter; left time: 120.7418s\n",
      "\titers: 500, epoch: 10 | loss: 0.1167448\n",
      "\tspeed: 0.2105s/iter; left time: 90.2858s\n",
      "\titers: 600, epoch: 10 | loss: 0.1517532\n",
      "\tspeed: 0.2316s/iter; left time: 76.1991s\n",
      "\titers: 700, epoch: 10 | loss: 0.1013608\n",
      "\tspeed: 0.2073s/iter; left time: 47.4764s\n",
      "\titers: 800, epoch: 10 | loss: 0.1201398\n",
      "\tspeed: 0.2169s/iter; left time: 27.9747s\n",
      "\titers: 900, epoch: 10 | loss: 0.1269317\n",
      "\tspeed: 0.2179s/iter; left time: 6.3184s\n",
      "Epoch: 10 running time: 3.479462198416392 min.\n",
      "Epoch: 10, Steps: 928 | Train Loss: 0.1110730 Vali Loss: 0.2201047 Test Loss: 0.7262554\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.7124937176704407, mae:0.5508580207824707\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6775677\n",
      "\tspeed: 0.2350s/iter; left time: 2157.1375s\n",
      "\titers: 200, epoch: 1 | loss: 0.3640030\n",
      "\tspeed: 0.2292s/iter; left time: 2081.4952s\n",
      "\titers: 300, epoch: 1 | loss: 0.2989636\n",
      "\tspeed: 0.2279s/iter; left time: 2047.1479s\n",
      "\titers: 400, epoch: 1 | loss: 0.2453915\n",
      "\tspeed: 0.2326s/iter; left time: 2066.0114s\n",
      "\titers: 500, epoch: 1 | loss: 0.2807707\n",
      "\tspeed: 0.2175s/iter; left time: 1909.8410s\n",
      "\titers: 600, epoch: 1 | loss: 0.2787383\n",
      "\tspeed: 0.2226s/iter; left time: 1932.4379s\n",
      "\titers: 700, epoch: 1 | loss: 0.2016090\n",
      "\tspeed: 0.2013s/iter; left time: 1727.2854s\n",
      "\titers: 800, epoch: 1 | loss: 0.2294064\n",
      "\tspeed: 0.2042s/iter; left time: 1732.2371s\n",
      "\titers: 900, epoch: 1 | loss: 0.3161609\n",
      "\tspeed: 0.2136s/iter; left time: 1790.5807s\n",
      "Epoch: 1 running time: 3.407988067468007 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3480901 Vali Loss: 0.2277368 Test Loss: 0.4148380\n",
      "Validation loss decreased (inf --> 0.227737).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2180307\n",
      "\tspeed: 0.7546s/iter; left time: 6227.7457s\n",
      "\titers: 200, epoch: 2 | loss: 0.2085225\n",
      "\tspeed: 0.2221s/iter; left time: 1810.4973s\n",
      "\titers: 300, epoch: 2 | loss: 0.1882353\n",
      "\tspeed: 0.2085s/iter; left time: 1678.7215s\n",
      "\titers: 400, epoch: 2 | loss: 0.1482250\n",
      "\tspeed: 0.2196s/iter; left time: 1746.6625s\n",
      "\titers: 500, epoch: 2 | loss: 0.1652783\n",
      "\tspeed: 0.2179s/iter; left time: 1710.9303s\n",
      "\titers: 600, epoch: 2 | loss: 0.1564811\n",
      "\tspeed: 0.2177s/iter; left time: 1687.5588s\n",
      "\titers: 700, epoch: 2 | loss: 0.1506560\n",
      "\tspeed: 0.2234s/iter; left time: 1709.6309s\n",
      "\titers: 800, epoch: 2 | loss: 0.1139677\n",
      "\tspeed: 0.2140s/iter; left time: 1616.2491s\n",
      "\titers: 900, epoch: 2 | loss: 0.1379601\n",
      "\tspeed: 0.2213s/iter; left time: 1649.4652s\n",
      "Epoch: 2 running time: 3.3926676511764526 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1819220 Vali Loss: 0.2255300 Test Loss: 0.4859501\n",
      "Validation loss decreased (0.227737 --> 0.225530).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1195258\n",
      "\tspeed: 0.7745s/iter; left time: 5673.1547s\n",
      "\titers: 200, epoch: 3 | loss: 0.1319589\n",
      "\tspeed: 0.2185s/iter; left time: 1578.6818s\n",
      "\titers: 300, epoch: 3 | loss: 0.1488410\n",
      "\tspeed: 0.2295s/iter; left time: 1635.2761s\n",
      "\titers: 400, epoch: 3 | loss: 0.1390737\n",
      "\tspeed: 0.2167s/iter; left time: 1522.4170s\n",
      "\titers: 500, epoch: 3 | loss: 0.1392269\n",
      "\tspeed: 0.2355s/iter; left time: 1630.6242s\n",
      "\titers: 600, epoch: 3 | loss: 0.1109999\n",
      "\tspeed: 0.2175s/iter; left time: 1484.5317s\n",
      "\titers: 700, epoch: 3 | loss: 0.1452042\n",
      "\tspeed: 0.2239s/iter; left time: 1505.6309s\n",
      "\titers: 800, epoch: 3 | loss: 0.1490888\n",
      "\tspeed: 0.2194s/iter; left time: 1453.4115s\n",
      "\titers: 900, epoch: 3 | loss: 0.1005414\n",
      "\tspeed: 0.2185s/iter; left time: 1425.6277s\n",
      "Epoch: 3 running time: 3.4425020019213357 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1430971 Vali Loss: 0.2341083 Test Loss: 0.5486788\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1656382\n",
      "\tspeed: 0.7387s/iter; left time: 4725.2356s\n",
      "\titers: 200, epoch: 4 | loss: 0.1159478\n",
      "\tspeed: 0.2120s/iter; left time: 1334.6949s\n",
      "\titers: 300, epoch: 4 | loss: 0.1778239\n",
      "\tspeed: 0.2095s/iter; left time: 1298.0183s\n",
      "\titers: 400, epoch: 4 | loss: 0.1446034\n",
      "\tspeed: 0.2100s/iter; left time: 1280.6522s\n",
      "\titers: 500, epoch: 4 | loss: 0.1611169\n",
      "\tspeed: 0.2192s/iter; left time: 1314.6797s\n",
      "\titers: 600, epoch: 4 | loss: 0.1564849\n",
      "\tspeed: 0.2300s/iter; left time: 1356.1713s\n",
      "\titers: 700, epoch: 4 | loss: 0.1303415\n",
      "\tspeed: 0.2227s/iter; left time: 1291.2094s\n",
      "\titers: 800, epoch: 4 | loss: 0.1569451\n",
      "\tspeed: 0.2114s/iter; left time: 1204.4479s\n",
      "\titers: 900, epoch: 4 | loss: 0.1373687\n",
      "\tspeed: 0.2215s/iter; left time: 1239.9061s\n",
      "Epoch: 4 running time: 3.37516793012619 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1394093 Vali Loss: 0.2204870 Test Loss: 0.5289410\n",
      "Validation loss decreased (0.225530 --> 0.220487).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1581812\n",
      "\tspeed: 0.7384s/iter; left time: 4038.1941s\n",
      "\titers: 200, epoch: 5 | loss: 0.1351563\n",
      "\tspeed: 0.2111s/iter; left time: 1133.3029s\n",
      "\titers: 300, epoch: 5 | loss: 0.1260451\n",
      "\tspeed: 0.2171s/iter; left time: 1143.8796s\n",
      "\titers: 400, epoch: 5 | loss: 0.0954446\n",
      "\tspeed: 0.2122s/iter; left time: 1096.8872s\n",
      "\titers: 500, epoch: 5 | loss: 0.1417578\n",
      "\tspeed: 0.2158s/iter; left time: 1093.8490s\n",
      "\titers: 600, epoch: 5 | loss: 0.0966996\n",
      "\tspeed: 0.2351s/iter; left time: 1167.9675s\n",
      "\titers: 700, epoch: 5 | loss: 0.1205827\n",
      "\tspeed: 0.2256s/iter; left time: 1098.3659s\n",
      "\titers: 800, epoch: 5 | loss: 0.1236687\n",
      "\tspeed: 0.2014s/iter; left time: 960.4093s\n",
      "\titers: 900, epoch: 5 | loss: 0.1089422\n",
      "\tspeed: 0.2157s/iter; left time: 1007.3336s\n",
      "Epoch: 5 running time: 3.378078313668569 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1204051 Vali Loss: 0.2353316 Test Loss: 0.7861300\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1382838\n",
      "\tspeed: 0.7317s/iter; left time: 3322.7923s\n",
      "\titers: 200, epoch: 6 | loss: 0.1593824\n",
      "\tspeed: 0.2277s/iter; left time: 1011.0794s\n",
      "\titers: 300, epoch: 6 | loss: 0.1178460\n",
      "\tspeed: 0.2287s/iter; left time: 992.7895s\n",
      "\titers: 400, epoch: 6 | loss: 0.0982466\n",
      "\tspeed: 0.2368s/iter; left time: 1004.4245s\n",
      "\titers: 500, epoch: 6 | loss: 0.1203185\n",
      "\tspeed: 0.2135s/iter; left time: 884.1047s\n",
      "\titers: 600, epoch: 6 | loss: 0.1183658\n",
      "\tspeed: 0.2145s/iter; left time: 866.9282s\n",
      "\titers: 700, epoch: 6 | loss: 0.1245822\n",
      "\tspeed: 0.2087s/iter; left time: 822.4642s\n",
      "\titers: 800, epoch: 6 | loss: 0.0993444\n",
      "\tspeed: 0.2131s/iter; left time: 818.5037s\n",
      "\titers: 900, epoch: 6 | loss: 0.1245846\n",
      "\tspeed: 0.2136s/iter; left time: 799.1492s\n",
      "Epoch: 6 running time: 3.404575455188751 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.1203430 Vali Loss: 0.2149195 Test Loss: 0.7663371\n",
      "Validation loss decreased (0.220487 --> 0.214920).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1211145\n",
      "\tspeed: 0.7290s/iter; left time: 2633.8543s\n",
      "\titers: 200, epoch: 7 | loss: 0.1302898\n",
      "\tspeed: 0.2189s/iter; left time: 769.1428s\n",
      "\titers: 300, epoch: 7 | loss: 0.1178078\n",
      "\tspeed: 0.2112s/iter; left time: 720.6961s\n",
      "\titers: 400, epoch: 7 | loss: 0.1204915\n",
      "\tspeed: 0.2266s/iter; left time: 750.6522s\n",
      "\titers: 500, epoch: 7 | loss: 0.1240376\n",
      "\tspeed: 0.2196s/iter; left time: 705.4627s\n",
      "\titers: 600, epoch: 7 | loss: 0.1187112\n",
      "\tspeed: 0.2391s/iter; left time: 744.3547s\n",
      "\titers: 700, epoch: 7 | loss: 0.1062241\n",
      "\tspeed: 0.2238s/iter; left time: 674.2573s\n",
      "\titers: 800, epoch: 7 | loss: 0.1438784\n",
      "\tspeed: 0.2342s/iter; left time: 682.2457s\n",
      "\titers: 900, epoch: 7 | loss: 0.1151199\n",
      "\tspeed: 0.2396s/iter; left time: 674.0965s\n",
      "Epoch: 7 running time: 3.501260284582774 min.\n",
      "Epoch: 7, Steps: 928 | Train Loss: 0.1140314 Vali Loss: 0.2263145 Test Loss: 0.7530709\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0797758\n",
      "\tspeed: 0.7294s/iter; left time: 1958.4083s\n",
      "\titers: 200, epoch: 8 | loss: 0.1361746\n",
      "\tspeed: 0.2193s/iter; left time: 566.8814s\n",
      "\titers: 300, epoch: 8 | loss: 0.1381432\n",
      "\tspeed: 0.2134s/iter; left time: 530.2815s\n",
      "\titers: 400, epoch: 8 | loss: 0.1003275\n",
      "\tspeed: 0.2329s/iter; left time: 555.5269s\n",
      "\titers: 500, epoch: 8 | loss: 0.0825176\n",
      "\tspeed: 0.2101s/iter; left time: 480.1381s\n",
      "\titers: 600, epoch: 8 | loss: 0.1267942\n",
      "\tspeed: 0.2193s/iter; left time: 479.2637s\n",
      "\titers: 700, epoch: 8 | loss: 0.0912614\n",
      "\tspeed: 0.2085s/iter; left time: 434.7861s\n",
      "\titers: 800, epoch: 8 | loss: 0.0971316\n",
      "\tspeed: 0.2205s/iter; left time: 437.6093s\n",
      "\titers: 900, epoch: 8 | loss: 0.1127703\n",
      "\tspeed: 0.2149s/iter; left time: 405.1805s\n",
      "Epoch: 8 running time: 3.391059664885203 min.\n",
      "Epoch: 8, Steps: 928 | Train Loss: 0.1141634 Vali Loss: 0.2255553 Test Loss: 0.7575937\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1160442\n",
      "\tspeed: 0.7276s/iter; left time: 1278.4808s\n",
      "\titers: 200, epoch: 9 | loss: 0.1190113\n",
      "\tspeed: 0.2270s/iter; left time: 376.1404s\n",
      "\titers: 300, epoch: 9 | loss: 0.1529249\n",
      "\tspeed: 0.2115s/iter; left time: 329.3016s\n",
      "\titers: 400, epoch: 9 | loss: 0.1225664\n",
      "\tspeed: 0.2125s/iter; left time: 309.6041s\n",
      "\titers: 500, epoch: 9 | loss: 0.0974206\n",
      "\tspeed: 0.2175s/iter; left time: 295.1522s\n",
      "\titers: 600, epoch: 9 | loss: 0.1361649\n",
      "\tspeed: 0.2245s/iter; left time: 282.1350s\n",
      "\titers: 700, epoch: 9 | loss: 0.0978478\n",
      "\tspeed: 0.2352s/iter; left time: 272.1473s\n",
      "\titers: 800, epoch: 9 | loss: 0.1242841\n",
      "\tspeed: 0.2264s/iter; left time: 239.3233s\n",
      "\titers: 900, epoch: 9 | loss: 0.1347261\n",
      "\tspeed: 0.2301s/iter; left time: 220.1925s\n",
      "Epoch: 9 running time: 3.4538586934407554 min.\n",
      "Epoch: 9, Steps: 928 | Train Loss: 0.1142999 Vali Loss: 0.2229156 Test Loss: 0.7365579\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.7333622574806213, mae:0.5520970821380615\n",
      "\n",
      "Time intermediate for ES dataset: 78.95431648095449 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4198261\n",
      "\tspeed: 0.2370s/iter; left time: 2176.2955s\n",
      "\titers: 200, epoch: 1 | loss: 0.3194746\n",
      "\tspeed: 0.2175s/iter; left time: 1975.1699s\n",
      "\titers: 300, epoch: 1 | loss: 0.3066442\n",
      "\tspeed: 0.2094s/iter; left time: 1880.2400s\n",
      "\titers: 400, epoch: 1 | loss: 0.2289752\n",
      "\tspeed: 0.2205s/iter; left time: 1958.5581s\n",
      "\titers: 500, epoch: 1 | loss: 0.2139557\n",
      "\tspeed: 0.2174s/iter; left time: 1909.1510s\n",
      "\titers: 600, epoch: 1 | loss: 0.3013504\n",
      "\tspeed: 0.2167s/iter; left time: 1881.4590s\n",
      "\titers: 700, epoch: 1 | loss: 0.2550539\n",
      "\tspeed: 0.2225s/iter; left time: 1909.1803s\n",
      "\titers: 800, epoch: 1 | loss: 0.1937786\n",
      "\tspeed: 0.2110s/iter; left time: 1789.1693s\n",
      "\titers: 900, epoch: 1 | loss: 0.2039541\n",
      "\tspeed: 0.2163s/iter; left time: 1812.5119s\n",
      "Epoch: 1 running time: 3.3649957299232485 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3249407 Vali Loss: 0.3279966 Test Loss: 0.4189684\n",
      "Validation loss decreased (inf --> 0.327997).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2206074\n",
      "\tspeed: 0.7404s/iter; left time: 6110.5159s\n",
      "\titers: 200, epoch: 2 | loss: 0.1420978\n",
      "\tspeed: 0.2272s/iter; left time: 1852.1071s\n",
      "\titers: 300, epoch: 2 | loss: 0.1692234\n",
      "\tspeed: 0.2186s/iter; left time: 1760.6786s\n",
      "\titers: 400, epoch: 2 | loss: 0.2245784\n",
      "\tspeed: 0.2278s/iter; left time: 1811.6042s\n",
      "\titers: 500, epoch: 2 | loss: 0.1574414\n",
      "\tspeed: 0.2159s/iter; left time: 1695.4398s\n",
      "\titers: 600, epoch: 2 | loss: 0.2061567\n",
      "\tspeed: 0.2203s/iter; left time: 1707.5989s\n",
      "\titers: 700, epoch: 2 | loss: 0.0997239\n",
      "\tspeed: 0.2333s/iter; left time: 1785.7178s\n",
      "\titers: 800, epoch: 2 | loss: 0.1542777\n",
      "\tspeed: 0.2319s/iter; left time: 1751.5496s\n",
      "\titers: 900, epoch: 2 | loss: 0.1597306\n",
      "\tspeed: 0.2194s/iter; left time: 1635.5477s\n",
      "Epoch: 2 running time: 3.4673680305480956 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1866410 Vali Loss: 0.3044862 Test Loss: 0.4551168\n",
      "Validation loss decreased (0.327997 --> 0.304486).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1116059\n",
      "\tspeed: 0.7205s/iter; left time: 5277.3059s\n",
      "\titers: 200, epoch: 3 | loss: 0.1297897\n",
      "\tspeed: 0.2158s/iter; left time: 1559.3115s\n",
      "\titers: 300, epoch: 3 | loss: 0.1430361\n",
      "\tspeed: 0.2111s/iter; left time: 1503.9150s\n",
      "\titers: 400, epoch: 3 | loss: 0.1455229\n",
      "\tspeed: 0.2211s/iter; left time: 1552.9758s\n",
      "\titers: 500, epoch: 3 | loss: 0.1524698\n",
      "\tspeed: 0.2133s/iter; left time: 1476.9550s\n",
      "\titers: 600, epoch: 3 | loss: 0.1099781\n",
      "\tspeed: 0.2167s/iter; left time: 1478.8582s\n",
      "\titers: 700, epoch: 3 | loss: 0.1406624\n",
      "\tspeed: 0.2169s/iter; left time: 1458.7658s\n",
      "\titers: 800, epoch: 3 | loss: 0.1277185\n",
      "\tspeed: 0.2084s/iter; left time: 1380.4594s\n",
      "\titers: 900, epoch: 3 | loss: 0.1074370\n",
      "\tspeed: 0.2223s/iter; left time: 1450.3204s\n",
      "Epoch: 3 running time: 3.3540127635002137 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1336764 Vali Loss: 0.3908321 Test Loss: 0.6462814\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1292737\n",
      "\tspeed: 0.7131s/iter; left time: 4561.9120s\n",
      "\titers: 200, epoch: 4 | loss: 0.1466596\n",
      "\tspeed: 0.2192s/iter; left time: 1380.5886s\n",
      "\titers: 300, epoch: 4 | loss: 0.1094998\n",
      "\tspeed: 0.2163s/iter; left time: 1340.2488s\n",
      "\titers: 400, epoch: 4 | loss: 0.1369257\n",
      "\tspeed: 0.2213s/iter; left time: 1349.0869s\n",
      "\titers: 500, epoch: 4 | loss: 0.0956849\n",
      "\tspeed: 0.2109s/iter; left time: 1264.5110s\n",
      "\titers: 600, epoch: 4 | loss: 0.1200634\n",
      "\tspeed: 0.2087s/iter; left time: 1230.6493s\n",
      "\titers: 700, epoch: 4 | loss: 0.0895878\n",
      "\tspeed: 0.2243s/iter; left time: 1300.2490s\n",
      "\titers: 800, epoch: 4 | loss: 0.1267439\n",
      "\tspeed: 0.2228s/iter; left time: 1269.0608s\n",
      "\titers: 900, epoch: 4 | loss: 0.0957237\n",
      "\tspeed: 0.2129s/iter; left time: 1191.6815s\n",
      "Epoch: 4 running time: 3.3604551394780477 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1297656 Vali Loss: 0.3251540 Test Loss: 0.5085688\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1217635\n",
      "\tspeed: 0.7381s/iter; left time: 4036.9266s\n",
      "\titers: 200, epoch: 5 | loss: 0.1653290\n",
      "\tspeed: 0.2350s/iter; left time: 1261.6922s\n",
      "\titers: 300, epoch: 5 | loss: 0.1314113\n",
      "\tspeed: 0.2447s/iter; left time: 1289.5325s\n",
      "\titers: 400, epoch: 5 | loss: 0.1791107\n",
      "\tspeed: 0.2217s/iter; left time: 1145.8758s\n",
      "\titers: 500, epoch: 5 | loss: 0.1526350\n",
      "\tspeed: 0.2112s/iter; left time: 1070.7908s\n",
      "\titers: 600, epoch: 5 | loss: 0.1105525\n",
      "\tspeed: 0.2182s/iter; left time: 1084.0711s\n",
      "\titers: 700, epoch: 5 | loss: 0.1770318\n",
      "\tspeed: 0.2130s/iter; left time: 1037.3208s\n",
      "\titers: 800, epoch: 5 | loss: 0.1423966\n",
      "\tspeed: 0.2234s/iter; left time: 1065.4877s\n",
      "\titers: 900, epoch: 5 | loss: 0.1182408\n",
      "\tspeed: 0.2178s/iter; left time: 1016.7036s\n",
      "Epoch: 5 running time: 3.5113924702008563 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1294095 Vali Loss: 0.3392290 Test Loss: 0.5515720\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5524942278862, mae:0.416003942489624\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5986541\n",
      "\tspeed: 0.2229s/iter; left time: 2046.1485s\n",
      "\titers: 200, epoch: 1 | loss: 0.4920947\n",
      "\tspeed: 0.2236s/iter; left time: 2030.5051s\n",
      "\titers: 300, epoch: 1 | loss: 0.3708616\n",
      "\tspeed: 0.2158s/iter; left time: 1938.3793s\n",
      "\titers: 400, epoch: 1 | loss: 0.2568259\n",
      "\tspeed: 0.2425s/iter; left time: 2153.3286s\n",
      "\titers: 500, epoch: 1 | loss: 0.2954612\n",
      "\tspeed: 0.2198s/iter; left time: 1929.7693s\n",
      "\titers: 600, epoch: 1 | loss: 0.2252063\n",
      "\tspeed: 0.2119s/iter; left time: 1839.9331s\n",
      "\titers: 700, epoch: 1 | loss: 0.2090849\n",
      "\tspeed: 0.2084s/iter; left time: 1788.3170s\n",
      "\titers: 800, epoch: 1 | loss: 0.1796948\n",
      "\tspeed: 0.2193s/iter; left time: 1860.0783s\n",
      "\titers: 900, epoch: 1 | loss: 0.1779559\n",
      "\tspeed: 0.2204s/iter; left time: 1847.0414s\n",
      "Epoch: 1 running time: 3.421979669729869 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3206470 Vali Loss: 0.3626406 Test Loss: 0.4930951\n",
      "Validation loss decreased (inf --> 0.362641).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1830090\n",
      "\tspeed: 0.7635s/iter; left time: 6301.4435s\n",
      "\titers: 200, epoch: 2 | loss: 0.2411208\n",
      "\tspeed: 0.2199s/iter; left time: 1792.6250s\n",
      "\titers: 300, epoch: 2 | loss: 0.1811313\n",
      "\tspeed: 0.2163s/iter; left time: 1741.8843s\n",
      "\titers: 400, epoch: 2 | loss: 0.1658240\n",
      "\tspeed: 0.2320s/iter; left time: 1845.3564s\n",
      "\titers: 500, epoch: 2 | loss: 0.2401054\n",
      "\tspeed: 0.2363s/iter; left time: 1855.3786s\n",
      "\titers: 600, epoch: 2 | loss: 0.1468146\n",
      "\tspeed: 0.2481s/iter; left time: 1923.4099s\n",
      "\titers: 700, epoch: 2 | loss: 0.1155555\n",
      "\tspeed: 0.2167s/iter; left time: 1658.1856s\n",
      "\titers: 800, epoch: 2 | loss: 0.2069310\n",
      "\tspeed: 0.2218s/iter; left time: 1675.6181s\n",
      "\titers: 900, epoch: 2 | loss: 0.1952388\n",
      "\tspeed: 0.2126s/iter; left time: 1584.6433s\n",
      "Epoch: 2 running time: 3.5087326486905415 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1822533 Vali Loss: 0.3502125 Test Loss: 0.5663123\n",
      "Validation loss decreased (0.362641 --> 0.350213).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1263138\n",
      "\tspeed: 0.7355s/iter; left time: 5387.3375s\n",
      "\titers: 200, epoch: 3 | loss: 0.1300225\n",
      "\tspeed: 0.2099s/iter; left time: 1516.5650s\n",
      "\titers: 300, epoch: 3 | loss: 0.1721862\n",
      "\tspeed: 0.2142s/iter; left time: 1526.4861s\n",
      "\titers: 400, epoch: 3 | loss: 0.1038932\n",
      "\tspeed: 0.2157s/iter; left time: 1515.5124s\n",
      "\titers: 500, epoch: 3 | loss: 0.0910043\n",
      "\tspeed: 0.2247s/iter; left time: 1556.2641s\n",
      "\titers: 600, epoch: 3 | loss: 0.1340497\n",
      "\tspeed: 0.2145s/iter; left time: 1464.1615s\n",
      "\titers: 700, epoch: 3 | loss: 0.0892724\n",
      "\tspeed: 0.2247s/iter; left time: 1511.4145s\n",
      "\titers: 800, epoch: 3 | loss: 0.1433773\n",
      "\tspeed: 0.2211s/iter; left time: 1464.8272s\n",
      "\titers: 900, epoch: 3 | loss: 0.0970182\n",
      "\tspeed: 0.2331s/iter; left time: 1521.2030s\n",
      "Epoch: 3 running time: 3.4100214719772337 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1371782 Vali Loss: 0.3068826 Test Loss: 0.4539941\n",
      "Validation loss decreased (0.350213 --> 0.306883).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1062823\n",
      "\tspeed: 0.7674s/iter; left time: 4908.7401s\n",
      "\titers: 200, epoch: 4 | loss: 0.1074571\n",
      "\tspeed: 0.2306s/iter; left time: 1452.2513s\n",
      "\titers: 300, epoch: 4 | loss: 0.0977832\n",
      "\tspeed: 0.2309s/iter; left time: 1430.6474s\n",
      "\titers: 400, epoch: 4 | loss: 0.1084305\n",
      "\tspeed: 0.2299s/iter; left time: 1401.9872s\n",
      "\titers: 500, epoch: 4 | loss: 0.0747710\n",
      "\tspeed: 0.2245s/iter; left time: 1346.1834s\n",
      "\titers: 600, epoch: 4 | loss: 0.1385528\n",
      "\tspeed: 0.2110s/iter; left time: 1244.2617s\n",
      "\titers: 700, epoch: 4 | loss: 0.1102511\n",
      "\tspeed: 0.2122s/iter; left time: 1229.8341s\n",
      "\titers: 800, epoch: 4 | loss: 0.1035782\n",
      "\tspeed: 0.2182s/iter; left time: 1243.0952s\n",
      "\titers: 900, epoch: 4 | loss: 0.0992812\n",
      "\tspeed: 0.2186s/iter; left time: 1223.5526s\n",
      "Epoch: 4 running time: 3.4292178948720298 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1073275 Vali Loss: 0.3374715 Test Loss: 0.4736606\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0766002\n",
      "\tspeed: 0.7300s/iter; left time: 3992.6353s\n",
      "\titers: 200, epoch: 5 | loss: 0.1259600\n",
      "\tspeed: 0.2167s/iter; left time: 1163.6962s\n",
      "\titers: 300, epoch: 5 | loss: 0.1122619\n",
      "\tspeed: 0.2252s/iter; left time: 1186.3418s\n",
      "\titers: 400, epoch: 5 | loss: 0.1024820\n",
      "\tspeed: 0.2178s/iter; left time: 1125.5661s\n",
      "\titers: 500, epoch: 5 | loss: 0.0983990\n",
      "\tspeed: 0.1813s/iter; left time: 918.7935s\n",
      "\titers: 600, epoch: 5 | loss: 0.1254518\n",
      "\tspeed: 0.1716s/iter; left time: 852.6962s\n",
      "\titers: 700, epoch: 5 | loss: 0.1143795\n",
      "\tspeed: 0.1754s/iter; left time: 853.9348s\n",
      "\titers: 800, epoch: 5 | loss: 0.1058686\n",
      "\tspeed: 0.1736s/iter; left time: 827.8629s\n",
      "\titers: 900, epoch: 5 | loss: 0.1028120\n",
      "\tspeed: 0.1745s/iter; left time: 814.6551s\n",
      "Epoch: 5 running time: 3.01177103916804 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1063839 Vali Loss: 0.3389081 Test Loss: 0.5285038\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1080203\n",
      "\tspeed: 0.5843s/iter; left time: 2653.1028s\n",
      "\titers: 200, epoch: 6 | loss: 0.0949952\n",
      "\tspeed: 0.1816s/iter; left time: 806.3711s\n",
      "\titers: 300, epoch: 6 | loss: 0.1152635\n",
      "\tspeed: 0.1776s/iter; left time: 771.1103s\n",
      "\titers: 400, epoch: 6 | loss: 0.1304547\n",
      "\tspeed: 0.1808s/iter; left time: 766.7019s\n",
      "\titers: 500, epoch: 6 | loss: 0.1238176\n",
      "\tspeed: 0.1779s/iter; left time: 736.6451s\n",
      "\titers: 600, epoch: 6 | loss: 0.0780342\n",
      "\tspeed: 0.1765s/iter; left time: 713.2560s\n",
      "\titers: 700, epoch: 6 | loss: 0.0884783\n",
      "\tspeed: 0.1757s/iter; left time: 692.2725s\n",
      "\titers: 800, epoch: 6 | loss: 0.1017065\n",
      "\tspeed: 0.1806s/iter; left time: 693.7920s\n",
      "\titers: 900, epoch: 6 | loss: 0.0841121\n",
      "\tspeed: 0.1788s/iter; left time: 668.9506s\n",
      "Epoch: 6 running time: 2.7742780486742658 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.1065005 Vali Loss: 0.3302884 Test Loss: 0.5051561\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5053262114524841, mae:0.39145344495773315\n",
      "\n",
      "Time intermediate for FR dataset: 45.39894268512726 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5948445\n",
      "\tspeed: 0.2075s/iter; left time: 1905.4523s\n",
      "\titers: 200, epoch: 1 | loss: 0.3137118\n",
      "\tspeed: 0.1824s/iter; left time: 1656.2158s\n",
      "\titers: 300, epoch: 1 | loss: 0.3050707\n",
      "\tspeed: 0.1829s/iter; left time: 1642.3681s\n",
      "\titers: 400, epoch: 1 | loss: 0.2142981\n",
      "\tspeed: 0.1812s/iter; left time: 1609.2651s\n",
      "\titers: 500, epoch: 1 | loss: 0.2534176\n",
      "\tspeed: 0.1840s/iter; left time: 1615.3855s\n",
      "\titers: 600, epoch: 1 | loss: 0.2895187\n",
      "\tspeed: 0.1818s/iter; left time: 1578.0168s\n",
      "\titers: 700, epoch: 1 | loss: 0.2615122\n",
      "\tspeed: 0.1841s/iter; left time: 1580.0572s\n",
      "\titers: 800, epoch: 1 | loss: 0.1668461\n",
      "\tspeed: 0.1869s/iter; left time: 1585.0427s\n",
      "\titers: 900, epoch: 1 | loss: 0.1469535\n",
      "\tspeed: 0.1819s/iter; left time: 1524.1088s\n",
      "Epoch: 1 running time: 2.856202701727549 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3309264 Vali Loss: 0.2197509 Test Loss: 0.2517466\n",
      "Validation loss decreased (inf --> 0.219751).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2055304\n",
      "\tspeed: 0.5987s/iter; left time: 4941.3436s\n",
      "\titers: 200, epoch: 2 | loss: 0.2192316\n",
      "\tspeed: 0.1832s/iter; left time: 1493.6512s\n",
      "\titers: 300, epoch: 2 | loss: 0.2165480\n",
      "\tspeed: 0.1805s/iter; left time: 1453.9039s\n",
      "\titers: 400, epoch: 2 | loss: 0.2391240\n",
      "\tspeed: 0.1852s/iter; left time: 1472.6906s\n",
      "\titers: 500, epoch: 2 | loss: 0.1964405\n",
      "\tspeed: 0.1799s/iter; left time: 1413.0106s\n",
      "\titers: 600, epoch: 2 | loss: 0.1463928\n",
      "\tspeed: 0.1766s/iter; left time: 1368.8365s\n",
      "\titers: 700, epoch: 2 | loss: 0.1395702\n",
      "\tspeed: 0.1718s/iter; left time: 1314.8240s\n",
      "\titers: 800, epoch: 2 | loss: 0.1694333\n",
      "\tspeed: 0.1794s/iter; left time: 1355.2251s\n",
      "\titers: 900, epoch: 2 | loss: 0.1357260\n",
      "\tspeed: 0.1775s/iter; left time: 1323.0256s\n",
      "Epoch: 2 running time: 2.7901930173238116 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1861585 Vali Loss: 0.1942947 Test Loss: 0.2503152\n",
      "Validation loss decreased (0.219751 --> 0.194295).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1096328\n",
      "\tspeed: 0.6126s/iter; left time: 4487.3319s\n",
      "\titers: 200, epoch: 3 | loss: 0.1392770\n",
      "\tspeed: 0.1788s/iter; left time: 1291.8126s\n",
      "\titers: 300, epoch: 3 | loss: 0.1790193\n",
      "\tspeed: 0.1726s/iter; left time: 1230.0662s\n",
      "\titers: 400, epoch: 3 | loss: 0.1234157\n",
      "\tspeed: 0.1786s/iter; left time: 1254.5963s\n",
      "\titers: 500, epoch: 3 | loss: 0.1360627\n",
      "\tspeed: 0.1798s/iter; left time: 1245.1654s\n",
      "\titers: 600, epoch: 3 | loss: 0.1113684\n",
      "\tspeed: 0.1806s/iter; left time: 1232.3629s\n",
      "\titers: 700, epoch: 3 | loss: 0.1148054\n",
      "\tspeed: 0.1850s/iter; left time: 1244.4422s\n",
      "\titers: 800, epoch: 3 | loss: 0.1259226\n",
      "\tspeed: 0.1812s/iter; left time: 1200.4007s\n",
      "\titers: 900, epoch: 3 | loss: 0.1528486\n",
      "\tspeed: 0.1797s/iter; left time: 1172.6765s\n",
      "Epoch: 3 running time: 2.7837960680325824 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1459047 Vali Loss: 0.1970121 Test Loss: 0.2462642\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1648377\n",
      "\tspeed: 0.5723s/iter; left time: 3660.7276s\n",
      "\titers: 200, epoch: 4 | loss: 0.1931548\n",
      "\tspeed: 0.1763s/iter; left time: 1109.8703s\n",
      "\titers: 300, epoch: 4 | loss: 0.1303199\n",
      "\tspeed: 0.1766s/iter; left time: 1094.6438s\n",
      "\titers: 400, epoch: 4 | loss: 0.1084270\n",
      "\tspeed: 0.1789s/iter; left time: 1090.5626s\n",
      "\titers: 500, epoch: 4 | loss: 0.1336843\n",
      "\tspeed: 0.1772s/iter; left time: 1062.3729s\n",
      "\titers: 600, epoch: 4 | loss: 0.1357737\n",
      "\tspeed: 0.1811s/iter; left time: 1068.0933s\n",
      "\titers: 700, epoch: 4 | loss: 0.1209709\n",
      "\tspeed: 0.1749s/iter; left time: 1013.8825s\n",
      "\titers: 800, epoch: 4 | loss: 0.1557422\n",
      "\tspeed: 0.1779s/iter; left time: 1013.4197s\n",
      "\titers: 900, epoch: 4 | loss: 0.1267643\n",
      "\tspeed: 0.1804s/iter; left time: 1009.4676s\n",
      "Epoch: 4 running time: 2.7561147689819334 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1403524 Vali Loss: 0.1958514 Test Loss: 0.2545160\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1660999\n",
      "\tspeed: 0.5712s/iter; left time: 3123.7512s\n",
      "\titers: 200, epoch: 5 | loss: 0.2006668\n",
      "\tspeed: 0.1794s/iter; left time: 963.1702s\n",
      "\titers: 300, epoch: 5 | loss: 0.1531962\n",
      "\tspeed: 0.1764s/iter; left time: 929.5993s\n",
      "\titers: 400, epoch: 5 | loss: 0.1444213\n",
      "\tspeed: 0.1741s/iter; left time: 899.7029s\n",
      "\titers: 500, epoch: 5 | loss: 0.1417822\n",
      "\tspeed: 0.1818s/iter; left time: 921.4457s\n",
      "\titers: 600, epoch: 5 | loss: 0.1055227\n",
      "\tspeed: 0.1810s/iter; left time: 899.1542s\n",
      "\titers: 700, epoch: 5 | loss: 0.1157189\n",
      "\tspeed: 0.1812s/iter; left time: 882.4612s\n",
      "\titers: 800, epoch: 5 | loss: 0.1326165\n",
      "\tspeed: 0.1821s/iter; left time: 868.2577s\n",
      "\titers: 900, epoch: 5 | loss: 0.1537693\n",
      "\tspeed: 0.1829s/iter; left time: 853.9215s\n",
      "Epoch: 5 running time: 2.793397104740143 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1397551 Vali Loss: 0.1926194 Test Loss: 0.2488521\n",
      "Validation loss decreased (0.194295 --> 0.192619).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1352777\n",
      "\tspeed: 0.5712s/iter; left time: 2593.7258s\n",
      "\titers: 200, epoch: 6 | loss: 0.1090058\n",
      "\tspeed: 0.1796s/iter; left time: 797.5044s\n",
      "\titers: 300, epoch: 6 | loss: 0.0791817\n",
      "\tspeed: 0.1817s/iter; left time: 788.6974s\n",
      "\titers: 400, epoch: 6 | loss: 0.1413531\n",
      "\tspeed: 0.1822s/iter; left time: 772.7839s\n",
      "\titers: 500, epoch: 6 | loss: 0.0962419\n",
      "\tspeed: 0.1802s/iter; left time: 746.0856s\n",
      "\titers: 600, epoch: 6 | loss: 0.0927272\n",
      "\tspeed: 0.1794s/iter; left time: 725.0065s\n",
      "\titers: 700, epoch: 6 | loss: 0.1017943\n",
      "\tspeed: 0.1807s/iter; left time: 712.3218s\n",
      "\titers: 800, epoch: 6 | loss: 0.1154261\n",
      "\tspeed: 0.1767s/iter; left time: 678.8907s\n",
      "\titers: 900, epoch: 6 | loss: 0.1515590\n",
      "\tspeed: 0.1756s/iter; left time: 656.7558s\n",
      "Epoch: 6 running time: 2.7828286488850913 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.1259266 Vali Loss: 0.1949924 Test Loss: 0.2527017\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1729407\n",
      "\tspeed: 0.5391s/iter; left time: 1947.6550s\n",
      "\titers: 200, epoch: 7 | loss: 0.1348797\n",
      "\tspeed: 0.1710s/iter; left time: 600.8898s\n",
      "\titers: 300, epoch: 7 | loss: 0.1430940\n",
      "\tspeed: 0.1747s/iter; left time: 596.3565s\n",
      "\titers: 400, epoch: 7 | loss: 0.1450382\n",
      "\tspeed: 0.1717s/iter; left time: 568.9975s\n",
      "\titers: 500, epoch: 7 | loss: 0.1310306\n",
      "\tspeed: 0.1728s/iter; left time: 555.2804s\n",
      "\titers: 600, epoch: 7 | loss: 0.0859069\n",
      "\tspeed: 0.1716s/iter; left time: 534.1428s\n",
      "\titers: 700, epoch: 7 | loss: 0.1037397\n",
      "\tspeed: 0.1735s/iter; left time: 522.7982s\n",
      "\titers: 800, epoch: 7 | loss: 0.1277768\n",
      "\tspeed: 0.1718s/iter; left time: 500.3504s\n",
      "\titers: 900, epoch: 7 | loss: 0.1179371\n",
      "\tspeed: 0.1694s/iter; left time: 476.5559s\n",
      "Epoch: 7 running time: 2.6651385903358458 min.\n",
      "Epoch: 7, Steps: 928 | Train Loss: 0.1262682 Vali Loss: 0.1955309 Test Loss: 0.2447750\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1609773\n",
      "\tspeed: 0.5609s/iter; left time: 1505.8935s\n",
      "\titers: 200, epoch: 8 | loss: 0.1074493\n",
      "\tspeed: 0.1714s/iter; left time: 443.1911s\n",
      "\titers: 300, epoch: 8 | loss: 0.1698424\n",
      "\tspeed: 0.1707s/iter; left time: 424.1385s\n",
      "\titers: 400, epoch: 8 | loss: 0.1432760\n",
      "\tspeed: 0.1669s/iter; left time: 398.1636s\n",
      "\titers: 500, epoch: 8 | loss: 0.1076733\n",
      "\tspeed: 0.1616s/iter; left time: 369.1632s\n",
      "\titers: 600, epoch: 8 | loss: 0.1261444\n",
      "\tspeed: 0.1610s/iter; left time: 351.8400s\n",
      "\titers: 700, epoch: 8 | loss: 0.1507255\n",
      "\tspeed: 0.1593s/iter; left time: 332.0806s\n",
      "\titers: 800, epoch: 8 | loss: 0.1249737\n",
      "\tspeed: 0.1529s/iter; left time: 303.4380s\n",
      "\titers: 900, epoch: 8 | loss: 0.1204470\n",
      "\tspeed: 0.1569s/iter; left time: 295.7570s\n",
      "Epoch: 8 running time: 2.546384871006012 min.\n",
      "Epoch: 8, Steps: 928 | Train Loss: 0.1265634 Vali Loss: 0.1923497 Test Loss: 0.2454326\n",
      "Validation loss decreased (0.192619 --> 0.192350).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0918119\n",
      "\tspeed: 0.4679s/iter; left time: 822.1541s\n",
      "\titers: 200, epoch: 9 | loss: 0.1152987\n",
      "\tspeed: 0.1553s/iter; left time: 257.2518s\n",
      "\titers: 300, epoch: 9 | loss: 0.1377630\n",
      "\tspeed: 0.1555s/iter; left time: 242.1313s\n",
      "\titers: 400, epoch: 9 | loss: 0.1493542\n",
      "\tspeed: 0.1550s/iter; left time: 225.8666s\n",
      "\titers: 500, epoch: 9 | loss: 0.1051709\n",
      "\tspeed: 0.1551s/iter; left time: 210.4635s\n",
      "\titers: 600, epoch: 9 | loss: 0.0884599\n",
      "\tspeed: 0.1553s/iter; left time: 195.1934s\n",
      "\titers: 700, epoch: 9 | loss: 0.1293938\n",
      "\tspeed: 0.1554s/iter; left time: 179.7983s\n",
      "\titers: 800, epoch: 9 | loss: 0.1216945\n",
      "\tspeed: 0.1555s/iter; left time: 164.3491s\n",
      "\titers: 900, epoch: 9 | loss: 0.1227430\n",
      "\tspeed: 0.1555s/iter; left time: 148.7705s\n",
      "Epoch: 9 running time: 2.4089552402496337 min.\n",
      "Epoch: 9, Steps: 928 | Train Loss: 0.1241857 Vali Loss: 0.1930519 Test Loss: 0.2469041\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0904140\n",
      "\tspeed: 0.4549s/iter; left time: 377.0939s\n",
      "\titers: 200, epoch: 10 | loss: 0.1094718\n",
      "\tspeed: 0.1549s/iter; left time: 112.9262s\n",
      "\titers: 300, epoch: 10 | loss: 0.1460893\n",
      "\tspeed: 0.1551s/iter; left time: 97.5891s\n",
      "\titers: 400, epoch: 10 | loss: 0.1196252\n",
      "\tspeed: 0.1526s/iter; left time: 80.7088s\n",
      "\titers: 500, epoch: 10 | loss: 0.1093522\n",
      "\tspeed: 0.1492s/iter; left time: 64.0132s\n",
      "\titers: 600, epoch: 10 | loss: 0.1065638\n",
      "\tspeed: 0.1468s/iter; left time: 48.2924s\n",
      "\titers: 700, epoch: 10 | loss: 0.1217977\n",
      "\tspeed: 0.1468s/iter; left time: 33.6216s\n",
      "\titers: 800, epoch: 10 | loss: 0.1384902\n",
      "\tspeed: 0.1496s/iter; left time: 19.2999s\n",
      "\titers: 900, epoch: 10 | loss: 0.1235683\n",
      "\tspeed: 0.1550s/iter; left time: 4.4957s\n",
      "Epoch: 10 running time: 2.353959894180298 min.\n",
      "Epoch: 10, Steps: 928 | Train Loss: 0.1241756 Vali Loss: 0.1931823 Test Loss: 0.2466850\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.24546140432357788, mae:0.31220290064811707\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29705\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6704766\n",
      "\tspeed: 0.1571s/iter; left time: 1442.7191s\n",
      "\titers: 200, epoch: 1 | loss: 0.3873678\n",
      "\tspeed: 0.1550s/iter; left time: 1407.9091s\n",
      "\titers: 300, epoch: 1 | loss: 0.3364401\n",
      "\tspeed: 0.1554s/iter; left time: 1395.3890s\n",
      "\titers: 400, epoch: 1 | loss: 0.2153812\n",
      "\tspeed: 0.1554s/iter; left time: 1380.3528s\n",
      "\titers: 500, epoch: 1 | loss: 0.2505493\n",
      "\tspeed: 0.1554s/iter; left time: 1364.3599s\n",
      "\titers: 600, epoch: 1 | loss: 0.2214930\n",
      "\tspeed: 0.1551s/iter; left time: 1346.2202s\n",
      "\titers: 700, epoch: 1 | loss: 0.2020128\n",
      "\tspeed: 0.1554s/iter; left time: 1333.6849s\n",
      "\titers: 800, epoch: 1 | loss: 0.2108610\n",
      "\tspeed: 0.1553s/iter; left time: 1317.0072s\n",
      "\titers: 900, epoch: 1 | loss: 0.2329813\n",
      "\tspeed: 0.1555s/iter; left time: 1302.8399s\n",
      "Epoch: 1 running time: 2.4077902396519977 min.\n",
      "Epoch: 1, Steps: 928 | Train Loss: 0.3478113 Vali Loss: 0.2054080 Test Loss: 0.2479245\n",
      "Validation loss decreased (inf --> 0.205408).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2089171\n",
      "\tspeed: 0.4618s/iter; left time: 3811.4669s\n",
      "\titers: 200, epoch: 2 | loss: 0.1628228\n",
      "\tspeed: 0.1550s/iter; left time: 1263.9940s\n",
      "\titers: 300, epoch: 2 | loss: 0.2257443\n",
      "\tspeed: 0.1551s/iter; left time: 1249.4035s\n",
      "\titers: 400, epoch: 2 | loss: 0.1739377\n",
      "\tspeed: 0.1565s/iter; left time: 1244.3192s\n",
      "\titers: 500, epoch: 2 | loss: 0.1627107\n",
      "\tspeed: 0.1562s/iter; left time: 1226.6615s\n",
      "\titers: 600, epoch: 2 | loss: 0.1842586\n",
      "\tspeed: 0.1553s/iter; left time: 1204.2332s\n",
      "\titers: 700, epoch: 2 | loss: 0.1538051\n",
      "\tspeed: 0.1565s/iter; left time: 1197.7198s\n",
      "\titers: 800, epoch: 2 | loss: 0.2175616\n",
      "\tspeed: 0.1566s/iter; left time: 1182.6939s\n",
      "\titers: 900, epoch: 2 | loss: 0.1750983\n",
      "\tspeed: 0.1558s/iter; left time: 1160.8750s\n",
      "Epoch: 2 running time: 2.414795450369517 min.\n",
      "Epoch: 2, Steps: 928 | Train Loss: 0.1877092 Vali Loss: 0.2042387 Test Loss: 0.2439435\n",
      "Validation loss decreased (0.205408 --> 0.204239).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1199070\n",
      "\tspeed: 0.4694s/iter; left time: 3438.3451s\n",
      "\titers: 200, epoch: 3 | loss: 0.1599158\n",
      "\tspeed: 0.1552s/iter; left time: 1121.0347s\n",
      "\titers: 300, epoch: 3 | loss: 0.1426761\n",
      "\tspeed: 0.1551s/iter; left time: 1105.3406s\n",
      "\titers: 400, epoch: 3 | loss: 0.1523114\n",
      "\tspeed: 0.1552s/iter; left time: 1090.2685s\n",
      "\titers: 500, epoch: 3 | loss: 0.1564855\n",
      "\tspeed: 0.1553s/iter; left time: 1075.1984s\n",
      "\titers: 600, epoch: 3 | loss: 0.1323139\n",
      "\tspeed: 0.1552s/iter; left time: 1059.3131s\n",
      "\titers: 700, epoch: 3 | loss: 0.1407535\n",
      "\tspeed: 0.1553s/iter; left time: 1044.5450s\n",
      "\titers: 800, epoch: 3 | loss: 0.1751176\n",
      "\tspeed: 0.1551s/iter; left time: 1027.6897s\n",
      "\titers: 900, epoch: 3 | loss: 0.1539092\n",
      "\tspeed: 0.1551s/iter; left time: 1012.3211s\n",
      "Epoch: 3 running time: 2.4053981701533 min.\n",
      "Epoch: 3, Steps: 928 | Train Loss: 0.1499913 Vali Loss: 0.1841172 Test Loss: 0.2521711\n",
      "Validation loss decreased (0.204239 --> 0.184117).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1421318\n",
      "\tspeed: 0.4653s/iter; left time: 2976.6189s\n",
      "\titers: 200, epoch: 4 | loss: 0.1435197\n",
      "\tspeed: 0.1555s/iter; left time: 979.2261s\n",
      "\titers: 300, epoch: 4 | loss: 0.1360458\n",
      "\tspeed: 0.1553s/iter; left time: 962.5157s\n",
      "\titers: 400, epoch: 4 | loss: 0.1250557\n",
      "\tspeed: 0.1553s/iter; left time: 946.8067s\n",
      "\titers: 500, epoch: 4 | loss: 0.1545034\n",
      "\tspeed: 0.1556s/iter; left time: 933.3029s\n",
      "\titers: 600, epoch: 4 | loss: 0.1279275\n",
      "\tspeed: 0.1552s/iter; left time: 915.2093s\n",
      "\titers: 700, epoch: 4 | loss: 0.1105532\n",
      "\tspeed: 0.1552s/iter; left time: 899.8735s\n",
      "\titers: 800, epoch: 4 | loss: 0.1114108\n",
      "\tspeed: 0.1554s/iter; left time: 885.0647s\n",
      "\titers: 900, epoch: 4 | loss: 0.1460306\n",
      "\tspeed: 0.1552s/iter; left time: 868.8036s\n",
      "Epoch: 4 running time: 2.4087215781211855 min.\n",
      "Epoch: 4, Steps: 928 | Train Loss: 0.1218533 Vali Loss: 0.1920059 Test Loss: 0.2680869\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1241807\n",
      "\tspeed: 0.4550s/iter; left time: 2488.3272s\n",
      "\titers: 200, epoch: 5 | loss: 0.1033353\n",
      "\tspeed: 0.1550s/iter; left time: 832.1983s\n",
      "\titers: 300, epoch: 5 | loss: 0.0990523\n",
      "\tspeed: 0.1550s/iter; left time: 816.6526s\n",
      "\titers: 400, epoch: 5 | loss: 0.1287144\n",
      "\tspeed: 0.1553s/iter; left time: 802.5726s\n",
      "\titers: 500, epoch: 5 | loss: 0.1377876\n",
      "\tspeed: 0.1549s/iter; left time: 784.9970s\n",
      "\titers: 600, epoch: 5 | loss: 0.1343285\n",
      "\tspeed: 0.1550s/iter; left time: 770.2784s\n",
      "\titers: 700, epoch: 5 | loss: 0.1338946\n",
      "\tspeed: 0.1554s/iter; left time: 756.7462s\n",
      "\titers: 800, epoch: 5 | loss: 0.1025835\n",
      "\tspeed: 0.1554s/iter; left time: 741.3395s\n",
      "\titers: 900, epoch: 5 | loss: 0.0836453\n",
      "\tspeed: 0.1555s/iter; left time: 726.0458s\n",
      "Epoch: 5 running time: 2.405818998813629 min.\n",
      "Epoch: 5, Steps: 928 | Train Loss: 0.1186329 Vali Loss: 0.1994166 Test Loss: 0.2607958\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1178720\n",
      "\tspeed: 0.4560s/iter; left time: 2070.6698s\n",
      "\titers: 200, epoch: 6 | loss: 0.1150615\n",
      "\tspeed: 0.1559s/iter; left time: 692.3918s\n",
      "\titers: 300, epoch: 6 | loss: 0.1171837\n",
      "\tspeed: 0.1559s/iter; left time: 676.9457s\n",
      "\titers: 400, epoch: 6 | loss: 0.1059201\n",
      "\tspeed: 0.1556s/iter; left time: 660.0676s\n",
      "\titers: 500, epoch: 6 | loss: 0.1523478\n",
      "\tspeed: 0.1554s/iter; left time: 643.5917s\n",
      "\titers: 600, epoch: 6 | loss: 0.0855035\n",
      "\tspeed: 0.1556s/iter; left time: 628.7566s\n",
      "\titers: 700, epoch: 6 | loss: 0.1236316\n",
      "\tspeed: 0.1557s/iter; left time: 613.5366s\n",
      "\titers: 800, epoch: 6 | loss: 0.1057357\n",
      "\tspeed: 0.1555s/iter; left time: 597.0844s\n",
      "\titers: 900, epoch: 6 | loss: 0.0966832\n",
      "\tspeed: 0.1555s/iter; left time: 581.7079s\n",
      "Epoch: 6 running time: 2.4150441845258075 min.\n",
      "Epoch: 6, Steps: 928 | Train Loss: 0.1189270 Vali Loss: 0.1910140 Test Loss: 0.2555986\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl512_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.25549784302711487, mae:0.31259214878082275\n",
      "\n",
      "Time intermediate for IT dataset: 49.72307836612065 min.\n",
      "Total time: 266.01663768291473 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"512\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq len 96, pred len 24, but 15/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5979588\n",
      "\tspeed: 0.1158s/iter; left time: 1078.4204s\n",
      "\titers: 200, epoch: 1 | loss: 0.3988746\n",
      "\tspeed: 0.0979s/iter; left time: 901.8832s\n",
      "\titers: 300, epoch: 1 | loss: 0.4951838\n",
      "\tspeed: 0.0982s/iter; left time: 894.5374s\n",
      "\titers: 400, epoch: 1 | loss: 0.3251971\n",
      "\tspeed: 0.0973s/iter; left time: 877.1021s\n",
      "\titers: 500, epoch: 1 | loss: 0.2972392\n",
      "\tspeed: 0.0973s/iter; left time: 866.8448s\n",
      "\titers: 600, epoch: 1 | loss: 0.3141234\n",
      "\tspeed: 0.0973s/iter; left time: 857.7271s\n",
      "\titers: 700, epoch: 1 | loss: 0.3582392\n",
      "\tspeed: 0.0970s/iter; left time: 845.2229s\n",
      "\titers: 800, epoch: 1 | loss: 0.3090230\n",
      "\tspeed: 0.0971s/iter; left time: 836.2026s\n",
      "\titers: 900, epoch: 1 | loss: 0.2600375\n",
      "\tspeed: 0.0974s/iter; left time: 829.1754s\n",
      "Epoch: 1 running time: 1.5416242758433023 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3885667 Vali Loss: 0.4530054 Test Loss: 0.5098668\n",
      "Validation loss decreased (inf --> 0.453005).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3331507\n",
      "\tspeed: 0.2866s/iter; left time: 2398.7635s\n",
      "\titers: 200, epoch: 2 | loss: 0.2316457\n",
      "\tspeed: 0.0972s/iter; left time: 804.1470s\n",
      "\titers: 300, epoch: 2 | loss: 0.2807677\n",
      "\tspeed: 0.0971s/iter; left time: 793.6601s\n",
      "\titers: 400, epoch: 2 | loss: 0.2144712\n",
      "\tspeed: 0.0971s/iter; left time: 783.2218s\n",
      "\titers: 500, epoch: 2 | loss: 0.3082660\n",
      "\tspeed: 0.0970s/iter; left time: 773.4547s\n",
      "\titers: 600, epoch: 2 | loss: 0.2161382\n",
      "\tspeed: 0.0970s/iter; left time: 763.7828s\n",
      "\titers: 700, epoch: 2 | loss: 0.2845892\n",
      "\tspeed: 0.0974s/iter; left time: 756.4156s\n",
      "\titers: 800, epoch: 2 | loss: 0.3125113\n",
      "\tspeed: 0.0971s/iter; left time: 744.7997s\n",
      "\titers: 900, epoch: 2 | loss: 0.2776034\n",
      "\tspeed: 0.0970s/iter; left time: 734.6490s\n",
      "Epoch: 2 running time: 1.528982925415039 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2735988 Vali Loss: 0.4486056 Test Loss: 0.5243754\n",
      "Validation loss decreased (0.453005 --> 0.448606).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2147057\n",
      "\tspeed: 0.2887s/iter; left time: 2144.4999s\n",
      "\titers: 200, epoch: 3 | loss: 0.2043640\n",
      "\tspeed: 0.0981s/iter; left time: 719.0805s\n",
      "\titers: 300, epoch: 3 | loss: 0.2868434\n",
      "\tspeed: 0.0976s/iter; left time: 705.7299s\n",
      "\titers: 400, epoch: 3 | loss: 0.2317903\n",
      "\tspeed: 0.0975s/iter; left time: 694.9661s\n",
      "\titers: 500, epoch: 3 | loss: 0.2354711\n",
      "\tspeed: 0.0980s/iter; left time: 688.6581s\n",
      "\titers: 600, epoch: 3 | loss: 0.2605993\n",
      "\tspeed: 0.0976s/iter; left time: 676.1623s\n",
      "\titers: 700, epoch: 3 | loss: 0.2369445\n",
      "\tspeed: 0.0977s/iter; left time: 667.2937s\n",
      "\titers: 800, epoch: 3 | loss: 0.1739601\n",
      "\tspeed: 0.0973s/iter; left time: 654.9646s\n",
      "\titers: 900, epoch: 3 | loss: 0.1518188\n",
      "\tspeed: 0.0973s/iter; left time: 644.9557s\n",
      "Epoch: 3 running time: 1.537677800655365 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2293410 Vali Loss: 0.4364339 Test Loss: 0.5246060\n",
      "Validation loss decreased (0.448606 --> 0.436434).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2126841\n",
      "\tspeed: 0.2866s/iter; left time: 1859.2794s\n",
      "\titers: 200, epoch: 4 | loss: 0.2242936\n",
      "\tspeed: 0.0976s/iter; left time: 623.5230s\n",
      "\titers: 300, epoch: 4 | loss: 0.1782434\n",
      "\tspeed: 0.0979s/iter; left time: 615.7044s\n",
      "\titers: 400, epoch: 4 | loss: 0.1621198\n",
      "\tspeed: 0.0978s/iter; left time: 605.3837s\n",
      "\titers: 500, epoch: 4 | loss: 0.2091303\n",
      "\tspeed: 0.0977s/iter; left time: 594.5612s\n",
      "\titers: 600, epoch: 4 | loss: 0.1637490\n",
      "\tspeed: 0.0979s/iter; left time: 586.2668s\n",
      "\titers: 700, epoch: 4 | loss: 0.1828185\n",
      "\tspeed: 0.0974s/iter; left time: 573.7421s\n",
      "\titers: 800, epoch: 4 | loss: 0.1856257\n",
      "\tspeed: 0.0976s/iter; left time: 565.1349s\n",
      "\titers: 900, epoch: 4 | loss: 0.2655327\n",
      "\tspeed: 0.0983s/iter; left time: 559.0763s\n",
      "Epoch: 4 running time: 1.5392526427904765 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1967255 Vali Loss: 0.4372559 Test Loss: 0.5368163\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1526068\n",
      "\tspeed: 0.2774s/iter; left time: 1538.5954s\n",
      "\titers: 200, epoch: 5 | loss: 0.1602567\n",
      "\tspeed: 0.0973s/iter; left time: 529.9277s\n",
      "\titers: 300, epoch: 5 | loss: 0.1484557\n",
      "\tspeed: 0.0973s/iter; left time: 520.0539s\n",
      "\titers: 400, epoch: 5 | loss: 0.2294284\n",
      "\tspeed: 0.0973s/iter; left time: 510.3750s\n",
      "\titers: 500, epoch: 5 | loss: 0.1819390\n",
      "\tspeed: 0.0972s/iter; left time: 500.4320s\n",
      "\titers: 600, epoch: 5 | loss: 0.1645809\n",
      "\tspeed: 0.0971s/iter; left time: 490.1103s\n",
      "\titers: 700, epoch: 5 | loss: 0.1930625\n",
      "\tspeed: 0.0972s/iter; left time: 480.8000s\n",
      "\titers: 800, epoch: 5 | loss: 0.1259595\n",
      "\tspeed: 0.0975s/iter; left time: 472.6213s\n",
      "\titers: 900, epoch: 5 | loss: 0.2315474\n",
      "\tspeed: 0.0975s/iter; left time: 462.7698s\n",
      "Epoch: 5 running time: 1.528203014532725 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1935644 Vali Loss: 0.4452737 Test Loss: 0.5369565\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1955694\n",
      "\tspeed: 0.2752s/iter; left time: 1267.6496s\n",
      "\titers: 200, epoch: 6 | loss: 0.1591563\n",
      "\tspeed: 0.0974s/iter; left time: 438.7296s\n",
      "\titers: 300, epoch: 6 | loss: 0.2532864\n",
      "\tspeed: 0.0971s/iter; left time: 427.9357s\n",
      "\titers: 400, epoch: 6 | loss: 0.2315865\n",
      "\tspeed: 0.0975s/iter; left time: 419.8145s\n",
      "\titers: 500, epoch: 6 | loss: 0.1531271\n",
      "\tspeed: 0.0977s/iter; left time: 411.0539s\n",
      "\titers: 600, epoch: 6 | loss: 0.1807413\n",
      "\tspeed: 0.0892s/iter; left time: 366.1870s\n",
      "\titers: 700, epoch: 6 | loss: 0.1352026\n",
      "\tspeed: 0.0804s/iter; left time: 322.0285s\n",
      "\titers: 800, epoch: 6 | loss: 0.1791342\n",
      "\tspeed: 0.0804s/iter; left time: 313.9638s\n",
      "\titers: 900, epoch: 6 | loss: 0.2129019\n",
      "\tspeed: 0.0805s/iter; left time: 306.5291s\n",
      "Epoch: 6 running time: 1.42306702931722 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1944405 Vali Loss: 0.4411336 Test Loss: 0.5357661\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5371220707893372, mae:0.4859105050563812\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.5073603\n",
      "\tspeed: 0.1007s/iter; left time: 938.0378s\n",
      "\titers: 200, epoch: 1 | loss: 0.3586431\n",
      "\tspeed: 0.0976s/iter; left time: 899.1510s\n",
      "\titers: 300, epoch: 1 | loss: 0.3429819\n",
      "\tspeed: 0.0979s/iter; left time: 891.7569s\n",
      "\titers: 400, epoch: 1 | loss: 0.3443952\n",
      "\tspeed: 0.0977s/iter; left time: 880.6653s\n",
      "\titers: 500, epoch: 1 | loss: 0.2737124\n",
      "\tspeed: 0.0981s/iter; left time: 873.9722s\n",
      "\titers: 600, epoch: 1 | loss: 0.3054211\n",
      "\tspeed: 0.0982s/iter; left time: 865.5937s\n",
      "\titers: 700, epoch: 1 | loss: 0.3219874\n",
      "\tspeed: 0.0979s/iter; left time: 852.6847s\n",
      "\titers: 800, epoch: 1 | loss: 0.2815325\n",
      "\tspeed: 0.0977s/iter; left time: 840.9757s\n",
      "\titers: 900, epoch: 1 | loss: 0.3119482\n",
      "\tspeed: 0.0976s/iter; left time: 830.5884s\n",
      "Epoch: 1 running time: 1.5409079670906067 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3923510 Vali Loss: 0.4391314 Test Loss: 0.5391274\n",
      "Validation loss decreased (inf --> 0.439131).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2579631\n",
      "\tspeed: 0.2990s/iter; left time: 2502.3614s\n",
      "\titers: 200, epoch: 2 | loss: 0.2406520\n",
      "\tspeed: 0.0979s/iter; left time: 809.9776s\n",
      "\titers: 300, epoch: 2 | loss: 0.2644797\n",
      "\tspeed: 0.0978s/iter; left time: 799.3551s\n",
      "\titers: 400, epoch: 2 | loss: 0.3358616\n",
      "\tspeed: 0.0978s/iter; left time: 789.0743s\n",
      "\titers: 500, epoch: 2 | loss: 0.2559831\n",
      "\tspeed: 0.0983s/iter; left time: 783.3774s\n",
      "\titers: 600, epoch: 2 | loss: 0.2725189\n",
      "\tspeed: 0.0980s/iter; left time: 771.1548s\n",
      "\titers: 700, epoch: 2 | loss: 0.2762024\n",
      "\tspeed: 0.0978s/iter; left time: 760.2521s\n",
      "\titers: 800, epoch: 2 | loss: 0.3274258\n",
      "\tspeed: 0.0981s/iter; left time: 752.4282s\n",
      "\titers: 900, epoch: 2 | loss: 0.2026954\n",
      "\tspeed: 0.0979s/iter; left time: 740.9632s\n",
      "Epoch: 2 running time: 1.5421135624249775 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.2742993 Vali Loss: 0.4626770 Test Loss: 0.5380990\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3293621\n",
      "\tspeed: 0.2790s/iter; left time: 2073.0374s\n",
      "\titers: 200, epoch: 3 | loss: 0.3228712\n",
      "\tspeed: 0.0977s/iter; left time: 716.1715s\n",
      "\titers: 300, epoch: 3 | loss: 0.3338396\n",
      "\tspeed: 0.0977s/iter; left time: 706.1690s\n",
      "\titers: 400, epoch: 3 | loss: 0.2747551\n",
      "\tspeed: 0.0976s/iter; left time: 695.8993s\n",
      "\titers: 500, epoch: 3 | loss: 0.2328539\n",
      "\tspeed: 0.0978s/iter; left time: 687.4855s\n",
      "\titers: 600, epoch: 3 | loss: 0.2259090\n",
      "\tspeed: 0.0978s/iter; left time: 677.7858s\n",
      "\titers: 700, epoch: 3 | loss: 0.3119092\n",
      "\tspeed: 0.0975s/iter; left time: 665.6786s\n",
      "\titers: 800, epoch: 3 | loss: 0.2195392\n",
      "\tspeed: 0.0977s/iter; left time: 657.5710s\n",
      "\titers: 900, epoch: 3 | loss: 0.2340386\n",
      "\tspeed: 0.0977s/iter; left time: 647.6402s\n",
      "Epoch: 3 running time: 1.5375199913978577 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2637777 Vali Loss: 0.4304126 Test Loss: 0.5036249\n",
      "Validation loss decreased (0.439131 --> 0.430413).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2275028\n",
      "\tspeed: 0.2873s/iter; left time: 1864.1290s\n",
      "\titers: 200, epoch: 4 | loss: 0.2615315\n",
      "\tspeed: 0.0980s/iter; left time: 625.7576s\n",
      "\titers: 300, epoch: 4 | loss: 0.2585377\n",
      "\tspeed: 0.0978s/iter; left time: 614.8628s\n",
      "\titers: 400, epoch: 4 | loss: 0.2519680\n",
      "\tspeed: 0.0977s/iter; left time: 604.3260s\n",
      "\titers: 500, epoch: 4 | loss: 0.2040586\n",
      "\tspeed: 0.0981s/iter; left time: 596.9708s\n",
      "\titers: 600, epoch: 4 | loss: 0.2315200\n",
      "\tspeed: 0.0980s/iter; left time: 586.5436s\n",
      "\titers: 700, epoch: 4 | loss: 0.1896393\n",
      "\tspeed: 0.0980s/iter; left time: 577.2629s\n",
      "\titers: 800, epoch: 4 | loss: 0.1836942\n",
      "\tspeed: 0.0979s/iter; left time: 566.4811s\n",
      "\titers: 900, epoch: 4 | loss: 0.1867187\n",
      "\tspeed: 0.0978s/iter; left time: 556.1676s\n",
      "Epoch: 4 running time: 1.540101130803426 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2252880 Vali Loss: 0.4670930 Test Loss: 0.5725424\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2393911\n",
      "\tspeed: 0.2631s/iter; left time: 1459.2680s\n",
      "\titers: 200, epoch: 5 | loss: 0.3050546\n",
      "\tspeed: 0.0806s/iter; left time: 438.9588s\n",
      "\titers: 300, epoch: 5 | loss: 0.1836839\n",
      "\tspeed: 0.0805s/iter; left time: 430.3165s\n",
      "\titers: 400, epoch: 5 | loss: 0.1934770\n",
      "\tspeed: 0.0805s/iter; left time: 422.3918s\n",
      "\titers: 500, epoch: 5 | loss: 0.2627646\n",
      "\tspeed: 0.0806s/iter; left time: 414.9089s\n",
      "\titers: 600, epoch: 5 | loss: 0.1645602\n",
      "\tspeed: 0.0806s/iter; left time: 406.8596s\n",
      "\titers: 700, epoch: 5 | loss: 0.2533809\n",
      "\tspeed: 0.0844s/iter; left time: 417.6078s\n",
      "\titers: 800, epoch: 5 | loss: 0.2177523\n",
      "\tspeed: 0.0949s/iter; left time: 460.0042s\n",
      "\titers: 900, epoch: 5 | loss: 0.1967264\n",
      "\tspeed: 0.0804s/iter; left time: 381.8841s\n",
      "Epoch: 5 running time: 1.2995675841967265 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2229527 Vali Loss: 0.4358316 Test Loss: 0.5285925\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2324658\n",
      "\tspeed: 0.2721s/iter; left time: 1253.1136s\n",
      "\titers: 200, epoch: 6 | loss: 0.1990679\n",
      "\tspeed: 0.0976s/iter; left time: 439.9005s\n",
      "\titers: 300, epoch: 6 | loss: 0.1909583\n",
      "\tspeed: 0.0977s/iter; left time: 430.6843s\n",
      "\titers: 400, epoch: 6 | loss: 0.2697339\n",
      "\tspeed: 0.0978s/iter; left time: 420.9609s\n",
      "\titers: 500, epoch: 6 | loss: 0.2036924\n",
      "\tspeed: 0.0976s/iter; left time: 410.5886s\n",
      "\titers: 600, epoch: 6 | loss: 0.1933667\n",
      "\tspeed: 0.0976s/iter; left time: 400.9276s\n",
      "\titers: 700, epoch: 6 | loss: 0.1714088\n",
      "\tspeed: 0.0976s/iter; left time: 390.9650s\n",
      "\titers: 800, epoch: 6 | loss: 0.2410200\n",
      "\tspeed: 0.0974s/iter; left time: 380.2546s\n",
      "\titers: 900, epoch: 6 | loss: 0.2566103\n",
      "\tspeed: 0.0974s/iter; left time: 370.8558s\n",
      "Epoch: 6 running time: 1.5370829780896504 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.2223095 Vali Loss: 0.4308074 Test Loss: 0.5192907\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_DE_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.5217770934104919, mae:0.48574259877204895\n",
      "\n",
      "Time intermediate for DE dataset: 21.413901205857595 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4583763\n",
      "\tspeed: 0.1153s/iter; left time: 1073.4076s\n",
      "\titers: 200, epoch: 1 | loss: 0.3878897\n",
      "\tspeed: 0.0976s/iter; left time: 899.0506s\n",
      "\titers: 300, epoch: 1 | loss: 0.3311236\n",
      "\tspeed: 0.0973s/iter; left time: 886.6361s\n",
      "\titers: 400, epoch: 1 | loss: 0.3112401\n",
      "\tspeed: 0.0975s/iter; left time: 878.2287s\n",
      "\titers: 500, epoch: 1 | loss: 0.2713794\n",
      "\tspeed: 0.0972s/iter; left time: 866.3334s\n",
      "\titers: 600, epoch: 1 | loss: 0.3417489\n",
      "\tspeed: 0.0973s/iter; left time: 857.0520s\n",
      "\titers: 700, epoch: 1 | loss: 0.4003856\n",
      "\tspeed: 0.0968s/iter; left time: 843.5195s\n",
      "\titers: 800, epoch: 1 | loss: 0.2808123\n",
      "\tspeed: 0.0966s/iter; left time: 831.5436s\n",
      "\titers: 900, epoch: 1 | loss: 0.2576330\n",
      "\tspeed: 0.0963s/iter; left time: 819.4720s\n",
      "Epoch: 1 running time: 1.5358296394348145 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4064409 Vali Loss: 0.4893223 Test Loss: 0.8045193\n",
      "Validation loss decreased (inf --> 0.489322).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2919916\n",
      "\tspeed: 0.2896s/iter; left time: 2423.9875s\n",
      "\titers: 200, epoch: 2 | loss: 0.3346875\n",
      "\tspeed: 0.0973s/iter; left time: 804.4452s\n",
      "\titers: 300, epoch: 2 | loss: 0.3632451\n",
      "\tspeed: 0.0967s/iter; left time: 789.7959s\n",
      "\titers: 400, epoch: 2 | loss: 0.3304225\n",
      "\tspeed: 0.0972s/iter; left time: 784.7106s\n",
      "\titers: 500, epoch: 2 | loss: 0.2533335\n",
      "\tspeed: 0.0968s/iter; left time: 771.5427s\n",
      "\titers: 600, epoch: 2 | loss: 0.2403303\n",
      "\tspeed: 0.0971s/iter; left time: 764.3816s\n",
      "\titers: 700, epoch: 2 | loss: 0.2708574\n",
      "\tspeed: 0.0967s/iter; left time: 751.4633s\n",
      "\titers: 800, epoch: 2 | loss: 0.2765436\n",
      "\tspeed: 0.0972s/iter; left time: 745.6382s\n",
      "\titers: 900, epoch: 2 | loss: 0.2758223\n",
      "\tspeed: 0.0973s/iter; left time: 736.2649s\n",
      "Epoch: 2 running time: 1.5277405619621276 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3068440 Vali Loss: 0.4491566 Test Loss: 0.6592773\n",
      "Validation loss decreased (0.489322 --> 0.449157).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2303527\n",
      "\tspeed: 0.2873s/iter; left time: 2134.3776s\n",
      "\titers: 200, epoch: 3 | loss: 0.2558223\n",
      "\tspeed: 0.0974s/iter; left time: 713.6154s\n",
      "\titers: 300, epoch: 3 | loss: 0.2449559\n",
      "\tspeed: 0.0974s/iter; left time: 704.0844s\n",
      "\titers: 400, epoch: 3 | loss: 0.2271148\n",
      "\tspeed: 0.0969s/iter; left time: 691.0086s\n",
      "\titers: 500, epoch: 3 | loss: 0.2730961\n",
      "\tspeed: 0.0969s/iter; left time: 680.8591s\n",
      "\titers: 600, epoch: 3 | loss: 0.2544571\n",
      "\tspeed: 0.0971s/iter; left time: 673.1014s\n",
      "\titers: 700, epoch: 3 | loss: 0.3101209\n",
      "\tspeed: 0.0972s/iter; left time: 663.6518s\n",
      "\titers: 800, epoch: 3 | loss: 0.2352293\n",
      "\tspeed: 0.0970s/iter; left time: 652.7136s\n",
      "\titers: 900, epoch: 3 | loss: 0.2528077\n",
      "\tspeed: 0.0971s/iter; left time: 643.9492s\n",
      "Epoch: 3 running time: 1.5291958053906758 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2642540 Vali Loss: 0.4695423 Test Loss: 0.7489555\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2098035\n",
      "\tspeed: 0.2774s/iter; left time: 1799.8113s\n",
      "\titers: 200, epoch: 4 | loss: 0.2045452\n",
      "\tspeed: 0.0970s/iter; left time: 619.4811s\n",
      "\titers: 300, epoch: 4 | loss: 0.2922118\n",
      "\tspeed: 0.0960s/iter; left time: 603.5027s\n",
      "\titers: 400, epoch: 4 | loss: 0.2247067\n",
      "\tspeed: 0.0935s/iter; left time: 578.6152s\n",
      "\titers: 500, epoch: 4 | loss: 0.1968964\n",
      "\tspeed: 0.0958s/iter; left time: 583.0985s\n",
      "\titers: 600, epoch: 4 | loss: 0.2191588\n",
      "\tspeed: 0.0967s/iter; left time: 579.0689s\n",
      "\titers: 700, epoch: 4 | loss: 0.2817483\n",
      "\tspeed: 0.0967s/iter; left time: 569.4650s\n",
      "\titers: 800, epoch: 4 | loss: 0.2403954\n",
      "\tspeed: 0.0968s/iter; left time: 560.3039s\n",
      "\titers: 900, epoch: 4 | loss: 0.4045563\n",
      "\tspeed: 0.0969s/iter; left time: 551.1872s\n",
      "Epoch: 4 running time: 1.5158695816993712 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2588831 Vali Loss: 0.4756983 Test Loss: 0.7903039\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2433960\n",
      "\tspeed: 0.2774s/iter; left time: 1538.8468s\n",
      "\titers: 200, epoch: 5 | loss: 0.2428084\n",
      "\tspeed: 0.0970s/iter; left time: 528.4953s\n",
      "\titers: 300, epoch: 5 | loss: 0.2466180\n",
      "\tspeed: 0.0968s/iter; left time: 517.3354s\n",
      "\titers: 400, epoch: 5 | loss: 0.2195630\n",
      "\tspeed: 0.0970s/iter; left time: 508.7635s\n",
      "\titers: 500, epoch: 5 | loss: 0.2149318\n",
      "\tspeed: 0.0975s/iter; left time: 501.8738s\n",
      "\titers: 600, epoch: 5 | loss: 0.2161719\n",
      "\tspeed: 0.0973s/iter; left time: 490.9104s\n",
      "\titers: 700, epoch: 5 | loss: 0.2809151\n",
      "\tspeed: 0.0974s/iter; left time: 481.9422s\n",
      "\titers: 800, epoch: 5 | loss: 0.1828536\n",
      "\tspeed: 0.0971s/iter; left time: 470.7358s\n",
      "\titers: 900, epoch: 5 | loss: 0.2432564\n",
      "\tspeed: 0.0971s/iter; left time: 461.1666s\n",
      "Epoch: 5 running time: 1.528537138303121 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2585287 Vali Loss: 0.4641178 Test Loss: 0.7524729\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.7526057958602905, mae:0.6016281843185425\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.6014726\n",
      "\tspeed: 0.1011s/iter; left time: 940.8772s\n",
      "\titers: 200, epoch: 1 | loss: 0.3770287\n",
      "\tspeed: 0.0973s/iter; left time: 896.1426s\n",
      "\titers: 300, epoch: 1 | loss: 0.2769428\n",
      "\tspeed: 0.0971s/iter; left time: 884.5791s\n",
      "\titers: 400, epoch: 1 | loss: 0.4088581\n",
      "\tspeed: 0.0969s/iter; left time: 873.2955s\n",
      "\titers: 500, epoch: 1 | loss: 0.3499027\n",
      "\tspeed: 0.0970s/iter; left time: 864.5678s\n",
      "\titers: 600, epoch: 1 | loss: 0.3194351\n",
      "\tspeed: 0.0970s/iter; left time: 855.0977s\n",
      "\titers: 700, epoch: 1 | loss: 0.3058459\n",
      "\tspeed: 0.0972s/iter; left time: 846.6560s\n",
      "\titers: 800, epoch: 1 | loss: 0.3182074\n",
      "\tspeed: 0.0971s/iter; left time: 836.3387s\n",
      "\titers: 900, epoch: 1 | loss: 0.3454969\n",
      "\tspeed: 0.0978s/iter; left time: 832.6520s\n",
      "Epoch: 1 running time: 1.5332942048708598 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.4013765 Vali Loss: 0.4784800 Test Loss: 0.7416211\n",
      "Validation loss decreased (inf --> 0.478480).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2842442\n",
      "\tspeed: 0.2868s/iter; left time: 2400.6099s\n",
      "\titers: 200, epoch: 2 | loss: 0.3157755\n",
      "\tspeed: 0.0978s/iter; left time: 808.4071s\n",
      "\titers: 300, epoch: 2 | loss: 0.3066674\n",
      "\tspeed: 0.0976s/iter; left time: 797.4706s\n",
      "\titers: 400, epoch: 2 | loss: 0.2174160\n",
      "\tspeed: 0.0976s/iter; left time: 787.3130s\n",
      "\titers: 500, epoch: 2 | loss: 0.2734478\n",
      "\tspeed: 0.0975s/iter; left time: 777.1696s\n",
      "\titers: 600, epoch: 2 | loss: 0.2302928\n",
      "\tspeed: 0.0974s/iter; left time: 766.7028s\n",
      "\titers: 700, epoch: 2 | loss: 0.2463083\n",
      "\tspeed: 0.0976s/iter; left time: 758.1018s\n",
      "\titers: 800, epoch: 2 | loss: 0.2253865\n",
      "\tspeed: 0.0975s/iter; left time: 747.5563s\n",
      "\titers: 900, epoch: 2 | loss: 0.2695571\n",
      "\tspeed: 0.0974s/iter; left time: 737.2177s\n",
      "Epoch: 2 running time: 1.5345231413841247 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.3046960 Vali Loss: 0.4594091 Test Loss: 0.6731076\n",
      "Validation loss decreased (0.478480 --> 0.459409).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3146747\n",
      "\tspeed: 0.2949s/iter; left time: 2190.4721s\n",
      "\titers: 200, epoch: 3 | loss: 0.2338927\n",
      "\tspeed: 0.0973s/iter; left time: 713.2922s\n",
      "\titers: 300, epoch: 3 | loss: 0.3158037\n",
      "\tspeed: 0.0974s/iter; left time: 704.0324s\n",
      "\titers: 400, epoch: 3 | loss: 0.3068777\n",
      "\tspeed: 0.0976s/iter; left time: 696.1266s\n",
      "\titers: 500, epoch: 3 | loss: 0.3275349\n",
      "\tspeed: 0.0975s/iter; left time: 685.2863s\n",
      "\titers: 600, epoch: 3 | loss: 0.2629035\n",
      "\tspeed: 0.0969s/iter; left time: 671.3850s\n",
      "\titers: 700, epoch: 3 | loss: 0.2878890\n",
      "\tspeed: 0.0970s/iter; left time: 662.2302s\n",
      "\titers: 800, epoch: 3 | loss: 0.3027056\n",
      "\tspeed: 0.0969s/iter; left time: 652.2306s\n",
      "\titers: 900, epoch: 3 | loss: 0.3356416\n",
      "\tspeed: 0.0970s/iter; left time: 642.6886s\n",
      "Epoch: 3 running time: 1.530323874950409 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.2658854 Vali Loss: 0.4861600 Test Loss: 0.7527768\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2781807\n",
      "\tspeed: 0.2778s/iter; left time: 1802.5274s\n",
      "\titers: 200, epoch: 4 | loss: 0.3753445\n",
      "\tspeed: 0.0969s/iter; left time: 619.1721s\n",
      "\titers: 300, epoch: 4 | loss: 0.3602874\n",
      "\tspeed: 0.0968s/iter; left time: 608.9753s\n",
      "\titers: 400, epoch: 4 | loss: 0.2305577\n",
      "\tspeed: 0.0968s/iter; left time: 599.0189s\n",
      "\titers: 500, epoch: 4 | loss: 0.2350097\n",
      "\tspeed: 0.0845s/iter; left time: 514.6801s\n",
      "\titers: 600, epoch: 4 | loss: 0.2469003\n",
      "\tspeed: 0.0835s/iter; left time: 499.9751s\n",
      "\titers: 700, epoch: 4 | loss: 0.2643796\n",
      "\tspeed: 0.0972s/iter; left time: 572.4663s\n",
      "\titers: 800, epoch: 4 | loss: 0.2468953\n",
      "\tspeed: 0.0974s/iter; left time: 563.6940s\n",
      "\titers: 900, epoch: 4 | loss: 0.2336895\n",
      "\tspeed: 0.0974s/iter; left time: 553.8941s\n",
      "Epoch: 4 running time: 1.4848694761594137 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.2597380 Vali Loss: 0.4754927 Test Loss: 0.7511238\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2265940\n",
      "\tspeed: 0.2783s/iter; left time: 1543.6741s\n",
      "\titers: 200, epoch: 5 | loss: 0.4220915\n",
      "\tspeed: 0.0971s/iter; left time: 529.0198s\n",
      "\titers: 300, epoch: 5 | loss: 0.2612588\n",
      "\tspeed: 0.0971s/iter; left time: 519.0021s\n",
      "\titers: 400, epoch: 5 | loss: 0.2463430\n",
      "\tspeed: 0.0969s/iter; left time: 508.6252s\n",
      "\titers: 500, epoch: 5 | loss: 0.3009900\n",
      "\tspeed: 0.0971s/iter; left time: 499.9144s\n",
      "\titers: 600, epoch: 5 | loss: 0.2159840\n",
      "\tspeed: 0.0971s/iter; left time: 489.8522s\n",
      "\titers: 700, epoch: 5 | loss: 0.2511646\n",
      "\tspeed: 0.0970s/iter; left time: 480.0852s\n",
      "\titers: 800, epoch: 5 | loss: 0.2161874\n",
      "\tspeed: 0.0971s/iter; left time: 470.8730s\n",
      "\titers: 900, epoch: 5 | loss: 0.2488052\n",
      "\tspeed: 0.0970s/iter; left time: 460.4050s\n",
      "Epoch: 5 running time: 1.528868595759074 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.2601587 Vali Loss: 0.4624516 Test Loss: 0.6776870\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.6759302020072937, mae:0.5716946125030518\n",
      "\n",
      "Time intermediate for GB dataset: 18.104497134685516 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4183913\n",
      "\tspeed: 0.1158s/iter; left time: 1077.8975s\n",
      "\titers: 200, epoch: 1 | loss: 0.3176747\n",
      "\tspeed: 0.0974s/iter; left time: 896.6988s\n",
      "\titers: 300, epoch: 1 | loss: 0.2389105\n",
      "\tspeed: 0.0970s/iter; left time: 883.3525s\n",
      "\titers: 400, epoch: 1 | loss: 0.2031928\n",
      "\tspeed: 0.0971s/iter; left time: 875.3118s\n",
      "\titers: 500, epoch: 1 | loss: 0.3285361\n",
      "\tspeed: 0.0972s/iter; left time: 865.9697s\n",
      "\titers: 600, epoch: 1 | loss: 0.2553721\n",
      "\tspeed: 0.0972s/iter; left time: 856.1438s\n",
      "\titers: 700, epoch: 1 | loss: 0.1943906\n",
      "\tspeed: 0.0970s/iter; left time: 845.3274s\n",
      "\titers: 800, epoch: 1 | loss: 0.1912926\n",
      "\tspeed: 0.0974s/iter; left time: 838.5727s\n",
      "\titers: 900, epoch: 1 | loss: 0.2323324\n",
      "\tspeed: 0.0973s/iter; left time: 828.4329s\n",
      "Epoch: 1 running time: 1.5380592226982117 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.3024487 Vali Loss: 0.2142127 Test Loss: 0.4085582\n",
      "Validation loss decreased (inf --> 0.214213).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1885877\n",
      "\tspeed: 0.2872s/iter; left time: 2403.6359s\n",
      "\titers: 200, epoch: 2 | loss: 0.2571721\n",
      "\tspeed: 0.0974s/iter; left time: 805.2311s\n",
      "\titers: 300, epoch: 2 | loss: 0.2305319\n",
      "\tspeed: 0.0970s/iter; left time: 792.8477s\n",
      "\titers: 400, epoch: 2 | loss: 0.1730416\n",
      "\tspeed: 0.0969s/iter; left time: 781.9758s\n",
      "\titers: 500, epoch: 2 | loss: 0.1834777\n",
      "\tspeed: 0.0970s/iter; left time: 772.8616s\n",
      "\titers: 600, epoch: 2 | loss: 0.2235559\n",
      "\tspeed: 0.0968s/iter; left time: 761.4389s\n",
      "\titers: 700, epoch: 2 | loss: 0.1894432\n",
      "\tspeed: 0.0970s/iter; left time: 753.9698s\n",
      "\titers: 800, epoch: 2 | loss: 0.2054119\n",
      "\tspeed: 0.0969s/iter; left time: 743.0356s\n",
      "\titers: 900, epoch: 2 | loss: 0.1851327\n",
      "\tspeed: 0.0969s/iter; left time: 733.6979s\n",
      "Epoch: 2 running time: 1.5273565689722697 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1871432 Vali Loss: 0.2124030 Test Loss: 0.4091493\n",
      "Validation loss decreased (0.214213 --> 0.212403).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1869826\n",
      "\tspeed: 0.2866s/iter; left time: 2128.8844s\n",
      "\titers: 200, epoch: 3 | loss: 0.1273448\n",
      "\tspeed: 0.0972s/iter; left time: 712.0354s\n",
      "\titers: 300, epoch: 3 | loss: 0.1840195\n",
      "\tspeed: 0.0973s/iter; left time: 703.4396s\n",
      "\titers: 400, epoch: 3 | loss: 0.1545658\n",
      "\tspeed: 0.0974s/iter; left time: 694.4069s\n",
      "\titers: 500, epoch: 3 | loss: 0.1362446\n",
      "\tspeed: 0.0974s/iter; left time: 684.8974s\n",
      "\titers: 600, epoch: 3 | loss: 0.1392688\n",
      "\tspeed: 0.0971s/iter; left time: 672.6977s\n",
      "\titers: 700, epoch: 3 | loss: 0.1216528\n",
      "\tspeed: 0.0970s/iter; left time: 662.5240s\n",
      "\titers: 800, epoch: 3 | loss: 0.1669460\n",
      "\tspeed: 0.0970s/iter; left time: 652.6511s\n",
      "\titers: 900, epoch: 3 | loss: 0.1219197\n",
      "\tspeed: 0.0971s/iter; left time: 643.6203s\n",
      "Epoch: 3 running time: 1.5294891595840454 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1588624 Vali Loss: 0.1953210 Test Loss: 0.4135751\n",
      "Validation loss decreased (0.212403 --> 0.195321).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1350125\n",
      "\tspeed: 0.2865s/iter; left time: 1858.6510s\n",
      "\titers: 200, epoch: 4 | loss: 0.1716562\n",
      "\tspeed: 0.0971s/iter; left time: 620.3514s\n",
      "\titers: 300, epoch: 4 | loss: 0.1169807\n",
      "\tspeed: 0.0969s/iter; left time: 609.4267s\n",
      "\titers: 400, epoch: 4 | loss: 0.1615296\n",
      "\tspeed: 0.0969s/iter; left time: 599.3261s\n",
      "\titers: 500, epoch: 4 | loss: 0.1219966\n",
      "\tspeed: 0.0970s/iter; left time: 590.3008s\n",
      "\titers: 600, epoch: 4 | loss: 0.1076881\n",
      "\tspeed: 0.0968s/iter; left time: 579.6821s\n",
      "\titers: 700, epoch: 4 | loss: 0.1156947\n",
      "\tspeed: 0.0967s/iter; left time: 569.6305s\n",
      "\titers: 800, epoch: 4 | loss: 0.1478185\n",
      "\tspeed: 0.0969s/iter; left time: 561.0384s\n",
      "\titers: 900, epoch: 4 | loss: 0.1011140\n",
      "\tspeed: 0.0967s/iter; left time: 550.2020s\n",
      "Epoch: 4 running time: 1.5251440723737082 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1432803 Vali Loss: 0.1973028 Test Loss: 0.4132233\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1584335\n",
      "\tspeed: 0.2772s/iter; left time: 1537.6530s\n",
      "\titers: 200, epoch: 5 | loss: 0.1448087\n",
      "\tspeed: 0.0974s/iter; left time: 530.5363s\n",
      "\titers: 300, epoch: 5 | loss: 0.0993634\n",
      "\tspeed: 0.0970s/iter; left time: 518.5948s\n",
      "\titers: 400, epoch: 5 | loss: 0.1636676\n",
      "\tspeed: 0.0973s/iter; left time: 510.6281s\n",
      "\titers: 500, epoch: 5 | loss: 0.1651998\n",
      "\tspeed: 0.0970s/iter; left time: 499.2504s\n",
      "\titers: 600, epoch: 5 | loss: 0.1470332\n",
      "\tspeed: 0.0974s/iter; left time: 491.6153s\n",
      "\titers: 700, epoch: 5 | loss: 0.1403046\n",
      "\tspeed: 0.0972s/iter; left time: 480.6698s\n",
      "\titers: 800, epoch: 5 | loss: 0.1216519\n",
      "\tspeed: 0.0973s/iter; left time: 471.4950s\n",
      "\titers: 900, epoch: 5 | loss: 0.1480536\n",
      "\tspeed: 0.0971s/iter; left time: 460.9754s\n",
      "Epoch: 5 running time: 1.5300809462865195 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1410971 Vali Loss: 0.1985141 Test Loss: 0.4454126\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1084764\n",
      "\tspeed: 0.2769s/iter; left time: 1275.5060s\n",
      "\titers: 200, epoch: 6 | loss: 0.1506579\n",
      "\tspeed: 0.0968s/iter; left time: 436.2712s\n",
      "\titers: 300, epoch: 6 | loss: 0.1849733\n",
      "\tspeed: 0.0968s/iter; left time: 426.7089s\n",
      "\titers: 400, epoch: 6 | loss: 0.1305673\n",
      "\tspeed: 0.0966s/iter; left time: 415.8669s\n",
      "\titers: 500, epoch: 6 | loss: 0.1266187\n",
      "\tspeed: 0.0968s/iter; left time: 407.1302s\n",
      "\titers: 600, epoch: 6 | loss: 0.1479029\n",
      "\tspeed: 0.0966s/iter; left time: 396.7646s\n",
      "\titers: 700, epoch: 6 | loss: 0.1476173\n",
      "\tspeed: 0.0966s/iter; left time: 386.8964s\n",
      "\titers: 800, epoch: 6 | loss: 0.1148601\n",
      "\tspeed: 0.0969s/iter; left time: 378.3632s\n",
      "\titers: 900, epoch: 6 | loss: 0.1208827\n",
      "\tspeed: 0.0966s/iter; left time: 367.6705s\n",
      "Epoch: 6 running time: 1.5233680446942648 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1403683 Vali Loss: 0.1970393 Test Loss: 0.4280555\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.4280802011489868, mae:0.4263731837272644\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4615516\n",
      "\tspeed: 0.0999s/iter; left time: 929.8457s\n",
      "\titers: 200, epoch: 1 | loss: 0.3717112\n",
      "\tspeed: 0.0974s/iter; left time: 897.0835s\n",
      "\titers: 300, epoch: 1 | loss: 0.2420592\n",
      "\tspeed: 0.0975s/iter; left time: 888.5572s\n",
      "\titers: 400, epoch: 1 | loss: 0.2892628\n",
      "\tspeed: 0.0974s/iter; left time: 877.8954s\n",
      "\titers: 500, epoch: 1 | loss: 0.1950183\n",
      "\tspeed: 0.0976s/iter; left time: 869.6414s\n",
      "\titers: 600, epoch: 1 | loss: 0.1980560\n",
      "\tspeed: 0.0974s/iter; left time: 858.4842s\n",
      "\titers: 700, epoch: 1 | loss: 0.1808688\n",
      "\tspeed: 0.0977s/iter; left time: 850.9654s\n",
      "\titers: 800, epoch: 1 | loss: 0.1822334\n",
      "\tspeed: 0.0976s/iter; left time: 840.3172s\n",
      "\titers: 900, epoch: 1 | loss: 0.1763375\n",
      "\tspeed: 0.0972s/iter; left time: 826.9987s\n",
      "Epoch: 1 running time: 1.5346640308698019 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2927992 Vali Loss: 0.2297717 Test Loss: 0.4258846\n",
      "Validation loss decreased (inf --> 0.229772).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1611677\n",
      "\tspeed: 0.2891s/iter; left time: 2419.8587s\n",
      "\titers: 200, epoch: 2 | loss: 0.1260963\n",
      "\tspeed: 0.0974s/iter; left time: 805.8141s\n",
      "\titers: 300, epoch: 2 | loss: 0.2033694\n",
      "\tspeed: 0.0973s/iter; left time: 795.2339s\n",
      "\titers: 400, epoch: 2 | loss: 0.1830730\n",
      "\tspeed: 0.0974s/iter; left time: 786.3393s\n",
      "\titers: 500, epoch: 2 | loss: 0.1794108\n",
      "\tspeed: 0.0970s/iter; left time: 773.0474s\n",
      "\titers: 600, epoch: 2 | loss: 0.1824974\n",
      "\tspeed: 0.0971s/iter; left time: 763.9041s\n",
      "\titers: 700, epoch: 2 | loss: 0.1287629\n",
      "\tspeed: 0.0969s/iter; left time: 753.1014s\n",
      "\titers: 800, epoch: 2 | loss: 0.1632250\n",
      "\tspeed: 0.0966s/iter; left time: 741.1055s\n",
      "\titers: 900, epoch: 2 | loss: 0.2081246\n",
      "\tspeed: 0.0969s/iter; left time: 733.4652s\n",
      "Epoch: 2 running time: 1.5284551660219827 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1834269 Vali Loss: 0.2105469 Test Loss: 0.3801900\n",
      "Validation loss decreased (0.229772 --> 0.210547).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1647339\n",
      "\tspeed: 0.2849s/iter; left time: 2116.2744s\n",
      "\titers: 200, epoch: 3 | loss: 0.1362578\n",
      "\tspeed: 0.0967s/iter; left time: 708.3956s\n",
      "\titers: 300, epoch: 3 | loss: 0.1556100\n",
      "\tspeed: 0.0966s/iter; left time: 698.2137s\n",
      "\titers: 400, epoch: 3 | loss: 0.1345252\n",
      "\tspeed: 0.0966s/iter; left time: 688.8747s\n",
      "\titers: 500, epoch: 3 | loss: 0.1254028\n",
      "\tspeed: 0.0970s/iter; left time: 681.8779s\n",
      "\titers: 600, epoch: 3 | loss: 0.2033450\n",
      "\tspeed: 0.0967s/iter; left time: 670.3239s\n",
      "\titers: 700, epoch: 3 | loss: 0.1246391\n",
      "\tspeed: 0.0967s/iter; left time: 660.4162s\n",
      "\titers: 800, epoch: 3 | loss: 0.1602493\n",
      "\tspeed: 0.0967s/iter; left time: 650.6929s\n",
      "\titers: 900, epoch: 3 | loss: 0.1329695\n",
      "\tspeed: 0.0969s/iter; left time: 642.0242s\n",
      "Epoch: 3 running time: 1.5223548769950868 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1547469 Vali Loss: 0.2031728 Test Loss: 0.4471638\n",
      "Validation loss decreased (0.210547 --> 0.203173).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1356620\n",
      "\tspeed: 0.2841s/iter; left time: 1843.1212s\n",
      "\titers: 200, epoch: 4 | loss: 0.1749342\n",
      "\tspeed: 0.0971s/iter; left time: 620.4980s\n",
      "\titers: 300, epoch: 4 | loss: 0.1217998\n",
      "\tspeed: 0.0972s/iter; left time: 611.3108s\n",
      "\titers: 400, epoch: 4 | loss: 0.1738788\n",
      "\tspeed: 0.0970s/iter; left time: 600.5071s\n",
      "\titers: 500, epoch: 4 | loss: 0.1177602\n",
      "\tspeed: 0.0971s/iter; left time: 591.0000s\n",
      "\titers: 600, epoch: 4 | loss: 0.1449556\n",
      "\tspeed: 0.0970s/iter; left time: 580.6506s\n",
      "\titers: 700, epoch: 4 | loss: 0.1269836\n",
      "\tspeed: 0.0970s/iter; left time: 571.3133s\n",
      "\titers: 800, epoch: 4 | loss: 0.1536709\n",
      "\tspeed: 0.0971s/iter; left time: 562.0988s\n",
      "\titers: 900, epoch: 4 | loss: 0.1160391\n",
      "\tspeed: 0.0971s/iter; left time: 552.1688s\n",
      "Epoch: 4 running time: 1.5282944758733115 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1378072 Vali Loss: 0.2048355 Test Loss: 0.4576509\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1078732\n",
      "\tspeed: 0.2770s/iter; left time: 1536.5457s\n",
      "\titers: 200, epoch: 5 | loss: 0.1233190\n",
      "\tspeed: 0.0968s/iter; left time: 527.1000s\n",
      "\titers: 300, epoch: 5 | loss: 0.1551157\n",
      "\tspeed: 0.0969s/iter; left time: 517.9337s\n",
      "\titers: 400, epoch: 5 | loss: 0.1220378\n",
      "\tspeed: 0.0970s/iter; left time: 508.7779s\n",
      "\titers: 500, epoch: 5 | loss: 0.1117541\n",
      "\tspeed: 0.0971s/iter; left time: 499.8121s\n",
      "\titers: 600, epoch: 5 | loss: 0.1184531\n",
      "\tspeed: 0.0972s/iter; left time: 490.4581s\n",
      "\titers: 700, epoch: 5 | loss: 0.0843452\n",
      "\tspeed: 0.0971s/iter; left time: 480.1214s\n",
      "\titers: 800, epoch: 5 | loss: 0.0975149\n",
      "\tspeed: 0.0973s/iter; left time: 471.8128s\n",
      "\titers: 900, epoch: 5 | loss: 0.1737675\n",
      "\tspeed: 0.0977s/iter; left time: 463.8499s\n",
      "Epoch: 5 running time: 1.5288769880930582 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1347826 Vali Loss: 0.2028673 Test Loss: 0.4539377\n",
      "Validation loss decreased (0.203173 --> 0.202867).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1141522\n",
      "\tspeed: 0.2933s/iter; left time: 1350.9551s\n",
      "\titers: 200, epoch: 6 | loss: 0.1382700\n",
      "\tspeed: 0.0969s/iter; left time: 436.6151s\n",
      "\titers: 300, epoch: 6 | loss: 0.1220936\n",
      "\tspeed: 0.0971s/iter; left time: 427.8729s\n",
      "\titers: 400, epoch: 6 | loss: 0.1189913\n",
      "\tspeed: 0.0970s/iter; left time: 417.6156s\n",
      "\titers: 500, epoch: 6 | loss: 0.1511369\n",
      "\tspeed: 0.0971s/iter; left time: 408.4690s\n",
      "\titers: 600, epoch: 6 | loss: 0.1193791\n",
      "\tspeed: 0.0971s/iter; left time: 398.6746s\n",
      "\titers: 700, epoch: 6 | loss: 0.1090093\n",
      "\tspeed: 0.0971s/iter; left time: 389.1774s\n",
      "\titers: 800, epoch: 6 | loss: 0.1128828\n",
      "\tspeed: 0.0971s/iter; left time: 379.0802s\n",
      "\titers: 900, epoch: 6 | loss: 0.1082211\n",
      "\tspeed: 0.0969s/iter; left time: 368.6783s\n",
      "Epoch: 6 running time: 1.527395252386729 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1281722 Vali Loss: 0.2059329 Test Loss: 0.4533737\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0912404\n",
      "\tspeed: 0.2766s/iter; left time: 1013.6105s\n",
      "\titers: 200, epoch: 7 | loss: 0.1264090\n",
      "\tspeed: 0.0968s/iter; left time: 345.0834s\n",
      "\titers: 300, epoch: 7 | loss: 0.0980890\n",
      "\tspeed: 0.0972s/iter; left time: 336.7476s\n",
      "\titers: 400, epoch: 7 | loss: 0.1341912\n",
      "\tspeed: 0.0972s/iter; left time: 326.9935s\n",
      "\titers: 500, epoch: 7 | loss: 0.1008856\n",
      "\tspeed: 0.0971s/iter; left time: 316.9761s\n",
      "\titers: 600, epoch: 7 | loss: 0.1226514\n",
      "\tspeed: 0.0973s/iter; left time: 307.9266s\n",
      "\titers: 700, epoch: 7 | loss: 0.1260828\n",
      "\tspeed: 0.0865s/iter; left time: 265.0538s\n",
      "\titers: 800, epoch: 7 | loss: 0.1012535\n",
      "\tspeed: 0.0808s/iter; left time: 239.6281s\n",
      "\titers: 900, epoch: 7 | loss: 0.1512322\n",
      "\tspeed: 0.0809s/iter; left time: 231.6744s\n",
      "Epoch: 7 running time: 1.444990885257721 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1276407 Vali Loss: 0.2089041 Test Loss: 0.4533260\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1030174\n",
      "\tspeed: 0.2712s/iter; left time: 738.7806s\n",
      "\titers: 200, epoch: 8 | loss: 0.1066345\n",
      "\tspeed: 0.0977s/iter; left time: 256.3191s\n",
      "\titers: 300, epoch: 8 | loss: 0.1468738\n",
      "\tspeed: 0.0975s/iter; left time: 246.0589s\n",
      "\titers: 400, epoch: 8 | loss: 0.1466367\n",
      "\tspeed: 0.0974s/iter; left time: 236.1327s\n",
      "\titers: 500, epoch: 8 | loss: 0.1612978\n",
      "\tspeed: 0.0974s/iter; left time: 226.3297s\n",
      "\titers: 600, epoch: 8 | loss: 0.0999708\n",
      "\tspeed: 0.0973s/iter; left time: 216.3952s\n",
      "\titers: 700, epoch: 8 | loss: 0.1657367\n",
      "\tspeed: 0.0972s/iter; left time: 206.5432s\n",
      "\titers: 800, epoch: 8 | loss: 0.1215136\n",
      "\tspeed: 0.0973s/iter; left time: 196.9180s\n",
      "\titers: 900, epoch: 8 | loss: 0.0876883\n",
      "\tspeed: 0.0972s/iter; left time: 186.9795s\n",
      "Epoch: 8 running time: 1.5330780347188313 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1278688 Vali Loss: 0.2073265 Test Loss: 0.4550519\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_ES_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.45397818088531494, mae:0.43187764286994934\n",
      "\n",
      "Time intermediate for ES dataset: 25.097303875287373 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.3871442\n",
      "\tspeed: 0.1143s/iter; left time: 1064.2687s\n",
      "\titers: 200, epoch: 1 | loss: 0.2673644\n",
      "\tspeed: 0.0976s/iter; left time: 899.0601s\n",
      "\titers: 300, epoch: 1 | loss: 0.2006789\n",
      "\tspeed: 0.0976s/iter; left time: 889.2121s\n",
      "\titers: 400, epoch: 1 | loss: 0.1515370\n",
      "\tspeed: 0.0971s/iter; left time: 875.2521s\n",
      "\titers: 500, epoch: 1 | loss: 0.2455160\n",
      "\tspeed: 0.0969s/iter; left time: 863.4253s\n",
      "\titers: 600, epoch: 1 | loss: 0.2780952\n",
      "\tspeed: 0.0971s/iter; left time: 855.8937s\n",
      "\titers: 700, epoch: 1 | loss: 0.2736951\n",
      "\tspeed: 0.0971s/iter; left time: 846.2519s\n",
      "\titers: 800, epoch: 1 | loss: 0.2647220\n",
      "\tspeed: 0.0971s/iter; left time: 836.1259s\n",
      "\titers: 900, epoch: 1 | loss: 0.1626280\n",
      "\tspeed: 0.0970s/iter; left time: 825.6210s\n",
      "Epoch: 1 running time: 1.5368265668551127 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2831952 Vali Loss: 0.3403730 Test Loss: 0.4731552\n",
      "Validation loss decreased (inf --> 0.340373).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2109833\n",
      "\tspeed: 0.2935s/iter; left time: 2456.3714s\n",
      "\titers: 200, epoch: 2 | loss: 0.2622070\n",
      "\tspeed: 0.0972s/iter; left time: 803.8910s\n",
      "\titers: 300, epoch: 2 | loss: 0.2403401\n",
      "\tspeed: 0.0977s/iter; left time: 798.3416s\n",
      "\titers: 400, epoch: 2 | loss: 0.1878872\n",
      "\tspeed: 0.0970s/iter; left time: 782.8157s\n",
      "\titers: 500, epoch: 2 | loss: 0.1452242\n",
      "\tspeed: 0.0975s/iter; left time: 777.3137s\n",
      "\titers: 600, epoch: 2 | loss: 0.1755536\n",
      "\tspeed: 0.0974s/iter; left time: 766.8489s\n",
      "\titers: 700, epoch: 2 | loss: 0.1722669\n",
      "\tspeed: 0.0975s/iter; left time: 757.5816s\n",
      "\titers: 800, epoch: 2 | loss: 0.1908424\n",
      "\tspeed: 0.0972s/iter; left time: 745.7017s\n",
      "\titers: 900, epoch: 2 | loss: 0.1633042\n",
      "\tspeed: 0.0972s/iter; left time: 736.1416s\n",
      "Epoch: 2 running time: 1.531830076376597 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1822287 Vali Loss: 0.2944102 Test Loss: 0.4071387\n",
      "Validation loss decreased (0.340373 --> 0.294410).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1837488\n",
      "\tspeed: 0.2908s/iter; left time: 2160.5219s\n",
      "\titers: 200, epoch: 3 | loss: 0.1310508\n",
      "\tspeed: 0.0970s/iter; left time: 710.8809s\n",
      "\titers: 300, epoch: 3 | loss: 0.1907982\n",
      "\tspeed: 0.0971s/iter; left time: 701.7495s\n",
      "\titers: 400, epoch: 3 | loss: 0.1499288\n",
      "\tspeed: 0.0968s/iter; left time: 689.8634s\n",
      "\titers: 500, epoch: 3 | loss: 0.1474892\n",
      "\tspeed: 0.0973s/iter; left time: 683.7511s\n",
      "\titers: 600, epoch: 3 | loss: 0.1105659\n",
      "\tspeed: 0.0927s/iter; left time: 642.5040s\n",
      "\titers: 700, epoch: 3 | loss: 0.0873594\n",
      "\tspeed: 0.0933s/iter; left time: 636.8864s\n",
      "\titers: 800, epoch: 3 | loss: 0.1873035\n",
      "\tspeed: 0.0971s/iter; left time: 653.6624s\n",
      "\titers: 900, epoch: 3 | loss: 0.1809686\n",
      "\tspeed: 0.0973s/iter; left time: 644.9507s\n",
      "Epoch: 3 running time: 1.5141734639803568 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1509856 Vali Loss: 0.2864288 Test Loss: 0.3756933\n",
      "Validation loss decreased (0.294410 --> 0.286429).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1160307\n",
      "\tspeed: 0.3213s/iter; left time: 2084.3258s\n",
      "\titers: 200, epoch: 4 | loss: 0.1975920\n",
      "\tspeed: 0.0968s/iter; left time: 618.4475s\n",
      "\titers: 300, epoch: 4 | loss: 0.1360142\n",
      "\tspeed: 0.0971s/iter; left time: 610.3446s\n",
      "\titers: 400, epoch: 4 | loss: 0.1175752\n",
      "\tspeed: 0.0975s/iter; left time: 603.3163s\n",
      "\titers: 500, epoch: 4 | loss: 0.1336189\n",
      "\tspeed: 0.0975s/iter; left time: 593.8769s\n",
      "\titers: 600, epoch: 4 | loss: 0.1228819\n",
      "\tspeed: 0.0975s/iter; left time: 583.8841s\n",
      "\titers: 700, epoch: 4 | loss: 0.1417742\n",
      "\tspeed: 0.0976s/iter; left time: 574.4098s\n",
      "\titers: 800, epoch: 4 | loss: 0.1064660\n",
      "\tspeed: 0.0974s/iter; left time: 563.8421s\n",
      "\titers: 900, epoch: 4 | loss: 0.1395320\n",
      "\tspeed: 0.0972s/iter; left time: 552.6575s\n",
      "Epoch: 4 running time: 1.5309617122014363 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1304822 Vali Loss: 0.3309550 Test Loss: 0.4242738\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1633688\n",
      "\tspeed: 0.2769s/iter; left time: 1536.0213s\n",
      "\titers: 200, epoch: 5 | loss: 0.1172312\n",
      "\tspeed: 0.0973s/iter; left time: 530.2214s\n",
      "\titers: 300, epoch: 5 | loss: 0.1190069\n",
      "\tspeed: 0.0974s/iter; left time: 520.6963s\n",
      "\titers: 400, epoch: 5 | loss: 0.1134198\n",
      "\tspeed: 0.0973s/iter; left time: 510.6490s\n",
      "\titers: 500, epoch: 5 | loss: 0.1366301\n",
      "\tspeed: 0.0972s/iter; left time: 500.1992s\n",
      "\titers: 600, epoch: 5 | loss: 0.1186669\n",
      "\tspeed: 0.0970s/iter; left time: 489.3615s\n",
      "\titers: 700, epoch: 5 | loss: 0.1451427\n",
      "\tspeed: 0.0970s/iter; left time: 479.6507s\n",
      "\titers: 800, epoch: 5 | loss: 0.1125119\n",
      "\tspeed: 0.0970s/iter; left time: 470.2731s\n",
      "\titers: 900, epoch: 5 | loss: 0.1232951\n",
      "\tspeed: 0.0971s/iter; left time: 461.0936s\n",
      "Epoch: 5 running time: 1.529065724213918 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1284068 Vali Loss: 0.3053068 Test Loss: 0.3945112\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1270653\n",
      "\tspeed: 0.2785s/iter; left time: 1282.9739s\n",
      "\titers: 200, epoch: 6 | loss: 0.1581656\n",
      "\tspeed: 0.0978s/iter; left time: 440.8601s\n",
      "\titers: 300, epoch: 6 | loss: 0.1384328\n",
      "\tspeed: 0.0976s/iter; left time: 430.0756s\n",
      "\titers: 400, epoch: 6 | loss: 0.1312439\n",
      "\tspeed: 0.0976s/iter; left time: 420.1212s\n",
      "\titers: 500, epoch: 6 | loss: 0.0986435\n",
      "\tspeed: 0.0974s/iter; left time: 409.5344s\n",
      "\titers: 600, epoch: 6 | loss: 0.1524227\n",
      "\tspeed: 0.0975s/iter; left time: 400.3371s\n",
      "\titers: 700, epoch: 6 | loss: 0.1449386\n",
      "\tspeed: 0.0978s/iter; left time: 391.7243s\n",
      "\titers: 800, epoch: 6 | loss: 0.1303601\n",
      "\tspeed: 0.0974s/iter; left time: 380.2652s\n",
      "\titers: 900, epoch: 6 | loss: 0.1455246\n",
      "\tspeed: 0.0973s/iter; left time: 370.1737s\n",
      "Epoch: 6 running time: 1.53598503669103 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1284552 Vali Loss: 0.2947669 Test Loss: 0.3761651\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.376305490732193, mae:0.3465350866317749\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.3645963\n",
      "\tspeed: 0.1004s/iter; left time: 934.6928s\n",
      "\titers: 200, epoch: 1 | loss: 0.2530006\n",
      "\tspeed: 0.0972s/iter; left time: 894.9604s\n",
      "\titers: 300, epoch: 1 | loss: 0.2502516\n",
      "\tspeed: 0.0971s/iter; left time: 884.9338s\n",
      "\titers: 400, epoch: 1 | loss: 0.3324000\n",
      "\tspeed: 0.0971s/iter; left time: 874.6701s\n",
      "\titers: 500, epoch: 1 | loss: 0.1925101\n",
      "\tspeed: 0.0973s/iter; left time: 867.2071s\n",
      "\titers: 600, epoch: 1 | loss: 0.2108352\n",
      "\tspeed: 0.0973s/iter; left time: 857.6725s\n",
      "\titers: 700, epoch: 1 | loss: 0.2098276\n",
      "\tspeed: 0.0974s/iter; left time: 848.8154s\n",
      "\titers: 800, epoch: 1 | loss: 0.2060880\n",
      "\tspeed: 0.0972s/iter; left time: 836.9771s\n",
      "\titers: 900, epoch: 1 | loss: 0.1404011\n",
      "\tspeed: 0.0971s/iter; left time: 826.0035s\n",
      "Epoch: 1 running time: 1.5318753600120545 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2730209 Vali Loss: 0.3222533 Test Loss: 0.4460111\n",
      "Validation loss decreased (inf --> 0.322253).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1984005\n",
      "\tspeed: 0.2887s/iter; left time: 2416.2879s\n",
      "\titers: 200, epoch: 2 | loss: 0.2007069\n",
      "\tspeed: 0.0973s/iter; left time: 804.5059s\n",
      "\titers: 300, epoch: 2 | loss: 0.1559824\n",
      "\tspeed: 0.0974s/iter; left time: 795.6465s\n",
      "\titers: 400, epoch: 2 | loss: 0.1357851\n",
      "\tspeed: 0.0973s/iter; left time: 785.4594s\n",
      "\titers: 500, epoch: 2 | loss: 0.1429573\n",
      "\tspeed: 0.0859s/iter; left time: 684.8173s\n",
      "\titers: 600, epoch: 2 | loss: 0.2798784\n",
      "\tspeed: 0.0806s/iter; left time: 634.6494s\n",
      "\titers: 700, epoch: 2 | loss: 0.1987832\n",
      "\tspeed: 0.0805s/iter; left time: 625.2673s\n",
      "\titers: 800, epoch: 2 | loss: 0.2771645\n",
      "\tspeed: 0.0805s/iter; left time: 617.1290s\n",
      "\titers: 900, epoch: 2 | loss: 0.2133608\n",
      "\tspeed: 0.0866s/iter; left time: 655.6825s\n",
      "Epoch: 2 running time: 1.4106388449668885 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1810938 Vali Loss: 0.2826883 Test Loss: 0.3611123\n",
      "Validation loss decreased (0.322253 --> 0.282688).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1094134\n",
      "\tspeed: 0.2867s/iter; left time: 2129.7555s\n",
      "\titers: 200, epoch: 3 | loss: 0.1831797\n",
      "\tspeed: 0.0969s/iter; left time: 710.4166s\n",
      "\titers: 300, epoch: 3 | loss: 0.1286767\n",
      "\tspeed: 0.0967s/iter; left time: 699.1545s\n",
      "\titers: 400, epoch: 3 | loss: 0.2115197\n",
      "\tspeed: 0.0972s/iter; left time: 693.1521s\n",
      "\titers: 500, epoch: 3 | loss: 0.1578527\n",
      "\tspeed: 0.0972s/iter; left time: 682.9155s\n",
      "\titers: 600, epoch: 3 | loss: 0.2022129\n",
      "\tspeed: 0.0971s/iter; left time: 673.0702s\n",
      "\titers: 700, epoch: 3 | loss: 0.1397685\n",
      "\tspeed: 0.0971s/iter; left time: 663.0712s\n",
      "\titers: 800, epoch: 3 | loss: 0.1687072\n",
      "\tspeed: 0.0969s/iter; left time: 651.9069s\n",
      "\titers: 900, epoch: 3 | loss: 0.1681002\n",
      "\tspeed: 0.0968s/iter; left time: 641.8170s\n",
      "Epoch: 3 running time: 1.5261711756388345 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1517087 Vali Loss: 0.3062576 Test Loss: 0.4026063\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2024036\n",
      "\tspeed: 0.2771s/iter; left time: 1798.1090s\n",
      "\titers: 200, epoch: 4 | loss: 0.1484924\n",
      "\tspeed: 0.0971s/iter; left time: 620.4485s\n",
      "\titers: 300, epoch: 4 | loss: 0.1262355\n",
      "\tspeed: 0.0970s/iter; left time: 610.0219s\n",
      "\titers: 400, epoch: 4 | loss: 0.1609347\n",
      "\tspeed: 0.0971s/iter; left time: 600.8408s\n",
      "\titers: 500, epoch: 4 | loss: 0.1002636\n",
      "\tspeed: 0.0976s/iter; left time: 593.9800s\n",
      "\titers: 600, epoch: 4 | loss: 0.1552093\n",
      "\tspeed: 0.0979s/iter; left time: 586.2292s\n",
      "\titers: 700, epoch: 4 | loss: 0.1250455\n",
      "\tspeed: 0.0974s/iter; left time: 573.2192s\n",
      "\titers: 800, epoch: 4 | loss: 0.1357763\n",
      "\tspeed: 0.0977s/iter; left time: 565.2395s\n",
      "\titers: 900, epoch: 4 | loss: 0.1558176\n",
      "\tspeed: 0.0977s/iter; left time: 555.8122s\n",
      "Epoch: 4 running time: 1.532899002234141 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1458911 Vali Loss: 0.2872951 Test Loss: 0.3685558\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1385542\n",
      "\tspeed: 0.2771s/iter; left time: 1537.2016s\n",
      "\titers: 200, epoch: 5 | loss: 0.1521357\n",
      "\tspeed: 0.0847s/iter; left time: 461.3175s\n",
      "\titers: 300, epoch: 5 | loss: 0.1631263\n",
      "\tspeed: 0.0803s/iter; left time: 429.3974s\n",
      "\titers: 400, epoch: 5 | loss: 0.1367502\n",
      "\tspeed: 0.0803s/iter; left time: 421.2547s\n",
      "\titers: 500, epoch: 5 | loss: 0.1269555\n",
      "\tspeed: 0.0804s/iter; left time: 414.0155s\n",
      "\titers: 600, epoch: 5 | loss: 0.1773288\n",
      "\tspeed: 0.0803s/iter; left time: 405.2106s\n",
      "\titers: 700, epoch: 5 | loss: 0.1564832\n",
      "\tspeed: 0.0803s/iter; left time: 397.3045s\n",
      "\titers: 800, epoch: 5 | loss: 0.1027914\n",
      "\tspeed: 0.0886s/iter; left time: 429.5130s\n",
      "\titers: 900, epoch: 5 | loss: 0.1174207\n",
      "\tspeed: 0.0887s/iter; left time: 421.0737s\n",
      "Epoch: 5 running time: 1.3400564392407734 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1455864 Vali Loss: 0.2870651 Test Loss: 0.3670011\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_FR_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.3677647113800049, mae:0.3457011580467224\n",
      "\n",
      "Time intermediate for FR dataset: 19.65734891096751 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4839092\n",
      "\tspeed: 0.1166s/iter; left time: 1086.0314s\n",
      "\titers: 200, epoch: 1 | loss: 0.2879023\n",
      "\tspeed: 0.0975s/iter; left time: 898.1821s\n",
      "\titers: 300, epoch: 1 | loss: 0.1979710\n",
      "\tspeed: 0.0972s/iter; left time: 885.5872s\n",
      "\titers: 400, epoch: 1 | loss: 0.2898935\n",
      "\tspeed: 0.0973s/iter; left time: 876.9078s\n",
      "\titers: 500, epoch: 1 | loss: 0.2302311\n",
      "\tspeed: 0.0972s/iter; left time: 866.3837s\n",
      "\titers: 600, epoch: 1 | loss: 0.1686959\n",
      "\tspeed: 0.0973s/iter; left time: 857.1091s\n",
      "\titers: 700, epoch: 1 | loss: 0.1522209\n",
      "\tspeed: 0.0973s/iter; left time: 847.3855s\n",
      "\titers: 800, epoch: 1 | loss: 0.1437175\n",
      "\tspeed: 0.0970s/iter; left time: 835.4727s\n",
      "\titers: 900, epoch: 1 | loss: 0.1686486\n",
      "\tspeed: 0.0899s/iter; left time: 765.3963s\n",
      "Epoch: 1 running time: 1.5161388357480368 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2949341 Vali Loss: 0.2057955 Test Loss: 0.2377230\n",
      "Validation loss decreased (inf --> 0.205795).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1871080\n",
      "\tspeed: 0.2789s/iter; left time: 2334.1092s\n",
      "\titers: 200, epoch: 2 | loss: 0.2467183\n",
      "\tspeed: 0.0973s/iter; left time: 804.9909s\n",
      "\titers: 300, epoch: 2 | loss: 0.2306656\n",
      "\tspeed: 0.0973s/iter; left time: 794.8072s\n",
      "\titers: 400, epoch: 2 | loss: 0.2101787\n",
      "\tspeed: 0.0974s/iter; left time: 786.3231s\n",
      "\titers: 500, epoch: 2 | loss: 0.1622231\n",
      "\tspeed: 0.0972s/iter; left time: 774.7563s\n",
      "\titers: 600, epoch: 2 | loss: 0.1890053\n",
      "\tspeed: 0.0970s/iter; left time: 763.1458s\n",
      "\titers: 700, epoch: 2 | loss: 0.1664769\n",
      "\tspeed: 0.0970s/iter; left time: 753.5148s\n",
      "\titers: 800, epoch: 2 | loss: 0.1611083\n",
      "\tspeed: 0.0970s/iter; left time: 744.1080s\n",
      "\titers: 900, epoch: 2 | loss: 0.2106168\n",
      "\tspeed: 0.0972s/iter; left time: 735.8354s\n",
      "Epoch: 2 running time: 1.5299260497093201 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1814834 Vali Loss: 0.1802848 Test Loss: 0.2164322\n",
      "Validation loss decreased (0.205795 --> 0.180285).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1289343\n",
      "\tspeed: 0.3010s/iter; left time: 2235.9511s\n",
      "\titers: 200, epoch: 3 | loss: 0.1253333\n",
      "\tspeed: 0.0973s/iter; left time: 713.2604s\n",
      "\titers: 300, epoch: 3 | loss: 0.2193986\n",
      "\tspeed: 0.0970s/iter; left time: 701.1262s\n",
      "\titers: 400, epoch: 3 | loss: 0.1899805\n",
      "\tspeed: 0.0970s/iter; left time: 691.6948s\n",
      "\titers: 500, epoch: 3 | loss: 0.1395585\n",
      "\tspeed: 0.0947s/iter; left time: 665.7867s\n",
      "\titers: 600, epoch: 3 | loss: 0.1718399\n",
      "\tspeed: 0.0807s/iter; left time: 559.4069s\n",
      "\titers: 700, epoch: 3 | loss: 0.1389031\n",
      "\tspeed: 0.0806s/iter; left time: 550.4816s\n",
      "\titers: 800, epoch: 3 | loss: 0.1707161\n",
      "\tspeed: 0.0910s/iter; left time: 612.1392s\n",
      "\titers: 900, epoch: 3 | loss: 0.2030998\n",
      "\tspeed: 0.0975s/iter; left time: 646.2408s\n",
      "Epoch: 3 running time: 1.4607286334037781 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1552277 Vali Loss: 0.1816653 Test Loss: 0.2102130\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1277877\n",
      "\tspeed: 0.2780s/iter; left time: 1803.8482s\n",
      "\titers: 200, epoch: 4 | loss: 0.1270532\n",
      "\tspeed: 0.0974s/iter; left time: 622.1548s\n",
      "\titers: 300, epoch: 4 | loss: 0.1585241\n",
      "\tspeed: 0.0975s/iter; left time: 613.0739s\n",
      "\titers: 400, epoch: 4 | loss: 0.1155248\n",
      "\tspeed: 0.0974s/iter; left time: 602.4322s\n",
      "\titers: 500, epoch: 4 | loss: 0.2004012\n",
      "\tspeed: 0.0972s/iter; left time: 591.9065s\n",
      "\titers: 600, epoch: 4 | loss: 0.1365615\n",
      "\tspeed: 0.0971s/iter; left time: 581.4415s\n",
      "\titers: 700, epoch: 4 | loss: 0.2043141\n",
      "\tspeed: 0.0971s/iter; left time: 571.8372s\n",
      "\titers: 800, epoch: 4 | loss: 0.1183679\n",
      "\tspeed: 0.0969s/iter; left time: 560.6396s\n",
      "\titers: 900, epoch: 4 | loss: 0.1802714\n",
      "\tspeed: 0.0969s/iter; left time: 550.8840s\n",
      "Epoch: 4 running time: 1.53014417886734 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1510249 Vali Loss: 0.1730414 Test Loss: 0.2099670\n",
      "Validation loss decreased (0.180285 --> 0.173041).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1369129\n",
      "\tspeed: 0.2882s/iter; left time: 1598.3821s\n",
      "\titers: 200, epoch: 5 | loss: 0.1516403\n",
      "\tspeed: 0.0968s/iter; left time: 527.4314s\n",
      "\titers: 300, epoch: 5 | loss: 0.1417980\n",
      "\tspeed: 0.0970s/iter; left time: 518.6522s\n",
      "\titers: 400, epoch: 5 | loss: 0.1680433\n",
      "\tspeed: 0.0972s/iter; left time: 510.1418s\n",
      "\titers: 500, epoch: 5 | loss: 0.1167932\n",
      "\tspeed: 0.0969s/iter; left time: 498.7647s\n",
      "\titers: 600, epoch: 5 | loss: 0.1098669\n",
      "\tspeed: 0.0969s/iter; left time: 488.8832s\n",
      "\titers: 700, epoch: 5 | loss: 0.1567369\n",
      "\tspeed: 0.0967s/iter; left time: 478.2988s\n",
      "\titers: 800, epoch: 5 | loss: 0.1263465\n",
      "\tspeed: 0.0969s/iter; left time: 469.4937s\n",
      "\titers: 900, epoch: 5 | loss: 0.1337125\n",
      "\tspeed: 0.0966s/iter; left time: 458.6421s\n",
      "Epoch: 5 running time: 1.5253217061360678 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1402030 Vali Loss: 0.1705149 Test Loss: 0.2082625\n",
      "Validation loss decreased (0.173041 --> 0.170515).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1152303\n",
      "\tspeed: 0.2862s/iter; left time: 1318.3531s\n",
      "\titers: 200, epoch: 6 | loss: 0.1238060\n",
      "\tspeed: 0.0971s/iter; left time: 437.4159s\n",
      "\titers: 300, epoch: 6 | loss: 0.1190059\n",
      "\tspeed: 0.0972s/iter; left time: 428.1053s\n",
      "\titers: 400, epoch: 6 | loss: 0.1147590\n",
      "\tspeed: 0.0968s/iter; left time: 417.0236s\n",
      "\titers: 500, epoch: 6 | loss: 0.1253050\n",
      "\tspeed: 0.0970s/iter; left time: 408.1636s\n",
      "\titers: 600, epoch: 6 | loss: 0.1378735\n",
      "\tspeed: 0.0969s/iter; left time: 397.8370s\n",
      "\titers: 700, epoch: 6 | loss: 0.1760725\n",
      "\tspeed: 0.0970s/iter; left time: 388.7361s\n",
      "\titers: 800, epoch: 6 | loss: 0.1508690\n",
      "\tspeed: 0.0968s/iter; left time: 378.1120s\n",
      "\titers: 900, epoch: 6 | loss: 0.1142941\n",
      "\tspeed: 0.0967s/iter; left time: 368.0968s\n",
      "Epoch: 6 running time: 1.5257254719734192 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1335333 Vali Loss: 0.1734702 Test Loss: 0.2075192\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1488219\n",
      "\tspeed: 0.2776s/iter; left time: 1017.3604s\n",
      "\titers: 200, epoch: 7 | loss: 0.1125568\n",
      "\tspeed: 0.0969s/iter; left time: 345.3349s\n",
      "\titers: 300, epoch: 7 | loss: 0.1032985\n",
      "\tspeed: 0.0970s/iter; left time: 336.1966s\n",
      "\titers: 400, epoch: 7 | loss: 0.1683031\n",
      "\tspeed: 0.0968s/iter; left time: 325.7489s\n",
      "\titers: 500, epoch: 7 | loss: 0.1305165\n",
      "\tspeed: 0.0970s/iter; left time: 316.7585s\n",
      "\titers: 600, epoch: 7 | loss: 0.1270051\n",
      "\tspeed: 0.0969s/iter; left time: 306.7320s\n",
      "\titers: 700, epoch: 7 | loss: 0.1417654\n",
      "\tspeed: 0.0969s/iter; left time: 297.0611s\n",
      "\titers: 800, epoch: 7 | loss: 0.1238963\n",
      "\tspeed: 0.0969s/iter; left time: 287.2317s\n",
      "\titers: 900, epoch: 7 | loss: 0.1218866\n",
      "\tspeed: 0.0968s/iter; left time: 277.3624s\n",
      "Epoch: 7 running time: 1.5260143399238586 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1330590 Vali Loss: 0.1745494 Test Loss: 0.2093001\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1437543\n",
      "\tspeed: 0.2758s/iter; left time: 751.2136s\n",
      "\titers: 200, epoch: 8 | loss: 0.1501963\n",
      "\tspeed: 0.0804s/iter; left time: 210.9426s\n",
      "\titers: 300, epoch: 8 | loss: 0.1663292\n",
      "\tspeed: 0.0805s/iter; left time: 203.1765s\n",
      "\titers: 400, epoch: 8 | loss: 0.1561915\n",
      "\tspeed: 0.0821s/iter; left time: 199.1101s\n",
      "\titers: 500, epoch: 8 | loss: 0.1561173\n",
      "\tspeed: 0.0971s/iter; left time: 225.6907s\n",
      "\titers: 600, epoch: 8 | loss: 0.1387023\n",
      "\tspeed: 0.0943s/iter; left time: 209.6413s\n",
      "\titers: 700, epoch: 8 | loss: 0.1437005\n",
      "\tspeed: 0.0804s/iter; left time: 170.6885s\n",
      "\titers: 800, epoch: 8 | loss: 0.1436442\n",
      "\tspeed: 0.0804s/iter; left time: 162.7610s\n",
      "\titers: 900, epoch: 8 | loss: 0.1478741\n",
      "\tspeed: 0.0803s/iter; left time: 154.5864s\n",
      "Epoch: 8 running time: 1.3457509438196817 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1326532 Vali Loss: 0.1729570 Test Loss: 0.2086088\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.2078585922718048, mae:0.28059908747673035\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 30121\n",
      "val 6481\n",
      "test 6457\n",
      "\titers: 100, epoch: 1 | loss: 0.4013638\n",
      "\tspeed: 0.1004s/iter; left time: 934.7901s\n",
      "\titers: 200, epoch: 1 | loss: 0.2693781\n",
      "\tspeed: 0.0947s/iter; left time: 872.3170s\n",
      "\titers: 300, epoch: 1 | loss: 0.2633201\n",
      "\tspeed: 0.0894s/iter; left time: 814.8679s\n",
      "\titers: 400, epoch: 1 | loss: 0.2480822\n",
      "\tspeed: 0.0809s/iter; left time: 728.6534s\n",
      "\titers: 500, epoch: 1 | loss: 0.2404238\n",
      "\tspeed: 0.0809s/iter; left time: 720.6061s\n",
      "\titers: 600, epoch: 1 | loss: 0.2604980\n",
      "\tspeed: 0.0974s/iter; left time: 858.5177s\n",
      "\titers: 700, epoch: 1 | loss: 0.2608070\n",
      "\tspeed: 0.0977s/iter; left time: 851.4170s\n",
      "\titers: 800, epoch: 1 | loss: 0.2468152\n",
      "\tspeed: 0.0984s/iter; left time: 847.1648s\n",
      "\titers: 900, epoch: 1 | loss: 0.1903339\n",
      "\tspeed: 0.0981s/iter; left time: 835.1420s\n",
      "Epoch: 1 running time: 1.4651720841725668 min.\n",
      "Epoch: 1, Steps: 941 | Train Loss: 0.2930441 Vali Loss: 0.1992720 Test Loss: 0.2345044\n",
      "Validation loss decreased (inf --> 0.199272).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1392040\n",
      "\tspeed: 0.2902s/iter; left time: 2428.9433s\n",
      "\titers: 200, epoch: 2 | loss: 0.2477982\n",
      "\tspeed: 0.0973s/iter; left time: 804.7236s\n",
      "\titers: 300, epoch: 2 | loss: 0.1878473\n",
      "\tspeed: 0.0975s/iter; left time: 796.3219s\n",
      "\titers: 400, epoch: 2 | loss: 0.1826797\n",
      "\tspeed: 0.0975s/iter; left time: 786.8773s\n",
      "\titers: 500, epoch: 2 | loss: 0.1646761\n",
      "\tspeed: 0.0974s/iter; left time: 776.3010s\n",
      "\titers: 600, epoch: 2 | loss: 0.1707533\n",
      "\tspeed: 0.0974s/iter; left time: 766.5610s\n",
      "\titers: 700, epoch: 2 | loss: 0.1851241\n",
      "\tspeed: 0.0973s/iter; left time: 756.2775s\n",
      "\titers: 800, epoch: 2 | loss: 0.1617048\n",
      "\tspeed: 0.0853s/iter; left time: 654.1885s\n",
      "\titers: 900, epoch: 2 | loss: 0.2263282\n",
      "\tspeed: 0.0892s/iter; left time: 675.0035s\n",
      "Epoch: 2 running time: 1.5001471519470215 min.\n",
      "Epoch: 2, Steps: 941 | Train Loss: 0.1814804 Vali Loss: 0.1809550 Test Loss: 0.2198144\n",
      "Validation loss decreased (0.199272 --> 0.180955).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1686900\n",
      "\tspeed: 0.2901s/iter; left time: 2155.4136s\n",
      "\titers: 200, epoch: 3 | loss: 0.1713548\n",
      "\tspeed: 0.0973s/iter; left time: 712.7599s\n",
      "\titers: 300, epoch: 3 | loss: 0.1577954\n",
      "\tspeed: 0.0973s/iter; left time: 703.3057s\n",
      "\titers: 400, epoch: 3 | loss: 0.1615140\n",
      "\tspeed: 0.0973s/iter; left time: 693.3356s\n",
      "\titers: 500, epoch: 3 | loss: 0.1664101\n",
      "\tspeed: 0.0971s/iter; left time: 682.5307s\n",
      "\titers: 600, epoch: 3 | loss: 0.1760536\n",
      "\tspeed: 0.0971s/iter; left time: 672.5504s\n",
      "\titers: 700, epoch: 3 | loss: 0.1883275\n",
      "\tspeed: 0.0972s/iter; left time: 663.5265s\n",
      "\titers: 800, epoch: 3 | loss: 0.1471296\n",
      "\tspeed: 0.0970s/iter; left time: 653.0219s\n",
      "\titers: 900, epoch: 3 | loss: 0.1359546\n",
      "\tspeed: 0.0971s/iter; left time: 643.9129s\n",
      "Epoch: 3 running time: 1.52936509847641 min.\n",
      "Epoch: 3, Steps: 941 | Train Loss: 0.1541760 Vali Loss: 0.1816497 Test Loss: 0.2243523\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1347603\n",
      "\tspeed: 0.2769s/iter; left time: 1796.7577s\n",
      "\titers: 200, epoch: 4 | loss: 0.2269212\n",
      "\tspeed: 0.0976s/iter; left time: 623.3903s\n",
      "\titers: 300, epoch: 4 | loss: 0.1412654\n",
      "\tspeed: 0.0974s/iter; left time: 612.7080s\n",
      "\titers: 400, epoch: 4 | loss: 0.1667671\n",
      "\tspeed: 0.0971s/iter; left time: 600.6817s\n",
      "\titers: 500, epoch: 4 | loss: 0.1234495\n",
      "\tspeed: 0.0974s/iter; left time: 592.8435s\n",
      "\titers: 600, epoch: 4 | loss: 0.1387905\n",
      "\tspeed: 0.0975s/iter; left time: 583.7818s\n",
      "\titers: 700, epoch: 4 | loss: 0.1710704\n",
      "\tspeed: 0.0972s/iter; left time: 572.4523s\n",
      "\titers: 800, epoch: 4 | loss: 0.1381954\n",
      "\tspeed: 0.0896s/iter; left time: 518.4038s\n",
      "\titers: 900, epoch: 4 | loss: 0.1563915\n",
      "\tspeed: 0.0808s/iter; left time: 459.8365s\n",
      "Epoch: 4 running time: 1.479530425866445 min.\n",
      "Epoch: 4, Steps: 941 | Train Loss: 0.1500792 Vali Loss: 0.1795949 Test Loss: 0.2172265\n",
      "Validation loss decreased (0.180955 --> 0.179595).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1630919\n",
      "\tspeed: 0.2778s/iter; left time: 1541.0190s\n",
      "\titers: 200, epoch: 5 | loss: 0.1436630\n",
      "\tspeed: 0.0973s/iter; left time: 529.7415s\n",
      "\titers: 300, epoch: 5 | loss: 0.1557225\n",
      "\tspeed: 0.0972s/iter; left time: 519.9725s\n",
      "\titers: 400, epoch: 5 | loss: 0.1628073\n",
      "\tspeed: 0.0973s/iter; left time: 510.6655s\n",
      "\titers: 500, epoch: 5 | loss: 0.1102765\n",
      "\tspeed: 0.0972s/iter; left time: 500.4544s\n",
      "\titers: 600, epoch: 5 | loss: 0.1188116\n",
      "\tspeed: 0.0975s/iter; left time: 492.0877s\n",
      "\titers: 700, epoch: 5 | loss: 0.1028605\n",
      "\tspeed: 0.0975s/iter; left time: 482.2473s\n",
      "\titers: 800, epoch: 5 | loss: 0.1280677\n",
      "\tspeed: 0.0976s/iter; left time: 472.9933s\n",
      "\titers: 900, epoch: 5 | loss: 0.1363173\n",
      "\tspeed: 0.0977s/iter; left time: 463.5608s\n",
      "Epoch: 5 running time: 1.5335821469624837 min.\n",
      "Epoch: 5, Steps: 941 | Train Loss: 0.1394359 Vali Loss: 0.1735027 Test Loss: 0.2180997\n",
      "Validation loss decreased (0.179595 --> 0.173503).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0928828\n",
      "\tspeed: 0.2869s/iter; left time: 1321.3865s\n",
      "\titers: 200, epoch: 6 | loss: 0.1335224\n",
      "\tspeed: 0.0973s/iter; left time: 438.3864s\n",
      "\titers: 300, epoch: 6 | loss: 0.1311455\n",
      "\tspeed: 0.0973s/iter; left time: 428.7166s\n",
      "\titers: 400, epoch: 6 | loss: 0.1267805\n",
      "\tspeed: 0.0973s/iter; left time: 418.9693s\n",
      "\titers: 500, epoch: 6 | loss: 0.1182488\n",
      "\tspeed: 0.0974s/iter; left time: 409.7429s\n",
      "\titers: 600, epoch: 6 | loss: 0.1311987\n",
      "\tspeed: 0.0973s/iter; left time: 399.3830s\n",
      "\titers: 700, epoch: 6 | loss: 0.1573736\n",
      "\tspeed: 0.0971s/iter; left time: 388.9250s\n",
      "\titers: 800, epoch: 6 | loss: 0.1325682\n",
      "\tspeed: 0.0971s/iter; left time: 379.3302s\n",
      "\titers: 900, epoch: 6 | loss: 0.1179233\n",
      "\tspeed: 0.0971s/iter; left time: 369.6844s\n",
      "Epoch: 6 running time: 1.5322825034459433 min.\n",
      "Epoch: 6, Steps: 941 | Train Loss: 0.1324553 Vali Loss: 0.1758273 Test Loss: 0.2212599\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1398861\n",
      "\tspeed: 0.2772s/iter; left time: 1015.7551s\n",
      "\titers: 200, epoch: 7 | loss: 0.1191580\n",
      "\tspeed: 0.0970s/iter; left time: 345.9199s\n",
      "\titers: 300, epoch: 7 | loss: 0.1381503\n",
      "\tspeed: 0.0973s/iter; left time: 337.1856s\n",
      "\titers: 400, epoch: 7 | loss: 0.0865805\n",
      "\tspeed: 0.0975s/iter; left time: 328.1952s\n",
      "\titers: 500, epoch: 7 | loss: 0.1445700\n",
      "\tspeed: 0.0972s/iter; left time: 317.3466s\n",
      "\titers: 600, epoch: 7 | loss: 0.1895263\n",
      "\tspeed: 0.0971s/iter; left time: 307.1885s\n",
      "\titers: 700, epoch: 7 | loss: 0.1291548\n",
      "\tspeed: 0.0972s/iter; left time: 297.7739s\n",
      "\titers: 800, epoch: 7 | loss: 0.1390143\n",
      "\tspeed: 0.0972s/iter; left time: 288.2814s\n",
      "\titers: 900, epoch: 7 | loss: 0.0849210\n",
      "\tspeed: 0.0972s/iter; left time: 278.3587s\n",
      "Epoch: 7 running time: 1.5293039798736572 min.\n",
      "Epoch: 7, Steps: 941 | Train Loss: 0.1316744 Vali Loss: 0.1737460 Test Loss: 0.2176384\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1227411\n",
      "\tspeed: 0.2788s/iter; left time: 759.5824s\n",
      "\titers: 200, epoch: 8 | loss: 0.0986009\n",
      "\tspeed: 0.0974s/iter; left time: 255.6203s\n",
      "\titers: 300, epoch: 8 | loss: 0.1985808\n",
      "\tspeed: 0.0973s/iter; left time: 245.5741s\n",
      "\titers: 400, epoch: 8 | loss: 0.1137289\n",
      "\tspeed: 0.0971s/iter; left time: 235.3070s\n",
      "\titers: 500, epoch: 8 | loss: 0.1218776\n",
      "\tspeed: 0.0976s/iter; left time: 226.8412s\n",
      "\titers: 600, epoch: 8 | loss: 0.1038841\n",
      "\tspeed: 0.0913s/iter; left time: 203.0739s\n",
      "\titers: 700, epoch: 8 | loss: 0.1212349\n",
      "\tspeed: 0.0981s/iter; left time: 208.3269s\n",
      "\titers: 800, epoch: 8 | loss: 0.1504612\n",
      "\tspeed: 0.0973s/iter; left time: 196.9717s\n",
      "\titers: 900, epoch: 8 | loss: 0.1470879\n",
      "\tspeed: 0.0867s/iter; left time: 166.8361s\n",
      "Epoch: 8 running time: 1.4944848974545797 min.\n",
      "Epoch: 8, Steps: 941 | Train Loss: 0.1319000 Vali Loss: 0.1725450 Test Loss: 0.2144987\n",
      "Validation loss decreased (0.173503 --> 0.172545).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1202987\n",
      "\tspeed: 0.2791s/iter; left time: 497.6350s\n",
      "\titers: 200, epoch: 9 | loss: 0.1174619\n",
      "\tspeed: 0.0970s/iter; left time: 163.3099s\n",
      "\titers: 300, epoch: 9 | loss: 0.1188463\n",
      "\tspeed: 0.0971s/iter; left time: 153.6958s\n",
      "\titers: 400, epoch: 9 | loss: 0.1098071\n",
      "\tspeed: 0.0971s/iter; left time: 144.0006s\n",
      "\titers: 500, epoch: 9 | loss: 0.1158853\n",
      "\tspeed: 0.0970s/iter; left time: 134.2170s\n",
      "\titers: 600, epoch: 9 | loss: 0.1356356\n",
      "\tspeed: 0.0969s/iter; left time: 124.3808s\n",
      "\titers: 700, epoch: 9 | loss: 0.1347722\n",
      "\tspeed: 0.0970s/iter; left time: 114.7636s\n",
      "\titers: 800, epoch: 9 | loss: 0.1334302\n",
      "\tspeed: 0.0831s/iter; left time: 90.0144s\n",
      "\titers: 900, epoch: 9 | loss: 0.0995047\n",
      "\tspeed: 0.0809s/iter; left time: 79.5341s\n",
      "Epoch: 9 running time: 1.4661731481552125 min.\n",
      "Epoch: 9, Steps: 941 | Train Loss: 0.1301715 Vali Loss: 0.1736400 Test Loss: 0.2173969\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1325151\n",
      "\tspeed: 0.2705s/iter; left time: 227.7331s\n",
      "\titers: 200, epoch: 10 | loss: 0.1358923\n",
      "\tspeed: 0.0972s/iter; left time: 72.0900s\n",
      "\titers: 300, epoch: 10 | loss: 0.1808116\n",
      "\tspeed: 0.0972s/iter; left time: 62.4191s\n",
      "\titers: 400, epoch: 10 | loss: 0.1246796\n",
      "\tspeed: 0.0974s/iter; left time: 52.7918s\n",
      "\titers: 500, epoch: 10 | loss: 0.1443893\n",
      "\tspeed: 0.0973s/iter; left time: 43.0127s\n",
      "\titers: 600, epoch: 10 | loss: 0.0978366\n",
      "\tspeed: 0.0976s/iter; left time: 33.3884s\n",
      "\titers: 700, epoch: 10 | loss: 0.1169745\n",
      "\tspeed: 0.0977s/iter; left time: 23.6415s\n",
      "\titers: 800, epoch: 10 | loss: 0.1286295\n",
      "\tspeed: 0.0969s/iter; left time: 13.7637s\n",
      "\titers: 900, epoch: 10 | loss: 0.1493730\n",
      "\tspeed: 0.0970s/iter; left time: 4.0732s\n",
      "Epoch: 10 running time: 1.5308138728141785 min.\n",
      "Epoch: 10, Steps: 941 | Train Loss: 0.1302067 Vali Loss: 0.1725865 Test Loss: 0.2167616\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__24_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl5_df2048_fc5_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6457\n",
      "mse:0.2147572636604309, mae:0.28201374411582947\n",
      "\n",
      "Time intermediate for IT dataset: 31.76006961663564 min.\n",
      "Total time: 116.03314847151438 min.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    # Write model output into txt file\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "140/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "datasets = ['FR_data.csv']\n",
    "num_cols = [\"3\"]\n",
    "pred_len = \"24\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", current_path,\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", model_id,\n",
    "                \"--model\", model,\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "    int_start = time.time()\n",
    "\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    #folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp='GB_data_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vol/cs-hu/riabchuv/hu-home/my_work/datasets/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GB_UKM_load_actual_entsoe_transparency</th>\n",
       "      <th>GB_UKM_solar_generation_actual</th>\n",
       "      <th>GB_UKM_wind_generation_actual</th>\n",
       "      <th>GB_UKM_wind_offshore_generation_actual</th>\n",
       "      <th>GB_UKM_wind_onshore_generation_actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-10-26 00:00:00</th>\n",
       "      <td>30680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>3463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 01:00:00</th>\n",
       "      <td>29218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5194.0</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>3383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 02:00:00</th>\n",
       "      <td>28016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4389.0</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>2633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 03:00:00</th>\n",
       "      <td>27402.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5104.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>3417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26 04:00:00</th>\n",
       "      <td>27490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5206.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>3456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 19:00:00</th>\n",
       "      <td>49595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 20:00:00</th>\n",
       "      <td>46550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 21:00:00</th>\n",
       "      <td>42752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 22:00:00</th>\n",
       "      <td>38984.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-04 23:00:00</th>\n",
       "      <td>34184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     GB_UKM_load_actual_entsoe_transparency  \\\n",
       "date                                                          \n",
       "2015-10-26 00:00:00                                 30680.0   \n",
       "2015-10-26 01:00:00                                 29218.0   \n",
       "2015-10-26 02:00:00                                 28016.0   \n",
       "2015-10-26 03:00:00                                 27402.0   \n",
       "2015-10-26 04:00:00                                 27490.0   \n",
       "...                                                     ...   \n",
       "2015-11-04 19:00:00                                 49595.0   \n",
       "2015-11-04 20:00:00                                 46550.0   \n",
       "2015-11-04 21:00:00                                 42752.0   \n",
       "2015-11-04 22:00:00                                 38984.0   \n",
       "2015-11-04 23:00:00                                 34184.0   \n",
       "\n",
       "                     GB_UKM_solar_generation_actual  \\\n",
       "date                                                  \n",
       "2015-10-26 00:00:00                             0.0   \n",
       "2015-10-26 01:00:00                             0.0   \n",
       "2015-10-26 02:00:00                             0.0   \n",
       "2015-10-26 03:00:00                             0.0   \n",
       "2015-10-26 04:00:00                             0.0   \n",
       "...                                             ...   \n",
       "2015-11-04 19:00:00                             0.0   \n",
       "2015-11-04 20:00:00                             0.0   \n",
       "2015-11-04 21:00:00                             0.0   \n",
       "2015-11-04 22:00:00                             0.0   \n",
       "2015-11-04 23:00:00                             0.0   \n",
       "\n",
       "                     GB_UKM_wind_generation_actual  \\\n",
       "date                                                 \n",
       "2015-10-26 00:00:00                         5348.0   \n",
       "2015-10-26 01:00:00                         5194.0   \n",
       "2015-10-26 02:00:00                         4389.0   \n",
       "2015-10-26 03:00:00                         5104.0   \n",
       "2015-10-26 04:00:00                         5206.0   \n",
       "...                                            ...   \n",
       "2015-11-04 19:00:00                         1012.0   \n",
       "2015-11-04 20:00:00                         1246.0   \n",
       "2015-11-04 21:00:00                         1441.0   \n",
       "2015-11-04 22:00:00                         1430.0   \n",
       "2015-11-04 23:00:00                         1400.0   \n",
       "\n",
       "                     GB_UKM_wind_offshore_generation_actual  \\\n",
       "date                                                          \n",
       "2015-10-26 00:00:00                                  1885.0   \n",
       "2015-10-26 01:00:00                                  1810.0   \n",
       "2015-10-26 02:00:00                                  1756.0   \n",
       "2015-10-26 03:00:00                                  1687.0   \n",
       "2015-10-26 04:00:00                                  1749.0   \n",
       "...                                                     ...   \n",
       "2015-11-04 19:00:00                                   584.0   \n",
       "2015-11-04 20:00:00                                   792.0   \n",
       "2015-11-04 21:00:00                                   915.0   \n",
       "2015-11-04 22:00:00                                   912.0   \n",
       "2015-11-04 23:00:00                                   875.0   \n",
       "\n",
       "                     GB_UKM_wind_onshore_generation_actual  \n",
       "date                                                        \n",
       "2015-10-26 00:00:00                                 3463.0  \n",
       "2015-10-26 01:00:00                                 3383.0  \n",
       "2015-10-26 02:00:00                                 2633.0  \n",
       "2015-10-26 03:00:00                                 3417.0  \n",
       "2015-10-26 04:00:00                                 3456.0  \n",
       "...                                                    ...  \n",
       "2015-11-04 19:00:00                                  429.0  \n",
       "2015-11-04 20:00:00                                  455.0  \n",
       "2015-11-04 21:00:00                                  526.0  \n",
       "2015-11-04 22:00:00                                  518.0  \n",
       "2015-11-04 23:00:00                                  525.0  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(current_path, dp), index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.039925122261047365 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0024538 Vali Loss: nan Test Loss: 1.2172809\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.01469890276590983 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6911948 Vali Loss: nan Test Loss: 1.4431949\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.014795935153961182 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5788186 Vali Loss: nan Test Loss: 1.0701528\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.015081985791524252 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5235551 Vali Loss: nan Test Loss: 0.9722701\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.014554961522420248 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.5137975 Vali Loss: nan Test Loss: 0.9586014\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.01539984146753947 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4784191 Vali Loss: nan Test Loss: 0.9611190\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.013930594921112061 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4814504 Vali Loss: nan Test Loss: 1.0199240\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014625775814056396 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4706157 Vali Loss: nan Test Loss: 0.9661508\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.015251608689626057 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4747935 Vali Loss: nan Test Loss: 0.9857954\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014180195331573487 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4762641 Vali Loss: nan Test Loss: 1.0232698\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.985942542552948, mae:0.7991647124290466\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.014746411641438802 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0295902 Vali Loss: nan Test Loss: 1.0363106\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.015388592084248861 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6777777 Vali Loss: nan Test Loss: 1.3939192\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.014550344149271647 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5698880 Vali Loss: nan Test Loss: 0.9854409\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.014660712083180745 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5290706 Vali Loss: nan Test Loss: 0.8179453\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.015316299597422282 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.4899310 Vali Loss: nan Test Loss: 0.8191302\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.014206993579864501 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4454456 Vali Loss: nan Test Loss: 0.8206318\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.014676713943481445 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4726178 Vali Loss: nan Test Loss: 0.8455051\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014798065026601156 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4536124 Vali Loss: nan Test Loss: 0.8689371\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014613501230875651 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4576709 Vali Loss: nan Test Loss: 0.9006981\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014256656169891357 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4339121 Vali Loss: nan Test Loss: 0.8823724\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.8921732306480408, mae:0.7604851722717285\n",
      "\n",
      "Time intermediate for GB dataset: 0.9230153759320577 min.\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.0379722277323405 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0133409 Vali Loss: nan Test Loss: 1.1593016\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.014988048871358236 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6685318 Vali Loss: nan Test Loss: 1.5033898\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.015134803454081218 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5853137 Vali Loss: nan Test Loss: 1.0170285\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.015229304631551107 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5230797 Vali Loss: nan Test Loss: 0.9165366\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.01482845942179362 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.5061147 Vali Loss: nan Test Loss: 0.9382831\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.014434333642323811 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4742088 Vali Loss: nan Test Loss: 0.9203134\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.014867182572682698 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4816395 Vali Loss: nan Test Loss: 0.9207461\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014206230640411377 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4701661 Vali Loss: nan Test Loss: 0.9398845\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014715186754862468 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4746773 Vali Loss: nan Test Loss: 0.9466402\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.014221092065175375 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4755139 Vali Loss: nan Test Loss: 0.9503284\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.9485630989074707, mae:0.7849948406219482\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 running time: 0.014317051569620768 min.\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0192896 Vali Loss: nan Test Loss: 1.0006683\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 running time: 0.016005825996398926 min.\n",
      "Epoch: 2, Steps: 4 | Train Loss: 0.6856293 Vali Loss: nan Test Loss: 1.3917358\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 running time: 0.015333712100982666 min.\n",
      "Epoch: 3, Steps: 4 | Train Loss: 0.5695837 Vali Loss: nan Test Loss: 1.0196233\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 running time: 0.014633981386820476 min.\n",
      "Epoch: 4, Steps: 4 | Train Loss: 0.5196934 Vali Loss: nan Test Loss: 0.8472352\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 running time: 0.015336012840270996 min.\n",
      "Epoch: 5, Steps: 4 | Train Loss: 0.4939889 Vali Loss: nan Test Loss: 0.8318195\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 running time: 0.01478735605875651 min.\n",
      "Epoch: 6, Steps: 4 | Train Loss: 0.4583812 Vali Loss: nan Test Loss: 0.8314679\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 running time: 0.01449210246404012 min.\n",
      "Epoch: 7, Steps: 4 | Train Loss: 0.4641255 Vali Loss: nan Test Loss: 0.8144280\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 running time: 0.014517529805501302 min.\n",
      "Epoch: 8, Steps: 4 | Train Loss: 0.4530064 Vali Loss: nan Test Loss: 0.8637968\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 running time: 0.014745950698852539 min.\n",
      "Epoch: 9, Steps: 4 | Train Loss: 0.4684971 Vali Loss: nan Test Loss: 0.8901826\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 running time: 0.015699354807535808 min.\n",
      "Epoch: 10, Steps: 4 | Train Loss: 0.4408530 Vali Loss: nan Test Loss: 0.9042748\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast__10_GB_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (1, 32, 10, 5) (1, 32, 10, 5)\n",
      "test shape: (32, 10, 5) (32, 10, 5)\n",
      "mse:0.944560706615448, mae:0.7830029129981995\n",
      "\n",
      "Time intermediate for GB dataset: 0.935037616888682 min.\n",
      "Total time: 1.8580724199612935 min.\n"
     ]
    }
   ],
   "source": [
    "# 5.413828869660695 min. without test\n",
    "# 6.30 min with test\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "datasets = ['GB_data_small.csv', 'GB_data_small.csv']\n",
    "num_cols = [\"5\", \"5\"]\n",
    "pred_len = \"10\"\n",
    "model = \"Informer\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "            \"--task_name\", \"long_term_forecast\",\n",
    "            \"--is_training\", \"1\", #True\n",
    "            \"--root_path\", current_path, #\"/content/my_work/datasets/\",\n",
    "            \"--data_path\", dataset,\n",
    "            #\"--train_epochs\", \"1\",\n",
    "            \"--model_id\", model_id,\n",
    "            \"--model\", model,\n",
    "            \"--data\", \"custom\", # This ensures a 70%,10%,20% train,val,test split see data_provider/data_loader.py\n",
    "            \"--features\", \"M\", # Multivariate\n",
    "            \"--seq_len\", \"10\",\n",
    "            \"--label_len\", \"5\",\n",
    "            \"--pred_len\", pred_len,\n",
    "            \"--e_layers\", \"3\", # Hyperparameters as in original model\n",
    "            \"--d_layers\", \"2\",\n",
    "            \"--factor\", \"3\",\n",
    "            #\"--gpu\", gpu_index,\n",
    "            # '--use_multi_gpu', \n",
    "            \"--enc_in\", num_cols[i],\n",
    "            \"--dec_in\", num_cols[i],\n",
    "            \"--c_out\", num_cols[i],\n",
    "            \"--des\", \"Exp\",\n",
    "            \"--itr\", \"2\",\n",
    "        ]\n",
    "    int_start = time.time()\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "\n",
    "    # folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.86847895,   1.1777946 ,   1.0852624 ,   3.2318454 ,\n",
       "       769.5208    ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics\n",
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/metrics.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 10, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds\n",
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/pred.npy\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
