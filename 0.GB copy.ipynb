{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['GB']\n",
    "num_cols = [5]\n",
    "seq_lens = [512]\n",
    "\n",
    "model = \"PatchTST\"\n",
    "loss = \"MSE\"\n",
    "itr=1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_GB.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0259924\n",
      "\tspeed: 0.0729s/iter; left time: 1617.5844s\n",
      "\titers: 200, epoch: 1 | loss: 0.0246011\n",
      "\tspeed: 0.0468s/iter; left time: 1034.4643s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0283259 Vali Loss: 0.0284845 Test Loss: 0.0390218\n",
      "Validation loss decreased (inf --> 0.028484).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0159642\n",
      "\tspeed: 0.0876s/iter; left time: 1925.1383s\n",
      "\titers: 200, epoch: 2 | loss: 0.0148842\n",
      "\tspeed: 0.0471s/iter; left time: 1031.1201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0157466 Vali Loss: 0.0198990 Test Loss: 0.0255800\n",
      "Validation loss decreased (0.028484 --> 0.019899).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0146333\n",
      "\tspeed: 0.0876s/iter; left time: 1906.4000s\n",
      "\titers: 200, epoch: 3 | loss: 0.0138821\n",
      "\tspeed: 0.0464s/iter; left time: 1004.7402s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.70s\n",
      "Steps: 223 | Train Loss: 0.0139770 Vali Loss: 0.0191788 Test Loss: 0.0251260\n",
      "Validation loss decreased (0.019899 --> 0.019179).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0149807\n",
      "\tspeed: 0.0840s/iter; left time: 1809.5605s\n",
      "\titers: 200, epoch: 4 | loss: 0.0159410\n",
      "\tspeed: 0.0470s/iter; left time: 1007.1777s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.74s\n",
      "Steps: 223 | Train Loss: 0.0136111 Vali Loss: 0.0193769 Test Loss: 0.0251419\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0124624\n",
      "\tspeed: 0.0875s/iter; left time: 1864.0136s\n",
      "\titers: 200, epoch: 5 | loss: 0.0124338\n",
      "\tspeed: 0.0473s/iter; left time: 1002.4096s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0133677 Vali Loss: 0.0192061 Test Loss: 0.0252979\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0119990\n",
      "\tspeed: 0.0871s/iter; left time: 1837.2484s\n",
      "\titers: 200, epoch: 6 | loss: 0.0148567\n",
      "\tspeed: 0.0467s/iter; left time: 980.0954s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 223 | Train Loss: 0.0131876 Vali Loss: 0.0190067 Test Loss: 0.0248291\n",
      "Validation loss decreased (0.019179 --> 0.019007).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0132022\n",
      "\tspeed: 0.0884s/iter; left time: 1843.5593s\n",
      "\titers: 200, epoch: 7 | loss: 0.0121673\n",
      "\tspeed: 0.0473s/iter; left time: 981.6510s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0130462 Vali Loss: 0.0191168 Test Loss: 0.0251083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0140444\n",
      "\tspeed: 0.0877s/iter; left time: 1809.9073s\n",
      "\titers: 200, epoch: 8 | loss: 0.0126242\n",
      "\tspeed: 0.0474s/iter; left time: 973.6919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 223 | Train Loss: 0.0129650 Vali Loss: 0.0188185 Test Loss: 0.0248416\n",
      "Validation loss decreased (0.019007 --> 0.018819).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0131284\n",
      "\tspeed: 0.0858s/iter; left time: 1752.6386s\n",
      "\titers: 200, epoch: 9 | loss: 0.0119249\n",
      "\tspeed: 0.0470s/iter; left time: 954.2785s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.78s\n",
      "Steps: 223 | Train Loss: 0.0128425 Vali Loss: 0.0188309 Test Loss: 0.0245018\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0121159\n",
      "\tspeed: 0.0883s/iter; left time: 1782.6647s\n",
      "\titers: 200, epoch: 10 | loss: 0.0139951\n",
      "\tspeed: 0.0476s/iter; left time: 956.6477s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 223 | Train Loss: 0.0127500 Vali Loss: 0.0188925 Test Loss: 0.0247047\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0142653\n",
      "\tspeed: 0.0870s/iter; left time: 1738.2746s\n",
      "\titers: 200, epoch: 11 | loss: 0.0131171\n",
      "\tspeed: 0.0476s/iter; left time: 946.1610s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0126456 Vali Loss: 0.0187767 Test Loss: 0.0250738\n",
      "Validation loss decreased (0.018819 --> 0.018777).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0104647\n",
      "\tspeed: 0.0855s/iter; left time: 1688.5784s\n",
      "\titers: 200, epoch: 12 | loss: 0.0143763\n",
      "\tspeed: 0.0476s/iter; left time: 935.9378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0125827 Vali Loss: 0.0188292 Test Loss: 0.0245414\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0120354\n",
      "\tspeed: 0.0843s/iter; left time: 1645.7883s\n",
      "\titers: 200, epoch: 13 | loss: 0.0132992\n",
      "\tspeed: 0.0479s/iter; left time: 931.1112s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 223 | Train Loss: 0.0125262 Vali Loss: 0.0188340 Test Loss: 0.0245886\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0122780\n",
      "\tspeed: 0.0879s/iter; left time: 1697.3040s\n",
      "\titers: 200, epoch: 14 | loss: 0.0123523\n",
      "\tspeed: 0.0473s/iter; left time: 908.7489s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0124646 Vali Loss: 0.0188329 Test Loss: 0.0246720\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0123033\n",
      "\tspeed: 0.0857s/iter; left time: 1635.1172s\n",
      "\titers: 200, epoch: 15 | loss: 0.0120831\n",
      "\tspeed: 0.0476s/iter; left time: 903.8159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0124087 Vali Loss: 0.0187291 Test Loss: 0.0243788\n",
      "Validation loss decreased (0.018777 --> 0.018729).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0116331\n",
      "\tspeed: 0.0883s/iter; left time: 1664.8124s\n",
      "\titers: 200, epoch: 16 | loss: 0.0112471\n",
      "\tspeed: 0.0480s/iter; left time: 899.4691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.01s\n",
      "Steps: 223 | Train Loss: 0.0123589 Vali Loss: 0.0187595 Test Loss: 0.0244636\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0136333\n",
      "\tspeed: 0.0870s/iter; left time: 1620.8749s\n",
      "\titers: 200, epoch: 17 | loss: 0.0123088\n",
      "\tspeed: 0.0471s/iter; left time: 872.4565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.83s\n",
      "Steps: 223 | Train Loss: 0.0123212 Vali Loss: 0.0186955 Test Loss: 0.0244878\n",
      "Validation loss decreased (0.018729 --> 0.018696).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0125058\n",
      "\tspeed: 0.0836s/iter; left time: 1539.5299s\n",
      "\titers: 200, epoch: 18 | loss: 0.0120818\n",
      "\tspeed: 0.0471s/iter; left time: 861.9594s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.71s\n",
      "Steps: 223 | Train Loss: 0.0122789 Vali Loss: 0.0187429 Test Loss: 0.0246242\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0123783\n",
      "\tspeed: 0.0844s/iter; left time: 1534.5664s\n",
      "\titers: 200, epoch: 19 | loss: 0.0137483\n",
      "\tspeed: 0.0477s/iter; left time: 863.3255s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0122455 Vali Loss: 0.0188140 Test Loss: 0.0245240\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0122385\n",
      "\tspeed: 0.0871s/iter; left time: 1565.1836s\n",
      "\titers: 200, epoch: 20 | loss: 0.0100243\n",
      "\tspeed: 0.0469s/iter; left time: 838.2041s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.81s\n",
      "Steps: 223 | Train Loss: 0.0121935 Vali Loss: 0.0188129 Test Loss: 0.0245216\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0109536\n",
      "\tspeed: 0.0846s/iter; left time: 1500.9527s\n",
      "\titers: 200, epoch: 21 | loss: 0.0136657\n",
      "\tspeed: 0.0478s/iter; left time: 842.6052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 223 | Train Loss: 0.0121563 Vali Loss: 0.0187994 Test Loss: 0.0245719\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0120630\n",
      "\tspeed: 0.0878s/iter; left time: 1537.9847s\n",
      "\titers: 200, epoch: 22 | loss: 0.0122324\n",
      "\tspeed: 0.0478s/iter; left time: 832.7527s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0121276 Vali Loss: 0.0187267 Test Loss: 0.0245971\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0132323\n",
      "\tspeed: 0.0874s/iter; left time: 1511.4554s\n",
      "\titers: 200, epoch: 23 | loss: 0.0106442\n",
      "\tspeed: 0.0470s/iter; left time: 807.7792s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 223 | Train Loss: 0.0121155 Vali Loss: 0.0187053 Test Loss: 0.0245364\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0125100\n",
      "\tspeed: 0.0859s/iter; left time: 1466.0375s\n",
      "\titers: 200, epoch: 24 | loss: 0.0125616\n",
      "\tspeed: 0.0480s/iter; left time: 815.0888s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.00s\n",
      "Steps: 223 | Train Loss: 0.0120919 Vali Loss: 0.0187602 Test Loss: 0.0246057\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0128259\n",
      "\tspeed: 0.0878s/iter; left time: 1478.5698s\n",
      "\titers: 200, epoch: 25 | loss: 0.0117420\n",
      "\tspeed: 0.0477s/iter; left time: 798.7245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 223 | Train Loss: 0.0120816 Vali Loss: 0.0188265 Test Loss: 0.0245693\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0112484\n",
      "\tspeed: 0.0879s/iter; left time: 1460.7121s\n",
      "\titers: 200, epoch: 26 | loss: 0.0115009\n",
      "\tspeed: 0.0475s/iter; left time: 784.5309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0120537 Vali Loss: 0.0188555 Test Loss: 0.0245836\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0128660\n",
      "\tspeed: 0.0868s/iter; left time: 1423.1087s\n",
      "\titers: 200, epoch: 27 | loss: 0.0128459\n",
      "\tspeed: 0.0478s/iter; left time: 779.5444s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0120271 Vali Loss: 0.0187563 Test Loss: 0.0245329\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.024487832561135292, rmse:0.1564858853816986, mae:0.1026926264166832, rse:0.5398319363594055\n",
      "Intermediate time for GB and pred_len 24: 00h:06m:12.81s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0310066\n",
      "\tspeed: 0.0755s/iter; left time: 1667.7171s\n",
      "\titers: 200, epoch: 1 | loss: 0.0256376\n",
      "\tspeed: 0.0471s/iter; left time: 1036.0691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 222 | Train Loss: 0.0307500 Vali Loss: 0.0325461 Test Loss: 0.0466486\n",
      "Validation loss decreased (inf --> 0.032546).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0226989\n",
      "\tspeed: 0.0897s/iter; left time: 1961.8102s\n",
      "\titers: 200, epoch: 2 | loss: 0.0240538\n",
      "\tspeed: 0.0480s/iter; left time: 1044.4161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.99s\n",
      "Steps: 222 | Train Loss: 0.0228539 Vali Loss: 0.0286221 Test Loss: 0.0401569\n",
      "Validation loss decreased (0.032546 --> 0.028622).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0210763\n",
      "\tspeed: 0.0894s/iter; left time: 1935.1764s\n",
      "\titers: 200, epoch: 3 | loss: 0.0222665\n",
      "\tspeed: 0.0479s/iter; left time: 1032.9880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0213867 Vali Loss: 0.0285050 Test Loss: 0.0408070\n",
      "Validation loss decreased (0.028622 --> 0.028505).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0192114\n",
      "\tspeed: 0.0914s/iter; left time: 1958.9105s\n",
      "\titers: 200, epoch: 4 | loss: 0.0215106\n",
      "\tspeed: 0.0482s/iter; left time: 1027.2887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 222 | Train Loss: 0.0209945 Vali Loss: 0.0283547 Test Loss: 0.0408883\n",
      "Validation loss decreased (0.028505 --> 0.028355).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0211985\n",
      "\tspeed: 0.0901s/iter; left time: 1910.8924s\n",
      "\titers: 200, epoch: 5 | loss: 0.0218957\n",
      "\tspeed: 0.0476s/iter; left time: 1004.0915s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 222 | Train Loss: 0.0206841 Vali Loss: 0.0281620 Test Loss: 0.0400784\n",
      "Validation loss decreased (0.028355 --> 0.028162).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0197395\n",
      "\tspeed: 0.0908s/iter; left time: 1905.4511s\n",
      "\titers: 200, epoch: 6 | loss: 0.0203199\n",
      "\tspeed: 0.0478s/iter; left time: 999.2435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 222 | Train Loss: 0.0204184 Vali Loss: 0.0282535 Test Loss: 0.0406600\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0193540\n",
      "\tspeed: 0.0892s/iter; left time: 1853.3197s\n",
      "\titers: 200, epoch: 7 | loss: 0.0196574\n",
      "\tspeed: 0.0472s/iter; left time: 975.2716s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.78s\n",
      "Steps: 222 | Train Loss: 0.0201654 Vali Loss: 0.0283629 Test Loss: 0.0403851\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0203656\n",
      "\tspeed: 0.0849s/iter; left time: 1743.4764s\n",
      "\titers: 200, epoch: 8 | loss: 0.0195816\n",
      "\tspeed: 0.0473s/iter; left time: 967.1436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.73s\n",
      "Steps: 222 | Train Loss: 0.0199174 Vali Loss: 0.0284603 Test Loss: 0.0404209\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0188830\n",
      "\tspeed: 0.0837s/iter; left time: 1701.0425s\n",
      "\titers: 200, epoch: 9 | loss: 0.0194778\n",
      "\tspeed: 0.0477s/iter; left time: 963.9741s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 222 | Train Loss: 0.0196882 Vali Loss: 0.0284817 Test Loss: 0.0405771\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0192994\n",
      "\tspeed: 0.0836s/iter; left time: 1680.2925s\n",
      "\titers: 200, epoch: 10 | loss: 0.0190058\n",
      "\tspeed: 0.0474s/iter; left time: 949.0071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.75s\n",
      "Steps: 222 | Train Loss: 0.0194750 Vali Loss: 0.0283545 Test Loss: 0.0410616\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0189386\n",
      "\tspeed: 0.0877s/iter; left time: 1744.2445s\n",
      "\titers: 200, epoch: 11 | loss: 0.0185040\n",
      "\tspeed: 0.0478s/iter; left time: 945.7971s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 222 | Train Loss: 0.0192353 Vali Loss: 0.0286885 Test Loss: 0.0414736\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0179484\n",
      "\tspeed: 0.0890s/iter; left time: 1750.5034s\n",
      "\titers: 200, epoch: 12 | loss: 0.0180923\n",
      "\tspeed: 0.0474s/iter; left time: 928.0717s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 222 | Train Loss: 0.0190230 Vali Loss: 0.0285809 Test Loss: 0.0414058\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0176465\n",
      "\tspeed: 0.0841s/iter; left time: 1633.8161s\n",
      "\titers: 200, epoch: 13 | loss: 0.0193337\n",
      "\tspeed: 0.0481s/iter; left time: 929.5026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 222 | Train Loss: 0.0188276 Vali Loss: 0.0286796 Test Loss: 0.0423112\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0188346\n",
      "\tspeed: 0.0906s/iter; left time: 1740.8810s\n",
      "\titers: 200, epoch: 14 | loss: 0.0186280\n",
      "\tspeed: 0.0482s/iter; left time: 922.2026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 222 | Train Loss: 0.0186396 Vali Loss: 0.0289282 Test Loss: 0.0422525\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0203469\n",
      "\tspeed: 0.0890s/iter; left time: 1689.8485s\n",
      "\titers: 200, epoch: 15 | loss: 0.0194815\n",
      "\tspeed: 0.0475s/iter; left time: 898.0595s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 222 | Train Loss: 0.0184927 Vali Loss: 0.0289733 Test Loss: 0.0426640\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04007839038968086, rmse:0.20019587874412537, mae:0.14094528555870056, rse:0.6923052072525024\n",
      "Intermediate time for GB and pred_len 96: 00h:03m:34.91s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0290644\n",
      "\tspeed: 0.0781s/iter; left time: 1726.2218s\n",
      "\titers: 200, epoch: 1 | loss: 0.0283159\n",
      "\tspeed: 0.0479s/iter; left time: 1052.8035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.42s\n",
      "Steps: 222 | Train Loss: 0.0315331 Vali Loss: 0.0335719 Test Loss: 0.0481259\n",
      "Validation loss decreased (inf --> 0.033572).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0229541\n",
      "\tspeed: 0.0932s/iter; left time: 2038.9862s\n",
      "\titers: 200, epoch: 2 | loss: 0.0229998\n",
      "\tspeed: 0.0482s/iter; left time: 1048.6904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 222 | Train Loss: 0.0243021 Vali Loss: 0.0301776 Test Loss: 0.0425740\n",
      "Validation loss decreased (0.033572 --> 0.030178).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0232384\n",
      "\tspeed: 0.0926s/iter; left time: 2006.4145s\n",
      "\titers: 200, epoch: 3 | loss: 0.0242960\n",
      "\tspeed: 0.0484s/iter; left time: 1043.8592s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 222 | Train Loss: 0.0228905 Vali Loss: 0.0299546 Test Loss: 0.0426070\n",
      "Validation loss decreased (0.030178 --> 0.029955).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0240797\n",
      "\tspeed: 0.0937s/iter; left time: 2008.1709s\n",
      "\titers: 200, epoch: 4 | loss: 0.0210708\n",
      "\tspeed: 0.0482s/iter; left time: 1028.6801s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.07s\n",
      "Steps: 222 | Train Loss: 0.0224280 Vali Loss: 0.0298746 Test Loss: 0.0425166\n",
      "Validation loss decreased (0.029955 --> 0.029875).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0218961\n",
      "\tspeed: 0.0928s/iter; left time: 1968.6502s\n",
      "\titers: 200, epoch: 5 | loss: 0.0230778\n",
      "\tspeed: 0.0484s/iter; left time: 1021.7747s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.12s\n",
      "Steps: 222 | Train Loss: 0.0220308 Vali Loss: 0.0301946 Test Loss: 0.0431376\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0205828\n",
      "\tspeed: 0.0926s/iter; left time: 1942.9965s\n",
      "\titers: 200, epoch: 6 | loss: 0.0217106\n",
      "\tspeed: 0.0482s/iter; left time: 1006.9238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.07s\n",
      "Steps: 222 | Train Loss: 0.0216309 Vali Loss: 0.0304557 Test Loss: 0.0433500\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0208696\n",
      "\tspeed: 0.0919s/iter; left time: 1908.8342s\n",
      "\titers: 200, epoch: 7 | loss: 0.0206528\n",
      "\tspeed: 0.0483s/iter; left time: 998.6409s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.08s\n",
      "Steps: 222 | Train Loss: 0.0212453 Vali Loss: 0.0307189 Test Loss: 0.0444726\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0202371\n",
      "\tspeed: 0.0914s/iter; left time: 1877.9257s\n",
      "\titers: 200, epoch: 8 | loss: 0.0194056\n",
      "\tspeed: 0.0486s/iter; left time: 993.6685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 222 | Train Loss: 0.0208566 Vali Loss: 0.0309050 Test Loss: 0.0446516\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0206947\n",
      "\tspeed: 0.0908s/iter; left time: 1846.2679s\n",
      "\titers: 200, epoch: 9 | loss: 0.0204771\n",
      "\tspeed: 0.0484s/iter; left time: 978.4904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.03s\n",
      "Steps: 222 | Train Loss: 0.0205095 Vali Loss: 0.0311708 Test Loss: 0.0440427\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0200099\n",
      "\tspeed: 0.0917s/iter; left time: 1843.5050s\n",
      "\titers: 200, epoch: 10 | loss: 0.0207653\n",
      "\tspeed: 0.0482s/iter; left time: 963.2984s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 222 | Train Loss: 0.0201604 Vali Loss: 0.0313169 Test Loss: 0.0443286\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209964\n",
      "\tspeed: 0.0926s/iter; left time: 1840.1423s\n",
      "\titers: 200, epoch: 11 | loss: 0.0193575\n",
      "\tspeed: 0.0488s/iter; left time: 964.4115s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.12s\n",
      "Steps: 222 | Train Loss: 0.0197860 Vali Loss: 0.0315444 Test Loss: 0.0445627\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0209620\n",
      "\tspeed: 0.0917s/iter; left time: 1803.1563s\n",
      "\titers: 200, epoch: 12 | loss: 0.0191670\n",
      "\tspeed: 0.0485s/iter; left time: 948.8995s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 222 | Train Loss: 0.0194906 Vali Loss: 0.0318858 Test Loss: 0.0450109\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0183557\n",
      "\tspeed: 0.0917s/iter; left time: 1781.6379s\n",
      "\titers: 200, epoch: 13 | loss: 0.0196181\n",
      "\tspeed: 0.0483s/iter; left time: 933.3383s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 222 | Train Loss: 0.0192031 Vali Loss: 0.0319863 Test Loss: 0.0463496\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0176900\n",
      "\tspeed: 0.0927s/iter; left time: 1780.3855s\n",
      "\titers: 200, epoch: 14 | loss: 0.0185688\n",
      "\tspeed: 0.0484s/iter; left time: 925.5769s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.13s\n",
      "Steps: 222 | Train Loss: 0.0189770 Vali Loss: 0.0320392 Test Loss: 0.0460689\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04251662269234657, rmse:0.20619559288024902, mae:0.14658014476299286, rse:0.7149097323417664\n",
      "Intermediate time for GB and pred_len 168: 00h:03m:29.82s\n",
      "Intermediate time for GB: 00h:13m:17.54s\n",
      "Total time: 00h:13m:17.54s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"GB_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.1466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "GB      24        0.0245  0.1565  0.1027\n",
       "        96        0.0401  0.2002  0.1409\n",
       "        168       0.0425  0.2062  0.1466"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False).round(4)\n",
    "patchtst_df.to_csv(os.path.join(path, 'GB_patchtst_512.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No RevIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_no_revin_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1296732\n",
      "\tspeed: 0.0740s/iter; left time: 1642.5872s\n",
      "\titers: 200, epoch: 1 | loss: 0.1225555\n",
      "\tspeed: 0.0473s/iter; left time: 1045.5261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.18s\n",
      "Steps: 223 | Train Loss: 0.1316660 Vali Loss: 0.0944922 Test Loss: 0.1096129\n",
      "Validation loss decreased (inf --> 0.094492).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0377319\n",
      "\tspeed: 0.0875s/iter; left time: 1922.6678s\n",
      "\titers: 200, epoch: 2 | loss: 0.0261822\n",
      "\tspeed: 0.0474s/iter; left time: 1037.0308s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0454864 Vali Loss: 0.0248178 Test Loss: 0.0346409\n",
      "Validation loss decreased (0.094492 --> 0.024818).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0243949\n",
      "\tspeed: 0.0876s/iter; left time: 1905.5515s\n",
      "\titers: 200, epoch: 3 | loss: 0.0203790\n",
      "\tspeed: 0.0471s/iter; left time: 1019.0757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.82s\n",
      "Steps: 223 | Train Loss: 0.0222196 Vali Loss: 0.0212921 Test Loss: 0.0278196\n",
      "Validation loss decreased (0.024818 --> 0.021292).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0207562\n",
      "\tspeed: 0.0881s/iter; left time: 1897.3854s\n",
      "\titers: 200, epoch: 4 | loss: 0.0200337\n",
      "\tspeed: 0.0478s/iter; left time: 1024.9207s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0183250 Vali Loss: 0.0207108 Test Loss: 0.0272615\n",
      "Validation loss decreased (0.021292 --> 0.020711).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0151180\n",
      "\tspeed: 0.0869s/iter; left time: 1851.8966s\n",
      "\titers: 200, epoch: 5 | loss: 0.0161550\n",
      "\tspeed: 0.0474s/iter; left time: 1006.0479s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0166715 Vali Loss: 0.0204913 Test Loss: 0.0278639\n",
      "Validation loss decreased (0.020711 --> 0.020491).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0140818\n",
      "\tspeed: 0.0869s/iter; left time: 1833.0432s\n",
      "\titers: 200, epoch: 6 | loss: 0.0173617\n",
      "\tspeed: 0.0475s/iter; left time: 997.5723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0157911 Vali Loss: 0.0205434 Test Loss: 0.0280114\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0153887\n",
      "\tspeed: 0.0875s/iter; left time: 1826.5269s\n",
      "\titers: 200, epoch: 7 | loss: 0.0141236\n",
      "\tspeed: 0.0476s/iter; left time: 987.4697s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0154214 Vali Loss: 0.0204262 Test Loss: 0.0280592\n",
      "Validation loss decreased (0.020491 --> 0.020426).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0162005\n",
      "\tspeed: 0.0887s/iter; left time: 1831.1464s\n",
      "\titers: 200, epoch: 8 | loss: 0.0151308\n",
      "\tspeed: 0.0476s/iter; left time: 976.8516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0151685 Vali Loss: 0.0205546 Test Loss: 0.0274767\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0159596\n",
      "\tspeed: 0.0874s/iter; left time: 1785.1890s\n",
      "\titers: 200, epoch: 9 | loss: 0.0138793\n",
      "\tspeed: 0.0473s/iter; left time: 961.0273s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0149537 Vali Loss: 0.0201726 Test Loss: 0.0266926\n",
      "Validation loss decreased (0.020426 --> 0.020173).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0142701\n",
      "\tspeed: 0.0866s/iter; left time: 1749.6645s\n",
      "\titers: 200, epoch: 10 | loss: 0.0156937\n",
      "\tspeed: 0.0475s/iter; left time: 955.0107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0147354 Vali Loss: 0.0200238 Test Loss: 0.0267262\n",
      "Validation loss decreased (0.020173 --> 0.020024).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0162956\n",
      "\tspeed: 0.0879s/iter; left time: 1755.7273s\n",
      "\titers: 200, epoch: 11 | loss: 0.0157453\n",
      "\tspeed: 0.0474s/iter; left time: 942.1569s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0147208 Vali Loss: 0.0202973 Test Loss: 0.0269148\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0120478\n",
      "\tspeed: 0.0876s/iter; left time: 1729.6445s\n",
      "\titers: 200, epoch: 12 | loss: 0.0157311\n",
      "\tspeed: 0.0470s/iter; left time: 924.1245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.81s\n",
      "Steps: 223 | Train Loss: 0.0144875 Vali Loss: 0.0199512 Test Loss: 0.0266800\n",
      "Validation loss decreased (0.020024 --> 0.019951).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0137041\n",
      "\tspeed: 0.0844s/iter; left time: 1648.3102s\n",
      "\titers: 200, epoch: 13 | loss: 0.0147808\n",
      "\tspeed: 0.0477s/iter; left time: 926.0410s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 223 | Train Loss: 0.0143374 Vali Loss: 0.0200135 Test Loss: 0.0267759\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0139965\n",
      "\tspeed: 0.0871s/iter; left time: 1681.9929s\n",
      "\titers: 200, epoch: 14 | loss: 0.0139776\n",
      "\tspeed: 0.0476s/iter; left time: 913.7762s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0142924 Vali Loss: 0.0201385 Test Loss: 0.0269570\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0141281\n",
      "\tspeed: 0.0869s/iter; left time: 1657.4672s\n",
      "\titers: 200, epoch: 15 | loss: 0.0135372\n",
      "\tspeed: 0.0475s/iter; left time: 901.6724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 223 | Train Loss: 0.0141842 Vali Loss: 0.0197869 Test Loss: 0.0266150\n",
      "Validation loss decreased (0.019951 --> 0.019787).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0136077\n",
      "\tspeed: 0.0884s/iter; left time: 1666.2931s\n",
      "\titers: 200, epoch: 16 | loss: 0.0126454\n",
      "\tspeed: 0.0476s/iter; left time: 893.6879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 223 | Train Loss: 0.0141280 Vali Loss: 0.0200392 Test Loss: 0.0273709\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0148440\n",
      "\tspeed: 0.0862s/iter; left time: 1606.8698s\n",
      "\titers: 200, epoch: 17 | loss: 0.0133331\n",
      "\tspeed: 0.0473s/iter; left time: 876.0289s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0140650 Vali Loss: 0.0198028 Test Loss: 0.0266699\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0139154\n",
      "\tspeed: 0.0871s/iter; left time: 1603.0976s\n",
      "\titers: 200, epoch: 18 | loss: 0.0142051\n",
      "\tspeed: 0.0479s/iter; left time: 877.7034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 223 | Train Loss: 0.0139809 Vali Loss: 0.0200569 Test Loss: 0.0271863\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0132001\n",
      "\tspeed: 0.0869s/iter; left time: 1581.0645s\n",
      "\titers: 200, epoch: 19 | loss: 0.0152973\n",
      "\tspeed: 0.0475s/iter; left time: 858.7440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0138989 Vali Loss: 0.0201339 Test Loss: 0.0271165\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0138798\n",
      "\tspeed: 0.0872s/iter; left time: 1566.0552s\n",
      "\titers: 200, epoch: 20 | loss: 0.0122192\n",
      "\tspeed: 0.0472s/iter; left time: 843.5571s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0138612 Vali Loss: 0.0199807 Test Loss: 0.0271351\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0130000\n",
      "\tspeed: 0.0868s/iter; left time: 1540.2505s\n",
      "\titers: 200, epoch: 21 | loss: 0.0156088\n",
      "\tspeed: 0.0477s/iter; left time: 841.3036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 223 | Train Loss: 0.0138289 Vali Loss: 0.0198444 Test Loss: 0.0270159\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0137042\n",
      "\tspeed: 0.0871s/iter; left time: 1526.4545s\n",
      "\titers: 200, epoch: 22 | loss: 0.0144793\n",
      "\tspeed: 0.0475s/iter; left time: 827.5578s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0137971 Vali Loss: 0.0200208 Test Loss: 0.0274525\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0149143\n",
      "\tspeed: 0.0881s/iter; left time: 1523.1815s\n",
      "\titers: 200, epoch: 23 | loss: 0.0120591\n",
      "\tspeed: 0.0475s/iter; left time: 817.4438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.0137535 Vali Loss: 0.0198494 Test Loss: 0.0272250\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0139830\n",
      "\tspeed: 0.0878s/iter; left time: 1499.6625s\n",
      "\titers: 200, epoch: 24 | loss: 0.0142577\n",
      "\tspeed: 0.0475s/iter; left time: 805.8607s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 223 | Train Loss: 0.0137470 Vali Loss: 0.0199485 Test Loss: 0.0269865\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0144327\n",
      "\tspeed: 0.0867s/iter; left time: 1460.5066s\n",
      "\titers: 200, epoch: 25 | loss: 0.0133431\n",
      "\tspeed: 0.0475s/iter; left time: 796.1332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0136602 Vali Loss: 0.0199846 Test Loss: 0.0274140\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.026614956557750702, rmse:0.1631409078836441, mae:0.10954777151346207, rse:0.5627899169921875\n",
      "Intermediate time for GB and pred_len 24: 00h:05m:48.71s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_no_revin_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1323240\n",
      "\tspeed: 0.0763s/iter; left time: 1686.4147s\n",
      "\titers: 200, epoch: 1 | loss: 0.1149986\n",
      "\tspeed: 0.0475s/iter; left time: 1044.6460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.01s\n",
      "Steps: 222 | Train Loss: 0.1320870 Vali Loss: 0.0994640 Test Loss: 0.1168359\n",
      "Validation loss decreased (inf --> 0.099464).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0393860\n",
      "\tspeed: 0.0897s/iter; left time: 1963.5289s\n",
      "\titers: 200, epoch: 2 | loss: 0.0319159\n",
      "\tspeed: 0.0478s/iter; left time: 1040.1042s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0481805 Vali Loss: 0.0330101 Test Loss: 0.0472668\n",
      "Validation loss decreased (0.099464 --> 0.033010).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0261547\n",
      "\tspeed: 0.0881s/iter; left time: 1908.2823s\n",
      "\titers: 200, epoch: 3 | loss: 0.0259022\n",
      "\tspeed: 0.0475s/iter; left time: 1024.1054s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.73s\n",
      "Steps: 222 | Train Loss: 0.0273623 Vali Loss: 0.0302135 Test Loss: 0.0438073\n",
      "Validation loss decreased (0.033010 --> 0.030213).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0226433\n",
      "\tspeed: 0.0903s/iter; left time: 1935.5955s\n",
      "\titers: 200, epoch: 4 | loss: 0.0245221\n",
      "\tspeed: 0.0477s/iter; left time: 1016.6755s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0241833 Vali Loss: 0.0304314 Test Loss: 0.0490374\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0251091\n",
      "\tspeed: 0.0869s/iter; left time: 1844.4702s\n",
      "\titers: 200, epoch: 5 | loss: 0.0233933\n",
      "\tspeed: 0.0476s/iter; left time: 1004.7181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.82s\n",
      "Steps: 222 | Train Loss: 0.0231457 Vali Loss: 0.0307405 Test Loss: 0.0483087\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0226541\n",
      "\tspeed: 0.0838s/iter; left time: 1758.9097s\n",
      "\titers: 200, epoch: 6 | loss: 0.0229256\n",
      "\tspeed: 0.0480s/iter; left time: 1002.8295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.83s\n",
      "Steps: 222 | Train Loss: 0.0226541 Vali Loss: 0.0304898 Test Loss: 0.0448133\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0223542\n",
      "\tspeed: 0.0878s/iter; left time: 1823.8740s\n",
      "\titers: 200, epoch: 7 | loss: 0.0214087\n",
      "\tspeed: 0.0477s/iter; left time: 986.3182s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 222 | Train Loss: 0.0222953 Vali Loss: 0.0299704 Test Loss: 0.0423198\n",
      "Validation loss decreased (0.030213 --> 0.029970).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0221992\n",
      "\tspeed: 0.0883s/iter; left time: 1813.4008s\n",
      "\titers: 200, epoch: 8 | loss: 0.0222362\n",
      "\tspeed: 0.0472s/iter; left time: 964.4174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.79s\n",
      "Steps: 222 | Train Loss: 0.0220033 Vali Loss: 0.0299967 Test Loss: 0.0426890\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0217263\n",
      "\tspeed: 0.0857s/iter; left time: 1742.4903s\n",
      "\titers: 200, epoch: 9 | loss: 0.0209402\n",
      "\tspeed: 0.0480s/iter; left time: 970.0854s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 222 | Train Loss: 0.0217836 Vali Loss: 0.0296291 Test Loss: 0.0417050\n",
      "Validation loss decreased (0.029970 --> 0.029629).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0209449\n",
      "\tspeed: 0.0894s/iter; left time: 1796.9183s\n",
      "\titers: 200, epoch: 10 | loss: 0.0209933\n",
      "\tspeed: 0.0477s/iter; left time: 953.9196s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0215704 Vali Loss: 0.0298925 Test Loss: 0.0420922\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209658\n",
      "\tspeed: 0.0881s/iter; left time: 1750.6300s\n",
      "\titers: 200, epoch: 11 | loss: 0.0199631\n",
      "\tspeed: 0.0471s/iter; left time: 931.4474s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 222 | Train Loss: 0.0214490 Vali Loss: 0.0293610 Test Loss: 0.0405640\n",
      "Validation loss decreased (0.029629 --> 0.029361).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0216354\n",
      "\tspeed: 0.0885s/iter; left time: 1740.0859s\n",
      "\titers: 200, epoch: 12 | loss: 0.0204532\n",
      "\tspeed: 0.0476s/iter; left time: 931.8699s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0212971 Vali Loss: 0.0297991 Test Loss: 0.0413098\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0192091\n",
      "\tspeed: 0.0883s/iter; left time: 1716.0711s\n",
      "\titers: 200, epoch: 13 | loss: 0.0213485\n",
      "\tspeed: 0.0477s/iter; left time: 922.8742s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0212116 Vali Loss: 0.0300655 Test Loss: 0.0417966\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0204063\n",
      "\tspeed: 0.0881s/iter; left time: 1692.1524s\n",
      "\titers: 200, epoch: 14 | loss: 0.0211266\n",
      "\tspeed: 0.0476s/iter; left time: 910.1613s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0211267 Vali Loss: 0.0298509 Test Loss: 0.0411874\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0234450\n",
      "\tspeed: 0.0883s/iter; left time: 1677.4829s\n",
      "\titers: 200, epoch: 15 | loss: 0.0221137\n",
      "\tspeed: 0.0478s/iter; left time: 903.4866s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 222 | Train Loss: 0.0210179 Vali Loss: 0.0296575 Test Loss: 0.0406219\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0207905\n",
      "\tspeed: 0.0882s/iter; left time: 1656.1539s\n",
      "\titers: 200, epoch: 16 | loss: 0.0185365\n",
      "\tspeed: 0.0477s/iter; left time: 890.4702s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0209206 Vali Loss: 0.0299610 Test Loss: 0.0415653\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0202313\n",
      "\tspeed: 0.0891s/iter; left time: 1652.6109s\n",
      "\titers: 200, epoch: 17 | loss: 0.0208116\n",
      "\tspeed: 0.0477s/iter; left time: 879.5135s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 222 | Train Loss: 0.0208105 Vali Loss: 0.0303820 Test Loss: 0.0420282\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0221342\n",
      "\tspeed: 0.0888s/iter; left time: 1628.1746s\n",
      "\titers: 200, epoch: 18 | loss: 0.0200218\n",
      "\tspeed: 0.0477s/iter; left time: 869.8479s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 222 | Train Loss: 0.0207714 Vali Loss: 0.0299171 Test Loss: 0.0420240\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0193397\n",
      "\tspeed: 0.0864s/iter; left time: 1564.2127s\n",
      "\titers: 200, epoch: 19 | loss: 0.0221446\n",
      "\tspeed: 0.0473s/iter; left time: 850.9640s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.71s\n",
      "Steps: 222 | Train Loss: 0.0206576 Vali Loss: 0.0299749 Test Loss: 0.0419878\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0193885\n",
      "\tspeed: 0.0879s/iter; left time: 1572.2826s\n",
      "\titers: 200, epoch: 20 | loss: 0.0205301\n",
      "\tspeed: 0.0482s/iter; left time: 856.7414s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 222 | Train Loss: 0.0206312 Vali Loss: 0.0298362 Test Loss: 0.0417427\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0197740\n",
      "\tspeed: 0.0881s/iter; left time: 1556.1679s\n",
      "\titers: 200, epoch: 21 | loss: 0.0227434\n",
      "\tspeed: 0.0471s/iter; left time: 826.8828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 222 | Train Loss: 0.0206033 Vali Loss: 0.0300504 Test Loss: 0.0424300\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.040564022958278656, rmse:0.2014051228761673, mae:0.1430739164352417, rse:0.6964869499206543\n",
      "Intermediate time for GB and pred_len 96: 00h:04m:56.24s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_no_revin_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1281811\n",
      "\tspeed: 0.0750s/iter; left time: 1656.4701s\n",
      "\titers: 200, epoch: 1 | loss: 0.1206248\n",
      "\tspeed: 0.0480s/iter; left time: 1055.9307s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 222 | Train Loss: 0.1323972 Vali Loss: 0.1000619 Test Loss: 0.1167016\n",
      "Validation loss decreased (inf --> 0.100062).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0420797\n",
      "\tspeed: 0.0913s/iter; left time: 1997.1654s\n",
      "\titers: 200, epoch: 2 | loss: 0.0311064\n",
      "\tspeed: 0.0473s/iter; left time: 1029.3463s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 222 | Train Loss: 0.0486551 Vali Loss: 0.0341354 Test Loss: 0.0488688\n",
      "Validation loss decreased (0.100062 --> 0.034135).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0288977\n",
      "\tspeed: 0.0887s/iter; left time: 1919.9292s\n",
      "\titers: 200, epoch: 3 | loss: 0.0273455\n",
      "\tspeed: 0.0472s/iter; left time: 1018.1812s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.81s\n",
      "Steps: 222 | Train Loss: 0.0279936 Vali Loss: 0.0316944 Test Loss: 0.0481999\n",
      "Validation loss decreased (0.034135 --> 0.031694).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0267540\n",
      "\tspeed: 0.0909s/iter; left time: 1948.2807s\n",
      "\titers: 200, epoch: 4 | loss: 0.0237245\n",
      "\tspeed: 0.0475s/iter; left time: 1012.3997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.83s\n",
      "Steps: 222 | Train Loss: 0.0252568 Vali Loss: 0.0321467 Test Loss: 0.0511265\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0232651\n",
      "\tspeed: 0.0885s/iter; left time: 1876.9821s\n",
      "\titers: 200, epoch: 5 | loss: 0.0249807\n",
      "\tspeed: 0.0473s/iter; left time: 997.9422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 222 | Train Loss: 0.0244643 Vali Loss: 0.0326758 Test Loss: 0.0522774\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0223662\n",
      "\tspeed: 0.0831s/iter; left time: 1743.4788s\n",
      "\titers: 200, epoch: 6 | loss: 0.0243166\n",
      "\tspeed: 0.0480s/iter; left time: 1001.9034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.79s\n",
      "Steps: 222 | Train Loss: 0.0239696 Vali Loss: 0.0324708 Test Loss: 0.0497946\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0234233\n",
      "\tspeed: 0.0886s/iter; left time: 1840.0848s\n",
      "\titers: 200, epoch: 7 | loss: 0.0236040\n",
      "\tspeed: 0.0474s/iter; left time: 979.6082s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0235794 Vali Loss: 0.0324006 Test Loss: 0.0479286\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0232106\n",
      "\tspeed: 0.0884s/iter; left time: 1816.4281s\n",
      "\titers: 200, epoch: 8 | loss: 0.0215790\n",
      "\tspeed: 0.0473s/iter; left time: 966.1345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.82s\n",
      "Steps: 222 | Train Loss: 0.0232308 Vali Loss: 0.0325789 Test Loss: 0.0474472\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0229674\n",
      "\tspeed: 0.0903s/iter; left time: 1834.5781s\n",
      "\titers: 200, epoch: 9 | loss: 0.0228480\n",
      "\tspeed: 0.0480s/iter; left time: 970.4687s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 222 | Train Loss: 0.0230120 Vali Loss: 0.0318836 Test Loss: 0.0453121\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0225253\n",
      "\tspeed: 0.0902s/iter; left time: 1813.0776s\n",
      "\titers: 200, epoch: 10 | loss: 0.0229859\n",
      "\tspeed: 0.0479s/iter; left time: 958.8118s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 222 | Train Loss: 0.0228531 Vali Loss: 0.0317617 Test Loss: 0.0442896\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0240105\n",
      "\tspeed: 0.0884s/iter; left time: 1756.8440s\n",
      "\titers: 200, epoch: 11 | loss: 0.0231950\n",
      "\tspeed: 0.0476s/iter; left time: 942.3438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 222 | Train Loss: 0.0226830 Vali Loss: 0.0319622 Test Loss: 0.0452901\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0240522\n",
      "\tspeed: 0.0850s/iter; left time: 1671.8097s\n",
      "\titers: 200, epoch: 12 | loss: 0.0224349\n",
      "\tspeed: 0.0473s/iter; left time: 925.5889s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.71s\n",
      "Steps: 222 | Train Loss: 0.0225790 Vali Loss: 0.0319162 Test Loss: 0.0447830\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0210353\n",
      "\tspeed: 0.0882s/iter; left time: 1714.4892s\n",
      "\titers: 200, epoch: 13 | loss: 0.0226072\n",
      "\tspeed: 0.0481s/iter; left time: 929.8547s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0224231 Vali Loss: 0.0321067 Test Loss: 0.0451294\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.048199884593486786, rmse:0.21954472362995148, mae:0.15564729273319244, rse:0.7611930966377258\n",
      "Intermediate time for GB and pred_len 168: 00h:03m:09.72s\n",
      "Intermediate time for GB: 00h:13m:54.67s\n",
      "Total time: 00h:13m:54.67s\n"
     ]
    }
   ],
   "source": [
    "patchtst_results = []\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"GB_no_revin_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --revin 0 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.1556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "GB      24        0.0266  0.1631  0.1095\n",
       "        96        0.0406  0.2014  0.1431\n",
       "        168       0.0482  0.2195  0.1556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False).round(4)\n",
    "patchtst_df.to_csv(os.path.join(path, 'GB_patchtst_no_revin.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/informer/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "seq_len = 96\n",
    "model = \"Informer\"\n",
    "itr = 1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_gb_no_outliers.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.0001\n",
    "#n_heads = 16\n",
    "e_layers = 2\n",
    "d_layers = 1\n",
    "loss = \"MSE\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Lists to store the results\n",
    "informer_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_24_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0764679\n",
      "\tspeed: 0.0758s/iter; left time: 1366.7560s\n",
      "\titers: 200, epoch: 1 | loss: 0.0657391\n",
      "\tspeed: 0.0462s/iter; left time: 828.6651s\n",
      "\titers: 300, epoch: 1 | loss: 0.0562565\n",
      "\tspeed: 0.0461s/iter; left time: 821.3823s\n",
      "\titers: 400, epoch: 1 | loss: 0.0548274\n",
      "\tspeed: 0.0428s/iter; left time: 757.6466s\n",
      "\titers: 500, epoch: 1 | loss: 0.0477097\n",
      "\tspeed: 0.0414s/iter; left time: 729.4165s\n",
      "\titers: 600, epoch: 1 | loss: 0.0463367\n",
      "\tspeed: 0.0413s/iter; left time: 723.7216s\n",
      "\titers: 700, epoch: 1 | loss: 0.0439485\n",
      "\tspeed: 0.0429s/iter; left time: 746.5383s\n",
      "\titers: 800, epoch: 1 | loss: 0.0382239\n",
      "\tspeed: 0.0473s/iter; left time: 819.5221s\n",
      "\titers: 900, epoch: 1 | loss: 0.0366430\n",
      "\tspeed: 0.0467s/iter; left time: 804.5277s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:41.14s\n",
      "Steps: 906 | Train Loss: 0.0594367 Vali Loss: 0.0429403 Test Loss: 0.0600197\n",
      "Validation loss decreased (inf --> 0.042940).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0303652\n",
      "\tspeed: 0.1042s/iter; left time: 1784.1190s\n",
      "\titers: 200, epoch: 2 | loss: 0.0261540\n",
      "\tspeed: 0.0473s/iter; left time: 804.5889s\n",
      "\titers: 300, epoch: 2 | loss: 0.0150766\n",
      "\tspeed: 0.0470s/iter; left time: 794.2834s\n",
      "\titers: 400, epoch: 2 | loss: 0.0167487\n",
      "\tspeed: 0.0466s/iter; left time: 782.8286s\n",
      "\titers: 500, epoch: 2 | loss: 0.0202715\n",
      "\tspeed: 0.0461s/iter; left time: 769.9011s\n",
      "\titers: 600, epoch: 2 | loss: 0.0124757\n",
      "\tspeed: 0.0463s/iter; left time: 769.8667s\n",
      "\titers: 700, epoch: 2 | loss: 0.0234306\n",
      "\tspeed: 0.0461s/iter; left time: 761.6331s\n",
      "\titers: 800, epoch: 2 | loss: 0.0192762\n",
      "\tspeed: 0.0459s/iter; left time: 752.6298s\n",
      "\titers: 900, epoch: 2 | loss: 0.0130124\n",
      "\tspeed: 0.0465s/iter; left time: 759.2532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:42.25s\n",
      "Steps: 906 | Train Loss: 0.0197113 Vali Loss: 0.0220081 Test Loss: 0.0308800\n",
      "Validation loss decreased (0.042940 --> 0.022008).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0158460\n",
      "\tspeed: 0.0993s/iter; left time: 1609.6451s\n",
      "\titers: 200, epoch: 3 | loss: 0.0151482\n",
      "\tspeed: 0.0412s/iter; left time: 664.3162s\n",
      "\titers: 300, epoch: 3 | loss: 0.0177428\n",
      "\tspeed: 0.0433s/iter; left time: 692.4326s\n",
      "\titers: 400, epoch: 3 | loss: 0.0127546\n",
      "\tspeed: 0.0415s/iter; left time: 659.6006s\n",
      "\titers: 500, epoch: 3 | loss: 0.0152168\n",
      "\tspeed: 0.0414s/iter; left time: 654.9877s\n",
      "\titers: 600, epoch: 3 | loss: 0.0130471\n",
      "\tspeed: 0.0413s/iter; left time: 648.0942s\n",
      "\titers: 700, epoch: 3 | loss: 0.0149760\n",
      "\tspeed: 0.0455s/iter; left time: 709.5643s\n",
      "\titers: 800, epoch: 3 | loss: 0.0108637\n",
      "\tspeed: 0.0464s/iter; left time: 719.7219s\n",
      "\titers: 900, epoch: 3 | loss: 0.0151149\n",
      "\tspeed: 0.0464s/iter; left time: 715.2005s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:39.53s\n",
      "Steps: 906 | Train Loss: 0.0149118 Vali Loss: 0.0218676 Test Loss: 0.0311270\n",
      "Validation loss decreased (0.022008 --> 0.021868).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0119106\n",
      "\tspeed: 0.0999s/iter; left time: 1528.0759s\n",
      "\titers: 200, epoch: 4 | loss: 0.0157133\n",
      "\tspeed: 0.0465s/iter; left time: 706.5120s\n",
      "\titers: 300, epoch: 4 | loss: 0.0150046\n",
      "\tspeed: 0.0464s/iter; left time: 700.4594s\n",
      "\titers: 400, epoch: 4 | loss: 0.0168810\n",
      "\tspeed: 0.0468s/iter; left time: 701.5732s\n",
      "\titers: 500, epoch: 4 | loss: 0.0203729\n",
      "\tspeed: 0.0466s/iter; left time: 695.0318s\n",
      "\titers: 600, epoch: 4 | loss: 0.0154422\n",
      "\tspeed: 0.0460s/iter; left time: 680.2294s\n",
      "\titers: 700, epoch: 4 | loss: 0.0118690\n",
      "\tspeed: 0.0435s/iter; left time: 639.4654s\n",
      "\titers: 800, epoch: 4 | loss: 0.0124499\n",
      "\tspeed: 0.0466s/iter; left time: 680.8255s\n",
      "\titers: 900, epoch: 4 | loss: 0.0121385\n",
      "\tspeed: 0.0458s/iter; left time: 664.7619s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:41.93s\n",
      "Steps: 906 | Train Loss: 0.0138662 Vali Loss: 0.0221879 Test Loss: 0.0294972\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0200112\n",
      "\tspeed: 0.0942s/iter; left time: 1355.9903s\n",
      "\titers: 200, epoch: 5 | loss: 0.0109799\n",
      "\tspeed: 0.0474s/iter; left time: 677.9581s\n",
      "\titers: 300, epoch: 5 | loss: 0.0117807\n",
      "\tspeed: 0.0471s/iter; left time: 668.1902s\n",
      "\titers: 400, epoch: 5 | loss: 0.0172931\n",
      "\tspeed: 0.0463s/iter; left time: 652.6405s\n",
      "\titers: 500, epoch: 5 | loss: 0.0109414\n",
      "\tspeed: 0.0468s/iter; left time: 654.4330s\n",
      "\titers: 600, epoch: 5 | loss: 0.0122645\n",
      "\tspeed: 0.0470s/iter; left time: 653.2040s\n",
      "\titers: 700, epoch: 5 | loss: 0.0151734\n",
      "\tspeed: 0.0459s/iter; left time: 633.9620s\n",
      "\titers: 800, epoch: 5 | loss: 0.0145623\n",
      "\tspeed: 0.0462s/iter; left time: 633.3743s\n",
      "\titers: 900, epoch: 5 | loss: 0.0112677\n",
      "\tspeed: 0.0469s/iter; left time: 638.3172s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:42.51s\n",
      "Steps: 906 | Train Loss: 0.0130095 Vali Loss: 0.0240887 Test Loss: 0.0324934\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0120146\n",
      "\tspeed: 0.0936s/iter; left time: 1262.4139s\n",
      "\titers: 200, epoch: 6 | loss: 0.0134536\n",
      "\tspeed: 0.0411s/iter; left time: 550.8468s\n",
      "\titers: 300, epoch: 6 | loss: 0.0109227\n",
      "\tspeed: 0.0411s/iter; left time: 545.6947s\n",
      "\titers: 400, epoch: 6 | loss: 0.0137217\n",
      "\tspeed: 0.0471s/iter; left time: 621.0566s\n",
      "\titers: 500, epoch: 6 | loss: 0.0131301\n",
      "\tspeed: 0.0455s/iter; left time: 595.3180s\n",
      "\titers: 600, epoch: 6 | loss: 0.0103798\n",
      "\tspeed: 0.0467s/iter; left time: 607.0220s\n",
      "\titers: 700, epoch: 6 | loss: 0.0114151\n",
      "\tspeed: 0.0463s/iter; left time: 596.6293s\n",
      "\titers: 800, epoch: 6 | loss: 0.0096706\n",
      "\tspeed: 0.0463s/iter; left time: 592.4021s\n",
      "\titers: 900, epoch: 6 | loss: 0.0158617\n",
      "\tspeed: 0.0468s/iter; left time: 593.3785s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:41.27s\n",
      "Steps: 906 | Train Loss: 0.0120912 Vali Loss: 0.0258931 Test Loss: 0.0350914\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0107834\n",
      "\tspeed: 0.0922s/iter; left time: 1160.0097s\n",
      "\titers: 200, epoch: 7 | loss: 0.0110156\n",
      "\tspeed: 0.0472s/iter; left time: 589.8094s\n",
      "\titers: 300, epoch: 7 | loss: 0.0107683\n",
      "\tspeed: 0.0428s/iter; left time: 530.4415s\n",
      "\titers: 400, epoch: 7 | loss: 0.0116925\n",
      "\tspeed: 0.0413s/iter; left time: 507.5918s\n",
      "\titers: 500, epoch: 7 | loss: 0.0104368\n",
      "\tspeed: 0.0449s/iter; left time: 546.8833s\n",
      "\titers: 600, epoch: 7 | loss: 0.0069574\n",
      "\tspeed: 0.0459s/iter; left time: 554.8449s\n",
      "\titers: 700, epoch: 7 | loss: 0.0106184\n",
      "\tspeed: 0.0465s/iter; left time: 557.7375s\n",
      "\titers: 800, epoch: 7 | loss: 0.0090532\n",
      "\tspeed: 0.0473s/iter; left time: 562.6580s\n",
      "\titers: 900, epoch: 7 | loss: 0.0105725\n",
      "\tspeed: 0.0459s/iter; left time: 540.5507s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:41.22s\n",
      "Steps: 906 | Train Loss: 0.0112189 Vali Loss: 0.0227329 Test Loss: 0.0320340\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0106522\n",
      "\tspeed: 0.0939s/iter; left time: 1097.1025s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081547\n",
      "\tspeed: 0.0466s/iter; left time: 540.0157s\n",
      "\titers: 300, epoch: 8 | loss: 0.0101518\n",
      "\tspeed: 0.0471s/iter; left time: 540.4347s\n",
      "\titers: 400, epoch: 8 | loss: 0.0103827\n",
      "\tspeed: 0.0469s/iter; left time: 533.6337s\n",
      "\titers: 500, epoch: 8 | loss: 0.0097602\n",
      "\tspeed: 0.0467s/iter; left time: 526.9957s\n",
      "\titers: 600, epoch: 8 | loss: 0.0106588\n",
      "\tspeed: 0.0471s/iter; left time: 526.8117s\n",
      "\titers: 700, epoch: 8 | loss: 0.0106670\n",
      "\tspeed: 0.0470s/iter; left time: 520.7833s\n",
      "\titers: 800, epoch: 8 | loss: 0.0084209\n",
      "\tspeed: 0.0455s/iter; left time: 499.9283s\n",
      "\titers: 900, epoch: 8 | loss: 0.0084942\n",
      "\tspeed: 0.0436s/iter; left time: 474.5200s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:42.21s\n",
      "Steps: 906 | Train Loss: 0.0102566 Vali Loss: 0.0264436 Test Loss: 0.0365799\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.031130269169807434, rmse:0.17643772065639496, mae:0.12031001597642899, rse:0.6083531379699707\n",
      "Intermediate time for GB and pred_len 24: 00h:06m:20.86s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_96_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0893760\n",
      "\tspeed: 0.0771s/iter; left time: 1386.5858s\n",
      "\titers: 200, epoch: 1 | loss: 0.0699951\n",
      "\tspeed: 0.0478s/iter; left time: 854.5878s\n",
      "\titers: 300, epoch: 1 | loss: 0.0674433\n",
      "\tspeed: 0.0480s/iter; left time: 853.9300s\n",
      "\titers: 400, epoch: 1 | loss: 0.0645395\n",
      "\tspeed: 0.0489s/iter; left time: 864.4706s\n",
      "\titers: 500, epoch: 1 | loss: 0.0576673\n",
      "\tspeed: 0.0483s/iter; left time: 848.8444s\n",
      "\titers: 600, epoch: 1 | loss: 0.0536212\n",
      "\tspeed: 0.0477s/iter; left time: 833.7362s\n",
      "\titers: 700, epoch: 1 | loss: 0.0497722\n",
      "\tspeed: 0.0484s/iter; left time: 841.1261s\n",
      "\titers: 800, epoch: 1 | loss: 0.0523500\n",
      "\tspeed: 0.0484s/iter; left time: 836.7344s\n",
      "\titers: 900, epoch: 1 | loss: 0.0492230\n",
      "\tspeed: 0.0483s/iter; left time: 829.6894s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:44.40s\n",
      "Steps: 904 | Train Loss: 0.0649915 Vali Loss: 0.0547138 Test Loss: 0.0850027\n",
      "Validation loss decreased (inf --> 0.054714).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0341445\n",
      "\tspeed: 0.1083s/iter; left time: 1848.9912s\n",
      "\titers: 200, epoch: 2 | loss: 0.0328115\n",
      "\tspeed: 0.0477s/iter; left time: 809.6190s\n",
      "\titers: 300, epoch: 2 | loss: 0.0280633\n",
      "\tspeed: 0.0472s/iter; left time: 797.4198s\n",
      "\titers: 400, epoch: 2 | loss: 0.0278533\n",
      "\tspeed: 0.0486s/iter; left time: 815.4622s\n",
      "\titers: 500, epoch: 2 | loss: 0.0259066\n",
      "\tspeed: 0.0486s/iter; left time: 810.1108s\n",
      "\titers: 600, epoch: 2 | loss: 0.0295652\n",
      "\tspeed: 0.0487s/iter; left time: 806.7384s\n",
      "\titers: 700, epoch: 2 | loss: 0.0231918\n",
      "\tspeed: 0.0479s/iter; left time: 788.7876s\n",
      "\titers: 800, epoch: 2 | loss: 0.0256015\n",
      "\tspeed: 0.0476s/iter; left time: 778.8492s\n",
      "\titers: 900, epoch: 2 | loss: 0.0304700\n",
      "\tspeed: 0.0489s/iter; left time: 796.3451s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:43.64s\n",
      "Steps: 904 | Train Loss: 0.0301226 Vali Loss: 0.0388117 Test Loss: 0.0579192\n",
      "Validation loss decreased (0.054714 --> 0.038812).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0234212\n",
      "\tspeed: 0.1146s/iter; left time: 1853.1882s\n",
      "\titers: 200, epoch: 3 | loss: 0.0262930\n",
      "\tspeed: 0.0485s/iter; left time: 778.8745s\n",
      "\titers: 300, epoch: 3 | loss: 0.0239697\n",
      "\tspeed: 0.0477s/iter; left time: 762.6793s\n",
      "\titers: 400, epoch: 3 | loss: 0.0237850\n",
      "\tspeed: 0.0472s/iter; left time: 748.9891s\n",
      "\titers: 500, epoch: 3 | loss: 0.0224860\n",
      "\tspeed: 0.0487s/iter; left time: 768.5099s\n",
      "\titers: 600, epoch: 3 | loss: 0.0251826\n",
      "\tspeed: 0.0487s/iter; left time: 763.4707s\n",
      "\titers: 700, epoch: 3 | loss: 0.0222447\n",
      "\tspeed: 0.0479s/iter; left time: 746.6295s\n",
      "\titers: 800, epoch: 3 | loss: 0.0206792\n",
      "\tspeed: 0.0479s/iter; left time: 741.5689s\n",
      "\titers: 900, epoch: 3 | loss: 0.0221146\n",
      "\tspeed: 0.0482s/iter; left time: 740.9391s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:43.82s\n",
      "Steps: 904 | Train Loss: 0.0232364 Vali Loss: 0.0331869 Test Loss: 0.0506150\n",
      "Validation loss decreased (0.038812 --> 0.033187).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0239859\n",
      "\tspeed: 0.1116s/iter; left time: 1703.9495s\n",
      "\titers: 200, epoch: 4 | loss: 0.0222580\n",
      "\tspeed: 0.0489s/iter; left time: 741.4599s\n",
      "\titers: 300, epoch: 4 | loss: 0.0253674\n",
      "\tspeed: 0.0482s/iter; left time: 726.1311s\n",
      "\titers: 400, epoch: 4 | loss: 0.0194161\n",
      "\tspeed: 0.0480s/iter; left time: 718.8724s\n",
      "\titers: 500, epoch: 4 | loss: 0.0188118\n",
      "\tspeed: 0.0483s/iter; left time: 718.5034s\n",
      "\titers: 600, epoch: 4 | loss: 0.0208789\n",
      "\tspeed: 0.0472s/iter; left time: 697.7479s\n",
      "\titers: 700, epoch: 4 | loss: 0.0178792\n",
      "\tspeed: 0.0483s/iter; left time: 708.7075s\n",
      "\titers: 800, epoch: 4 | loss: 0.0212071\n",
      "\tspeed: 0.0485s/iter; left time: 706.0308s\n",
      "\titers: 900, epoch: 4 | loss: 0.0216732\n",
      "\tspeed: 0.0487s/iter; left time: 705.3533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:43.91s\n",
      "Steps: 904 | Train Loss: 0.0205480 Vali Loss: 0.0346284 Test Loss: 0.0530353\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0180327\n",
      "\tspeed: 0.1026s/iter; left time: 1474.5301s\n",
      "\titers: 200, epoch: 5 | loss: 0.0200579\n",
      "\tspeed: 0.0466s/iter; left time: 664.2468s\n",
      "\titers: 300, epoch: 5 | loss: 0.0168916\n",
      "\tspeed: 0.0472s/iter; left time: 668.4474s\n",
      "\titers: 400, epoch: 5 | loss: 0.0205630\n",
      "\tspeed: 0.0483s/iter; left time: 678.9023s\n",
      "\titers: 500, epoch: 5 | loss: 0.0182274\n",
      "\tspeed: 0.0476s/iter; left time: 664.2421s\n",
      "\titers: 600, epoch: 5 | loss: 0.0207174\n",
      "\tspeed: 0.0485s/iter; left time: 672.5713s\n",
      "\titers: 700, epoch: 5 | loss: 0.0234901\n",
      "\tspeed: 0.0483s/iter; left time: 664.1794s\n",
      "\titers: 800, epoch: 5 | loss: 0.0209549\n",
      "\tspeed: 0.0479s/iter; left time: 654.5739s\n",
      "\titers: 900, epoch: 5 | loss: 0.0205995\n",
      "\tspeed: 0.0487s/iter; left time: 661.2406s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:43.50s\n",
      "Steps: 904 | Train Loss: 0.0186726 Vali Loss: 0.0334696 Test Loss: 0.0521068\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0173234\n",
      "\tspeed: 0.1095s/iter; left time: 1474.0086s\n",
      "\titers: 200, epoch: 6 | loss: 0.0156628\n",
      "\tspeed: 0.0485s/iter; left time: 648.0930s\n",
      "\titers: 300, epoch: 6 | loss: 0.0159969\n",
      "\tspeed: 0.0491s/iter; left time: 651.0299s\n",
      "\titers: 400, epoch: 6 | loss: 0.0178492\n",
      "\tspeed: 0.0485s/iter; left time: 637.6560s\n",
      "\titers: 500, epoch: 6 | loss: 0.0150308\n",
      "\tspeed: 0.0494s/iter; left time: 644.9581s\n",
      "\titers: 600, epoch: 6 | loss: 0.0172063\n",
      "\tspeed: 0.0481s/iter; left time: 623.9105s\n",
      "\titers: 700, epoch: 6 | loss: 0.0152361\n",
      "\tspeed: 0.0471s/iter; left time: 605.9831s\n",
      "\titers: 800, epoch: 6 | loss: 0.0163435\n",
      "\tspeed: 0.0472s/iter; left time: 602.5518s\n",
      "\titers: 900, epoch: 6 | loss: 0.0171424\n",
      "\tspeed: 0.0481s/iter; left time: 608.9784s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:43.88s\n",
      "Steps: 904 | Train Loss: 0.0166152 Vali Loss: 0.0380499 Test Loss: 0.0581647\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0139970\n",
      "\tspeed: 0.1053s/iter; left time: 1322.0537s\n",
      "\titers: 200, epoch: 7 | loss: 0.0148535\n",
      "\tspeed: 0.0484s/iter; left time: 602.5231s\n",
      "\titers: 300, epoch: 7 | loss: 0.0137906\n",
      "\tspeed: 0.0463s/iter; left time: 572.2347s\n",
      "\titers: 400, epoch: 7 | loss: 0.0162590\n",
      "\tspeed: 0.0478s/iter; left time: 585.8204s\n",
      "\titers: 500, epoch: 7 | loss: 0.0165481\n",
      "\tspeed: 0.0502s/iter; left time: 609.8847s\n",
      "\titers: 600, epoch: 7 | loss: 0.0164570\n",
      "\tspeed: 0.0503s/iter; left time: 606.7374s\n",
      "\titers: 700, epoch: 7 | loss: 0.0176711\n",
      "\tspeed: 0.0482s/iter; left time: 575.8652s\n",
      "\titers: 800, epoch: 7 | loss: 0.0125647\n",
      "\tspeed: 0.0490s/iter; left time: 581.4989s\n",
      "\titers: 900, epoch: 7 | loss: 0.0137664\n",
      "\tspeed: 0.0484s/iter; left time: 569.3538s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:44.13s\n",
      "Steps: 904 | Train Loss: 0.0148866 Vali Loss: 0.0356738 Test Loss: 0.0604467\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0133834\n",
      "\tspeed: 0.0895s/iter; left time: 1042.6091s\n",
      "\titers: 200, epoch: 8 | loss: 0.0131127\n",
      "\tspeed: 0.0482s/iter; left time: 556.9965s\n",
      "\titers: 300, epoch: 8 | loss: 0.0137572\n",
      "\tspeed: 0.0477s/iter; left time: 545.9729s\n",
      "\titers: 400, epoch: 8 | loss: 0.0119892\n",
      "\tspeed: 0.0478s/iter; left time: 542.4755s\n",
      "\titers: 500, epoch: 8 | loss: 0.0147413\n",
      "\tspeed: 0.0481s/iter; left time: 541.4741s\n",
      "\titers: 600, epoch: 8 | loss: 0.0135280\n",
      "\tspeed: 0.0480s/iter; left time: 534.8598s\n",
      "\titers: 700, epoch: 8 | loss: 0.0128642\n",
      "\tspeed: 0.0480s/iter; left time: 530.6420s\n",
      "\titers: 800, epoch: 8 | loss: 0.0143640\n",
      "\tspeed: 0.0486s/iter; left time: 532.8132s\n",
      "\titers: 900, epoch: 8 | loss: 0.0136676\n",
      "\tspeed: 0.0483s/iter; left time: 523.9539s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:42.45s\n",
      "Steps: 904 | Train Loss: 0.0132417 Vali Loss: 0.0376644 Test Loss: 0.0605138\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.05061672627925873, rmse:0.22498160600662231, mae:0.1600819081068039, rse:0.7780177593231201\n",
      "Intermediate time for GB and pred_len 96: 00h:06m:47.22s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_168_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0828232\n",
      "\tspeed: 0.0798s/iter; left time: 1431.9005s\n",
      "\titers: 200, epoch: 1 | loss: 0.0749127\n",
      "\tspeed: 0.0515s/iter; left time: 919.2858s\n",
      "\titers: 300, epoch: 1 | loss: 0.0620970\n",
      "\tspeed: 0.0510s/iter; left time: 904.3161s\n",
      "\titers: 400, epoch: 1 | loss: 0.0595553\n",
      "\tspeed: 0.0451s/iter; left time: 795.3632s\n",
      "\titers: 500, epoch: 1 | loss: 0.0616825\n",
      "\tspeed: 0.0435s/iter; left time: 762.7402s\n",
      "\titers: 600, epoch: 1 | loss: 0.0584624\n",
      "\tspeed: 0.0487s/iter; left time: 849.7259s\n",
      "\titers: 700, epoch: 1 | loss: 0.0525494\n",
      "\tspeed: 0.0447s/iter; left time: 775.2312s\n",
      "\titers: 800, epoch: 1 | loss: 0.0509944\n",
      "\tspeed: 0.0462s/iter; left time: 796.0521s\n",
      "\titers: 900, epoch: 1 | loss: 0.0496514\n",
      "\tspeed: 0.0421s/iter; left time: 721.8641s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:43.24s\n",
      "Steps: 902 | Train Loss: 0.0657342 Vali Loss: 0.0569065 Test Loss: 0.0867981\n",
      "Validation loss decreased (inf --> 0.056906).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0415353\n",
      "\tspeed: 0.1158s/iter; left time: 1973.2567s\n",
      "\titers: 200, epoch: 2 | loss: 0.0353375\n",
      "\tspeed: 0.0497s/iter; left time: 841.3736s\n",
      "\titers: 300, epoch: 2 | loss: 0.0351734\n",
      "\tspeed: 0.0507s/iter; left time: 854.4021s\n",
      "\titers: 400, epoch: 2 | loss: 0.0284817\n",
      "\tspeed: 0.0495s/iter; left time: 828.2770s\n",
      "\titers: 500, epoch: 2 | loss: 0.0283058\n",
      "\tspeed: 0.0514s/iter; left time: 854.6885s\n",
      "\titers: 600, epoch: 2 | loss: 0.0289276\n",
      "\tspeed: 0.0507s/iter; left time: 838.2517s\n",
      "\titers: 700, epoch: 2 | loss: 0.0289890\n",
      "\tspeed: 0.0446s/iter; left time: 733.8829s\n",
      "\titers: 800, epoch: 2 | loss: 0.0284246\n",
      "\tspeed: 0.0498s/iter; left time: 813.6006s\n",
      "\titers: 900, epoch: 2 | loss: 0.0308674\n",
      "\tspeed: 0.0498s/iter; left time: 808.1484s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:44.56s\n",
      "Steps: 902 | Train Loss: 0.0330005 Vali Loss: 0.0403766 Test Loss: 0.0631403\n",
      "Validation loss decreased (0.056906 --> 0.040377).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0281291\n",
      "\tspeed: 0.1277s/iter; left time: 2060.7461s\n",
      "\titers: 200, epoch: 3 | loss: 0.0253809\n",
      "\tspeed: 0.0510s/iter; left time: 818.6629s\n",
      "\titers: 300, epoch: 3 | loss: 0.0304348\n",
      "\tspeed: 0.0464s/iter; left time: 739.8460s\n",
      "\titers: 400, epoch: 3 | loss: 0.0270059\n",
      "\tspeed: 0.0479s/iter; left time: 758.9623s\n",
      "\titers: 500, epoch: 3 | loss: 0.0259681\n",
      "\tspeed: 0.0449s/iter; left time: 706.9402s\n",
      "\titers: 600, epoch: 3 | loss: 0.0313333\n",
      "\tspeed: 0.0437s/iter; left time: 683.0197s\n",
      "\titers: 700, epoch: 3 | loss: 0.0257676\n",
      "\tspeed: 0.0485s/iter; left time: 754.1090s\n",
      "\titers: 800, epoch: 3 | loss: 0.0252682\n",
      "\tspeed: 0.0448s/iter; left time: 691.4457s\n",
      "\titers: 900, epoch: 3 | loss: 0.0248985\n",
      "\tspeed: 0.0448s/iter; left time: 687.0964s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:42.53s\n",
      "Steps: 902 | Train Loss: 0.0268744 Vali Loss: 0.0439434 Test Loss: 0.0653365\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0279412\n",
      "\tspeed: 0.1087s/iter; left time: 1655.2970s\n",
      "\titers: 200, epoch: 4 | loss: 0.0242079\n",
      "\tspeed: 0.0515s/iter; left time: 779.8743s\n",
      "\titers: 300, epoch: 4 | loss: 0.0269835\n",
      "\tspeed: 0.0482s/iter; left time: 724.6813s\n",
      "\titers: 400, epoch: 4 | loss: 0.0255507\n",
      "\tspeed: 0.0493s/iter; left time: 736.8745s\n",
      "\titers: 500, epoch: 4 | loss: 0.0225070\n",
      "\tspeed: 0.0490s/iter; left time: 726.9783s\n",
      "\titers: 600, epoch: 4 | loss: 0.0230421\n",
      "\tspeed: 0.0465s/iter; left time: 685.7421s\n",
      "\titers: 700, epoch: 4 | loss: 0.0248679\n",
      "\tspeed: 0.0446s/iter; left time: 652.6208s\n",
      "\titers: 800, epoch: 4 | loss: 0.0225724\n",
      "\tspeed: 0.0466s/iter; left time: 677.3341s\n",
      "\titers: 900, epoch: 4 | loss: 0.0205266\n",
      "\tspeed: 0.0507s/iter; left time: 731.9488s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:43.61s\n",
      "Steps: 902 | Train Loss: 0.0245145 Vali Loss: 0.0396597 Test Loss: 0.0621735\n",
      "Validation loss decreased (0.040377 --> 0.039660).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0214768\n",
      "\tspeed: 0.1148s/iter; left time: 1644.8283s\n",
      "\titers: 200, epoch: 5 | loss: 0.0234881\n",
      "\tspeed: 0.0434s/iter; left time: 618.2559s\n",
      "\titers: 300, epoch: 5 | loss: 0.0215285\n",
      "\tspeed: 0.0502s/iter; left time: 710.1507s\n",
      "\titers: 400, epoch: 5 | loss: 0.0217437\n",
      "\tspeed: 0.0490s/iter; left time: 687.4988s\n",
      "\titers: 500, epoch: 5 | loss: 0.0214513\n",
      "\tspeed: 0.0457s/iter; left time: 637.1036s\n",
      "\titers: 600, epoch: 5 | loss: 0.0207977\n",
      "\tspeed: 0.0444s/iter; left time: 614.7112s\n",
      "\titers: 700, epoch: 5 | loss: 0.0178466\n",
      "\tspeed: 0.0433s/iter; left time: 594.3988s\n",
      "\titers: 800, epoch: 5 | loss: 0.0219532\n",
      "\tspeed: 0.0466s/iter; left time: 635.3836s\n",
      "\titers: 900, epoch: 5 | loss: 0.0201745\n",
      "\tspeed: 0.0465s/iter; left time: 629.1004s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:41.78s\n",
      "Steps: 902 | Train Loss: 0.0214388 Vali Loss: 0.0403047 Test Loss: 0.0659283\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0192441\n",
      "\tspeed: 0.1159s/iter; left time: 1557.1226s\n",
      "\titers: 200, epoch: 6 | loss: 0.0194779\n",
      "\tspeed: 0.0498s/iter; left time: 663.6091s\n",
      "\titers: 300, epoch: 6 | loss: 0.0164167\n",
      "\tspeed: 0.0473s/iter; left time: 626.0888s\n",
      "\titers: 400, epoch: 6 | loss: 0.0161506\n",
      "\tspeed: 0.0480s/iter; left time: 630.2631s\n",
      "\titers: 500, epoch: 6 | loss: 0.0196075\n",
      "\tspeed: 0.0443s/iter; left time: 576.6737s\n",
      "\titers: 600, epoch: 6 | loss: 0.0166038\n",
      "\tspeed: 0.0430s/iter; left time: 556.3854s\n",
      "\titers: 700, epoch: 6 | loss: 0.0136157\n",
      "\tspeed: 0.0423s/iter; left time: 542.8662s\n",
      "\titers: 800, epoch: 6 | loss: 0.0160816\n",
      "\tspeed: 0.0406s/iter; left time: 516.5055s\n",
      "\titers: 900, epoch: 6 | loss: 0.0160592\n",
      "\tspeed: 0.0487s/iter; left time: 615.6642s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:41.75s\n",
      "Steps: 902 | Train Loss: 0.0170734 Vali Loss: 0.0388602 Test Loss: 0.0664243\n",
      "Validation loss decreased (0.039660 --> 0.038860).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0169348\n",
      "\tspeed: 0.1197s/iter; left time: 1499.2094s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150197\n",
      "\tspeed: 0.0498s/iter; left time: 618.6299s\n",
      "\titers: 300, epoch: 7 | loss: 0.0177164\n",
      "\tspeed: 0.0515s/iter; left time: 634.8088s\n",
      "\titers: 400, epoch: 7 | loss: 0.0144806\n",
      "\tspeed: 0.0467s/iter; left time: 571.1652s\n",
      "\titers: 500, epoch: 7 | loss: 0.0147240\n",
      "\tspeed: 0.0456s/iter; left time: 552.4845s\n",
      "\titers: 600, epoch: 7 | loss: 0.0131048\n",
      "\tspeed: 0.0433s/iter; left time: 521.1404s\n",
      "\titers: 700, epoch: 7 | loss: 0.0154541\n",
      "\tspeed: 0.0421s/iter; left time: 502.5001s\n",
      "\titers: 800, epoch: 7 | loss: 0.0132857\n",
      "\tspeed: 0.0407s/iter; left time: 481.6981s\n",
      "\titers: 900, epoch: 7 | loss: 0.0172589\n",
      "\tspeed: 0.0515s/iter; left time: 604.4133s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:42.38s\n",
      "Steps: 902 | Train Loss: 0.0149902 Vali Loss: 0.0377542 Test Loss: 0.0672273\n",
      "Validation loss decreased (0.038860 --> 0.037754).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0124112\n",
      "\tspeed: 0.1281s/iter; left time: 1489.3496s\n",
      "\titers: 200, epoch: 8 | loss: 0.0133382\n",
      "\tspeed: 0.0456s/iter; left time: 525.3094s\n",
      "\titers: 300, epoch: 8 | loss: 0.0144903\n",
      "\tspeed: 0.0450s/iter; left time: 514.1598s\n",
      "\titers: 400, epoch: 8 | loss: 0.0129757\n",
      "\tspeed: 0.0449s/iter; left time: 508.2410s\n",
      "\titers: 500, epoch: 8 | loss: 0.0129781\n",
      "\tspeed: 0.0482s/iter; left time: 540.6267s\n",
      "\titers: 600, epoch: 8 | loss: 0.0131203\n",
      "\tspeed: 0.0509s/iter; left time: 566.6880s\n",
      "\titers: 700, epoch: 8 | loss: 0.0113418\n",
      "\tspeed: 0.0464s/iter; left time: 511.7481s\n",
      "\titers: 800, epoch: 8 | loss: 0.0117694\n",
      "\tspeed: 0.0422s/iter; left time: 460.9609s\n",
      "\titers: 900, epoch: 8 | loss: 0.0125589\n",
      "\tspeed: 0.0458s/iter; left time: 495.9403s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:42.33s\n",
      "Steps: 902 | Train Loss: 0.0136115 Vali Loss: 0.0394928 Test Loss: 0.0707811\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0127657\n",
      "\tspeed: 0.1156s/iter; left time: 1239.8685s\n",
      "\titers: 200, epoch: 9 | loss: 0.0124597\n",
      "\tspeed: 0.0510s/iter; left time: 541.8199s\n",
      "\titers: 300, epoch: 9 | loss: 0.0134299\n",
      "\tspeed: 0.0483s/iter; left time: 508.7619s\n",
      "\titers: 400, epoch: 9 | loss: 0.0110699\n",
      "\tspeed: 0.0440s/iter; left time: 458.2046s\n",
      "\titers: 500, epoch: 9 | loss: 0.0132655\n",
      "\tspeed: 0.0449s/iter; left time: 464.1038s\n",
      "\titers: 600, epoch: 9 | loss: 0.0112603\n",
      "\tspeed: 0.0476s/iter; left time: 486.7437s\n",
      "\titers: 700, epoch: 9 | loss: 0.0130516\n",
      "\tspeed: 0.0503s/iter; left time: 509.4891s\n",
      "\titers: 800, epoch: 9 | loss: 0.0126161\n",
      "\tspeed: 0.0503s/iter; left time: 504.1978s\n",
      "\titers: 900, epoch: 9 | loss: 0.0119859\n",
      "\tspeed: 0.0475s/iter; left time: 471.7056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:43.53s\n",
      "Steps: 902 | Train Loss: 0.0123658 Vali Loss: 0.0383595 Test Loss: 0.0662519\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0107819\n",
      "\tspeed: 0.1182s/iter; left time: 1160.9095s\n",
      "\titers: 200, epoch: 10 | loss: 0.0127241\n",
      "\tspeed: 0.0461s/iter; left time: 448.0598s\n",
      "\titers: 300, epoch: 10 | loss: 0.0123100\n",
      "\tspeed: 0.0513s/iter; left time: 494.1205s\n",
      "\titers: 400, epoch: 10 | loss: 0.0109170\n",
      "\tspeed: 0.0498s/iter; left time: 473.7998s\n",
      "\titers: 500, epoch: 10 | loss: 0.0114692\n",
      "\tspeed: 0.0472s/iter; left time: 444.5972s\n",
      "\titers: 600, epoch: 10 | loss: 0.0110846\n",
      "\tspeed: 0.0486s/iter; left time: 453.0009s\n",
      "\titers: 700, epoch: 10 | loss: 0.0117878\n",
      "\tspeed: 0.0487s/iter; left time: 448.9940s\n",
      "\titers: 800, epoch: 10 | loss: 0.0111483\n",
      "\tspeed: 0.0482s/iter; left time: 439.7616s\n",
      "\titers: 900, epoch: 10 | loss: 0.0121212\n",
      "\tspeed: 0.0502s/iter; left time: 452.7225s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:44.11s\n",
      "Steps: 902 | Train Loss: 0.0112837 Vali Loss: 0.0393359 Test Loss: 0.0676974\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0106947\n",
      "\tspeed: 0.1221s/iter; left time: 1089.5686s\n",
      "\titers: 200, epoch: 11 | loss: 0.0114429\n",
      "\tspeed: 0.0489s/iter; left time: 431.0235s\n",
      "\titers: 300, epoch: 11 | loss: 0.0102906\n",
      "\tspeed: 0.0497s/iter; left time: 433.6481s\n",
      "\titers: 400, epoch: 11 | loss: 0.0102502\n",
      "\tspeed: 0.0472s/iter; left time: 406.8472s\n",
      "\titers: 500, epoch: 11 | loss: 0.0127098\n",
      "\tspeed: 0.0486s/iter; left time: 414.2730s\n",
      "\titers: 600, epoch: 11 | loss: 0.0100624\n",
      "\tspeed: 0.0424s/iter; left time: 357.0879s\n",
      "\titers: 700, epoch: 11 | loss: 0.0098379\n",
      "\tspeed: 0.0435s/iter; left time: 362.1510s\n",
      "\titers: 800, epoch: 11 | loss: 0.0095954\n",
      "\tspeed: 0.0422s/iter; left time: 346.5217s\n",
      "\titers: 900, epoch: 11 | loss: 0.0092578\n",
      "\tspeed: 0.0478s/iter; left time: 387.8455s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:42.46s\n",
      "Steps: 902 | Train Loss: 0.0103650 Vali Loss: 0.0400552 Test Loss: 0.0669268\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0088414\n",
      "\tspeed: 0.1224s/iter; left time: 981.7626s\n",
      "\titers: 200, epoch: 12 | loss: 0.0110274\n",
      "\tspeed: 0.0465s/iter; left time: 368.5353s\n",
      "\titers: 300, epoch: 12 | loss: 0.0086307\n",
      "\tspeed: 0.0481s/iter; left time: 375.9058s\n",
      "\titers: 400, epoch: 12 | loss: 0.0093599\n",
      "\tspeed: 0.0472s/iter; left time: 364.0621s\n",
      "\titers: 500, epoch: 12 | loss: 0.0102409\n",
      "\tspeed: 0.0456s/iter; left time: 347.0510s\n",
      "\titers: 600, epoch: 12 | loss: 0.0096047\n",
      "\tspeed: 0.0482s/iter; left time: 362.3747s\n",
      "\titers: 700, epoch: 12 | loss: 0.0099307\n",
      "\tspeed: 0.0472s/iter; left time: 350.2989s\n",
      "\titers: 800, epoch: 12 | loss: 0.0111330\n",
      "\tspeed: 0.0488s/iter; left time: 357.3821s\n",
      "\titers: 900, epoch: 12 | loss: 0.0091989\n",
      "\tspeed: 0.0491s/iter; left time: 354.2778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:43.41s\n",
      "Steps: 902 | Train Loss: 0.0096406 Vali Loss: 0.0401528 Test Loss: 0.0665900\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.06714829802513123, rmse:0.2591298818588257, mae:0.18294008076190948, rse:0.8983698487281799\n",
      "Intermediate time for GB and pred_len 168: 00h:10m:12.15s\n",
      "Intermediate time for GB: 00h:23m:20.22s\n",
      "Total time: 00h:23m:20.22s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --label_len 48 \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --d_layers {d_layers} \\\n",
    "              --factor 5 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --dec_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --dropout 0.1 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --overlapping_windows \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                informer_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "informer_df = convert_results_into_df(informer_results, if_loss_fnc=False, itr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.1764</td>\n",
       "      <td>0.1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.1601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.2591</td>\n",
       "      <td>0.1829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Iteration     MSE    RMSE     MAE\n",
       "Country Pred_len                                   \n",
       "GB      24                1  0.0311  0.1764  0.1203\n",
       "        96                1  0.0506  0.2250  0.1601\n",
       "        168               1  0.0671  0.2591  0.1829"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'results/informer'\n",
    "\n",
    "informer_df.to_csv(os.path.join(path, 'GB_informer.csv'))\n",
    "informer_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
