{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. TimeLLM](#1-timellm)\n",
    "\n",
    "\n",
    "Results for TimeLLM. I run this code partitionally, but complete results are in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"2\"\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 512\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-01 01:13:46,535] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 01:13:47,624] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 01:13:47,624] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 01:13:47,624] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 01:13:47,724] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 01:13:47,724] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 01:13:48,374] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 01:13:48,375] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 01:13:48,375] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 01:13:48,376] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 01:13:48,377] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 01:13:48,377] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 01:13:48,377] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 01:13:48,377] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 01:13:48,377] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 01:13:48,377] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 01:13:48,644] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 01:13:48,645] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 01:13:48,682] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 192.76 GB, percent = 25.5%\n",
      "[2024-11-01 01:13:48,811] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 01:13:48,811] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 01:13:48,812] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 192.76 GB, percent = 25.5%\n",
      "[2024-11-01 01:13:48,812] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 01:13:48,929] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 01:13:48,930] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 01:13:48,930] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 192.76 GB, percent = 25.5%\n",
      "[2024-11-01 01:13:48,931] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 01:13:48,931] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 01:13:48,931] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 01:13:48,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 01:13:48,932] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 01:13:48,932] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f61b02f71d0>\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 01:13:48,933] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 01:13:48,934] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 01:13:48,935] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1466742\n",
      "\tspeed: 0.1720s/iter; left time: 15348.9424s\n",
      "\titers: 200, epoch: 1 | loss: 0.1377795\n",
      "\tspeed: 0.1311s/iter; left time: 11686.5654s\n",
      "\titers: 300, epoch: 1 | loss: 0.1637115\n",
      "\tspeed: 0.1304s/iter; left time: 11613.8875s\n",
      "\titers: 400, epoch: 1 | loss: 0.1077301\n",
      "\tspeed: 0.1311s/iter; left time: 11662.3304s\n",
      "\titers: 500, epoch: 1 | loss: 0.1226833\n",
      "\tspeed: 0.1490s/iter; left time: 13243.5812s\n",
      "\titers: 600, epoch: 1 | loss: 0.1199201\n",
      "\tspeed: 0.1737s/iter; left time: 15417.1514s\n",
      "\titers: 700, epoch: 1 | loss: 0.1068131\n",
      "\tspeed: 0.1768s/iter; left time: 15673.9475s\n",
      "\titers: 800, epoch: 1 | loss: 0.1092960\n",
      "\tspeed: 0.1741s/iter; left time: 15420.1731s\n",
      "\titers: 900, epoch: 1 | loss: 0.0919815\n",
      "\tspeed: 0.1720s/iter; left time: 15217.7282s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1038064\n",
      "\tspeed: 0.1746s/iter; left time: 15426.0227s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0846845\n",
      "\tspeed: 0.1755s/iter; left time: 15488.9210s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0949691\n",
      "\tspeed: 0.1752s/iter; left time: 15447.5047s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0737247\n",
      "\tspeed: 0.1758s/iter; left time: 15483.7817s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1031198\n",
      "\tspeed: 0.1752s/iter; left time: 15409.8681s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0780963\n",
      "\tspeed: 0.1729s/iter; left time: 15193.3389s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978673\n",
      "\tspeed: 0.1702s/iter; left time: 14935.5128s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1247796\n",
      "\tspeed: 0.1683s/iter; left time: 14752.5401s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062685\n",
      "\tspeed: 0.1733s/iter; left time: 15175.1279s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1049088\n",
      "\tspeed: 0.1714s/iter; left time: 14991.8066s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0870789\n",
      "\tspeed: 0.1742s/iter; left time: 15216.7327s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0814199\n",
      "\tspeed: 0.1740s/iter; left time: 15187.6068s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0885388\n",
      "\tspeed: 0.1741s/iter; left time: 15175.2167s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0876981\n",
      "\tspeed: 0.1725s/iter; left time: 15021.8698s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0750865\n",
      "\tspeed: 0.1877s/iter; left time: 16318.7573s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0770076\n",
      "\tspeed: 0.1822s/iter; left time: 15829.1500s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0930856\n",
      "\tspeed: 0.1850s/iter; left time: 16051.4728s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0914229\n",
      "\tspeed: 0.1959s/iter; left time: 16975.7173s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0742800\n",
      "\tspeed: 0.1348s/iter; left time: 11667.3887s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0699159\n",
      "\tspeed: 0.1304s/iter; left time: 11276.7654s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0888919\n",
      "\tspeed: 0.1307s/iter; left time: 11287.1111s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0732918\n",
      "\tspeed: 0.1297s/iter; left time: 11186.5664s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0881244\n",
      "\tspeed: 0.1321s/iter; left time: 11378.9437s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0822907\n",
      "\tspeed: 0.1372s/iter; left time: 11808.7004s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0753193\n",
      "\tspeed: 0.1238s/iter; left time: 10642.4978s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0816539\n",
      "\tspeed: 0.1299s/iter; left time: 11152.7013s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0759263\n",
      "\tspeed: 0.1258s/iter; left time: 10787.0354s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0969475\n",
      "\tspeed: 0.1373s/iter; left time: 11765.1133s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0810152\n",
      "\tspeed: 0.1361s/iter; left time: 11648.9178s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0943139\n",
      "\tspeed: 0.1311s/iter; left time: 11205.3676s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0698772\n",
      "\tspeed: 0.1250s/iter; left time: 10668.8025s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0704658\n",
      "\tspeed: 0.1298s/iter; left time: 11069.3625s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1035011\n",
      "\tspeed: 0.1351s/iter; left time: 11506.7142s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0949263\n",
      "\tspeed: 0.1367s/iter; left time: 11631.9644s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0885897\n",
      "\tspeed: 0.1369s/iter; left time: 11630.9999s\n",
      "Epoch: 1 cost time: 00h:11m:28.65s\n",
      "Epoch: 1 | Train Loss: 0.0973537 Vali Loss: 0.0928246 Test Loss: 0.1050737\n",
      "Validation loss decreased (inf --> 0.092825).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0701588\n",
      "\tspeed: 1.8397s/iter; left time: 155997.6031s\n",
      "\titers: 200, epoch: 2 | loss: 0.1044648\n",
      "\tspeed: 0.1250s/iter; left time: 10582.6015s\n",
      "\titers: 300, epoch: 2 | loss: 0.0884274\n",
      "\tspeed: 0.1254s/iter; left time: 10605.1158s\n",
      "\titers: 400, epoch: 2 | loss: 0.1147017\n",
      "\tspeed: 0.1229s/iter; left time: 10383.1679s\n",
      "\titers: 500, epoch: 2 | loss: 0.0937675\n",
      "\tspeed: 0.1190s/iter; left time: 10038.6051s\n",
      "\titers: 600, epoch: 2 | loss: 0.0921706\n",
      "\tspeed: 0.1236s/iter; left time: 10418.2877s\n",
      "\titers: 700, epoch: 2 | loss: 0.0930738\n",
      "\tspeed: 0.1196s/iter; left time: 10069.0209s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000150\n",
      "\tspeed: 0.1253s/iter; left time: 10534.2938s\n",
      "\titers: 900, epoch: 2 | loss: 0.0975661\n",
      "\tspeed: 0.1206s/iter; left time: 10128.7696s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0700671\n",
      "\tspeed: 0.1173s/iter; left time: 9844.4580s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0929756\n",
      "\tspeed: 0.1198s/iter; left time: 10037.0371s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0745666\n",
      "\tspeed: 0.1220s/iter; left time: 10210.0538s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0924439\n",
      "\tspeed: 0.1261s/iter; left time: 10538.2378s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1111560\n",
      "\tspeed: 0.1260s/iter; left time: 10518.7261s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0793124\n",
      "\tspeed: 0.1235s/iter; left time: 10302.7127s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0794686\n",
      "\tspeed: 0.1160s/iter; left time: 9666.0832s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0875069\n",
      "\tspeed: 0.1223s/iter; left time: 10177.9038s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0863800\n",
      "\tspeed: 0.1234s/iter; left time: 10251.4920s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0864655\n",
      "\tspeed: 0.1239s/iter; left time: 10280.0741s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0874661\n",
      "\tspeed: 0.1196s/iter; left time: 9915.6803s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0986785\n",
      "\tspeed: 0.1241s/iter; left time: 10272.4187s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0935980\n",
      "\tspeed: 0.1261s/iter; left time: 10429.9927s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0926060\n",
      "\tspeed: 0.1229s/iter; left time: 10149.4789s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731041\n",
      "\tspeed: 0.1211s/iter; left time: 9988.3056s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0878800\n",
      "\tspeed: 0.1199s/iter; left time: 9879.4931s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1000494\n",
      "\tspeed: 0.1190s/iter; left time: 9788.7570s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0909730\n",
      "\tspeed: 0.1166s/iter; left time: 9583.3741s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1120826\n",
      "\tspeed: 0.1257s/iter; left time: 10318.9937s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0753991\n",
      "\tspeed: 0.1164s/iter; left time: 9544.3258s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0918223\n",
      "\tspeed: 0.1264s/iter; left time: 10348.4450s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1169008\n",
      "\tspeed: 0.1260s/iter; left time: 10307.7682s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0799625\n",
      "\tspeed: 0.1254s/iter; left time: 10244.0151s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0924724\n",
      "\tspeed: 0.1149s/iter; left time: 9378.4254s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0893005\n",
      "\tspeed: 0.1160s/iter; left time: 9451.9056s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0870048\n",
      "\tspeed: 0.1199s/iter; left time: 9758.8241s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0749805\n",
      "\tspeed: 0.1230s/iter; left time: 9996.5343s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0773853\n",
      "\tspeed: 0.1240s/iter; left time: 10068.0629s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0819646\n",
      "\tspeed: 0.1187s/iter; left time: 9625.2425s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0823601\n",
      "\tspeed: 0.1251s/iter; left time: 10135.1473s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0686158\n",
      "\tspeed: 0.1269s/iter; left time: 10268.6608s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0982631\n",
      "\tspeed: 0.1246s/iter; left time: 10064.8182s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0787107\n",
      "\tspeed: 0.1241s/iter; left time: 10011.1850s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0879422\n",
      "\tspeed: 0.1264s/iter; left time: 10188.8222s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0896147\n",
      "\tspeed: 0.1248s/iter; left time: 10042.9961s\n",
      "Epoch: 2 cost time: 00h:09m:07.39s\n",
      "Epoch: 2 | Train Loss: 0.0860910 Vali Loss: 0.0922121 Test Loss: 0.1065872\n",
      "Validation loss decreased (0.092825 --> 0.092212).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0907432\n",
      "\tspeed: 1.6384s/iter; left time: 131605.7640s\n",
      "\titers: 200, epoch: 3 | loss: 0.0779589\n",
      "\tspeed: 0.1243s/iter; left time: 9973.0253s\n",
      "\titers: 300, epoch: 3 | loss: 0.0885364\n",
      "\tspeed: 0.1280s/iter; left time: 10258.0784s\n",
      "\titers: 400, epoch: 3 | loss: 0.0848934\n",
      "\tspeed: 0.1285s/iter; left time: 10280.3919s\n",
      "\titers: 500, epoch: 3 | loss: 0.0842507\n",
      "\tspeed: 0.1276s/iter; left time: 10200.6627s\n",
      "\titers: 600, epoch: 3 | loss: 0.0788545\n",
      "\tspeed: 0.1253s/iter; left time: 10004.4945s\n",
      "\titers: 700, epoch: 3 | loss: 0.0803074\n",
      "\tspeed: 0.1196s/iter; left time: 9535.5871s\n",
      "\titers: 800, epoch: 3 | loss: 0.0902780\n",
      "\tspeed: 0.1201s/iter; left time: 9563.2326s\n",
      "\titers: 900, epoch: 3 | loss: 0.0700542\n",
      "\tspeed: 0.1224s/iter; left time: 9735.4265s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0909660\n",
      "\tspeed: 0.1252s/iter; left time: 9940.5088s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0937846\n",
      "\tspeed: 0.1263s/iter; left time: 10015.3475s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0607558\n",
      "\tspeed: 0.1257s/iter; left time: 9956.2601s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0803862\n",
      "\tspeed: 0.1181s/iter; left time: 9347.7165s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0932646\n",
      "\tspeed: 0.1252s/iter; left time: 9894.9792s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0733327\n",
      "\tspeed: 0.1247s/iter; left time: 9838.8822s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0595024\n",
      "\tspeed: 0.1254s/iter; left time: 9886.6863s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0652814\n",
      "\tspeed: 0.1227s/iter; left time: 9660.1343s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0982525\n",
      "\tspeed: 0.1231s/iter; left time: 9681.7754s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0716447\n",
      "\tspeed: 0.1248s/iter; left time: 9796.1519s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0835190\n",
      "\tspeed: 0.1258s/iter; left time: 9868.0218s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0834547\n",
      "\tspeed: 0.1270s/iter; left time: 9949.3213s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0948817\n",
      "\tspeed: 0.1164s/iter; left time: 9103.7785s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0844729\n",
      "\tspeed: 0.1279s/iter; left time: 9989.2713s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0765860\n",
      "\tspeed: 0.1258s/iter; left time: 9811.6570s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0919627\n",
      "\tspeed: 0.1261s/iter; left time: 9822.4583s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0797675\n",
      "\tspeed: 0.1143s/iter; left time: 8895.3972s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0741416\n",
      "\tspeed: 0.1279s/iter; left time: 9941.6814s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0829531\n",
      "\tspeed: 0.1251s/iter; left time: 9711.3739s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0889192\n",
      "\tspeed: 0.1251s/iter; left time: 9700.0550s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0789738\n",
      "\tspeed: 0.1224s/iter; left time: 9473.5702s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0949245\n",
      "\tspeed: 0.1221s/iter; left time: 9440.9616s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0981605\n",
      "\tspeed: 0.1252s/iter; left time: 9671.0077s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0789458\n",
      "\tspeed: 0.1261s/iter; left time: 9728.1743s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0741549\n",
      "\tspeed: 0.1245s/iter; left time: 9588.6716s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0773597\n",
      "\tspeed: 0.1183s/iter; left time: 9103.3499s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0935745\n",
      "\tspeed: 0.1284s/iter; left time: 9864.8752s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0769829\n",
      "\tspeed: 0.1281s/iter; left time: 9825.4101s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0949859\n",
      "\tspeed: 0.1258s/iter; left time: 9636.4106s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0808080\n",
      "\tspeed: 0.1166s/iter; left time: 8924.0939s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0665414\n",
      "\tspeed: 0.1208s/iter; left time: 9233.4311s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0754559\n",
      "\tspeed: 0.1209s/iter; left time: 9228.2114s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0699142\n",
      "\tspeed: 0.1190s/iter; left time: 9073.9133s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0632910\n",
      "\tspeed: 0.1174s/iter; left time: 8933.8075s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0808305\n",
      "\tspeed: 0.1207s/iter; left time: 9177.2720s\n",
      "Epoch: 3 cost time: 00h:09m:13.14s\n",
      "Epoch: 3 | Train Loss: 0.0838997 Vali Loss: 0.0905142 Test Loss: 0.1035439\n",
      "Validation loss decreased (0.092212 --> 0.090514).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0914321\n",
      "\tspeed: 1.7845s/iter; left time: 135367.9697s\n",
      "\titers: 200, epoch: 4 | loss: 0.0923484\n",
      "\tspeed: 0.1205s/iter; left time: 9127.9675s\n",
      "\titers: 300, epoch: 4 | loss: 0.0886503\n",
      "\tspeed: 0.1287s/iter; left time: 9735.5612s\n",
      "\titers: 400, epoch: 4 | loss: 0.0890385\n",
      "\tspeed: 0.1726s/iter; left time: 13043.6124s\n",
      "\titers: 500, epoch: 4 | loss: 0.0889764\n",
      "\tspeed: 0.1765s/iter; left time: 13315.6297s\n",
      "\titers: 600, epoch: 4 | loss: 0.0682504\n",
      "\tspeed: 0.1695s/iter; left time: 12770.4607s\n",
      "\titers: 700, epoch: 4 | loss: 0.0858520\n",
      "\tspeed: 0.1698s/iter; left time: 12780.1781s\n",
      "\titers: 800, epoch: 4 | loss: 0.0807579\n",
      "\tspeed: 0.1719s/iter; left time: 12917.0464s\n",
      "\titers: 900, epoch: 4 | loss: 0.0771627\n",
      "\tspeed: 0.1724s/iter; left time: 12938.1110s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0891313\n",
      "\tspeed: 0.1711s/iter; left time: 12828.5135s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0808600\n",
      "\tspeed: 0.1704s/iter; left time: 12755.0142s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1001335\n",
      "\tspeed: 0.1724s/iter; left time: 12889.6694s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1131133\n",
      "\tspeed: 0.1690s/iter; left time: 12617.0767s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0778967\n",
      "\tspeed: 0.1738s/iter; left time: 12960.3054s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0659208\n",
      "\tspeed: 0.1754s/iter; left time: 13062.3332s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0821595\n",
      "\tspeed: 0.1746s/iter; left time: 12984.3965s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0919877\n",
      "\tspeed: 0.1753s/iter; left time: 13017.2381s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0923949\n",
      "\tspeed: 0.1717s/iter; left time: 12729.9337s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0738678\n",
      "\tspeed: 0.1756s/iter; left time: 13001.3578s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0819451\n",
      "\tspeed: 0.1712s/iter; left time: 12662.4492s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0814312\n",
      "\tspeed: 0.1725s/iter; left time: 12738.3235s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0888639\n",
      "\tspeed: 0.1821s/iter; left time: 13434.7490s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1003453\n",
      "\tspeed: 0.1787s/iter; left time: 13162.4271s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0999414\n",
      "\tspeed: 0.1737s/iter; left time: 12780.3083s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0742211\n",
      "\tspeed: 0.1878s/iter; left time: 13797.4448s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0718067\n",
      "\tspeed: 0.1666s/iter; left time: 12220.5034s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0911036\n",
      "\tspeed: 0.1212s/iter; left time: 8879.4585s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877281\n",
      "\tspeed: 0.1215s/iter; left time: 8890.9273s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0797919\n",
      "\tspeed: 0.1266s/iter; left time: 9250.0594s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0775673\n",
      "\tspeed: 0.1285s/iter; left time: 9372.9094s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0879129\n",
      "\tspeed: 0.1283s/iter; left time: 9344.0534s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0780287\n",
      "\tspeed: 0.1246s/iter; left time: 9064.0367s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0876071\n",
      "\tspeed: 0.1190s/iter; left time: 8643.3071s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0757971\n",
      "\tspeed: 0.1193s/iter; left time: 8656.3878s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0894305\n",
      "\tspeed: 0.1222s/iter; left time: 8851.0792s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0851934\n",
      "\tspeed: 0.1271s/iter; left time: 9194.9322s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0877569\n",
      "\tspeed: 0.1245s/iter; left time: 8992.5078s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0781913\n",
      "\tspeed: 0.1270s/iter; left time: 9161.1854s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0874085\n",
      "\tspeed: 0.1189s/iter; left time: 8567.3724s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0781264\n",
      "\tspeed: 0.1261s/iter; left time: 9070.3929s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0745134\n",
      "\tspeed: 0.1265s/iter; left time: 9088.8993s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0732169\n",
      "\tspeed: 0.1278s/iter; left time: 9169.5542s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0699919\n",
      "\tspeed: 0.1165s/iter; left time: 8348.1299s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0951408\n",
      "\tspeed: 0.1180s/iter; left time: 8444.8277s\n",
      "Epoch: 4 cost time: 00h:11m:07.66s\n",
      "Epoch: 4 | Train Loss: 0.0825824 Vali Loss: 0.0908698 Test Loss: 0.1047861\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0933834\n",
      "\tspeed: 1.5809s/iter; left time: 112857.3726s\n",
      "\titers: 200, epoch: 5 | loss: 0.0743399\n",
      "\tspeed: 0.1287s/iter; left time: 9176.5420s\n",
      "\titers: 300, epoch: 5 | loss: 0.0725531\n",
      "\tspeed: 0.1256s/iter; left time: 8940.3673s\n",
      "\titers: 400, epoch: 5 | loss: 0.0881155\n",
      "\tspeed: 0.1193s/iter; left time: 8482.6711s\n",
      "\titers: 500, epoch: 5 | loss: 0.0945252\n",
      "\tspeed: 0.1248s/iter; left time: 8856.8884s\n",
      "\titers: 600, epoch: 5 | loss: 0.0865966\n",
      "\tspeed: 0.1248s/iter; left time: 8849.4854s\n",
      "\titers: 700, epoch: 5 | loss: 0.0829508\n",
      "\tspeed: 0.1289s/iter; left time: 9123.6575s\n",
      "\titers: 800, epoch: 5 | loss: 0.0887444\n",
      "\tspeed: 0.1229s/iter; left time: 8691.1113s\n",
      "\titers: 900, epoch: 5 | loss: 0.1051521\n",
      "\tspeed: 0.1201s/iter; left time: 8479.9401s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0809733\n",
      "\tspeed: 0.1213s/iter; left time: 8551.3112s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0754441\n",
      "\tspeed: 0.1210s/iter; left time: 8515.9684s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0971770\n",
      "\tspeed: 0.1260s/iter; left time: 8853.3758s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0774984\n",
      "\tspeed: 0.1166s/iter; left time: 8184.2796s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0693193\n",
      "\tspeed: 0.1253s/iter; left time: 8778.7467s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0622493\n",
      "\tspeed: 0.1241s/iter; left time: 8687.4124s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0728563\n",
      "\tspeed: 0.1194s/iter; left time: 8345.2309s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0667490\n",
      "\tspeed: 0.1237s/iter; left time: 8632.1543s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0948240\n",
      "\tspeed: 0.1240s/iter; left time: 8643.1353s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0625855\n",
      "\tspeed: 0.1237s/iter; left time: 8608.3545s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0731169\n",
      "\tspeed: 0.1184s/iter; left time: 8230.8268s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0691220\n",
      "\tspeed: 0.1185s/iter; left time: 8224.5671s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0799258\n",
      "\tspeed: 0.1181s/iter; left time: 8184.1160s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0892581\n",
      "\tspeed: 0.1237s/iter; left time: 8557.2416s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0849659\n",
      "\tspeed: 0.1247s/iter; left time: 8617.8482s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0802421\n",
      "\tspeed: 0.1268s/iter; left time: 8744.4843s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0774631\n",
      "\tspeed: 0.1246s/iter; left time: 8584.8164s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0812496\n",
      "\tspeed: 0.1194s/iter; left time: 8212.6434s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0752958\n",
      "\tspeed: 0.1158s/iter; left time: 7955.0540s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0816401\n",
      "\tspeed: 0.1073s/iter; left time: 7356.2214s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0708815\n",
      "\tspeed: 0.1225s/iter; left time: 8393.1949s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1038611\n",
      "\tspeed: 0.1237s/iter; left time: 8462.9521s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0952071\n",
      "\tspeed: 0.1139s/iter; left time: 7778.7730s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0748858\n",
      "\tspeed: 0.1165s/iter; left time: 7945.0654s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0783696\n",
      "\tspeed: 0.1253s/iter; left time: 8530.7660s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0848109\n",
      "\tspeed: 0.1268s/iter; left time: 8620.5989s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0840811\n",
      "\tspeed: 0.1270s/iter; left time: 8622.5168s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0691066\n",
      "\tspeed: 0.1197s/iter; left time: 8112.3051s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0793731\n",
      "\tspeed: 0.1260s/iter; left time: 8526.7072s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0673810\n",
      "\tspeed: 0.1276s/iter; left time: 8627.6479s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0856006\n",
      "\tspeed: 0.1283s/iter; left time: 8657.7051s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0628996\n",
      "\tspeed: 0.1236s/iter; left time: 8327.4982s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0903348\n",
      "\tspeed: 0.1198s/iter; left time: 8057.8724s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0866672\n",
      "\tspeed: 0.1236s/iter; left time: 8304.4123s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0803752\n",
      "\tspeed: 0.1273s/iter; left time: 8538.5735s\n",
      "Epoch: 5 cost time: 00h:09m:08.81s\n",
      "Epoch: 5 | Train Loss: 0.0813836 Vali Loss: 0.0900973 Test Loss: 0.1040331\n",
      "Validation loss decreased (0.090514 --> 0.090097).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0863184\n",
      "\tspeed: 1.6468s/iter; left time: 110205.6878s\n",
      "\titers: 200, epoch: 6 | loss: 0.0890369\n",
      "\tspeed: 0.1240s/iter; left time: 8282.9836s\n",
      "\titers: 300, epoch: 6 | loss: 0.0835939\n",
      "\tspeed: 0.1260s/iter; left time: 8407.2603s\n",
      "\titers: 400, epoch: 6 | loss: 0.0703793\n",
      "\tspeed: 0.1282s/iter; left time: 8540.9462s\n",
      "\titers: 500, epoch: 6 | loss: 0.0827197\n",
      "\tspeed: 0.1267s/iter; left time: 8430.6563s\n",
      "\titers: 600, epoch: 6 | loss: 0.0714470\n",
      "\tspeed: 0.1202s/iter; left time: 7983.0282s\n",
      "\titers: 700, epoch: 6 | loss: 0.0747727\n",
      "\tspeed: 0.1253s/iter; left time: 8311.4229s\n",
      "\titers: 800, epoch: 6 | loss: 0.0732784\n",
      "\tspeed: 0.1237s/iter; left time: 8193.5322s\n",
      "\titers: 900, epoch: 6 | loss: 0.0771534\n",
      "\tspeed: 0.1245s/iter; left time: 8229.0321s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0741465\n",
      "\tspeed: 0.1205s/iter; left time: 7956.5036s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0746656\n",
      "\tspeed: 0.1213s/iter; left time: 7996.0624s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0931855\n",
      "\tspeed: 0.1270s/iter; left time: 8360.6905s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0977927\n",
      "\tspeed: 0.1267s/iter; left time: 8329.5985s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0876032\n",
      "\tspeed: 0.1272s/iter; left time: 8346.7622s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0801408\n",
      "\tspeed: 0.1260s/iter; left time: 8256.7073s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721369\n",
      "\tspeed: 0.1277s/iter; left time: 8353.4044s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0922342\n",
      "\tspeed: 0.1279s/iter; left time: 8351.4377s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0814926\n",
      "\tspeed: 0.1248s/iter; left time: 8138.1314s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0799462\n",
      "\tspeed: 0.1195s/iter; left time: 7780.9491s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0895512\n",
      "\tspeed: 0.1301s/iter; left time: 8456.1306s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0853165\n",
      "\tspeed: 0.1288s/iter; left time: 8362.0493s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1018588\n",
      "\tspeed: 0.1284s/iter; left time: 8322.9629s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0753619\n",
      "\tspeed: 0.1235s/iter; left time: 7990.5506s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0675035\n",
      "\tspeed: 0.1303s/iter; left time: 8418.4006s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0995846\n",
      "\tspeed: 0.1285s/iter; left time: 8290.4280s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0759167\n",
      "\tspeed: 0.1251s/iter; left time: 8059.7831s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0709326\n",
      "\tspeed: 0.1229s/iter; left time: 7904.4154s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1127540\n",
      "\tspeed: 0.1243s/iter; left time: 7985.0156s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0813409\n",
      "\tspeed: 0.1256s/iter; left time: 8055.5400s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0786561\n",
      "\tspeed: 0.1249s/iter; left time: 7995.1135s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0826245\n",
      "\tspeed: 0.1231s/iter; left time: 7867.7223s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0986693\n",
      "\tspeed: 0.1262s/iter; left time: 8053.2830s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0849674\n",
      "\tspeed: 0.1282s/iter; left time: 8166.3516s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0744465\n",
      "\tspeed: 0.1275s/iter; left time: 8113.8172s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0913548\n",
      "\tspeed: 0.1244s/iter; left time: 7900.0867s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0656657\n",
      "\tspeed: 0.1278s/iter; left time: 8105.7430s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0794955\n",
      "\tspeed: 0.1291s/iter; left time: 8175.8079s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0760554\n",
      "\tspeed: 0.1251s/iter; left time: 7911.0424s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0779494\n",
      "\tspeed: 0.1106s/iter; left time: 6983.7378s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0917149\n",
      "\tspeed: 0.1165s/iter; left time: 7339.0169s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0724742\n",
      "\tspeed: 0.1165s/iter; left time: 7333.1419s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0769409\n",
      "\tspeed: 0.1162s/iter; left time: 7299.6282s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0667945\n",
      "\tspeed: 0.1145s/iter; left time: 7184.5724s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0704586\n",
      "\tspeed: 0.1219s/iter; left time: 7630.4354s\n",
      "Epoch: 6 cost time: 00h:09m:16.56s\n",
      "Epoch: 6 | Train Loss: 0.0803718 Vali Loss: 0.0905898 Test Loss: 0.1044702\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0694771\n",
      "\tspeed: 1.7982s/iter; left time: 112300.2155s\n",
      "\titers: 200, epoch: 7 | loss: 0.0701124\n",
      "\tspeed: 0.1209s/iter; left time: 7538.2893s\n",
      "\titers: 300, epoch: 7 | loss: 0.0611065\n",
      "\tspeed: 0.1209s/iter; left time: 7528.2949s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853083\n",
      "\tspeed: 0.1209s/iter; left time: 7515.4190s\n",
      "\titers: 500, epoch: 7 | loss: 0.0865817\n",
      "\tspeed: 0.1206s/iter; left time: 7480.9827s\n",
      "\titers: 600, epoch: 7 | loss: 0.0731895\n",
      "\tspeed: 0.1207s/iter; left time: 7474.7410s\n",
      "\titers: 700, epoch: 7 | loss: 0.1010686\n",
      "\tspeed: 0.1211s/iter; left time: 7489.8926s\n",
      "\titers: 800, epoch: 7 | loss: 0.0807473\n",
      "\tspeed: 0.1215s/iter; left time: 7500.0067s\n",
      "\titers: 900, epoch: 7 | loss: 0.0729549\n",
      "\tspeed: 0.1211s/iter; left time: 7467.4319s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0784136\n",
      "\tspeed: 0.1209s/iter; left time: 7440.1999s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0915089\n",
      "\tspeed: 0.1163s/iter; left time: 7145.1830s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0813137\n",
      "\tspeed: 0.1210s/iter; left time: 7424.2265s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0710811\n",
      "\tspeed: 0.1213s/iter; left time: 7431.4419s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674738\n",
      "\tspeed: 0.1212s/iter; left time: 7412.9268s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0731219\n",
      "\tspeed: 0.1209s/iter; left time: 7378.7334s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0795863\n",
      "\tspeed: 0.1209s/iter; left time: 7366.8095s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0768837\n",
      "\tspeed: 0.1152s/iter; left time: 7009.1558s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0973881\n",
      "\tspeed: 0.1211s/iter; left time: 7358.9135s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0784142\n",
      "\tspeed: 0.1210s/iter; left time: 7336.4699s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0736109\n",
      "\tspeed: 0.1209s/iter; left time: 7321.6983s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0847631\n",
      "\tspeed: 0.1210s/iter; left time: 7316.8136s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0700621\n",
      "\tspeed: 0.1212s/iter; left time: 7314.8792s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0826743\n",
      "\tspeed: 0.1100s/iter; left time: 6625.1468s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0855349\n",
      "\tspeed: 0.1199s/iter; left time: 7213.0936s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0894471\n",
      "\tspeed: 0.1207s/iter; left time: 7249.1864s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0709336\n",
      "\tspeed: 0.1206s/iter; left time: 7231.8714s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0635979\n",
      "\tspeed: 0.1209s/iter; left time: 7236.3565s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0727365\n",
      "\tspeed: 0.1204s/iter; left time: 7192.1700s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0891747\n",
      "\tspeed: 0.1210s/iter; left time: 7219.6751s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0914883\n",
      "\tspeed: 0.1206s/iter; left time: 7180.0788s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0926042\n",
      "\tspeed: 0.1197s/iter; left time: 7118.3800s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0622337\n",
      "\tspeed: 0.1205s/iter; left time: 7150.8150s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0735094\n",
      "\tspeed: 0.1214s/iter; left time: 7194.1479s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0951522\n",
      "\tspeed: 0.1210s/iter; left time: 7156.0428s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0720976\n",
      "\tspeed: 0.1208s/iter; left time: 7133.6626s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0690883\n",
      "\tspeed: 0.1202s/iter; left time: 7087.2836s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0841284\n",
      "\tspeed: 0.1194s/iter; left time: 7028.5145s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0866462\n",
      "\tspeed: 0.1210s/iter; left time: 7109.4341s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0677450\n",
      "\tspeed: 0.1198s/iter; left time: 7026.1268s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0808895\n",
      "\tspeed: 0.1212s/iter; left time: 7093.7075s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0839261\n",
      "\tspeed: 0.1209s/iter; left time: 7067.3073s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0723493\n",
      "\tspeed: 0.1192s/iter; left time: 6955.4562s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0775747\n",
      "\tspeed: 0.1204s/iter; left time: 7015.1342s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0767770\n",
      "\tspeed: 0.1201s/iter; left time: 6985.1360s\n",
      "Epoch: 7 cost time: 00h:08m:58.02s\n",
      "Epoch: 7 | Train Loss: 0.0794193 Vali Loss: 0.0901961 Test Loss: 0.1045509\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0895511\n",
      "\tspeed: 1.5212s/iter; left time: 88207.1003s\n",
      "\titers: 200, epoch: 8 | loss: 0.0866904\n",
      "\tspeed: 0.1204s/iter; left time: 6970.8545s\n",
      "\titers: 300, epoch: 8 | loss: 0.0797954\n",
      "\tspeed: 0.1215s/iter; left time: 7018.0655s\n",
      "\titers: 400, epoch: 8 | loss: 0.0820298\n",
      "\tspeed: 0.1207s/iter; left time: 6963.7742s\n",
      "\titers: 500, epoch: 8 | loss: 0.0783670\n",
      "\tspeed: 0.1217s/iter; left time: 7005.3954s\n",
      "\titers: 600, epoch: 8 | loss: 0.0902877\n",
      "\tspeed: 0.1212s/iter; left time: 6966.0031s\n",
      "\titers: 700, epoch: 8 | loss: 0.0648628\n",
      "\tspeed: 0.1219s/iter; left time: 6994.9089s\n",
      "\titers: 800, epoch: 8 | loss: 0.0814682\n",
      "\tspeed: 0.1212s/iter; left time: 6945.0145s\n",
      "\titers: 900, epoch: 8 | loss: 0.0808761\n",
      "\tspeed: 0.1215s/iter; left time: 6947.2596s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0752130\n",
      "\tspeed: 0.1207s/iter; left time: 6890.7967s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0748815\n",
      "\tspeed: 0.1198s/iter; left time: 6826.0007s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0795547\n",
      "\tspeed: 0.1193s/iter; left time: 6787.2986s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0994396\n",
      "\tspeed: 0.1217s/iter; left time: 6912.5380s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0792517\n",
      "\tspeed: 0.1210s/iter; left time: 6860.9223s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0784374\n",
      "\tspeed: 0.1215s/iter; left time: 6875.3035s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0859944\n",
      "\tspeed: 0.1213s/iter; left time: 6854.1435s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0819602\n",
      "\tspeed: 0.1210s/iter; left time: 6823.7614s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0977695\n",
      "\tspeed: 0.1209s/iter; left time: 6806.1989s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0645730\n",
      "\tspeed: 0.1204s/iter; left time: 6765.9107s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0558155\n",
      "\tspeed: 0.1209s/iter; left time: 6781.5572s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0748824\n",
      "\tspeed: 0.1208s/iter; left time: 6763.3492s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0751968\n",
      "\tspeed: 0.1212s/iter; left time: 6773.9601s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0775747\n",
      "\tspeed: 0.1210s/iter; left time: 6750.0608s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0743494\n",
      "\tspeed: 0.1207s/iter; left time: 6723.3804s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0847968\n",
      "\tspeed: 0.1211s/iter; left time: 6728.9237s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0757934\n",
      "\tspeed: 0.1209s/iter; left time: 6707.4027s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0962444\n",
      "\tspeed: 0.1205s/iter; left time: 6676.6244s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0798034\n",
      "\tspeed: 0.1213s/iter; left time: 6704.1033s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0616579\n",
      "\tspeed: 0.1204s/iter; left time: 6643.6597s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0749053\n",
      "\tspeed: 0.1200s/iter; left time: 6610.7084s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0779971\n",
      "\tspeed: 0.1204s/iter; left time: 6618.3640s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0703546\n",
      "\tspeed: 0.1613s/iter; left time: 8852.6133s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0924814\n",
      "\tspeed: 0.1661s/iter; left time: 9102.5012s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0760914\n",
      "\tspeed: 0.1659s/iter; left time: 9073.4357s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0540513\n",
      "\tspeed: 0.1661s/iter; left time: 9068.4956s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0791183\n",
      "\tspeed: 0.1644s/iter; left time: 8957.7017s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0715009\n",
      "\tspeed: 0.1651s/iter; left time: 8978.4116s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0779355\n",
      "\tspeed: 0.1633s/iter; left time: 8865.3051s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0706417\n",
      "\tspeed: 0.1629s/iter; left time: 8825.7888s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0706924\n",
      "\tspeed: 0.1641s/iter; left time: 8873.0921s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0704471\n",
      "\tspeed: 0.1785s/iter; left time: 9634.0532s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0729290\n",
      "\tspeed: 0.2067s/iter; left time: 11136.7974s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0735274\n",
      "\tspeed: 0.1271s/iter; left time: 6834.2211s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0819775\n",
      "\tspeed: 0.1223s/iter; left time: 6564.5279s\n",
      "Epoch: 8 cost time: 00h:09m:55.42s\n",
      "Epoch: 8 | Train Loss: 0.0784829 Vali Loss: 0.0906655 Test Loss: 0.1073768\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0807247\n",
      "\tspeed: 1.5940s/iter; left time: 85304.2874s\n",
      "\titers: 200, epoch: 9 | loss: 0.0876306\n",
      "\tspeed: 0.1179s/iter; left time: 6300.3524s\n",
      "\titers: 300, epoch: 9 | loss: 0.0690702\n",
      "\tspeed: 0.1253s/iter; left time: 6681.5234s\n",
      "\titers: 400, epoch: 9 | loss: 0.0685977\n",
      "\tspeed: 0.1225s/iter; left time: 6521.2911s\n",
      "\titers: 500, epoch: 9 | loss: 0.1049898\n",
      "\tspeed: 0.1206s/iter; left time: 6404.2369s\n",
      "\titers: 600, epoch: 9 | loss: 0.0862811\n",
      "\tspeed: 0.1230s/iter; left time: 6522.3767s\n",
      "\titers: 700, epoch: 9 | loss: 0.0700331\n",
      "\tspeed: 0.1261s/iter; left time: 6673.5633s\n",
      "\titers: 800, epoch: 9 | loss: 0.0694490\n",
      "\tspeed: 0.1274s/iter; left time: 6728.0138s\n",
      "\titers: 900, epoch: 9 | loss: 0.0769776\n",
      "\tspeed: 0.1298s/iter; left time: 6845.0558s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0690558\n",
      "\tspeed: 0.1256s/iter; left time: 6607.6701s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0718805\n",
      "\tspeed: 0.1291s/iter; left time: 6781.6749s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0790950\n",
      "\tspeed: 0.1299s/iter; left time: 6807.3502s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0878323\n",
      "\tspeed: 0.1274s/iter; left time: 6665.4423s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0601362\n",
      "\tspeed: 0.1173s/iter; left time: 6126.5728s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0813463\n",
      "\tspeed: 0.1201s/iter; left time: 6260.5145s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0808004\n",
      "\tspeed: 0.1260s/iter; left time: 6554.7038s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0846742\n",
      "\tspeed: 0.1268s/iter; left time: 6582.9410s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0694577\n",
      "\tspeed: 0.1111s/iter; left time: 5758.8993s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0717922\n",
      "\tspeed: 0.1279s/iter; left time: 6613.0082s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0688320\n",
      "\tspeed: 0.1304s/iter; left time: 6728.5997s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0924541\n",
      "\tspeed: 0.1244s/iter; left time: 6409.6553s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0821416\n",
      "\tspeed: 0.1162s/iter; left time: 5974.0566s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0760271\n",
      "\tspeed: 0.1305s/iter; left time: 6696.3125s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0767931\n",
      "\tspeed: 0.1291s/iter; left time: 6614.2296s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0697705\n",
      "\tspeed: 0.1180s/iter; left time: 6030.9660s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0924100\n",
      "\tspeed: 0.1255s/iter; left time: 6402.4300s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0637605\n",
      "\tspeed: 0.1300s/iter; left time: 6616.9880s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0797227\n",
      "\tspeed: 0.1213s/iter; left time: 6161.8290s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0813500\n",
      "\tspeed: 0.1302s/iter; left time: 6602.2529s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0780612\n",
      "\tspeed: 0.1283s/iter; left time: 6494.6933s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0720700\n",
      "\tspeed: 0.1147s/iter; left time: 5795.3900s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0765664\n",
      "\tspeed: 0.1155s/iter; left time: 5822.8621s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0852109\n",
      "\tspeed: 0.1283s/iter; left time: 6456.4421s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0872862\n",
      "\tspeed: 0.1218s/iter; left time: 6118.5373s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0673751\n",
      "\tspeed: 0.1239s/iter; left time: 6209.2619s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0731839\n",
      "\tspeed: 0.1135s/iter; left time: 5679.0106s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0864574\n",
      "\tspeed: 0.1150s/iter; left time: 5742.7550s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0817628\n",
      "\tspeed: 0.1164s/iter; left time: 5796.6027s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0986147\n",
      "\tspeed: 0.1272s/iter; left time: 6324.3863s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0703890\n",
      "\tspeed: 0.1221s/iter; left time: 6059.2363s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0744173\n",
      "\tspeed: 0.1171s/iter; left time: 5796.8956s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0785395\n",
      "\tspeed: 0.1130s/iter; left time: 5583.3218s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0744364\n",
      "\tspeed: 0.1254s/iter; left time: 6184.5594s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0595877\n",
      "\tspeed: 0.1156s/iter; left time: 5688.7236s\n",
      "Epoch: 9 cost time: 00h:09m:09.99s\n",
      "Epoch: 9 | Train Loss: 0.0773867 Vali Loss: 0.0910278 Test Loss: 0.1069496\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0671151\n",
      "\tspeed: 1.5389s/iter; left time: 75480.2256s\n",
      "\titers: 200, epoch: 10 | loss: 0.0828441\n",
      "\tspeed: 0.1204s/iter; left time: 5895.5535s\n",
      "\titers: 300, epoch: 10 | loss: 0.0783897\n",
      "\tspeed: 0.1096s/iter; left time: 5353.1622s\n",
      "\titers: 400, epoch: 10 | loss: 0.0834453\n",
      "\tspeed: 0.1255s/iter; left time: 6120.1368s\n",
      "\titers: 500, epoch: 10 | loss: 0.0739451\n",
      "\tspeed: 0.1267s/iter; left time: 6162.5482s\n",
      "\titers: 600, epoch: 10 | loss: 0.0635547\n",
      "\tspeed: 0.1131s/iter; left time: 5489.5249s\n",
      "\titers: 700, epoch: 10 | loss: 0.0789368\n",
      "\tspeed: 0.1179s/iter; left time: 5712.6613s\n",
      "\titers: 800, epoch: 10 | loss: 0.0980061\n",
      "\tspeed: 0.1237s/iter; left time: 5980.6908s\n",
      "\titers: 900, epoch: 10 | loss: 0.0827744\n",
      "\tspeed: 0.1120s/iter; left time: 5401.5866s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0858516\n",
      "\tspeed: 0.1206s/iter; left time: 5808.9077s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0609591\n",
      "\tspeed: 0.1193s/iter; left time: 5732.8090s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0810569\n",
      "\tspeed: 0.1074s/iter; left time: 5149.8732s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0937729\n",
      "\tspeed: 0.1254s/iter; left time: 5999.7811s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0835219\n",
      "\tspeed: 0.1246s/iter; left time: 5950.7140s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0743610\n",
      "\tspeed: 0.1144s/iter; left time: 5450.3609s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0712053\n",
      "\tspeed: 0.1164s/iter; left time: 5534.1431s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0691880\n",
      "\tspeed: 0.1250s/iter; left time: 5929.3824s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0734778\n",
      "\tspeed: 0.1118s/iter; left time: 5292.0153s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0740059\n",
      "\tspeed: 0.1257s/iter; left time: 5939.9027s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0803476\n",
      "\tspeed: 0.1137s/iter; left time: 5360.6308s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0573778\n",
      "\tspeed: 0.1160s/iter; left time: 5459.6980s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633182\n",
      "\tspeed: 0.1258s/iter; left time: 5905.7278s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0607398\n",
      "\tspeed: 0.1252s/iter; left time: 5867.4105s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0995448\n",
      "\tspeed: 0.1049s/iter; left time: 4903.4606s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0664814\n",
      "\tspeed: 0.1198s/iter; left time: 5586.6849s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0725956\n",
      "\tspeed: 0.1246s/iter; left time: 5800.8275s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0891252\n",
      "\tspeed: 0.1145s/iter; left time: 5319.7502s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0708456\n",
      "\tspeed: 0.1203s/iter; left time: 5574.2059s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0679720\n",
      "\tspeed: 0.1191s/iter; left time: 5508.8572s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0656617\n",
      "\tspeed: 0.1145s/iter; left time: 5284.7287s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0661887\n",
      "\tspeed: 0.1271s/iter; left time: 5854.3015s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0758295\n",
      "\tspeed: 0.1172s/iter; left time: 5387.0060s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0698776\n",
      "\tspeed: 0.1091s/iter; left time: 5002.7150s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0789215\n",
      "\tspeed: 0.1250s/iter; left time: 5719.3431s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0749588\n",
      "\tspeed: 0.1241s/iter; left time: 5664.3148s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1019792\n",
      "\tspeed: 0.1162s/iter; left time: 5292.5549s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0910538\n",
      "\tspeed: 0.1122s/iter; left time: 5100.5829s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0882741\n",
      "\tspeed: 0.1146s/iter; left time: 5196.1296s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0684295\n",
      "\tspeed: 0.1176s/iter; left time: 5319.9542s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0834578\n",
      "\tspeed: 0.1258s/iter; left time: 5678.2564s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0795520\n",
      "\tspeed: 0.1127s/iter; left time: 5077.6624s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0775932\n",
      "\tspeed: 0.1112s/iter; left time: 4997.2488s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0749520\n",
      "\tspeed: 0.1146s/iter; left time: 5138.2708s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0773880\n",
      "\tspeed: 0.1211s/iter; left time: 5417.2335s\n",
      "Epoch: 10 cost time: 00h:08m:49.31s\n",
      "Epoch: 10 | Train Loss: 0.0764753 Vali Loss: 0.0915059 Test Loss: 0.1088750\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.025583267211914062, rmse:0.1599477082490921, mae:0.10403311252593994, rse:0.5514940619468689\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 24: 02h:01m:51.92s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-01 03:15:39,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 03:15:40,279] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 03:15:40,280] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 03:15:40,280] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 03:15:40,382] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 03:15:40,382] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 03:15:41,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 03:15:41,059] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 03:15:41,059] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 03:15:41,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 03:15:41,061] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 03:15:41,061] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 03:15:41,061] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 03:15:41,061] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 03:15:41,061] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 03:15:41,061] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 03:15:41,369] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 03:15:41,370] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 03:15:41,370] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 94.66 GB, percent = 12.5%\n",
      "[2024-11-01 03:15:41,486] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 03:15:41,487] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 03:15:41,488] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 94.66 GB, percent = 12.5%\n",
      "[2024-11-01 03:15:41,488] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 03:15:41,599] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 03:15:41,600] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 03:15:41,600] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 94.67 GB, percent = 12.5%\n",
      "[2024-11-01 03:15:41,601] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 03:15:41,601] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 03:15:41,601] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 03:15:41,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 03:15:41,602] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 03:15:41,602] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f82fbfd9390>\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 03:15:41,603] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 03:15:41,604] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 03:15:41,605] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688266\n",
      "\tspeed: 0.1575s/iter; left time: 14027.7250s\n",
      "\titers: 200, epoch: 1 | loss: 0.1466758\n",
      "\tspeed: 0.1311s/iter; left time: 11661.9554s\n",
      "\titers: 300, epoch: 1 | loss: 0.1657802\n",
      "\tspeed: 0.1262s/iter; left time: 11207.3371s\n",
      "\titers: 400, epoch: 1 | loss: 0.1365485\n",
      "\tspeed: 0.1167s/iter; left time: 10354.2093s\n",
      "\titers: 500, epoch: 1 | loss: 0.1343239\n",
      "\tspeed: 0.1302s/iter; left time: 11544.6879s\n",
      "\titers: 600, epoch: 1 | loss: 0.1273262\n",
      "\tspeed: 0.1200s/iter; left time: 10623.4214s\n",
      "\titers: 700, epoch: 1 | loss: 0.1142251\n",
      "\tspeed: 0.1274s/iter; left time: 11268.7681s\n",
      "\titers: 800, epoch: 1 | loss: 0.1129106\n",
      "\tspeed: 0.1305s/iter; left time: 11527.7212s\n",
      "\titers: 900, epoch: 1 | loss: 0.1081580\n",
      "\tspeed: 0.1186s/iter; left time: 10464.9944s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1082976\n",
      "\tspeed: 0.1309s/iter; left time: 11535.9366s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1104180\n",
      "\tspeed: 0.1225s/iter; left time: 10788.7691s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1293156\n",
      "\tspeed: 0.1270s/iter; left time: 11165.4803s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1199676\n",
      "\tspeed: 0.1310s/iter; left time: 11503.7325s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0853492\n",
      "\tspeed: 0.1188s/iter; left time: 10422.3720s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1185155\n",
      "\tspeed: 0.1312s/iter; left time: 11494.4435s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1091379\n",
      "\tspeed: 0.1228s/iter; left time: 10747.2676s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1387339\n",
      "\tspeed: 0.1318s/iter; left time: 11527.8781s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1115096\n",
      "\tspeed: 0.1271s/iter; left time: 11097.5765s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1182949\n",
      "\tspeed: 0.1220s/iter; left time: 10647.1327s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1110286\n",
      "\tspeed: 0.1310s/iter; left time: 11412.6244s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1118804\n",
      "\tspeed: 0.1251s/iter; left time: 10885.5706s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1165919\n",
      "\tspeed: 0.1309s/iter; left time: 11377.4827s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1117706\n",
      "\tspeed: 0.1196s/iter; left time: 10384.6279s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1129438\n",
      "\tspeed: 0.1147s/iter; left time: 9951.7706s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1135126\n",
      "\tspeed: 0.1151s/iter; left time: 9968.6266s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1112166\n",
      "\tspeed: 0.1302s/iter; left time: 11265.9500s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1203404\n",
      "\tspeed: 0.1311s/iter; left time: 11335.0066s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1182374\n",
      "\tspeed: 0.1301s/iter; left time: 11235.6324s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1069434\n",
      "\tspeed: 0.1311s/iter; left time: 11305.8249s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1128676\n",
      "\tspeed: 0.1308s/iter; left time: 11267.1700s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1112418\n",
      "\tspeed: 0.1273s/iter; left time: 10948.9477s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1109535\n",
      "\tspeed: 0.1203s/iter; left time: 10337.3559s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1022410\n",
      "\tspeed: 0.1307s/iter; left time: 11217.8769s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1067109\n",
      "\tspeed: 0.1301s/iter; left time: 11151.0671s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1093592\n",
      "\tspeed: 0.1305s/iter; left time: 11176.0767s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1050644\n",
      "\tspeed: 0.1239s/iter; left time: 10595.1182s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1453399\n",
      "\tspeed: 0.1148s/iter; left time: 9809.7097s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1064153\n",
      "\tspeed: 0.1129s/iter; left time: 9631.0546s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1275871\n",
      "\tspeed: 0.1254s/iter; left time: 10689.2496s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1056856\n",
      "\tspeed: 0.1307s/iter; left time: 11127.3856s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1163018\n",
      "\tspeed: 0.1308s/iter; left time: 11127.0224s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1195716\n",
      "\tspeed: 0.1308s/iter; left time: 11107.5034s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1011549\n",
      "\tspeed: 0.1305s/iter; left time: 11070.0554s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1301714\n",
      "\tspeed: 0.1309s/iter; left time: 11092.3154s\n",
      "Epoch: 1 cost time: 00h:09m:22.60s\n",
      "Epoch: 1 | Train Loss: 0.1159376 Vali Loss: 0.1171800 Test Loss: 0.1386980\n",
      "Validation loss decreased (inf --> 0.117180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1323460\n",
      "\tspeed: 1.7094s/iter; left time: 144586.5191s\n",
      "\titers: 200, epoch: 2 | loss: 0.0998212\n",
      "\tspeed: 0.1195s/iter; left time: 10095.7532s\n",
      "\titers: 300, epoch: 2 | loss: 0.1072860\n",
      "\tspeed: 0.1192s/iter; left time: 10055.0768s\n",
      "\titers: 400, epoch: 2 | loss: 0.1047579\n",
      "\tspeed: 0.1188s/iter; left time: 10015.5315s\n",
      "\titers: 500, epoch: 2 | loss: 0.0988569\n",
      "\tspeed: 0.1134s/iter; left time: 9544.0556s\n",
      "\titers: 600, epoch: 2 | loss: 0.1400726\n",
      "\tspeed: 0.1184s/iter; left time: 9956.6535s\n",
      "\titers: 700, epoch: 2 | loss: 0.1279359\n",
      "\tspeed: 0.1191s/iter; left time: 9998.9267s\n",
      "\titers: 800, epoch: 2 | loss: 0.1120330\n",
      "\tspeed: 0.1184s/iter; left time: 9932.6729s\n",
      "\titers: 900, epoch: 2 | loss: 0.1051929\n",
      "\tspeed: 0.1191s/iter; left time: 9979.4613s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1209462\n",
      "\tspeed: 0.1189s/iter; left time: 9950.7809s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1275426\n",
      "\tspeed: 0.1189s/iter; left time: 9941.4127s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1159983\n",
      "\tspeed: 0.1199s/iter; left time: 10010.0811s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1053278\n",
      "\tspeed: 0.1180s/iter; left time: 9838.4378s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0974504\n",
      "\tspeed: 0.1190s/iter; left time: 9912.5643s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0961491\n",
      "\tspeed: 0.1184s/iter; left time: 9850.9704s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1127011\n",
      "\tspeed: 0.1177s/iter; left time: 9781.9858s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1136763\n",
      "\tspeed: 0.1191s/iter; left time: 9883.4609s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1032368\n",
      "\tspeed: 0.1201s/iter; left time: 9958.1969s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0907084\n",
      "\tspeed: 0.1094s/iter; left time: 9058.9709s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1084257\n",
      "\tspeed: 0.1198s/iter; left time: 9908.8980s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1318485\n",
      "\tspeed: 0.1198s/iter; left time: 9891.9861s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1092268\n",
      "\tspeed: 0.1192s/iter; left time: 9831.4260s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0971450\n",
      "\tspeed: 0.1203s/iter; left time: 9909.0084s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1082254\n",
      "\tspeed: 0.1206s/iter; left time: 9920.9096s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1057774\n",
      "\tspeed: 0.1203s/iter; left time: 9883.1622s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1233867\n",
      "\tspeed: 0.1204s/iter; left time: 9881.9210s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1121541\n",
      "\tspeed: 0.1193s/iter; left time: 9781.9166s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1023491\n",
      "\tspeed: 0.1193s/iter; left time: 9767.0038s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1325897\n",
      "\tspeed: 0.1192s/iter; left time: 9749.1633s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0932151\n",
      "\tspeed: 0.1208s/iter; left time: 9868.1327s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1183982\n",
      "\tspeed: 0.1206s/iter; left time: 9840.5462s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1071068\n",
      "\tspeed: 0.1188s/iter; left time: 9681.4000s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0916324\n",
      "\tspeed: 0.1203s/iter; left time: 9793.1775s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0934265\n",
      "\tspeed: 0.1191s/iter; left time: 9682.0646s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1121869\n",
      "\tspeed: 0.1198s/iter; left time: 9726.2179s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1178536\n",
      "\tspeed: 0.1197s/iter; left time: 9706.3992s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1048463\n",
      "\tspeed: 0.1175s/iter; left time: 9513.6224s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1119690\n",
      "\tspeed: 0.1202s/iter; left time: 9722.2820s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1302823\n",
      "\tspeed: 0.1197s/iter; left time: 9666.3038s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1027673\n",
      "\tspeed: 0.1209s/iter; left time: 9753.9588s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1058483\n",
      "\tspeed: 0.1199s/iter; left time: 9659.0734s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1003960\n",
      "\tspeed: 0.1182s/iter; left time: 9511.6928s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0997679\n",
      "\tspeed: 0.1432s/iter; left time: 11511.5047s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1080339\n",
      "\tspeed: 0.1341s/iter; left time: 10766.9849s\n",
      "Epoch: 2 cost time: 00h:08m:54.86s\n",
      "Epoch: 2 | Train Loss: 0.1063434 Vali Loss: 0.1159233 Test Loss: 0.1404574\n",
      "Validation loss decreased (0.117180 --> 0.115923).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0953313\n",
      "\tspeed: 1.4749s/iter; left time: 118179.3408s\n",
      "\titers: 200, epoch: 3 | loss: 0.0960641\n",
      "\tspeed: 0.1141s/iter; left time: 9134.8983s\n",
      "\titers: 300, epoch: 3 | loss: 0.1168496\n",
      "\tspeed: 0.1184s/iter; left time: 9462.1697s\n",
      "\titers: 400, epoch: 3 | loss: 0.1128009\n",
      "\tspeed: 0.1200s/iter; left time: 9581.9245s\n",
      "\titers: 500, epoch: 3 | loss: 0.1003084\n",
      "\tspeed: 0.1202s/iter; left time: 9583.5408s\n",
      "\titers: 600, epoch: 3 | loss: 0.1153115\n",
      "\tspeed: 0.1191s/iter; left time: 9484.1723s\n",
      "\titers: 700, epoch: 3 | loss: 0.1167073\n",
      "\tspeed: 0.1201s/iter; left time: 9553.4735s\n",
      "\titers: 800, epoch: 3 | loss: 0.1098146\n",
      "\tspeed: 0.1201s/iter; left time: 9539.8277s\n",
      "\titers: 900, epoch: 3 | loss: 0.0888241\n",
      "\tspeed: 0.1194s/iter; left time: 9472.3830s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963814\n",
      "\tspeed: 0.1203s/iter; left time: 9529.0400s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1247230\n",
      "\tspeed: 0.1189s/iter; left time: 9410.9381s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1112344\n",
      "\tspeed: 0.1206s/iter; left time: 9531.5513s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1090816\n",
      "\tspeed: 0.1180s/iter; left time: 9314.3980s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1014220\n",
      "\tspeed: 0.1165s/iter; left time: 9180.6156s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1205137\n",
      "\tspeed: 0.1189s/iter; left time: 9362.2127s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0969793\n",
      "\tspeed: 0.1195s/iter; left time: 9392.4116s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1139035\n",
      "\tspeed: 0.1185s/iter; left time: 9307.1855s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1128580\n",
      "\tspeed: 0.1196s/iter; left time: 9382.3632s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1043471\n",
      "\tspeed: 0.1200s/iter; left time: 9395.4582s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0987270\n",
      "\tspeed: 0.1203s/iter; left time: 9409.0469s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0881278\n",
      "\tspeed: 0.1206s/iter; left time: 9423.5255s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1081288\n",
      "\tspeed: 0.1207s/iter; left time: 9418.0676s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976076\n",
      "\tspeed: 0.1191s/iter; left time: 9283.4718s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0961220\n",
      "\tspeed: 0.1199s/iter; left time: 9331.3736s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0965293\n",
      "\tspeed: 0.1174s/iter; left time: 9122.0139s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1095369\n",
      "\tspeed: 0.1161s/iter; left time: 9015.4675s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1212904\n",
      "\tspeed: 0.1127s/iter; left time: 8739.1714s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1003948\n",
      "\tspeed: 0.1206s/iter; left time: 9338.7340s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1081474\n",
      "\tspeed: 0.1042s/iter; left time: 8061.2092s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1015081\n",
      "\tspeed: 0.1070s/iter; left time: 8264.3555s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0989254\n",
      "\tspeed: 0.1555s/iter; left time: 11992.9539s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1039556\n",
      "\tspeed: 0.1541s/iter; left time: 11869.2753s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0903572\n",
      "\tspeed: 0.1196s/iter; left time: 9199.9572s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0759418\n",
      "\tspeed: 0.1216s/iter; left time: 9340.2991s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1176762\n",
      "\tspeed: 0.1202s/iter; left time: 9225.8803s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0941180\n",
      "\tspeed: 0.1194s/iter; left time: 9151.1134s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1044800\n",
      "\tspeed: 0.1174s/iter; left time: 8981.4854s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1119280\n",
      "\tspeed: 0.1162s/iter; left time: 8880.4155s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1174034\n",
      "\tspeed: 0.1197s/iter; left time: 9134.6322s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0906112\n",
      "\tspeed: 0.1202s/iter; left time: 9163.5647s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1110326\n",
      "\tspeed: 0.1207s/iter; left time: 9187.3950s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1211934\n",
      "\tspeed: 0.1201s/iter; left time: 9131.0203s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1134209\n",
      "\tspeed: 0.1160s/iter; left time: 8806.9903s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0881434\n",
      "\tspeed: 0.1209s/iter; left time: 9169.2010s\n",
      "Epoch: 3 cost time: 00h:08m:54.67s\n",
      "Epoch: 3 | Train Loss: 0.1022726 Vali Loss: 0.1180037 Test Loss: 0.1440704\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0873546\n",
      "\tspeed: 1.4664s/iter; left time: 110965.8672s\n",
      "\titers: 200, epoch: 4 | loss: 0.1036950\n",
      "\tspeed: 0.1208s/iter; left time: 9131.8373s\n",
      "\titers: 300, epoch: 4 | loss: 0.0934119\n",
      "\tspeed: 0.1203s/iter; left time: 9080.9209s\n",
      "\titers: 400, epoch: 4 | loss: 0.1056632\n",
      "\tspeed: 0.1200s/iter; left time: 9041.9870s\n",
      "\titers: 500, epoch: 4 | loss: 0.0883508\n",
      "\tspeed: 0.1200s/iter; left time: 9033.3314s\n",
      "\titers: 600, epoch: 4 | loss: 0.0899846\n",
      "\tspeed: 0.1200s/iter; left time: 9016.7647s\n",
      "\titers: 700, epoch: 4 | loss: 0.0998617\n",
      "\tspeed: 0.1198s/iter; left time: 8990.9683s\n",
      "\titers: 800, epoch: 4 | loss: 0.0895698\n",
      "\tspeed: 0.1119s/iter; left time: 8385.5677s\n",
      "\titers: 900, epoch: 4 | loss: 0.1108057\n",
      "\tspeed: 0.1028s/iter; left time: 7695.6481s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1005503\n",
      "\tspeed: 0.1206s/iter; left time: 9016.9386s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0851609\n",
      "\tspeed: 0.1203s/iter; left time: 8983.5150s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0922965\n",
      "\tspeed: 0.1184s/iter; left time: 8829.7198s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0940839\n",
      "\tspeed: 0.1205s/iter; left time: 8974.1140s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0969528\n",
      "\tspeed: 0.1214s/iter; left time: 9027.9595s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0909017\n",
      "\tspeed: 0.1211s/iter; left time: 8994.6806s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0886021\n",
      "\tspeed: 0.1202s/iter; left time: 8918.6701s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0961353\n",
      "\tspeed: 0.1192s/iter; left time: 8826.5497s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0882971\n",
      "\tspeed: 0.1193s/iter; left time: 8827.9789s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0977799\n",
      "\tspeed: 0.1206s/iter; left time: 8906.6594s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0880991\n",
      "\tspeed: 0.1205s/iter; left time: 8887.7536s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0910579\n",
      "\tspeed: 0.1210s/iter; left time: 8911.9773s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0977751\n",
      "\tspeed: 0.1170s/iter; left time: 8604.0962s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0931557\n",
      "\tspeed: 0.1198s/iter; left time: 8799.9978s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1089414\n",
      "\tspeed: 0.1203s/iter; left time: 8825.1518s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1060289\n",
      "\tspeed: 0.1206s/iter; left time: 8836.8375s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0834869\n",
      "\tspeed: 0.1204s/iter; left time: 8806.9357s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1122872\n",
      "\tspeed: 0.1202s/iter; left time: 8783.7626s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0878354\n",
      "\tspeed: 0.1203s/iter; left time: 8781.8364s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0907886\n",
      "\tspeed: 0.1205s/iter; left time: 8780.7849s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0977618\n",
      "\tspeed: 0.1203s/iter; left time: 8753.5562s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0923466\n",
      "\tspeed: 0.1210s/iter; left time: 8794.0598s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0991563\n",
      "\tspeed: 0.1208s/iter; left time: 8768.5507s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1049987\n",
      "\tspeed: 0.1179s/iter; left time: 8543.1184s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0708893\n",
      "\tspeed: 0.1199s/iter; left time: 8678.8664s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0792143\n",
      "\tspeed: 0.1207s/iter; left time: 8723.7831s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0895646\n",
      "\tspeed: 0.1204s/iter; left time: 8691.0389s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1049187\n",
      "\tspeed: 0.1090s/iter; left time: 7857.3492s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0842904\n",
      "\tspeed: 0.1201s/iter; left time: 8640.4468s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0941178\n",
      "\tspeed: 0.1195s/iter; left time: 8584.9143s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1109897\n",
      "\tspeed: 0.1204s/iter; left time: 8640.2160s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1001586\n",
      "\tspeed: 0.1204s/iter; left time: 8631.5231s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1093832\n",
      "\tspeed: 0.1193s/iter; left time: 8534.7834s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0876529\n",
      "\tspeed: 0.1214s/iter; left time: 8674.4360s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952323\n",
      "\tspeed: 0.1204s/iter; left time: 8591.1841s\n",
      "Epoch: 4 cost time: 00h:08m:52.29s\n",
      "Epoch: 4 | Train Loss: 0.0973941 Vali Loss: 0.1181420 Test Loss: 0.1481285\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0899206\n",
      "\tspeed: 1.4706s/iter; left time: 104726.4150s\n",
      "\titers: 200, epoch: 5 | loss: 0.1038604\n",
      "\tspeed: 0.1200s/iter; left time: 8533.3901s\n",
      "\titers: 300, epoch: 5 | loss: 0.1103195\n",
      "\tspeed: 0.1129s/iter; left time: 8018.8887s\n",
      "\titers: 400, epoch: 5 | loss: 0.0988987\n",
      "\tspeed: 0.1047s/iter; left time: 7425.3580s\n",
      "\titers: 500, epoch: 5 | loss: 0.1051342\n",
      "\tspeed: 0.1194s/iter; left time: 8454.0719s\n",
      "\titers: 600, epoch: 5 | loss: 0.0994484\n",
      "\tspeed: 0.1204s/iter; left time: 8516.0204s\n",
      "\titers: 700, epoch: 5 | loss: 0.0807362\n",
      "\tspeed: 0.1187s/iter; left time: 8379.2261s\n",
      "\titers: 800, epoch: 5 | loss: 0.1006454\n",
      "\tspeed: 0.1196s/iter; left time: 8432.8799s\n",
      "\titers: 900, epoch: 5 | loss: 0.1117228\n",
      "\tspeed: 0.1183s/iter; left time: 8329.6994s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0958334\n",
      "\tspeed: 0.1198s/iter; left time: 8422.4688s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0945488\n",
      "\tspeed: 0.1204s/iter; left time: 8453.1215s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0999299\n",
      "\tspeed: 0.1191s/iter; left time: 8349.4524s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0833874\n",
      "\tspeed: 0.1166s/iter; left time: 8166.5798s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0800731\n",
      "\tspeed: 0.1130s/iter; left time: 7903.1613s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0873000\n",
      "\tspeed: 0.1197s/iter; left time: 8356.3277s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0974982\n",
      "\tspeed: 0.1198s/iter; left time: 8352.9471s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0832424\n",
      "\tspeed: 0.1187s/iter; left time: 8260.3997s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0964874\n",
      "\tspeed: 0.1192s/iter; left time: 8282.9744s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1015488\n",
      "\tspeed: 0.1194s/iter; left time: 8287.9705s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0962706\n",
      "\tspeed: 0.1193s/iter; left time: 8271.0642s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0944651\n",
      "\tspeed: 0.1206s/iter; left time: 8346.2935s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0883529\n",
      "\tspeed: 0.1199s/iter; left time: 8290.0272s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0954471\n",
      "\tspeed: 0.1188s/iter; left time: 8195.5945s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0827175\n",
      "\tspeed: 0.1209s/iter; left time: 8334.5616s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1006347\n",
      "\tspeed: 0.1178s/iter; left time: 8105.8780s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0951255\n",
      "\tspeed: 0.1197s/iter; left time: 8222.8002s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1033527\n",
      "\tspeed: 0.1204s/iter; left time: 8260.3845s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0756043\n",
      "\tspeed: 0.1163s/iter; left time: 7968.6201s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0850169\n",
      "\tspeed: 0.1187s/iter; left time: 8120.8543s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1090293\n",
      "\tspeed: 0.1196s/iter; left time: 8167.7409s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0975026\n",
      "\tspeed: 0.1199s/iter; left time: 8181.5988s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0954872\n",
      "\tspeed: 0.1187s/iter; left time: 8087.5449s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0834685\n",
      "\tspeed: 0.1203s/iter; left time: 8179.5385s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0841382\n",
      "\tspeed: 0.1204s/iter; left time: 8177.8860s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0745072\n",
      "\tspeed: 0.1192s/iter; left time: 8084.6962s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1053615\n",
      "\tspeed: 0.1205s/iter; left time: 8158.0915s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0791975\n",
      "\tspeed: 0.1213s/iter; left time: 8200.6674s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0998375\n",
      "\tspeed: 0.1203s/iter; left time: 8124.2779s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0949914\n",
      "\tspeed: 0.1201s/iter; left time: 8095.8593s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0968361\n",
      "\tspeed: 0.1206s/iter; left time: 8117.6828s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0942416\n",
      "\tspeed: 0.1200s/iter; left time: 8063.4474s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1008289\n",
      "\tspeed: 0.1210s/iter; left time: 8119.0893s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0779671\n",
      "\tspeed: 0.1203s/iter; left time: 8060.7706s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0973403\n",
      "\tspeed: 0.1206s/iter; left time: 8068.7534s\n",
      "Epoch: 5 cost time: 00h:08m:50.76s\n",
      "Epoch: 5 | Train Loss: 0.0930669 Vali Loss: 0.1177695 Test Loss: 0.1494393\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0950565\n",
      "\tspeed: 1.4622s/iter; left time: 97612.0649s\n",
      "\titers: 200, epoch: 6 | loss: 0.0880194\n",
      "\tspeed: 0.1204s/iter; left time: 8026.5113s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845609\n",
      "\tspeed: 0.1156s/iter; left time: 7694.1706s\n",
      "\titers: 400, epoch: 6 | loss: 0.0856797\n",
      "\tspeed: 0.1174s/iter; left time: 7800.8841s\n",
      "\titers: 500, epoch: 6 | loss: 0.0811084\n",
      "\tspeed: 0.1203s/iter; left time: 7985.1788s\n",
      "\titers: 600, epoch: 6 | loss: 0.1066922\n",
      "\tspeed: 0.1205s/iter; left time: 7987.0147s\n",
      "\titers: 700, epoch: 6 | loss: 0.0946200\n",
      "\tspeed: 0.1204s/iter; left time: 7965.1051s\n",
      "\titers: 800, epoch: 6 | loss: 0.1000760\n",
      "\tspeed: 0.1203s/iter; left time: 7945.8868s\n",
      "\titers: 900, epoch: 6 | loss: 0.0695545\n",
      "\tspeed: 0.1019s/iter; left time: 6722.8686s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1009072\n",
      "\tspeed: 0.1085s/iter; left time: 7143.8240s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0820811\n",
      "\tspeed: 0.1197s/iter; left time: 7870.3069s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1012075\n",
      "\tspeed: 0.1202s/iter; left time: 7888.6042s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0863402\n",
      "\tspeed: 0.1106s/iter; left time: 7252.1631s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0826545\n",
      "\tspeed: 0.1201s/iter; left time: 7863.2727s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0777465\n",
      "\tspeed: 0.1194s/iter; left time: 7806.4292s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1057802\n",
      "\tspeed: 0.1206s/iter; left time: 7867.1432s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0942861\n",
      "\tspeed: 0.1203s/iter; left time: 7839.3424s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0920153\n",
      "\tspeed: 0.1203s/iter; left time: 7826.2045s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0782236\n",
      "\tspeed: 0.1194s/iter; left time: 7755.3000s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0907045\n",
      "\tspeed: 0.1198s/iter; left time: 7766.6119s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0759282\n",
      "\tspeed: 0.1202s/iter; left time: 7781.8083s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1017066\n",
      "\tspeed: 0.1197s/iter; left time: 7738.4182s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0679502\n",
      "\tspeed: 0.1206s/iter; left time: 7782.3964s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0776927\n",
      "\tspeed: 0.1170s/iter; left time: 7540.0842s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0888472\n",
      "\tspeed: 0.1206s/iter; left time: 7761.8660s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0768875\n",
      "\tspeed: 0.1207s/iter; left time: 7752.5761s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0782935\n",
      "\tspeed: 0.1201s/iter; left time: 7703.7262s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0903737\n",
      "\tspeed: 0.1206s/iter; left time: 7726.5885s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1004810\n",
      "\tspeed: 0.1199s/iter; left time: 7665.9887s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0948790\n",
      "\tspeed: 0.1196s/iter; left time: 7634.2215s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0886455\n",
      "\tspeed: 0.1193s/iter; left time: 7604.0567s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0854608\n",
      "\tspeed: 0.1187s/iter; left time: 7553.4596s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0769016\n",
      "\tspeed: 0.1201s/iter; left time: 7633.0253s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0817375\n",
      "\tspeed: 0.1205s/iter; left time: 7648.3160s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0888851\n",
      "\tspeed: 0.1208s/iter; left time: 7653.4108s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0727457\n",
      "\tspeed: 0.1206s/iter; left time: 7629.6762s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0903441\n",
      "\tspeed: 0.1199s/iter; left time: 7571.0870s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0950212\n",
      "\tspeed: 0.1203s/iter; left time: 7583.9066s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0870087\n",
      "\tspeed: 0.1201s/iter; left time: 7563.2798s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0871792\n",
      "\tspeed: 0.1198s/iter; left time: 7530.5665s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0932686\n",
      "\tspeed: 0.1189s/iter; left time: 7461.3394s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0874465\n",
      "\tspeed: 0.1190s/iter; left time: 7458.8442s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0889675\n",
      "\tspeed: 0.1196s/iter; left time: 7480.2658s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0926063\n",
      "\tspeed: 0.1193s/iter; left time: 7450.3856s\n",
      "Epoch: 6 cost time: 00h:08m:50.39s\n",
      "Epoch: 6 | Train Loss: 0.0893757 Vali Loss: 0.1196648 Test Loss: 0.1541289\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1048594\n",
      "\tspeed: 1.4631s/iter; left time: 91147.4115s\n",
      "\titers: 200, epoch: 7 | loss: 0.0843566\n",
      "\tspeed: 0.1206s/iter; left time: 7498.7482s\n",
      "\titers: 300, epoch: 7 | loss: 0.0917804\n",
      "\tspeed: 0.1201s/iter; left time: 7458.6207s\n",
      "\titers: 400, epoch: 7 | loss: 0.0795149\n",
      "\tspeed: 0.1198s/iter; left time: 7427.3699s\n",
      "\titers: 500, epoch: 7 | loss: 0.0842911\n",
      "\tspeed: 0.1204s/iter; left time: 7452.7693s\n",
      "\titers: 600, epoch: 7 | loss: 0.0841490\n",
      "\tspeed: 0.1189s/iter; left time: 7348.1628s\n",
      "\titers: 700, epoch: 7 | loss: 0.1009502\n",
      "\tspeed: 0.1201s/iter; left time: 7412.9278s\n",
      "\titers: 800, epoch: 7 | loss: 0.0739675\n",
      "\tspeed: 0.1198s/iter; left time: 7380.0387s\n",
      "\titers: 900, epoch: 7 | loss: 0.0888571\n",
      "\tspeed: 0.1189s/iter; left time: 7312.4949s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0870971\n",
      "\tspeed: 0.1203s/iter; left time: 7386.9845s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0928913\n",
      "\tspeed: 0.1201s/iter; left time: 7362.1718s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0886939\n",
      "\tspeed: 0.1201s/iter; left time: 7351.5318s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0893422\n",
      "\tspeed: 0.1201s/iter; left time: 7339.4798s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0847866\n",
      "\tspeed: 0.1201s/iter; left time: 7323.7716s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0769473\n",
      "\tspeed: 0.1117s/iter; left time: 6799.5656s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0924601\n",
      "\tspeed: 0.1051s/iter; left time: 6388.3946s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0808676\n",
      "\tspeed: 0.1201s/iter; left time: 7292.0577s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0922723\n",
      "\tspeed: 0.1195s/iter; left time: 7240.8977s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0814242\n",
      "\tspeed: 0.1200s/iter; left time: 7259.2358s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0870774\n",
      "\tspeed: 0.1194s/iter; left time: 7209.0168s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0811431\n",
      "\tspeed: 0.1191s/iter; left time: 7182.9229s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0951526\n",
      "\tspeed: 0.1194s/iter; left time: 7185.1179s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0774082\n",
      "\tspeed: 0.1194s/iter; left time: 7174.9913s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0838267\n",
      "\tspeed: 0.1189s/iter; left time: 7136.0462s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0840728\n",
      "\tspeed: 0.1187s/iter; left time: 7109.2061s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0857305\n",
      "\tspeed: 0.1201s/iter; left time: 7183.3663s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0911212\n",
      "\tspeed: 0.1199s/iter; left time: 7155.4635s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0788338\n",
      "\tspeed: 0.1205s/iter; left time: 7180.6713s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0812203\n",
      "\tspeed: 0.1189s/iter; left time: 7074.7929s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0968771\n",
      "\tspeed: 0.1194s/iter; left time: 7094.9477s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0815360\n",
      "\tspeed: 0.1197s/iter; left time: 7098.6095s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0820606\n",
      "\tspeed: 0.1195s/iter; left time: 7076.6017s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0775566\n",
      "\tspeed: 0.1202s/iter; left time: 7104.0119s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1014951\n",
      "\tspeed: 0.1096s/iter; left time: 6464.7371s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0893218\n",
      "\tspeed: 0.1114s/iter; left time: 6561.6180s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0940796\n",
      "\tspeed: 0.1200s/iter; left time: 7057.5390s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0836638\n",
      "\tspeed: 0.1205s/iter; left time: 7071.3557s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1027704\n",
      "\tspeed: 0.1201s/iter; left time: 7036.9115s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0680492\n",
      "\tspeed: 0.1197s/iter; left time: 7000.7407s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1034615\n",
      "\tspeed: 0.1201s/iter; left time: 7014.2495s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0949323\n",
      "\tspeed: 0.1202s/iter; left time: 7007.5574s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0865043\n",
      "\tspeed: 0.1194s/iter; left time: 6948.3949s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0960433\n",
      "\tspeed: 0.1170s/iter; left time: 6795.0164s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0965981\n",
      "\tspeed: 0.1204s/iter; left time: 6983.1908s\n",
      "Epoch: 7 cost time: 00h:08m:50.02s\n",
      "Epoch: 7 | Train Loss: 0.0862290 Vali Loss: 0.1190629 Test Loss: 0.1528910\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04200141876935959, rmse:0.2049424797296524, mae:0.14045733213424683, rse:0.7086868286132812\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 96: 01h:19m:23.87s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-01 04:35:03,380] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 04:35:04,528] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 04:35:04,529] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 04:35:04,529] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 04:35:04,633] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 04:35:04,634] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 04:35:05,322] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 04:35:05,324] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 04:35:05,324] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 04:35:05,325] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 04:35:05,326] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 04:35:05,326] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 04:35:05,326] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 04:35:05,326] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 04:35:05,326] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 04:35:05,326] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 04:35:05,638] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 04:35:05,639] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 04:35:05,662] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.48 GB, percent = 9.9%\n",
      "[2024-11-01 04:35:05,788] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 04:35:05,789] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 04:35:05,790] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.48 GB, percent = 9.9%\n",
      "[2024-11-01 04:35:05,790] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 04:35:05,901] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 04:35:05,902] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 04:35:05,902] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-01 04:35:05,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 04:35:05,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 04:35:05,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 04:35:05,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 04:35:05,904] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 04:35:05,904] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f77c544cf50>\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 04:35:05,905] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 04:35:05,906] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 04:35:05,907] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1482348\n",
      "\tspeed: 0.1744s/iter; left time: 15489.3728s\n",
      "\titers: 200, epoch: 1 | loss: 0.1453139\n",
      "\tspeed: 0.1312s/iter; left time: 11643.9785s\n",
      "\titers: 300, epoch: 1 | loss: 0.1553313\n",
      "\tspeed: 0.1308s/iter; left time: 11590.6256s\n",
      "\titers: 400, epoch: 1 | loss: 0.1554058\n",
      "\tspeed: 0.1312s/iter; left time: 11615.4949s\n",
      "\titers: 500, epoch: 1 | loss: 0.1471658\n",
      "\tspeed: 0.1310s/iter; left time: 11580.9007s\n",
      "\titers: 600, epoch: 1 | loss: 0.1542638\n",
      "\tspeed: 0.1308s/iter; left time: 11553.6236s\n",
      "\titers: 700, epoch: 1 | loss: 0.1336013\n",
      "\tspeed: 0.1310s/iter; left time: 11560.6945s\n",
      "\titers: 800, epoch: 1 | loss: 0.1162374\n",
      "\tspeed: 0.1310s/iter; left time: 11544.3222s\n",
      "\titers: 900, epoch: 1 | loss: 0.1247154\n",
      "\tspeed: 0.1272s/iter; left time: 11198.4237s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1020275\n",
      "\tspeed: 0.1307s/iter; left time: 11495.2002s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1375454\n",
      "\tspeed: 0.1309s/iter; left time: 11492.3153s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1161896\n",
      "\tspeed: 0.1309s/iter; left time: 11481.4712s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1136029\n",
      "\tspeed: 0.1313s/iter; left time: 11502.8437s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1167578\n",
      "\tspeed: 0.1307s/iter; left time: 11437.7022s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1177376\n",
      "\tspeed: 0.1309s/iter; left time: 11440.1571s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1085610\n",
      "\tspeed: 0.1306s/iter; left time: 11407.6720s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1154755\n",
      "\tspeed: 0.1309s/iter; left time: 11415.4095s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0951475\n",
      "\tspeed: 0.1305s/iter; left time: 11371.7059s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1343727\n",
      "\tspeed: 0.1309s/iter; left time: 11395.1983s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1305830\n",
      "\tspeed: 0.1307s/iter; left time: 11363.3162s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1169467\n",
      "\tspeed: 0.1310s/iter; left time: 11369.7142s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1000786\n",
      "\tspeed: 0.1309s/iter; left time: 11349.3600s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0901878\n",
      "\tspeed: 0.1305s/iter; left time: 11300.0338s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1336366\n",
      "\tspeed: 0.1311s/iter; left time: 11340.2752s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1131255\n",
      "\tspeed: 0.1310s/iter; left time: 11319.2071s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1157848\n",
      "\tspeed: 0.1310s/iter; left time: 11307.8601s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1087803\n",
      "\tspeed: 0.1308s/iter; left time: 11273.9040s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1142708\n",
      "\tspeed: 0.1305s/iter; left time: 11241.0230s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1243213\n",
      "\tspeed: 0.1309s/iter; left time: 11256.8672s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0919846\n",
      "\tspeed: 0.1310s/iter; left time: 11254.4900s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1445014\n",
      "\tspeed: 0.1310s/iter; left time: 11239.6106s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0993669\n",
      "\tspeed: 0.1284s/iter; left time: 11004.5878s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1163438\n",
      "\tspeed: 0.1308s/iter; left time: 11200.4105s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1209128\n",
      "\tspeed: 0.1309s/iter; left time: 11194.1330s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0993587\n",
      "\tspeed: 0.1307s/iter; left time: 11167.2156s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1136707\n",
      "\tspeed: 0.1310s/iter; left time: 11173.5448s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1139015\n",
      "\tspeed: 0.1307s/iter; left time: 11141.4003s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1168825\n",
      "\tspeed: 0.1308s/iter; left time: 11134.4358s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059878\n",
      "\tspeed: 0.1311s/iter; left time: 11146.6503s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1127900\n",
      "\tspeed: 0.1264s/iter; left time: 10732.3464s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1130453\n",
      "\tspeed: 0.1310s/iter; left time: 11109.8603s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1120209\n",
      "\tspeed: 0.1307s/iter; left time: 11077.2114s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1165368\n",
      "\tspeed: 0.1230s/iter; left time: 10406.4833s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1169618\n",
      "\tspeed: 0.1286s/iter; left time: 10870.6954s\n",
      "Epoch: 1 cost time: 00h:09m:40.98s\n",
      "Epoch: 1 | Train Loss: 0.1205897 Vali Loss: 0.1209952 Test Loss: 0.1438364\n",
      "Validation loss decreased (inf --> 0.120995).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1197857\n",
      "\tspeed: 1.6567s/iter; left time: 139784.7019s\n",
      "\titers: 200, epoch: 2 | loss: 0.1099122\n",
      "\tspeed: 0.1205s/iter; left time: 10154.5353s\n",
      "\titers: 300, epoch: 2 | loss: 0.1050249\n",
      "\tspeed: 0.1205s/iter; left time: 10146.0552s\n",
      "\titers: 400, epoch: 2 | loss: 0.1127598\n",
      "\tspeed: 0.1205s/iter; left time: 10130.8889s\n",
      "\titers: 500, epoch: 2 | loss: 0.0847004\n",
      "\tspeed: 0.1204s/iter; left time: 10111.1450s\n",
      "\titers: 600, epoch: 2 | loss: 0.1266375\n",
      "\tspeed: 0.1208s/iter; left time: 10128.8954s\n",
      "\titers: 700, epoch: 2 | loss: 0.1209411\n",
      "\tspeed: 0.1182s/iter; left time: 9904.0936s\n",
      "\titers: 800, epoch: 2 | loss: 0.1068922\n",
      "\tspeed: 0.1208s/iter; left time: 10108.1544s\n",
      "\titers: 900, epoch: 2 | loss: 0.0993572\n",
      "\tspeed: 0.1209s/iter; left time: 10106.5541s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1088792\n",
      "\tspeed: 0.1205s/iter; left time: 10060.7808s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1103762\n",
      "\tspeed: 0.1207s/iter; left time: 10062.2417s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1110987\n",
      "\tspeed: 0.1207s/iter; left time: 10052.5846s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0816215\n",
      "\tspeed: 0.1204s/iter; left time: 10014.7473s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1149543\n",
      "\tspeed: 0.1204s/iter; left time: 10006.1057s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1127167\n",
      "\tspeed: 0.1203s/iter; left time: 9984.3990s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1143629\n",
      "\tspeed: 0.1201s/iter; left time: 9955.6109s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1134126\n",
      "\tspeed: 0.1205s/iter; left time: 9977.8425s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1078830\n",
      "\tspeed: 0.1199s/iter; left time: 9912.9619s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0992961\n",
      "\tspeed: 0.1200s/iter; left time: 9909.9458s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1067948\n",
      "\tspeed: 0.1193s/iter; left time: 9837.7826s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1038578\n",
      "\tspeed: 0.1199s/iter; left time: 9877.5514s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177262\n",
      "\tspeed: 0.1052s/iter; left time: 8654.4739s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1152344\n",
      "\tspeed: 0.1019s/iter; left time: 8370.5519s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1077954\n",
      "\tspeed: 0.1082s/iter; left time: 8879.6234s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1127032\n",
      "\tspeed: 0.1193s/iter; left time: 9776.4222s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1118686\n",
      "\tspeed: 0.1203s/iter; left time: 9850.6207s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1096208\n",
      "\tspeed: 0.1202s/iter; left time: 9829.3320s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0965174\n",
      "\tspeed: 0.1206s/iter; left time: 9851.1311s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1007073\n",
      "\tspeed: 0.1207s/iter; left time: 9846.7172s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0923950\n",
      "\tspeed: 0.1071s/iter; left time: 8724.0858s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1115430\n",
      "\tspeed: 0.1022s/iter; left time: 8315.6496s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1126868\n",
      "\tspeed: 0.1034s/iter; left time: 8403.7845s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1034839\n",
      "\tspeed: 0.1206s/iter; left time: 9790.7659s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1078405\n",
      "\tspeed: 0.1207s/iter; left time: 9786.1077s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1037599\n",
      "\tspeed: 0.1209s/iter; left time: 9791.7455s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0952058\n",
      "\tspeed: 0.1208s/iter; left time: 9769.1219s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1051804\n",
      "\tspeed: 0.1209s/iter; left time: 9766.6777s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1044087\n",
      "\tspeed: 0.1207s/iter; left time: 9735.7998s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1066130\n",
      "\tspeed: 0.1202s/iter; left time: 9688.5349s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092677\n",
      "\tspeed: 0.1209s/iter; left time: 9728.5880s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1083696\n",
      "\tspeed: 0.1206s/iter; left time: 9691.7563s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1197105\n",
      "\tspeed: 0.1203s/iter; left time: 9656.6568s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1068081\n",
      "\tspeed: 0.1204s/iter; left time: 9655.2676s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1261329\n",
      "\tspeed: 0.1204s/iter; left time: 9641.9087s\n",
      "Epoch: 2 cost time: 00h:08m:46.40s\n",
      "Epoch: 2 | Train Loss: 0.1098766 Vali Loss: 0.1225332 Test Loss: 0.1477604\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0998430\n",
      "\tspeed: 1.4317s/iter; left time: 114430.4912s\n",
      "\titers: 200, epoch: 3 | loss: 0.1029584\n",
      "\tspeed: 0.1174s/iter; left time: 9370.8466s\n",
      "\titers: 300, epoch: 3 | loss: 0.0965041\n",
      "\tspeed: 0.1203s/iter; left time: 9590.3716s\n",
      "\titers: 400, epoch: 3 | loss: 0.1077358\n",
      "\tspeed: 0.1205s/iter; left time: 9593.7343s\n",
      "\titers: 500, epoch: 3 | loss: 0.1056079\n",
      "\tspeed: 0.1209s/iter; left time: 9617.2400s\n",
      "\titers: 600, epoch: 3 | loss: 0.1190159\n",
      "\tspeed: 0.1206s/iter; left time: 9580.5925s\n",
      "\titers: 700, epoch: 3 | loss: 0.1028953\n",
      "\tspeed: 0.1207s/iter; left time: 9574.9051s\n",
      "\titers: 800, epoch: 3 | loss: 0.1252995\n",
      "\tspeed: 0.1209s/iter; left time: 9575.2761s\n",
      "\titers: 900, epoch: 3 | loss: 0.0910322\n",
      "\tspeed: 0.1206s/iter; left time: 9542.7023s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1096593\n",
      "\tspeed: 0.1206s/iter; left time: 9532.5357s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0958329\n",
      "\tspeed: 0.1205s/iter; left time: 9514.7747s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1051666\n",
      "\tspeed: 0.1203s/iter; left time: 9482.3237s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1145009\n",
      "\tspeed: 0.1209s/iter; left time: 9518.8913s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1089370\n",
      "\tspeed: 0.1209s/iter; left time: 9505.3003s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0982486\n",
      "\tspeed: 0.1207s/iter; left time: 9478.7850s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1116064\n",
      "\tspeed: 0.1203s/iter; left time: 9433.2706s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1152646\n",
      "\tspeed: 0.1203s/iter; left time: 9421.5386s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1033108\n",
      "\tspeed: 0.1208s/iter; left time: 9447.6685s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1160483\n",
      "\tspeed: 0.1199s/iter; left time: 9370.0343s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1133725\n",
      "\tspeed: 0.1022s/iter; left time: 7972.7087s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0946887\n",
      "\tspeed: 0.1019s/iter; left time: 7937.1427s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1068842\n",
      "\tspeed: 0.1165s/iter; left time: 9068.9008s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1091909\n",
      "\tspeed: 0.1202s/iter; left time: 9345.0775s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0988755\n",
      "\tspeed: 0.1210s/iter; left time: 9396.1780s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0938124\n",
      "\tspeed: 0.1208s/iter; left time: 9368.8664s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1091160\n",
      "\tspeed: 0.1207s/iter; left time: 9346.1320s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1095193\n",
      "\tspeed: 0.1204s/iter; left time: 9307.8747s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1060785\n",
      "\tspeed: 0.1204s/iter; left time: 9295.9643s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0896944\n",
      "\tspeed: 0.1204s/iter; left time: 9284.3652s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0934717\n",
      "\tspeed: 0.1201s/iter; left time: 9249.8228s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1221983\n",
      "\tspeed: 0.1204s/iter; left time: 9260.0851s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0963738\n",
      "\tspeed: 0.1202s/iter; left time: 9233.4493s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1190489\n",
      "\tspeed: 0.1205s/iter; left time: 9245.6127s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1036496\n",
      "\tspeed: 0.1205s/iter; left time: 9230.7809s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1124387\n",
      "\tspeed: 0.1206s/iter; left time: 9230.4399s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1170041\n",
      "\tspeed: 0.1207s/iter; left time: 9225.7649s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1089111\n",
      "\tspeed: 0.1208s/iter; left time: 9219.6902s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0951907\n",
      "\tspeed: 0.1207s/iter; left time: 9198.5566s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0979321\n",
      "\tspeed: 0.1207s/iter; left time: 9191.6299s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0999806\n",
      "\tspeed: 0.1198s/iter; left time: 9108.5758s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0940368\n",
      "\tspeed: 0.1203s/iter; left time: 9132.9958s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0999659\n",
      "\tspeed: 0.1208s/iter; left time: 9160.1375s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0951753\n",
      "\tspeed: 0.1197s/iter; left time: 9064.6799s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0947775\n",
      "\tspeed: 0.1207s/iter; left time: 9129.6460s\n",
      "Epoch: 3 cost time: 00h:08m:50.81s\n",
      "Epoch: 3 | Train Loss: 0.1043304 Vali Loss: 0.1245715 Test Loss: 0.1501074\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0944092\n",
      "\tspeed: 1.4326s/iter; left time: 108140.5465s\n",
      "\titers: 200, epoch: 4 | loss: 0.0867483\n",
      "\tspeed: 0.1208s/iter; left time: 9104.9022s\n",
      "\titers: 300, epoch: 4 | loss: 0.1064334\n",
      "\tspeed: 0.1207s/iter; left time: 9085.1943s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956620\n",
      "\tspeed: 0.1202s/iter; left time: 9038.9481s\n",
      "\titers: 500, epoch: 4 | loss: 0.1117885\n",
      "\tspeed: 0.1206s/iter; left time: 9055.4508s\n",
      "\titers: 600, epoch: 4 | loss: 0.1063067\n",
      "\tspeed: 0.1041s/iter; left time: 7809.3217s\n",
      "\titers: 700, epoch: 4 | loss: 0.0915639\n",
      "\tspeed: 0.1019s/iter; left time: 7631.2419s\n",
      "\titers: 800, epoch: 4 | loss: 0.1026455\n",
      "\tspeed: 0.1020s/iter; left time: 7630.4242s\n",
      "\titers: 900, epoch: 4 | loss: 0.1035407\n",
      "\tspeed: 0.1019s/iter; left time: 7613.0623s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1075997\n",
      "\tspeed: 0.1025s/iter; left time: 7641.4298s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0891981\n",
      "\tspeed: 0.1157s/iter; left time: 8614.0021s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1041374\n",
      "\tspeed: 0.1202s/iter; left time: 8944.0972s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1022310\n",
      "\tspeed: 0.1204s/iter; left time: 8944.2167s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1072065\n",
      "\tspeed: 0.1204s/iter; left time: 8930.9287s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0838472\n",
      "\tspeed: 0.1208s/iter; left time: 8950.5413s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0923450\n",
      "\tspeed: 0.1196s/iter; left time: 8848.8683s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1015804\n",
      "\tspeed: 0.1200s/iter; left time: 8867.8344s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1218250\n",
      "\tspeed: 0.1203s/iter; left time: 8876.8024s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1150391\n",
      "\tspeed: 0.1202s/iter; left time: 8855.6591s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1164713\n",
      "\tspeed: 0.1200s/iter; left time: 8832.7249s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1034276\n",
      "\tspeed: 0.1202s/iter; left time: 8830.5942s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0898664\n",
      "\tspeed: 0.1203s/iter; left time: 8827.5469s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0815171\n",
      "\tspeed: 0.1206s/iter; left time: 8835.4872s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1059342\n",
      "\tspeed: 0.1205s/iter; left time: 8817.6487s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1042058\n",
      "\tspeed: 0.1208s/iter; left time: 8827.8275s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0878983\n",
      "\tspeed: 0.1206s/iter; left time: 8798.4687s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0832504\n",
      "\tspeed: 0.1204s/iter; left time: 8774.4080s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0833858\n",
      "\tspeed: 0.1202s/iter; left time: 8747.0164s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1033833\n",
      "\tspeed: 0.1205s/iter; left time: 8758.2982s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0952015\n",
      "\tspeed: 0.1206s/iter; left time: 8754.3077s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0781978\n",
      "\tspeed: 0.1200s/iter; left time: 8696.0727s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1017636\n",
      "\tspeed: 0.1196s/iter; left time: 8658.4880s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1010068\n",
      "\tspeed: 0.1199s/iter; left time: 8663.6760s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1017999\n",
      "\tspeed: 0.1203s/iter; left time: 8685.3873s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0878085\n",
      "\tspeed: 0.1203s/iter; left time: 8674.7251s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0985364\n",
      "\tspeed: 0.1204s/iter; left time: 8668.3959s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0910970\n",
      "\tspeed: 0.1204s/iter; left time: 8654.5903s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0913839\n",
      "\tspeed: 0.1207s/iter; left time: 8662.8658s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918390\n",
      "\tspeed: 0.1199s/iter; left time: 8595.0173s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1008365\n",
      "\tspeed: 0.1201s/iter; left time: 8596.1364s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0926459\n",
      "\tspeed: 0.1206s/iter; left time: 8622.4651s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0991397\n",
      "\tspeed: 0.1197s/iter; left time: 8546.1725s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0954166\n",
      "\tspeed: 0.1205s/iter; left time: 8587.7673s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0878920\n",
      "\tspeed: 0.1206s/iter; left time: 8586.9872s\n",
      "Epoch: 4 cost time: 00h:08m:45.93s\n",
      "Epoch: 4 | Train Loss: 0.0984264 Vali Loss: 0.1281532 Test Loss: 0.1525455\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1027972\n",
      "\tspeed: 1.4323s/iter; left time: 101743.8017s\n",
      "\titers: 200, epoch: 5 | loss: 0.1047456\n",
      "\tspeed: 0.1204s/iter; left time: 8539.9559s\n",
      "\titers: 300, epoch: 5 | loss: 0.0907510\n",
      "\tspeed: 0.1207s/iter; left time: 8549.4724s\n",
      "\titers: 400, epoch: 5 | loss: 0.0943936\n",
      "\tspeed: 0.1208s/iter; left time: 8542.8848s\n",
      "\titers: 500, epoch: 5 | loss: 0.0821496\n",
      "\tspeed: 0.1206s/iter; left time: 8521.0863s\n",
      "\titers: 600, epoch: 5 | loss: 0.0936060\n",
      "\tspeed: 0.1201s/iter; left time: 8470.2102s\n",
      "\titers: 700, epoch: 5 | loss: 0.0996595\n",
      "\tspeed: 0.1208s/iter; left time: 8508.2723s\n",
      "\titers: 800, epoch: 5 | loss: 0.0979459\n",
      "\tspeed: 0.1206s/iter; left time: 8482.4131s\n",
      "\titers: 900, epoch: 5 | loss: 0.1001264\n",
      "\tspeed: 0.1202s/iter; left time: 8445.8160s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1090219\n",
      "\tspeed: 0.1204s/iter; left time: 8447.3211s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1011967\n",
      "\tspeed: 0.1205s/iter; left time: 8440.3294s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0915010\n",
      "\tspeed: 0.1203s/iter; left time: 8414.2945s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1061302\n",
      "\tspeed: 0.1205s/iter; left time: 8418.3765s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1118270\n",
      "\tspeed: 0.1199s/iter; left time: 8359.0774s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0901711\n",
      "\tspeed: 0.1205s/iter; left time: 8389.0106s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0990031\n",
      "\tspeed: 0.1199s/iter; left time: 8340.3583s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1073154\n",
      "\tspeed: 0.1205s/iter; left time: 8370.5610s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0957215\n",
      "\tspeed: 0.1208s/iter; left time: 8375.3749s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0969568\n",
      "\tspeed: 0.1204s/iter; left time: 8337.6589s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1063399\n",
      "\tspeed: 0.1204s/iter; left time: 8324.2537s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1092876\n",
      "\tspeed: 0.1202s/iter; left time: 8296.8827s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0891375\n",
      "\tspeed: 0.1197s/iter; left time: 8251.7040s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0779103\n",
      "\tspeed: 0.1196s/iter; left time: 8235.1469s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0807228\n",
      "\tspeed: 0.1204s/iter; left time: 8276.7003s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0742790\n",
      "\tspeed: 0.1200s/iter; left time: 8237.0216s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0908970\n",
      "\tspeed: 0.1200s/iter; left time: 8222.8659s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1129368\n",
      "\tspeed: 0.1202s/iter; left time: 8228.0971s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0733277\n",
      "\tspeed: 0.1203s/iter; left time: 8220.7651s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1080936\n",
      "\tspeed: 0.1207s/iter; left time: 8237.3385s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0774377\n",
      "\tspeed: 0.1202s/iter; left time: 8188.8571s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0847868\n",
      "\tspeed: 0.1197s/iter; left time: 8142.6788s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0902236\n",
      "\tspeed: 0.1199s/iter; left time: 8145.1479s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0947069\n",
      "\tspeed: 0.1199s/iter; left time: 8136.4458s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0958610\n",
      "\tspeed: 0.1202s/iter; left time: 8141.4599s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0908997\n",
      "\tspeed: 0.1189s/iter; left time: 8041.8481s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0939663\n",
      "\tspeed: 0.1187s/iter; left time: 8019.7759s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0827342\n",
      "\tspeed: 0.1202s/iter; left time: 8108.7739s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1009660\n",
      "\tspeed: 0.1207s/iter; left time: 8127.2106s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0889867\n",
      "\tspeed: 0.1202s/iter; left time: 8080.3905s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0949150\n",
      "\tspeed: 0.1204s/iter; left time: 8080.9788s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1014487\n",
      "\tspeed: 0.1201s/iter; left time: 8050.2590s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0968869\n",
      "\tspeed: 0.1199s/iter; left time: 8023.4169s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0901529\n",
      "\tspeed: 0.1205s/iter; left time: 8056.6744s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0915154\n",
      "\tspeed: 0.1201s/iter; left time: 8012.8253s\n",
      "Epoch: 5 cost time: 00h:08m:54.91s\n",
      "Epoch: 5 | Train Loss: 0.0934047 Vali Loss: 0.1274233 Test Loss: 0.1514446\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0924059\n",
      "\tspeed: 1.4311s/iter; left time: 95295.5926s\n",
      "\titers: 200, epoch: 6 | loss: 0.0953507\n",
      "\tspeed: 0.1173s/iter; left time: 7801.3269s\n",
      "\titers: 300, epoch: 6 | loss: 0.0743414\n",
      "\tspeed: 0.1204s/iter; left time: 7994.2597s\n",
      "\titers: 400, epoch: 6 | loss: 0.0888031\n",
      "\tspeed: 0.1205s/iter; left time: 7991.1817s\n",
      "\titers: 500, epoch: 6 | loss: 0.1043609\n",
      "\tspeed: 0.1204s/iter; left time: 7970.6442s\n",
      "\titers: 600, epoch: 6 | loss: 0.0847825\n",
      "\tspeed: 0.1201s/iter; left time: 7938.0247s\n",
      "\titers: 700, epoch: 6 | loss: 0.1001649\n",
      "\tspeed: 0.1203s/iter; left time: 7940.4229s\n",
      "\titers: 800, epoch: 6 | loss: 0.1008426\n",
      "\tspeed: 0.1203s/iter; left time: 7925.8063s\n",
      "\titers: 900, epoch: 6 | loss: 0.0914951\n",
      "\tspeed: 0.1209s/iter; left time: 7954.7739s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0961552\n",
      "\tspeed: 0.1204s/iter; left time: 7906.1569s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0863231\n",
      "\tspeed: 0.1204s/iter; left time: 7897.5717s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0925186\n",
      "\tspeed: 0.1207s/iter; left time: 7904.2000s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0857735\n",
      "\tspeed: 0.1202s/iter; left time: 7862.4778s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0983386\n",
      "\tspeed: 0.1197s/iter; left time: 7814.0619s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0876124\n",
      "\tspeed: 0.1194s/iter; left time: 7781.5253s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0781590\n",
      "\tspeed: 0.1201s/iter; left time: 7816.9930s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0933763\n",
      "\tspeed: 0.1206s/iter; left time: 7836.4507s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0982791\n",
      "\tspeed: 0.1207s/iter; left time: 7830.7647s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0728418\n",
      "\tspeed: 0.1201s/iter; left time: 7781.1932s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0914764\n",
      "\tspeed: 0.1201s/iter; left time: 7770.7539s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0815687\n",
      "\tspeed: 0.1194s/iter; left time: 7711.6663s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0897236\n",
      "\tspeed: 0.1204s/iter; left time: 7765.1621s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0844050\n",
      "\tspeed: 0.1204s/iter; left time: 7752.0869s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0881767\n",
      "\tspeed: 0.1208s/iter; left time: 7766.4122s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0834182\n",
      "\tspeed: 0.1203s/iter; left time: 7725.0040s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0942059\n",
      "\tspeed: 0.1198s/iter; left time: 7678.0162s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0970327\n",
      "\tspeed: 0.1199s/iter; left time: 7672.3607s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989593\n",
      "\tspeed: 0.1207s/iter; left time: 7713.4506s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0968164\n",
      "\tspeed: 0.1206s/iter; left time: 7692.3892s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0916561\n",
      "\tspeed: 0.1201s/iter; left time: 7649.4173s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0938281\n",
      "\tspeed: 0.1203s/iter; left time: 7650.9546s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0751842\n",
      "\tspeed: 0.1097s/iter; left time: 6966.2813s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0846767\n",
      "\tspeed: 0.1133s/iter; left time: 7180.7382s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0979014\n",
      "\tspeed: 0.1207s/iter; left time: 7639.1621s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0967091\n",
      "\tspeed: 0.1209s/iter; left time: 7642.1549s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0888710\n",
      "\tspeed: 0.1157s/iter; left time: 7296.9831s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0812877\n",
      "\tspeed: 0.1018s/iter; left time: 6413.1401s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0757051\n",
      "\tspeed: 0.1020s/iter; left time: 6414.7247s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0898106\n",
      "\tspeed: 0.1018s/iter; left time: 6392.8255s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1019688\n",
      "\tspeed: 0.1021s/iter; left time: 6397.8908s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0886026\n",
      "\tspeed: 0.1021s/iter; left time: 6390.1358s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0841476\n",
      "\tspeed: 0.1024s/iter; left time: 6397.6305s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0868416\n",
      "\tspeed: 0.1119s/iter; left time: 6980.2972s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0976313\n",
      "\tspeed: 0.1023s/iter; left time: 6373.2837s\n",
      "Epoch: 6 cost time: 00h:08m:37.72s\n",
      "Epoch: 6 | Train Loss: 0.0896105 Vali Loss: 0.1277131 Test Loss: 0.1531950\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04277150332927704, rmse:0.20681272447109222, mae:0.14383640885353088, rse:0.7167763113975525\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 168: 01h:08m:01.96s\n",
      "\n",
      "Intermediate time for GB: 04h:29m:17.75s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-01 05:43:05,597] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 05:43:06,809] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 05:43:06,810] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 05:43:06,810] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 05:43:06,912] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 05:43:06,912] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 05:43:07,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 05:43:07,554] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 05:43:07,554] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 05:43:07,555] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 05:43:07,555] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 05:43:07,555] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 05:43:07,556] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 05:43:07,556] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 05:43:07,556] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 05:43:07,556] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 05:43:07,840] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 05:43:07,841] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 05:43:07,841] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.62 GB, percent = 9.9%\n",
      "[2024-11-01 05:43:08,005] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 05:43:08,006] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 05:43:08,007] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.62 GB, percent = 9.9%\n",
      "[2024-11-01 05:43:08,007] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 05:43:08,137] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 05:43:08,138] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 05:43:08,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.62 GB, percent = 9.9%\n",
      "[2024-11-01 05:43:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 05:43:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 05:43:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 05:43:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 05:43:08,140] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 05:43:08,140] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 05:43:08,140] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 05:43:08,140] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 05:43:08,140] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5e81f8ef90>\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 05:43:08,141] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 05:43:08,142] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 05:43:08,143] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 05:43:08,143] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1790063\n",
      "\tspeed: 0.1624s/iter; left time: 8690.7018s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687252\n",
      "\tspeed: 0.1135s/iter; left time: 6063.5858s\n",
      "\titers: 300, epoch: 1 | loss: 0.1309537\n",
      "\tspeed: 0.1131s/iter; left time: 6030.1079s\n",
      "\titers: 400, epoch: 1 | loss: 0.1245550\n",
      "\tspeed: 0.1135s/iter; left time: 6038.4411s\n",
      "\titers: 500, epoch: 1 | loss: 0.1051500\n",
      "\tspeed: 0.1205s/iter; left time: 6402.1524s\n",
      "\titers: 600, epoch: 1 | loss: 0.0886984\n",
      "\tspeed: 0.1310s/iter; left time: 6945.6594s\n",
      "\titers: 700, epoch: 1 | loss: 0.0941334\n",
      "\tspeed: 0.1308s/iter; left time: 6920.4657s\n",
      "\titers: 800, epoch: 1 | loss: 0.0780938\n",
      "\tspeed: 0.1310s/iter; left time: 6919.9708s\n",
      "\titers: 900, epoch: 1 | loss: 0.0840373\n",
      "\tspeed: 0.1310s/iter; left time: 6905.6371s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0893998\n",
      "\tspeed: 0.1310s/iter; left time: 6891.6378s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0767727\n",
      "\tspeed: 0.1308s/iter; left time: 6869.0145s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0874630\n",
      "\tspeed: 0.1307s/iter; left time: 6852.2000s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0802690\n",
      "\tspeed: 0.1310s/iter; left time: 6856.4015s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0984690\n",
      "\tspeed: 0.1291s/iter; left time: 6740.3281s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0814931\n",
      "\tspeed: 0.1309s/iter; left time: 6821.9413s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0775438\n",
      "\tspeed: 0.1310s/iter; left time: 6815.8779s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801652\n",
      "\tspeed: 0.1310s/iter; left time: 6801.1195s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0743842\n",
      "\tspeed: 0.1308s/iter; left time: 6777.5319s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0854426\n",
      "\tspeed: 0.1314s/iter; left time: 6796.3543s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0780391\n",
      "\tspeed: 0.1310s/iter; left time: 6761.8078s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0724551\n",
      "\tspeed: 0.1307s/iter; left time: 6734.6830s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0770094\n",
      "\tspeed: 0.1310s/iter; left time: 6736.1966s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0864393\n",
      "\tspeed: 0.1311s/iter; left time: 6726.9443s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0777969\n",
      "\tspeed: 0.1310s/iter; left time: 6711.2728s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0745067\n",
      "\tspeed: 0.1307s/iter; left time: 6683.6805s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0927263\n",
      "\tspeed: 0.1310s/iter; left time: 6682.8800s\n",
      "Epoch: 1 cost time: 00h:05m:44.41s\n",
      "Epoch: 1 | Train Loss: 0.0993755 Vali Loss: 0.0662276 Test Loss: 0.0754743\n",
      "Validation loss decreased (inf --> 0.066228).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0936529\n",
      "\tspeed: 1.1660s/iter; left time: 59281.6643s\n",
      "\titers: 200, epoch: 2 | loss: 0.0912237\n",
      "\tspeed: 0.1198s/iter; left time: 6078.1371s\n",
      "\titers: 300, epoch: 2 | loss: 0.0807760\n",
      "\tspeed: 0.1211s/iter; left time: 6132.6657s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864775\n",
      "\tspeed: 0.1209s/iter; left time: 6111.5642s\n",
      "\titers: 500, epoch: 2 | loss: 0.0780736\n",
      "\tspeed: 0.1206s/iter; left time: 6083.3792s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745789\n",
      "\tspeed: 0.1207s/iter; left time: 6074.2224s\n",
      "\titers: 700, epoch: 2 | loss: 0.0697930\n",
      "\tspeed: 0.1209s/iter; left time: 6073.8950s\n",
      "\titers: 800, epoch: 2 | loss: 0.0780450\n",
      "\tspeed: 0.1212s/iter; left time: 6076.2172s\n",
      "\titers: 900, epoch: 2 | loss: 0.0729557\n",
      "\tspeed: 0.1199s/iter; left time: 6001.4255s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0800805\n",
      "\tspeed: 0.1191s/iter; left time: 5945.6363s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0736994\n",
      "\tspeed: 0.1182s/iter; left time: 5890.0203s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0718818\n",
      "\tspeed: 0.1184s/iter; left time: 5891.3732s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0728408\n",
      "\tspeed: 0.1186s/iter; left time: 5889.2283s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0732936\n",
      "\tspeed: 0.1211s/iter; left time: 6001.6212s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0862661\n",
      "\tspeed: 0.1210s/iter; left time: 5979.7951s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0885015\n",
      "\tspeed: 0.1187s/iter; left time: 5854.8532s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0699046\n",
      "\tspeed: 0.1182s/iter; left time: 5819.2530s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0673015\n",
      "\tspeed: 0.1183s/iter; left time: 5812.3248s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0852279\n",
      "\tspeed: 0.1214s/iter; left time: 5954.3076s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0779036\n",
      "\tspeed: 0.1208s/iter; left time: 5912.7780s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0709876\n",
      "\tspeed: 0.1209s/iter; left time: 5905.0322s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0774782\n",
      "\tspeed: 0.1209s/iter; left time: 5891.9709s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0751852\n",
      "\tspeed: 0.1196s/iter; left time: 5817.9519s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0819853\n",
      "\tspeed: 0.1015s/iter; left time: 4926.7577s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0652728\n",
      "\tspeed: 0.1038s/iter; left time: 5026.9045s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0770506\n",
      "\tspeed: 0.1124s/iter; left time: 5434.5683s\n",
      "Epoch: 2 cost time: 00h:05m:16.29s\n",
      "Epoch: 2 | Train Loss: 0.0773266 Vali Loss: 0.0630829 Test Loss: 0.0720199\n",
      "Validation loss decreased (0.066228 --> 0.063083).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0664506\n",
      "\tspeed: 1.0032s/iter; left time: 48313.6279s\n",
      "\titers: 200, epoch: 3 | loss: 0.0780226\n",
      "\tspeed: 0.1209s/iter; left time: 5811.9929s\n",
      "\titers: 300, epoch: 3 | loss: 0.0809838\n",
      "\tspeed: 0.1209s/iter; left time: 5799.4593s\n",
      "\titers: 400, epoch: 3 | loss: 0.0751593\n",
      "\tspeed: 0.1209s/iter; left time: 5786.5951s\n",
      "\titers: 500, epoch: 3 | loss: 0.0690148\n",
      "\tspeed: 0.1214s/iter; left time: 5797.1837s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651948\n",
      "\tspeed: 0.1200s/iter; left time: 5721.0269s\n",
      "\titers: 700, epoch: 3 | loss: 0.0696828\n",
      "\tspeed: 0.1212s/iter; left time: 5766.1125s\n",
      "\titers: 800, epoch: 3 | loss: 0.0909723\n",
      "\tspeed: 0.1209s/iter; left time: 5737.7699s\n",
      "\titers: 900, epoch: 3 | loss: 0.0693156\n",
      "\tspeed: 0.1213s/iter; left time: 5742.6098s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0763046\n",
      "\tspeed: 0.1213s/iter; left time: 5734.0594s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0654815\n",
      "\tspeed: 0.1212s/iter; left time: 5717.1370s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0816431\n",
      "\tspeed: 0.1216s/iter; left time: 5723.7706s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0731011\n",
      "\tspeed: 0.1213s/iter; left time: 5697.6374s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0798532\n",
      "\tspeed: 0.1210s/iter; left time: 5671.0190s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0781588\n",
      "\tspeed: 0.1209s/iter; left time: 5653.9723s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0688082\n",
      "\tspeed: 0.1209s/iter; left time: 5642.6452s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0722890\n",
      "\tspeed: 0.1216s/iter; left time: 5662.5405s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0578647\n",
      "\tspeed: 0.1211s/iter; left time: 5628.1987s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0831098\n",
      "\tspeed: 0.1209s/iter; left time: 5606.5359s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0657302\n",
      "\tspeed: 0.1200s/iter; left time: 5548.9515s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0686098\n",
      "\tspeed: 0.1208s/iter; left time: 5574.4165s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0793400\n",
      "\tspeed: 0.1210s/iter; left time: 5574.9651s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0739723\n",
      "\tspeed: 0.1209s/iter; left time: 5555.0098s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0883555\n",
      "\tspeed: 0.1213s/iter; left time: 5561.7231s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0636316\n",
      "\tspeed: 0.1209s/iter; left time: 5530.1957s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0704639\n",
      "\tspeed: 0.1206s/iter; left time: 5506.7030s\n",
      "Epoch: 3 cost time: 00h:05m:24.85s\n",
      "Epoch: 3 | Train Loss: 0.0731855 Vali Loss: 0.0623539 Test Loss: 0.0720562\n",
      "Validation loss decreased (0.063083 --> 0.062354).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0695748\n",
      "\tspeed: 1.0186s/iter; left time: 46324.1427s\n",
      "\titers: 200, epoch: 4 | loss: 0.0786053\n",
      "\tspeed: 0.1198s/iter; left time: 5435.5754s\n",
      "\titers: 300, epoch: 4 | loss: 0.0584018\n",
      "\tspeed: 0.1187s/iter; left time: 5374.0002s\n",
      "\titers: 400, epoch: 4 | loss: 0.0929034\n",
      "\tspeed: 0.1199s/iter; left time: 5417.3815s\n",
      "\titers: 500, epoch: 4 | loss: 0.0779471\n",
      "\tspeed: 0.1213s/iter; left time: 5468.8569s\n",
      "\titers: 600, epoch: 4 | loss: 0.0695291\n",
      "\tspeed: 0.1193s/iter; left time: 5366.4642s\n",
      "\titers: 700, epoch: 4 | loss: 0.0666811\n",
      "\tspeed: 0.1190s/iter; left time: 5338.7525s\n",
      "\titers: 800, epoch: 4 | loss: 0.0734229\n",
      "\tspeed: 0.1211s/iter; left time: 5420.3805s\n",
      "\titers: 900, epoch: 4 | loss: 0.0612417\n",
      "\tspeed: 0.1208s/iter; left time: 5398.3621s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0748140\n",
      "\tspeed: 0.1201s/iter; left time: 5354.0281s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0748632\n",
      "\tspeed: 0.1018s/iter; left time: 4527.8624s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0688734\n",
      "\tspeed: 0.1019s/iter; left time: 4519.9893s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0741981\n",
      "\tspeed: 0.1162s/iter; left time: 5144.5647s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0658868\n",
      "\tspeed: 0.1207s/iter; left time: 5331.3798s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0664924\n",
      "\tspeed: 0.1209s/iter; left time: 5330.1389s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0765673\n",
      "\tspeed: 0.1204s/iter; left time: 5294.5138s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0755662\n",
      "\tspeed: 0.1186s/iter; left time: 5202.3408s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0777128\n",
      "\tspeed: 0.1213s/iter; left time: 5309.3576s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0851511\n",
      "\tspeed: 0.1204s/iter; left time: 5258.1519s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0781193\n",
      "\tspeed: 0.1202s/iter; left time: 5236.3739s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0825148\n",
      "\tspeed: 0.1209s/iter; left time: 5255.3762s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0718397\n",
      "\tspeed: 0.1200s/iter; left time: 5204.7438s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0678933\n",
      "\tspeed: 0.1209s/iter; left time: 5233.9092s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0727424\n",
      "\tspeed: 0.1208s/iter; left time: 5214.8152s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0754367\n",
      "\tspeed: 0.1191s/iter; left time: 5132.0321s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0694987\n",
      "\tspeed: 0.1200s/iter; left time: 5156.4168s\n",
      "Epoch: 4 cost time: 00h:05m:18.55s\n",
      "Epoch: 4 | Train Loss: 0.0712121 Vali Loss: 0.0611709 Test Loss: 0.0710917\n",
      "Validation loss decreased (0.062354 --> 0.061171).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0630679\n",
      "\tspeed: 1.0193s/iter; left time: 43623.6654s\n",
      "\titers: 200, epoch: 5 | loss: 0.0689823\n",
      "\tspeed: 0.1209s/iter; left time: 5163.3937s\n",
      "\titers: 300, epoch: 5 | loss: 0.0810203\n",
      "\tspeed: 0.1192s/iter; left time: 5076.8151s\n",
      "\titers: 400, epoch: 5 | loss: 0.0599910\n",
      "\tspeed: 0.1207s/iter; left time: 5130.0472s\n",
      "\titers: 500, epoch: 5 | loss: 0.0720650\n",
      "\tspeed: 0.1208s/iter; left time: 5122.3364s\n",
      "\titers: 600, epoch: 5 | loss: 0.0723064\n",
      "\tspeed: 0.1202s/iter; left time: 5083.7421s\n",
      "\titers: 700, epoch: 5 | loss: 0.0714625\n",
      "\tspeed: 0.1202s/iter; left time: 5073.3234s\n",
      "\titers: 800, epoch: 5 | loss: 0.0736922\n",
      "\tspeed: 0.1199s/iter; left time: 5046.8219s\n",
      "\titers: 900, epoch: 5 | loss: 0.0730744\n",
      "\tspeed: 0.1208s/iter; left time: 5073.4437s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0762590\n",
      "\tspeed: 0.1210s/iter; left time: 5068.3875s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0776123\n",
      "\tspeed: 0.1213s/iter; left time: 5069.5665s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0766415\n",
      "\tspeed: 0.1201s/iter; left time: 5005.8368s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0700914\n",
      "\tspeed: 0.1220s/iter; left time: 5074.9620s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0647124\n",
      "\tspeed: 0.1208s/iter; left time: 5011.9821s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0750828\n",
      "\tspeed: 0.1209s/iter; left time: 5005.2576s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0726999\n",
      "\tspeed: 0.1212s/iter; left time: 5007.2251s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0699233\n",
      "\tspeed: 0.1209s/iter; left time: 4981.4279s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0737871\n",
      "\tspeed: 0.1209s/iter; left time: 4967.9873s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0672794\n",
      "\tspeed: 0.1209s/iter; left time: 4955.5860s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0721373\n",
      "\tspeed: 0.1206s/iter; left time: 4932.6615s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0577321\n",
      "\tspeed: 0.1192s/iter; left time: 4861.3750s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0599433\n",
      "\tspeed: 0.1208s/iter; left time: 4916.6219s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0708436\n",
      "\tspeed: 0.1210s/iter; left time: 4913.0122s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0647120\n",
      "\tspeed: 0.1209s/iter; left time: 4894.8729s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0707248\n",
      "\tspeed: 0.1202s/iter; left time: 4854.2203s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0699116\n",
      "\tspeed: 0.1213s/iter; left time: 4888.7165s\n",
      "Epoch: 5 cost time: 00h:05m:24.05s\n",
      "Epoch: 5 | Train Loss: 0.0699275 Vali Loss: 0.0589704 Test Loss: 0.0681652\n",
      "Validation loss decreased (0.061171 --> 0.058970).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0585817\n",
      "\tspeed: 1.0192s/iter; left time: 40886.2149s\n",
      "\titers: 200, epoch: 6 | loss: 0.0743628\n",
      "\tspeed: 0.1189s/iter; left time: 4759.6888s\n",
      "\titers: 300, epoch: 6 | loss: 0.0816830\n",
      "\tspeed: 0.1211s/iter; left time: 4835.6442s\n",
      "\titers: 400, epoch: 6 | loss: 0.0686249\n",
      "\tspeed: 0.1209s/iter; left time: 4814.2293s\n",
      "\titers: 500, epoch: 6 | loss: 0.0658081\n",
      "\tspeed: 0.1211s/iter; left time: 4810.8043s\n",
      "\titers: 600, epoch: 6 | loss: 0.0699370\n",
      "\tspeed: 0.1208s/iter; left time: 4786.1248s\n",
      "\titers: 700, epoch: 6 | loss: 0.0579729\n",
      "\tspeed: 0.1207s/iter; left time: 4770.6786s\n",
      "\titers: 800, epoch: 6 | loss: 0.0594627\n",
      "\tspeed: 0.1212s/iter; left time: 4777.6334s\n",
      "\titers: 900, epoch: 6 | loss: 0.0765333\n",
      "\tspeed: 0.1212s/iter; left time: 4765.4845s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0679004\n",
      "\tspeed: 0.1210s/iter; left time: 4745.3763s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0628908\n",
      "\tspeed: 0.1207s/iter; left time: 4721.8951s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0643912\n",
      "\tspeed: 0.1207s/iter; left time: 4707.3291s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0718672\n",
      "\tspeed: 0.1209s/iter; left time: 4705.7991s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0650968\n",
      "\tspeed: 0.1202s/iter; left time: 4666.9745s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0803147\n",
      "\tspeed: 0.1208s/iter; left time: 4676.7782s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0721307\n",
      "\tspeed: 0.1209s/iter; left time: 4669.6062s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0664947\n",
      "\tspeed: 0.1213s/iter; left time: 4671.1980s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0713947\n",
      "\tspeed: 0.1209s/iter; left time: 4645.8173s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0666180\n",
      "\tspeed: 0.1211s/iter; left time: 4641.0913s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0742579\n",
      "\tspeed: 0.1209s/iter; left time: 4620.1656s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0744588\n",
      "\tspeed: 0.1205s/iter; left time: 4593.3489s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0583553\n",
      "\tspeed: 0.1226s/iter; left time: 4662.3897s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0706729\n",
      "\tspeed: 0.1211s/iter; left time: 4591.5552s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0550006\n",
      "\tspeed: 0.1205s/iter; left time: 4556.9942s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0605696\n",
      "\tspeed: 0.1210s/iter; left time: 4563.7298s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0701861\n",
      "\tspeed: 0.1211s/iter; left time: 4553.7139s\n",
      "Epoch: 6 cost time: 00h:05m:24.48s\n",
      "Epoch: 6 | Train Loss: 0.0689073 Vali Loss: 0.0598308 Test Loss: 0.0697114\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0657653\n",
      "\tspeed: 1.0131s/iter; left time: 37926.2136s\n",
      "\titers: 200, epoch: 7 | loss: 0.0665484\n",
      "\tspeed: 0.1206s/iter; left time: 4503.2273s\n",
      "\titers: 300, epoch: 7 | loss: 0.0624240\n",
      "\tspeed: 0.1204s/iter; left time: 4484.8498s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643224\n",
      "\tspeed: 0.1207s/iter; left time: 4481.9623s\n",
      "\titers: 500, epoch: 7 | loss: 0.0836057\n",
      "\tspeed: 0.1207s/iter; left time: 4471.9528s\n",
      "\titers: 600, epoch: 7 | loss: 0.0660850\n",
      "\tspeed: 0.1210s/iter; left time: 4467.5989s\n",
      "\titers: 700, epoch: 7 | loss: 0.0869880\n",
      "\tspeed: 0.1194s/iter; left time: 4397.0291s\n",
      "\titers: 800, epoch: 7 | loss: 0.0704906\n",
      "\tspeed: 0.1212s/iter; left time: 4453.1179s\n",
      "\titers: 900, epoch: 7 | loss: 0.0647015\n",
      "\tspeed: 0.1209s/iter; left time: 4430.5611s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0806929\n",
      "\tspeed: 0.1209s/iter; left time: 4417.3410s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0759010\n",
      "\tspeed: 0.1204s/iter; left time: 4386.2943s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0615634\n",
      "\tspeed: 0.1207s/iter; left time: 4383.8192s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0764042\n",
      "\tspeed: 0.1183s/iter; left time: 4288.3717s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0674644\n",
      "\tspeed: 0.1039s/iter; left time: 3753.1669s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0740616\n",
      "\tspeed: 0.1043s/iter; left time: 3758.6648s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0792665\n",
      "\tspeed: 0.1040s/iter; left time: 3738.7147s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0690767\n",
      "\tspeed: 0.1042s/iter; left time: 3732.7651s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0598226\n",
      "\tspeed: 0.1077s/iter; left time: 3847.3043s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0678398\n",
      "\tspeed: 0.1063s/iter; left time: 3789.7342s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0747984\n",
      "\tspeed: 0.1045s/iter; left time: 3715.1699s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0712984\n",
      "\tspeed: 0.1041s/iter; left time: 3688.5869s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0657289\n",
      "\tspeed: 0.1077s/iter; left time: 3807.1994s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0730863\n",
      "\tspeed: 0.1216s/iter; left time: 4282.8606s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0687535\n",
      "\tspeed: 0.1212s/iter; left time: 4260.1095s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0626324\n",
      "\tspeed: 0.1212s/iter; left time: 4245.7969s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0694073\n",
      "\tspeed: 0.1207s/iter; left time: 4217.0464s\n",
      "Epoch: 7 cost time: 00h:05m:10.08s\n",
      "Epoch: 7 | Train Loss: 0.0681056 Vali Loss: 0.0597853 Test Loss: 0.0696563\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0955697\n",
      "\tspeed: 0.9884s/iter; left time: 34351.4703s\n",
      "\titers: 200, epoch: 8 | loss: 0.0681316\n",
      "\tspeed: 0.1205s/iter; left time: 4176.8794s\n",
      "\titers: 300, epoch: 8 | loss: 0.0796526\n",
      "\tspeed: 0.1206s/iter; left time: 4168.8573s\n",
      "\titers: 400, epoch: 8 | loss: 0.0672699\n",
      "\tspeed: 0.1206s/iter; left time: 4155.2464s\n",
      "\titers: 500, epoch: 8 | loss: 0.0579451\n",
      "\tspeed: 0.1211s/iter; left time: 4161.6305s\n",
      "\titers: 600, epoch: 8 | loss: 0.0730144\n",
      "\tspeed: 0.1208s/iter; left time: 4138.5426s\n",
      "\titers: 700, epoch: 8 | loss: 0.0561597\n",
      "\tspeed: 0.1208s/iter; left time: 4125.5295s\n",
      "\titers: 800, epoch: 8 | loss: 0.0630030\n",
      "\tspeed: 0.1209s/iter; left time: 4117.2168s\n",
      "\titers: 900, epoch: 8 | loss: 0.0696586\n",
      "\tspeed: 0.1208s/iter; left time: 4100.0669s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0760989\n",
      "\tspeed: 0.1207s/iter; left time: 4085.5731s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0601159\n",
      "\tspeed: 0.1207s/iter; left time: 4072.8285s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0697419\n",
      "\tspeed: 0.1209s/iter; left time: 4069.1088s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0581929\n",
      "\tspeed: 0.1207s/iter; left time: 4049.7350s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0630530\n",
      "\tspeed: 0.1210s/iter; left time: 4047.8329s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0653922\n",
      "\tspeed: 0.1207s/iter; left time: 4024.8266s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0719701\n",
      "\tspeed: 0.1202s/iter; left time: 3996.2554s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0626506\n",
      "\tspeed: 0.1187s/iter; left time: 3934.7344s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0601200\n",
      "\tspeed: 0.1187s/iter; left time: 3922.0110s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0739461\n",
      "\tspeed: 0.1184s/iter; left time: 3902.2806s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0698656\n",
      "\tspeed: 0.1193s/iter; left time: 3921.0538s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0620731\n",
      "\tspeed: 0.1195s/iter; left time: 3912.8978s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0645657\n",
      "\tspeed: 0.1205s/iter; left time: 3933.9129s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0703310\n",
      "\tspeed: 0.1202s/iter; left time: 3912.8405s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0673083\n",
      "\tspeed: 0.1204s/iter; left time: 3906.0553s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0710015\n",
      "\tspeed: 0.1207s/iter; left time: 3906.1880s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0631191\n",
      "\tspeed: 0.1208s/iter; left time: 3896.6290s\n",
      "Epoch: 8 cost time: 00h:05m:21.55s\n",
      "Epoch: 8 | Train Loss: 0.0675166 Vali Loss: 0.0581114 Test Loss: 0.0668070\n",
      "Validation loss decreased (0.058970 --> 0.058111).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0710958\n",
      "\tspeed: 1.0242s/iter; left time: 32850.7383s\n",
      "\titers: 200, epoch: 9 | loss: 0.0710134\n",
      "\tspeed: 0.1202s/iter; left time: 3842.7428s\n",
      "\titers: 300, epoch: 9 | loss: 0.0564387\n",
      "\tspeed: 0.1201s/iter; left time: 3829.0666s\n",
      "\titers: 400, epoch: 9 | loss: 0.0593314\n",
      "\tspeed: 0.1202s/iter; left time: 3819.6406s\n",
      "\titers: 500, epoch: 9 | loss: 0.0549250\n",
      "\tspeed: 0.1214s/iter; left time: 3844.5622s\n",
      "\titers: 600, epoch: 9 | loss: 0.0573312\n",
      "\tspeed: 0.1207s/iter; left time: 3810.4766s\n",
      "\titers: 700, epoch: 9 | loss: 0.0749223\n",
      "\tspeed: 0.1187s/iter; left time: 3736.0753s\n",
      "\titers: 800, epoch: 9 | loss: 0.0616827\n",
      "\tspeed: 0.1209s/iter; left time: 3793.1171s\n",
      "\titers: 900, epoch: 9 | loss: 0.0696181\n",
      "\tspeed: 0.1209s/iter; left time: 3781.8642s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0527223\n",
      "\tspeed: 0.1206s/iter; left time: 3759.6943s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0648112\n",
      "\tspeed: 0.1204s/iter; left time: 3739.9555s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0737804\n",
      "\tspeed: 0.1209s/iter; left time: 3744.7446s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0596698\n",
      "\tspeed: 0.1209s/iter; left time: 3733.3668s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0660387\n",
      "\tspeed: 0.1208s/iter; left time: 3716.3023s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0742668\n",
      "\tspeed: 0.1195s/iter; left time: 3664.2769s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0563254\n",
      "\tspeed: 0.1158s/iter; left time: 3541.6499s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0555071\n",
      "\tspeed: 0.1204s/iter; left time: 3667.4328s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0757507\n",
      "\tspeed: 0.1092s/iter; left time: 3317.3949s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0711249\n",
      "\tspeed: 0.1211s/iter; left time: 3665.2564s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0721804\n",
      "\tspeed: 0.1209s/iter; left time: 3647.8859s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0665950\n",
      "\tspeed: 0.1208s/iter; left time: 3632.0443s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0581865\n",
      "\tspeed: 0.1206s/iter; left time: 3615.4835s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0679084\n",
      "\tspeed: 0.1206s/iter; left time: 3603.1639s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0549888\n",
      "\tspeed: 0.1207s/iter; left time: 3593.8016s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0570390\n",
      "\tspeed: 0.1216s/iter; left time: 3608.9673s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0600057\n",
      "\tspeed: 0.1175s/iter; left time: 3475.3147s\n",
      "Epoch: 9 cost time: 00h:05m:20.52s\n",
      "Epoch: 9 | Train Loss: 0.0669224 Vali Loss: 0.0591406 Test Loss: 0.0683875\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0737776\n",
      "\tspeed: 0.9680s/iter; left time: 28450.2405s\n",
      "\titers: 200, epoch: 10 | loss: 0.0724111\n",
      "\tspeed: 0.1086s/iter; left time: 3180.0918s\n",
      "\titers: 300, epoch: 10 | loss: 0.0616018\n",
      "\tspeed: 0.1188s/iter; left time: 3468.2310s\n",
      "\titers: 400, epoch: 10 | loss: 0.0517321\n",
      "\tspeed: 0.1200s/iter; left time: 3491.8455s\n",
      "\titers: 500, epoch: 10 | loss: 0.0636202\n",
      "\tspeed: 0.1213s/iter; left time: 3517.5174s\n",
      "\titers: 600, epoch: 10 | loss: 0.0686904\n",
      "\tspeed: 0.1209s/iter; left time: 3493.7526s\n",
      "\titers: 700, epoch: 10 | loss: 0.0717656\n",
      "\tspeed: 0.1217s/iter; left time: 3504.7039s\n",
      "\titers: 800, epoch: 10 | loss: 0.0680537\n",
      "\tspeed: 0.1205s/iter; left time: 3457.1965s\n",
      "\titers: 900, epoch: 10 | loss: 0.0653778\n",
      "\tspeed: 0.1204s/iter; left time: 3441.7255s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0478307\n",
      "\tspeed: 0.1202s/iter; left time: 3426.1200s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0693082\n",
      "\tspeed: 0.1209s/iter; left time: 3433.5158s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0719121\n",
      "\tspeed: 0.1213s/iter; left time: 3432.1968s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0745866\n",
      "\tspeed: 0.1220s/iter; left time: 3440.3871s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0660974\n",
      "\tspeed: 0.1208s/iter; left time: 3393.8799s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0666839\n",
      "\tspeed: 0.1211s/iter; left time: 3390.1735s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0883639\n",
      "\tspeed: 0.1209s/iter; left time: 3372.3402s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0611924\n",
      "\tspeed: 0.1209s/iter; left time: 3360.3830s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0584188\n",
      "\tspeed: 0.1208s/iter; left time: 3345.9582s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0601275\n",
      "\tspeed: 0.1208s/iter; left time: 3334.4666s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0625480\n",
      "\tspeed: 0.1212s/iter; left time: 3332.7025s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0705743\n",
      "\tspeed: 0.1209s/iter; left time: 3311.6721s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0633123\n",
      "\tspeed: 0.1185s/iter; left time: 3235.0795s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0581706\n",
      "\tspeed: 0.1110s/iter; left time: 3017.9555s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0554645\n",
      "\tspeed: 0.1199s/iter; left time: 3247.2439s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0550835\n",
      "\tspeed: 0.1216s/iter; left time: 3282.5404s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0590457\n",
      "\tspeed: 0.1213s/iter; left time: 3261.2997s\n",
      "Epoch: 10 cost time: 00h:05m:20.43s\n",
      "Epoch: 10 | Train Loss: 0.0663446 Vali Loss: 0.0583170 Test Loss: 0.0684556\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0767739\n",
      "\tspeed: 1.0024s/iter; left time: 26774.4833s\n",
      "\titers: 200, epoch: 11 | loss: 0.0565607\n",
      "\tspeed: 0.1212s/iter; left time: 3225.9090s\n",
      "\titers: 300, epoch: 11 | loss: 0.0709955\n",
      "\tspeed: 0.1206s/iter; left time: 3197.3566s\n",
      "\titers: 400, epoch: 11 | loss: 0.0625823\n",
      "\tspeed: 0.1199s/iter; left time: 3165.5866s\n",
      "\titers: 500, epoch: 11 | loss: 0.0661458\n",
      "\tspeed: 0.1208s/iter; left time: 3177.3516s\n",
      "\titers: 600, epoch: 11 | loss: 0.0606733\n",
      "\tspeed: 0.1199s/iter; left time: 3141.6557s\n",
      "\titers: 700, epoch: 11 | loss: 0.0683432\n",
      "\tspeed: 0.1204s/iter; left time: 3144.4046s\n",
      "\titers: 800, epoch: 11 | loss: 0.0626959\n",
      "\tspeed: 0.1209s/iter; left time: 3143.5221s\n",
      "\titers: 900, epoch: 11 | loss: 0.0729644\n",
      "\tspeed: 0.1206s/iter; left time: 3126.1173s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0603989\n",
      "\tspeed: 0.1202s/iter; left time: 3102.1290s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0806362\n",
      "\tspeed: 0.1209s/iter; left time: 3108.6647s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0656467\n",
      "\tspeed: 0.1207s/iter; left time: 3092.4238s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0590451\n",
      "\tspeed: 0.1204s/iter; left time: 3071.7247s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0639197\n",
      "\tspeed: 0.1206s/iter; left time: 3063.5683s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0649276\n",
      "\tspeed: 0.1213s/iter; left time: 3070.5942s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0672397\n",
      "\tspeed: 0.1209s/iter; left time: 3048.3964s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0771958\n",
      "\tspeed: 0.1208s/iter; left time: 3033.0554s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0633096\n",
      "\tspeed: 0.1209s/iter; left time: 3024.6658s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0575457\n",
      "\tspeed: 0.1208s/iter; left time: 3009.7995s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0709095\n",
      "\tspeed: 0.1199s/iter; left time: 2975.9801s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0625228\n",
      "\tspeed: 0.1195s/iter; left time: 2953.3664s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0722681\n",
      "\tspeed: 0.1209s/iter; left time: 2976.3788s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0579663\n",
      "\tspeed: 0.1210s/iter; left time: 2965.9585s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0612664\n",
      "\tspeed: 0.1206s/iter; left time: 2944.2459s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0591755\n",
      "\tspeed: 0.1213s/iter; left time: 2948.4781s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0699752\n",
      "\tspeed: 0.1215s/iter; left time: 2942.5130s\n",
      "Epoch: 11 cost time: 00h:05m:24.05s\n",
      "Epoch: 11 | Train Loss: 0.0658757 Vali Loss: 0.0578032 Test Loss: 0.0670076\n",
      "Validation loss decreased (0.058111 --> 0.057803).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0724038\n",
      "\tspeed: 1.0214s/iter; left time: 24543.3694s\n",
      "\titers: 200, epoch: 12 | loss: 0.0599244\n",
      "\tspeed: 0.1204s/iter; left time: 2882.0146s\n",
      "\titers: 300, epoch: 12 | loss: 0.0652725\n",
      "\tspeed: 0.1205s/iter; left time: 2871.3903s\n",
      "\titers: 400, epoch: 12 | loss: 0.0576787\n",
      "\tspeed: 0.1189s/iter; left time: 2820.5972s\n",
      "\titers: 500, epoch: 12 | loss: 0.0657673\n",
      "\tspeed: 0.1195s/iter; left time: 2822.9288s\n",
      "\titers: 600, epoch: 12 | loss: 0.0662366\n",
      "\tspeed: 0.1210s/iter; left time: 2846.7848s\n",
      "\titers: 700, epoch: 12 | loss: 0.0807744\n",
      "\tspeed: 0.1211s/iter; left time: 2836.5630s\n",
      "\titers: 800, epoch: 12 | loss: 0.0683301\n",
      "\tspeed: 0.1209s/iter; left time: 2820.4726s\n",
      "\titers: 900, epoch: 12 | loss: 0.0653859\n",
      "\tspeed: 0.1208s/iter; left time: 2806.3520s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0721641\n",
      "\tspeed: 0.1211s/iter; left time: 2800.9883s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0629323\n",
      "\tspeed: 0.1209s/iter; left time: 2784.2518s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0567239\n",
      "\tspeed: 0.1214s/iter; left time: 2782.8378s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0589142\n",
      "\tspeed: 0.1211s/iter; left time: 2764.1547s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0682594\n",
      "\tspeed: 0.1208s/iter; left time: 2745.5598s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0660052\n",
      "\tspeed: 0.1198s/iter; left time: 2711.3688s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0597542\n",
      "\tspeed: 0.1189s/iter; left time: 2679.3007s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0569995\n",
      "\tspeed: 0.1182s/iter; left time: 2651.7655s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0621652\n",
      "\tspeed: 0.1197s/iter; left time: 2672.1835s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0574851\n",
      "\tspeed: 0.1208s/iter; left time: 2685.8105s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0754092\n",
      "\tspeed: 0.1194s/iter; left time: 2642.0530s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0705758\n",
      "\tspeed: 0.1209s/iter; left time: 2663.8259s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0638501\n",
      "\tspeed: 0.1195s/iter; left time: 2620.6709s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0607917\n",
      "\tspeed: 0.1208s/iter; left time: 2638.0143s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0704667\n",
      "\tspeed: 0.1203s/iter; left time: 2614.3010s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0649680\n",
      "\tspeed: 0.1187s/iter; left time: 2567.1300s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0622617\n",
      "\tspeed: 0.1198s/iter; left time: 2580.3634s\n",
      "Epoch: 12 cost time: 00h:05m:22.82s\n",
      "Epoch: 12 | Train Loss: 0.0654141 Vali Loss: 0.0592872 Test Loss: 0.0696159\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0571517\n",
      "\tspeed: 1.0020s/iter; left time: 21392.3232s\n",
      "\titers: 200, epoch: 13 | loss: 0.0563359\n",
      "\tspeed: 0.1211s/iter; left time: 2573.5299s\n",
      "\titers: 300, epoch: 13 | loss: 0.0752222\n",
      "\tspeed: 0.1202s/iter; left time: 2542.1827s\n",
      "\titers: 400, epoch: 13 | loss: 0.0552250\n",
      "\tspeed: 0.1208s/iter; left time: 2543.7394s\n",
      "\titers: 500, epoch: 13 | loss: 0.0716464\n",
      "\tspeed: 0.1211s/iter; left time: 2537.0455s\n",
      "\titers: 600, epoch: 13 | loss: 0.0713721\n",
      "\tspeed: 0.1211s/iter; left time: 2525.8000s\n",
      "\titers: 700, epoch: 13 | loss: 0.0675928\n",
      "\tspeed: 0.1124s/iter; left time: 2332.4560s\n",
      "\titers: 800, epoch: 13 | loss: 0.0716297\n",
      "\tspeed: 0.1115s/iter; left time: 2302.6765s\n",
      "\titers: 900, epoch: 13 | loss: 0.0656308\n",
      "\tspeed: 0.1212s/iter; left time: 2490.8294s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0822458\n",
      "\tspeed: 0.1209s/iter; left time: 2472.6043s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0752804\n",
      "\tspeed: 0.1211s/iter; left time: 2464.3106s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0652262\n",
      "\tspeed: 0.1198s/iter; left time: 2426.5868s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0598756\n",
      "\tspeed: 0.1194s/iter; left time: 2406.7184s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0691182\n",
      "\tspeed: 0.1189s/iter; left time: 2383.9434s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0635491\n",
      "\tspeed: 0.1201s/iter; left time: 2395.1932s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0895910\n",
      "\tspeed: 0.1193s/iter; left time: 2368.0658s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0710886\n",
      "\tspeed: 0.1086s/iter; left time: 2143.8676s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0558507\n",
      "\tspeed: 0.1145s/iter; left time: 2249.9822s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0647579\n",
      "\tspeed: 0.1211s/iter; left time: 2367.5407s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0736879\n",
      "\tspeed: 0.1209s/iter; left time: 2351.4952s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0558424\n",
      "\tspeed: 0.1209s/iter; left time: 2339.2551s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0630306\n",
      "\tspeed: 0.1070s/iter; left time: 2058.7933s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0580184\n",
      "\tspeed: 0.1198s/iter; left time: 2294.8957s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643669\n",
      "\tspeed: 0.1212s/iter; left time: 2309.2027s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0530160\n",
      "\tspeed: 0.1209s/iter; left time: 2291.1351s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0617090\n",
      "\tspeed: 0.1209s/iter; left time: 2279.3256s\n",
      "Epoch: 13 cost time: 00h:05m:18.80s\n",
      "Epoch: 13 | Train Loss: 0.0649905 Vali Loss: 0.0579648 Test Loss: 0.0673596\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0698010\n",
      "\tspeed: 1.0080s/iter; left time: 18818.0647s\n",
      "\titers: 200, epoch: 14 | loss: 0.0725076\n",
      "\tspeed: 0.1212s/iter; left time: 2251.1916s\n",
      "\titers: 300, epoch: 14 | loss: 0.0715118\n",
      "\tspeed: 0.1214s/iter; left time: 2242.3808s\n",
      "\titers: 400, epoch: 14 | loss: 0.0609961\n",
      "\tspeed: 0.1194s/iter; left time: 2192.5082s\n",
      "\titers: 500, epoch: 14 | loss: 0.0552948\n",
      "\tspeed: 0.1204s/iter; left time: 2199.0651s\n",
      "\titers: 600, epoch: 14 | loss: 0.0565410\n",
      "\tspeed: 0.1167s/iter; left time: 2120.1241s\n",
      "\titers: 700, epoch: 14 | loss: 0.0706945\n",
      "\tspeed: 0.1040s/iter; left time: 1879.1361s\n",
      "\titers: 800, epoch: 14 | loss: 0.0687059\n",
      "\tspeed: 0.1038s/iter; left time: 1865.9370s\n",
      "\titers: 900, epoch: 14 | loss: 0.0700648\n",
      "\tspeed: 0.1039s/iter; left time: 1857.1746s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0679078\n",
      "\tspeed: 0.1070s/iter; left time: 1901.8525s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0623200\n",
      "\tspeed: 0.1214s/iter; left time: 2144.4022s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0606130\n",
      "\tspeed: 0.1061s/iter; left time: 1863.2860s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0608021\n",
      "\tspeed: 0.1040s/iter; left time: 1816.4396s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0654804\n",
      "\tspeed: 0.1045s/iter; left time: 1815.0370s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0613914\n",
      "\tspeed: 0.1070s/iter; left time: 1846.9774s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0539721\n",
      "\tspeed: 0.1047s/iter; left time: 1797.4113s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0673941\n",
      "\tspeed: 0.1043s/iter; left time: 1780.3201s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0644400\n",
      "\tspeed: 0.1046s/iter; left time: 1774.5108s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0617564\n",
      "\tspeed: 0.1038s/iter; left time: 1750.7404s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0573019\n",
      "\tspeed: 0.1036s/iter; left time: 1737.9306s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0627567\n",
      "\tspeed: 0.1035s/iter; left time: 1725.4768s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0622240\n",
      "\tspeed: 0.1033s/iter; left time: 1710.6912s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0786616\n",
      "\tspeed: 0.1049s/iter; left time: 1727.5067s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0765208\n",
      "\tspeed: 0.1034s/iter; left time: 1692.7239s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0689429\n",
      "\tspeed: 0.1039s/iter; left time: 1690.4411s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0559283\n",
      "\tspeed: 0.1065s/iter; left time: 1722.1034s\n",
      "Epoch: 14 cost time: 00h:04m:52.10s\n",
      "Epoch: 14 | Train Loss: 0.0645770 Vali Loss: 0.0574409 Test Loss: 0.0664566\n",
      "Validation loss decreased (0.057803 --> 0.057441).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0564922\n",
      "\tspeed: 1.0137s/iter; left time: 16206.0672s\n",
      "\titers: 200, epoch: 15 | loss: 0.0621809\n",
      "\tspeed: 0.1215s/iter; left time: 1929.8089s\n",
      "\titers: 300, epoch: 15 | loss: 0.0641433\n",
      "\tspeed: 0.1201s/iter; left time: 1895.9220s\n",
      "\titers: 400, epoch: 15 | loss: 0.0578732\n",
      "\tspeed: 0.1190s/iter; left time: 1866.4919s\n",
      "\titers: 500, epoch: 15 | loss: 0.0655350\n",
      "\tspeed: 0.1192s/iter; left time: 1858.3363s\n",
      "\titers: 600, epoch: 15 | loss: 0.0686196\n",
      "\tspeed: 0.1209s/iter; left time: 1872.5052s\n",
      "\titers: 700, epoch: 15 | loss: 0.0824154\n",
      "\tspeed: 0.1209s/iter; left time: 1860.5507s\n",
      "\titers: 800, epoch: 15 | loss: 0.0650468\n",
      "\tspeed: 0.1194s/iter; left time: 1825.2374s\n",
      "\titers: 900, epoch: 15 | loss: 0.0688387\n",
      "\tspeed: 0.1181s/iter; left time: 1793.5052s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0771232\n",
      "\tspeed: 0.1196s/iter; left time: 1804.9983s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0594005\n",
      "\tspeed: 0.1209s/iter; left time: 1812.2207s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0690505\n",
      "\tspeed: 0.1211s/iter; left time: 1802.8164s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0685179\n",
      "\tspeed: 0.1215s/iter; left time: 1796.2036s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0714361\n",
      "\tspeed: 0.1213s/iter; left time: 1781.2040s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0690282\n",
      "\tspeed: 0.1216s/iter; left time: 1773.8319s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0619706\n",
      "\tspeed: 0.1209s/iter; left time: 1751.6200s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0708041\n",
      "\tspeed: 0.1212s/iter; left time: 1743.5116s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0704809\n",
      "\tspeed: 0.1201s/iter; left time: 1715.8590s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0758016\n",
      "\tspeed: 0.1212s/iter; left time: 1719.3418s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0559394\n",
      "\tspeed: 0.1215s/iter; left time: 1711.7223s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0584624\n",
      "\tspeed: 0.1217s/iter; left time: 1702.1276s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0783027\n",
      "\tspeed: 0.1213s/iter; left time: 1684.9381s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0668146\n",
      "\tspeed: 0.1207s/iter; left time: 1663.9823s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0637044\n",
      "\tspeed: 0.1209s/iter; left time: 1655.2921s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0603980\n",
      "\tspeed: 0.1211s/iter; left time: 1645.4828s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0710332\n",
      "\tspeed: 0.1209s/iter; left time: 1631.0377s\n",
      "Epoch: 15 cost time: 00h:05m:24.00s\n",
      "Epoch: 15 | Train Loss: 0.0641804 Vali Loss: 0.0605943 Test Loss: 0.0706357\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0574767\n",
      "\tspeed: 0.9987s/iter; left time: 13288.6371s\n",
      "\titers: 200, epoch: 16 | loss: 0.0573915\n",
      "\tspeed: 0.1210s/iter; left time: 1598.3605s\n",
      "\titers: 300, epoch: 16 | loss: 0.0506497\n",
      "\tspeed: 0.1208s/iter; left time: 1583.0926s\n",
      "\titers: 400, epoch: 16 | loss: 0.0601425\n",
      "\tspeed: 0.1205s/iter; left time: 1567.3932s\n",
      "\titers: 500, epoch: 16 | loss: 0.0669079\n",
      "\tspeed: 0.1206s/iter; left time: 1556.8002s\n",
      "\titers: 600, epoch: 16 | loss: 0.0705684\n",
      "\tspeed: 0.1208s/iter; left time: 1546.9123s\n",
      "\titers: 700, epoch: 16 | loss: 0.0603802\n",
      "\tspeed: 0.1209s/iter; left time: 1536.3318s\n",
      "\titers: 800, epoch: 16 | loss: 0.0707469\n",
      "\tspeed: 0.1206s/iter; left time: 1520.0035s\n",
      "\titers: 900, epoch: 16 | loss: 0.0820457\n",
      "\tspeed: 0.1209s/iter; left time: 1512.1507s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0660811\n",
      "\tspeed: 0.1207s/iter; left time: 1497.2666s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0602575\n",
      "\tspeed: 0.1205s/iter; left time: 1483.1420s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0644291\n",
      "\tspeed: 0.1209s/iter; left time: 1476.2661s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0740635\n",
      "\tspeed: 0.1213s/iter; left time: 1468.4957s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0634106\n",
      "\tspeed: 0.1211s/iter; left time: 1453.3450s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0686077\n",
      "\tspeed: 0.1210s/iter; left time: 1441.1272s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0558413\n",
      "\tspeed: 0.1207s/iter; left time: 1425.5261s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0561398\n",
      "\tspeed: 0.1197s/iter; left time: 1401.1049s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0640715\n",
      "\tspeed: 0.1194s/iter; left time: 1386.2993s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0615180\n",
      "\tspeed: 0.1189s/iter; left time: 1368.0313s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0579631\n",
      "\tspeed: 0.1176s/iter; left time: 1341.7936s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0742498\n",
      "\tspeed: 0.1210s/iter; left time: 1368.4452s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0673364\n",
      "\tspeed: 0.1209s/iter; left time: 1354.9910s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0641308\n",
      "\tspeed: 0.1201s/iter; left time: 1334.0378s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0701997\n",
      "\tspeed: 0.1196s/iter; left time: 1315.8830s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0703389\n",
      "\tspeed: 0.1195s/iter; left time: 1303.4728s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0659333\n",
      "\tspeed: 0.1212s/iter; left time: 1309.2839s\n",
      "Epoch: 16 cost time: 00h:05m:23.44s\n",
      "Epoch: 16 | Train Loss: 0.0638276 Vali Loss: 0.0593083 Test Loss: 0.0686335\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0632415\n",
      "\tspeed: 1.0042s/iter; left time: 10670.0459s\n",
      "\titers: 200, epoch: 17 | loss: 0.0688461\n",
      "\tspeed: 0.1210s/iter; left time: 1273.2619s\n",
      "\titers: 300, epoch: 17 | loss: 0.0631828\n",
      "\tspeed: 0.1209s/iter; left time: 1260.4342s\n",
      "\titers: 400, epoch: 17 | loss: 0.0736408\n",
      "\tspeed: 0.1209s/iter; left time: 1248.2520s\n",
      "\titers: 500, epoch: 17 | loss: 0.0770576\n",
      "\tspeed: 0.1203s/iter; left time: 1230.3587s\n",
      "\titers: 600, epoch: 17 | loss: 0.0678533\n",
      "\tspeed: 0.1207s/iter; left time: 1221.9485s\n",
      "\titers: 700, epoch: 17 | loss: 0.0549429\n",
      "\tspeed: 0.1206s/iter; left time: 1209.1622s\n",
      "\titers: 800, epoch: 17 | loss: 0.0515392\n",
      "\tspeed: 0.1201s/iter; left time: 1192.4224s\n",
      "\titers: 900, epoch: 17 | loss: 0.0676008\n",
      "\tspeed: 0.1213s/iter; left time: 1191.8200s\n",
      "\titers: 1000, epoch: 17 | loss: 0.0647659\n",
      "\tspeed: 0.1208s/iter; left time: 1174.5761s\n",
      "\titers: 1100, epoch: 17 | loss: 0.0694209\n",
      "\tspeed: 0.1201s/iter; left time: 1155.6223s\n",
      "\titers: 1200, epoch: 17 | loss: 0.0576050\n",
      "\tspeed: 0.1210s/iter; left time: 1152.5943s\n",
      "\titers: 1300, epoch: 17 | loss: 0.0682291\n",
      "\tspeed: 0.1210s/iter; left time: 1140.6939s\n",
      "\titers: 1400, epoch: 17 | loss: 0.0641747\n",
      "\tspeed: 0.1208s/iter; left time: 1126.4672s\n",
      "\titers: 1500, epoch: 17 | loss: 0.0657903\n",
      "\tspeed: 0.1209s/iter; left time: 1115.4537s\n",
      "\titers: 1600, epoch: 17 | loss: 0.0568783\n",
      "\tspeed: 0.1208s/iter; left time: 1102.2980s\n",
      "\titers: 1700, epoch: 17 | loss: 0.0658871\n",
      "\tspeed: 0.1208s/iter; left time: 1090.2853s\n",
      "\titers: 1800, epoch: 17 | loss: 0.0624544\n",
      "\tspeed: 0.1209s/iter; left time: 1079.2423s\n",
      "\titers: 1900, epoch: 17 | loss: 0.0611300\n",
      "\tspeed: 0.1211s/iter; left time: 1068.3705s\n",
      "\titers: 2000, epoch: 17 | loss: 0.0487223\n",
      "\tspeed: 0.1209s/iter; left time: 1054.9119s\n",
      "\titers: 2100, epoch: 17 | loss: 0.0584179\n",
      "\tspeed: 0.1193s/iter; left time: 1028.8500s\n",
      "\titers: 2200, epoch: 17 | loss: 0.0644555\n",
      "\tspeed: 0.1217s/iter; left time: 1037.1654s\n",
      "\titers: 2300, epoch: 17 | loss: 0.0529996\n",
      "\tspeed: 0.1208s/iter; left time: 1017.9694s\n",
      "\titers: 2400, epoch: 17 | loss: 0.0567144\n",
      "\tspeed: 0.1146s/iter; left time: 953.8380s\n",
      "\titers: 2500, epoch: 17 | loss: 0.0710186\n",
      "\tspeed: 0.1012s/iter; left time: 832.3481s\n",
      "\titers: 2600, epoch: 17 | loss: 0.0712463\n",
      "\tspeed: 0.1015s/iter; left time: 824.7488s\n",
      "Epoch: 17 cost time: 00h:05m:19.68s\n",
      "Epoch: 17 | Train Loss: 0.0634753 Vali Loss: 0.0594251 Test Loss: 0.0680345\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0608606\n",
      "\tspeed: 1.0129s/iter; left time: 8046.1915s\n",
      "\titers: 200, epoch: 18 | loss: 0.0630744\n",
      "\tspeed: 0.1182s/iter; left time: 927.2213s\n",
      "\titers: 300, epoch: 18 | loss: 0.0573201\n",
      "\tspeed: 0.1192s/iter; left time: 923.0656s\n",
      "\titers: 400, epoch: 18 | loss: 0.0669892\n",
      "\tspeed: 0.1187s/iter; left time: 907.1523s\n",
      "\titers: 500, epoch: 18 | loss: 0.0642769\n",
      "\tspeed: 0.1179s/iter; left time: 889.2121s\n",
      "\titers: 600, epoch: 18 | loss: 0.0601702\n",
      "\tspeed: 0.1182s/iter; left time: 879.8324s\n",
      "\titers: 700, epoch: 18 | loss: 0.0813551\n",
      "\tspeed: 0.1188s/iter; left time: 872.4266s\n",
      "\titers: 800, epoch: 18 | loss: 0.0769780\n",
      "\tspeed: 0.1192s/iter; left time: 863.4640s\n",
      "\titers: 900, epoch: 18 | loss: 0.0527711\n",
      "\tspeed: 0.1195s/iter; left time: 853.8915s\n",
      "\titers: 1000, epoch: 18 | loss: 0.0642087\n",
      "\tspeed: 0.1194s/iter; left time: 840.8665s\n",
      "\titers: 1100, epoch: 18 | loss: 0.0753492\n",
      "\tspeed: 0.1177s/iter; left time: 817.1250s\n",
      "\titers: 1200, epoch: 18 | loss: 0.0503302\n",
      "\tspeed: 0.1179s/iter; left time: 806.8906s\n",
      "\titers: 1300, epoch: 18 | loss: 0.0638088\n",
      "\tspeed: 0.1182s/iter; left time: 797.1014s\n",
      "\titers: 1400, epoch: 18 | loss: 0.0659324\n",
      "\tspeed: 0.1188s/iter; left time: 789.2472s\n",
      "\titers: 1500, epoch: 18 | loss: 0.0653730\n",
      "\tspeed: 0.1194s/iter; left time: 781.2210s\n",
      "\titers: 1600, epoch: 18 | loss: 0.0739145\n",
      "\tspeed: 0.1184s/iter; left time: 763.0992s\n",
      "\titers: 1700, epoch: 18 | loss: 0.0535310\n",
      "\tspeed: 0.1191s/iter; left time: 755.4238s\n",
      "\titers: 1800, epoch: 18 | loss: 0.0655848\n",
      "\tspeed: 0.1196s/iter; left time: 746.5053s\n",
      "\titers: 1900, epoch: 18 | loss: 0.0589777\n",
      "\tspeed: 0.1195s/iter; left time: 734.3371s\n",
      "\titers: 2000, epoch: 18 | loss: 0.0685786\n",
      "\tspeed: 0.1187s/iter; left time: 717.2185s\n",
      "\titers: 2100, epoch: 18 | loss: 0.0555254\n",
      "\tspeed: 0.1193s/iter; left time: 708.8238s\n",
      "\titers: 2200, epoch: 18 | loss: 0.0631094\n",
      "\tspeed: 0.1189s/iter; left time: 695.1114s\n",
      "\titers: 2300, epoch: 18 | loss: 0.0528505\n",
      "\tspeed: 0.1196s/iter; left time: 687.0646s\n",
      "\titers: 2400, epoch: 18 | loss: 0.0643650\n",
      "\tspeed: 0.1191s/iter; left time: 672.0389s\n",
      "\titers: 2500, epoch: 18 | loss: 0.0647579\n",
      "\tspeed: 0.1181s/iter; left time: 654.7355s\n",
      "\titers: 2600, epoch: 18 | loss: 0.0699769\n",
      "\tspeed: 0.1180s/iter; left time: 642.3417s\n",
      "Epoch: 18 cost time: 00h:05m:18.80s\n",
      "Epoch: 18 | Train Loss: 0.0629919 Vali Loss: 0.0601041 Test Loss: 0.0698167\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0543833\n",
      "\tspeed: 0.9989s/iter; left time: 5257.2623s\n",
      "\titers: 200, epoch: 19 | loss: 0.0554405\n",
      "\tspeed: 0.1198s/iter; left time: 618.7181s\n",
      "\titers: 300, epoch: 19 | loss: 0.0701018\n",
      "\tspeed: 0.1211s/iter; left time: 613.1900s\n",
      "\titers: 400, epoch: 19 | loss: 0.0615413\n",
      "\tspeed: 0.1208s/iter; left time: 599.4730s\n",
      "\titers: 500, epoch: 19 | loss: 0.0614020\n",
      "\tspeed: 0.1210s/iter; left time: 588.4562s\n",
      "\titers: 600, epoch: 19 | loss: 0.0573202\n",
      "\tspeed: 0.1186s/iter; left time: 565.1220s\n",
      "\titers: 700, epoch: 19 | loss: 0.0611640\n",
      "\tspeed: 0.1193s/iter; left time: 556.1352s\n",
      "\titers: 800, epoch: 19 | loss: 0.0607653\n",
      "\tspeed: 0.1209s/iter; left time: 551.7603s\n",
      "\titers: 900, epoch: 19 | loss: 0.0566523\n",
      "\tspeed: 0.1210s/iter; left time: 540.0489s\n",
      "\titers: 1000, epoch: 19 | loss: 0.0622958\n",
      "\tspeed: 0.1191s/iter; left time: 519.5486s\n",
      "\titers: 1100, epoch: 19 | loss: 0.0550250\n",
      "\tspeed: 0.1190s/iter; left time: 507.0982s\n",
      "\titers: 1200, epoch: 19 | loss: 0.0663746\n",
      "\tspeed: 0.1207s/iter; left time: 502.6407s\n",
      "\titers: 1300, epoch: 19 | loss: 0.0650760\n",
      "\tspeed: 0.1209s/iter; left time: 491.1242s\n",
      "\titers: 1400, epoch: 19 | loss: 0.0588802\n",
      "\tspeed: 0.1209s/iter; left time: 479.1690s\n",
      "\titers: 1500, epoch: 19 | loss: 0.0603579\n",
      "\tspeed: 0.1194s/iter; left time: 461.1579s\n",
      "\titers: 1600, epoch: 19 | loss: 0.0569299\n",
      "\tspeed: 0.1194s/iter; left time: 449.4411s\n",
      "\titers: 1700, epoch: 19 | loss: 0.0491759\n",
      "\tspeed: 0.1211s/iter; left time: 443.6634s\n",
      "\titers: 1800, epoch: 19 | loss: 0.0864070\n",
      "\tspeed: 0.1209s/iter; left time: 430.8147s\n",
      "\titers: 1900, epoch: 19 | loss: 0.0639838\n",
      "\tspeed: 0.1205s/iter; left time: 417.3587s\n",
      "\titers: 2000, epoch: 19 | loss: 0.0655531\n",
      "\tspeed: 0.1211s/iter; left time: 407.3135s\n",
      "\titers: 2100, epoch: 19 | loss: 0.0550640\n",
      "\tspeed: 0.1208s/iter; left time: 394.1868s\n",
      "\titers: 2200, epoch: 19 | loss: 0.0624830\n",
      "\tspeed: 0.1212s/iter; left time: 383.3613s\n",
      "\titers: 2300, epoch: 19 | loss: 0.0577416\n",
      "\tspeed: 0.1210s/iter; left time: 370.4737s\n",
      "\titers: 2400, epoch: 19 | loss: 0.0651264\n",
      "\tspeed: 0.1207s/iter; left time: 357.7418s\n",
      "\titers: 2500, epoch: 19 | loss: 0.0614737\n",
      "\tspeed: 0.1209s/iter; left time: 346.1169s\n",
      "\titers: 2600, epoch: 19 | loss: 0.0525950\n",
      "\tspeed: 0.1208s/iter; left time: 333.7576s\n",
      "Epoch: 19 cost time: 00h:05m:23.45s\n",
      "Epoch: 19 | Train Loss: 0.0626410 Vali Loss: 0.0593832 Test Loss: 0.0687746\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010672738775610924, rmse:0.10330894589424133, mae:0.0664566159248352, rse:0.3033173382282257\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 24: 02h:07m:51.19s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-01 07:50:56,871] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 07:50:58,095] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 07:50:58,095] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 07:50:58,095] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 07:50:58,192] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 07:50:58,192] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 07:50:58,843] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 07:50:58,844] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 07:50:58,844] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 07:50:58,846] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 07:50:58,846] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 07:50:58,846] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 07:50:58,846] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 07:50:58,846] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 07:50:58,846] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 07:50:58,846] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 07:50:59,167] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 07:50:59,168] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 07:50:59,168] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.5 GB, percent = 9.9%\n",
      "[2024-11-01 07:50:59,296] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 07:50:59,297] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 07:50:59,297] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.5 GB, percent = 9.9%\n",
      "[2024-11-01 07:50:59,297] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 07:50:59,409] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 07:50:59,410] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 07:50:59,410] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.5 GB, percent = 9.9%\n",
      "[2024-11-01 07:50:59,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 07:50:59,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 07:50:59,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 07:50:59,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2e807005d0>\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 07:50:59,412] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 07:50:59,413] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 07:50:59,414] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1911721\n",
      "\tspeed: 0.1754s/iter; left time: 9360.4956s\n",
      "\titers: 200, epoch: 1 | loss: 0.1927420\n",
      "\tspeed: 0.1310s/iter; left time: 6979.3060s\n",
      "\titers: 300, epoch: 1 | loss: 0.1694221\n",
      "\tspeed: 0.1306s/iter; left time: 6943.0207s\n",
      "\titers: 400, epoch: 1 | loss: 0.1720468\n",
      "\tspeed: 0.1310s/iter; left time: 6953.3096s\n",
      "\titers: 500, epoch: 1 | loss: 0.1218105\n",
      "\tspeed: 0.1308s/iter; left time: 6927.9581s\n",
      "\titers: 600, epoch: 1 | loss: 0.1114848\n",
      "\tspeed: 0.1308s/iter; left time: 6914.9915s\n",
      "\titers: 700, epoch: 1 | loss: 0.1140701\n",
      "\tspeed: 0.1314s/iter; left time: 6936.9193s\n",
      "\titers: 800, epoch: 1 | loss: 0.1041137\n",
      "\tspeed: 0.1311s/iter; left time: 6905.2141s\n",
      "\titers: 900, epoch: 1 | loss: 0.1242962\n",
      "\tspeed: 0.1309s/iter; left time: 6882.0769s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1122826\n",
      "\tspeed: 0.1304s/iter; left time: 6844.6689s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1078091\n",
      "\tspeed: 0.1314s/iter; left time: 6884.2236s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0886153\n",
      "\tspeed: 0.1312s/iter; left time: 6859.0208s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1067653\n",
      "\tspeed: 0.1310s/iter; left time: 6836.4633s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1063584\n",
      "\tspeed: 0.1314s/iter; left time: 6844.1412s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1006202\n",
      "\tspeed: 0.1309s/iter; left time: 6803.2115s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0992378\n",
      "\tspeed: 0.1310s/iter; left time: 6796.5774s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1159680\n",
      "\tspeed: 0.1311s/iter; left time: 6788.6589s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0989420\n",
      "\tspeed: 0.1313s/iter; left time: 6784.2494s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1210326\n",
      "\tspeed: 0.1308s/iter; left time: 6747.6174s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0897738\n",
      "\tspeed: 0.1310s/iter; left time: 6742.8906s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1054735\n",
      "\tspeed: 0.1312s/iter; left time: 6741.9535s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0856309\n",
      "\tspeed: 0.1312s/iter; left time: 6728.5408s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1068027\n",
      "\tspeed: 0.1311s/iter; left time: 6707.7314s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0811701\n",
      "\tspeed: 0.1308s/iter; left time: 6679.3676s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0951465\n",
      "\tspeed: 0.1322s/iter; left time: 6741.5955s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1027023\n",
      "\tspeed: 0.1318s/iter; left time: 6706.4274s\n",
      "Epoch: 1 cost time: 00h:05m:51.74s\n",
      "Epoch: 1 | Train Loss: 0.1174485 Vali Loss: 0.0865509 Test Loss: 0.1004628\n",
      "Validation loss decreased (inf --> 0.086551).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1076370\n",
      "\tspeed: 1.1297s/iter; left time: 57285.9966s\n",
      "\titers: 200, epoch: 2 | loss: 0.1094375\n",
      "\tspeed: 0.1085s/iter; left time: 5492.2475s\n",
      "\titers: 300, epoch: 2 | loss: 0.0885852\n",
      "\tspeed: 0.1209s/iter; left time: 6107.8326s\n",
      "\titers: 400, epoch: 2 | loss: 0.0921508\n",
      "\tspeed: 0.1208s/iter; left time: 6091.1367s\n",
      "\titers: 500, epoch: 2 | loss: 0.0874121\n",
      "\tspeed: 0.1209s/iter; left time: 6082.2267s\n",
      "\titers: 600, epoch: 2 | loss: 0.0942486\n",
      "\tspeed: 0.1206s/iter; left time: 6056.0924s\n",
      "\titers: 700, epoch: 2 | loss: 0.0875211\n",
      "\tspeed: 0.1210s/iter; left time: 6060.4486s\n",
      "\titers: 800, epoch: 2 | loss: 0.0975673\n",
      "\tspeed: 0.1205s/iter; left time: 6023.6666s\n",
      "\titers: 900, epoch: 2 | loss: 0.0873782\n",
      "\tspeed: 0.1192s/iter; left time: 5949.7093s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0910329\n",
      "\tspeed: 0.1209s/iter; left time: 6021.0561s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0909847\n",
      "\tspeed: 0.1182s/iter; left time: 5875.6703s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0977218\n",
      "\tspeed: 0.1209s/iter; left time: 5999.6375s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0851496\n",
      "\tspeed: 0.1196s/iter; left time: 5919.5161s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0878843\n",
      "\tspeed: 0.1190s/iter; left time: 5880.2872s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0895761\n",
      "\tspeed: 0.1194s/iter; left time: 5888.1052s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0866336\n",
      "\tspeed: 0.1235s/iter; left time: 6078.7080s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1094462\n",
      "\tspeed: 0.1210s/iter; left time: 5943.9146s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1022051\n",
      "\tspeed: 0.1203s/iter; left time: 5896.5519s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0886707\n",
      "\tspeed: 0.1209s/iter; left time: 5912.9688s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0902492\n",
      "\tspeed: 0.1208s/iter; left time: 5897.6043s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0888425\n",
      "\tspeed: 0.1207s/iter; left time: 5878.6715s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0878865\n",
      "\tspeed: 0.1210s/iter; left time: 5882.0693s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0795095\n",
      "\tspeed: 0.1209s/iter; left time: 5865.0695s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0914220\n",
      "\tspeed: 0.1071s/iter; left time: 5184.0068s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0818894\n",
      "\tspeed: 0.1209s/iter; left time: 5841.2454s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0820524\n",
      "\tspeed: 0.1210s/iter; left time: 5834.0523s\n",
      "Epoch: 2 cost time: 00h:05m:20.38s\n",
      "Epoch: 2 | Train Loss: 0.0935248 Vali Loss: 0.0811140 Test Loss: 0.0944012\n",
      "Validation loss decreased (0.086551 --> 0.081114).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0952333\n",
      "\tspeed: 0.9978s/iter; left time: 47927.4442s\n",
      "\titers: 200, epoch: 3 | loss: 0.0934332\n",
      "\tspeed: 0.1195s/iter; left time: 5727.9276s\n",
      "\titers: 300, epoch: 3 | loss: 0.0905903\n",
      "\tspeed: 0.1194s/iter; left time: 5710.2731s\n",
      "\titers: 400, epoch: 3 | loss: 0.0940271\n",
      "\tspeed: 0.1184s/iter; left time: 5649.6792s\n",
      "\titers: 500, epoch: 3 | loss: 0.0887500\n",
      "\tspeed: 0.1185s/iter; left time: 5643.1274s\n",
      "\titers: 600, epoch: 3 | loss: 0.0991548\n",
      "\tspeed: 0.1205s/iter; left time: 5726.6554s\n",
      "\titers: 700, epoch: 3 | loss: 0.0992021\n",
      "\tspeed: 0.1197s/iter; left time: 5678.2305s\n",
      "\titers: 800, epoch: 3 | loss: 0.0928352\n",
      "\tspeed: 0.1191s/iter; left time: 5637.8056s\n",
      "\titers: 900, epoch: 3 | loss: 0.0962132\n",
      "\tspeed: 0.1184s/iter; left time: 5591.5248s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0963722\n",
      "\tspeed: 0.1182s/iter; left time: 5571.5489s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0940291\n",
      "\tspeed: 0.1189s/iter; left time: 5591.8360s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0946871\n",
      "\tspeed: 0.1188s/iter; left time: 5575.5951s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0897035\n",
      "\tspeed: 0.1189s/iter; left time: 5567.3215s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0817999\n",
      "\tspeed: 0.1191s/iter; left time: 5565.6310s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0934589\n",
      "\tspeed: 0.1190s/iter; left time: 5549.1860s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1075991\n",
      "\tspeed: 0.1182s/iter; left time: 5500.5783s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029885\n",
      "\tspeed: 0.1181s/iter; left time: 5486.0548s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0796154\n",
      "\tspeed: 0.1179s/iter; left time: 5460.3760s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1007038\n",
      "\tspeed: 0.1183s/iter; left time: 5470.1309s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0810669\n",
      "\tspeed: 0.1187s/iter; left time: 5478.1816s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1120119\n",
      "\tspeed: 0.1196s/iter; left time: 5507.4934s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1014793\n",
      "\tspeed: 0.1194s/iter; left time: 5482.6509s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0782545\n",
      "\tspeed: 0.1198s/iter; left time: 5488.5750s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0805253\n",
      "\tspeed: 0.1199s/iter; left time: 5483.4642s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0871867\n",
      "\tspeed: 0.1188s/iter; left time: 5420.9702s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0826894\n",
      "\tspeed: 0.1188s/iter; left time: 5409.7105s\n",
      "Epoch: 3 cost time: 00h:05m:18.60s\n",
      "Epoch: 3 | Train Loss: 0.0897070 Vali Loss: 0.0801801 Test Loss: 0.0940483\n",
      "Validation loss decreased (0.081114 --> 0.080180).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0769089\n",
      "\tspeed: 0.9959s/iter; left time: 45170.8816s\n",
      "\titers: 200, epoch: 4 | loss: 0.0909401\n",
      "\tspeed: 0.1204s/iter; left time: 5448.5463s\n",
      "\titers: 300, epoch: 4 | loss: 0.0769373\n",
      "\tspeed: 0.1193s/iter; left time: 5387.4849s\n",
      "\titers: 400, epoch: 4 | loss: 0.0873026\n",
      "\tspeed: 0.1199s/iter; left time: 5401.5405s\n",
      "\titers: 500, epoch: 4 | loss: 0.0947193\n",
      "\tspeed: 0.1184s/iter; left time: 5325.1860s\n",
      "\titers: 600, epoch: 4 | loss: 0.0831833\n",
      "\tspeed: 0.1189s/iter; left time: 5334.2103s\n",
      "\titers: 700, epoch: 4 | loss: 0.0980527\n",
      "\tspeed: 0.1193s/iter; left time: 5339.8231s\n",
      "\titers: 800, epoch: 4 | loss: 0.0867212\n",
      "\tspeed: 0.1173s/iter; left time: 5240.4465s\n",
      "\titers: 900, epoch: 4 | loss: 0.0815352\n",
      "\tspeed: 0.1013s/iter; left time: 4515.8182s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0801502\n",
      "\tspeed: 0.1012s/iter; left time: 4498.2598s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0883447\n",
      "\tspeed: 0.1014s/iter; left time: 4499.2234s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0877123\n",
      "\tspeed: 0.1013s/iter; left time: 4482.7002s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0836246\n",
      "\tspeed: 0.1016s/iter; left time: 4486.9680s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0986883\n",
      "\tspeed: 0.1051s/iter; left time: 4630.0921s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0836472\n",
      "\tspeed: 0.1186s/iter; left time: 5215.3094s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0853868\n",
      "\tspeed: 0.1185s/iter; left time: 5196.0761s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0907262\n",
      "\tspeed: 0.1183s/iter; left time: 5178.6856s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0784356\n",
      "\tspeed: 0.1187s/iter; left time: 5184.3071s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0888463\n",
      "\tspeed: 0.1174s/iter; left time: 5113.7049s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0986566\n",
      "\tspeed: 0.1016s/iter; left time: 4414.9981s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0922876\n",
      "\tspeed: 0.1015s/iter; left time: 4401.7449s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0821516\n",
      "\tspeed: 0.1060s/iter; left time: 4584.2357s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0900341\n",
      "\tspeed: 0.1184s/iter; left time: 5109.8788s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0951754\n",
      "\tspeed: 0.1182s/iter; left time: 5088.1733s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0837031\n",
      "\tspeed: 0.1181s/iter; left time: 5075.1864s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0842158\n",
      "\tspeed: 0.1181s/iter; left time: 5062.9119s\n",
      "Epoch: 4 cost time: 00h:05m:03.12s\n",
      "Epoch: 4 | Train Loss: 0.0869057 Vali Loss: 0.0800106 Test Loss: 0.0955628\n",
      "Validation loss decreased (0.080180 --> 0.080011).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0871619\n",
      "\tspeed: 0.9900s/iter; left time: 42257.2283s\n",
      "\titers: 200, epoch: 5 | loss: 0.1035479\n",
      "\tspeed: 0.1042s/iter; left time: 4435.3738s\n",
      "\titers: 300, epoch: 5 | loss: 0.0778850\n",
      "\tspeed: 0.1017s/iter; left time: 4319.0314s\n",
      "\titers: 400, epoch: 5 | loss: 0.0712645\n",
      "\tspeed: 0.1015s/iter; left time: 4300.7944s\n",
      "\titers: 500, epoch: 5 | loss: 0.0783551\n",
      "\tspeed: 0.1152s/iter; left time: 4872.9946s\n",
      "\titers: 600, epoch: 5 | loss: 0.0905214\n",
      "\tspeed: 0.1152s/iter; left time: 4860.3331s\n",
      "\titers: 700, epoch: 5 | loss: 0.0817809\n",
      "\tspeed: 0.1054s/iter; left time: 4435.5948s\n",
      "\titers: 800, epoch: 5 | loss: 0.0813011\n",
      "\tspeed: 0.1014s/iter; left time: 4255.2100s\n",
      "\titers: 900, epoch: 5 | loss: 0.0899853\n",
      "\tspeed: 0.1014s/iter; left time: 4246.9693s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0864681\n",
      "\tspeed: 0.1015s/iter; left time: 4242.9781s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0875253\n",
      "\tspeed: 0.1107s/iter; left time: 4614.4292s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0901085\n",
      "\tspeed: 0.1142s/iter; left time: 4750.1228s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0639921\n",
      "\tspeed: 0.1019s/iter; left time: 4226.5821s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0969867\n",
      "\tspeed: 0.1067s/iter; left time: 4416.2699s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0761939\n",
      "\tspeed: 0.1180s/iter; left time: 4871.8385s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0842133\n",
      "\tspeed: 0.1179s/iter; left time: 4853.7220s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0735740\n",
      "\tspeed: 0.1140s/iter; left time: 4685.2370s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0814190\n",
      "\tspeed: 0.1077s/iter; left time: 4412.6678s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0850187\n",
      "\tspeed: 0.1198s/iter; left time: 4898.0522s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0746921\n",
      "\tspeed: 0.1186s/iter; left time: 4836.3797s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0881317\n",
      "\tspeed: 0.1193s/iter; left time: 4854.8966s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0845793\n",
      "\tspeed: 0.1190s/iter; left time: 4827.8831s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0848546\n",
      "\tspeed: 0.1175s/iter; left time: 4755.2586s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0750976\n",
      "\tspeed: 0.1045s/iter; left time: 4219.6292s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0834830\n",
      "\tspeed: 0.1194s/iter; left time: 4808.6567s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0875506\n",
      "\tspeed: 0.1204s/iter; left time: 4840.0455s\n",
      "Epoch: 5 cost time: 00h:04m:59.04s\n",
      "Epoch: 5 | Train Loss: 0.0848302 Vali Loss: 0.0812559 Test Loss: 0.0953243\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1068747\n",
      "\tspeed: 0.9808s/iter; left time: 39240.8034s\n",
      "\titers: 200, epoch: 6 | loss: 0.0783328\n",
      "\tspeed: 0.1209s/iter; left time: 4824.9128s\n",
      "\titers: 300, epoch: 6 | loss: 0.0739404\n",
      "\tspeed: 0.1204s/iter; left time: 4791.4928s\n",
      "\titers: 400, epoch: 6 | loss: 0.0745426\n",
      "\tspeed: 0.1208s/iter; left time: 4797.3489s\n",
      "\titers: 500, epoch: 6 | loss: 0.0807710\n",
      "\tspeed: 0.1188s/iter; left time: 4705.5004s\n",
      "\titers: 600, epoch: 6 | loss: 0.0781820\n",
      "\tspeed: 0.1195s/iter; left time: 4721.0329s\n",
      "\titers: 700, epoch: 6 | loss: 0.0852243\n",
      "\tspeed: 0.1194s/iter; left time: 4703.7205s\n",
      "\titers: 800, epoch: 6 | loss: 0.0808892\n",
      "\tspeed: 0.1199s/iter; left time: 4712.9422s\n",
      "\titers: 900, epoch: 6 | loss: 0.0863012\n",
      "\tspeed: 0.1196s/iter; left time: 4689.8551s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0845646\n",
      "\tspeed: 0.1188s/iter; left time: 4645.4725s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0786446\n",
      "\tspeed: 0.1193s/iter; left time: 4652.6836s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0738035\n",
      "\tspeed: 0.1192s/iter; left time: 4638.1092s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0962725\n",
      "\tspeed: 0.1193s/iter; left time: 4629.0365s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0873117\n",
      "\tspeed: 0.1183s/iter; left time: 4580.8513s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0892730\n",
      "\tspeed: 0.1190s/iter; left time: 4595.6594s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0773154\n",
      "\tspeed: 0.1192s/iter; left time: 4589.3222s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0777241\n",
      "\tspeed: 0.1191s/iter; left time: 4574.1587s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0818046\n",
      "\tspeed: 0.1196s/iter; left time: 4581.6482s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0856868\n",
      "\tspeed: 0.1198s/iter; left time: 4578.1556s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849679\n",
      "\tspeed: 0.1194s/iter; left time: 4551.3619s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0735170\n",
      "\tspeed: 0.1197s/iter; left time: 4550.3702s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0838584\n",
      "\tspeed: 0.1197s/iter; left time: 4537.3662s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0831294\n",
      "\tspeed: 0.1189s/iter; left time: 4494.7647s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0829203\n",
      "\tspeed: 0.1193s/iter; left time: 4497.2487s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0855648\n",
      "\tspeed: 0.1192s/iter; left time: 4482.3846s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0735175\n",
      "\tspeed: 0.1205s/iter; left time: 4518.4545s\n",
      "Epoch: 6 cost time: 00h:05m:20.16s\n",
      "Epoch: 6 | Train Loss: 0.0828304 Vali Loss: 0.0819640 Test Loss: 0.0980078\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0902265\n",
      "\tspeed: 0.9834s/iter; left time: 36716.0617s\n",
      "\titers: 200, epoch: 7 | loss: 0.0986805\n",
      "\tspeed: 0.1201s/iter; left time: 4473.0452s\n",
      "\titers: 300, epoch: 7 | loss: 0.0778703\n",
      "\tspeed: 0.1196s/iter; left time: 4442.3417s\n",
      "\titers: 400, epoch: 7 | loss: 0.0782931\n",
      "\tspeed: 0.1204s/iter; left time: 4458.9880s\n",
      "\titers: 500, epoch: 7 | loss: 0.0746187\n",
      "\tspeed: 0.1201s/iter; left time: 4437.4885s\n",
      "\titers: 600, epoch: 7 | loss: 0.0827935\n",
      "\tspeed: 0.1059s/iter; left time: 3899.4507s\n",
      "\titers: 700, epoch: 7 | loss: 0.0739331\n",
      "\tspeed: 0.1170s/iter; left time: 4298.9803s\n",
      "\titers: 800, epoch: 7 | loss: 0.0809667\n",
      "\tspeed: 0.1199s/iter; left time: 4392.0960s\n",
      "\titers: 900, epoch: 7 | loss: 0.0907432\n",
      "\tspeed: 0.1210s/iter; left time: 4419.3239s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0945175\n",
      "\tspeed: 0.1209s/iter; left time: 4406.3380s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0788761\n",
      "\tspeed: 0.1209s/iter; left time: 4394.1175s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0823496\n",
      "\tspeed: 0.1209s/iter; left time: 4381.3035s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0750478\n",
      "\tspeed: 0.1212s/iter; left time: 4378.3511s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0663611\n",
      "\tspeed: 0.1209s/iter; left time: 4357.0366s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0791697\n",
      "\tspeed: 0.1209s/iter; left time: 4345.9159s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0843642\n",
      "\tspeed: 0.1208s/iter; left time: 4330.2623s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0820326\n",
      "\tspeed: 0.1207s/iter; left time: 4314.0233s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0798094\n",
      "\tspeed: 0.1209s/iter; left time: 4307.9842s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0762156\n",
      "\tspeed: 0.1209s/iter; left time: 4297.3726s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0743921\n",
      "\tspeed: 0.1200s/iter; left time: 4252.7464s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0891102\n",
      "\tspeed: 0.1193s/iter; left time: 4214.2596s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0900708\n",
      "\tspeed: 0.1209s/iter; left time: 4259.5207s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0966768\n",
      "\tspeed: 0.1208s/iter; left time: 4244.8479s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0648704\n",
      "\tspeed: 0.1209s/iter; left time: 4236.0058s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0843473\n",
      "\tspeed: 0.1209s/iter; left time: 4225.1824s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0859990\n",
      "\tspeed: 0.1208s/iter; left time: 4208.2258s\n",
      "Epoch: 7 cost time: 00h:05m:20.14s\n",
      "Epoch: 7 | Train Loss: 0.0812575 Vali Loss: 0.0815965 Test Loss: 0.0964520\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0794488\n",
      "\tspeed: 0.9819s/iter; left time: 34034.9664s\n",
      "\titers: 200, epoch: 8 | loss: 0.0848540\n",
      "\tspeed: 0.1201s/iter; left time: 4151.6897s\n",
      "\titers: 300, epoch: 8 | loss: 0.0769302\n",
      "\tspeed: 0.1194s/iter; left time: 4113.7928s\n",
      "\titers: 400, epoch: 8 | loss: 0.0856332\n",
      "\tspeed: 0.1198s/iter; left time: 4118.2853s\n",
      "\titers: 500, epoch: 8 | loss: 0.0824820\n",
      "\tspeed: 0.1197s/iter; left time: 4100.8118s\n",
      "\titers: 600, epoch: 8 | loss: 0.0854290\n",
      "\tspeed: 0.1194s/iter; left time: 4078.7769s\n",
      "\titers: 700, epoch: 8 | loss: 0.0854288\n",
      "\tspeed: 0.1204s/iter; left time: 4102.4972s\n",
      "\titers: 800, epoch: 8 | loss: 0.0680083\n",
      "\tspeed: 0.1207s/iter; left time: 4099.6089s\n",
      "\titers: 900, epoch: 8 | loss: 0.0768383\n",
      "\tspeed: 0.1200s/iter; left time: 4062.1910s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0693779\n",
      "\tspeed: 0.1189s/iter; left time: 4016.0190s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0754587\n",
      "\tspeed: 0.1195s/iter; left time: 4023.1921s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0802685\n",
      "\tspeed: 0.1192s/iter; left time: 4000.5629s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0774171\n",
      "\tspeed: 0.1195s/iter; left time: 3998.8722s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754229\n",
      "\tspeed: 0.1189s/iter; left time: 3965.7609s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0888831\n",
      "\tspeed: 0.1191s/iter; left time: 3961.5468s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0705786\n",
      "\tspeed: 0.1191s/iter; left time: 3949.1609s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0788224\n",
      "\tspeed: 0.1195s/iter; left time: 3952.6026s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0689727\n",
      "\tspeed: 0.1197s/iter; left time: 3945.4672s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0841972\n",
      "\tspeed: 0.1209s/iter; left time: 3974.3977s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0773610\n",
      "\tspeed: 0.1042s/iter; left time: 3413.5680s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0929009\n",
      "\tspeed: 0.1015s/iter; left time: 3315.0942s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0882238\n",
      "\tspeed: 0.1013s/iter; left time: 3300.0701s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0731173\n",
      "\tspeed: 0.1015s/iter; left time: 3296.4591s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0878904\n",
      "\tspeed: 0.1020s/iter; left time: 3301.4878s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0880044\n",
      "\tspeed: 0.1016s/iter; left time: 3278.1524s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0914146\n",
      "\tspeed: 0.1017s/iter; left time: 3269.9655s\n",
      "Epoch: 8 cost time: 00h:05m:06.70s\n",
      "Epoch: 8 | Train Loss: 0.0798089 Vali Loss: 0.0830694 Test Loss: 0.0950187\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0826529\n",
      "\tspeed: 0.9672s/iter; left time: 30940.0481s\n",
      "\titers: 200, epoch: 9 | loss: 0.0691043\n",
      "\tspeed: 0.1201s/iter; left time: 3830.4263s\n",
      "\titers: 300, epoch: 9 | loss: 0.0827234\n",
      "\tspeed: 0.1191s/iter; left time: 3784.9011s\n",
      "\titers: 400, epoch: 9 | loss: 0.0850475\n",
      "\tspeed: 0.1197s/iter; left time: 3792.7301s\n",
      "\titers: 500, epoch: 9 | loss: 0.0775021\n",
      "\tspeed: 0.1201s/iter; left time: 3792.9018s\n",
      "\titers: 600, epoch: 9 | loss: 0.0795878\n",
      "\tspeed: 0.1207s/iter; left time: 3799.8979s\n",
      "\titers: 700, epoch: 9 | loss: 0.0837808\n",
      "\tspeed: 0.1202s/iter; left time: 3773.3191s\n",
      "\titers: 800, epoch: 9 | loss: 0.0798738\n",
      "\tspeed: 0.1201s/iter; left time: 3757.1875s\n",
      "\titers: 900, epoch: 9 | loss: 0.0779057\n",
      "\tspeed: 0.1193s/iter; left time: 3720.9957s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0727953\n",
      "\tspeed: 0.1185s/iter; left time: 3685.5577s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0807575\n",
      "\tspeed: 0.1183s/iter; left time: 3666.8271s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0787443\n",
      "\tspeed: 0.1191s/iter; left time: 3678.8357s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0788000\n",
      "\tspeed: 0.1192s/iter; left time: 3671.4641s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0729088\n",
      "\tspeed: 0.1187s/iter; left time: 3643.3088s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0709907\n",
      "\tspeed: 0.1190s/iter; left time: 3639.4448s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0749613\n",
      "\tspeed: 0.1184s/iter; left time: 3609.1792s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0788948\n",
      "\tspeed: 0.1183s/iter; left time: 3595.0639s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0830306\n",
      "\tspeed: 0.1185s/iter; left time: 3590.5527s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0778918\n",
      "\tspeed: 0.1192s/iter; left time: 3598.7288s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0789471\n",
      "\tspeed: 0.1186s/iter; left time: 3568.9530s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0754702\n",
      "\tspeed: 0.1188s/iter; left time: 3561.9679s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0705925\n",
      "\tspeed: 0.1195s/iter; left time: 3571.0963s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0731009\n",
      "\tspeed: 0.1182s/iter; left time: 3520.6403s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0785734\n",
      "\tspeed: 0.1182s/iter; left time: 3508.0558s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0922928\n",
      "\tspeed: 0.1187s/iter; left time: 3511.9756s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0796332\n",
      "\tspeed: 0.1184s/iter; left time: 3492.1721s\n",
      "Epoch: 9 cost time: 00h:05m:19.03s\n",
      "Epoch: 9 | Train Loss: 0.0784978 Vali Loss: 0.0850671 Test Loss: 0.0985102\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02088882215321064, rmse:0.14452965557575226, mae:0.09556279331445694, rse:0.4245525300502777\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 96: 01h:00m:35.28s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-01 08:51:32,150] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 08:51:33,307] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 08:51:33,308] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 08:51:33,308] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 08:51:33,412] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 08:51:33,412] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 08:51:34,111] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 08:51:34,112] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 08:51:34,112] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 08:51:34,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 08:51:34,114] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 08:51:34,114] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 08:51:34,114] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 08:51:34,114] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 08:51:34,114] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 08:51:34,115] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 08:51:34,425] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 08:51:34,426] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 08:51:34,426] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 9.9%\n",
      "[2024-11-01 08:51:34,545] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 08:51:34,546] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 08:51:34,546] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 9.9%\n",
      "[2024-11-01 08:51:34,546] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 08:51:34,660] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 08:51:34,661] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 08:51:34,661] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 9.9%\n",
      "[2024-11-01 08:51:34,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 08:51:34,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 08:51:34,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 08:51:34,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 08:51:34,663] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 08:51:34,663] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 08:51:34,663] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 08:51:34,663] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 08:51:34,663] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb2655ba550>\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 08:51:34,664] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 08:51:34,665] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 08:51:34,666] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 08:51:34,666] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 08:51:34,666] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2009432\n",
      "\tspeed: 0.1747s/iter; left time: 9298.7261s\n",
      "\titers: 200, epoch: 1 | loss: 0.2018673\n",
      "\tspeed: 0.1310s/iter; left time: 6962.6375s\n",
      "\titers: 300, epoch: 1 | loss: 0.1664647\n",
      "\tspeed: 0.1305s/iter; left time: 6924.0238s\n",
      "\titers: 400, epoch: 1 | loss: 0.1907074\n",
      "\tspeed: 0.1314s/iter; left time: 6956.6166s\n",
      "\titers: 500, epoch: 1 | loss: 0.1524435\n",
      "\tspeed: 0.1314s/iter; left time: 6943.1725s\n",
      "\titers: 600, epoch: 1 | loss: 0.1277965\n",
      "\tspeed: 0.1310s/iter; left time: 6907.5972s\n",
      "\titers: 700, epoch: 1 | loss: 0.1040149\n",
      "\tspeed: 0.1319s/iter; left time: 6945.5219s\n",
      "\titers: 800, epoch: 1 | loss: 0.1075614\n",
      "\tspeed: 0.1324s/iter; left time: 6955.5543s\n",
      "\titers: 900, epoch: 1 | loss: 0.1123559\n",
      "\tspeed: 0.1312s/iter; left time: 6882.5495s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1132720\n",
      "\tspeed: 0.1323s/iter; left time: 6923.2595s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0960071\n",
      "\tspeed: 0.1341s/iter; left time: 7007.2147s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1164301\n",
      "\tspeed: 0.1340s/iter; left time: 6986.4790s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1128735\n",
      "\tspeed: 0.1386s/iter; left time: 7212.9009s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1040649\n",
      "\tspeed: 0.1381s/iter; left time: 7172.0965s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1173225\n",
      "\tspeed: 0.1367s/iter; left time: 7087.1683s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1031440\n",
      "\tspeed: 0.1357s/iter; left time: 7022.7082s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0966299\n",
      "\tspeed: 0.1365s/iter; left time: 7050.3313s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1064342\n",
      "\tspeed: 0.1373s/iter; left time: 7074.5296s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0987400\n",
      "\tspeed: 0.1397s/iter; left time: 7184.2855s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1042845\n",
      "\tspeed: 0.1376s/iter; left time: 7062.8991s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0977351\n",
      "\tspeed: 0.1341s/iter; left time: 6873.9340s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0965085\n",
      "\tspeed: 0.1368s/iter; left time: 6997.5355s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0990303\n",
      "\tspeed: 0.1378s/iter; left time: 7031.0795s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1100298\n",
      "\tspeed: 0.1376s/iter; left time: 7007.4443s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0987217\n",
      "\tspeed: 0.1347s/iter; left time: 6850.7436s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1110397\n",
      "\tspeed: 0.1366s/iter; left time: 6930.4782s\n",
      "Epoch: 1 cost time: 00h:06m:00.44s\n",
      "Epoch: 1 | Train Loss: 0.1213860 Vali Loss: 0.0894020 Test Loss: 0.1034134\n",
      "Validation loss decreased (inf --> 0.089402).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0940944\n",
      "\tspeed: 1.1583s/iter; left time: 58580.2074s\n",
      "\titers: 200, epoch: 2 | loss: 0.0937534\n",
      "\tspeed: 0.1211s/iter; left time: 6113.4005s\n",
      "\titers: 300, epoch: 2 | loss: 0.1080840\n",
      "\tspeed: 0.1210s/iter; left time: 6097.1060s\n",
      "\titers: 400, epoch: 2 | loss: 0.0966061\n",
      "\tspeed: 0.1213s/iter; left time: 6099.9390s\n",
      "\titers: 500, epoch: 2 | loss: 0.1093938\n",
      "\tspeed: 0.1211s/iter; left time: 6075.9139s\n",
      "\titers: 600, epoch: 2 | loss: 0.0923856\n",
      "\tspeed: 0.1210s/iter; left time: 6059.9944s\n",
      "\titers: 700, epoch: 2 | loss: 0.0916145\n",
      "\tspeed: 0.1210s/iter; left time: 6044.4129s\n",
      "\titers: 800, epoch: 2 | loss: 0.0939326\n",
      "\tspeed: 0.1270s/iter; left time: 6334.5841s\n",
      "\titers: 900, epoch: 2 | loss: 0.1001902\n",
      "\tspeed: 0.1224s/iter; left time: 6093.3779s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0946725\n",
      "\tspeed: 0.1216s/iter; left time: 6040.9823s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0976080\n",
      "\tspeed: 0.1278s/iter; left time: 6333.9909s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017308\n",
      "\tspeed: 0.1228s/iter; left time: 6076.6304s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1020761\n",
      "\tspeed: 0.1233s/iter; left time: 6085.5197s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0914178\n",
      "\tspeed: 0.1252s/iter; left time: 6167.6726s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1076127\n",
      "\tspeed: 0.1282s/iter; left time: 6305.9078s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0961113\n",
      "\tspeed: 0.1237s/iter; left time: 6071.0985s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1049188\n",
      "\tspeed: 0.1224s/iter; left time: 5993.4304s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0905886\n",
      "\tspeed: 0.1245s/iter; left time: 6084.7285s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0959672\n",
      "\tspeed: 0.1236s/iter; left time: 6026.2853s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0967659\n",
      "\tspeed: 0.1231s/iter; left time: 5990.0663s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1091169\n",
      "\tspeed: 0.1255s/iter; left time: 6096.4158s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0912857\n",
      "\tspeed: 0.1252s/iter; left time: 6067.9433s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0814347\n",
      "\tspeed: 0.1250s/iter; left time: 6044.3762s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1000843\n",
      "\tspeed: 0.1259s/iter; left time: 6077.9370s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0908363\n",
      "\tspeed: 0.1233s/iter; left time: 5940.1597s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0960613\n",
      "\tspeed: 0.1229s/iter; left time: 5910.0111s\n",
      "Epoch: 2 cost time: 00h:05m:30.20s\n",
      "Epoch: 2 | Train Loss: 0.0975315 Vali Loss: 0.0867351 Test Loss: 0.0988254\n",
      "Validation loss decreased (0.089402 --> 0.086735).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0956383\n",
      "\tspeed: 1.0111s/iter; left time: 48439.5313s\n",
      "\titers: 200, epoch: 3 | loss: 0.0700900\n",
      "\tspeed: 0.1210s/iter; left time: 5785.0743s\n",
      "\titers: 300, epoch: 3 | loss: 0.0907799\n",
      "\tspeed: 0.1237s/iter; left time: 5901.0291s\n",
      "\titers: 400, epoch: 3 | loss: 0.1059120\n",
      "\tspeed: 0.1211s/iter; left time: 5764.3139s\n",
      "\titers: 500, epoch: 3 | loss: 0.0847025\n",
      "\tspeed: 0.1252s/iter; left time: 5947.8382s\n",
      "\titers: 600, epoch: 3 | loss: 0.1060325\n",
      "\tspeed: 0.1260s/iter; left time: 5973.7358s\n",
      "\titers: 700, epoch: 3 | loss: 0.0842078\n",
      "\tspeed: 0.1254s/iter; left time: 5933.6104s\n",
      "\titers: 800, epoch: 3 | loss: 0.1065284\n",
      "\tspeed: 0.1280s/iter; left time: 6044.3419s\n",
      "\titers: 900, epoch: 3 | loss: 0.0883381\n",
      "\tspeed: 0.1258s/iter; left time: 5926.1728s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0973774\n",
      "\tspeed: 0.1240s/iter; left time: 5826.7167s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1029167\n",
      "\tspeed: 0.1263s/iter; left time: 5924.9679s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1063046\n",
      "\tspeed: 0.1249s/iter; left time: 5848.5177s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0910941\n",
      "\tspeed: 0.1252s/iter; left time: 5848.2638s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0989402\n",
      "\tspeed: 0.1244s/iter; left time: 5796.8497s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0897562\n",
      "\tspeed: 0.1248s/iter; left time: 5806.1507s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1038726\n",
      "\tspeed: 0.1234s/iter; left time: 5728.8166s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1029803\n",
      "\tspeed: 0.1230s/iter; left time: 5697.1814s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0976344\n",
      "\tspeed: 0.1254s/iter; left time: 5795.1136s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0954050\n",
      "\tspeed: 0.1267s/iter; left time: 5840.7057s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0907561\n",
      "\tspeed: 0.1243s/iter; left time: 5719.1525s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1184174\n",
      "\tspeed: 0.1246s/iter; left time: 5718.2625s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0903510\n",
      "\tspeed: 0.1227s/iter; left time: 5621.3098s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0935952\n",
      "\tspeed: 0.1255s/iter; left time: 5734.8855s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0925637\n",
      "\tspeed: 0.1246s/iter; left time: 5681.4104s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0878491\n",
      "\tspeed: 0.1232s/iter; left time: 5607.6450s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1013690\n",
      "\tspeed: 0.1243s/iter; left time: 5642.8874s\n",
      "Epoch: 3 cost time: 00h:05m:32.53s\n",
      "Epoch: 3 | Train Loss: 0.0937625 Vali Loss: 0.0846537 Test Loss: 0.0967681\n",
      "Validation loss decreased (0.086735 --> 0.084654).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0943716\n",
      "\tspeed: 1.0252s/iter; left time: 46378.2763s\n",
      "\titers: 200, epoch: 4 | loss: 0.0859998\n",
      "\tspeed: 0.1253s/iter; left time: 5655.0744s\n",
      "\titers: 300, epoch: 4 | loss: 0.0874903\n",
      "\tspeed: 0.1238s/iter; left time: 5573.7535s\n",
      "\titers: 400, epoch: 4 | loss: 0.1020916\n",
      "\tspeed: 0.1207s/iter; left time: 5425.0566s\n",
      "\titers: 500, epoch: 4 | loss: 0.0874067\n",
      "\tspeed: 0.1232s/iter; left time: 5522.5633s\n",
      "\titers: 600, epoch: 4 | loss: 0.1072563\n",
      "\tspeed: 0.1241s/iter; left time: 5553.2763s\n",
      "\titers: 700, epoch: 4 | loss: 0.0964151\n",
      "\tspeed: 0.1276s/iter; left time: 5695.1298s\n",
      "\titers: 800, epoch: 4 | loss: 0.0861687\n",
      "\tspeed: 0.1246s/iter; left time: 5549.8543s\n",
      "\titers: 900, epoch: 4 | loss: 0.0826063\n",
      "\tspeed: 0.1228s/iter; left time: 5457.9045s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0798693\n",
      "\tspeed: 0.1227s/iter; left time: 5439.7553s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0928330\n",
      "\tspeed: 0.1249s/iter; left time: 5523.6816s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0862791\n",
      "\tspeed: 0.1213s/iter; left time: 5354.4790s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0830672\n",
      "\tspeed: 0.1236s/iter; left time: 5444.9874s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0836192\n",
      "\tspeed: 0.1264s/iter; left time: 5552.6533s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0898090\n",
      "\tspeed: 0.1274s/iter; left time: 5586.3566s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0897531\n",
      "\tspeed: 0.1289s/iter; left time: 5639.5776s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0794996\n",
      "\tspeed: 0.1250s/iter; left time: 5454.2893s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0978874\n",
      "\tspeed: 0.1252s/iter; left time: 5450.0811s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0965451\n",
      "\tspeed: 0.1261s/iter; left time: 5476.9408s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0926374\n",
      "\tspeed: 0.1249s/iter; left time: 5414.4854s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1069342\n",
      "\tspeed: 0.1238s/iter; left time: 5353.9662s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0779019\n",
      "\tspeed: 0.1226s/iter; left time: 5290.9306s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0973944\n",
      "\tspeed: 0.1237s/iter; left time: 5324.0519s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0905181\n",
      "\tspeed: 0.1245s/iter; left time: 5346.3310s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0938979\n",
      "\tspeed: 0.1257s/iter; left time: 5384.2739s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0907974\n",
      "\tspeed: 0.1224s/iter; left time: 5231.6316s\n",
      "Epoch: 4 cost time: 00h:05m:32.69s\n",
      "Epoch: 4 | Train Loss: 0.0911554 Vali Loss: 0.0860957 Test Loss: 0.0986955\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0987655\n",
      "\tspeed: 1.0020s/iter; left time: 42659.2306s\n",
      "\titers: 200, epoch: 5 | loss: 0.0867120\n",
      "\tspeed: 0.1229s/iter; left time: 5218.8214s\n",
      "\titers: 300, epoch: 5 | loss: 0.0962949\n",
      "\tspeed: 0.1225s/iter; left time: 5188.6062s\n",
      "\titers: 400, epoch: 5 | loss: 0.0800745\n",
      "\tspeed: 0.1210s/iter; left time: 5115.4481s\n",
      "\titers: 500, epoch: 5 | loss: 0.0932057\n",
      "\tspeed: 0.1209s/iter; left time: 5099.6139s\n",
      "\titers: 600, epoch: 5 | loss: 0.0953641\n",
      "\tspeed: 0.1209s/iter; left time: 5087.6144s\n",
      "\titers: 700, epoch: 5 | loss: 0.0878022\n",
      "\tspeed: 0.1214s/iter; left time: 5097.1209s\n",
      "\titers: 800, epoch: 5 | loss: 0.0875066\n",
      "\tspeed: 0.1269s/iter; left time: 5312.8693s\n",
      "\titers: 900, epoch: 5 | loss: 0.0935306\n",
      "\tspeed: 0.1231s/iter; left time: 5142.1674s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0829855\n",
      "\tspeed: 0.1220s/iter; left time: 5083.7024s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0839508\n",
      "\tspeed: 0.1249s/iter; left time: 5190.4297s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0897008\n",
      "\tspeed: 0.1217s/iter; left time: 5047.7393s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0881073\n",
      "\tspeed: 0.1213s/iter; left time: 5016.9101s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0853253\n",
      "\tspeed: 0.1221s/iter; left time: 5037.6247s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0889603\n",
      "\tspeed: 0.1213s/iter; left time: 4995.7151s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1002251\n",
      "\tspeed: 0.1224s/iter; left time: 5025.9293s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1032137\n",
      "\tspeed: 0.1209s/iter; left time: 4953.0105s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0937006\n",
      "\tspeed: 0.1179s/iter; left time: 4819.2304s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0855721\n",
      "\tspeed: 0.1217s/iter; left time: 4962.4263s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0733401\n",
      "\tspeed: 0.1213s/iter; left time: 4934.6589s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0945749\n",
      "\tspeed: 0.1212s/iter; left time: 4915.5249s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0890472\n",
      "\tspeed: 0.1216s/iter; left time: 4919.5263s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0819968\n",
      "\tspeed: 0.1219s/iter; left time: 4923.2759s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0929638\n",
      "\tspeed: 0.1214s/iter; left time: 4889.8668s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0771377\n",
      "\tspeed: 0.1218s/iter; left time: 4892.6592s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0927820\n",
      "\tspeed: 0.1209s/iter; left time: 4844.7317s\n",
      "Epoch: 5 cost time: 00h:05m:25.60s\n",
      "Epoch: 5 | Train Loss: 0.0889107 Vali Loss: 0.0877582 Test Loss: 0.1025786\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0778098\n",
      "\tspeed: 0.9853s/iter; left time: 39318.8701s\n",
      "\titers: 200, epoch: 6 | loss: 0.0997249\n",
      "\tspeed: 0.1209s/iter; left time: 4812.0434s\n",
      "\titers: 300, epoch: 6 | loss: 0.0860444\n",
      "\tspeed: 0.1205s/iter; left time: 4785.6072s\n",
      "\titers: 400, epoch: 6 | loss: 0.0840693\n",
      "\tspeed: 0.1208s/iter; left time: 4784.3181s\n",
      "\titers: 500, epoch: 6 | loss: 0.0925132\n",
      "\tspeed: 0.1218s/iter; left time: 4810.5278s\n",
      "\titers: 600, epoch: 6 | loss: 0.0898937\n",
      "\tspeed: 0.1219s/iter; left time: 4801.9659s\n",
      "\titers: 700, epoch: 6 | loss: 0.0814919\n",
      "\tspeed: 0.1212s/iter; left time: 4765.3109s\n",
      "\titers: 800, epoch: 6 | loss: 0.1018386\n",
      "\tspeed: 0.1223s/iter; left time: 4796.2991s\n",
      "\titers: 900, epoch: 6 | loss: 0.0892932\n",
      "\tspeed: 0.1220s/iter; left time: 4771.4083s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1022972\n",
      "\tspeed: 0.1215s/iter; left time: 4739.7651s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0904634\n",
      "\tspeed: 0.1219s/iter; left time: 4743.9487s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0951061\n",
      "\tspeed: 0.1215s/iter; left time: 4715.2855s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0853480\n",
      "\tspeed: 0.1216s/iter; left time: 4707.4551s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0741242\n",
      "\tspeed: 0.1220s/iter; left time: 4710.1310s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0868103\n",
      "\tspeed: 0.1221s/iter; left time: 4703.2494s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0908482\n",
      "\tspeed: 0.1210s/iter; left time: 4646.4541s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0931770\n",
      "\tspeed: 0.1210s/iter; left time: 4636.3783s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0878102\n",
      "\tspeed: 0.1216s/iter; left time: 4644.2799s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0864693\n",
      "\tspeed: 0.1212s/iter; left time: 4619.1132s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0849704\n",
      "\tspeed: 0.1215s/iter; left time: 4616.8922s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0827954\n",
      "\tspeed: 0.1224s/iter; left time: 4641.5312s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0875449\n",
      "\tspeed: 0.1215s/iter; left time: 4593.1797s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0922521\n",
      "\tspeed: 0.1210s/iter; left time: 4562.5609s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0752860\n",
      "\tspeed: 0.1224s/iter; left time: 4601.1472s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0840485\n",
      "\tspeed: 0.1214s/iter; left time: 4553.4182s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0845857\n",
      "\tspeed: 0.1218s/iter; left time: 4555.7339s\n",
      "Epoch: 6 cost time: 00h:05m:24.67s\n",
      "Epoch: 6 | Train Loss: 0.0867293 Vali Loss: 0.0884304 Test Loss: 0.1025518\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0863808\n",
      "\tspeed: 0.9872s/iter; left time: 36762.2306s\n",
      "\titers: 200, epoch: 7 | loss: 0.0894201\n",
      "\tspeed: 0.1209s/iter; left time: 4491.5510s\n",
      "\titers: 300, epoch: 7 | loss: 0.0827672\n",
      "\tspeed: 0.1215s/iter; left time: 4501.7148s\n",
      "\titers: 400, epoch: 7 | loss: 0.0769152\n",
      "\tspeed: 0.1217s/iter; left time: 4495.2310s\n",
      "\titers: 500, epoch: 7 | loss: 0.0987583\n",
      "\tspeed: 0.1219s/iter; left time: 4492.3762s\n",
      "\titers: 600, epoch: 7 | loss: 0.0983056\n",
      "\tspeed: 0.1217s/iter; left time: 4471.4719s\n",
      "\titers: 700, epoch: 7 | loss: 0.0806711\n",
      "\tspeed: 0.1211s/iter; left time: 4437.7577s\n",
      "\titers: 800, epoch: 7 | loss: 0.0801147\n",
      "\tspeed: 0.1217s/iter; left time: 4448.3418s\n",
      "\titers: 900, epoch: 7 | loss: 0.0864820\n",
      "\tspeed: 0.1211s/iter; left time: 4413.1006s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0967652\n",
      "\tspeed: 0.1211s/iter; left time: 4400.4862s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0854905\n",
      "\tspeed: 0.1218s/iter; left time: 4413.8199s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0898021\n",
      "\tspeed: 0.1215s/iter; left time: 4391.5048s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0775288\n",
      "\tspeed: 0.1211s/iter; left time: 4364.7440s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0811367\n",
      "\tspeed: 0.1220s/iter; left time: 4384.5554s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0821564\n",
      "\tspeed: 0.1221s/iter; left time: 4376.5042s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0834867\n",
      "\tspeed: 0.1209s/iter; left time: 4319.9650s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0906567\n",
      "\tspeed: 0.1214s/iter; left time: 4325.1408s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0855301\n",
      "\tspeed: 0.1215s/iter; left time: 4318.3206s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0763849\n",
      "\tspeed: 0.1217s/iter; left time: 4314.5973s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0703013\n",
      "\tspeed: 0.1210s/iter; left time: 4276.2867s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0814764\n",
      "\tspeed: 0.1210s/iter; left time: 4264.3762s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0879330\n",
      "\tspeed: 0.1218s/iter; left time: 4280.7708s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0835790\n",
      "\tspeed: 0.1211s/iter; left time: 4244.9109s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0896361\n",
      "\tspeed: 0.1218s/iter; left time: 4255.1946s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0880383\n",
      "\tspeed: 0.1086s/iter; left time: 3781.9075s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0784809\n",
      "\tspeed: 0.1211s/iter; left time: 4206.5790s\n",
      "Epoch: 7 cost time: 00h:05m:23.26s\n",
      "Epoch: 7 | Train Loss: 0.0847558 Vali Loss: 0.0862495 Test Loss: 0.0994056\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0741981\n",
      "\tspeed: 0.9810s/iter; left time: 33914.3502s\n",
      "\titers: 200, epoch: 8 | loss: 0.0925967\n",
      "\tspeed: 0.1212s/iter; left time: 4178.9310s\n",
      "\titers: 300, epoch: 8 | loss: 0.0885595\n",
      "\tspeed: 0.1228s/iter; left time: 4222.0146s\n",
      "\titers: 400, epoch: 8 | loss: 0.0859595\n",
      "\tspeed: 0.1206s/iter; left time: 4133.5993s\n",
      "\titers: 500, epoch: 8 | loss: 0.0826866\n",
      "\tspeed: 0.1214s/iter; left time: 4146.8640s\n",
      "\titers: 600, epoch: 8 | loss: 0.0911786\n",
      "\tspeed: 0.1172s/iter; left time: 3994.1187s\n",
      "\titers: 700, epoch: 8 | loss: 0.0777338\n",
      "\tspeed: 0.1022s/iter; left time: 3472.4484s\n",
      "\titers: 800, epoch: 8 | loss: 0.0907550\n",
      "\tspeed: 0.1116s/iter; left time: 3780.6471s\n",
      "\titers: 900, epoch: 8 | loss: 0.0851398\n",
      "\tspeed: 0.1206s/iter; left time: 4072.8584s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0833690\n",
      "\tspeed: 0.1202s/iter; left time: 4048.5154s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0865234\n",
      "\tspeed: 0.1221s/iter; left time: 4098.9670s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0920289\n",
      "\tspeed: 0.1208s/iter; left time: 4043.0623s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0871832\n",
      "\tspeed: 0.1211s/iter; left time: 4040.1468s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0754475\n",
      "\tspeed: 0.1219s/iter; left time: 4056.4614s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0724243\n",
      "\tspeed: 0.1219s/iter; left time: 4043.3316s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0741268\n",
      "\tspeed: 0.1235s/iter; left time: 4084.5362s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0789550\n",
      "\tspeed: 0.1219s/iter; left time: 4018.3443s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0848181\n",
      "\tspeed: 0.1211s/iter; left time: 3980.7807s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0913343\n",
      "\tspeed: 0.1215s/iter; left time: 3982.0553s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0802559\n",
      "\tspeed: 0.1212s/iter; left time: 3960.6434s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0726332\n",
      "\tspeed: 0.1208s/iter; left time: 3935.0157s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0801809\n",
      "\tspeed: 0.1210s/iter; left time: 3929.5657s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0922091\n",
      "\tspeed: 0.1212s/iter; left time: 3922.2804s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0758863\n",
      "\tspeed: 0.1217s/iter; left time: 3928.1347s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0947716\n",
      "\tspeed: 0.1214s/iter; left time: 3904.1909s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0881028\n",
      "\tspeed: 0.1208s/iter; left time: 3872.8132s\n",
      "Epoch: 8 cost time: 00h:05m:21.09s\n",
      "Epoch: 8 | Train Loss: 0.0829597 Vali Loss: 0.0875380 Test Loss: 0.1024083\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02114104852080345, rmse:0.14539961516857147, mae:0.09676817059516907, rse:0.4269324839115143\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 168: 00h:56m:01.46s\n",
      "\n",
      "Intermediate time for ES: 04h:04m:27.92s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-01 09:47:33,435] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 09:47:34,670] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 09:47:34,670] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 09:47:34,670] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 09:47:34,778] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 09:47:34,778] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 09:47:35,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 09:47:35,476] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 09:47:35,476] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 09:47:35,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 09:47:35,478] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 09:47:35,478] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 09:47:35,478] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 09:47:35,478] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 09:47:35,478] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 09:47:35,478] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 09:47:35,789] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 09:47:35,790] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 09:47:35,790] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.58 GB, percent = 9.9%\n",
      "[2024-11-01 09:47:35,932] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 09:47:35,933] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 09:47:35,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.58 GB, percent = 9.9%\n",
      "[2024-11-01 09:47:35,933] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 09:47:36,049] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 09:47:36,050] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 09:47:36,050] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.58 GB, percent = 9.9%\n",
      "[2024-11-01 09:47:36,051] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 09:47:36,051] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 09:47:36,051] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 09:47:36,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 09:47:36,052] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 09:47:36,052] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 09:47:36,052] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 09:47:36,052] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 09:47:36,052] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd380ca1fd0>\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 09:47:36,053] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 09:47:36,054] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 09:47:36,055] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 09:47:36,055] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 09:47:36,055] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 09:47:36,055] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 09:47:36,055] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1535002\n",
      "\tspeed: 0.1767s/iter; left time: 9456.9561s\n",
      "\titers: 200, epoch: 1 | loss: 0.1159472\n",
      "\tspeed: 0.1306s/iter; left time: 6977.8591s\n",
      "\titers: 300, epoch: 1 | loss: 0.1058322\n",
      "\tspeed: 0.1308s/iter; left time: 6976.8535s\n",
      "\titers: 400, epoch: 1 | loss: 0.0828711\n",
      "\tspeed: 0.1309s/iter; left time: 6964.6522s\n",
      "\titers: 500, epoch: 1 | loss: 0.0717254\n",
      "\tspeed: 0.1308s/iter; left time: 6946.9785s\n",
      "\titers: 600, epoch: 1 | loss: 0.0627306\n",
      "\tspeed: 0.1309s/iter; left time: 6938.6058s\n",
      "\titers: 700, epoch: 1 | loss: 0.0745939\n",
      "\tspeed: 0.1307s/iter; left time: 6916.4001s\n",
      "\titers: 800, epoch: 1 | loss: 0.0592833\n",
      "\tspeed: 0.1309s/iter; left time: 6916.8188s\n",
      "\titers: 900, epoch: 1 | loss: 0.0581057\n",
      "\tspeed: 0.1308s/iter; left time: 6895.9582s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0899070\n",
      "\tspeed: 0.1311s/iter; left time: 6898.2324s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0486953\n",
      "\tspeed: 0.1308s/iter; left time: 6868.7595s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0688207\n",
      "\tspeed: 0.1309s/iter; left time: 6861.1983s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0650532\n",
      "\tspeed: 0.1310s/iter; left time: 6852.9696s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0587849\n",
      "\tspeed: 0.1308s/iter; left time: 6832.8113s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0618791\n",
      "\tspeed: 0.1308s/iter; left time: 6815.5254s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0509475\n",
      "\tspeed: 0.1302s/iter; left time: 6772.8913s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0735843\n",
      "\tspeed: 0.1304s/iter; left time: 6770.2498s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0482276\n",
      "\tspeed: 0.1308s/iter; left time: 6777.4642s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0562918\n",
      "\tspeed: 0.1304s/iter; left time: 6746.8120s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0636575\n",
      "\tspeed: 0.1307s/iter; left time: 6746.6387s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0499355\n",
      "\tspeed: 0.1304s/iter; left time: 6718.0000s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0701026\n",
      "\tspeed: 0.1309s/iter; left time: 6729.1525s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0674836\n",
      "\tspeed: 0.1308s/iter; left time: 6711.1604s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0581415\n",
      "\tspeed: 0.1306s/iter; left time: 6691.8900s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0673574\n",
      "\tspeed: 0.1308s/iter; left time: 6684.2609s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0543020\n",
      "\tspeed: 0.1309s/iter; left time: 6676.6414s\n",
      "Epoch: 1 cost time: 00h:05m:51.82s\n",
      "Epoch: 1 | Train Loss: 0.0739179 Vali Loss: 0.0605835 Test Loss: 0.0654644\n",
      "Validation loss decreased (inf --> 0.060583).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0619027\n",
      "\tspeed: 1.1601s/iter; left time: 58981.6326s\n",
      "\titers: 200, epoch: 2 | loss: 0.0812935\n",
      "\tspeed: 0.1206s/iter; left time: 6117.9054s\n",
      "\titers: 300, epoch: 2 | loss: 0.0682675\n",
      "\tspeed: 0.1208s/iter; left time: 6116.1616s\n",
      "\titers: 400, epoch: 2 | loss: 0.0864105\n",
      "\tspeed: 0.1208s/iter; left time: 6103.5345s\n",
      "\titers: 500, epoch: 2 | loss: 0.0529756\n",
      "\tspeed: 0.1207s/iter; left time: 6090.2955s\n",
      "\titers: 600, epoch: 2 | loss: 0.0543025\n",
      "\tspeed: 0.1204s/iter; left time: 6062.0852s\n",
      "\titers: 700, epoch: 2 | loss: 0.0502084\n",
      "\tspeed: 0.1206s/iter; left time: 6060.1081s\n",
      "\titers: 800, epoch: 2 | loss: 0.0579205\n",
      "\tspeed: 0.1202s/iter; left time: 6026.8903s\n",
      "\titers: 900, epoch: 2 | loss: 0.0560098\n",
      "\tspeed: 0.1207s/iter; left time: 6041.6591s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0432923\n",
      "\tspeed: 0.1190s/iter; left time: 5944.9101s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0533719\n",
      "\tspeed: 0.1204s/iter; left time: 6002.5974s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0699260\n",
      "\tspeed: 0.1210s/iter; left time: 6019.6795s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0667628\n",
      "\tspeed: 0.1206s/iter; left time: 5987.4322s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0512179\n",
      "\tspeed: 0.1207s/iter; left time: 5978.6194s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0579801\n",
      "\tspeed: 0.1210s/iter; left time: 5984.1324s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0674562\n",
      "\tspeed: 0.1212s/iter; left time: 5978.8969s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0445822\n",
      "\tspeed: 0.1206s/iter; left time: 5937.7977s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0552087\n",
      "\tspeed: 0.1204s/iter; left time: 5915.8043s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0708642\n",
      "\tspeed: 0.1200s/iter; left time: 5886.4199s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0525380\n",
      "\tspeed: 0.1209s/iter; left time: 5918.0652s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0523798\n",
      "\tspeed: 0.1207s/iter; left time: 5894.2877s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0721295\n",
      "\tspeed: 0.1203s/iter; left time: 5865.6717s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0610736\n",
      "\tspeed: 0.1209s/iter; left time: 5880.8741s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0564372\n",
      "\tspeed: 0.1208s/iter; left time: 5863.7943s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0445440\n",
      "\tspeed: 0.1211s/iter; left time: 5865.4601s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0545508\n",
      "\tspeed: 0.1207s/iter; left time: 5835.2821s\n",
      "Epoch: 2 cost time: 00h:05m:23.84s\n",
      "Epoch: 2 | Train Loss: 0.0589849 Vali Loss: 0.0589729 Test Loss: 0.0641147\n",
      "Validation loss decreased (0.060583 --> 0.058973).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0636515\n",
      "\tspeed: 1.0160s/iter; left time: 48927.5380s\n",
      "\titers: 200, epoch: 3 | loss: 0.0587107\n",
      "\tspeed: 0.1025s/iter; left time: 4924.6393s\n",
      "\titers: 300, epoch: 3 | loss: 0.0566106\n",
      "\tspeed: 0.1142s/iter; left time: 5475.7523s\n",
      "\titers: 400, epoch: 3 | loss: 0.0612787\n",
      "\tspeed: 0.1210s/iter; left time: 5788.9074s\n",
      "\titers: 500, epoch: 3 | loss: 0.0468024\n",
      "\tspeed: 0.1202s/iter; left time: 5742.4917s\n",
      "\titers: 600, epoch: 3 | loss: 0.0613940\n",
      "\tspeed: 0.1217s/iter; left time: 5797.8605s\n",
      "\titers: 700, epoch: 3 | loss: 0.0506664\n",
      "\tspeed: 0.1201s/iter; left time: 5714.1696s\n",
      "\titers: 800, epoch: 3 | loss: 0.0535050\n",
      "\tspeed: 0.1205s/iter; left time: 5719.1032s\n",
      "\titers: 900, epoch: 3 | loss: 0.0552201\n",
      "\tspeed: 0.1210s/iter; left time: 5731.9854s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0510626\n",
      "\tspeed: 0.1208s/iter; left time: 5709.5338s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0414352\n",
      "\tspeed: 0.1204s/iter; left time: 5676.8729s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0635938\n",
      "\tspeed: 0.1203s/iter; left time: 5659.0936s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0423877\n",
      "\tspeed: 0.1204s/iter; left time: 5654.3160s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0537974\n",
      "\tspeed: 0.1209s/iter; left time: 5663.5680s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0664770\n",
      "\tspeed: 0.1213s/iter; left time: 5671.2559s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0489237\n",
      "\tspeed: 0.1210s/iter; left time: 5646.0563s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0435316\n",
      "\tspeed: 0.1209s/iter; left time: 5629.9411s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0485576\n",
      "\tspeed: 0.1211s/iter; left time: 5626.2829s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0613631\n",
      "\tspeed: 0.1207s/iter; left time: 5595.8934s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0441004\n",
      "\tspeed: 0.1205s/iter; left time: 5576.2035s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0450289\n",
      "\tspeed: 0.1209s/iter; left time: 5580.6217s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0547356\n",
      "\tspeed: 0.1199s/iter; left time: 5522.4641s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0599108\n",
      "\tspeed: 0.1206s/iter; left time: 5543.7689s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0434626\n",
      "\tspeed: 0.1202s/iter; left time: 5512.1879s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0669618\n",
      "\tspeed: 0.1206s/iter; left time: 5519.0605s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0644993\n",
      "\tspeed: 0.1205s/iter; left time: 5501.0091s\n",
      "Epoch: 3 cost time: 00h:05m:21.19s\n",
      "Epoch: 3 | Train Loss: 0.0563925 Vali Loss: 0.0578105 Test Loss: 0.0628996\n",
      "Validation loss decreased (0.058973 --> 0.057811).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0500212\n",
      "\tspeed: 1.0162s/iter; left time: 46213.7863s\n",
      "\titers: 200, epoch: 4 | loss: 0.0662548\n",
      "\tspeed: 0.1204s/iter; left time: 5464.4848s\n",
      "\titers: 300, epoch: 4 | loss: 0.0432065\n",
      "\tspeed: 0.1207s/iter; left time: 5466.1180s\n",
      "\titers: 400, epoch: 4 | loss: 0.0570253\n",
      "\tspeed: 0.1199s/iter; left time: 5415.9655s\n",
      "\titers: 500, epoch: 4 | loss: 0.0514172\n",
      "\tspeed: 0.1205s/iter; left time: 5431.6101s\n",
      "\titers: 600, epoch: 4 | loss: 0.0459452\n",
      "\tspeed: 0.1091s/iter; left time: 4908.1816s\n",
      "\titers: 700, epoch: 4 | loss: 0.0489160\n",
      "\tspeed: 0.1208s/iter; left time: 5422.1681s\n",
      "\titers: 800, epoch: 4 | loss: 0.0543107\n",
      "\tspeed: 0.1202s/iter; left time: 5381.8183s\n",
      "\titers: 900, epoch: 4 | loss: 0.0444514\n",
      "\tspeed: 0.1201s/iter; left time: 5366.4132s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0546588\n",
      "\tspeed: 0.1207s/iter; left time: 5382.3771s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0588563\n",
      "\tspeed: 0.1208s/iter; left time: 5374.2767s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0534603\n",
      "\tspeed: 0.1149s/iter; left time: 5100.9132s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0535597\n",
      "\tspeed: 0.1205s/iter; left time: 5335.0038s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0503390\n",
      "\tspeed: 0.1206s/iter; left time: 5326.8381s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0542765\n",
      "\tspeed: 0.1205s/iter; left time: 5311.8158s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0546105\n",
      "\tspeed: 0.1213s/iter; left time: 5333.3962s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0538735\n",
      "\tspeed: 0.1206s/iter; left time: 5290.2169s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0518390\n",
      "\tspeed: 0.1203s/iter; left time: 5266.6494s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0519650\n",
      "\tspeed: 0.1201s/iter; left time: 5244.7686s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0477860\n",
      "\tspeed: 0.1195s/iter; left time: 5206.5835s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0585879\n",
      "\tspeed: 0.1210s/iter; left time: 5260.3706s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0599166\n",
      "\tspeed: 0.1203s/iter; left time: 5218.4697s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0504315\n",
      "\tspeed: 0.1197s/iter; left time: 5179.5132s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0512709\n",
      "\tspeed: 0.1201s/iter; left time: 5186.8751s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0432640\n",
      "\tspeed: 0.1206s/iter; left time: 5195.3247s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0526977\n",
      "\tspeed: 0.1207s/iter; left time: 5188.1003s\n",
      "Epoch: 4 cost time: 00h:05m:21.74s\n",
      "Epoch: 4 | Train Loss: 0.0547071 Vali Loss: 0.0565705 Test Loss: 0.0614690\n",
      "Validation loss decreased (0.057811 --> 0.056570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0500493\n",
      "\tspeed: 1.0188s/iter; left time: 43599.9651s\n",
      "\titers: 200, epoch: 5 | loss: 0.0452871\n",
      "\tspeed: 0.1208s/iter; left time: 5158.5571s\n",
      "\titers: 300, epoch: 5 | loss: 0.0708607\n",
      "\tspeed: 0.1207s/iter; left time: 5141.8266s\n",
      "\titers: 400, epoch: 5 | loss: 0.0393255\n",
      "\tspeed: 0.1207s/iter; left time: 5129.9464s\n",
      "\titers: 500, epoch: 5 | loss: 0.0584477\n",
      "\tspeed: 0.1202s/iter; left time: 5096.1367s\n",
      "\titers: 600, epoch: 5 | loss: 0.0544610\n",
      "\tspeed: 0.1114s/iter; left time: 4710.6961s\n",
      "\titers: 700, epoch: 5 | loss: 0.0538923\n",
      "\tspeed: 0.1191s/iter; left time: 5025.2685s\n",
      "\titers: 800, epoch: 5 | loss: 0.0542743\n",
      "\tspeed: 0.1202s/iter; left time: 5060.8179s\n",
      "\titers: 900, epoch: 5 | loss: 0.0646305\n",
      "\tspeed: 0.1209s/iter; left time: 5077.8855s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0515460\n",
      "\tspeed: 0.1205s/iter; left time: 5049.4144s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0574071\n",
      "\tspeed: 0.1205s/iter; left time: 5036.0520s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0493728\n",
      "\tspeed: 0.1209s/iter; left time: 5041.5865s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0618911\n",
      "\tspeed: 0.1211s/iter; left time: 5038.3230s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0536328\n",
      "\tspeed: 0.1210s/iter; left time: 5022.7344s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0613508\n",
      "\tspeed: 0.1212s/iter; left time: 5016.2771s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0503679\n",
      "\tspeed: 0.1211s/iter; left time: 5001.8521s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0521905\n",
      "\tspeed: 0.1209s/iter; left time: 4980.1324s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0556836\n",
      "\tspeed: 0.1206s/iter; left time: 4956.0639s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0641983\n",
      "\tspeed: 0.1206s/iter; left time: 4944.9392s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0585940\n",
      "\tspeed: 0.1207s/iter; left time: 4936.5343s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0435131\n",
      "\tspeed: 0.1202s/iter; left time: 4905.7362s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0479925\n",
      "\tspeed: 0.1209s/iter; left time: 4919.7308s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0534774\n",
      "\tspeed: 0.1206s/iter; left time: 4897.5109s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0596892\n",
      "\tspeed: 0.1208s/iter; left time: 4893.3134s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0679414\n",
      "\tspeed: 0.1208s/iter; left time: 4881.2000s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0447534\n",
      "\tspeed: 0.1210s/iter; left time: 4874.0178s\n",
      "Epoch: 5 cost time: 00h:05m:23.01s\n",
      "Epoch: 5 | Train Loss: 0.0534883 Vali Loss: 0.0565160 Test Loss: 0.0616658\n",
      "Validation loss decreased (0.056570 --> 0.056516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0517764\n",
      "\tspeed: 1.0191s/iter; left time: 40883.5887s\n",
      "\titers: 200, epoch: 6 | loss: 0.0638383\n",
      "\tspeed: 0.1204s/iter; left time: 4818.3253s\n",
      "\titers: 300, epoch: 6 | loss: 0.0642103\n",
      "\tspeed: 0.1212s/iter; left time: 4838.7733s\n",
      "\titers: 400, epoch: 6 | loss: 0.0472873\n",
      "\tspeed: 0.1201s/iter; left time: 4781.2298s\n",
      "\titers: 500, epoch: 6 | loss: 0.0434228\n",
      "\tspeed: 0.1203s/iter; left time: 4779.3461s\n",
      "\titers: 600, epoch: 6 | loss: 0.0501320\n",
      "\tspeed: 0.1208s/iter; left time: 4786.0051s\n",
      "\titers: 700, epoch: 6 | loss: 0.0372537\n",
      "\tspeed: 0.1206s/iter; left time: 4764.8733s\n",
      "\titers: 800, epoch: 6 | loss: 0.0626084\n",
      "\tspeed: 0.1208s/iter; left time: 4761.5108s\n",
      "\titers: 900, epoch: 6 | loss: 0.0585214\n",
      "\tspeed: 0.1203s/iter; left time: 4729.6506s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0676461\n",
      "\tspeed: 0.1203s/iter; left time: 4718.0472s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0532079\n",
      "\tspeed: 0.1200s/iter; left time: 4694.1628s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0608200\n",
      "\tspeed: 0.1206s/iter; left time: 4705.5866s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0481969\n",
      "\tspeed: 0.1201s/iter; left time: 4674.5202s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0451223\n",
      "\tspeed: 0.1209s/iter; left time: 4692.6621s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0623270\n",
      "\tspeed: 0.1207s/iter; left time: 4674.1433s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0533873\n",
      "\tspeed: 0.1198s/iter; left time: 4624.6211s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0445180\n",
      "\tspeed: 0.1210s/iter; left time: 4660.9453s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0662053\n",
      "\tspeed: 0.1209s/iter; left time: 4644.0757s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0493251\n",
      "\tspeed: 0.1207s/iter; left time: 4626.2547s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0448093\n",
      "\tspeed: 0.1208s/iter; left time: 4616.8936s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0606518\n",
      "\tspeed: 0.1205s/iter; left time: 4593.2189s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0587236\n",
      "\tspeed: 0.1203s/iter; left time: 4572.5865s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0420482\n",
      "\tspeed: 0.1206s/iter; left time: 4574.2905s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0426998\n",
      "\tspeed: 0.1205s/iter; left time: 4555.1738s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0466667\n",
      "\tspeed: 0.1205s/iter; left time: 4545.8888s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0537168\n",
      "\tspeed: 0.1205s/iter; left time: 4533.2946s\n",
      "Epoch: 6 cost time: 00h:05m:23.72s\n",
      "Epoch: 6 | Train Loss: 0.0525643 Vali Loss: 0.0556040 Test Loss: 0.0610864\n",
      "Validation loss decreased (0.056516 --> 0.055604).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0668757\n",
      "\tspeed: 1.0185s/iter; left time: 38129.1857s\n",
      "\titers: 200, epoch: 7 | loss: 0.0545903\n",
      "\tspeed: 0.1205s/iter; left time: 4498.6741s\n",
      "\titers: 300, epoch: 7 | loss: 0.0470054\n",
      "\tspeed: 0.1211s/iter; left time: 4509.3425s\n",
      "\titers: 400, epoch: 7 | loss: 0.0559453\n",
      "\tspeed: 0.1206s/iter; left time: 4478.5608s\n",
      "\titers: 500, epoch: 7 | loss: 0.0481483\n",
      "\tspeed: 0.1201s/iter; left time: 4447.6729s\n",
      "\titers: 600, epoch: 7 | loss: 0.0419734\n",
      "\tspeed: 0.1204s/iter; left time: 4446.9116s\n",
      "\titers: 700, epoch: 7 | loss: 0.0507655\n",
      "\tspeed: 0.1200s/iter; left time: 4420.6055s\n",
      "\titers: 800, epoch: 7 | loss: 0.0553637\n",
      "\tspeed: 0.1192s/iter; left time: 4379.0402s\n",
      "\titers: 900, epoch: 7 | loss: 0.0417836\n",
      "\tspeed: 0.1198s/iter; left time: 4389.8594s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0594274\n",
      "\tspeed: 0.1196s/iter; left time: 4370.7764s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0467496\n",
      "\tspeed: 0.1196s/iter; left time: 4358.5357s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0645316\n",
      "\tspeed: 0.1208s/iter; left time: 4387.7828s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0443441\n",
      "\tspeed: 0.1199s/iter; left time: 4343.4639s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0480593\n",
      "\tspeed: 0.1179s/iter; left time: 4260.5835s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0565358\n",
      "\tspeed: 0.1212s/iter; left time: 4366.0785s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0584448\n",
      "\tspeed: 0.1209s/iter; left time: 4343.2550s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0491271\n",
      "\tspeed: 0.1208s/iter; left time: 4327.7441s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0501366\n",
      "\tspeed: 0.1216s/iter; left time: 4345.1652s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0490542\n",
      "\tspeed: 0.1200s/iter; left time: 4276.1900s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0508287\n",
      "\tspeed: 0.1203s/iter; left time: 4275.3045s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0515859\n",
      "\tspeed: 0.1207s/iter; left time: 4276.9953s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0453970\n",
      "\tspeed: 0.1202s/iter; left time: 4247.4094s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0537278\n",
      "\tspeed: 0.1208s/iter; left time: 4258.0536s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0542764\n",
      "\tspeed: 0.1068s/iter; left time: 3750.9417s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0465567\n",
      "\tspeed: 0.1021s/iter; left time: 3577.9362s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0497594\n",
      "\tspeed: 0.1161s/iter; left time: 4055.0245s\n",
      "Epoch: 7 cost time: 00h:05m:19.43s\n",
      "Epoch: 7 | Train Loss: 0.0518482 Vali Loss: 0.0550725 Test Loss: 0.0603381\n",
      "Validation loss decreased (0.055604 --> 0.055073).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0724908\n",
      "\tspeed: 1.0163s/iter; left time: 35320.0094s\n",
      "\titers: 200, epoch: 8 | loss: 0.0371657\n",
      "\tspeed: 0.1129s/iter; left time: 3911.5390s\n",
      "\titers: 300, epoch: 8 | loss: 0.0593067\n",
      "\tspeed: 0.1161s/iter; left time: 4010.1167s\n",
      "\titers: 400, epoch: 8 | loss: 0.0512719\n",
      "\tspeed: 0.1210s/iter; left time: 4169.9432s\n",
      "\titers: 500, epoch: 8 | loss: 0.0472560\n",
      "\tspeed: 0.1206s/iter; left time: 4144.2491s\n",
      "\titers: 600, epoch: 8 | loss: 0.0475125\n",
      "\tspeed: 0.1206s/iter; left time: 4132.7054s\n",
      "\titers: 700, epoch: 8 | loss: 0.0528827\n",
      "\tspeed: 0.1207s/iter; left time: 4123.3320s\n",
      "\titers: 800, epoch: 8 | loss: 0.0542871\n",
      "\tspeed: 0.1209s/iter; left time: 4117.0930s\n",
      "\titers: 900, epoch: 8 | loss: 0.0525813\n",
      "\tspeed: 0.1211s/iter; left time: 4112.3121s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0576860\n",
      "\tspeed: 0.1081s/iter; left time: 3661.0480s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0429319\n",
      "\tspeed: 0.1159s/iter; left time: 3910.9687s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0528561\n",
      "\tspeed: 0.1208s/iter; left time: 4064.9739s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0534785\n",
      "\tspeed: 0.1200s/iter; left time: 4027.1384s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0459838\n",
      "\tspeed: 0.1205s/iter; left time: 4032.2009s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0495800\n",
      "\tspeed: 0.1207s/iter; left time: 4026.0832s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0621961\n",
      "\tspeed: 0.1207s/iter; left time: 4012.7311s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0502770\n",
      "\tspeed: 0.1208s/iter; left time: 4005.7230s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0546408\n",
      "\tspeed: 0.1214s/iter; left time: 4014.2178s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0553437\n",
      "\tspeed: 0.1210s/iter; left time: 3987.6481s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0498420\n",
      "\tspeed: 0.1210s/iter; left time: 3976.0902s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0456712\n",
      "\tspeed: 0.1206s/iter; left time: 3950.6493s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0570142\n",
      "\tspeed: 0.1208s/iter; left time: 3943.2124s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0586175\n",
      "\tspeed: 0.1208s/iter; left time: 3933.2592s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0472034\n",
      "\tspeed: 0.1135s/iter; left time: 3685.0316s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0537372\n",
      "\tspeed: 0.1106s/iter; left time: 3579.6205s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0509919\n",
      "\tspeed: 0.1196s/iter; left time: 3858.3864s\n",
      "Epoch: 8 cost time: 00h:05m:19.32s\n",
      "Epoch: 8 | Train Loss: 0.0511598 Vali Loss: 0.0561661 Test Loss: 0.0617564\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0486406\n",
      "\tspeed: 1.0057s/iter; left time: 32254.3415s\n",
      "\titers: 200, epoch: 9 | loss: 0.0556457\n",
      "\tspeed: 0.1211s/iter; left time: 3870.9459s\n",
      "\titers: 300, epoch: 9 | loss: 0.0395175\n",
      "\tspeed: 0.1209s/iter; left time: 3854.3437s\n",
      "\titers: 400, epoch: 9 | loss: 0.0557750\n",
      "\tspeed: 0.1207s/iter; left time: 3835.9641s\n",
      "\titers: 500, epoch: 9 | loss: 0.0424190\n",
      "\tspeed: 0.1204s/iter; left time: 3813.3195s\n",
      "\titers: 600, epoch: 9 | loss: 0.0351209\n",
      "\tspeed: 0.1212s/iter; left time: 3826.9381s\n",
      "\titers: 700, epoch: 9 | loss: 0.0446073\n",
      "\tspeed: 0.1204s/iter; left time: 3788.4783s\n",
      "\titers: 800, epoch: 9 | loss: 0.0384792\n",
      "\tspeed: 0.1205s/iter; left time: 3780.1015s\n",
      "\titers: 900, epoch: 9 | loss: 0.0720906\n",
      "\tspeed: 0.1209s/iter; left time: 3780.3002s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0432853\n",
      "\tspeed: 0.1209s/iter; left time: 3768.9995s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0520263\n",
      "\tspeed: 0.1210s/iter; left time: 3759.5277s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0406994\n",
      "\tspeed: 0.1208s/iter; left time: 3742.4004s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0368552\n",
      "\tspeed: 0.1204s/iter; left time: 3717.7139s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0537488\n",
      "\tspeed: 0.1211s/iter; left time: 3726.4136s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0565366\n",
      "\tspeed: 0.1214s/iter; left time: 3722.9692s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0400267\n",
      "\tspeed: 0.1210s/iter; left time: 3699.1753s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0363783\n",
      "\tspeed: 0.1208s/iter; left time: 3681.4039s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0432666\n",
      "\tspeed: 0.1210s/iter; left time: 3676.3661s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0467523\n",
      "\tspeed: 0.1209s/iter; left time: 3660.5201s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0596244\n",
      "\tspeed: 0.1206s/iter; left time: 3639.1054s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0576644\n",
      "\tspeed: 0.1209s/iter; left time: 3635.1925s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0496884\n",
      "\tspeed: 0.1208s/iter; left time: 3620.1523s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0494345\n",
      "\tspeed: 0.1209s/iter; left time: 3610.2338s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0386619\n",
      "\tspeed: 0.1187s/iter; left time: 3534.6992s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0411887\n",
      "\tspeed: 0.1209s/iter; left time: 3587.4411s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0443211\n",
      "\tspeed: 0.1205s/iter; left time: 3562.9188s\n",
      "Epoch: 9 cost time: 00h:05m:24.11s\n",
      "Epoch: 9 | Train Loss: 0.0506318 Vali Loss: 0.0546863 Test Loss: 0.0599749\n",
      "Validation loss decreased (0.055073 --> 0.054686).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0549749\n",
      "\tspeed: 1.0167s/iter; left time: 29882.2757s\n",
      "\titers: 200, epoch: 10 | loss: 0.0383406\n",
      "\tspeed: 0.1201s/iter; left time: 3518.4641s\n",
      "\titers: 300, epoch: 10 | loss: 0.0472804\n",
      "\tspeed: 0.1206s/iter; left time: 3520.6253s\n",
      "\titers: 400, epoch: 10 | loss: 0.0422506\n",
      "\tspeed: 0.1207s/iter; left time: 3511.2358s\n",
      "\titers: 500, epoch: 10 | loss: 0.0550114\n",
      "\tspeed: 0.1200s/iter; left time: 3479.8145s\n",
      "\titers: 600, epoch: 10 | loss: 0.0441338\n",
      "\tspeed: 0.1200s/iter; left time: 3467.7047s\n",
      "\titers: 700, epoch: 10 | loss: 0.0540485\n",
      "\tspeed: 0.1203s/iter; left time: 3464.5516s\n",
      "\titers: 800, epoch: 10 | loss: 0.0546766\n",
      "\tspeed: 0.1202s/iter; left time: 3447.9440s\n",
      "\titers: 900, epoch: 10 | loss: 0.0499431\n",
      "\tspeed: 0.1201s/iter; left time: 3434.2522s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0570361\n",
      "\tspeed: 0.1205s/iter; left time: 3432.9802s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0438565\n",
      "\tspeed: 0.1199s/iter; left time: 3404.0302s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0453070\n",
      "\tspeed: 0.1200s/iter; left time: 3393.7573s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0541585\n",
      "\tspeed: 0.1201s/iter; left time: 3386.5670s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0488800\n",
      "\tspeed: 0.1202s/iter; left time: 3375.9738s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0653748\n",
      "\tspeed: 0.1197s/iter; left time: 3351.8683s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0513718\n",
      "\tspeed: 0.1213s/iter; left time: 3384.0512s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0519211\n",
      "\tspeed: 0.1217s/iter; left time: 3383.2746s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0510913\n",
      "\tspeed: 0.1208s/iter; left time: 3345.4994s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0787238\n",
      "\tspeed: 0.1207s/iter; left time: 3330.2029s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0443497\n",
      "\tspeed: 0.1208s/iter; left time: 3321.5760s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0386278\n",
      "\tspeed: 0.1201s/iter; left time: 3289.8422s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0553838\n",
      "\tspeed: 0.1198s/iter; left time: 3269.8105s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0513475\n",
      "\tspeed: 0.1203s/iter; left time: 3271.4273s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0396648\n",
      "\tspeed: 0.1204s/iter; left time: 3261.9901s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0409211\n",
      "\tspeed: 0.1210s/iter; left time: 3267.0834s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0425374\n",
      "\tspeed: 0.1205s/iter; left time: 3240.6208s\n",
      "Epoch: 10 cost time: 00h:05m:23.48s\n",
      "Epoch: 10 | Train Loss: 0.0501928 Vali Loss: 0.0552204 Test Loss: 0.0600623\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0620002\n",
      "\tspeed: 0.9997s/iter; left time: 26701.9387s\n",
      "\titers: 200, epoch: 11 | loss: 0.0374523\n",
      "\tspeed: 0.1070s/iter; left time: 2847.6236s\n",
      "\titers: 300, epoch: 11 | loss: 0.0529428\n",
      "\tspeed: 0.1178s/iter; left time: 3121.9197s\n",
      "\titers: 400, epoch: 11 | loss: 0.0386790\n",
      "\tspeed: 0.1203s/iter; left time: 3177.4499s\n",
      "\titers: 500, epoch: 11 | loss: 0.0355124\n",
      "\tspeed: 0.1208s/iter; left time: 3177.7102s\n",
      "\titers: 600, epoch: 11 | loss: 0.0515568\n",
      "\tspeed: 0.1213s/iter; left time: 3179.3728s\n",
      "\titers: 700, epoch: 11 | loss: 0.0656706\n",
      "\tspeed: 0.1203s/iter; left time: 3141.3655s\n",
      "\titers: 800, epoch: 11 | loss: 0.0524726\n",
      "\tspeed: 0.1207s/iter; left time: 3139.5442s\n",
      "\titers: 900, epoch: 11 | loss: 0.0677181\n",
      "\tspeed: 0.1209s/iter; left time: 3132.7989s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0548737\n",
      "\tspeed: 0.1204s/iter; left time: 3108.1454s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0569288\n",
      "\tspeed: 0.1206s/iter; left time: 3099.7904s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0609900\n",
      "\tspeed: 0.1208s/iter; left time: 3094.1036s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0399404\n",
      "\tspeed: 0.1208s/iter; left time: 3080.7027s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0586750\n",
      "\tspeed: 0.1070s/iter; left time: 2718.2817s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0501058\n",
      "\tspeed: 0.1211s/iter; left time: 3065.4128s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0411009\n",
      "\tspeed: 0.1216s/iter; left time: 3064.6042s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0643648\n",
      "\tspeed: 0.1214s/iter; left time: 3048.2299s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0525917\n",
      "\tspeed: 0.1207s/iter; left time: 3019.3287s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0451737\n",
      "\tspeed: 0.1207s/iter; left time: 3007.2425s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0579281\n",
      "\tspeed: 0.1183s/iter; left time: 2935.5011s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0540055\n",
      "\tspeed: 0.1192s/iter; left time: 2945.4971s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0435182\n",
      "\tspeed: 0.1205s/iter; left time: 2966.6080s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0541128\n",
      "\tspeed: 0.1215s/iter; left time: 2977.8557s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0472108\n",
      "\tspeed: 0.1208s/iter; left time: 2950.0345s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0526714\n",
      "\tspeed: 0.1152s/iter; left time: 2801.0023s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0492443\n",
      "\tspeed: 0.1211s/iter; left time: 2932.5734s\n",
      "Epoch: 11 cost time: 00h:05m:20.51s\n",
      "Epoch: 11 | Train Loss: 0.0496507 Vali Loss: 0.0557215 Test Loss: 0.0603047\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0461969\n",
      "\tspeed: 1.0021s/iter; left time: 24079.8231s\n",
      "\titers: 200, epoch: 12 | loss: 0.0482488\n",
      "\tspeed: 0.1206s/iter; left time: 2886.3327s\n",
      "\titers: 300, epoch: 12 | loss: 0.0463038\n",
      "\tspeed: 0.1207s/iter; left time: 2876.0206s\n",
      "\titers: 400, epoch: 12 | loss: 0.0601207\n",
      "\tspeed: 0.1206s/iter; left time: 2861.6739s\n",
      "\titers: 500, epoch: 12 | loss: 0.0524425\n",
      "\tspeed: 0.1187s/iter; left time: 2806.0298s\n",
      "\titers: 600, epoch: 12 | loss: 0.0494985\n",
      "\tspeed: 0.1033s/iter; left time: 2431.4239s\n",
      "\titers: 700, epoch: 12 | loss: 0.0530444\n",
      "\tspeed: 0.1200s/iter; left time: 2812.5319s\n",
      "\titers: 800, epoch: 12 | loss: 0.0502007\n",
      "\tspeed: 0.1193s/iter; left time: 2784.1332s\n",
      "\titers: 900, epoch: 12 | loss: 0.0386652\n",
      "\tspeed: 0.1198s/iter; left time: 2782.8374s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0524950\n",
      "\tspeed: 0.1205s/iter; left time: 2787.0250s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0360246\n",
      "\tspeed: 0.1194s/iter; left time: 2748.7165s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0573246\n",
      "\tspeed: 0.1196s/iter; left time: 2742.8701s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0448311\n",
      "\tspeed: 0.1193s/iter; left time: 2724.6823s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0468765\n",
      "\tspeed: 0.1200s/iter; left time: 2728.0973s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0427486\n",
      "\tspeed: 0.1201s/iter; left time: 2718.5249s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0422873\n",
      "\tspeed: 0.1190s/iter; left time: 2681.4113s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0424657\n",
      "\tspeed: 0.1193s/iter; left time: 2675.8393s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0512158\n",
      "\tspeed: 0.1196s/iter; left time: 2671.3625s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0482876\n",
      "\tspeed: 0.1193s/iter; left time: 2652.0677s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0403065\n",
      "\tspeed: 0.1118s/iter; left time: 2475.1210s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0512401\n",
      "\tspeed: 0.1203s/iter; left time: 2650.2194s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0528152\n",
      "\tspeed: 0.1028s/iter; left time: 2253.9245s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0479994\n",
      "\tspeed: 0.1171s/iter; left time: 2555.5745s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0547172\n",
      "\tspeed: 0.1198s/iter; left time: 2602.2258s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0471118\n",
      "\tspeed: 0.1201s/iter; left time: 2598.6451s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0501922\n",
      "\tspeed: 0.1203s/iter; left time: 2590.6900s\n",
      "Epoch: 12 cost time: 00h:05m:17.44s\n",
      "Epoch: 12 | Train Loss: 0.0491253 Vali Loss: 0.0558346 Test Loss: 0.0616492\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0454289\n",
      "\tspeed: 0.9922s/iter; left time: 21183.2626s\n",
      "\titers: 200, epoch: 13 | loss: 0.0566784\n",
      "\tspeed: 0.1204s/iter; left time: 2558.8539s\n",
      "\titers: 300, epoch: 13 | loss: 0.0492385\n",
      "\tspeed: 0.1200s/iter; left time: 2538.1225s\n",
      "\titers: 400, epoch: 13 | loss: 0.0413138\n",
      "\tspeed: 0.1024s/iter; left time: 2154.6356s\n",
      "\titers: 500, epoch: 13 | loss: 0.0430361\n",
      "\tspeed: 0.1042s/iter; left time: 2183.3766s\n",
      "\titers: 600, epoch: 13 | loss: 0.0513702\n",
      "\tspeed: 0.1219s/iter; left time: 2541.8010s\n",
      "\titers: 700, epoch: 13 | loss: 0.0502950\n",
      "\tspeed: 0.1209s/iter; left time: 2509.3149s\n",
      "\titers: 800, epoch: 13 | loss: 0.0534758\n",
      "\tspeed: 0.1208s/iter; left time: 2494.9589s\n",
      "\titers: 900, epoch: 13 | loss: 0.0518013\n",
      "\tspeed: 0.1205s/iter; left time: 2476.1994s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0583335\n",
      "\tspeed: 0.1199s/iter; left time: 2451.5419s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0650049\n",
      "\tspeed: 0.1155s/iter; left time: 2349.3675s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0528312\n",
      "\tspeed: 0.1212s/iter; left time: 2454.4293s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0473740\n",
      "\tspeed: 0.1207s/iter; left time: 2432.3801s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0452519\n",
      "\tspeed: 0.1197s/iter; left time: 2400.6977s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0424276\n",
      "\tspeed: 0.1208s/iter; left time: 2410.4358s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0521983\n",
      "\tspeed: 0.1093s/iter; left time: 2169.8448s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0471075\n",
      "\tspeed: 0.1210s/iter; left time: 2390.1550s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0435151\n",
      "\tspeed: 0.1203s/iter; left time: 2363.1459s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0494255\n",
      "\tspeed: 0.1212s/iter; left time: 2368.5964s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0469670\n",
      "\tspeed: 0.1209s/iter; left time: 2351.9989s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0472251\n",
      "\tspeed: 0.1197s/iter; left time: 2316.4476s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0486188\n",
      "\tspeed: 0.1211s/iter; left time: 2331.5692s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0509845\n",
      "\tspeed: 0.1206s/iter; left time: 2308.8339s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0457661\n",
      "\tspeed: 0.1200s/iter; left time: 2285.7200s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0508229\n",
      "\tspeed: 0.1191s/iter; left time: 2256.1184s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0558406\n",
      "\tspeed: 0.1207s/iter; left time: 2274.6115s\n",
      "Epoch: 13 cost time: 00h:05m:17.53s\n",
      "Epoch: 13 | Train Loss: 0.0487104 Vali Loss: 0.0579919 Test Loss: 0.0634963\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0413323\n",
      "\tspeed: 0.9986s/iter; left time: 18641.6608s\n",
      "\titers: 200, epoch: 14 | loss: 0.0421818\n",
      "\tspeed: 0.1204s/iter; left time: 2235.6244s\n",
      "\titers: 300, epoch: 14 | loss: 0.0630368\n",
      "\tspeed: 0.1207s/iter; left time: 2228.6635s\n",
      "\titers: 400, epoch: 14 | loss: 0.0455646\n",
      "\tspeed: 0.1202s/iter; left time: 2207.3190s\n",
      "\titers: 500, epoch: 14 | loss: 0.0373397\n",
      "\tspeed: 0.1202s/iter; left time: 2195.2879s\n",
      "\titers: 600, epoch: 14 | loss: 0.0462459\n",
      "\tspeed: 0.1212s/iter; left time: 2201.5756s\n",
      "\titers: 700, epoch: 14 | loss: 0.0535143\n",
      "\tspeed: 0.1202s/iter; left time: 2171.6056s\n",
      "\titers: 800, epoch: 14 | loss: 0.0559032\n",
      "\tspeed: 0.1203s/iter; left time: 2161.3690s\n",
      "\titers: 900, epoch: 14 | loss: 0.0622907\n",
      "\tspeed: 0.1210s/iter; left time: 2161.3614s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0430358\n",
      "\tspeed: 0.1142s/iter; left time: 2029.2671s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0412004\n",
      "\tspeed: 0.1202s/iter; left time: 2123.7152s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0447963\n",
      "\tspeed: 0.1199s/iter; left time: 2106.5802s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0473538\n",
      "\tspeed: 0.1210s/iter; left time: 2114.3970s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0410066\n",
      "\tspeed: 0.1206s/iter; left time: 2094.8129s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0531902\n",
      "\tspeed: 0.1203s/iter; left time: 2077.9892s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0443050\n",
      "\tspeed: 0.1205s/iter; left time: 2069.2638s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0529488\n",
      "\tspeed: 0.1207s/iter; left time: 2059.9307s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0568460\n",
      "\tspeed: 0.1202s/iter; left time: 2039.3879s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0442665\n",
      "\tspeed: 0.1200s/iter; left time: 2024.2595s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0497327\n",
      "\tspeed: 0.1199s/iter; left time: 2010.5553s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0499501\n",
      "\tspeed: 0.1201s/iter; left time: 2002.1074s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0507595\n",
      "\tspeed: 0.1204s/iter; left time: 1995.5327s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0538237\n",
      "\tspeed: 0.1201s/iter; left time: 1977.4425s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0460961\n",
      "\tspeed: 0.1209s/iter; left time: 1979.1287s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0627327\n",
      "\tspeed: 0.1209s/iter; left time: 1967.4317s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0460155\n",
      "\tspeed: 0.1210s/iter; left time: 1955.6607s\n",
      "Epoch: 14 cost time: 00h:05m:22.77s\n",
      "Epoch: 14 | Train Loss: 0.0482707 Vali Loss: 0.0567447 Test Loss: 0.0618993\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.011070186272263527, rmse:0.10521495342254639, mae:0.059974875301122665, rse:0.40591442584991455\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 24: 01h:35m:12.27s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-01 11:22:45,402] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 11:22:46,572] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 11:22:46,572] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 11:22:46,572] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 11:22:46,679] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 11:22:46,679] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 11:22:47,360] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 11:22:47,362] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 11:22:47,362] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 11:22:47,364] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 11:22:47,364] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 11:22:47,364] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 11:22:47,364] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 11:22:47,364] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 11:22:47,364] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 11:22:47,364] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 11:22:47,672] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 11:22:47,673] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 11:22:47,673] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.67 GB, percent = 9.9%\n",
      "[2024-11-01 11:22:47,791] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 11:22:47,792] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 11:22:47,792] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.67 GB, percent = 9.9%\n",
      "[2024-11-01 11:22:47,792] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 11:22:47,904] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 11:22:47,905] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 11:22:47,905] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.67 GB, percent = 9.9%\n",
      "[2024-11-01 11:22:47,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 11:22:47,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 11:22:47,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 11:22:47,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 11:22:47,907] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 11:22:47,907] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 11:22:47,907] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 11:22:47,907] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 11:22:47,907] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f166ebf0710>\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 11:22:47,908] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 11:22:47,909] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 11:22:47,910] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 11:22:47,910] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 11:22:47,910] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 11:22:47,910] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1506957\n",
      "\tspeed: 0.1751s/iter; left time: 9349.6598s\n",
      "\titers: 200, epoch: 1 | loss: 0.1479850\n",
      "\tspeed: 0.1310s/iter; left time: 6979.0771s\n",
      "\titers: 300, epoch: 1 | loss: 0.1137763\n",
      "\tspeed: 0.1308s/iter; left time: 6956.4687s\n",
      "\titers: 400, epoch: 1 | loss: 0.1076533\n",
      "\tspeed: 0.1313s/iter; left time: 6969.1401s\n",
      "\titers: 500, epoch: 1 | loss: 0.0889011\n",
      "\tspeed: 0.1306s/iter; left time: 6918.4580s\n",
      "\titers: 600, epoch: 1 | loss: 0.0769084\n",
      "\tspeed: 0.1311s/iter; left time: 6930.7115s\n",
      "\titers: 700, epoch: 1 | loss: 0.0896481\n",
      "\tspeed: 0.1306s/iter; left time: 6894.0510s\n",
      "\titers: 800, epoch: 1 | loss: 0.0794844\n",
      "\tspeed: 0.1300s/iter; left time: 6846.7115s\n",
      "\titers: 900, epoch: 1 | loss: 0.0815863\n",
      "\tspeed: 0.1303s/iter; left time: 6849.9779s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0779093\n",
      "\tspeed: 0.1301s/iter; left time: 6829.4902s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0663487\n",
      "\tspeed: 0.1156s/iter; left time: 6055.3742s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0627032\n",
      "\tspeed: 0.1242s/iter; left time: 6493.1432s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0696527\n",
      "\tspeed: 0.1296s/iter; left time: 6762.5623s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0784859\n",
      "\tspeed: 0.1319s/iter; left time: 6870.0200s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0762833\n",
      "\tspeed: 0.1203s/iter; left time: 6252.6836s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0648971\n",
      "\tspeed: 0.1133s/iter; left time: 5876.9669s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801568\n",
      "\tspeed: 0.1134s/iter; left time: 5870.4367s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0775711\n",
      "\tspeed: 0.1139s/iter; left time: 5884.7935s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0935053\n",
      "\tspeed: 0.1232s/iter; left time: 6356.8452s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0565456\n",
      "\tspeed: 0.1303s/iter; left time: 6710.0954s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0903167\n",
      "\tspeed: 0.1304s/iter; left time: 6700.4210s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0752582\n",
      "\tspeed: 0.1306s/iter; left time: 6699.3190s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0761704\n",
      "\tspeed: 0.1308s/iter; left time: 6692.0513s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0646984\n",
      "\tspeed: 0.1300s/iter; left time: 6638.9959s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0703335\n",
      "\tspeed: 0.1303s/iter; left time: 6642.7903s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0783555\n",
      "\tspeed: 0.1311s/iter; left time: 6669.5708s\n",
      "Epoch: 1 cost time: 00h:05m:41.46s\n",
      "Epoch: 1 | Train Loss: 0.0856850 Vali Loss: 0.0756439 Test Loss: 0.0843909\n",
      "Validation loss decreased (inf --> 0.075644).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0820017\n",
      "\tspeed: 1.1350s/iter; left time: 57551.0438s\n",
      "\titers: 200, epoch: 2 | loss: 0.0687233\n",
      "\tspeed: 0.1201s/iter; left time: 6075.4324s\n",
      "\titers: 300, epoch: 2 | loss: 0.0772514\n",
      "\tspeed: 0.1212s/iter; left time: 6119.5730s\n",
      "\titers: 400, epoch: 2 | loss: 0.0848139\n",
      "\tspeed: 0.1200s/iter; left time: 6049.9097s\n",
      "\titers: 500, epoch: 2 | loss: 0.0686562\n",
      "\tspeed: 0.1196s/iter; left time: 6016.0186s\n",
      "\titers: 600, epoch: 2 | loss: 0.0745008\n",
      "\tspeed: 0.1191s/iter; left time: 5980.6560s\n",
      "\titers: 700, epoch: 2 | loss: 0.0605222\n",
      "\tspeed: 0.1199s/iter; left time: 6008.6909s\n",
      "\titers: 800, epoch: 2 | loss: 0.0737291\n",
      "\tspeed: 0.1197s/iter; left time: 5984.8640s\n",
      "\titers: 900, epoch: 2 | loss: 0.0731786\n",
      "\tspeed: 0.1198s/iter; left time: 5977.5350s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0705629\n",
      "\tspeed: 0.1191s/iter; left time: 5934.1243s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0684758\n",
      "\tspeed: 0.1190s/iter; left time: 5916.1633s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1017207\n",
      "\tspeed: 0.1193s/iter; left time: 5920.5217s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0608158\n",
      "\tspeed: 0.1200s/iter; left time: 5942.9356s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0733602\n",
      "\tspeed: 0.1187s/iter; left time: 5863.9424s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0823769\n",
      "\tspeed: 0.1206s/iter; left time: 5948.0708s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0670033\n",
      "\tspeed: 0.1201s/iter; left time: 5910.3427s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0635963\n",
      "\tspeed: 0.1200s/iter; left time: 5891.7780s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0702989\n",
      "\tspeed: 0.1201s/iter; left time: 5885.1621s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0682929\n",
      "\tspeed: 0.1204s/iter; left time: 5887.8482s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0600829\n",
      "\tspeed: 0.1195s/iter; left time: 5834.6618s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0654664\n",
      "\tspeed: 0.1207s/iter; left time: 5879.7385s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0597576\n",
      "\tspeed: 0.1205s/iter; left time: 5855.9107s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0667420\n",
      "\tspeed: 0.1201s/iter; left time: 5826.0785s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0765030\n",
      "\tspeed: 0.1205s/iter; left time: 5833.4914s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0623692\n",
      "\tspeed: 0.1200s/iter; left time: 5796.7179s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0587819\n",
      "\tspeed: 0.1186s/iter; left time: 5717.6883s\n",
      "Epoch: 2 cost time: 00h:05m:21.12s\n",
      "Epoch: 2 | Train Loss: 0.0710048 Vali Loss: 0.0745253 Test Loss: 0.0838230\n",
      "Validation loss decreased (0.075644 --> 0.074525).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0658879\n",
      "\tspeed: 1.0101s/iter; left time: 48519.0051s\n",
      "\titers: 200, epoch: 3 | loss: 0.0615703\n",
      "\tspeed: 0.1201s/iter; left time: 5757.7553s\n",
      "\titers: 300, epoch: 3 | loss: 0.0726208\n",
      "\tspeed: 0.1216s/iter; left time: 5817.9997s\n",
      "\titers: 400, epoch: 3 | loss: 0.0762616\n",
      "\tspeed: 0.1195s/iter; left time: 5702.2888s\n",
      "\titers: 500, epoch: 3 | loss: 0.0660659\n",
      "\tspeed: 0.1199s/iter; left time: 5710.6232s\n",
      "\titers: 600, epoch: 3 | loss: 0.0651958\n",
      "\tspeed: 0.1206s/iter; left time: 5733.7320s\n",
      "\titers: 700, epoch: 3 | loss: 0.0897581\n",
      "\tspeed: 0.1188s/iter; left time: 5636.3366s\n",
      "\titers: 800, epoch: 3 | loss: 0.0652855\n",
      "\tspeed: 0.1201s/iter; left time: 5683.1165s\n",
      "\titers: 900, epoch: 3 | loss: 0.0668169\n",
      "\tspeed: 0.1193s/iter; left time: 5636.9228s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0626059\n",
      "\tspeed: 0.1187s/iter; left time: 5594.6056s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0549380\n",
      "\tspeed: 0.1198s/iter; left time: 5634.8127s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0706437\n",
      "\tspeed: 0.1199s/iter; left time: 5628.5179s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0740227\n",
      "\tspeed: 0.1207s/iter; left time: 5651.0140s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0664061\n",
      "\tspeed: 0.1199s/iter; left time: 5603.8054s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0757374\n",
      "\tspeed: 0.1191s/iter; left time: 5554.4733s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0855467\n",
      "\tspeed: 0.1190s/iter; left time: 5537.0620s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0756636\n",
      "\tspeed: 0.1183s/iter; left time: 5492.5795s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0538661\n",
      "\tspeed: 0.1195s/iter; left time: 5536.2299s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0783621\n",
      "\tspeed: 0.1196s/iter; left time: 5529.0552s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0645575\n",
      "\tspeed: 0.1186s/iter; left time: 5471.5984s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0941539\n",
      "\tspeed: 0.1196s/iter; left time: 5505.2675s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0742499\n",
      "\tspeed: 0.1183s/iter; left time: 5433.0405s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0604884\n",
      "\tspeed: 0.1195s/iter; left time: 5475.9703s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0683079\n",
      "\tspeed: 0.1182s/iter; left time: 5403.4890s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0758141\n",
      "\tspeed: 0.1195s/iter; left time: 5452.7680s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0615954\n",
      "\tspeed: 0.1201s/iter; left time: 5466.9327s\n",
      "Epoch: 3 cost time: 00h:05m:20.18s\n",
      "Epoch: 3 | Train Loss: 0.0687785 Vali Loss: 0.0725997 Test Loss: 0.0817405\n",
      "Validation loss decreased (0.074525 --> 0.072600).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0635042\n",
      "\tspeed: 1.0078s/iter; left time: 45711.6120s\n",
      "\titers: 200, epoch: 4 | loss: 0.0766480\n",
      "\tspeed: 0.1207s/iter; left time: 5463.4321s\n",
      "\titers: 300, epoch: 4 | loss: 0.0603194\n",
      "\tspeed: 0.1204s/iter; left time: 5437.8544s\n",
      "\titers: 400, epoch: 4 | loss: 0.0879604\n",
      "\tspeed: 0.1205s/iter; left time: 5430.1768s\n",
      "\titers: 500, epoch: 4 | loss: 0.0716096\n",
      "\tspeed: 0.1209s/iter; left time: 5436.9420s\n",
      "\titers: 600, epoch: 4 | loss: 0.0661810\n",
      "\tspeed: 0.1209s/iter; left time: 5425.1984s\n",
      "\titers: 700, epoch: 4 | loss: 0.0630168\n",
      "\tspeed: 0.1196s/iter; left time: 5354.5827s\n",
      "\titers: 800, epoch: 4 | loss: 0.0750177\n",
      "\tspeed: 0.1195s/iter; left time: 5336.1849s\n",
      "\titers: 900, epoch: 4 | loss: 0.0560872\n",
      "\tspeed: 0.1191s/iter; left time: 5306.5630s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0561997\n",
      "\tspeed: 0.1202s/iter; left time: 5344.6300s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0634589\n",
      "\tspeed: 0.1199s/iter; left time: 5316.7201s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0602584\n",
      "\tspeed: 0.1196s/iter; left time: 5292.1789s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0809753\n",
      "\tspeed: 0.1202s/iter; left time: 5308.1347s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0696282\n",
      "\tspeed: 0.1201s/iter; left time: 5292.5759s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0582918\n",
      "\tspeed: 0.1183s/iter; left time: 5199.8582s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0587403\n",
      "\tspeed: 0.1184s/iter; left time: 5194.5858s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0575944\n",
      "\tspeed: 0.1196s/iter; left time: 5233.5142s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0667402\n",
      "\tspeed: 0.1204s/iter; left time: 5256.4078s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0664581\n",
      "\tspeed: 0.1200s/iter; left time: 5227.1697s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0745811\n",
      "\tspeed: 0.1057s/iter; left time: 4592.6947s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0695823\n",
      "\tspeed: 0.1174s/iter; left time: 5090.6185s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0542657\n",
      "\tspeed: 0.1201s/iter; left time: 5196.0761s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0670997\n",
      "\tspeed: 0.1195s/iter; left time: 5157.0217s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0595331\n",
      "\tspeed: 0.1195s/iter; left time: 5146.2626s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0590853\n",
      "\tspeed: 0.1203s/iter; left time: 5169.8527s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0622148\n",
      "\tspeed: 0.1173s/iter; left time: 5025.6819s\n",
      "Epoch: 4 cost time: 00h:05m:19.20s\n",
      "Epoch: 4 | Train Loss: 0.0669450 Vali Loss: 0.0741470 Test Loss: 0.0824094\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0655506\n",
      "\tspeed: 0.9811s/iter; left time: 41879.6652s\n",
      "\titers: 200, epoch: 5 | loss: 0.0827420\n",
      "\tspeed: 0.1192s/iter; left time: 5075.7628s\n",
      "\titers: 300, epoch: 5 | loss: 0.0619815\n",
      "\tspeed: 0.1205s/iter; left time: 5119.3541s\n",
      "\titers: 400, epoch: 5 | loss: 0.0519550\n",
      "\tspeed: 0.1201s/iter; left time: 5090.9910s\n",
      "\titers: 500, epoch: 5 | loss: 0.0678731\n",
      "\tspeed: 0.1197s/iter; left time: 5061.8721s\n",
      "\titers: 600, epoch: 5 | loss: 0.0686385\n",
      "\tspeed: 0.1192s/iter; left time: 5029.9501s\n",
      "\titers: 700, epoch: 5 | loss: 0.0715512\n",
      "\tspeed: 0.1182s/iter; left time: 4974.5708s\n",
      "\titers: 800, epoch: 5 | loss: 0.0702961\n",
      "\tspeed: 0.1191s/iter; left time: 4999.8394s\n",
      "\titers: 900, epoch: 5 | loss: 0.0748347\n",
      "\tspeed: 0.1183s/iter; left time: 4953.3427s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0546004\n",
      "\tspeed: 0.1190s/iter; left time: 4970.7516s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0699323\n",
      "\tspeed: 0.1186s/iter; left time: 4942.9108s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0556661\n",
      "\tspeed: 0.1196s/iter; left time: 4975.6329s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0645581\n",
      "\tspeed: 0.1193s/iter; left time: 4949.8879s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0688608\n",
      "\tspeed: 0.1187s/iter; left time: 4911.0268s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0485171\n",
      "\tspeed: 0.1185s/iter; left time: 4893.2452s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0664087\n",
      "\tspeed: 0.1189s/iter; left time: 4896.9899s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0548556\n",
      "\tspeed: 0.1192s/iter; left time: 4896.7069s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0654618\n",
      "\tspeed: 0.1187s/iter; left time: 4866.6101s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0673945\n",
      "\tspeed: 0.1180s/iter; left time: 4826.1755s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0469344\n",
      "\tspeed: 0.1187s/iter; left time: 4840.5091s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0672865\n",
      "\tspeed: 0.1192s/iter; left time: 4848.7010s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0703256\n",
      "\tspeed: 0.1198s/iter; left time: 4862.7784s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0843584\n",
      "\tspeed: 0.1199s/iter; left time: 4853.1134s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0603486\n",
      "\tspeed: 0.1193s/iter; left time: 4817.5601s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761832\n",
      "\tspeed: 0.1198s/iter; left time: 4824.4121s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0514573\n",
      "\tspeed: 0.1185s/iter; left time: 4763.6939s\n",
      "Epoch: 5 cost time: 00h:05m:18.86s\n",
      "Epoch: 5 | Train Loss: 0.0649131 Vali Loss: 0.0754854 Test Loss: 0.0844793\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0773710\n",
      "\tspeed: 0.9776s/iter; left time: 39113.4777s\n",
      "\titers: 200, epoch: 6 | loss: 0.0613844\n",
      "\tspeed: 0.1206s/iter; left time: 4814.1503s\n",
      "\titers: 300, epoch: 6 | loss: 0.0576228\n",
      "\tspeed: 0.1196s/iter; left time: 4761.6156s\n",
      "\titers: 400, epoch: 6 | loss: 0.0596324\n",
      "\tspeed: 0.1206s/iter; left time: 4790.4277s\n",
      "\titers: 500, epoch: 6 | loss: 0.0579083\n",
      "\tspeed: 0.1200s/iter; left time: 4754.0140s\n",
      "\titers: 600, epoch: 6 | loss: 0.0726682\n",
      "\tspeed: 0.1194s/iter; left time: 4716.1232s\n",
      "\titers: 700, epoch: 6 | loss: 0.0726198\n",
      "\tspeed: 0.1185s/iter; left time: 4669.9108s\n",
      "\titers: 800, epoch: 6 | loss: 0.0645957\n",
      "\tspeed: 0.1189s/iter; left time: 4673.2563s\n",
      "\titers: 900, epoch: 6 | loss: 0.0725977\n",
      "\tspeed: 0.1201s/iter; left time: 4708.5216s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0638610\n",
      "\tspeed: 0.1196s/iter; left time: 4676.2139s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0556347\n",
      "\tspeed: 0.1191s/iter; left time: 4645.1850s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0472695\n",
      "\tspeed: 0.1195s/iter; left time: 4649.6084s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0792933\n",
      "\tspeed: 0.1093s/iter; left time: 4241.4612s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0675426\n",
      "\tspeed: 0.1108s/iter; left time: 4287.9202s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0533914\n",
      "\tspeed: 0.1146s/iter; left time: 4424.1330s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0623400\n",
      "\tspeed: 0.1192s/iter; left time: 4591.5030s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0586607\n",
      "\tspeed: 0.1199s/iter; left time: 4607.3276s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0620784\n",
      "\tspeed: 0.1191s/iter; left time: 4564.6311s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0596254\n",
      "\tspeed: 0.1188s/iter; left time: 4540.4091s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0587260\n",
      "\tspeed: 0.1197s/iter; left time: 4563.1867s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0641739\n",
      "\tspeed: 0.1197s/iter; left time: 4550.2529s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0735683\n",
      "\tspeed: 0.1195s/iter; left time: 4528.7028s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0535805\n",
      "\tspeed: 0.1197s/iter; left time: 4525.2919s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0671832\n",
      "\tspeed: 0.1162s/iter; left time: 4381.6351s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0640754\n",
      "\tspeed: 0.1070s/iter; left time: 4022.6295s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0475231\n",
      "\tspeed: 0.1158s/iter; left time: 4343.4676s\n",
      "Epoch: 6 cost time: 00h:05m:15.84s\n",
      "Epoch: 6 | Train Loss: 0.0630431 Vali Loss: 0.0751659 Test Loss: 0.0838745\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0737136\n",
      "\tspeed: 0.9827s/iter; left time: 36690.7581s\n",
      "\titers: 200, epoch: 7 | loss: 0.0773198\n",
      "\tspeed: 0.1189s/iter; left time: 4428.4602s\n",
      "\titers: 300, epoch: 7 | loss: 0.0562561\n",
      "\tspeed: 0.1148s/iter; left time: 4264.7351s\n",
      "\titers: 400, epoch: 7 | loss: 0.0674500\n",
      "\tspeed: 0.1020s/iter; left time: 3777.3044s\n",
      "\titers: 500, epoch: 7 | loss: 0.0538294\n",
      "\tspeed: 0.1021s/iter; left time: 3769.8766s\n",
      "\titers: 600, epoch: 7 | loss: 0.0683504\n",
      "\tspeed: 0.1027s/iter; left time: 3782.2837s\n",
      "\titers: 700, epoch: 7 | loss: 0.0571279\n",
      "\tspeed: 0.1186s/iter; left time: 4356.7608s\n",
      "\titers: 800, epoch: 7 | loss: 0.0532890\n",
      "\tspeed: 0.1117s/iter; left time: 4093.1072s\n",
      "\titers: 900, epoch: 7 | loss: 0.0591477\n",
      "\tspeed: 0.1193s/iter; left time: 4357.2459s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0772009\n",
      "\tspeed: 0.1191s/iter; left time: 4340.1853s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0556193\n",
      "\tspeed: 0.1078s/iter; left time: 3918.5615s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0560169\n",
      "\tspeed: 0.1193s/iter; left time: 4323.4877s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0597722\n",
      "\tspeed: 0.1203s/iter; left time: 4347.9546s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0604149\n",
      "\tspeed: 0.1030s/iter; left time: 3711.2144s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0788255\n",
      "\tspeed: 0.1017s/iter; left time: 3655.6555s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0704609\n",
      "\tspeed: 0.1097s/iter; left time: 3931.3058s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0629530\n",
      "\tspeed: 0.1019s/iter; left time: 3640.2886s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0610598\n",
      "\tspeed: 0.1085s/iter; left time: 3865.0256s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0446289\n",
      "\tspeed: 0.1200s/iter; left time: 4263.8326s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0594098\n",
      "\tspeed: 0.1202s/iter; left time: 4260.5824s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0708012\n",
      "\tspeed: 0.1161s/iter; left time: 4104.2443s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0720370\n",
      "\tspeed: 0.1182s/iter; left time: 4163.7099s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0789985\n",
      "\tspeed: 0.1197s/iter; left time: 4205.8899s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0545131\n",
      "\tspeed: 0.1126s/iter; left time: 3944.1270s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0678101\n",
      "\tspeed: 0.1164s/iter; left time: 4065.8396s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0619310\n",
      "\tspeed: 0.1203s/iter; left time: 4189.5843s\n",
      "Epoch: 7 cost time: 00h:05m:03.66s\n",
      "Epoch: 7 | Train Loss: 0.0613680 Vali Loss: 0.0761664 Test Loss: 0.0845503\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0591804\n",
      "\tspeed: 0.9790s/iter; left time: 33934.6230s\n",
      "\titers: 200, epoch: 8 | loss: 0.0601895\n",
      "\tspeed: 0.1196s/iter; left time: 4133.0989s\n",
      "\titers: 300, epoch: 8 | loss: 0.0537779\n",
      "\tspeed: 0.1198s/iter; left time: 4128.7480s\n",
      "\titers: 400, epoch: 8 | loss: 0.0596471\n",
      "\tspeed: 0.1198s/iter; left time: 4117.4278s\n",
      "\titers: 500, epoch: 8 | loss: 0.0748389\n",
      "\tspeed: 0.1204s/iter; left time: 4124.1304s\n",
      "\titers: 600, epoch: 8 | loss: 0.0619526\n",
      "\tspeed: 0.1195s/iter; left time: 4082.4990s\n",
      "\titers: 700, epoch: 8 | loss: 0.0492099\n",
      "\tspeed: 0.1188s/iter; left time: 4045.3253s\n",
      "\titers: 800, epoch: 8 | loss: 0.0553380\n",
      "\tspeed: 0.1196s/iter; left time: 4063.0445s\n",
      "\titers: 900, epoch: 8 | loss: 0.0660264\n",
      "\tspeed: 0.1195s/iter; left time: 4047.2326s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0512587\n",
      "\tspeed: 0.1194s/iter; left time: 4030.1679s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0606477\n",
      "\tspeed: 0.1188s/iter; left time: 3998.5403s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0488810\n",
      "\tspeed: 0.1191s/iter; left time: 3998.0484s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0636803\n",
      "\tspeed: 0.1186s/iter; left time: 3967.5197s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0603122\n",
      "\tspeed: 0.1175s/iter; left time: 3920.6324s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0753196\n",
      "\tspeed: 0.1020s/iter; left time: 3391.2001s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0476781\n",
      "\tspeed: 0.1022s/iter; left time: 3387.8970s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0543786\n",
      "\tspeed: 0.1025s/iter; left time: 3389.6880s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0479689\n",
      "\tspeed: 0.1025s/iter; left time: 3378.3512s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0567113\n",
      "\tspeed: 0.1021s/iter; left time: 3355.2348s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0584764\n",
      "\tspeed: 0.1057s/iter; left time: 3463.6862s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0674268\n",
      "\tspeed: 0.1190s/iter; left time: 3886.3887s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0617867\n",
      "\tspeed: 0.1192s/iter; left time: 3881.7200s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0588714\n",
      "\tspeed: 0.1193s/iter; left time: 3873.9034s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0716897\n",
      "\tspeed: 0.1202s/iter; left time: 3888.4492s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0649153\n",
      "\tspeed: 0.1200s/iter; left time: 3872.2716s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0538609\n",
      "\tspeed: 0.1197s/iter; left time: 3851.3189s\n",
      "Epoch: 8 cost time: 00h:05m:09.84s\n",
      "Epoch: 8 | Train Loss: 0.0601105 Vali Loss: 0.0785164 Test Loss: 0.0872316\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018459701910614967, rmse:0.13586649298667908, mae:0.08174050599336624, rse:0.5256097912788391\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 96: 00h:54m:09.08s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85371\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-01 12:16:54,048] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 12:16:55,149] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 12:16:55,149] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 12:16:55,149] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 12:16:55,247] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 12:16:55,247] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 12:16:55,874] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 12:16:55,875] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 12:16:55,876] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 12:16:55,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 12:16:55,877] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 12:16:55,877] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 12:16:55,877] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 12:16:55,877] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 12:16:55,877] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 12:16:55,877] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 12:16:56,153] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 12:16:56,154] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 12:16:56,186] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.51 GB, percent = 11.2%\n",
      "[2024-11-01 12:16:56,315] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 12:16:56,316] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 12:16:56,317] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.51 GB, percent = 11.2%\n",
      "[2024-11-01 12:16:56,317] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 12:16:56,441] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 12:16:56,442] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-01 12:16:56,442] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.52 GB, percent = 11.2%\n",
      "[2024-11-01 12:16:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 12:16:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 12:16:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 12:16:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 12:16:56,444] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f21314ff050>\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 12:16:56,445] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 12:16:56,446] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 12:16:56,447] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1461806\n",
      "\tspeed: 0.1730s/iter; left time: 9210.0870s\n",
      "\titers: 200, epoch: 1 | loss: 0.1501896\n",
      "\tspeed: 0.1310s/iter; left time: 6959.3806s\n",
      "\titers: 300, epoch: 1 | loss: 0.1255645\n",
      "\tspeed: 0.1311s/iter; left time: 6951.9444s\n",
      "\titers: 400, epoch: 1 | loss: 0.0948358\n",
      "\tspeed: 0.1310s/iter; left time: 6934.5625s\n",
      "\titers: 500, epoch: 1 | loss: 0.0860671\n",
      "\tspeed: 0.1309s/iter; left time: 6917.5522s\n",
      "\titers: 600, epoch: 1 | loss: 0.0797759\n",
      "\tspeed: 0.1306s/iter; left time: 6885.6366s\n",
      "\titers: 700, epoch: 1 | loss: 0.0779733\n",
      "\tspeed: 0.1309s/iter; left time: 6890.0925s\n",
      "\titers: 800, epoch: 1 | loss: 0.0854992\n",
      "\tspeed: 0.1307s/iter; left time: 6867.4979s\n",
      "\titers: 900, epoch: 1 | loss: 0.0824629\n",
      "\tspeed: 0.1310s/iter; left time: 6869.6733s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0905522\n",
      "\tspeed: 0.1303s/iter; left time: 6818.0561s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0848468\n",
      "\tspeed: 0.1205s/iter; left time: 6294.7008s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0831938\n",
      "\tspeed: 0.1180s/iter; left time: 6151.8401s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0840276\n",
      "\tspeed: 0.1308s/iter; left time: 6808.4912s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0715065\n",
      "\tspeed: 0.1310s/iter; left time: 6806.7839s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0868700\n",
      "\tspeed: 0.1310s/iter; left time: 6789.1651s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0816183\n",
      "\tspeed: 0.1308s/iter; left time: 6766.4134s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0746674\n",
      "\tspeed: 0.1237s/iter; left time: 6389.0172s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0842547\n",
      "\tspeed: 0.1130s/iter; left time: 5825.4907s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0750883\n",
      "\tspeed: 0.1130s/iter; left time: 5814.9845s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0767707\n",
      "\tspeed: 0.1150s/iter; left time: 5905.6888s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0823143\n",
      "\tspeed: 0.1305s/iter; left time: 6686.9631s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0773541\n",
      "\tspeed: 0.1180s/iter; left time: 6033.7108s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0822798\n",
      "\tspeed: 0.1129s/iter; left time: 5764.1653s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0697680\n",
      "\tspeed: 0.1214s/iter; left time: 6182.1408s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0781708\n",
      "\tspeed: 0.1217s/iter; left time: 6188.2546s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0923201\n",
      "\tspeed: 0.1308s/iter; left time: 6636.2513s\n",
      "Epoch: 1 cost time: 00h:05m:37.03s\n",
      "Epoch: 1 | Train Loss: 0.0882067 Vali Loss: 0.0789939 Test Loss: 0.0885378\n",
      "Validation loss decreased (inf --> 0.078994).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0682366\n",
      "\tspeed: 1.1204s/iter; left time: 56662.9128s\n",
      "\titers: 200, epoch: 2 | loss: 0.0756340\n",
      "\tspeed: 0.1210s/iter; left time: 6106.6466s\n",
      "\titers: 300, epoch: 2 | loss: 0.0757687\n",
      "\tspeed: 0.1207s/iter; left time: 6080.6581s\n",
      "\titers: 400, epoch: 2 | loss: 0.0809062\n",
      "\tspeed: 0.1209s/iter; left time: 6078.9142s\n",
      "\titers: 500, epoch: 2 | loss: 0.0785947\n",
      "\tspeed: 0.1196s/iter; left time: 6002.3453s\n",
      "\titers: 600, epoch: 2 | loss: 0.0788290\n",
      "\tspeed: 0.1201s/iter; left time: 6015.4931s\n",
      "\titers: 700, epoch: 2 | loss: 0.0643081\n",
      "\tspeed: 0.1208s/iter; left time: 6037.5475s\n",
      "\titers: 800, epoch: 2 | loss: 0.0724190\n",
      "\tspeed: 0.1202s/iter; left time: 5993.5757s\n",
      "\titers: 900, epoch: 2 | loss: 0.0765032\n",
      "\tspeed: 0.1199s/iter; left time: 5968.3381s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0680958\n",
      "\tspeed: 0.1206s/iter; left time: 5989.3898s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0636572\n",
      "\tspeed: 0.1202s/iter; left time: 5957.4190s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0647527\n",
      "\tspeed: 0.1063s/iter; left time: 5257.8187s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0695712\n",
      "\tspeed: 0.1167s/iter; left time: 5762.5015s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0781222\n",
      "\tspeed: 0.1202s/iter; left time: 5921.1014s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0884119\n",
      "\tspeed: 0.1198s/iter; left time: 5892.3395s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0732782\n",
      "\tspeed: 0.1200s/iter; left time: 5890.5311s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0834294\n",
      "\tspeed: 0.1196s/iter; left time: 5859.1336s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0762573\n",
      "\tspeed: 0.1204s/iter; left time: 5884.8618s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0638730\n",
      "\tspeed: 0.1206s/iter; left time: 5883.1452s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0719824\n",
      "\tspeed: 0.1206s/iter; left time: 5872.0541s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0811448\n",
      "\tspeed: 0.1202s/iter; left time: 5839.0606s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0702735\n",
      "\tspeed: 0.1207s/iter; left time: 5852.4566s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0598425\n",
      "\tspeed: 0.1205s/iter; left time: 5829.9909s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0801101\n",
      "\tspeed: 0.1205s/iter; left time: 5815.2579s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0596308\n",
      "\tspeed: 0.1193s/iter; left time: 5748.1064s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0712510\n",
      "\tspeed: 0.1202s/iter; left time: 5778.9095s\n",
      "Epoch: 2 cost time: 00h:05m:19.56s\n",
      "Epoch: 2 | Train Loss: 0.0743254 Vali Loss: 0.0777038 Test Loss: 0.0871744\n",
      "Validation loss decreased (0.078994 --> 0.077704).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0675267\n",
      "\tspeed: 0.9816s/iter; left time: 47025.8595s\n",
      "\titers: 200, epoch: 3 | loss: 0.0488273\n",
      "\tspeed: 0.1208s/iter; left time: 5775.7888s\n",
      "\titers: 300, epoch: 3 | loss: 0.0692624\n",
      "\tspeed: 0.1202s/iter; left time: 5734.2229s\n",
      "\titers: 400, epoch: 3 | loss: 0.0781940\n",
      "\tspeed: 0.1205s/iter; left time: 5736.9944s\n",
      "\titers: 500, epoch: 3 | loss: 0.0633163\n",
      "\tspeed: 0.1205s/iter; left time: 5725.1108s\n",
      "\titers: 600, epoch: 3 | loss: 0.0786292\n",
      "\tspeed: 0.1206s/iter; left time: 5718.5994s\n",
      "\titers: 700, epoch: 3 | loss: 0.0660528\n",
      "\tspeed: 0.1204s/iter; left time: 5696.1600s\n",
      "\titers: 800, epoch: 3 | loss: 0.0873496\n",
      "\tspeed: 0.1204s/iter; left time: 5683.3864s\n",
      "\titers: 900, epoch: 3 | loss: 0.0665047\n",
      "\tspeed: 0.1204s/iter; left time: 5670.9440s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0717470\n",
      "\tspeed: 0.1203s/iter; left time: 5656.4925s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0673361\n",
      "\tspeed: 0.1202s/iter; left time: 5636.8146s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0878783\n",
      "\tspeed: 0.1197s/iter; left time: 5603.4585s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0672138\n",
      "\tspeed: 0.1200s/iter; left time: 5605.4909s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0739039\n",
      "\tspeed: 0.1200s/iter; left time: 5592.3321s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0644442\n",
      "\tspeed: 0.1198s/iter; left time: 5571.6100s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0913087\n",
      "\tspeed: 0.1194s/iter; left time: 5541.4580s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0859381\n",
      "\tspeed: 0.1200s/iter; left time: 5556.2694s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0756321\n",
      "\tspeed: 0.1198s/iter; left time: 5533.6822s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0888247\n",
      "\tspeed: 0.1201s/iter; left time: 5535.6990s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0595378\n",
      "\tspeed: 0.1201s/iter; left time: 5526.0971s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0935414\n",
      "\tspeed: 0.1204s/iter; left time: 5528.2120s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0669725\n",
      "\tspeed: 0.1192s/iter; left time: 5461.0042s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0605008\n",
      "\tspeed: 0.1195s/iter; left time: 5461.4934s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0831451\n",
      "\tspeed: 0.1204s/iter; left time: 5491.7577s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0697700\n",
      "\tspeed: 0.1205s/iter; left time: 5481.7985s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0661712\n",
      "\tspeed: 0.1203s/iter; left time: 5462.8743s\n",
      "Epoch: 3 cost time: 00h:05m:20.24s\n",
      "Epoch: 3 | Train Loss: 0.0718358 Vali Loss: 0.0789278 Test Loss: 0.0876987\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0648829\n",
      "\tspeed: 0.9640s/iter; left time: 43610.8468s\n",
      "\titers: 200, epoch: 4 | loss: 0.0765537\n",
      "\tspeed: 0.1204s/iter; left time: 5434.6165s\n",
      "\titers: 300, epoch: 4 | loss: 0.0551690\n",
      "\tspeed: 0.1207s/iter; left time: 5436.4927s\n",
      "\titers: 400, epoch: 4 | loss: 0.0743216\n",
      "\tspeed: 0.1201s/iter; left time: 5396.6509s\n",
      "\titers: 500, epoch: 4 | loss: 0.0566139\n",
      "\tspeed: 0.1202s/iter; left time: 5389.5299s\n",
      "\titers: 600, epoch: 4 | loss: 0.0790194\n",
      "\tspeed: 0.1199s/iter; left time: 5364.1197s\n",
      "\titers: 700, epoch: 4 | loss: 0.0843454\n",
      "\tspeed: 0.1200s/iter; left time: 5358.3731s\n",
      "\titers: 800, epoch: 4 | loss: 0.0670473\n",
      "\tspeed: 0.1202s/iter; left time: 5352.7789s\n",
      "\titers: 900, epoch: 4 | loss: 0.0606182\n",
      "\tspeed: 0.1201s/iter; left time: 5337.1590s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0734311\n",
      "\tspeed: 0.1165s/iter; left time: 5164.0973s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0760170\n",
      "\tspeed: 0.1200s/iter; left time: 5309.5506s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0643949\n",
      "\tspeed: 0.1197s/iter; left time: 5281.7968s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0647174\n",
      "\tspeed: 0.1205s/iter; left time: 5306.1273s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0552365\n",
      "\tspeed: 0.1103s/iter; left time: 4847.3962s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0723340\n",
      "\tspeed: 0.1026s/iter; left time: 4498.7610s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0671190\n",
      "\tspeed: 0.1022s/iter; left time: 4468.8514s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0635535\n",
      "\tspeed: 0.1169s/iter; left time: 5100.5782s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0668017\n",
      "\tspeed: 0.1195s/iter; left time: 5201.0089s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0685256\n",
      "\tspeed: 0.1197s/iter; left time: 5200.6177s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0721530\n",
      "\tspeed: 0.1197s/iter; left time: 5187.0556s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0882780\n",
      "\tspeed: 0.1189s/iter; left time: 5140.8255s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0493336\n",
      "\tspeed: 0.1106s/iter; left time: 4771.5342s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0750259\n",
      "\tspeed: 0.1025s/iter; left time: 4409.4629s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0622560\n",
      "\tspeed: 0.1022s/iter; left time: 4388.7310s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0759433\n",
      "\tspeed: 0.1145s/iter; left time: 4904.1385s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0722388\n",
      "\tspeed: 0.1194s/iter; left time: 5102.9396s\n",
      "Epoch: 4 cost time: 00h:05m:10.23s\n",
      "Epoch: 4 | Train Loss: 0.0692130 Vali Loss: 0.0806310 Test Loss: 0.0891718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0739205\n",
      "\tspeed: 0.9680s/iter; left time: 41211.1901s\n",
      "\titers: 200, epoch: 5 | loss: 0.0671281\n",
      "\tspeed: 0.1204s/iter; left time: 5113.1638s\n",
      "\titers: 300, epoch: 5 | loss: 0.0831495\n",
      "\tspeed: 0.1204s/iter; left time: 5102.3000s\n",
      "\titers: 400, epoch: 5 | loss: 0.0562803\n",
      "\tspeed: 0.1204s/iter; left time: 5090.6244s\n",
      "\titers: 500, epoch: 5 | loss: 0.0612629\n",
      "\tspeed: 0.1205s/iter; left time: 5080.5162s\n",
      "\titers: 600, epoch: 5 | loss: 0.0725645\n",
      "\tspeed: 0.1209s/iter; left time: 5088.0638s\n",
      "\titers: 700, epoch: 5 | loss: 0.0736510\n",
      "\tspeed: 0.1196s/iter; left time: 5021.6411s\n",
      "\titers: 800, epoch: 5 | loss: 0.0621689\n",
      "\tspeed: 0.1191s/iter; left time: 4988.1594s\n",
      "\titers: 900, epoch: 5 | loss: 0.0703253\n",
      "\tspeed: 0.1074s/iter; left time: 4487.7927s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0583030\n",
      "\tspeed: 0.1067s/iter; left time: 4448.5081s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0703888\n",
      "\tspeed: 0.1202s/iter; left time: 4997.4951s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0585797\n",
      "\tspeed: 0.1203s/iter; left time: 4989.1111s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0742435\n",
      "\tspeed: 0.1202s/iter; left time: 4972.9731s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0719956\n",
      "\tspeed: 0.1200s/iter; left time: 4950.8812s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0606002\n",
      "\tspeed: 0.1203s/iter; left time: 4952.4289s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0692182\n",
      "\tspeed: 0.1195s/iter; left time: 4907.6715s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0654579\n",
      "\tspeed: 0.1191s/iter; left time: 4880.5084s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0808130\n",
      "\tspeed: 0.1197s/iter; left time: 4893.6735s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0635292\n",
      "\tspeed: 0.1197s/iter; left time: 4880.9477s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0533435\n",
      "\tspeed: 0.1207s/iter; left time: 4908.9935s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0679128\n",
      "\tspeed: 0.1208s/iter; left time: 4899.8088s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0734313\n",
      "\tspeed: 0.1119s/iter; left time: 4527.1646s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0613091\n",
      "\tspeed: 0.1022s/iter; left time: 4126.5804s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0723060\n",
      "\tspeed: 0.1024s/iter; left time: 4125.5745s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0571138\n",
      "\tspeed: 0.1120s/iter; left time: 4500.2247s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0643532\n",
      "\tspeed: 0.1203s/iter; left time: 4820.8371s\n",
      "Epoch: 5 cost time: 00h:05m:13.00s\n",
      "Epoch: 5 | Train Loss: 0.0666898 Vali Loss: 0.0816376 Test Loss: 0.0911526\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0592945\n",
      "\tspeed: 0.9657s/iter; left time: 38537.4886s\n",
      "\titers: 200, epoch: 6 | loss: 0.0701825\n",
      "\tspeed: 0.1211s/iter; left time: 4821.7485s\n",
      "\titers: 300, epoch: 6 | loss: 0.0733728\n",
      "\tspeed: 0.1185s/iter; left time: 4704.2811s\n",
      "\titers: 400, epoch: 6 | loss: 0.0500985\n",
      "\tspeed: 0.1180s/iter; left time: 4674.7087s\n",
      "\titers: 500, epoch: 6 | loss: 0.0708840\n",
      "\tspeed: 0.1204s/iter; left time: 4756.7930s\n",
      "\titers: 600, epoch: 6 | loss: 0.0746798\n",
      "\tspeed: 0.1205s/iter; left time: 4748.6832s\n",
      "\titers: 700, epoch: 6 | loss: 0.0672090\n",
      "\tspeed: 0.1203s/iter; left time: 4727.8518s\n",
      "\titers: 800, epoch: 6 | loss: 0.0675267\n",
      "\tspeed: 0.1203s/iter; left time: 4716.1069s\n",
      "\titers: 900, epoch: 6 | loss: 0.0650174\n",
      "\tspeed: 0.1200s/iter; left time: 4693.2471s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0685931\n",
      "\tspeed: 0.1204s/iter; left time: 4695.3959s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0609383\n",
      "\tspeed: 0.1209s/iter; left time: 4705.1570s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0727255\n",
      "\tspeed: 0.1204s/iter; left time: 4671.5887s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0623817\n",
      "\tspeed: 0.1203s/iter; left time: 4657.0666s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603095\n",
      "\tspeed: 0.1207s/iter; left time: 4658.2948s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0660724\n",
      "\tspeed: 0.1200s/iter; left time: 4622.5052s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0712819\n",
      "\tspeed: 0.1202s/iter; left time: 4615.8802s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0682647\n",
      "\tspeed: 0.1206s/iter; left time: 4620.1186s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0655491\n",
      "\tspeed: 0.1210s/iter; left time: 4622.4862s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0615094\n",
      "\tspeed: 0.1206s/iter; left time: 4594.5071s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0686113\n",
      "\tspeed: 0.1201s/iter; left time: 4564.8971s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0684695\n",
      "\tspeed: 0.1208s/iter; left time: 4580.1767s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0678513\n",
      "\tspeed: 0.1203s/iter; left time: 4549.4603s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0722934\n",
      "\tspeed: 0.1206s/iter; left time: 4546.5078s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0518117\n",
      "\tspeed: 0.1205s/iter; left time: 4533.1992s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0615523\n",
      "\tspeed: 0.1204s/iter; left time: 4514.2958s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0523729\n",
      "\tspeed: 0.1207s/iter; left time: 4516.2380s\n",
      "Epoch: 6 cost time: 00h:05m:21.38s\n",
      "Epoch: 6 | Train Loss: 0.0646321 Vali Loss: 0.0833404 Test Loss: 0.0923264\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0626126\n",
      "\tspeed: 0.9567s/iter; left time: 35627.9855s\n",
      "\titers: 200, epoch: 7 | loss: 0.0635089\n",
      "\tspeed: 0.1106s/iter; left time: 4108.5023s\n",
      "\titers: 300, epoch: 7 | loss: 0.0645225\n",
      "\tspeed: 0.1197s/iter; left time: 4433.4494s\n",
      "\titers: 400, epoch: 7 | loss: 0.0587169\n",
      "\tspeed: 0.1206s/iter; left time: 4455.9176s\n",
      "\titers: 500, epoch: 7 | loss: 0.0679099\n",
      "\tspeed: 0.1199s/iter; left time: 4416.9940s\n",
      "\titers: 600, epoch: 7 | loss: 0.0761093\n",
      "\tspeed: 0.1200s/iter; left time: 4406.9235s\n",
      "\titers: 700, epoch: 7 | loss: 0.0614073\n",
      "\tspeed: 0.1198s/iter; left time: 4388.3678s\n",
      "\titers: 800, epoch: 7 | loss: 0.0606054\n",
      "\tspeed: 0.1196s/iter; left time: 4370.5688s\n",
      "\titers: 900, epoch: 7 | loss: 0.0640666\n",
      "\tspeed: 0.1205s/iter; left time: 4391.2438s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0652940\n",
      "\tspeed: 0.1208s/iter; left time: 4390.7018s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0591778\n",
      "\tspeed: 0.1204s/iter; left time: 4362.7470s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0599352\n",
      "\tspeed: 0.1200s/iter; left time: 4336.2508s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0674185\n",
      "\tspeed: 0.1203s/iter; left time: 4336.9401s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0645608\n",
      "\tspeed: 0.1204s/iter; left time: 4327.4001s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0626351\n",
      "\tspeed: 0.1201s/iter; left time: 4304.2255s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0564335\n",
      "\tspeed: 0.1203s/iter; left time: 4298.8201s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0628764\n",
      "\tspeed: 0.1179s/iter; left time: 4200.4524s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0579112\n",
      "\tspeed: 0.1189s/iter; left time: 4226.9163s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0576657\n",
      "\tspeed: 0.1205s/iter; left time: 4270.9218s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0645774\n",
      "\tspeed: 0.1208s/iter; left time: 4269.8027s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0611573\n",
      "\tspeed: 0.1206s/iter; left time: 4250.7049s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0668616\n",
      "\tspeed: 0.1204s/iter; left time: 4231.6938s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0692711\n",
      "\tspeed: 0.1203s/iter; left time: 4215.0403s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0704450\n",
      "\tspeed: 0.1203s/iter; left time: 4203.9965s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0594643\n",
      "\tspeed: 0.1208s/iter; left time: 4207.7715s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0567257\n",
      "\tspeed: 0.1203s/iter; left time: 4179.7499s\n",
      "Epoch: 7 cost time: 00h:05m:18.88s\n",
      "Epoch: 7 | Train Loss: 0.0629600 Vali Loss: 0.0832727 Test Loss: 0.0938695\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02040581777691841, rmse:0.142848938703537, mae:0.08717440813779831, rse:0.5534131526947021\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 168: 00h:47m:38.09s\n",
      "\n",
      "Intermediate time for FR: 03h:16m:59.45s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-01 13:04:31,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 13:04:32,531] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 13:04:32,531] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 13:04:32,531] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 13:04:32,616] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 13:04:32,617] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 13:04:33,340] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 13:04:33,341] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 13:04:33,341] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 13:04:33,343] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 13:04:33,343] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 13:04:33,343] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 13:04:33,344] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 13:04:33,344] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 13:04:33,344] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 13:04:33,344] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 13:04:33,676] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 13:04:33,677] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 13:04:33,678] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.86 GB, percent = 12.3%\n",
      "[2024-11-01 13:04:33,804] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 13:04:33,805] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 13:04:33,805] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.86 GB, percent = 12.3%\n",
      "[2024-11-01 13:04:33,806] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 13:04:33,926] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 13:04:33,927] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 13:04:33,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 92.86 GB, percent = 12.3%\n",
      "[2024-11-01 13:04:33,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 13:04:33,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 13:04:33,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 13:04:33,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 13:04:33,929] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 13:04:33,929] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 13:04:33,929] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 13:04:33,929] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 13:04:33,929] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faced1c3790>\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 13:04:33,930] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 13:04:33,931] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 13:04:33,932] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1961632\n",
      "\tspeed: 0.1833s/iter; left time: 9810.8464s\n",
      "\titers: 200, epoch: 1 | loss: 0.1897899\n",
      "\tspeed: 0.1454s/iter; left time: 7768.9982s\n",
      "\titers: 300, epoch: 1 | loss: 0.1400119\n",
      "\tspeed: 0.1441s/iter; left time: 7684.5852s\n",
      "\titers: 400, epoch: 1 | loss: 0.1414553\n",
      "\tspeed: 0.1453s/iter; left time: 7731.8958s\n",
      "\titers: 500, epoch: 1 | loss: 0.1245781\n",
      "\tspeed: 0.1415s/iter; left time: 7516.0493s\n",
      "\titers: 600, epoch: 1 | loss: 0.0947767\n",
      "\tspeed: 0.1429s/iter; left time: 7579.3498s\n",
      "\titers: 700, epoch: 1 | loss: 0.0984841\n",
      "\tspeed: 0.1441s/iter; left time: 7625.4152s\n",
      "\titers: 800, epoch: 1 | loss: 0.0828382\n",
      "\tspeed: 0.1458s/iter; left time: 7701.1120s\n",
      "\titers: 900, epoch: 1 | loss: 0.0719742\n",
      "\tspeed: 0.1413s/iter; left time: 7451.6957s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1195307\n",
      "\tspeed: 0.1456s/iter; left time: 7663.5386s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0817368\n",
      "\tspeed: 0.1442s/iter; left time: 7571.5605s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0921901\n",
      "\tspeed: 0.1332s/iter; left time: 6984.4445s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0989637\n",
      "\tspeed: 0.1422s/iter; left time: 7440.3441s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0795155\n",
      "\tspeed: 0.1314s/iter; left time: 6862.9960s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0898348\n",
      "\tspeed: 0.1310s/iter; left time: 6829.2729s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0705952\n",
      "\tspeed: 0.1304s/iter; left time: 6783.1200s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0890884\n",
      "\tspeed: 0.1308s/iter; left time: 6790.5149s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0708546\n",
      "\tspeed: 0.1308s/iter; left time: 6776.8064s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0823215\n",
      "\tspeed: 0.1311s/iter; left time: 6779.3560s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0903587\n",
      "\tspeed: 0.1304s/iter; left time: 6733.6372s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0667403\n",
      "\tspeed: 0.1308s/iter; left time: 6739.8755s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0840566\n",
      "\tspeed: 0.1307s/iter; left time: 6720.3720s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1007551\n",
      "\tspeed: 0.1306s/iter; left time: 6700.9664s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0791763\n",
      "\tspeed: 0.1306s/iter; left time: 6688.6590s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0936417\n",
      "\tspeed: 0.1310s/iter; left time: 6699.2369s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0681735\n",
      "\tspeed: 0.1304s/iter; left time: 6653.8129s\n",
      "Epoch: 1 cost time: 00h:06m:07.48s\n",
      "Epoch: 1 | Train Loss: 0.1042742 Vali Loss: 0.0697548 Test Loss: 0.0724680\n",
      "Validation loss decreased (inf --> 0.069755).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0968200\n",
      "\tspeed: 1.1572s/iter; left time: 58831.1176s\n",
      "\titers: 200, epoch: 2 | loss: 0.0908526\n",
      "\tspeed: 0.1203s/iter; left time: 6103.8041s\n",
      "\titers: 300, epoch: 2 | loss: 0.0880430\n",
      "\tspeed: 0.1174s/iter; left time: 5945.0386s\n",
      "\titers: 400, epoch: 2 | loss: 0.0909087\n",
      "\tspeed: 0.1202s/iter; left time: 6074.7429s\n",
      "\titers: 500, epoch: 2 | loss: 0.0755832\n",
      "\tspeed: 0.1197s/iter; left time: 6036.9880s\n",
      "\titers: 600, epoch: 2 | loss: 0.0719677\n",
      "\tspeed: 0.1204s/iter; left time: 6058.4529s\n",
      "\titers: 700, epoch: 2 | loss: 0.0632761\n",
      "\tspeed: 0.1204s/iter; left time: 6049.5524s\n",
      "\titers: 800, epoch: 2 | loss: 0.0799158\n",
      "\tspeed: 0.1198s/iter; left time: 6007.3276s\n",
      "\titers: 900, epoch: 2 | loss: 0.0778257\n",
      "\tspeed: 0.1202s/iter; left time: 6016.4166s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0686750\n",
      "\tspeed: 0.1200s/iter; left time: 5991.3429s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0759580\n",
      "\tspeed: 0.1205s/iter; left time: 6007.7322s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0850438\n",
      "\tspeed: 0.1204s/iter; left time: 5987.5987s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0768398\n",
      "\tspeed: 0.1202s/iter; left time: 5967.5839s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0734043\n",
      "\tspeed: 0.1197s/iter; left time: 5930.4064s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0702756\n",
      "\tspeed: 0.1202s/iter; left time: 5942.7494s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0901470\n",
      "\tspeed: 0.1207s/iter; left time: 5956.9218s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0736003\n",
      "\tspeed: 0.1205s/iter; left time: 5933.1071s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0812662\n",
      "\tspeed: 0.1202s/iter; left time: 5907.7950s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0856911\n",
      "\tspeed: 0.1160s/iter; left time: 5689.9465s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0710515\n",
      "\tspeed: 0.1190s/iter; left time: 5822.5993s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0731606\n",
      "\tspeed: 0.1040s/iter; left time: 5078.3971s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0908238\n",
      "\tspeed: 0.1160s/iter; left time: 5655.1282s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0716156\n",
      "\tspeed: 0.1196s/iter; left time: 5817.8012s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0662761\n",
      "\tspeed: 0.1199s/iter; left time: 5820.6415s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0705364\n",
      "\tspeed: 0.1197s/iter; left time: 5796.9391s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0787798\n",
      "\tspeed: 0.1191s/iter; left time: 5755.2663s\n",
      "Epoch: 2 cost time: 00h:05m:19.68s\n",
      "Epoch: 2 | Train Loss: 0.0792366 Vali Loss: 0.0630193 Test Loss: 0.0664905\n",
      "Validation loss decreased (0.069755 --> 0.063019).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0837417\n",
      "\tspeed: 1.0234s/iter; left time: 49285.2181s\n",
      "\titers: 200, epoch: 3 | loss: 0.0720257\n",
      "\tspeed: 0.1205s/iter; left time: 5791.6005s\n",
      "\titers: 300, epoch: 3 | loss: 0.0743371\n",
      "\tspeed: 0.1202s/iter; left time: 5765.9035s\n",
      "\titers: 400, epoch: 3 | loss: 0.0783527\n",
      "\tspeed: 0.1204s/iter; left time: 5761.3312s\n",
      "\titers: 500, epoch: 3 | loss: 0.0714736\n",
      "\tspeed: 0.1206s/iter; left time: 5758.4031s\n",
      "\titers: 600, epoch: 3 | loss: 0.0732643\n",
      "\tspeed: 0.1198s/iter; left time: 5710.6399s\n",
      "\titers: 700, epoch: 3 | loss: 0.0646055\n",
      "\tspeed: 0.1192s/iter; left time: 5667.4669s\n",
      "\titers: 800, epoch: 3 | loss: 0.0739734\n",
      "\tspeed: 0.1187s/iter; left time: 5633.6934s\n",
      "\titers: 900, epoch: 3 | loss: 0.0694846\n",
      "\tspeed: 0.1191s/iter; left time: 5642.0237s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0750011\n",
      "\tspeed: 0.1203s/iter; left time: 5685.5456s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0742698\n",
      "\tspeed: 0.1199s/iter; left time: 5653.5893s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0718982\n",
      "\tspeed: 0.1192s/iter; left time: 5609.0992s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0686082\n",
      "\tspeed: 0.1200s/iter; left time: 5635.4408s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0775685\n",
      "\tspeed: 0.1116s/iter; left time: 5230.8185s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0676080\n",
      "\tspeed: 0.1201s/iter; left time: 5616.3261s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0703222\n",
      "\tspeed: 0.1206s/iter; left time: 5627.3612s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0706295\n",
      "\tspeed: 0.1196s/iter; left time: 5568.7014s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0611054\n",
      "\tspeed: 0.1201s/iter; left time: 5579.5153s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0822397\n",
      "\tspeed: 0.1207s/iter; left time: 5596.1161s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0630606\n",
      "\tspeed: 0.1051s/iter; left time: 4860.7075s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0664413\n",
      "\tspeed: 0.1170s/iter; left time: 5399.2471s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0754516\n",
      "\tspeed: 0.1200s/iter; left time: 5528.1149s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0628932\n",
      "\tspeed: 0.1204s/iter; left time: 5534.6005s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0689445\n",
      "\tspeed: 0.1195s/iter; left time: 5480.3888s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0753874\n",
      "\tspeed: 0.1204s/iter; left time: 5509.7758s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0803154\n",
      "\tspeed: 0.1189s/iter; left time: 5429.6057s\n",
      "Epoch: 3 cost time: 00h:05m:19.44s\n",
      "Epoch: 3 | Train Loss: 0.0754979 Vali Loss: 0.0626933 Test Loss: 0.0662443\n",
      "Validation loss decreased (0.063019 --> 0.062693).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0701278\n",
      "\tspeed: 1.0181s/iter; left time: 46299.1632s\n",
      "\titers: 200, epoch: 4 | loss: 0.0728249\n",
      "\tspeed: 0.1202s/iter; left time: 5454.8825s\n",
      "\titers: 300, epoch: 4 | loss: 0.0778461\n",
      "\tspeed: 0.1191s/iter; left time: 5393.1478s\n",
      "\titers: 400, epoch: 4 | loss: 0.0785829\n",
      "\tspeed: 0.1196s/iter; left time: 5404.1341s\n",
      "\titers: 500, epoch: 4 | loss: 0.0790459\n",
      "\tspeed: 0.1193s/iter; left time: 5375.8122s\n",
      "\titers: 600, epoch: 4 | loss: 0.0710924\n",
      "\tspeed: 0.1195s/iter; left time: 5374.3405s\n",
      "\titers: 700, epoch: 4 | loss: 0.0721118\n",
      "\tspeed: 0.1201s/iter; left time: 5390.4976s\n",
      "\titers: 800, epoch: 4 | loss: 0.0836879\n",
      "\tspeed: 0.1082s/iter; left time: 4844.9978s\n",
      "\titers: 900, epoch: 4 | loss: 0.0670347\n",
      "\tspeed: 0.1172s/iter; left time: 5236.3352s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0732893\n",
      "\tspeed: 0.1160s/iter; left time: 5172.4386s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0734157\n",
      "\tspeed: 0.1194s/iter; left time: 5311.8766s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0836176\n",
      "\tspeed: 0.1179s/iter; left time: 5230.9850s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0914069\n",
      "\tspeed: 0.1095s/iter; left time: 4849.5702s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0554709\n",
      "\tspeed: 0.1139s/iter; left time: 5030.2668s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0638680\n",
      "\tspeed: 0.1194s/iter; left time: 5264.5478s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0843564\n",
      "\tspeed: 0.1085s/iter; left time: 4773.2362s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0867873\n",
      "\tspeed: 0.1176s/iter; left time: 5159.9130s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0825667\n",
      "\tspeed: 0.1186s/iter; left time: 5192.1792s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0808909\n",
      "\tspeed: 0.1170s/iter; left time: 5109.8835s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0671010\n",
      "\tspeed: 0.1202s/iter; left time: 5238.6629s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0800280\n",
      "\tspeed: 0.1201s/iter; left time: 5219.7348s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0652930\n",
      "\tspeed: 0.1197s/iter; left time: 5193.7120s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0597176\n",
      "\tspeed: 0.1194s/iter; left time: 5168.5659s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0868424\n",
      "\tspeed: 0.1202s/iter; left time: 5189.5003s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0782121\n",
      "\tspeed: 0.1045s/iter; left time: 4500.0510s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0802435\n",
      "\tspeed: 0.1030s/iter; left time: 4426.3750s\n",
      "Epoch: 4 cost time: 00h:05m:12.92s\n",
      "Epoch: 4 | Train Loss: 0.0732161 Vali Loss: 0.0602605 Test Loss: 0.0639531\n",
      "Validation loss decreased (0.062693 --> 0.060260).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0794265\n",
      "\tspeed: 1.0224s/iter; left time: 43756.3259s\n",
      "\titers: 200, epoch: 5 | loss: 0.0705508\n",
      "\tspeed: 0.1206s/iter; left time: 5149.2286s\n",
      "\titers: 300, epoch: 5 | loss: 0.0829213\n",
      "\tspeed: 0.1208s/iter; left time: 5145.2070s\n",
      "\titers: 400, epoch: 5 | loss: 0.0772983\n",
      "\tspeed: 0.1199s/iter; left time: 5095.2742s\n",
      "\titers: 500, epoch: 5 | loss: 0.0697670\n",
      "\tspeed: 0.1166s/iter; left time: 4942.0477s\n",
      "\titers: 600, epoch: 5 | loss: 0.0615391\n",
      "\tspeed: 0.1186s/iter; left time: 5016.8114s\n",
      "\titers: 700, epoch: 5 | loss: 0.0567918\n",
      "\tspeed: 0.1126s/iter; left time: 4752.5861s\n",
      "\titers: 800, epoch: 5 | loss: 0.0847099\n",
      "\tspeed: 0.1181s/iter; left time: 4972.8196s\n",
      "\titers: 900, epoch: 5 | loss: 0.0704188\n",
      "\tspeed: 0.1200s/iter; left time: 5039.4053s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0620770\n",
      "\tspeed: 0.1201s/iter; left time: 5030.8103s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0741380\n",
      "\tspeed: 0.1193s/iter; left time: 4985.7148s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0661858\n",
      "\tspeed: 0.1204s/iter; left time: 5020.2137s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0621480\n",
      "\tspeed: 0.1192s/iter; left time: 4959.9121s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0629700\n",
      "\tspeed: 0.1186s/iter; left time: 4921.5209s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0820492\n",
      "\tspeed: 0.1192s/iter; left time: 4935.7549s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0610860\n",
      "\tspeed: 0.1114s/iter; left time: 4599.7470s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0861114\n",
      "\tspeed: 0.1203s/iter; left time: 4955.9866s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0606930\n",
      "\tspeed: 0.1141s/iter; left time: 4690.0508s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0612833\n",
      "\tspeed: 0.1157s/iter; left time: 4745.0385s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0786318\n",
      "\tspeed: 0.1064s/iter; left time: 4351.3976s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0779789\n",
      "\tspeed: 0.1162s/iter; left time: 4738.6194s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0667817\n",
      "\tspeed: 0.1210s/iter; left time: 4922.3772s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0773585\n",
      "\tspeed: 0.1158s/iter; left time: 4702.1170s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0830474\n",
      "\tspeed: 0.1109s/iter; left time: 4489.9204s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0766403\n",
      "\tspeed: 0.1259s/iter; left time: 5086.0663s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0594169\n",
      "\tspeed: 0.1307s/iter; left time: 5267.9848s\n",
      "Epoch: 5 cost time: 00h:05m:18.84s\n",
      "Epoch: 5 | Train Loss: 0.0718611 Vali Loss: 0.0608224 Test Loss: 0.0644853\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0779639\n",
      "\tspeed: 1.1754s/iter; left time: 47150.9887s\n",
      "\titers: 200, epoch: 6 | loss: 0.0661459\n",
      "\tspeed: 0.1303s/iter; left time: 5214.0397s\n",
      "\titers: 300, epoch: 6 | loss: 0.0925239\n",
      "\tspeed: 0.1318s/iter; left time: 5260.2344s\n",
      "\titers: 400, epoch: 6 | loss: 0.0681258\n",
      "\tspeed: 0.1334s/iter; left time: 5311.6688s\n",
      "\titers: 500, epoch: 6 | loss: 0.0705238\n",
      "\tspeed: 0.1273s/iter; left time: 5056.2473s\n",
      "\titers: 600, epoch: 6 | loss: 0.0755477\n",
      "\tspeed: 0.1314s/iter; left time: 5203.7159s\n",
      "\titers: 700, epoch: 6 | loss: 0.0716280\n",
      "\tspeed: 0.1302s/iter; left time: 5143.2423s\n",
      "\titers: 800, epoch: 6 | loss: 0.0619959\n",
      "\tspeed: 0.1310s/iter; left time: 5163.9305s\n",
      "\titers: 900, epoch: 6 | loss: 0.0714130\n",
      "\tspeed: 0.1301s/iter; left time: 5115.7798s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0737989\n",
      "\tspeed: 0.1327s/iter; left time: 5203.3403s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0621136\n",
      "\tspeed: 0.1328s/iter; left time: 5195.2646s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0685738\n",
      "\tspeed: 0.1330s/iter; left time: 5189.4215s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0633010\n",
      "\tspeed: 0.1306s/iter; left time: 5083.8849s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0603377\n",
      "\tspeed: 0.1326s/iter; left time: 5147.7791s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0727599\n",
      "\tspeed: 0.1344s/iter; left time: 5205.2209s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0715380\n",
      "\tspeed: 0.1310s/iter; left time: 5057.7486s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0616430\n",
      "\tspeed: 0.1327s/iter; left time: 5110.9555s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0798777\n",
      "\tspeed: 0.1323s/iter; left time: 5084.3183s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0765420\n",
      "\tspeed: 0.1295s/iter; left time: 4962.1817s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0671012\n",
      "\tspeed: 0.1204s/iter; left time: 4599.4987s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0616646\n",
      "\tspeed: 0.1280s/iter; left time: 4879.6497s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0681724\n",
      "\tspeed: 0.1334s/iter; left time: 5072.1045s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0847921\n",
      "\tspeed: 0.1345s/iter; left time: 5099.5482s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0787449\n",
      "\tspeed: 0.1322s/iter; left time: 5000.8846s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0638925\n",
      "\tspeed: 0.1329s/iter; left time: 5011.7023s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0637040\n",
      "\tspeed: 0.1309s/iter; left time: 4923.8702s\n",
      "Epoch: 6 cost time: 00h:05m:52.34s\n",
      "Epoch: 6 | Train Loss: 0.0710204 Vali Loss: 0.0602071 Test Loss: 0.0637255\n",
      "Validation loss decreased (0.060260 --> 0.060207).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0805032\n",
      "\tspeed: 1.2019s/iter; left time: 44993.6354s\n",
      "\titers: 200, epoch: 7 | loss: 0.0928424\n",
      "\tspeed: 0.2726s/iter; left time: 10176.4829s\n",
      "\titers: 300, epoch: 7 | loss: 0.0699223\n",
      "\tspeed: 0.5141s/iter; left time: 19143.7504s\n",
      "\titers: 400, epoch: 7 | loss: 0.0772188\n",
      "\tspeed: 0.7631s/iter; left time: 28336.7197s\n",
      "\titers: 500, epoch: 7 | loss: 0.0806367\n",
      "\tspeed: 0.7636s/iter; left time: 28280.2318s\n",
      "\titers: 600, epoch: 7 | loss: 0.0634284\n",
      "\tspeed: 0.7528s/iter; left time: 27804.9709s\n",
      "\titers: 700, epoch: 7 | loss: 0.0699541\n",
      "\tspeed: 0.7426s/iter; left time: 27353.2393s\n",
      "\titers: 800, epoch: 7 | loss: 0.0892019\n",
      "\tspeed: 0.7607s/iter; left time: 27945.8585s\n",
      "\titers: 900, epoch: 7 | loss: 0.0712903\n",
      "\tspeed: 0.7552s/iter; left time: 27667.5327s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0747735\n",
      "\tspeed: 0.7578s/iter; left time: 27684.7751s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0782191\n",
      "\tspeed: 0.7544s/iter; left time: 27486.3879s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0647047\n",
      "\tspeed: 0.7484s/iter; left time: 27191.4154s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0666225\n",
      "\tspeed: 0.7504s/iter; left time: 27189.8257s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843146\n",
      "\tspeed: 0.7509s/iter; left time: 27133.3752s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0681136\n",
      "\tspeed: 0.7478s/iter; left time: 26945.4498s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0701010\n",
      "\tspeed: 0.7438s/iter; left time: 26727.4265s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0843430\n",
      "\tspeed: 0.7085s/iter; left time: 25387.3125s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0538953\n",
      "\tspeed: 0.7126s/iter; left time: 25463.9358s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0655326\n",
      "\tspeed: 0.6850s/iter; left time: 24409.1903s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0805343\n",
      "\tspeed: 0.7074s/iter; left time: 25136.3504s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0631403\n",
      "\tspeed: 0.6977s/iter; left time: 24723.7157s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0604495\n",
      "\tspeed: 0.6846s/iter; left time: 24188.8665s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0642110\n",
      "\tspeed: 0.6690s/iter; left time: 23570.5864s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0664487\n",
      "\tspeed: 0.6574s/iter; left time: 23098.1666s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0816911\n",
      "\tspeed: 0.6447s/iter; left time: 22587.5511s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0661209\n",
      "\tspeed: 0.6348s/iter; left time: 22176.0760s\n",
      "Epoch: 7 cost time: 00h:30m:02.44s\n",
      "Epoch: 7 | Train Loss: 0.0702127 Vali Loss: 0.0597712 Test Loss: 0.0634713\n",
      "Validation loss decreased (0.060207 --> 0.059771).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0825887\n",
      "\tspeed: 4.6095s/iter; left time: 160197.7928s\n",
      "\titers: 200, epoch: 8 | loss: 0.0750446\n",
      "\tspeed: 0.3246s/iter; left time: 11249.2383s\n",
      "\titers: 300, epoch: 8 | loss: 0.0714472\n",
      "\tspeed: 0.2551s/iter; left time: 8816.3554s\n",
      "\titers: 400, epoch: 8 | loss: 0.0683220\n",
      "\tspeed: 0.7112s/iter; left time: 24503.0054s\n",
      "\titers: 500, epoch: 8 | loss: 0.0658177\n",
      "\tspeed: 0.7142s/iter; left time: 24534.5410s\n",
      "\titers: 600, epoch: 8 | loss: 0.0671118\n",
      "\tspeed: 0.7222s/iter; left time: 24737.5661s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654151\n",
      "\tspeed: 0.6741s/iter; left time: 23021.5282s\n",
      "\titers: 800, epoch: 8 | loss: 0.0633116\n",
      "\tspeed: 0.7181s/iter; left time: 24452.7294s\n",
      "\titers: 900, epoch: 8 | loss: 0.0762280\n",
      "\tspeed: 0.7174s/iter; left time: 24359.7209s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0904688\n",
      "\tspeed: 0.7146s/iter; left time: 24193.1083s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0685867\n",
      "\tspeed: 0.7113s/iter; left time: 24009.8596s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0753753\n",
      "\tspeed: 0.7057s/iter; left time: 23748.5776s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0869639\n",
      "\tspeed: 0.6965s/iter; left time: 23371.7400s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0656767\n",
      "\tspeed: 0.6695s/iter; left time: 22398.7038s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0751337\n",
      "\tspeed: 0.6512s/iter; left time: 21720.6887s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0769074\n",
      "\tspeed: 0.6062s/iter; left time: 20158.3421s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0628735\n",
      "\tspeed: 0.5882s/iter; left time: 19502.4224s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0630130\n",
      "\tspeed: 0.5631s/iter; left time: 18611.1647s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0680705\n",
      "\tspeed: 0.4794s/iter; left time: 15797.6651s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0652012\n",
      "\tspeed: 0.4208s/iter; left time: 13825.5718s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0650971\n",
      "\tspeed: 0.3317s/iter; left time: 10862.9063s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0778571\n",
      "\tspeed: 0.3664s/iter; left time: 11963.3923s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0902112\n",
      "\tspeed: 0.3738s/iter; left time: 12170.2340s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0791112\n",
      "\tspeed: 0.3123s/iter; left time: 10135.5055s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0677406\n",
      "\tspeed: 0.3436s/iter; left time: 11116.9490s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0605854\n",
      "\tspeed: 0.2813s/iter; left time: 9073.0592s\n",
      "Epoch: 8 cost time: 00h:23m:28.03s\n",
      "Epoch: 8 | Train Loss: 0.0696060 Vali Loss: 0.0590203 Test Loss: 0.0629085\n",
      "Validation loss decreased (0.059771 --> 0.059020).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0732988\n",
      "\tspeed: 4.3752s/iter; left time: 140324.9766s\n",
      "\titers: 200, epoch: 9 | loss: 0.0632006\n",
      "\tspeed: 0.7223s/iter; left time: 23092.6556s\n",
      "\titers: 300, epoch: 9 | loss: 0.0675089\n",
      "\tspeed: 0.7233s/iter; left time: 23053.0791s\n",
      "\titers: 400, epoch: 9 | loss: 0.0753807\n",
      "\tspeed: 0.7218s/iter; left time: 22934.9957s\n",
      "\titers: 500, epoch: 9 | loss: 0.0688046\n",
      "\tspeed: 0.7213s/iter; left time: 22846.2242s\n",
      "\titers: 600, epoch: 9 | loss: 0.0563322\n",
      "\tspeed: 0.7189s/iter; left time: 22698.4932s\n",
      "\titers: 700, epoch: 9 | loss: 0.0637789\n",
      "\tspeed: 0.7214s/iter; left time: 22705.7106s\n",
      "\titers: 800, epoch: 9 | loss: 0.0552997\n",
      "\tspeed: 0.7192s/iter; left time: 22563.3068s\n",
      "\titers: 900, epoch: 9 | loss: 0.0730415\n",
      "\tspeed: 0.7191s/iter; left time: 22488.1704s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0624509\n",
      "\tspeed: 0.7211s/iter; left time: 22478.3028s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0611722\n",
      "\tspeed: 0.7118s/iter; left time: 22116.9126s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0624793\n",
      "\tspeed: 0.7194s/iter; left time: 22280.7683s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0554433\n",
      "\tspeed: 0.7182s/iter; left time: 22171.4464s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0854482\n",
      "\tspeed: 0.7182s/iter; left time: 22100.1287s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1009781\n",
      "\tspeed: 0.6917s/iter; left time: 21215.8174s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0588896\n",
      "\tspeed: 0.7189s/iter; left time: 21978.3551s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0664771\n",
      "\tspeed: 0.7186s/iter; left time: 21897.6830s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0875199\n",
      "\tspeed: 0.7169s/iter; left time: 21775.4159s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0652644\n",
      "\tspeed: 0.7179s/iter; left time: 21732.6895s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0729210\n",
      "\tspeed: 0.7177s/iter; left time: 21655.3899s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0595015\n",
      "\tspeed: 0.7170s/iter; left time: 21561.5036s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0645168\n",
      "\tspeed: 0.7147s/iter; left time: 21420.6462s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0664069\n",
      "\tspeed: 0.7131s/iter; left time: 21303.5937s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0560363\n",
      "\tspeed: 0.7055s/iter; left time: 21004.7656s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0640769\n",
      "\tspeed: 0.7129s/iter; left time: 21154.5235s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0609662\n",
      "\tspeed: 0.7118s/iter; left time: 21049.0637s\n",
      "Epoch: 9 cost time: 00h:32m:01.48s\n",
      "Epoch: 9 | Train Loss: 0.0688502 Vali Loss: 0.0579161 Test Loss: 0.0620261\n",
      "Validation loss decreased (0.059020 --> 0.057916).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0692350\n",
      "\tspeed: 6.6443s/iter; left time: 195287.7996s\n",
      "\titers: 200, epoch: 10 | loss: 0.0792829\n",
      "\tspeed: 0.5529s/iter; left time: 16194.6119s\n",
      "\titers: 300, epoch: 10 | loss: 0.0651196\n",
      "\tspeed: 0.4403s/iter; left time: 12851.8566s\n",
      "\titers: 400, epoch: 10 | loss: 0.0529624\n",
      "\tspeed: 0.3075s/iter; left time: 8944.8923s\n",
      "\titers: 500, epoch: 10 | loss: 0.0596995\n",
      "\tspeed: 0.4291s/iter; left time: 12440.6286s\n",
      "\titers: 600, epoch: 10 | loss: 0.0596051\n",
      "\tspeed: 0.4261s/iter; left time: 12310.4940s\n",
      "\titers: 700, epoch: 10 | loss: 0.0729908\n",
      "\tspeed: 0.3251s/iter; left time: 9359.6015s\n",
      "\titers: 800, epoch: 10 | loss: 0.0748923\n",
      "\tspeed: 0.2566s/iter; left time: 7363.0057s\n",
      "\titers: 900, epoch: 10 | loss: 0.0764702\n",
      "\tspeed: 0.5224s/iter; left time: 14936.5499s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0590842\n",
      "\tspeed: 0.6963s/iter; left time: 19838.3853s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0709747\n",
      "\tspeed: 0.7237s/iter; left time: 20546.0823s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0812026\n",
      "\tspeed: 0.7222s/iter; left time: 20433.7690s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0673435\n",
      "\tspeed: 0.7218s/iter; left time: 20350.2907s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0555878\n",
      "\tspeed: 0.7215s/iter; left time: 20267.5390s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0766573\n",
      "\tspeed: 0.7186s/iter; left time: 20116.0053s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0764899\n",
      "\tspeed: 0.7202s/iter; left time: 20088.8651s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0618597\n",
      "\tspeed: 0.7196s/iter; left time: 19999.0916s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0638845\n",
      "\tspeed: 0.7189s/iter; left time: 19906.7460s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0900016\n",
      "\tspeed: 0.7082s/iter; left time: 19541.6167s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0711123\n",
      "\tspeed: 0.7172s/iter; left time: 19718.1875s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0715640\n",
      "\tspeed: 0.7158s/iter; left time: 19607.5302s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0749826\n",
      "\tspeed: 0.7173s/iter; left time: 19575.6292s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0696980\n",
      "\tspeed: 0.6916s/iter; left time: 18805.6594s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0574644\n",
      "\tspeed: 0.7143s/iter; left time: 19350.6419s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0694450\n",
      "\tspeed: 0.7180s/iter; left time: 19381.4209s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0577533\n",
      "\tspeed: 0.7160s/iter; left time: 19255.6467s\n",
      "Epoch: 10 cost time: 00h:27m:37.85s\n",
      "Epoch: 10 | Train Loss: 0.0683724 Vali Loss: 0.0582843 Test Loss: 0.0620225\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0733247\n",
      "\tspeed: 6.3366s/iter; left time: 169257.6041s\n",
      "\titers: 200, epoch: 11 | loss: 0.0593850\n",
      "\tspeed: 0.3699s/iter; left time: 9844.4107s\n",
      "\titers: 300, epoch: 11 | loss: 0.0672786\n",
      "\tspeed: 0.2805s/iter; left time: 7436.6534s\n",
      "\titers: 400, epoch: 11 | loss: 0.0505884\n",
      "\tspeed: 0.2613s/iter; left time: 6901.5134s\n",
      "\titers: 500, epoch: 11 | loss: 0.0644879\n",
      "\tspeed: 0.2648s/iter; left time: 6966.8331s\n",
      "\titers: 600, epoch: 11 | loss: 0.0813994\n",
      "\tspeed: 0.3414s/iter; left time: 8947.5105s\n",
      "\titers: 700, epoch: 11 | loss: 0.0774051\n",
      "\tspeed: 0.4476s/iter; left time: 11686.6079s\n",
      "\titers: 800, epoch: 11 | loss: 0.0700469\n",
      "\tspeed: 0.3679s/iter; left time: 9568.3668s\n",
      "\titers: 900, epoch: 11 | loss: 0.0562205\n",
      "\tspeed: 0.2569s/iter; left time: 6655.9627s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0744258\n",
      "\tspeed: 0.2539s/iter; left time: 6552.7677s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0834967\n",
      "\tspeed: 0.3397s/iter; left time: 8734.2203s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0731772\n",
      "\tspeed: 0.7240s/iter; left time: 18543.2097s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0543189\n",
      "\tspeed: 0.7243s/iter; left time: 18478.0323s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0754633\n",
      "\tspeed: 0.7214s/iter; left time: 18331.6735s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0645236\n",
      "\tspeed: 0.7227s/iter; left time: 18292.4536s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0708787\n",
      "\tspeed: 0.7223s/iter; left time: 18209.5578s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0754559\n",
      "\tspeed: 0.7215s/iter; left time: 18116.8929s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0719775\n",
      "\tspeed: 0.7214s/iter; left time: 18041.9252s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0709032\n",
      "\tspeed: 0.7202s/iter; left time: 17940.4667s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0738048\n",
      "\tspeed: 0.7205s/iter; left time: 17875.5513s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0666654\n",
      "\tspeed: 0.7198s/iter; left time: 17786.1259s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0716899\n",
      "\tspeed: 0.7193s/iter; left time: 17702.6098s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0751313\n",
      "\tspeed: 0.7203s/iter; left time: 17655.6067s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0650967\n",
      "\tspeed: 0.7194s/iter; left time: 17560.9818s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0566285\n",
      "\tspeed: 0.7187s/iter; left time: 17472.7569s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0661598\n",
      "\tspeed: 0.7174s/iter; left time: 17369.9587s\n",
      "Epoch: 11 cost time: 00h:25m:03.91s\n",
      "Epoch: 11 | Train Loss: 0.0677539 Vali Loss: 0.0579457 Test Loss: 0.0615977\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0564539\n",
      "\tspeed: 7.2099s/iter; left time: 173253.7892s\n",
      "\titers: 200, epoch: 12 | loss: 0.0552143\n",
      "\tspeed: 0.7149s/iter; left time: 17106.8491s\n",
      "\titers: 300, epoch: 12 | loss: 0.0666377\n",
      "\tspeed: 0.6841s/iter; left time: 16301.1134s\n",
      "\titers: 400, epoch: 12 | loss: 0.0854366\n",
      "\tspeed: 0.7133s/iter; left time: 16927.5870s\n",
      "\titers: 500, epoch: 12 | loss: 0.0836129\n",
      "\tspeed: 0.7165s/iter; left time: 16931.6899s\n",
      "\titers: 600, epoch: 12 | loss: 0.0733599\n",
      "\tspeed: 0.7167s/iter; left time: 16864.1872s\n",
      "\titers: 700, epoch: 12 | loss: 0.0519824\n",
      "\tspeed: 0.7169s/iter; left time: 16797.1314s\n",
      "\titers: 800, epoch: 12 | loss: 0.0707659\n",
      "\tspeed: 0.7130s/iter; left time: 16634.9932s\n",
      "\titers: 900, epoch: 12 | loss: 0.0599417\n",
      "\tspeed: 0.7100s/iter; left time: 16492.8168s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0641551\n",
      "\tspeed: 0.6997s/iter; left time: 16183.0197s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0645704\n",
      "\tspeed: 0.7038s/iter; left time: 16208.1463s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0506950\n",
      "\tspeed: 0.6999s/iter; left time: 16047.7391s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0640931\n",
      "\tspeed: 0.6931s/iter; left time: 15824.5372s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0689833\n",
      "\tspeed: 0.6828s/iter; left time: 15520.7535s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0576060\n",
      "\tspeed: 0.6710s/iter; left time: 15183.7809s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0531667\n",
      "\tspeed: 0.6260s/iter; left time: 14103.6352s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0634687\n",
      "\tspeed: 0.5965s/iter; left time: 13378.4142s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0635579\n",
      "\tspeed: 0.5781s/iter; left time: 12908.9276s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0655776\n",
      "\tspeed: 0.5432s/iter; left time: 12075.9487s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0597161\n",
      "\tspeed: 0.4989s/iter; left time: 11039.9416s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0718565\n",
      "\tspeed: 0.5966s/iter; left time: 13142.0499s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0863532\n",
      "\tspeed: 0.5659s/iter; left time: 12410.9633s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0655009\n",
      "\tspeed: 0.4799s/iter; left time: 10477.0044s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0780334\n",
      "\tspeed: 0.4327s/iter; left time: 9403.1831s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685403\n",
      "\tspeed: 0.3394s/iter; left time: 7341.8798s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0669672\n",
      "\tspeed: 0.2542s/iter; left time: 5472.8932s\n",
      "Epoch: 12 cost time: 00h:27m:07.42s\n",
      "Epoch: 12 | Train Loss: 0.0672516 Vali Loss: 0.0583924 Test Loss: 0.0623229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0646751\n",
      "\tspeed: 6.3369s/iter; left time: 135287.3221s\n",
      "\titers: 200, epoch: 13 | loss: 0.0718063\n",
      "\tspeed: 0.7220s/iter; left time: 15341.9971s\n",
      "\titers: 300, epoch: 13 | loss: 0.0704412\n",
      "\tspeed: 0.7247s/iter; left time: 15326.9095s\n",
      "\titers: 400, epoch: 13 | loss: 0.0566958\n",
      "\tspeed: 0.7236s/iter; left time: 15231.4184s\n",
      "\titers: 500, epoch: 13 | loss: 0.0677867\n",
      "\tspeed: 0.7212s/iter; left time: 15108.6916s\n",
      "\titers: 600, epoch: 13 | loss: 0.0662406\n",
      "\tspeed: 0.7077s/iter; left time: 14755.7144s\n",
      "\titers: 700, epoch: 13 | loss: 0.0692414\n",
      "\tspeed: 0.7179s/iter; left time: 14895.9437s\n",
      "\titers: 800, epoch: 13 | loss: 0.0744722\n",
      "\tspeed: 0.7169s/iter; left time: 14804.1117s\n",
      "\titers: 900, epoch: 13 | loss: 0.0682855\n",
      "\tspeed: 0.7169s/iter; left time: 14732.3113s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0692520\n",
      "\tspeed: 0.7171s/iter; left time: 14664.0092s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0754888\n",
      "\tspeed: 0.7191s/iter; left time: 14633.0825s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0644453\n",
      "\tspeed: 0.7198s/iter; left time: 14575.8044s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0570012\n",
      "\tspeed: 0.7231s/iter; left time: 14568.8106s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0658361\n",
      "\tspeed: 0.7206s/iter; left time: 14448.0344s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0681140\n",
      "\tspeed: 0.7030s/iter; left time: 14024.8246s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0861393\n",
      "\tspeed: 0.7192s/iter; left time: 14275.1996s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0685929\n",
      "\tspeed: 0.7161s/iter; left time: 14142.3087s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0834594\n",
      "\tspeed: 0.7192s/iter; left time: 14130.5993s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0550227\n",
      "\tspeed: 0.7182s/iter; left time: 14040.1654s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0614033\n",
      "\tspeed: 0.7166s/iter; left time: 13936.8675s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0588906\n",
      "\tspeed: 0.7071s/iter; left time: 13681.3902s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0599210\n",
      "\tspeed: 0.7207s/iter; left time: 13872.2947s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0557182\n",
      "\tspeed: 0.7178s/iter; left time: 13745.6032s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0523902\n",
      "\tspeed: 0.7217s/iter; left time: 13747.9558s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0614735\n",
      "\tspeed: 0.7156s/iter; left time: 13559.7871s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0746370\n",
      "\tspeed: 0.7206s/iter; left time: 13582.5080s\n",
      "Epoch: 13 cost time: 00h:32m:05.29s\n",
      "Epoch: 13 | Train Loss: 0.0666866 Vali Loss: 0.0598697 Test Loss: 0.0634292\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0550120\n",
      "\tspeed: 7.0686s/iter; left time: 131956.4853s\n",
      "\titers: 200, epoch: 14 | loss: 0.0765724\n",
      "\tspeed: 0.6410s/iter; left time: 11901.8081s\n",
      "\titers: 300, epoch: 14 | loss: 0.0594586\n",
      "\tspeed: 0.6166s/iter; left time: 11387.2843s\n",
      "\titers: 400, epoch: 14 | loss: 0.0718415\n",
      "\tspeed: 0.5521s/iter; left time: 10140.8036s\n",
      "\titers: 500, epoch: 14 | loss: 0.0611447\n",
      "\tspeed: 0.5369s/iter; left time: 9807.6063s\n",
      "\titers: 600, epoch: 14 | loss: 0.0523001\n",
      "\tspeed: 0.6810s/iter; left time: 12372.1778s\n",
      "\titers: 700, epoch: 14 | loss: 0.0722972\n",
      "\tspeed: 0.6754s/iter; left time: 12202.6962s\n",
      "\titers: 800, epoch: 14 | loss: 0.0649306\n",
      "\tspeed: 0.6362s/iter; left time: 11431.3179s\n",
      "\titers: 900, epoch: 14 | loss: 0.0694602\n",
      "\tspeed: 0.6536s/iter; left time: 11679.2421s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0615104\n",
      "\tspeed: 0.6041s/iter; left time: 10733.7640s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0770956\n",
      "\tspeed: 0.4852s/iter; left time: 8572.9875s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0567148\n",
      "\tspeed: 0.3995s/iter; left time: 7018.4319s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0728452\n",
      "\tspeed: 0.2530s/iter; left time: 4419.0859s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0593382\n",
      "\tspeed: 0.2210s/iter; left time: 3838.3570s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0824114\n",
      "\tspeed: 0.2521s/iter; left time: 4353.2924s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0644274\n",
      "\tspeed: 0.3984s/iter; left time: 6839.0360s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0727649\n",
      "\tspeed: 0.7231s/iter; left time: 12342.3832s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0557669\n",
      "\tspeed: 0.7232s/iter; left time: 12271.9861s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0580711\n",
      "\tspeed: 0.7225s/iter; left time: 12186.7875s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0541300\n",
      "\tspeed: 0.7227s/iter; left time: 12118.0692s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0559602\n",
      "\tspeed: 0.7214s/iter; left time: 12024.3307s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0641657\n",
      "\tspeed: 0.7229s/iter; left time: 11977.5108s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0765522\n",
      "\tspeed: 0.7225s/iter; left time: 11898.6824s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0656291\n",
      "\tspeed: 0.7235s/iter; left time: 11841.4388s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0698191\n",
      "\tspeed: 0.6963s/iter; left time: 11326.6704s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0704088\n",
      "\tspeed: 0.7223s/iter; left time: 11678.8585s\n",
      "Epoch: 14 cost time: 00h:26m:47.93s\n",
      "Epoch: 14 | Train Loss: 0.0661358 Vali Loss: 0.0595082 Test Loss: 0.0636313\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010770452208817005, rmse:0.10378079116344452, mae:0.06202606484293938, rse:0.39211392402648926\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 24: 05h:40m:08.68s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Capture and log output in real-time\u001b[39;00m\n\u001b[1;32m     55\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 56\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>0.1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.2049</td>\n",
       "      <td>0.1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1445</td>\n",
       "      <td>0.0956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.0968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.0817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0227  0.1508  0.0954\n",
       "        96        0.0358  0.1892  0.1282\n",
       "        168       0.0377  0.1941  0.1336\n",
       "GB      24        0.0256  0.1599  0.1040\n",
       "        96        0.0420  0.2049  0.1405\n",
       "        168       0.0428  0.2068  0.1438\n",
       "ES      24        0.0107  0.1033  0.0665\n",
       "        96        0.0209  0.1445  0.0956\n",
       "        168       0.0211  0.1454  0.0968\n",
       "FR      24        0.0111  0.1052  0.0600\n",
       "        96        0.0185  0.1359  0.0817\n",
       "        168       0.0204  0.1428  0.0872\n",
       "IT      24        0.0108  0.1038  0.0620\n",
       "        96        0.0198  0.1406  0.0868\n",
       "        168       0.0198  0.1406  0.0889"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm.csv'))\n",
    "timellm_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
