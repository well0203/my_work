{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"2\"\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "seq_len = 512\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-10-31 18:51:32,563] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 18:51:33,723] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 18:51:33,723] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 18:51:33,723] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 18:51:33,820] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 18:51:33,820] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 18:51:34,483] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 18:51:34,485] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 18:51:34,485] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 18:51:34,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 18:51:34,487] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 18:51:34,487] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 18:51:34,487] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 18:51:34,487] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 18:51:34,487] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 18:51:34,487] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 18:51:34,818] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 18:51:34,819] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 18:51:34,819] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 189.54 GB, percent = 25.1%\n",
      "[2024-10-31 18:51:34,942] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 18:51:34,943] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 18:51:34,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 189.54 GB, percent = 25.1%\n",
      "[2024-10-31 18:51:34,943] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 18:51:35,065] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 18:51:35,066] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 18:51:35,066] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 189.55 GB, percent = 25.1%\n",
      "[2024-10-31 18:51:35,066] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 18:51:35,067] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 18:51:35,067] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 18:51:35,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 18:51:35,067] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1ea4446f50>\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 18:51:35,068] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 18:51:35,069] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 18:51:35,070] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1688982\n",
      "\tspeed: 0.1791s/iter; left time: 15989.3211s\n",
      "\titers: 200, epoch: 1 | loss: 0.1398762\n",
      "\tspeed: 0.1294s/iter; left time: 11539.6838s\n",
      "\titers: 300, epoch: 1 | loss: 0.1594033\n",
      "\tspeed: 0.1311s/iter; left time: 11677.8295s\n",
      "\titers: 400, epoch: 1 | loss: 0.1548022\n",
      "\tspeed: 0.1310s/iter; left time: 11656.5472s\n",
      "\titers: 500, epoch: 1 | loss: 0.1380969\n",
      "\tspeed: 0.1315s/iter; left time: 11684.6241s\n",
      "\titers: 600, epoch: 1 | loss: 0.1172558\n",
      "\tspeed: 0.1323s/iter; left time: 11739.0538s\n",
      "\titers: 700, epoch: 1 | loss: 0.1290968\n",
      "\tspeed: 0.1308s/iter; left time: 11594.0061s\n",
      "\titers: 800, epoch: 1 | loss: 0.1323915\n",
      "\tspeed: 0.1329s/iter; left time: 11770.4203s\n",
      "\titers: 900, epoch: 1 | loss: 0.0968730\n",
      "\tspeed: 0.1277s/iter; left time: 11300.4754s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1262417\n",
      "\tspeed: 0.1316s/iter; left time: 11632.1686s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1081160\n",
      "\tspeed: 0.1356s/iter; left time: 11970.7201s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1055638\n",
      "\tspeed: 0.1331s/iter; left time: 11730.6617s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0805435\n",
      "\tspeed: 0.1369s/iter; left time: 12054.2612s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1116071\n",
      "\tspeed: 0.1331s/iter; left time: 11708.0232s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0692826\n",
      "\tspeed: 0.1312s/iter; left time: 11525.5610s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0953681\n",
      "\tspeed: 0.1314s/iter; left time: 11534.2266s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1187560\n",
      "\tspeed: 0.1339s/iter; left time: 11734.4818s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0991687\n",
      "\tspeed: 0.1323s/iter; left time: 11580.5760s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1227836\n",
      "\tspeed: 0.1332s/iter; left time: 11649.9697s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0913499\n",
      "\tspeed: 0.1337s/iter; left time: 11679.2783s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0831885\n",
      "\tspeed: 0.1334s/iter; left time: 11637.0635s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1006144\n",
      "\tspeed: 0.1364s/iter; left time: 11886.0099s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1002881\n",
      "\tspeed: 0.1335s/iter; left time: 11620.8915s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0858188\n",
      "\tspeed: 0.1340s/iter; left time: 11654.4340s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0878481\n",
      "\tspeed: 0.1335s/iter; left time: 11597.6234s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0938092\n",
      "\tspeed: 0.1338s/iter; left time: 11606.9596s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1014624\n",
      "\tspeed: 0.1318s/iter; left time: 11421.9672s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0828544\n",
      "\tspeed: 0.1350s/iter; left time: 11683.1755s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0846184\n",
      "\tspeed: 0.1345s/iter; left time: 11625.8710s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0732674\n",
      "\tspeed: 0.1340s/iter; left time: 11576.0296s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0863261\n",
      "\tspeed: 0.1352s/iter; left time: 11664.6405s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1145399\n",
      "\tspeed: 0.1356s/iter; left time: 11679.6337s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0950736\n",
      "\tspeed: 0.1355s/iter; left time: 11657.5350s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1013337\n",
      "\tspeed: 0.1339s/iter; left time: 11513.0333s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0962714\n",
      "\tspeed: 0.1331s/iter; left time: 11427.8403s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045466\n",
      "\tspeed: 0.1349s/iter; left time: 11566.6417s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0921188\n",
      "\tspeed: 0.1335s/iter; left time: 11433.4074s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0915671\n",
      "\tspeed: 0.1355s/iter; left time: 11592.4753s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1059835\n",
      "\tspeed: 0.1325s/iter; left time: 11325.1041s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0626834\n",
      "\tspeed: 0.1316s/iter; left time: 11231.4829s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0807726\n",
      "\tspeed: 0.1325s/iter; left time: 11297.5527s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1075011\n",
      "\tspeed: 0.1329s/iter; left time: 11314.6568s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0953812\n",
      "\tspeed: 0.1341s/iter; left time: 11408.2906s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0955328\n",
      "\tspeed: 0.1288s/iter; left time: 10939.1918s\n",
      "Epoch: 1 cost time: 00h:09m:55.39s\n",
      "Epoch: 1 | Train Loss: 0.1053388 Vali Loss: 0.0971123 Test Loss: 0.0994308\n",
      "Validation loss decreased (inf --> 0.097112).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0704475\n",
      "\tspeed: 1.8558s/iter; left time: 157362.1844s\n",
      "\titers: 200, epoch: 2 | loss: 0.0855533\n",
      "\tspeed: 0.1220s/iter; left time: 10333.3626s\n",
      "\titers: 300, epoch: 2 | loss: 0.0774946\n",
      "\tspeed: 0.1217s/iter; left time: 10297.2518s\n",
      "\titers: 400, epoch: 2 | loss: 0.0883429\n",
      "\tspeed: 0.1213s/iter; left time: 10250.4496s\n",
      "\titers: 500, epoch: 2 | loss: 0.0867326\n",
      "\tspeed: 0.1210s/iter; left time: 10214.0189s\n",
      "\titers: 600, epoch: 2 | loss: 0.0939545\n",
      "\tspeed: 0.1217s/iter; left time: 10255.0841s\n",
      "\titers: 700, epoch: 2 | loss: 0.0739971\n",
      "\tspeed: 0.1213s/iter; left time: 10214.2973s\n",
      "\titers: 800, epoch: 2 | loss: 0.0977335\n",
      "\tspeed: 0.1212s/iter; left time: 10195.2114s\n",
      "\titers: 900, epoch: 2 | loss: 0.0857365\n",
      "\tspeed: 0.1200s/iter; left time: 10078.9321s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0865258\n",
      "\tspeed: 0.1203s/iter; left time: 10092.8555s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0972862\n",
      "\tspeed: 0.1210s/iter; left time: 10141.7496s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0832878\n",
      "\tspeed: 0.1161s/iter; left time: 9713.4101s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0947991\n",
      "\tspeed: 0.1188s/iter; left time: 9932.6039s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0940965\n",
      "\tspeed: 0.1221s/iter; left time: 10194.7167s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0825809\n",
      "\tspeed: 0.1215s/iter; left time: 10131.3893s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0861147\n",
      "\tspeed: 0.1211s/iter; left time: 10086.6144s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1065482\n",
      "\tspeed: 0.1218s/iter; left time: 10135.6613s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0846911\n",
      "\tspeed: 0.1217s/iter; left time: 10114.7992s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1024258\n",
      "\tspeed: 0.1209s/iter; left time: 10031.5488s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0896808\n",
      "\tspeed: 0.1142s/iter; left time: 9465.1164s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1056611\n",
      "\tspeed: 0.1147s/iter; left time: 9493.9302s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0911830\n",
      "\tspeed: 0.1199s/iter; left time: 9918.0093s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0980409\n",
      "\tspeed: 0.1217s/iter; left time: 10054.1998s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0799288\n",
      "\tspeed: 0.1208s/iter; left time: 9965.6891s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0872002\n",
      "\tspeed: 0.1214s/iter; left time: 10005.7893s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0915525\n",
      "\tspeed: 0.1214s/iter; left time: 9992.6488s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1013657\n",
      "\tspeed: 0.1204s/iter; left time: 9893.4991s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0794303\n",
      "\tspeed: 0.1214s/iter; left time: 9969.3309s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0900755\n",
      "\tspeed: 0.1211s/iter; left time: 9931.5008s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0776787\n",
      "\tspeed: 0.1201s/iter; left time: 9834.1956s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1028968\n",
      "\tspeed: 0.1210s/iter; left time: 9894.1738s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0771540\n",
      "\tspeed: 0.1213s/iter; left time: 9907.8363s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0871339\n",
      "\tspeed: 0.1201s/iter; left time: 9797.4496s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1009780\n",
      "\tspeed: 0.1191s/iter; left time: 9704.0939s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0891745\n",
      "\tspeed: 0.1209s/iter; left time: 9843.4291s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0985909\n",
      "\tspeed: 0.1209s/iter; left time: 9828.8051s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1067271\n",
      "\tspeed: 0.1216s/iter; left time: 9872.4539s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0852604\n",
      "\tspeed: 0.1212s/iter; left time: 9831.6632s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0817025\n",
      "\tspeed: 0.1201s/iter; left time: 9726.3672s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0846763\n",
      "\tspeed: 0.1180s/iter; left time: 9546.4893s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0871173\n",
      "\tspeed: 0.1224s/iter; left time: 9890.0109s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0959664\n",
      "\tspeed: 0.1205s/iter; left time: 9726.6420s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0784686\n",
      "\tspeed: 0.1211s/iter; left time: 9758.9479s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0950680\n",
      "\tspeed: 0.1192s/iter; left time: 9592.5268s\n",
      "Epoch: 2 cost time: 00h:08m:57.87s\n",
      "Epoch: 2 | Train Loss: 0.0889246 Vali Loss: 0.0918921 Test Loss: 0.0945349\n",
      "Validation loss decreased (0.097112 --> 0.091892).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1014844\n",
      "\tspeed: 1.5141s/iter; left time: 121619.0808s\n",
      "\titers: 200, epoch: 3 | loss: 0.0968780\n",
      "\tspeed: 0.1158s/iter; left time: 9289.6890s\n",
      "\titers: 300, epoch: 3 | loss: 0.0694327\n",
      "\tspeed: 0.1210s/iter; left time: 9692.1908s\n",
      "\titers: 400, epoch: 3 | loss: 0.0818534\n",
      "\tspeed: 0.1156s/iter; left time: 9247.1254s\n",
      "\titers: 500, epoch: 3 | loss: 0.1179019\n",
      "\tspeed: 0.1204s/iter; left time: 9623.2367s\n",
      "\titers: 600, epoch: 3 | loss: 0.0902409\n",
      "\tspeed: 0.1103s/iter; left time: 8803.9714s\n",
      "\titers: 700, epoch: 3 | loss: 0.0945199\n",
      "\tspeed: 0.1035s/iter; left time: 8248.9447s\n",
      "\titers: 800, epoch: 3 | loss: 0.0943223\n",
      "\tspeed: 0.1221s/iter; left time: 9718.7403s\n",
      "\titers: 900, epoch: 3 | loss: 0.0608129\n",
      "\tspeed: 0.1214s/iter; left time: 9657.6133s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0815312\n",
      "\tspeed: 0.1182s/iter; left time: 9385.8022s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0903181\n",
      "\tspeed: 0.1226s/iter; left time: 9724.1524s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0806858\n",
      "\tspeed: 0.1208s/iter; left time: 9574.0678s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0762755\n",
      "\tspeed: 0.1218s/iter; left time: 9639.2127s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0923751\n",
      "\tspeed: 0.1191s/iter; left time: 9412.3550s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0904863\n",
      "\tspeed: 0.1223s/iter; left time: 9650.4761s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0592667\n",
      "\tspeed: 0.1207s/iter; left time: 9516.3772s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0708085\n",
      "\tspeed: 0.1211s/iter; left time: 9532.3051s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0865551\n",
      "\tspeed: 0.1149s/iter; left time: 9032.1334s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0666124\n",
      "\tspeed: 0.1169s/iter; left time: 9183.4101s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0717531\n",
      "\tspeed: 0.1183s/iter; left time: 9278.7156s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1037164\n",
      "\tspeed: 0.1188s/iter; left time: 9307.1539s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0819335\n",
      "\tspeed: 0.1215s/iter; left time: 9506.6379s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0837101\n",
      "\tspeed: 0.1161s/iter; left time: 9072.8121s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0792184\n",
      "\tspeed: 0.1213s/iter; left time: 9461.5369s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0968816\n",
      "\tspeed: 0.1127s/iter; left time: 8785.1067s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0933213\n",
      "\tspeed: 0.1143s/iter; left time: 8891.6723s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0743180\n",
      "\tspeed: 0.1117s/iter; left time: 8679.7168s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0937308\n",
      "\tspeed: 0.1136s/iter; left time: 8819.0185s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0838305\n",
      "\tspeed: 0.1198s/iter; left time: 9287.5529s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0647273\n",
      "\tspeed: 0.1210s/iter; left time: 9367.6214s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0913395\n",
      "\tspeed: 0.1199s/iter; left time: 9272.2949s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0819122\n",
      "\tspeed: 0.1212s/iter; left time: 9360.1298s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0788185\n",
      "\tspeed: 0.1197s/iter; left time: 9234.5851s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0784381\n",
      "\tspeed: 0.1210s/iter; left time: 9321.6544s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0678186\n",
      "\tspeed: 0.1212s/iter; left time: 9321.3068s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0698414\n",
      "\tspeed: 0.1212s/iter; left time: 9314.4010s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0900844\n",
      "\tspeed: 0.1216s/iter; left time: 9332.3040s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0756922\n",
      "\tspeed: 0.1220s/iter; left time: 9349.3109s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0685307\n",
      "\tspeed: 0.1207s/iter; left time: 9237.0347s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0735308\n",
      "\tspeed: 0.1149s/iter; left time: 8783.5225s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0800254\n",
      "\tspeed: 0.1215s/iter; left time: 9274.1045s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0855180\n",
      "\tspeed: 0.1197s/iter; left time: 9127.2369s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0667157\n",
      "\tspeed: 0.1188s/iter; left time: 9043.9897s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0843554\n",
      "\tspeed: 0.1210s/iter; left time: 9199.0788s\n",
      "Epoch: 3 cost time: 00h:08m:51.03s\n",
      "Epoch: 3 | Train Loss: 0.0853331 Vali Loss: 0.0911640 Test Loss: 0.0942841\n",
      "Validation loss decreased (0.091892 --> 0.091164).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0739329\n",
      "\tspeed: 1.5751s/iter; left time: 119485.9570s\n",
      "\titers: 200, epoch: 4 | loss: 0.1006263\n",
      "\tspeed: 0.1209s/iter; left time: 9162.6537s\n",
      "\titers: 300, epoch: 4 | loss: 0.0738416\n",
      "\tspeed: 0.1219s/iter; left time: 9221.1973s\n",
      "\titers: 400, epoch: 4 | loss: 0.0781351\n",
      "\tspeed: 0.1209s/iter; left time: 9134.3896s\n",
      "\titers: 500, epoch: 4 | loss: 0.0675929\n",
      "\tspeed: 0.1616s/iter; left time: 12192.9210s\n",
      "\titers: 600, epoch: 4 | loss: 0.0722866\n",
      "\tspeed: 0.1212s/iter; left time: 9137.0161s\n",
      "\titers: 700, epoch: 4 | loss: 0.0852794\n",
      "\tspeed: 0.1158s/iter; left time: 8716.6295s\n",
      "\titers: 800, epoch: 4 | loss: 0.0830615\n",
      "\tspeed: 0.1210s/iter; left time: 9092.4862s\n",
      "\titers: 900, epoch: 4 | loss: 0.0746369\n",
      "\tspeed: 0.1613s/iter; left time: 12107.7911s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0714534\n",
      "\tspeed: 0.1234s/iter; left time: 9251.8166s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0721987\n",
      "\tspeed: 0.1207s/iter; left time: 9035.8895s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0848372\n",
      "\tspeed: 0.1205s/iter; left time: 9010.3220s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0901184\n",
      "\tspeed: 0.1573s/iter; left time: 11743.9495s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0873635\n",
      "\tspeed: 0.1285s/iter; left time: 9583.9484s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0672005\n",
      "\tspeed: 0.1224s/iter; left time: 9114.7411s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0815371\n",
      "\tspeed: 0.1203s/iter; left time: 8943.0981s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0967667\n",
      "\tspeed: 0.1208s/iter; left time: 8973.0430s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1003437\n",
      "\tspeed: 0.1211s/iter; left time: 8980.7266s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0772678\n",
      "\tspeed: 0.1542s/iter; left time: 11423.2844s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1023371\n",
      "\tspeed: 0.1282s/iter; left time: 9481.5764s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0959407\n",
      "\tspeed: 0.1217s/iter; left time: 8984.7863s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0707141\n",
      "\tspeed: 0.1211s/iter; left time: 8928.6897s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1181110\n",
      "\tspeed: 0.1264s/iter; left time: 9308.8616s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0988108\n",
      "\tspeed: 0.1570s/iter; left time: 11546.0326s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1022194\n",
      "\tspeed: 0.1148s/iter; left time: 8433.7342s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0755848\n",
      "\tspeed: 0.1217s/iter; left time: 8929.4115s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0852136\n",
      "\tspeed: 0.1394s/iter; left time: 10214.4611s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1073938\n",
      "\tspeed: 0.1217s/iter; left time: 8905.8013s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0776819\n",
      "\tspeed: 0.1183s/iter; left time: 8640.5381s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0915424\n",
      "\tspeed: 0.1195s/iter; left time: 8721.6179s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0914509\n",
      "\tspeed: 0.1557s/iter; left time: 11345.9065s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0769955\n",
      "\tspeed: 0.1204s/iter; left time: 8761.4653s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0912195\n",
      "\tspeed: 0.1214s/iter; left time: 8822.1983s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0864609\n",
      "\tspeed: 0.1212s/iter; left time: 8793.5615s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0855569\n",
      "\tspeed: 0.1217s/iter; left time: 8819.5937s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0880700\n",
      "\tspeed: 0.1445s/iter; left time: 10453.5241s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0775939\n",
      "\tspeed: 0.1287s/iter; left time: 9298.6587s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0855137\n",
      "\tspeed: 0.1210s/iter; left time: 8732.5153s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0877626\n",
      "\tspeed: 0.1311s/iter; left time: 9447.9111s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0802846\n",
      "\tspeed: 0.1531s/iter; left time: 11019.7332s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0870235\n",
      "\tspeed: 0.1202s/iter; left time: 8638.0521s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0716099\n",
      "\tspeed: 0.1345s/iter; left time: 9651.9577s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0780072\n",
      "\tspeed: 0.1451s/iter; left time: 10396.8526s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0948033\n",
      "\tspeed: 0.1208s/iter; left time: 8646.7113s\n",
      "Epoch: 4 cost time: 00h:09m:41.36s\n",
      "Epoch: 4 | Train Loss: 0.0836256 Vali Loss: 0.0897763 Test Loss: 0.0933365\n",
      "Validation loss decreased (0.091164 --> 0.089776).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0797964\n",
      "\tspeed: 1.6825s/iter; left time: 120113.0804s\n",
      "\titers: 200, epoch: 5 | loss: 0.0838478\n",
      "\tspeed: 0.1207s/iter; left time: 8605.6644s\n",
      "\titers: 300, epoch: 5 | loss: 0.0742453\n",
      "\tspeed: 0.1548s/iter; left time: 11019.3590s\n",
      "\titers: 400, epoch: 5 | loss: 0.0888728\n",
      "\tspeed: 0.1216s/iter; left time: 8646.1914s\n",
      "\titers: 500, epoch: 5 | loss: 0.0898506\n",
      "\tspeed: 0.1207s/iter; left time: 8569.6621s\n",
      "\titers: 600, epoch: 5 | loss: 0.0773116\n",
      "\tspeed: 0.1457s/iter; left time: 10326.1741s\n",
      "\titers: 700, epoch: 5 | loss: 0.0626944\n",
      "\tspeed: 0.1284s/iter; left time: 9086.7536s\n",
      "\titers: 800, epoch: 5 | loss: 0.0781008\n",
      "\tspeed: 0.1147s/iter; left time: 8110.4766s\n",
      "\titers: 900, epoch: 5 | loss: 0.0825634\n",
      "\tspeed: 0.1206s/iter; left time: 8514.5172s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0848518\n",
      "\tspeed: 0.1509s/iter; left time: 10635.2940s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0829829\n",
      "\tspeed: 0.1326s/iter; left time: 9335.2717s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0997171\n",
      "\tspeed: 0.1181s/iter; left time: 8299.2913s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0798143\n",
      "\tspeed: 0.1196s/iter; left time: 8395.6304s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0764720\n",
      "\tspeed: 0.1530s/iter; left time: 10727.0176s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0732888\n",
      "\tspeed: 0.1260s/iter; left time: 8821.3736s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0836849\n",
      "\tspeed: 0.1204s/iter; left time: 8415.5389s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0724503\n",
      "\tspeed: 0.1406s/iter; left time: 9809.6076s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0894237\n",
      "\tspeed: 0.1403s/iter; left time: 9779.9427s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0685364\n",
      "\tspeed: 0.1195s/iter; left time: 8316.1751s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0697801\n",
      "\tspeed: 0.1171s/iter; left time: 8135.1213s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0814900\n",
      "\tspeed: 0.1078s/iter; left time: 7481.2610s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0934933\n",
      "\tspeed: 0.1019s/iter; left time: 7062.4884s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861936\n",
      "\tspeed: 0.1025s/iter; left time: 7088.5370s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0938417\n",
      "\tspeed: 0.1554s/iter; left time: 10735.7135s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0799977\n",
      "\tspeed: 0.1214s/iter; left time: 8373.0285s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0855416\n",
      "\tspeed: 0.1211s/iter; left time: 8341.9388s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0920220\n",
      "\tspeed: 0.1405s/iter; left time: 9663.2917s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0856894\n",
      "\tspeed: 0.1414s/iter; left time: 9714.7333s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0774287\n",
      "\tspeed: 0.1193s/iter; left time: 8185.5089s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0683412\n",
      "\tspeed: 0.1201s/iter; left time: 8226.2273s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0973268\n",
      "\tspeed: 0.1302s/iter; left time: 8900.9362s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0831058\n",
      "\tspeed: 0.1522s/iter; left time: 10392.6295s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0682414\n",
      "\tspeed: 0.1210s/iter; left time: 8250.6529s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0845700\n",
      "\tspeed: 0.1202s/iter; left time: 8186.1621s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0771565\n",
      "\tspeed: 0.1234s/iter; left time: 8387.2050s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0795133\n",
      "\tspeed: 0.1561s/iter; left time: 10595.3193s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0733312\n",
      "\tspeed: 0.1199s/iter; left time: 8130.2394s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0908493\n",
      "\tspeed: 0.1191s/iter; left time: 8063.4771s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0670330\n",
      "\tspeed: 0.1192s/iter; left time: 8059.2152s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0631396\n",
      "\tspeed: 0.1589s/iter; left time: 10721.2013s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0828409\n",
      "\tspeed: 0.1196s/iter; left time: 8061.6134s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0800139\n",
      "\tspeed: 0.1210s/iter; left time: 8140.4813s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0884054\n",
      "\tspeed: 0.1562s/iter; left time: 10492.0341s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0831661\n",
      "\tspeed: 0.1185s/iter; left time: 7947.1889s\n",
      "Epoch: 5 cost time: 00h:09m:33.96s\n",
      "Epoch: 5 | Train Loss: 0.0823728 Vali Loss: 0.0899557 Test Loss: 0.0942943\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1001324\n",
      "\tspeed: 1.6708s/iter; left time: 111810.1650s\n",
      "\titers: 200, epoch: 6 | loss: 0.0886922\n",
      "\tspeed: 0.1318s/iter; left time: 8803.8995s\n",
      "\titers: 300, epoch: 6 | loss: 0.0843214\n",
      "\tspeed: 0.1528s/iter; left time: 10197.6064s\n",
      "\titers: 400, epoch: 6 | loss: 0.0734667\n",
      "\tspeed: 0.1210s/iter; left time: 8060.7556s\n",
      "\titers: 500, epoch: 6 | loss: 0.0924280\n",
      "\tspeed: 0.1455s/iter; left time: 9680.0229s\n",
      "\titers: 600, epoch: 6 | loss: 0.0591974\n",
      "\tspeed: 0.1367s/iter; left time: 9079.1461s\n",
      "\titers: 700, epoch: 6 | loss: 0.0633274\n",
      "\tspeed: 0.1209s/iter; left time: 8018.2186s\n",
      "\titers: 800, epoch: 6 | loss: 0.0738303\n",
      "\tspeed: 0.1202s/iter; left time: 7959.3180s\n",
      "\titers: 900, epoch: 6 | loss: 0.1071520\n",
      "\tspeed: 0.1158s/iter; left time: 7656.4232s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0735545\n",
      "\tspeed: 0.1233s/iter; left time: 8140.8660s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0729964\n",
      "\tspeed: 0.1556s/iter; left time: 10255.7375s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0962333\n",
      "\tspeed: 0.1186s/iter; left time: 7806.7293s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0923874\n",
      "\tspeed: 0.1213s/iter; left time: 7969.9441s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0971264\n",
      "\tspeed: 0.1160s/iter; left time: 7614.2612s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0751228\n",
      "\tspeed: 0.1507s/iter; left time: 9875.4807s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0779485\n",
      "\tspeed: 0.1091s/iter; left time: 7135.3030s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0823796\n",
      "\tspeed: 0.1208s/iter; left time: 7892.8797s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0782527\n",
      "\tspeed: 0.1473s/iter; left time: 9604.4940s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0854593\n",
      "\tspeed: 0.1331s/iter; left time: 8666.9516s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0940582\n",
      "\tspeed: 0.1176s/iter; left time: 7644.1771s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0806722\n",
      "\tspeed: 0.1544s/iter; left time: 10026.9168s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0820966\n",
      "\tspeed: 0.1203s/iter; left time: 7796.4247s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0738427\n",
      "\tspeed: 0.1197s/iter; left time: 7747.7731s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0772219\n",
      "\tspeed: 0.1564s/iter; left time: 10104.9567s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0898360\n",
      "\tspeed: 0.1205s/iter; left time: 7774.9168s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0820411\n",
      "\tspeed: 0.1266s/iter; left time: 8154.3153s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0796860\n",
      "\tspeed: 0.1542s/iter; left time: 9920.2409s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0989802\n",
      "\tspeed: 0.1162s/iter; left time: 7460.3102s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0941869\n",
      "\tspeed: 0.1145s/iter; left time: 7343.4061s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0823741\n",
      "\tspeed: 0.1236s/iter; left time: 7915.9428s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0726927\n",
      "\tspeed: 0.1581s/iter; left time: 10105.1874s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0867132\n",
      "\tspeed: 0.1213s/iter; left time: 7743.3463s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0877910\n",
      "\tspeed: 0.1177s/iter; left time: 7498.0555s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1008532\n",
      "\tspeed: 0.1493s/iter; left time: 9499.9750s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0882527\n",
      "\tspeed: 0.1319s/iter; left time: 8376.0138s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0798398\n",
      "\tspeed: 0.1200s/iter; left time: 7610.1461s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0708239\n",
      "\tspeed: 0.1251s/iter; left time: 7920.3229s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0720952\n",
      "\tspeed: 0.1517s/iter; left time: 9590.4479s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0832081\n",
      "\tspeed: 0.1141s/iter; left time: 7201.5126s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0772891\n",
      "\tspeed: 0.1192s/iter; left time: 7513.3598s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0615834\n",
      "\tspeed: 0.1415s/iter; left time: 8903.2743s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0647544\n",
      "\tspeed: 0.1340s/iter; left time: 8415.5606s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0650132\n",
      "\tspeed: 0.1066s/iter; left time: 6687.0348s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0766972\n",
      "\tspeed: 0.1067s/iter; left time: 6681.0852s\n",
      "Epoch: 6 cost time: 00h:09m:37.26s\n",
      "Epoch: 6 | Train Loss: 0.0813646 Vali Loss: 0.0895673 Test Loss: 0.0940461\n",
      "Validation loss decreased (0.089776 --> 0.089567).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0841343\n",
      "\tspeed: 1.6971s/iter; left time: 105990.2438s\n",
      "\titers: 200, epoch: 7 | loss: 0.0774401\n",
      "\tspeed: 0.1453s/iter; left time: 9059.3249s\n",
      "\titers: 300, epoch: 7 | loss: 0.0646475\n",
      "\tspeed: 0.1302s/iter; left time: 8104.1409s\n",
      "\titers: 400, epoch: 7 | loss: 0.0766103\n",
      "\tspeed: 0.1208s/iter; left time: 7509.1848s\n",
      "\titers: 500, epoch: 7 | loss: 0.0919630\n",
      "\tspeed: 0.1410s/iter; left time: 8748.9815s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806891\n",
      "\tspeed: 0.1334s/iter; left time: 8263.3842s\n",
      "\titers: 700, epoch: 7 | loss: 0.0947759\n",
      "\tspeed: 0.1209s/iter; left time: 7479.3932s\n",
      "\titers: 800, epoch: 7 | loss: 0.0886568\n",
      "\tspeed: 0.1441s/iter; left time: 8898.4921s\n",
      "\titers: 900, epoch: 7 | loss: 0.0786587\n",
      "\tspeed: 0.1364s/iter; left time: 8409.6439s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0800228\n",
      "\tspeed: 0.1187s/iter; left time: 7307.6731s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0899325\n",
      "\tspeed: 0.1234s/iter; left time: 7583.9053s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0851249\n",
      "\tspeed: 0.1509s/iter; left time: 9257.0258s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0694570\n",
      "\tspeed: 0.1199s/iter; left time: 7345.7630s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0889620\n",
      "\tspeed: 0.1271s/iter; left time: 7771.9487s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0885222\n",
      "\tspeed: 0.1511s/iter; left time: 9226.0427s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0703967\n",
      "\tspeed: 0.1193s/iter; left time: 7268.7828s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0967138\n",
      "\tspeed: 0.1283s/iter; left time: 7804.6455s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0742753\n",
      "\tspeed: 0.1533s/iter; left time: 9311.5068s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0714891\n",
      "\tspeed: 0.1197s/iter; left time: 7258.1470s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0690335\n",
      "\tspeed: 0.1208s/iter; left time: 7316.2707s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0778978\n",
      "\tspeed: 0.1536s/iter; left time: 9286.0480s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0716214\n",
      "\tspeed: 0.1047s/iter; left time: 6317.3215s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0747692\n",
      "\tspeed: 0.1207s/iter; left time: 7274.7034s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0834282\n",
      "\tspeed: 0.1451s/iter; left time: 8730.6239s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0855097\n",
      "\tspeed: 0.1285s/iter; left time: 7719.3942s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0906422\n",
      "\tspeed: 0.1209s/iter; left time: 7248.9929s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0708709\n",
      "\tspeed: 0.1379s/iter; left time: 8252.6684s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0859929\n",
      "\tspeed: 0.1377s/iter; left time: 8230.1849s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0687493\n",
      "\tspeed: 0.1206s/iter; left time: 7194.0892s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0796787\n",
      "\tspeed: 0.1326s/iter; left time: 7899.1205s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0671332\n",
      "\tspeed: 0.1472s/iter; left time: 8753.9574s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0683739\n",
      "\tspeed: 0.1200s/iter; left time: 7125.1924s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0691068\n",
      "\tspeed: 0.1379s/iter; left time: 8169.7836s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0798232\n",
      "\tspeed: 0.1398s/iter; left time: 8270.7923s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0687565\n",
      "\tspeed: 0.1051s/iter; left time: 6207.9130s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0730739\n",
      "\tspeed: 0.1269s/iter; left time: 7482.7341s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0768365\n",
      "\tspeed: 0.1406s/iter; left time: 8277.6120s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0917242\n",
      "\tspeed: 0.1198s/iter; left time: 7040.4023s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0721889\n",
      "\tspeed: 0.1491s/iter; left time: 8746.6480s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1057147\n",
      "\tspeed: 0.1304s/iter; left time: 7634.8557s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0877508\n",
      "\tspeed: 0.1103s/iter; left time: 6445.3567s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0842020\n",
      "\tspeed: 0.1505s/iter; left time: 8784.2172s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0788388\n",
      "\tspeed: 0.1296s/iter; left time: 7551.2531s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0780672\n",
      "\tspeed: 0.1202s/iter; left time: 6990.5437s\n",
      "Epoch: 7 cost time: 00h:09m:46.16s\n",
      "Epoch: 7 | Train Loss: 0.0802900 Vali Loss: 0.0892569 Test Loss: 0.0938624\n",
      "Validation loss decreased (0.089567 --> 0.089257).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0878868\n",
      "\tspeed: 1.6872s/iter; left time: 97831.4956s\n",
      "\titers: 200, epoch: 8 | loss: 0.0819466\n",
      "\tspeed: 0.1205s/iter; left time: 6976.1434s\n",
      "\titers: 300, epoch: 8 | loss: 0.0872314\n",
      "\tspeed: 0.1211s/iter; left time: 6995.2544s\n",
      "\titers: 400, epoch: 8 | loss: 0.0871727\n",
      "\tspeed: 0.1210s/iter; left time: 6979.9712s\n",
      "\titers: 500, epoch: 8 | loss: 0.0690386\n",
      "\tspeed: 0.1410s/iter; left time: 8119.3008s\n",
      "\titers: 600, epoch: 8 | loss: 0.0786107\n",
      "\tspeed: 0.1349s/iter; left time: 7752.5776s\n",
      "\titers: 700, epoch: 8 | loss: 0.0654001\n",
      "\tspeed: 0.1192s/iter; left time: 6840.1877s\n",
      "\titers: 800, epoch: 8 | loss: 0.0706811\n",
      "\tspeed: 0.1149s/iter; left time: 6583.8160s\n",
      "\titers: 900, epoch: 8 | loss: 0.0823400\n",
      "\tspeed: 0.1549s/iter; left time: 8857.3411s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0656414\n",
      "\tspeed: 0.1250s/iter; left time: 7134.1351s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0717802\n",
      "\tspeed: 0.1201s/iter; left time: 6844.8365s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0914649\n",
      "\tspeed: 0.1264s/iter; left time: 7191.4123s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0788195\n",
      "\tspeed: 0.1518s/iter; left time: 8619.8951s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0938862\n",
      "\tspeed: 0.1210s/iter; left time: 6857.0610s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0883940\n",
      "\tspeed: 0.1207s/iter; left time: 6827.9290s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0923529\n",
      "\tspeed: 0.1416s/iter; left time: 7999.9330s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1006962\n",
      "\tspeed: 0.1328s/iter; left time: 7488.5343s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0933289\n",
      "\tspeed: 0.1191s/iter; left time: 6702.1349s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0580641\n",
      "\tspeed: 0.1212s/iter; left time: 6810.4715s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0778803\n",
      "\tspeed: 0.1564s/iter; left time: 8771.1030s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0813645\n",
      "\tspeed: 0.1214s/iter; left time: 6794.5474s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0837200\n",
      "\tspeed: 0.1197s/iter; left time: 6688.4300s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0852636\n",
      "\tspeed: 0.1273s/iter; left time: 7103.4942s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0721436\n",
      "\tspeed: 0.1512s/iter; left time: 8417.9630s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0740744\n",
      "\tspeed: 0.1191s/iter; left time: 6618.6066s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0824070\n",
      "\tspeed: 0.1290s/iter; left time: 7158.7539s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0793054\n",
      "\tspeed: 0.1516s/iter; left time: 8396.6055s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0864588\n",
      "\tspeed: 0.1050s/iter; left time: 5804.8515s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0741376\n",
      "\tspeed: 0.1051s/iter; left time: 5800.5858s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0899623\n",
      "\tspeed: 0.1102s/iter; left time: 6072.9682s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0805951\n",
      "\tspeed: 0.1538s/iter; left time: 8459.3076s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0612919\n",
      "\tspeed: 0.1160s/iter; left time: 6367.2286s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0753164\n",
      "\tspeed: 0.1196s/iter; left time: 6550.9213s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0797729\n",
      "\tspeed: 0.1448s/iter; left time: 7920.2862s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0591326\n",
      "\tspeed: 0.1337s/iter; left time: 7298.2519s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0769536\n",
      "\tspeed: 0.1211s/iter; left time: 6598.1463s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0768749\n",
      "\tspeed: 0.1197s/iter; left time: 6509.4649s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0756438\n",
      "\tspeed: 0.1584s/iter; left time: 8597.5746s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0782435\n",
      "\tspeed: 0.1152s/iter; left time: 6242.1408s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0735168\n",
      "\tspeed: 0.1213s/iter; left time: 6559.2425s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0852071\n",
      "\tspeed: 0.1311s/iter; left time: 7078.2596s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0820105\n",
      "\tspeed: 0.1423s/iter; left time: 7665.8085s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0806806\n",
      "\tspeed: 0.1109s/iter; left time: 5962.9593s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0883520\n",
      "\tspeed: 0.1045s/iter; left time: 5609.5571s\n",
      "Epoch: 8 cost time: 00h:09m:34.58s\n",
      "Epoch: 8 | Train Loss: 0.0793348 Vali Loss: 0.0892188 Test Loss: 0.0953779\n",
      "Validation loss decreased (0.089257 --> 0.089219).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0749750\n",
      "\tspeed: 1.6773s/iter; left time: 89764.7265s\n",
      "\titers: 200, epoch: 9 | loss: 0.0862554\n",
      "\tspeed: 0.1135s/iter; left time: 6065.2316s\n",
      "\titers: 300, epoch: 9 | loss: 0.0647535\n",
      "\tspeed: 0.1344s/iter; left time: 7165.1353s\n",
      "\titers: 400, epoch: 9 | loss: 0.0730983\n",
      "\tspeed: 0.1320s/iter; left time: 7024.5095s\n",
      "\titers: 500, epoch: 9 | loss: 0.0695380\n",
      "\tspeed: 0.1122s/iter; left time: 5960.2705s\n",
      "\titers: 600, epoch: 9 | loss: 0.0691377\n",
      "\tspeed: 0.1210s/iter; left time: 6417.0322s\n",
      "\titers: 700, epoch: 9 | loss: 0.0765528\n",
      "\tspeed: 0.1578s/iter; left time: 8351.9232s\n",
      "\titers: 800, epoch: 9 | loss: 0.0613169\n",
      "\tspeed: 0.1209s/iter; left time: 6385.8888s\n",
      "\titers: 900, epoch: 9 | loss: 0.0732772\n",
      "\tspeed: 0.1164s/iter; left time: 6135.6456s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0607491\n",
      "\tspeed: 0.1489s/iter; left time: 7832.2444s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0597287\n",
      "\tspeed: 0.1216s/iter; left time: 6385.4394s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0944503\n",
      "\tspeed: 0.1198s/iter; left time: 6278.5210s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0612135\n",
      "\tspeed: 0.1364s/iter; left time: 7136.6241s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0633327\n",
      "\tspeed: 0.1396s/iter; left time: 7288.1260s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0837415\n",
      "\tspeed: 0.1143s/iter; left time: 5954.7379s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0839318\n",
      "\tspeed: 0.1176s/iter; left time: 6115.8891s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0879491\n",
      "\tspeed: 0.1477s/iter; left time: 7670.7071s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0665905\n",
      "\tspeed: 0.1292s/iter; left time: 6695.1245s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0713467\n",
      "\tspeed: 0.1193s/iter; left time: 6170.6154s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0669027\n",
      "\tspeed: 0.1151s/iter; left time: 5941.0115s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0749533\n",
      "\tspeed: 0.1541s/iter; left time: 7941.1463s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0823501\n",
      "\tspeed: 0.1139s/iter; left time: 5855.2614s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0919251\n",
      "\tspeed: 0.1204s/iter; left time: 6180.4084s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0902282\n",
      "\tspeed: 0.1537s/iter; left time: 7869.5237s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0695416\n",
      "\tspeed: 0.1191s/iter; left time: 6088.7717s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0872029\n",
      "\tspeed: 0.1158s/iter; left time: 5910.0955s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0668999\n",
      "\tspeed: 0.1503s/iter; left time: 7651.5917s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0708064\n",
      "\tspeed: 0.1022s/iter; left time: 5191.8166s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0776473\n",
      "\tspeed: 0.1030s/iter; left time: 5223.8619s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0897343\n",
      "\tspeed: 0.1531s/iter; left time: 7748.4527s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0689395\n",
      "\tspeed: 0.1170s/iter; left time: 5912.9757s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0872946\n",
      "\tspeed: 0.1195s/iter; left time: 6024.1047s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0869129\n",
      "\tspeed: 0.1420s/iter; left time: 7144.3381s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0875106\n",
      "\tspeed: 0.1223s/iter; left time: 6140.0551s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0698793\n",
      "\tspeed: 0.1173s/iter; left time: 5876.5451s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0638036\n",
      "\tspeed: 0.1303s/iter; left time: 6515.6347s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0692865\n",
      "\tspeed: 0.1507s/iter; left time: 7524.7252s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0771837\n",
      "\tspeed: 0.1186s/iter; left time: 5909.1991s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0842480\n",
      "\tspeed: 0.1195s/iter; left time: 5941.2705s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0725595\n",
      "\tspeed: 0.1591s/iter; left time: 7892.0822s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0908936\n",
      "\tspeed: 0.1193s/iter; left time: 5907.0966s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0893085\n",
      "\tspeed: 0.1202s/iter; left time: 5940.5455s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0930462\n",
      "\tspeed: 0.1522s/iter; left time: 7507.5584s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0681519\n",
      "\tspeed: 0.1307s/iter; left time: 6434.3234s\n",
      "Epoch: 9 cost time: 00h:09m:32.08s\n",
      "Epoch: 9 | Train Loss: 0.0783692 Vali Loss: 0.0899408 Test Loss: 0.0950079\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0844053\n",
      "\tspeed: 1.6820s/iter; left time: 82500.9715s\n",
      "\titers: 200, epoch: 10 | loss: 0.1195817\n",
      "\tspeed: 0.1220s/iter; left time: 5973.9608s\n",
      "\titers: 300, epoch: 10 | loss: 0.0776654\n",
      "\tspeed: 0.1599s/iter; left time: 7809.4879s\n",
      "\titers: 400, epoch: 10 | loss: 0.0721945\n",
      "\tspeed: 0.1211s/iter; left time: 5904.7809s\n",
      "\titers: 500, epoch: 10 | loss: 0.0767422\n",
      "\tspeed: 0.1216s/iter; left time: 5917.7887s\n",
      "\titers: 600, epoch: 10 | loss: 0.0819118\n",
      "\tspeed: 0.1581s/iter; left time: 7674.0386s\n",
      "\titers: 700, epoch: 10 | loss: 0.0688704\n",
      "\tspeed: 0.1237s/iter; left time: 5992.1816s\n",
      "\titers: 800, epoch: 10 | loss: 0.1010805\n",
      "\tspeed: 0.1214s/iter; left time: 5871.1173s\n",
      "\titers: 900, epoch: 10 | loss: 0.0890266\n",
      "\tspeed: 0.1314s/iter; left time: 6341.3654s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0743395\n",
      "\tspeed: 0.1494s/iter; left time: 7193.9878s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0623439\n",
      "\tspeed: 0.1205s/iter; left time: 5787.8720s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0886193\n",
      "\tspeed: 0.1126s/iter; left time: 5400.9335s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0903579\n",
      "\tspeed: 0.1496s/iter; left time: 7157.8160s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0970001\n",
      "\tspeed: 0.1310s/iter; left time: 6257.2289s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0763897\n",
      "\tspeed: 0.1210s/iter; left time: 5763.8773s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0659897\n",
      "\tspeed: 0.1204s/iter; left time: 5725.3021s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0725043\n",
      "\tspeed: 0.1306s/iter; left time: 6196.2052s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0614413\n",
      "\tspeed: 0.1510s/iter; left time: 7151.3524s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0763310\n",
      "\tspeed: 0.1200s/iter; left time: 5670.7051s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0819842\n",
      "\tspeed: 0.1210s/iter; left time: 5702.9646s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0654081\n",
      "\tspeed: 0.1416s/iter; left time: 6663.6799s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0653410\n",
      "\tspeed: 0.1365s/iter; left time: 6409.5470s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0690481\n",
      "\tspeed: 0.1202s/iter; left time: 5631.1843s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0936227\n",
      "\tspeed: 0.1329s/iter; left time: 6214.4770s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0709767\n",
      "\tspeed: 0.1484s/iter; left time: 6922.3963s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0593837\n",
      "\tspeed: 0.1213s/iter; left time: 5647.1096s\n",
      "\titers: 2700, epoch: 10 | loss: 0.1029781\n",
      "\tspeed: 0.1414s/iter; left time: 6567.3338s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0660925\n",
      "\tspeed: 0.1360s/iter; left time: 6301.7741s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0810724\n",
      "\tspeed: 0.1210s/iter; left time: 5594.4079s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0861325\n",
      "\tspeed: 0.1209s/iter; left time: 5577.3804s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0749175\n",
      "\tspeed: 0.1580s/iter; left time: 7275.6342s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0772474\n",
      "\tspeed: 0.1222s/iter; left time: 5613.8471s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0663424\n",
      "\tspeed: 0.1203s/iter; left time: 5514.8633s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0681792\n",
      "\tspeed: 0.1209s/iter; left time: 5531.8550s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0826567\n",
      "\tspeed: 0.1595s/iter; left time: 7279.9395s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0750645\n",
      "\tspeed: 0.1197s/iter; left time: 5451.1942s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0935027\n",
      "\tspeed: 0.1208s/iter; left time: 5488.5839s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0815245\n",
      "\tspeed: 0.1224s/iter; left time: 5550.5173s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0724220\n",
      "\tspeed: 0.1588s/iter; left time: 7186.9221s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0900971\n",
      "\tspeed: 0.1210s/iter; left time: 5464.2632s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0711545\n",
      "\tspeed: 0.1205s/iter; left time: 5427.1613s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0712163\n",
      "\tspeed: 0.1206s/iter; left time: 5421.5202s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0879692\n",
      "\tspeed: 0.1413s/iter; left time: 6335.4571s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0614536\n",
      "\tspeed: 0.1445s/iter; left time: 6464.5598s\n",
      "Epoch: 10 cost time: 00h:09m:45.63s\n",
      "Epoch: 10 | Train Loss: 0.0775587 Vali Loss: 0.0896562 Test Loss: 0.0955003\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0843296\n",
      "\tspeed: 1.6467s/iter; left time: 73410.8944s\n",
      "\titers: 200, epoch: 11 | loss: 0.0733311\n",
      "\tspeed: 0.1372s/iter; left time: 6103.6812s\n",
      "\titers: 300, epoch: 11 | loss: 0.0601665\n",
      "\tspeed: 0.1466s/iter; left time: 6507.9786s\n",
      "\titers: 400, epoch: 11 | loss: 0.0716488\n",
      "\tspeed: 0.1204s/iter; left time: 5331.1016s\n",
      "\titers: 500, epoch: 11 | loss: 0.0750899\n",
      "\tspeed: 0.1204s/iter; left time: 5320.8064s\n",
      "\titers: 600, epoch: 11 | loss: 0.0896189\n",
      "\tspeed: 0.1297s/iter; left time: 5717.7175s\n",
      "\titers: 700, epoch: 11 | loss: 0.0716856\n",
      "\tspeed: 0.1524s/iter; left time: 6701.6404s\n",
      "\titers: 800, epoch: 11 | loss: 0.0796735\n",
      "\tspeed: 0.1199s/iter; left time: 5261.2863s\n",
      "\titers: 900, epoch: 11 | loss: 0.0532320\n",
      "\tspeed: 0.1202s/iter; left time: 5262.5064s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0739114\n",
      "\tspeed: 0.1305s/iter; left time: 5700.7252s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0947660\n",
      "\tspeed: 0.1521s/iter; left time: 6630.5899s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0835808\n",
      "\tspeed: 0.1210s/iter; left time: 5262.6226s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0754700\n",
      "\tspeed: 0.1211s/iter; left time: 5255.3562s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0658226\n",
      "\tspeed: 0.1310s/iter; left time: 5670.0127s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0543421\n",
      "\tspeed: 0.1489s/iter; left time: 6430.0778s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0710039\n",
      "\tspeed: 0.1189s/iter; left time: 5122.7165s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0833827\n",
      "\tspeed: 0.1207s/iter; left time: 5188.3997s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0674744\n",
      "\tspeed: 0.1274s/iter; left time: 5463.6391s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0692966\n",
      "\tspeed: 0.1558s/iter; left time: 6665.8844s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0941740\n",
      "\tspeed: 0.1207s/iter; left time: 5151.5496s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0860964\n",
      "\tspeed: 0.1289s/iter; left time: 5490.4012s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0810799\n",
      "\tspeed: 0.1574s/iter; left time: 6684.8249s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0745259\n",
      "\tspeed: 0.1204s/iter; left time: 5104.0232s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0968850\n",
      "\tspeed: 0.1188s/iter; left time: 5021.7642s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0774877\n",
      "\tspeed: 0.1575s/iter; left time: 6642.8301s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0719790\n",
      "\tspeed: 0.1300s/iter; left time: 5469.4454s\n",
      "\titers: 2700, epoch: 11 | loss: 0.0613416\n",
      "\tspeed: 0.1201s/iter; left time: 5042.2762s\n",
      "\titers: 2800, epoch: 11 | loss: 0.0705149\n",
      "\tspeed: 0.1430s/iter; left time: 5988.1750s\n",
      "\titers: 2900, epoch: 11 | loss: 0.0600997\n",
      "\tspeed: 0.1413s/iter; left time: 5901.9280s\n",
      "\titers: 3000, epoch: 11 | loss: 0.0838152\n",
      "\tspeed: 0.1188s/iter; left time: 4952.1183s\n",
      "\titers: 3100, epoch: 11 | loss: 0.0801776\n",
      "\tspeed: 0.1384s/iter; left time: 5754.3264s\n",
      "\titers: 3200, epoch: 11 | loss: 0.0818057\n",
      "\tspeed: 0.1461s/iter; left time: 6059.3046s\n",
      "\titers: 3300, epoch: 11 | loss: 0.0967920\n",
      "\tspeed: 0.1119s/iter; left time: 4630.3169s\n",
      "\titers: 3400, epoch: 11 | loss: 0.0851540\n",
      "\tspeed: 0.1232s/iter; left time: 5085.6712s\n",
      "\titers: 3500, epoch: 11 | loss: 0.0771672\n",
      "\tspeed: 0.1522s/iter; left time: 6266.4869s\n",
      "\titers: 3600, epoch: 11 | loss: 0.0722381\n",
      "\tspeed: 0.1202s/iter; left time: 4939.0617s\n",
      "\titers: 3700, epoch: 11 | loss: 0.0777316\n",
      "\tspeed: 0.1207s/iter; left time: 4946.3759s\n",
      "\titers: 3800, epoch: 11 | loss: 0.0811023\n",
      "\tspeed: 0.1317s/iter; left time: 5382.7297s\n",
      "\titers: 3900, epoch: 11 | loss: 0.0592589\n",
      "\tspeed: 0.1534s/iter; left time: 6254.9859s\n",
      "\titers: 4000, epoch: 11 | loss: 0.0921532\n",
      "\tspeed: 0.1206s/iter; left time: 4906.7445s\n",
      "\titers: 4100, epoch: 11 | loss: 0.0703947\n",
      "\tspeed: 0.1207s/iter; left time: 4896.7554s\n",
      "\titers: 4200, epoch: 11 | loss: 0.0643912\n",
      "\tspeed: 0.1311s/iter; left time: 5307.3984s\n",
      "\titers: 4300, epoch: 11 | loss: 0.0791592\n",
      "\tspeed: 0.1530s/iter; left time: 6176.6614s\n",
      "\titers: 4400, epoch: 11 | loss: 0.0683983\n",
      "\tspeed: 0.1197s/iter; left time: 4820.4603s\n",
      "Epoch: 11 cost time: 00h:09m:48.09s\n",
      "Epoch: 11 | Train Loss: 0.0766384 Vali Loss: 0.0910036 Test Loss: 0.0970744\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0710792\n",
      "\tspeed: 1.6707s/iter; left time: 67015.0552s\n",
      "\titers: 200, epoch: 12 | loss: 0.0842273\n",
      "\tspeed: 0.1495s/iter; left time: 5981.4013s\n",
      "\titers: 300, epoch: 12 | loss: 0.0629760\n",
      "\tspeed: 0.1316s/iter; left time: 5251.1875s\n",
      "\titers: 400, epoch: 12 | loss: 0.0810391\n",
      "\tspeed: 0.1248s/iter; left time: 4969.6130s\n",
      "\titers: 500, epoch: 12 | loss: 0.1013014\n",
      "\tspeed: 0.1573s/iter; left time: 6248.7412s\n",
      "\titers: 600, epoch: 12 | loss: 0.0584819\n",
      "\tspeed: 0.1200s/iter; left time: 4752.4750s\n",
      "\titers: 700, epoch: 12 | loss: 0.0732251\n",
      "\tspeed: 0.1216s/iter; left time: 4806.4958s\n",
      "\titers: 800, epoch: 12 | loss: 0.0754556\n",
      "\tspeed: 0.1612s/iter; left time: 6353.0455s\n",
      "\titers: 900, epoch: 12 | loss: 0.0640929\n",
      "\tspeed: 0.1205s/iter; left time: 4736.1670s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0760040\n",
      "\tspeed: 0.1204s/iter; left time: 4723.1559s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0927217\n",
      "\tspeed: 0.1297s/iter; left time: 5072.2115s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0876345\n",
      "\tspeed: 0.1495s/iter; left time: 5831.9006s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0746380\n",
      "\tspeed: 0.1206s/iter; left time: 4692.0863s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0757993\n",
      "\tspeed: 0.1268s/iter; left time: 4921.5610s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0810570\n",
      "\tspeed: 0.1479s/iter; left time: 5724.7103s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0619826\n",
      "\tspeed: 0.1203s/iter; left time: 4646.6936s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0746262\n",
      "\tspeed: 0.1200s/iter; left time: 4622.0712s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0823452\n",
      "\tspeed: 0.1614s/iter; left time: 6199.2091s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0711706\n",
      "\tspeed: 0.1201s/iter; left time: 4601.9353s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0654976\n",
      "\tspeed: 0.1210s/iter; left time: 4622.7126s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0801418\n",
      "\tspeed: 0.1308s/iter; left time: 4984.0991s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0796125\n",
      "\tspeed: 0.1504s/iter; left time: 5718.2610s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0880586\n",
      "\tspeed: 0.1189s/iter; left time: 4508.4351s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0664747\n",
      "\tspeed: 0.1262s/iter; left time: 4773.4783s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0685178\n",
      "\tspeed: 0.1502s/iter; left time: 5662.9363s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0550742\n",
      "\tspeed: 0.1200s/iter; left time: 4513.6858s\n",
      "\titers: 2700, epoch: 12 | loss: 0.0827314\n",
      "\tspeed: 0.1201s/iter; left time: 4504.8870s\n",
      "\titers: 2800, epoch: 12 | loss: 0.0665130\n",
      "\tspeed: 0.1199s/iter; left time: 4486.0242s\n",
      "\titers: 2900, epoch: 12 | loss: 0.0725770\n",
      "\tspeed: 0.1475s/iter; left time: 5504.8553s\n",
      "\titers: 3000, epoch: 12 | loss: 0.0761487\n",
      "\tspeed: 0.1300s/iter; left time: 4838.5814s\n",
      "\titers: 3100, epoch: 12 | loss: 0.0791091\n",
      "\tspeed: 0.1203s/iter; left time: 4464.0211s\n",
      "\titers: 3200, epoch: 12 | loss: 0.0671570\n",
      "\tspeed: 0.1300s/iter; left time: 4812.7943s\n",
      "\titers: 3300, epoch: 12 | loss: 0.0749104\n",
      "\tspeed: 0.1507s/iter; left time: 5562.8798s\n",
      "\titers: 3400, epoch: 12 | loss: 0.0909807\n",
      "\tspeed: 0.1207s/iter; left time: 4441.6935s\n",
      "\titers: 3500, epoch: 12 | loss: 0.0923854\n",
      "\tspeed: 0.1210s/iter; left time: 4440.7868s\n",
      "\titers: 3600, epoch: 12 | loss: 0.0831650\n",
      "\tspeed: 0.1379s/iter; left time: 5050.0754s\n",
      "\titers: 3700, epoch: 12 | loss: 0.0813926\n",
      "\tspeed: 0.1450s/iter; left time: 5294.5065s\n",
      "\titers: 3800, epoch: 12 | loss: 0.0719712\n",
      "\tspeed: 0.1202s/iter; left time: 4378.0491s\n",
      "\titers: 3900, epoch: 12 | loss: 0.0697064\n",
      "\tspeed: 0.1287s/iter; left time: 4674.5955s\n",
      "\titers: 4000, epoch: 12 | loss: 0.0884542\n",
      "\tspeed: 0.1493s/iter; left time: 5408.3420s\n",
      "\titers: 4100, epoch: 12 | loss: 0.0673367\n",
      "\tspeed: 0.1197s/iter; left time: 4323.2589s\n",
      "\titers: 4200, epoch: 12 | loss: 0.0795303\n",
      "\tspeed: 0.1374s/iter; left time: 4948.3719s\n",
      "\titers: 4300, epoch: 12 | loss: 0.0724135\n",
      "\tspeed: 0.1409s/iter; left time: 5058.9675s\n",
      "\titers: 4400, epoch: 12 | loss: 0.0837259\n",
      "\tspeed: 0.1208s/iter; left time: 4324.9559s\n",
      "Epoch: 12 cost time: 00h:09m:50.33s\n",
      "Epoch: 12 | Train Loss: 0.0758073 Vali Loss: 0.0911360 Test Loss: 0.0977819\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0696547\n",
      "\tspeed: 1.6608s/iter; left time: 59200.1284s\n",
      "\titers: 200, epoch: 13 | loss: 0.0783792\n",
      "\tspeed: 0.1404s/iter; left time: 4990.2061s\n",
      "\titers: 300, epoch: 13 | loss: 0.0612777\n",
      "\tspeed: 0.1444s/iter; left time: 5119.4555s\n",
      "\titers: 400, epoch: 13 | loss: 0.0669738\n",
      "\tspeed: 0.1203s/iter; left time: 4253.0101s\n",
      "\titers: 500, epoch: 13 | loss: 0.0753981\n",
      "\tspeed: 0.1297s/iter; left time: 4570.3097s\n",
      "\titers: 600, epoch: 13 | loss: 0.0703675\n",
      "\tspeed: 0.1517s/iter; left time: 5330.7711s\n",
      "\titers: 700, epoch: 13 | loss: 0.0807081\n",
      "\tspeed: 0.1123s/iter; left time: 3935.5227s\n",
      "\titers: 800, epoch: 13 | loss: 0.0631655\n",
      "\tspeed: 0.1207s/iter; left time: 4216.9096s\n",
      "\titers: 900, epoch: 13 | loss: 0.0665464\n",
      "\tspeed: 0.1192s/iter; left time: 4154.5910s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0889466\n",
      "\tspeed: 0.1203s/iter; left time: 4179.3517s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0937114\n",
      "\tspeed: 0.1206s/iter; left time: 4178.0988s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0855564\n",
      "\tspeed: 0.1508s/iter; left time: 5210.7291s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0802188\n",
      "\tspeed: 0.1318s/iter; left time: 4540.1601s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0752418\n",
      "\tspeed: 0.1199s/iter; left time: 4118.0372s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0920139\n",
      "\tspeed: 0.1202s/iter; left time: 4115.9709s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0802863\n",
      "\tspeed: 0.1212s/iter; left time: 4138.8403s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0784608\n",
      "\tspeed: 0.1204s/iter; left time: 4100.0962s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0880799\n",
      "\tspeed: 0.1304s/iter; left time: 4426.9758s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0902761\n",
      "\tspeed: 0.1517s/iter; left time: 5134.8778s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0855317\n",
      "\tspeed: 0.1213s/iter; left time: 4093.7911s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0778534\n",
      "\tspeed: 0.1204s/iter; left time: 4051.0211s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0732491\n",
      "\tspeed: 0.1322s/iter; left time: 4435.5935s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0718696\n",
      "\tspeed: 0.1537s/iter; left time: 5138.8353s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0643891\n",
      "\tspeed: 0.1208s/iter; left time: 4027.9035s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0634482\n",
      "\tspeed: 0.1201s/iter; left time: 3992.3590s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0696910\n",
      "\tspeed: 0.1249s/iter; left time: 4138.7601s\n",
      "\titers: 2700, epoch: 13 | loss: 0.0817821\n",
      "\tspeed: 0.1544s/iter; left time: 5103.6425s\n",
      "\titers: 2800, epoch: 13 | loss: 0.0640855\n",
      "\tspeed: 0.1089s/iter; left time: 3588.5954s\n",
      "\titers: 2900, epoch: 13 | loss: 0.0694307\n",
      "\tspeed: 0.1201s/iter; left time: 3943.2079s\n",
      "\titers: 3000, epoch: 13 | loss: 0.0864336\n",
      "\tspeed: 0.1605s/iter; left time: 5255.3800s\n",
      "\titers: 3100, epoch: 13 | loss: 0.0733568\n",
      "\tspeed: 0.1202s/iter; left time: 3925.5034s\n",
      "\titers: 3200, epoch: 13 | loss: 0.0625381\n",
      "\tspeed: 0.1208s/iter; left time: 3931.4236s\n",
      "\titers: 3300, epoch: 13 | loss: 0.0875661\n",
      "\tspeed: 0.1379s/iter; left time: 4473.3903s\n",
      "\titers: 3400, epoch: 13 | loss: 0.0675188\n",
      "\tspeed: 0.1411s/iter; left time: 4562.7165s\n",
      "\titers: 3500, epoch: 13 | loss: 0.0647626\n",
      "\tspeed: 0.1119s/iter; left time: 3609.2749s\n",
      "\titers: 3600, epoch: 13 | loss: 0.0939891\n",
      "\tspeed: 0.1211s/iter; left time: 3893.5604s\n",
      "\titers: 3700, epoch: 13 | loss: 0.0905394\n",
      "\tspeed: 0.1586s/iter; left time: 5083.0744s\n",
      "\titers: 3800, epoch: 13 | loss: 0.0685588\n",
      "\tspeed: 0.1151s/iter; left time: 3675.9945s\n",
      "\titers: 3900, epoch: 13 | loss: 0.0648391\n",
      "\tspeed: 0.1203s/iter; left time: 3830.9470s\n",
      "\titers: 4000, epoch: 13 | loss: 0.0833651\n",
      "\tspeed: 0.1406s/iter; left time: 4463.2430s\n",
      "\titers: 4100, epoch: 13 | loss: 0.0707878\n",
      "\tspeed: 0.1389s/iter; left time: 4396.1421s\n",
      "\titers: 4200, epoch: 13 | loss: 0.0837740\n",
      "\tspeed: 0.1203s/iter; left time: 3793.8077s\n",
      "\titers: 4300, epoch: 13 | loss: 0.0526903\n",
      "\tspeed: 0.1211s/iter; left time: 3808.3084s\n",
      "\titers: 4400, epoch: 13 | loss: 0.0681901\n",
      "\tspeed: 0.1538s/iter; left time: 4820.6832s\n",
      "Epoch: 13 cost time: 00h:09m:39.16s\n",
      "Epoch: 13 | Train Loss: 0.0749393 Vali Loss: 0.0928503 Test Loss: 0.1000453\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02273400127887726, rmse:0.15077798068523407, mae:0.09537788480520248, rse:0.5325193405151367\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 24: 02h:37m:43.65s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-10-31 21:29:15,883] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 21:29:18,388] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 21:29:18,388] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 21:29:18,388] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 21:29:18,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 21:29:18,490] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 21:29:19,157] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 21:29:19,159] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 21:29:19,159] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 21:29:19,160] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 21:29:19,161] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 21:29:19,161] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 21:29:19,161] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 21:29:19,161] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 21:29:19,161] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 21:29:19,161] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 21:29:19,477] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 21:29:19,478] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 21:29:19,479] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.8 GB, percent = 25.7%\n",
      "[2024-10-31 21:29:19,625] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 21:29:19,626] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 21:29:19,626] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.8 GB, percent = 25.7%\n",
      "[2024-10-31 21:29:19,626] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 21:29:19,757] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 21:29:19,758] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 21:29:19,758] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 193.8 GB, percent = 25.7%\n",
      "[2024-10-31 21:29:19,759] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 21:29:19,759] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 21:29:19,759] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 21:29:19,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 21:29:19,760] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f61c49bb950>\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 21:29:19,761] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 21:29:19,762] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 21:29:19,763] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1857200\n",
      "\tspeed: 0.2228s/iter; left time: 19837.2706s\n",
      "\titers: 200, epoch: 1 | loss: 0.1687170\n",
      "\tspeed: 0.1312s/iter; left time: 11665.3898s\n",
      "\titers: 300, epoch: 1 | loss: 0.1724844\n",
      "\tspeed: 0.1275s/iter; left time: 11331.5341s\n",
      "\titers: 400, epoch: 1 | loss: 0.1656757\n",
      "\tspeed: 0.1212s/iter; left time: 10756.0390s\n",
      "\titers: 500, epoch: 1 | loss: 0.1675965\n",
      "\tspeed: 0.1678s/iter; left time: 14876.8321s\n",
      "\titers: 600, epoch: 1 | loss: 0.1443687\n",
      "\tspeed: 0.1311s/iter; left time: 11606.1350s\n",
      "\titers: 700, epoch: 1 | loss: 0.1422241\n",
      "\tspeed: 0.1259s/iter; left time: 11137.4794s\n",
      "\titers: 800, epoch: 1 | loss: 0.1226619\n",
      "\tspeed: 0.1298s/iter; left time: 11469.6908s\n",
      "\titers: 900, epoch: 1 | loss: 0.1330729\n",
      "\tspeed: 0.1671s/iter; left time: 14744.7505s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1138731\n",
      "\tspeed: 0.1311s/iter; left time: 11554.9099s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1071478\n",
      "\tspeed: 0.1303s/iter; left time: 11474.9060s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1303404\n",
      "\tspeed: 0.1307s/iter; left time: 11498.1139s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1238685\n",
      "\tspeed: 0.1538s/iter; left time: 13508.1174s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1032463\n",
      "\tspeed: 0.1380s/iter; left time: 12110.8463s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1194469\n",
      "\tspeed: 0.1145s/iter; left time: 10035.8265s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1207578\n",
      "\tspeed: 0.1304s/iter; left time: 11418.6313s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1394709\n",
      "\tspeed: 0.1281s/iter; left time: 11197.9367s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1160002\n",
      "\tspeed: 0.1662s/iter; left time: 14516.4179s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1103131\n",
      "\tspeed: 0.1309s/iter; left time: 11420.3830s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1130958\n",
      "\tspeed: 0.1310s/iter; left time: 11414.7452s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1089309\n",
      "\tspeed: 0.1301s/iter; left time: 11319.7440s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1244033\n",
      "\tspeed: 0.1622s/iter; left time: 14101.9482s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1284142\n",
      "\tspeed: 0.1302s/iter; left time: 11303.5399s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1186614\n",
      "\tspeed: 0.1285s/iter; left time: 11144.5636s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1169964\n",
      "\tspeed: 0.1513s/iter; left time: 13112.9964s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1132365\n",
      "\tspeed: 0.1498s/iter; left time: 12964.2629s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1326390\n",
      "\tspeed: 0.1309s/iter; left time: 11312.5818s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1291778\n",
      "\tspeed: 0.1304s/iter; left time: 11259.8777s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1308049\n",
      "\tspeed: 0.1543s/iter; left time: 13302.8517s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1142576\n",
      "\tspeed: 0.1470s/iter; left time: 12664.1308s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0980470\n",
      "\tspeed: 0.1287s/iter; left time: 11074.7031s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1026124\n",
      "\tspeed: 0.1332s/iter; left time: 11449.7419s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1019026\n",
      "\tspeed: 0.1523s/iter; left time: 13070.4958s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1156720\n",
      "\tspeed: 0.1310s/iter; left time: 11231.8749s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1253485\n",
      "\tspeed: 0.1365s/iter; left time: 11686.2320s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1046039\n",
      "\tspeed: 0.1621s/iter; left time: 13865.5288s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1413063\n",
      "\tspeed: 0.1310s/iter; left time: 11190.4438s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1070143\n",
      "\tspeed: 0.1311s/iter; left time: 11189.2488s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1359391\n",
      "\tspeed: 0.1530s/iter; left time: 13041.6953s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1120188\n",
      "\tspeed: 0.1468s/iter; left time: 12498.3120s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1373022\n",
      "\tspeed: 0.1311s/iter; left time: 11149.5429s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1048007\n",
      "\tspeed: 0.1308s/iter; left time: 11107.2007s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1046956\n",
      "\tspeed: 0.1675s/iter; left time: 14207.4128s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1285643\n",
      "\tspeed: 0.1248s/iter; left time: 10572.0927s\n",
      "Epoch: 1 cost time: 00h:10m:20.69s\n",
      "Epoch: 1 | Train Loss: 0.1245267 Vali Loss: 0.1197564 Test Loss: 0.1277876\n",
      "Validation loss decreased (inf --> 0.119756).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1362824\n",
      "\tspeed: 1.8524s/iter; left time: 156681.6822s\n",
      "\titers: 200, epoch: 2 | loss: 0.1034659\n",
      "\tspeed: 0.1208s/iter; left time: 10204.3256s\n",
      "\titers: 300, epoch: 2 | loss: 0.1078680\n",
      "\tspeed: 0.1487s/iter; left time: 12544.5177s\n",
      "\titers: 400, epoch: 2 | loss: 0.1090891\n",
      "\tspeed: 0.1182s/iter; left time: 9959.3467s\n",
      "\titers: 500, epoch: 2 | loss: 0.0995298\n",
      "\tspeed: 0.1208s/iter; left time: 10172.0625s\n",
      "\titers: 600, epoch: 2 | loss: 0.1253273\n",
      "\tspeed: 0.1207s/iter; left time: 10147.1868s\n",
      "\titers: 700, epoch: 2 | loss: 0.1308996\n",
      "\tspeed: 0.1441s/iter; left time: 12104.9090s\n",
      "\titers: 800, epoch: 2 | loss: 0.1199580\n",
      "\tspeed: 0.1386s/iter; left time: 11625.7887s\n",
      "\titers: 900, epoch: 2 | loss: 0.1126412\n",
      "\tspeed: 0.1194s/iter; left time: 10005.2187s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1188750\n",
      "\tspeed: 0.1207s/iter; left time: 10099.1701s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1189725\n",
      "\tspeed: 0.1205s/iter; left time: 10072.3692s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1217855\n",
      "\tspeed: 0.1526s/iter; left time: 12735.5070s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1159035\n",
      "\tspeed: 0.1205s/iter; left time: 10047.4742s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0984060\n",
      "\tspeed: 0.1204s/iter; left time: 10030.3748s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0966178\n",
      "\tspeed: 0.1515s/iter; left time: 12604.9615s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1044325\n",
      "\tspeed: 0.1246s/iter; left time: 10350.6445s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1260282\n",
      "\tspeed: 0.1199s/iter; left time: 9949.7037s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1024257\n",
      "\tspeed: 0.1463s/iter; left time: 12126.9851s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0874435\n",
      "\tspeed: 0.1357s/iter; left time: 11233.2718s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1199540\n",
      "\tspeed: 0.1201s/iter; left time: 9928.8380s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1297425\n",
      "\tspeed: 0.1142s/iter; left time: 9433.2969s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1000140\n",
      "\tspeed: 0.1206s/iter; left time: 9949.4777s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1035635\n",
      "\tspeed: 0.1299s/iter; left time: 10699.4000s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1033574\n",
      "\tspeed: 0.1532s/iter; left time: 12607.5039s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1072888\n",
      "\tspeed: 0.1206s/iter; left time: 9911.3347s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1190963\n",
      "\tspeed: 0.1207s/iter; left time: 9909.1189s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1066877\n",
      "\tspeed: 0.1327s/iter; left time: 10882.7715s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1129730\n",
      "\tspeed: 0.1430s/iter; left time: 11706.3079s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1384397\n",
      "\tspeed: 0.1200s/iter; left time: 9812.6067s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1114533\n",
      "\tspeed: 0.1212s/iter; left time: 9902.3464s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1163946\n",
      "\tspeed: 0.1294s/iter; left time: 10558.4800s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1102893\n",
      "\tspeed: 0.1514s/iter; left time: 12340.3496s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0999768\n",
      "\tspeed: 0.1203s/iter; left time: 9790.7820s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0817148\n",
      "\tspeed: 0.1197s/iter; left time: 9728.3412s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1148444\n",
      "\tspeed: 0.1301s/iter; left time: 10565.0041s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1212692\n",
      "\tspeed: 0.1506s/iter; left time: 12214.7543s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1155532\n",
      "\tspeed: 0.1165s/iter; left time: 9432.7674s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1284737\n",
      "\tspeed: 0.1087s/iter; left time: 8793.7912s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1278360\n",
      "\tspeed: 0.1203s/iter; left time: 9720.8441s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1092012\n",
      "\tspeed: 0.1496s/iter; left time: 12069.2576s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0933397\n",
      "\tspeed: 0.1213s/iter; left time: 9776.7060s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1042761\n",
      "\tspeed: 0.1345s/iter; left time: 10827.5039s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1036103\n",
      "\tspeed: 0.1416s/iter; left time: 11379.9974s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0886638\n",
      "\tspeed: 0.1202s/iter; left time: 9648.6005s\n",
      "Epoch: 2 cost time: 00h:09m:35.58s\n",
      "Epoch: 2 | Train Loss: 0.1098625 Vali Loss: 0.1185164 Test Loss: 0.1282422\n",
      "Validation loss decreased (0.119756 --> 0.118516).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1121855\n",
      "\tspeed: 1.7050s/iter; left time: 136619.0496s\n",
      "\titers: 200, epoch: 3 | loss: 0.0993601\n",
      "\tspeed: 0.1205s/iter; left time: 9644.2246s\n",
      "\titers: 300, epoch: 3 | loss: 0.0954742\n",
      "\tspeed: 0.1202s/iter; left time: 9608.0875s\n",
      "\titers: 400, epoch: 3 | loss: 0.1022189\n",
      "\tspeed: 0.1494s/iter; left time: 11926.4749s\n",
      "\titers: 500, epoch: 3 | loss: 0.1028623\n",
      "\tspeed: 0.1324s/iter; left time: 10555.7552s\n",
      "\titers: 600, epoch: 3 | loss: 0.1073780\n",
      "\tspeed: 0.1204s/iter; left time: 9584.3099s\n",
      "\titers: 700, epoch: 3 | loss: 0.1252172\n",
      "\tspeed: 0.1237s/iter; left time: 9840.4985s\n",
      "\titers: 800, epoch: 3 | loss: 0.0985473\n",
      "\tspeed: 0.1530s/iter; left time: 12150.8329s\n",
      "\titers: 900, epoch: 3 | loss: 0.1068274\n",
      "\tspeed: 0.1206s/iter; left time: 9567.2498s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1010967\n",
      "\tspeed: 0.1209s/iter; left time: 9575.6321s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1209155\n",
      "\tspeed: 0.1105s/iter; left time: 8746.5421s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1471111\n",
      "\tspeed: 0.1433s/iter; left time: 11326.1966s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0935464\n",
      "\tspeed: 0.1372s/iter; left time: 10828.0251s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1135274\n",
      "\tspeed: 0.1209s/iter; left time: 9530.2177s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0995740\n",
      "\tspeed: 0.1210s/iter; left time: 9529.0338s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0949781\n",
      "\tspeed: 0.1232s/iter; left time: 9683.4409s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1161713\n",
      "\tspeed: 0.1593s/iter; left time: 12508.0138s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1313455\n",
      "\tspeed: 0.1191s/iter; left time: 9337.7773s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1055434\n",
      "\tspeed: 0.1207s/iter; left time: 9451.0232s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1189179\n",
      "\tspeed: 0.1203s/iter; left time: 9407.9386s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0911239\n",
      "\tspeed: 0.1327s/iter; left time: 10368.4362s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1390544\n",
      "\tspeed: 0.1516s/iter; left time: 11826.2533s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0976654\n",
      "\tspeed: 0.1208s/iter; left time: 9415.9381s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1079981\n",
      "\tspeed: 0.1210s/iter; left time: 9415.5823s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1111981\n",
      "\tspeed: 0.1201s/iter; left time: 9334.2331s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1110975\n",
      "\tspeed: 0.1345s/iter; left time: 10439.3863s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1392076\n",
      "\tspeed: 0.1426s/iter; left time: 11057.4100s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1271442\n",
      "\tspeed: 0.1205s/iter; left time: 9332.4976s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1141754\n",
      "\tspeed: 0.1203s/iter; left time: 9299.8072s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1144832\n",
      "\tspeed: 0.1566s/iter; left time: 12092.3239s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0868490\n",
      "\tspeed: 0.1093s/iter; left time: 8432.0423s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1013458\n",
      "\tspeed: 0.1202s/iter; left time: 9256.8049s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0867222\n",
      "\tspeed: 0.1395s/iter; left time: 10732.6274s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0821788\n",
      "\tspeed: 0.1474s/iter; left time: 11327.6568s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1050519\n",
      "\tspeed: 0.1206s/iter; left time: 9251.8680s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1225686\n",
      "\tspeed: 0.1208s/iter; left time: 9258.2205s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1083653\n",
      "\tspeed: 0.1528s/iter; left time: 11694.7361s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0852168\n",
      "\tspeed: 0.1330s/iter; left time: 10166.8387s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1460541\n",
      "\tspeed: 0.1179s/iter; left time: 9002.2864s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0892489\n",
      "\tspeed: 0.1331s/iter; left time: 10148.4013s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1043740\n",
      "\tspeed: 0.1374s/iter; left time: 10456.2352s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1233985\n",
      "\tspeed: 0.1204s/iter; left time: 9153.7200s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1201573\n",
      "\tspeed: 0.1397s/iter; left time: 10608.1815s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0884113\n",
      "\tspeed: 0.1377s/iter; left time: 10442.3643s\n",
      "Epoch: 3 cost time: 00h:09m:41.98s\n",
      "Epoch: 3 | Train Loss: 0.1059200 Vali Loss: 0.1199043 Test Loss: 0.1320190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1017862\n",
      "\tspeed: 1.6342s/iter; left time: 123657.6602s\n",
      "\titers: 200, epoch: 4 | loss: 0.1054816\n",
      "\tspeed: 0.1542s/iter; left time: 11651.7391s\n",
      "\titers: 300, epoch: 4 | loss: 0.1008089\n",
      "\tspeed: 0.1224s/iter; left time: 9236.6989s\n",
      "\titers: 400, epoch: 4 | loss: 0.1103006\n",
      "\tspeed: 0.1194s/iter; left time: 8996.2846s\n",
      "\titers: 500, epoch: 4 | loss: 0.1008416\n",
      "\tspeed: 0.1383s/iter; left time: 10411.2531s\n",
      "\titers: 600, epoch: 4 | loss: 0.0872107\n",
      "\tspeed: 0.1379s/iter; left time: 10362.9206s\n",
      "\titers: 700, epoch: 4 | loss: 0.1059361\n",
      "\tspeed: 0.1167s/iter; left time: 8759.3240s\n",
      "\titers: 800, epoch: 4 | loss: 0.1094390\n",
      "\tspeed: 0.1202s/iter; left time: 9011.6328s\n",
      "\titers: 900, epoch: 4 | loss: 0.1180137\n",
      "\tspeed: 0.1328s/iter; left time: 9941.5273s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1055246\n",
      "\tspeed: 0.1364s/iter; left time: 10198.2564s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1000077\n",
      "\tspeed: 0.1184s/iter; left time: 8841.7538s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0994589\n",
      "\tspeed: 0.1135s/iter; left time: 8464.5988s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1067580\n",
      "\tspeed: 0.1197s/iter; left time: 8914.2984s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0998731\n",
      "\tspeed: 0.1529s/iter; left time: 11370.8496s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1033912\n",
      "\tspeed: 0.1206s/iter; left time: 8957.6547s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0937374\n",
      "\tspeed: 0.1239s/iter; left time: 9187.1257s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0970818\n",
      "\tspeed: 0.1504s/iter; left time: 11137.6893s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1050920\n",
      "\tspeed: 0.1199s/iter; left time: 8872.5141s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1291669\n",
      "\tspeed: 0.1299s/iter; left time: 9593.7135s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0771152\n",
      "\tspeed: 0.1468s/iter; left time: 10826.0734s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0940207\n",
      "\tspeed: 0.1206s/iter; left time: 8883.7334s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0884763\n",
      "\tspeed: 0.1387s/iter; left time: 10200.9555s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0902145\n",
      "\tspeed: 0.1388s/iter; left time: 10197.5674s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1145035\n",
      "\tspeed: 0.1198s/iter; left time: 8791.5591s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1050488\n",
      "\tspeed: 0.1514s/iter; left time: 11092.3122s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0954266\n",
      "\tspeed: 0.1254s/iter; left time: 9174.1143s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0995934\n",
      "\tspeed: 0.1203s/iter; left time: 8790.7084s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1045068\n",
      "\tspeed: 0.1397s/iter; left time: 10192.5030s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0896091\n",
      "\tspeed: 0.1361s/iter; left time: 9915.2674s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1133813\n",
      "\tspeed: 0.1201s/iter; left time: 8743.1096s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1004615\n",
      "\tspeed: 0.1384s/iter; left time: 10054.6553s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1033023\n",
      "\tspeed: 0.1403s/iter; left time: 10184.3969s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1076910\n",
      "\tspeed: 0.1207s/iter; left time: 8749.9764s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0782520\n",
      "\tspeed: 0.1558s/iter; left time: 11277.2845s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0946225\n",
      "\tspeed: 0.1152s/iter; left time: 8325.9921s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1047806\n",
      "\tspeed: 0.1208s/iter; left time: 8718.2807s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0901383\n",
      "\tspeed: 0.1502s/iter; left time: 10823.3012s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0860445\n",
      "\tspeed: 0.1151s/iter; left time: 8282.3806s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0885248\n",
      "\tspeed: 0.1199s/iter; left time: 8616.3890s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1189710\n",
      "\tspeed: 0.1451s/iter; left time: 10416.0725s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0907215\n",
      "\tspeed: 0.1325s/iter; left time: 9492.8908s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1236514\n",
      "\tspeed: 0.1208s/iter; left time: 8645.1304s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1064944\n",
      "\tspeed: 0.1203s/iter; left time: 8600.3627s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0968458\n",
      "\tspeed: 0.1471s/iter; left time: 10501.9215s\n",
      "Epoch: 4 cost time: 00h:09m:42.82s\n",
      "Epoch: 4 | Train Loss: 0.1021763 Vali Loss: 0.1203913 Test Loss: 0.1309304\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0960487\n",
      "\tspeed: 1.6037s/iter; left time: 114204.3345s\n",
      "\titers: 200, epoch: 5 | loss: 0.0997009\n",
      "\tspeed: 0.1234s/iter; left time: 8772.1768s\n",
      "\titers: 300, epoch: 5 | loss: 0.1327493\n",
      "\tspeed: 0.1391s/iter; left time: 9878.2791s\n",
      "\titers: 400, epoch: 5 | loss: 0.1110552\n",
      "\tspeed: 0.1349s/iter; left time: 9564.9740s\n",
      "\titers: 500, epoch: 5 | loss: 0.0997856\n",
      "\tspeed: 0.1203s/iter; left time: 8521.7706s\n",
      "\titers: 600, epoch: 5 | loss: 0.1085910\n",
      "\tspeed: 0.1272s/iter; left time: 8991.9604s\n",
      "\titers: 700, epoch: 5 | loss: 0.0906752\n",
      "\tspeed: 0.1519s/iter; left time: 10724.1571s\n",
      "\titers: 800, epoch: 5 | loss: 0.0881957\n",
      "\tspeed: 0.1205s/iter; left time: 8496.3136s\n",
      "\titers: 900, epoch: 5 | loss: 0.1219619\n",
      "\tspeed: 0.1268s/iter; left time: 8929.7758s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1071277\n",
      "\tspeed: 0.1546s/iter; left time: 10873.3255s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1084025\n",
      "\tspeed: 0.1202s/iter; left time: 8439.8887s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0964981\n",
      "\tspeed: 0.1231s/iter; left time: 8628.2586s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0842596\n",
      "\tspeed: 0.1593s/iter; left time: 11155.4025s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0926249\n",
      "\tspeed: 0.1211s/iter; left time: 8464.9976s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0907346\n",
      "\tspeed: 0.1210s/iter; left time: 8450.7858s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1106110\n",
      "\tspeed: 0.1451s/iter; left time: 10117.1060s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0988417\n",
      "\tspeed: 0.1361s/iter; left time: 9476.6170s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0920804\n",
      "\tspeed: 0.1208s/iter; left time: 8399.7770s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0919630\n",
      "\tspeed: 0.1401s/iter; left time: 9722.5162s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1004907\n",
      "\tspeed: 0.1395s/iter; left time: 9667.2462s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1181042\n",
      "\tspeed: 0.1204s/iter; left time: 8334.4255s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0876759\n",
      "\tspeed: 0.1283s/iter; left time: 8864.4635s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1073024\n",
      "\tspeed: 0.1503s/iter; left time: 10369.9535s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0821408\n",
      "\tspeed: 0.1207s/iter; left time: 8317.9456s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0953626\n",
      "\tspeed: 0.1179s/iter; left time: 8111.5827s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1104295\n",
      "\tspeed: 0.1307s/iter; left time: 8981.4432s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1122409\n",
      "\tspeed: 0.1480s/iter; left time: 10152.3862s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0743636\n",
      "\tspeed: 0.1210s/iter; left time: 8290.2840s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0947080\n",
      "\tspeed: 0.1208s/iter; left time: 8264.4572s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1082617\n",
      "\tspeed: 0.1337s/iter; left time: 9136.1067s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0972924\n",
      "\tspeed: 0.1453s/iter; left time: 9909.9216s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1161426\n",
      "\tspeed: 0.1209s/iter; left time: 8235.7296s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0934898\n",
      "\tspeed: 0.1205s/iter; left time: 8194.0800s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0890894\n",
      "\tspeed: 0.1386s/iter; left time: 9415.6284s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0774576\n",
      "\tspeed: 0.1468s/iter; left time: 9955.6319s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1041150\n",
      "\tspeed: 0.1208s/iter; left time: 8178.9483s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0764355\n",
      "\tspeed: 0.1206s/iter; left time: 8157.0534s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0967276\n",
      "\tspeed: 0.1447s/iter; left time: 9767.2221s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1057422\n",
      "\tspeed: 0.1354s/iter; left time: 9129.7903s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1008978\n",
      "\tspeed: 0.1208s/iter; left time: 8128.2820s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1179423\n",
      "\tspeed: 0.1208s/iter; left time: 8118.3224s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1178501\n",
      "\tspeed: 0.1427s/iter; left time: 9575.1051s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0911569\n",
      "\tspeed: 0.1290s/iter; left time: 8643.8998s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0975429\n",
      "\tspeed: 0.1170s/iter; left time: 7830.1215s\n",
      "Epoch: 5 cost time: 00h:09m:42.32s\n",
      "Epoch: 5 | Train Loss: 0.0987026 Vali Loss: 0.1216048 Test Loss: 0.1345417\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1011339\n",
      "\tspeed: 1.6048s/iter; left time: 107127.3740s\n",
      "\titers: 200, epoch: 6 | loss: 0.0883259\n",
      "\tspeed: 0.1163s/iter; left time: 7753.0863s\n",
      "\titers: 300, epoch: 6 | loss: 0.1075289\n",
      "\tspeed: 0.1029s/iter; left time: 6848.7057s\n",
      "\titers: 400, epoch: 6 | loss: 0.0989359\n",
      "\tspeed: 0.1220s/iter; left time: 8109.9106s\n",
      "\titers: 500, epoch: 6 | loss: 0.0862649\n",
      "\tspeed: 0.1379s/iter; left time: 9147.6659s\n",
      "\titers: 600, epoch: 6 | loss: 0.1002253\n",
      "\tspeed: 0.1120s/iter; left time: 7420.9720s\n",
      "\titers: 700, epoch: 6 | loss: 0.1083656\n",
      "\tspeed: 0.1196s/iter; left time: 7913.3557s\n",
      "\titers: 800, epoch: 6 | loss: 0.0928279\n",
      "\tspeed: 0.1310s/iter; left time: 8653.9960s\n",
      "\titers: 900, epoch: 6 | loss: 0.0807987\n",
      "\tspeed: 0.1508s/iter; left time: 9943.9008s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0948872\n",
      "\tspeed: 0.1199s/iter; left time: 7894.3091s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0827360\n",
      "\tspeed: 0.1199s/iter; left time: 7885.3003s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1174165\n",
      "\tspeed: 0.1297s/iter; left time: 8514.0891s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0992087\n",
      "\tspeed: 0.1508s/iter; left time: 9887.6720s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0848289\n",
      "\tspeed: 0.1175s/iter; left time: 7691.5387s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0922957\n",
      "\tspeed: 0.1208s/iter; left time: 7896.2571s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1061229\n",
      "\tspeed: 0.1203s/iter; left time: 7849.6662s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0937682\n",
      "\tspeed: 0.1396s/iter; left time: 9098.3824s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1026462\n",
      "\tspeed: 0.1311s/iter; left time: 8529.0044s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0818370\n",
      "\tspeed: 0.1283s/iter; left time: 8332.6380s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1049387\n",
      "\tspeed: 0.1408s/iter; left time: 9134.2236s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0856962\n",
      "\tspeed: 0.1174s/iter; left time: 7604.1086s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1132535\n",
      "\tspeed: 0.1507s/iter; left time: 9746.5224s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0780297\n",
      "\tspeed: 0.1217s/iter; left time: 7855.3381s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0942708\n",
      "\tspeed: 0.1310s/iter; left time: 8445.3675s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0917941\n",
      "\tspeed: 0.1501s/iter; left time: 9659.0142s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0872721\n",
      "\tspeed: 0.1201s/iter; left time: 7718.7537s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0825186\n",
      "\tspeed: 0.1230s/iter; left time: 7892.4193s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0905587\n",
      "\tspeed: 0.1488s/iter; left time: 9529.8986s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0887696\n",
      "\tspeed: 0.1205s/iter; left time: 7706.0768s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1022261\n",
      "\tspeed: 0.1208s/iter; left time: 7712.0089s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1016905\n",
      "\tspeed: 0.1310s/iter; left time: 8349.7578s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0889834\n",
      "\tspeed: 0.1464s/iter; left time: 9318.9635s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0868730\n",
      "\tspeed: 0.1206s/iter; left time: 7662.5306s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0905508\n",
      "\tspeed: 0.1394s/iter; left time: 8848.0028s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0885148\n",
      "\tspeed: 0.1385s/iter; left time: 8773.3326s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0922447\n",
      "\tspeed: 0.1204s/iter; left time: 7617.3406s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0886584\n",
      "\tspeed: 0.1511s/iter; left time: 9541.6918s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0922629\n",
      "\tspeed: 0.1305s/iter; left time: 8231.6898s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0939470\n",
      "\tspeed: 0.1208s/iter; left time: 7602.0121s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0905015\n",
      "\tspeed: 0.1538s/iter; left time: 9667.6786s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0758674\n",
      "\tspeed: 0.1298s/iter; left time: 8147.0458s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0957208\n",
      "\tspeed: 0.1210s/iter; left time: 7578.2761s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1053216\n",
      "\tspeed: 0.1543s/iter; left time: 9652.4183s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0906008\n",
      "\tspeed: 0.1260s/iter; left time: 7870.1056s\n",
      "Epoch: 6 cost time: 00h:09m:41.71s\n",
      "Epoch: 6 | Train Loss: 0.0954798 Vali Loss: 0.1227273 Test Loss: 0.1344287\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1033562\n",
      "\tspeed: 1.6175s/iter; left time: 100767.6528s\n",
      "\titers: 200, epoch: 7 | loss: 0.0896851\n",
      "\tspeed: 0.1532s/iter; left time: 9526.4070s\n",
      "\titers: 300, epoch: 7 | loss: 0.0913225\n",
      "\tspeed: 0.1040s/iter; left time: 6456.4733s\n",
      "\titers: 400, epoch: 7 | loss: 0.0853823\n",
      "\tspeed: 0.1185s/iter; left time: 7346.5372s\n",
      "\titers: 500, epoch: 7 | loss: 0.0857644\n",
      "\tspeed: 0.1308s/iter; left time: 8095.6350s\n",
      "\titers: 600, epoch: 7 | loss: 0.0806916\n",
      "\tspeed: 0.1518s/iter; left time: 9381.2032s\n",
      "\titers: 700, epoch: 7 | loss: 0.1021493\n",
      "\tspeed: 0.1204s/iter; left time: 7426.3761s\n",
      "\titers: 800, epoch: 7 | loss: 0.0732420\n",
      "\tspeed: 0.1204s/iter; left time: 7417.3480s\n",
      "\titers: 900, epoch: 7 | loss: 0.0850529\n",
      "\tspeed: 0.1564s/iter; left time: 9618.4855s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0993610\n",
      "\tspeed: 0.1221s/iter; left time: 7493.9789s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1097831\n",
      "\tspeed: 0.1193s/iter; left time: 7315.4069s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1054772\n",
      "\tspeed: 0.1606s/iter; left time: 9829.1026s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0870716\n",
      "\tspeed: 0.1201s/iter; left time: 7338.9015s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0907186\n",
      "\tspeed: 0.1200s/iter; left time: 7320.6886s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0836305\n",
      "\tspeed: 0.1294s/iter; left time: 7883.2776s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0796124\n",
      "\tspeed: 0.1456s/iter; left time: 8849.8643s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0783959\n",
      "\tspeed: 0.1163s/iter; left time: 7062.2697s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1006658\n",
      "\tspeed: 0.1280s/iter; left time: 7754.5098s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0884392\n",
      "\tspeed: 0.1511s/iter; left time: 9141.8232s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0906714\n",
      "\tspeed: 0.1157s/iter; left time: 6985.2262s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0930545\n",
      "\tspeed: 0.1201s/iter; left time: 7241.6238s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0999339\n",
      "\tspeed: 0.1395s/iter; left time: 8397.4509s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0763448\n",
      "\tspeed: 0.1401s/iter; left time: 8417.9997s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0873501\n",
      "\tspeed: 0.1187s/iter; left time: 7120.0993s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0835626\n",
      "\tspeed: 0.1289s/iter; left time: 7721.4880s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0899396\n",
      "\tspeed: 0.1481s/iter; left time: 8858.1208s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0973071\n",
      "\tspeed: 0.1207s/iter; left time: 7202.7669s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0889941\n",
      "\tspeed: 0.1356s/iter; left time: 8083.4355s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0885513\n",
      "\tspeed: 0.1428s/iter; left time: 8495.3955s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0899405\n",
      "\tspeed: 0.1200s/iter; left time: 7127.5966s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0913069\n",
      "\tspeed: 0.1330s/iter; left time: 7889.6326s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0929181\n",
      "\tspeed: 0.1523s/iter; left time: 9018.8924s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0748527\n",
      "\tspeed: 0.1211s/iter; left time: 7153.9634s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0883430\n",
      "\tspeed: 0.1414s/iter; left time: 8344.8554s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0995779\n",
      "\tspeed: 0.1405s/iter; left time: 8277.4893s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0879578\n",
      "\tspeed: 0.1200s/iter; left time: 7057.2977s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0936343\n",
      "\tspeed: 0.1535s/iter; left time: 9008.6502s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0912004\n",
      "\tspeed: 0.1298s/iter; left time: 7604.4086s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0808091\n",
      "\tspeed: 0.1142s/iter; left time: 6681.9484s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1068037\n",
      "\tspeed: 0.1476s/iter; left time: 8621.1588s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0853559\n",
      "\tspeed: 0.1207s/iter; left time: 7038.4170s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1038158\n",
      "\tspeed: 0.1275s/iter; left time: 7419.9829s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1103765\n",
      "\tspeed: 0.1555s/iter; left time: 9036.6214s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0968011\n",
      "\tspeed: 0.1202s/iter; left time: 6971.9976s\n",
      "Epoch: 7 cost time: 00h:09m:46.95s\n",
      "Epoch: 7 | Train Loss: 0.0925967 Vali Loss: 0.1245577 Test Loss: 0.1345884\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03578183427453041, rmse:0.18916086852550507, mae:0.12824216485023499, rse:0.6698496341705322\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 96: 01h:27m:14.27s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-10-31 22:56:28,867] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 22:56:30,027] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 22:56:30,027] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 22:56:30,027] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 22:56:30,130] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 22:56:30,130] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 22:56:30,769] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 22:56:30,770] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 22:56:30,770] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 22:56:30,772] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 22:56:30,772] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 22:56:30,772] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 22:56:30,772] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 22:56:30,772] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 22:56:30,772] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 22:56:30,772] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 22:56:31,058] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 22:56:31,059] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 22:56:31,060] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.82 GB, percent = 25.8%\n",
      "[2024-10-31 22:56:31,178] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 22:56:31,179] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-10-31 22:56:31,179] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.82 GB, percent = 25.8%\n",
      "[2024-10-31 22:56:31,179] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 22:56:31,293] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 22:56:31,294] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-10-31 22:56:31,294] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.82 GB, percent = 25.8%\n",
      "[2024-10-31 22:56:31,295] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 22:56:31,295] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 22:56:31,295] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 22:56:31,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 22:56:31,296] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 22:56:31,296] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 22:56:31,296] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbcee22a5d0>\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 22:56:31,297] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 22:56:31,298] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 22:56:31,299] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1492427\n",
      "\tspeed: 0.1759s/iter; left time: 15624.6134s\n",
      "\titers: 200, epoch: 1 | loss: 0.1605359\n",
      "\tspeed: 0.1582s/iter; left time: 14036.4255s\n",
      "\titers: 300, epoch: 1 | loss: 0.1671229\n",
      "\tspeed: 0.1309s/iter; left time: 11600.1030s\n",
      "\titers: 400, epoch: 1 | loss: 0.1654047\n",
      "\tspeed: 0.1342s/iter; left time: 11882.6741s\n",
      "\titers: 500, epoch: 1 | loss: 0.1528231\n",
      "\tspeed: 0.1695s/iter; left time: 14991.4997s\n",
      "\titers: 600, epoch: 1 | loss: 0.1641308\n",
      "\tspeed: 0.1315s/iter; left time: 11610.8218s\n",
      "\titers: 700, epoch: 1 | loss: 0.1460070\n",
      "\tspeed: 0.1287s/iter; left time: 11351.6210s\n",
      "\titers: 800, epoch: 1 | loss: 0.1177307\n",
      "\tspeed: 0.1677s/iter; left time: 14777.8589s\n",
      "\titers: 900, epoch: 1 | loss: 0.1349947\n",
      "\tspeed: 0.1359s/iter; left time: 11964.0729s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1090521\n",
      "\tspeed: 0.1300s/iter; left time: 11426.6714s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1334336\n",
      "\tspeed: 0.1518s/iter; left time: 13327.7829s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1300411\n",
      "\tspeed: 0.1510s/iter; left time: 13248.9572s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1205211\n",
      "\tspeed: 0.1298s/iter; left time: 11370.3772s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1304281\n",
      "\tspeed: 0.1299s/iter; left time: 11365.7661s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1271410\n",
      "\tspeed: 0.1502s/iter; left time: 13134.8476s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1058479\n",
      "\tspeed: 0.1536s/iter; left time: 13409.6358s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1209870\n",
      "\tspeed: 0.1308s/iter; left time: 11406.5693s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1023959\n",
      "\tspeed: 0.1303s/iter; left time: 11349.5125s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1413429\n",
      "\tspeed: 0.1686s/iter; left time: 14669.8994s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1427599\n",
      "\tspeed: 0.1309s/iter; left time: 11374.1668s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1329999\n",
      "\tspeed: 0.1308s/iter; left time: 11354.7346s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1120558\n",
      "\tspeed: 0.1701s/iter; left time: 14750.0173s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0989436\n",
      "\tspeed: 0.1307s/iter; left time: 11320.3556s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1160462\n",
      "\tspeed: 0.1310s/iter; left time: 11335.3124s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1234167\n",
      "\tspeed: 0.1594s/iter; left time: 13775.9303s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1299602\n",
      "\tspeed: 0.1338s/iter; left time: 11547.6114s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1306166\n",
      "\tspeed: 0.1310s/iter; left time: 11295.8846s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1211876\n",
      "\tspeed: 0.1307s/iter; left time: 11256.8811s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1157711\n",
      "\tspeed: 0.1633s/iter; left time: 14049.9520s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0999164\n",
      "\tspeed: 0.1307s/iter; left time: 11230.1418s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1516199\n",
      "\tspeed: 0.1308s/iter; left time: 11228.7954s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1097987\n",
      "\tspeed: 0.1388s/iter; left time: 11901.9006s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1195762\n",
      "\tspeed: 0.1584s/iter; left time: 13565.2455s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1289778\n",
      "\tspeed: 0.1312s/iter; left time: 11221.4390s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1021916\n",
      "\tspeed: 0.1305s/iter; left time: 11144.0033s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1183287\n",
      "\tspeed: 0.1430s/iter; left time: 12197.8004s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1355938\n",
      "\tspeed: 0.1580s/iter; left time: 13463.0375s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1252127\n",
      "\tspeed: 0.1308s/iter; left time: 11133.9495s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1068365\n",
      "\tspeed: 0.1579s/iter; left time: 13422.0564s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1084060\n",
      "\tspeed: 0.1434s/iter; left time: 12179.0360s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1262050\n",
      "\tspeed: 0.1297s/iter; left time: 11004.6816s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1099667\n",
      "\tspeed: 0.1386s/iter; left time: 11745.8015s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1208501\n",
      "\tspeed: 0.1580s/iter; left time: 13368.3783s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1325888\n",
      "\tspeed: 0.1305s/iter; left time: 11026.9726s\n",
      "Epoch: 1 cost time: 00h:10m:31.01s\n",
      "Epoch: 1 | Train Loss: 0.1270849 Vali Loss: 0.1233267 Test Loss: 0.1336193\n",
      "Validation loss decreased (inf --> 0.123327).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1183373\n",
      "\tspeed: 1.8820s/iter; left time: 158796.6472s\n",
      "\titers: 200, epoch: 2 | loss: 0.1098894\n",
      "\tspeed: 0.1173s/iter; left time: 9884.7211s\n",
      "\titers: 300, epoch: 2 | loss: 0.1131674\n",
      "\tspeed: 0.1078s/iter; left time: 9076.5730s\n",
      "\titers: 400, epoch: 2 | loss: 0.1160400\n",
      "\tspeed: 0.1478s/iter; left time: 12425.9262s\n",
      "\titers: 500, epoch: 2 | loss: 0.0886714\n",
      "\tspeed: 0.1332s/iter; left time: 11188.8918s\n",
      "\titers: 600, epoch: 2 | loss: 0.1193221\n",
      "\tspeed: 0.1204s/iter; left time: 10100.7859s\n",
      "\titers: 700, epoch: 2 | loss: 0.1303138\n",
      "\tspeed: 0.1333s/iter; left time: 11167.4743s\n",
      "\titers: 800, epoch: 2 | loss: 0.1092902\n",
      "\tspeed: 0.1460s/iter; left time: 12215.6480s\n",
      "\titers: 900, epoch: 2 | loss: 0.0996466\n",
      "\tspeed: 0.1081s/iter; left time: 9037.1343s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1157148\n",
      "\tspeed: 0.1077s/iter; left time: 8986.4920s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1112141\n",
      "\tspeed: 0.1562s/iter; left time: 13026.5717s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1180607\n",
      "\tspeed: 0.1179s/iter; left time: 9818.2311s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0878751\n",
      "\tspeed: 0.1091s/iter; left time: 9070.2974s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1204318\n",
      "\tspeed: 0.1312s/iter; left time: 10903.2790s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1055659\n",
      "\tspeed: 0.1453s/iter; left time: 12053.7195s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1180415\n",
      "\tspeed: 0.1200s/iter; left time: 9945.1464s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1256569\n",
      "\tspeed: 0.1194s/iter; left time: 9880.5806s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1117147\n",
      "\tspeed: 0.1552s/iter; left time: 12831.9240s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1046314\n",
      "\tspeed: 0.1204s/iter; left time: 9941.5590s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1071824\n",
      "\tspeed: 0.1159s/iter; left time: 9555.0635s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1128184\n",
      "\tspeed: 0.1525s/iter; left time: 12563.1475s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1227209\n",
      "\tspeed: 0.1254s/iter; left time: 10320.2536s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1197821\n",
      "\tspeed: 0.1193s/iter; left time: 9801.2546s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1249823\n",
      "\tspeed: 0.1435s/iter; left time: 11777.6256s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1261758\n",
      "\tspeed: 0.1356s/iter; left time: 11117.5660s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1112454\n",
      "\tspeed: 0.1205s/iter; left time: 9867.7948s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1138095\n",
      "\tspeed: 0.1210s/iter; left time: 9894.6377s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0994604\n",
      "\tspeed: 0.1531s/iter; left time: 12504.5200s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1096554\n",
      "\tspeed: 0.1264s/iter; left time: 10313.9280s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1068463\n",
      "\tspeed: 0.1203s/iter; left time: 9802.4262s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1206979\n",
      "\tspeed: 0.1309s/iter; left time: 10655.5039s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1085434\n",
      "\tspeed: 0.1472s/iter; left time: 11961.7991s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1045549\n",
      "\tspeed: 0.1198s/iter; left time: 9723.8775s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1207838\n",
      "\tspeed: 0.1294s/iter; left time: 10491.5027s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1154747\n",
      "\tspeed: 0.1443s/iter; left time: 11684.4159s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1074588\n",
      "\tspeed: 0.1055s/iter; left time: 8529.5609s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1014723\n",
      "\tspeed: 0.1022s/iter; left time: 8258.6511s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0964471\n",
      "\tspeed: 0.1439s/iter; left time: 11610.0558s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1228269\n",
      "\tspeed: 0.1314s/iter; left time: 10588.3093s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1099236\n",
      "\tspeed: 0.1203s/iter; left time: 9678.3293s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171050\n",
      "\tspeed: 0.1278s/iter; left time: 10270.6826s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1104717\n",
      "\tspeed: 0.1539s/iter; left time: 12351.8433s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1126500\n",
      "\tspeed: 0.1208s/iter; left time: 9686.3134s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1323302\n",
      "\tspeed: 0.1197s/iter; left time: 9585.9130s\n",
      "Epoch: 2 cost time: 00h:09m:34.51s\n",
      "Epoch: 2 | Train Loss: 0.1139773 Vali Loss: 0.1234052 Test Loss: 0.1342462\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1082153\n",
      "\tspeed: 1.6554s/iter; left time: 132316.4182s\n",
      "\titers: 200, epoch: 3 | loss: 0.1202841\n",
      "\tspeed: 0.1341s/iter; left time: 10705.9098s\n",
      "\titers: 300, epoch: 3 | loss: 0.0996917\n",
      "\tspeed: 0.1199s/iter; left time: 9563.1093s\n",
      "\titers: 400, epoch: 3 | loss: 0.1064174\n",
      "\tspeed: 0.1231s/iter; left time: 9802.1911s\n",
      "\titers: 500, epoch: 3 | loss: 0.1139071\n",
      "\tspeed: 0.1543s/iter; left time: 12272.7726s\n",
      "\titers: 600, epoch: 3 | loss: 0.1215968\n",
      "\tspeed: 0.1196s/iter; left time: 9499.0769s\n",
      "\titers: 700, epoch: 3 | loss: 0.1096018\n",
      "\tspeed: 0.1176s/iter; left time: 9327.3303s\n",
      "\titers: 800, epoch: 3 | loss: 0.1310975\n",
      "\tspeed: 0.1276s/iter; left time: 10111.7013s\n",
      "\titers: 900, epoch: 3 | loss: 0.1056969\n",
      "\tspeed: 0.1388s/iter; left time: 10985.5605s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1081555\n",
      "\tspeed: 0.1203s/iter; left time: 9509.5447s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0982659\n",
      "\tspeed: 0.1199s/iter; left time: 9462.5662s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1056586\n",
      "\tspeed: 0.1561s/iter; left time: 12307.2165s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1162706\n",
      "\tspeed: 0.1237s/iter; left time: 9737.6174s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1264145\n",
      "\tspeed: 0.1206s/iter; left time: 9484.5916s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1110645\n",
      "\tspeed: 0.1285s/iter; left time: 10088.6308s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1157964\n",
      "\tspeed: 0.1517s/iter; left time: 11899.6495s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1245955\n",
      "\tspeed: 0.1197s/iter; left time: 9372.7378s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1219978\n",
      "\tspeed: 0.1198s/iter; left time: 9374.2244s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1215639\n",
      "\tspeed: 0.1205s/iter; left time: 9412.8991s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1222039\n",
      "\tspeed: 0.1306s/iter; left time: 10189.2829s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0978573\n",
      "\tspeed: 0.1453s/iter; left time: 11324.5459s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1149430\n",
      "\tspeed: 0.1081s/iter; left time: 8413.9151s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1145440\n",
      "\tspeed: 0.1199s/iter; left time: 9316.2933s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0993142\n",
      "\tspeed: 0.1195s/iter; left time: 9279.1915s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0932033\n",
      "\tspeed: 0.1389s/iter; left time: 10772.0812s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1083821\n",
      "\tspeed: 0.1368s/iter; left time: 10593.8662s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1158013\n",
      "\tspeed: 0.1200s/iter; left time: 9281.2553s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1378757\n",
      "\tspeed: 0.1197s/iter; left time: 9242.0563s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0924771\n",
      "\tspeed: 0.1498s/iter; left time: 11556.8977s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1077057\n",
      "\tspeed: 0.1285s/iter; left time: 9896.0885s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1156989\n",
      "\tspeed: 0.1200s/iter; left time: 9233.9198s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1000350\n",
      "\tspeed: 0.1201s/iter; left time: 9228.5345s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1146565\n",
      "\tspeed: 0.1473s/iter; left time: 11305.8013s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1231436\n",
      "\tspeed: 0.1330s/iter; left time: 10191.1905s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1224817\n",
      "\tspeed: 0.1205s/iter; left time: 9221.3954s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1114092\n",
      "\tspeed: 0.1202s/iter; left time: 9187.1481s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1154894\n",
      "\tspeed: 0.1499s/iter; left time: 11442.6472s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0983998\n",
      "\tspeed: 0.1279s/iter; left time: 9751.5659s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1058075\n",
      "\tspeed: 0.1201s/iter; left time: 9140.8716s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1094842\n",
      "\tspeed: 0.1188s/iter; left time: 9030.7883s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1029801\n",
      "\tspeed: 0.1496s/iter; left time: 11357.4676s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1099910\n",
      "\tspeed: 0.1292s/iter; left time: 9793.8597s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1054428\n",
      "\tspeed: 0.1187s/iter; left time: 8986.8338s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0913317\n",
      "\tspeed: 0.1405s/iter; left time: 10629.0923s\n",
      "Epoch: 3 cost time: 00h:09m:37.25s\n",
      "Epoch: 3 | Train Loss: 0.1093807 Vali Loss: 0.1236754 Test Loss: 0.1367024\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0959466\n",
      "\tspeed: 1.5961s/iter; left time: 120480.5884s\n",
      "\titers: 200, epoch: 4 | loss: 0.0858854\n",
      "\tspeed: 0.1543s/iter; left time: 11629.1013s\n",
      "\titers: 300, epoch: 4 | loss: 0.0893636\n",
      "\tspeed: 0.1205s/iter; left time: 9073.3773s\n",
      "\titers: 400, epoch: 4 | loss: 0.1088450\n",
      "\tspeed: 0.1207s/iter; left time: 9075.5081s\n",
      "\titers: 500, epoch: 4 | loss: 0.1087373\n",
      "\tspeed: 0.1387s/iter; left time: 10416.5456s\n",
      "\titers: 600, epoch: 4 | loss: 0.1005550\n",
      "\tspeed: 0.1415s/iter; left time: 10609.1024s\n",
      "\titers: 700, epoch: 4 | loss: 0.0969683\n",
      "\tspeed: 0.1145s/iter; left time: 8576.0083s\n",
      "\titers: 800, epoch: 4 | loss: 0.1045174\n",
      "\tspeed: 0.1025s/iter; left time: 7662.7301s\n",
      "\titers: 900, epoch: 4 | loss: 0.1086594\n",
      "\tspeed: 0.1154s/iter; left time: 8615.0976s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1303164\n",
      "\tspeed: 0.1521s/iter; left time: 11344.3107s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1121879\n",
      "\tspeed: 0.1202s/iter; left time: 8954.6761s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1189122\n",
      "\tspeed: 0.1201s/iter; left time: 8934.2357s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1055549\n",
      "\tspeed: 0.1206s/iter; left time: 8956.1861s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1178071\n",
      "\tspeed: 0.1569s/iter; left time: 11641.5839s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1012621\n",
      "\tspeed: 0.1245s/iter; left time: 9221.5964s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1049342\n",
      "\tspeed: 0.1203s/iter; left time: 8899.0036s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1118138\n",
      "\tspeed: 0.1496s/iter; left time: 11049.9114s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1249317\n",
      "\tspeed: 0.1324s/iter; left time: 9766.9226s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1314284\n",
      "\tspeed: 0.1204s/iter; left time: 8874.2336s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1265800\n",
      "\tspeed: 0.1206s/iter; left time: 8874.9240s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1018833\n",
      "\tspeed: 0.1621s/iter; left time: 11909.6924s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1211214\n",
      "\tspeed: 0.1193s/iter; left time: 8756.7578s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0848068\n",
      "\tspeed: 0.1206s/iter; left time: 8839.2830s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1038890\n",
      "\tspeed: 0.1304s/iter; left time: 9545.4433s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1123954\n",
      "\tspeed: 0.1508s/iter; left time: 11020.1195s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0989040\n",
      "\tspeed: 0.1197s/iter; left time: 8736.5010s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1005402\n",
      "\tspeed: 0.1203s/iter; left time: 8764.7511s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1054526\n",
      "\tspeed: 0.1378s/iter; left time: 10027.8447s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1114677\n",
      "\tspeed: 0.1345s/iter; left time: 9773.2574s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1075196\n",
      "\tspeed: 0.1193s/iter; left time: 8657.4846s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0856683\n",
      "\tspeed: 0.1209s/iter; left time: 8765.6871s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1058800\n",
      "\tspeed: 0.1562s/iter; left time: 11306.6176s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1083319\n",
      "\tspeed: 0.1199s/iter; left time: 8668.2359s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0981609\n",
      "\tspeed: 0.1197s/iter; left time: 8641.1723s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0921983\n",
      "\tspeed: 0.1209s/iter; left time: 8717.5124s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1049358\n",
      "\tspeed: 0.1384s/iter; left time: 9961.4236s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0929390\n",
      "\tspeed: 0.1358s/iter; left time: 9760.2594s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0982588\n",
      "\tspeed: 0.1064s/iter; left time: 7638.2370s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0918985\n",
      "\tspeed: 0.1196s/iter; left time: 8573.8076s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1195650\n",
      "\tspeed: 0.1472s/iter; left time: 10534.3494s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1090801\n",
      "\tspeed: 0.1351s/iter; left time: 9656.4173s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1110307\n",
      "\tspeed: 0.1204s/iter; left time: 8595.5347s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1110436\n",
      "\tspeed: 0.1208s/iter; left time: 8612.0902s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1014118\n",
      "\tspeed: 0.1569s/iter; left time: 11168.4169s\n",
      "Epoch: 4 cost time: 00h:09m:36.15s\n",
      "Epoch: 4 | Train Loss: 0.1046495 Vali Loss: 0.1247817 Test Loss: 0.1472365\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1008341\n",
      "\tspeed: 1.5868s/iter; left time: 112723.0239s\n",
      "\titers: 200, epoch: 5 | loss: 0.1144764\n",
      "\tspeed: 0.1347s/iter; left time: 9557.0615s\n",
      "\titers: 300, epoch: 5 | loss: 0.1179324\n",
      "\tspeed: 0.1203s/iter; left time: 8521.7536s\n",
      "\titers: 400, epoch: 5 | loss: 0.1061344\n",
      "\tspeed: 0.1192s/iter; left time: 8433.2988s\n",
      "\titers: 500, epoch: 5 | loss: 0.0869319\n",
      "\tspeed: 0.1572s/iter; left time: 11104.2545s\n",
      "\titers: 600, epoch: 5 | loss: 0.0929676\n",
      "\tspeed: 0.1174s/iter; left time: 8280.7519s\n",
      "\titers: 700, epoch: 5 | loss: 0.1068786\n",
      "\tspeed: 0.1180s/iter; left time: 8309.6225s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935982\n",
      "\tspeed: 0.1290s/iter; left time: 9072.0218s\n",
      "\titers: 900, epoch: 5 | loss: 0.1054375\n",
      "\tspeed: 0.1483s/iter; left time: 10413.6047s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1086309\n",
      "\tspeed: 0.1199s/iter; left time: 8410.4937s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1122420\n",
      "\tspeed: 0.1166s/iter; left time: 8165.4214s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1004656\n",
      "\tspeed: 0.1519s/iter; left time: 10622.2138s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0959615\n",
      "\tspeed: 0.1260s/iter; left time: 8802.5739s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1126448\n",
      "\tspeed: 0.1200s/iter; left time: 8370.1891s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0902287\n",
      "\tspeed: 0.1274s/iter; left time: 8868.4050s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0997650\n",
      "\tspeed: 0.1420s/iter; left time: 9872.1772s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1175994\n",
      "\tspeed: 0.1029s/iter; left time: 7146.6681s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1059145\n",
      "\tspeed: 0.1023s/iter; left time: 7089.9715s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0982435\n",
      "\tspeed: 0.1357s/iter; left time: 9394.3629s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1148793\n",
      "\tspeed: 0.1326s/iter; left time: 9165.0599s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1237828\n",
      "\tspeed: 0.1190s/iter; left time: 8216.2611s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0931523\n",
      "\tspeed: 0.1202s/iter; left time: 8283.0429s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0861305\n",
      "\tspeed: 0.1595s/iter; left time: 10981.7679s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0990851\n",
      "\tspeed: 0.1225s/iter; left time: 8422.9487s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0847341\n",
      "\tspeed: 0.1206s/iter; left time: 8277.0872s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1076903\n",
      "\tspeed: 0.1197s/iter; left time: 8204.3958s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1186636\n",
      "\tspeed: 0.1238s/iter; left time: 8473.0081s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0813073\n",
      "\tspeed: 0.1583s/iter; left time: 10818.1232s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0954584\n",
      "\tspeed: 0.1203s/iter; left time: 8206.0969s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0905743\n",
      "\tspeed: 0.1205s/iter; left time: 8209.1790s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1002901\n",
      "\tspeed: 0.1210s/iter; left time: 8232.3391s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0847461\n",
      "\tspeed: 0.1468s/iter; left time: 9974.2529s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1024117\n",
      "\tspeed: 0.1299s/iter; left time: 8810.8269s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0998646\n",
      "\tspeed: 0.1202s/iter; left time: 8141.5231s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0978041\n",
      "\tspeed: 0.1402s/iter; left time: 9480.9619s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1110223\n",
      "\tspeed: 0.1419s/iter; left time: 9581.4196s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0857197\n",
      "\tspeed: 0.1206s/iter; left time: 8131.9129s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0900995\n",
      "\tspeed: 0.1198s/iter; left time: 8067.1218s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0950421\n",
      "\tspeed: 0.1274s/iter; left time: 8566.8646s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1002941\n",
      "\tspeed: 0.1552s/iter; left time: 10422.8892s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1072492\n",
      "\tspeed: 0.1198s/iter; left time: 8032.6979s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0923533\n",
      "\tspeed: 0.1203s/iter; left time: 8053.6159s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0978186\n",
      "\tspeed: 0.1201s/iter; left time: 8027.4373s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0912306\n",
      "\tspeed: 0.1469s/iter; left time: 9801.0463s\n",
      "Epoch: 5 cost time: 00h:09m:34.51s\n",
      "Epoch: 5 | Train Loss: 0.1001263 Vali Loss: 0.1304869 Test Loss: 0.1511040\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1040925\n",
      "\tspeed: 1.5384s/iter; left time: 102441.9456s\n",
      "\titers: 200, epoch: 6 | loss: 0.1047199\n",
      "\tspeed: 0.1204s/iter; left time: 8007.8539s\n",
      "\titers: 300, epoch: 6 | loss: 0.0804931\n",
      "\tspeed: 0.1299s/iter; left time: 8622.9246s\n",
      "\titers: 400, epoch: 6 | loss: 0.0875467\n",
      "\tspeed: 0.1518s/iter; left time: 10065.5801s\n",
      "\titers: 500, epoch: 6 | loss: 0.0984672\n",
      "\tspeed: 0.1201s/iter; left time: 7947.5729s\n",
      "\titers: 600, epoch: 6 | loss: 0.1009006\n",
      "\tspeed: 0.1207s/iter; left time: 7977.4011s\n",
      "\titers: 700, epoch: 6 | loss: 0.0918718\n",
      "\tspeed: 0.1205s/iter; left time: 7952.0579s\n",
      "\titers: 800, epoch: 6 | loss: 0.1056673\n",
      "\tspeed: 0.1408s/iter; left time: 9279.4978s\n",
      "\titers: 900, epoch: 6 | loss: 0.0862563\n",
      "\tspeed: 0.1413s/iter; left time: 9293.7271s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0999702\n",
      "\tspeed: 0.1204s/iter; left time: 7908.6448s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0961189\n",
      "\tspeed: 0.1181s/iter; left time: 7746.1876s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1147054\n",
      "\tspeed: 0.1191s/iter; left time: 7801.8267s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0912160\n",
      "\tspeed: 0.1550s/iter; left time: 10137.3324s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0981800\n",
      "\tspeed: 0.1254s/iter; left time: 8187.2322s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0838766\n",
      "\tspeed: 0.1200s/iter; left time: 7823.5905s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0883116\n",
      "\tspeed: 0.1342s/iter; left time: 8733.6978s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0896513\n",
      "\tspeed: 0.1454s/iter; left time: 9449.1071s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0974333\n",
      "\tspeed: 0.1194s/iter; left time: 7748.9081s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0899982\n",
      "\tspeed: 0.1225s/iter; left time: 7936.1282s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0925732\n",
      "\tspeed: 0.1552s/iter; left time: 10042.4806s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0867949\n",
      "\tspeed: 0.1167s/iter; left time: 7534.8939s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1011756\n",
      "\tspeed: 0.1209s/iter; left time: 7796.6268s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0931246\n",
      "\tspeed: 0.1462s/iter; left time: 9411.2734s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0912003\n",
      "\tspeed: 0.1327s/iter; left time: 8528.8997s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0813842\n",
      "\tspeed: 0.1197s/iter; left time: 7682.4912s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0974265\n",
      "\tspeed: 0.1393s/iter; left time: 8927.2236s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1037935\n",
      "\tspeed: 0.1413s/iter; left time: 9044.8802s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0945832\n",
      "\tspeed: 0.1192s/iter; left time: 7614.9697s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0993364\n",
      "\tspeed: 0.1460s/iter; left time: 9314.7197s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0983878\n",
      "\tspeed: 0.1345s/iter; left time: 8568.9704s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0967573\n",
      "\tspeed: 0.1196s/iter; left time: 7606.3067s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0809352\n",
      "\tspeed: 0.1518s/iter; left time: 9636.8706s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0907727\n",
      "\tspeed: 0.1130s/iter; left time: 7164.3198s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1019389\n",
      "\tspeed: 0.1173s/iter; left time: 7423.1448s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1059106\n",
      "\tspeed: 0.1601s/iter; left time: 10113.9241s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0917989\n",
      "\tspeed: 0.1216s/iter; left time: 7671.2563s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0842525\n",
      "\tspeed: 0.1309s/iter; left time: 8247.1992s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0876396\n",
      "\tspeed: 0.1517s/iter; left time: 9541.4374s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1118588\n",
      "\tspeed: 0.1196s/iter; left time: 7510.1725s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0991682\n",
      "\tspeed: 0.1202s/iter; left time: 7538.5674s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0930842\n",
      "\tspeed: 0.1509s/iter; left time: 9446.4604s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0981919\n",
      "\tspeed: 0.1290s/iter; left time: 8064.3315s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0870969\n",
      "\tspeed: 0.1197s/iter; left time: 7467.3843s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0964762\n",
      "\tspeed: 0.1617s/iter; left time: 10072.3223s\n",
      "Epoch: 6 cost time: 00h:09m:42.76s\n",
      "Epoch: 6 | Train Loss: 0.0960726 Vali Loss: 0.1291619 Test Loss: 0.1511165\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03767301142215729, rmse:0.19409537315368652, mae:0.1336192935705185, rse:0.6876434087753296\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 168: 01h:14m:40.58s\n",
      "\n",
      "Intermediate time for DE: 05h:19m:38.50s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143005\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-01 00:11:10,387] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-01 00:11:11,589] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-01 00:11:11,590] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-01 00:11:11,590] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-01 00:11:11,686] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-01 00:11:11,686] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-01 00:11:12,372] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-01 00:11:12,373] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-01 00:11:12,373] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-01 00:11:12,375] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-01 00:11:12,375] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-01 00:11:12,375] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-01 00:11:12,375] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-01 00:11:12,375] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-01 00:11:12,375] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-01 00:11:12,375] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-01 00:11:12,681] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-01 00:11:12,682] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-01 00:11:12,683] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.83 GB, percent = 25.8%\n",
      "[2024-11-01 00:11:12,802] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-01 00:11:12,803] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 00:11:12,804] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.83 GB, percent = 25.8%\n",
      "[2024-11-01 00:11:12,804] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-01 00:11:12,918] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-01 00:11:12,919] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-01 00:11:12,919] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 194.83 GB, percent = 25.8%\n",
      "[2024-11-01 00:11:12,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-01 00:11:12,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-01 00:11:12,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-01 00:11:12,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f20dcac4390>\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-01 00:11:12,921] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-01 00:11:12,922] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-01 00:11:12,923] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1466742\n",
      "\tspeed: 0.1842s/iter; left time: 16443.0614s\n",
      "\titers: 200, epoch: 1 | loss: 0.1377795\n",
      "\tspeed: 0.1594s/iter; left time: 14209.3235s\n",
      "\titers: 300, epoch: 1 | loss: 0.1637115\n",
      "\tspeed: 0.1307s/iter; left time: 11639.4050s\n",
      "\titers: 400, epoch: 1 | loss: 0.1077301\n",
      "\tspeed: 0.1404s/iter; left time: 12492.7699s\n",
      "\titers: 500, epoch: 1 | loss: 0.1226833\n",
      "\tspeed: 0.1597s/iter; left time: 14187.4399s\n",
      "\titers: 600, epoch: 1 | loss: 0.1199201\n",
      "\tspeed: 0.1309s/iter; left time: 11616.4899s\n",
      "\titers: 700, epoch: 1 | loss: 0.1068131\n",
      "\tspeed: 0.1367s/iter; left time: 12119.8537s\n",
      "\titers: 800, epoch: 1 | loss: 0.1092960\n",
      "\tspeed: 0.1644s/iter; left time: 14561.7574s\n",
      "\titers: 900, epoch: 1 | loss: 0.0919815\n",
      "\tspeed: 0.1307s/iter; left time: 11565.8034s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1038064\n",
      "\tspeed: 0.1311s/iter; left time: 11581.2634s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0846845\n",
      "\tspeed: 0.1308s/iter; left time: 11545.6549s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0949691\n",
      "\tspeed: 0.1669s/iter; left time: 14712.2023s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0737247\n",
      "\tspeed: 0.1314s/iter; left time: 11572.9310s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1031198\n",
      "\tspeed: 0.1307s/iter; left time: 11497.0322s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0780963\n",
      "\tspeed: 0.1309s/iter; left time: 11504.5312s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978673\n",
      "\tspeed: 0.1553s/iter; left time: 13632.8140s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1247796\n",
      "\tspeed: 0.1432s/iter; left time: 12550.8239s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062685\n",
      "\tspeed: 0.1306s/iter; left time: 11437.6817s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1049088\n",
      "\tspeed: 0.1307s/iter; left time: 11434.9713s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0870789\n",
      "\tspeed: 0.1656s/iter; left time: 14467.9332s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0814199\n",
      "\tspeed: 0.1308s/iter; left time: 11414.4713s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0885388\n",
      "\tspeed: 0.1305s/iter; left time: 11373.9602s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0876981\n",
      "\tspeed: 0.1609s/iter; left time: 14010.0581s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0750865\n",
      "\tspeed: 0.1216s/iter; left time: 10571.3052s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0770076\n",
      "\tspeed: 0.1273s/iter; left time: 11053.6354s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0930856\n",
      "\tspeed: 0.1283s/iter; left time: 11132.2442s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0914229\n",
      "\tspeed: 0.1591s/iter; left time: 13787.7470s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0742800\n",
      "\tspeed: 0.1309s/iter; left time: 11328.0202s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0699159\n",
      "\tspeed: 0.1308s/iter; left time: 11308.0468s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0888919\n",
      "\tspeed: 0.1500s/iter; left time: 12950.5898s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0732918\n",
      "\tspeed: 0.1466s/iter; left time: 12647.7381s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0881244\n",
      "\tspeed: 0.1310s/iter; left time: 11284.1797s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0822907\n",
      "\tspeed: 0.1482s/iter; left time: 12750.7046s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0753193\n",
      "\tspeed: 0.1484s/iter; left time: 12758.1894s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0816539\n",
      "\tspeed: 0.1306s/iter; left time: 11212.1935s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0759263\n",
      "\tspeed: 0.1448s/iter; left time: 12416.2454s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0969475\n",
      "\tspeed: 0.1547s/iter; left time: 13250.6012s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0810152\n",
      "\tspeed: 0.1307s/iter; left time: 11180.9261s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0943139\n",
      "\tspeed: 0.1310s/iter; left time: 11195.9193s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0698772\n",
      "\tspeed: 0.1524s/iter; left time: 13013.0314s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0704658\n",
      "\tspeed: 0.1465s/iter; left time: 12493.8278s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1035011\n",
      "\tspeed: 0.1309s/iter; left time: 11144.2436s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0949263\n",
      "\tspeed: 0.1309s/iter; left time: 11134.6980s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0885897\n",
      "\tspeed: 0.1612s/iter; left time: 13698.8002s\n",
      "Epoch: 1 cost time: 00h:10m:30.32s\n",
      "Epoch: 1 | Train Loss: 0.0973537 Vali Loss: 0.0928246 Test Loss: 0.1050737\n",
      "Validation loss decreased (inf --> 0.092825).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0701588\n",
      "\tspeed: 1.8576s/iter; left time: 157508.3623s\n",
      "\titers: 200, epoch: 2 | loss: 0.1044648\n",
      "\tspeed: 0.1312s/iter; left time: 11115.3616s\n",
      "\titers: 300, epoch: 2 | loss: 0.0884274\n",
      "\tspeed: 0.1523s/iter; left time: 12886.2732s\n",
      "\titers: 400, epoch: 2 | loss: 0.1147017\n",
      "\tspeed: 0.1212s/iter; left time: 10241.6494s\n",
      "\titers: 500, epoch: 2 | loss: 0.0937675\n",
      "\tspeed: 0.1207s/iter; left time: 10188.4041s\n",
      "\titers: 600, epoch: 2 | loss: 0.0921706\n",
      "\tspeed: 0.1400s/iter; left time: 11801.4006s\n",
      "\titers: 700, epoch: 2 | loss: 0.0930738\n",
      "\tspeed: 0.1438s/iter; left time: 12108.9323s\n",
      "\titers: 800, epoch: 2 | loss: 0.1000150\n",
      "\tspeed: 0.1203s/iter; left time: 10118.3086s\n",
      "\titers: 900, epoch: 2 | loss: 0.0975661\n",
      "\tspeed: 0.1206s/iter; left time: 10125.7000s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0700671\n",
      "\tspeed: 0.1504s/iter; left time: 12614.6083s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0929756\n",
      "\tspeed: 0.1308s/iter; left time: 10960.3850s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0745666\n",
      "\tspeed: 0.1212s/iter; left time: 10140.9604s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0924439\n",
      "\tspeed: 0.1288s/iter; left time: 10766.6960s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1111560\n",
      "\tspeed: 0.1487s/iter; left time: 12418.8436s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0793124\n",
      "\tspeed: 0.1200s/iter; left time: 10009.4415s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0794686\n",
      "\tspeed: 0.1206s/iter; left time: 10046.2119s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0875069\n",
      "\tspeed: 0.1401s/iter; left time: 11658.2405s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0863800\n",
      "\tspeed: 0.1401s/iter; left time: 11639.4421s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0864655\n",
      "\tspeed: 0.1208s/iter; left time: 10027.6648s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0874661\n",
      "\tspeed: 0.1212s/iter; left time: 10045.5625s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0986785\n",
      "\tspeed: 0.1603s/iter; left time: 13268.1135s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0935980\n",
      "\tspeed: 0.1204s/iter; left time: 9960.0649s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0926060\n",
      "\tspeed: 0.1209s/iter; left time: 9987.4815s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731041\n",
      "\tspeed: 0.1347s/iter; left time: 11113.8337s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0878800\n",
      "\tspeed: 0.1456s/iter; left time: 11998.0129s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1000494\n",
      "\tspeed: 0.1207s/iter; left time: 9929.3688s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0909730\n",
      "\tspeed: 0.1380s/iter; left time: 11339.6253s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1120826\n",
      "\tspeed: 0.1391s/iter; left time: 11420.8263s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0753991\n",
      "\tspeed: 0.1172s/iter; left time: 9607.2606s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0918223\n",
      "\tspeed: 0.1257s/iter; left time: 10290.3549s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1169008\n",
      "\tspeed: 0.1502s/iter; left time: 12289.2388s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0799625\n",
      "\tspeed: 0.1163s/iter; left time: 9500.7189s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0924724\n",
      "\tspeed: 0.1223s/iter; left time: 9975.1972s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0893005\n",
      "\tspeed: 0.1528s/iter; left time: 12453.7856s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0870048\n",
      "\tspeed: 0.1260s/iter; left time: 10259.1421s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0749805\n",
      "\tspeed: 0.1197s/iter; left time: 9729.8455s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0773853\n",
      "\tspeed: 0.1432s/iter; left time: 11626.2243s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0819646\n",
      "\tspeed: 0.1365s/iter; left time: 11067.6076s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0823601\n",
      "\tspeed: 0.1201s/iter; left time: 9723.3594s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0686158\n",
      "\tspeed: 0.1081s/iter; left time: 8746.2922s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0982631\n",
      "\tspeed: 0.1515s/iter; left time: 12239.3123s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0787107\n",
      "\tspeed: 0.1245s/iter; left time: 10046.6133s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0879422\n",
      "\tspeed: 0.1208s/iter; left time: 9731.8215s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0896147\n",
      "\tspeed: 0.1207s/iter; left time: 9712.4840s\n",
      "Epoch: 2 cost time: 00h:09m:44.82s\n",
      "Epoch: 2 | Train Loss: 0.0860910 Vali Loss: 0.0922121 Test Loss: 0.1065872\n",
      "Validation loss decreased (0.092825 --> 0.092212).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0907432\n",
      "\tspeed: 1.6869s/iter; left time: 135502.4043s\n",
      "\titers: 200, epoch: 3 | loss: 0.0779589\n",
      "\tspeed: 0.1136s/iter; left time: 9117.4498s\n",
      "\titers: 300, epoch: 3 | loss: 0.0885364\n",
      "\tspeed: 0.1307s/iter; left time: 10470.4521s\n",
      "\titers: 400, epoch: 3 | loss: 0.0848934\n",
      "\tspeed: 0.1489s/iter; left time: 11912.2564s\n",
      "\titers: 500, epoch: 3 | loss: 0.0842507\n",
      "\tspeed: 0.1187s/iter; left time: 9491.0705s\n",
      "\titers: 600, epoch: 3 | loss: 0.0788545\n",
      "\tspeed: 0.1174s/iter; left time: 9373.1155s\n",
      "\titers: 700, epoch: 3 | loss: 0.0803074\n",
      "\tspeed: 0.1437s/iter; left time: 11458.2091s\n",
      "\titers: 800, epoch: 3 | loss: 0.0902780\n",
      "\tspeed: 0.1398s/iter; left time: 11134.3400s\n",
      "\titers: 900, epoch: 3 | loss: 0.0700542\n",
      "\tspeed: 0.1166s/iter; left time: 9272.4577s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0909660\n",
      "\tspeed: 0.1219s/iter; left time: 9678.5350s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0937846\n",
      "\tspeed: 0.1593s/iter; left time: 12634.9898s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0607558\n",
      "\tspeed: 0.1205s/iter; left time: 9547.3656s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0803862\n",
      "\tspeed: 0.1156s/iter; left time: 9150.2100s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0932646\n",
      "\tspeed: 0.1392s/iter; left time: 10998.5723s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0733327\n",
      "\tspeed: 0.1356s/iter; left time: 10699.3295s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0595024\n",
      "\tspeed: 0.1183s/iter; left time: 9326.1918s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0652814\n",
      "\tspeed: 0.1128s/iter; left time: 8881.0725s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0982525\n",
      "\tspeed: 0.1562s/iter; left time: 12284.1296s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0716447\n",
      "\tspeed: 0.1181s/iter; left time: 9277.6962s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0835190\n",
      "\tspeed: 0.1206s/iter; left time: 9456.3632s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0834547\n",
      "\tspeed: 0.1342s/iter; left time: 10513.1120s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0948817\n",
      "\tspeed: 0.1420s/iter; left time: 11108.8163s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0844729\n",
      "\tspeed: 0.1205s/iter; left time: 9417.7414s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0765860\n",
      "\tspeed: 0.1191s/iter; left time: 9291.5435s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0919627\n",
      "\tspeed: 0.1331s/iter; left time: 10374.9443s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0797675\n",
      "\tspeed: 0.1480s/iter; left time: 11516.4298s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0741416\n",
      "\tspeed: 0.1166s/iter; left time: 9060.4424s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0829531\n",
      "\tspeed: 0.1194s/iter; left time: 9271.0168s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0889192\n",
      "\tspeed: 0.1522s/iter; left time: 11799.2662s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0789738\n",
      "\tspeed: 0.1227s/iter; left time: 9503.6639s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0949245\n",
      "\tspeed: 0.1178s/iter; left time: 9110.6579s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0981605\n",
      "\tspeed: 0.1266s/iter; left time: 9774.1385s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0789458\n",
      "\tspeed: 0.1517s/iter; left time: 11703.3896s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0741549\n",
      "\tspeed: 0.1158s/iter; left time: 8918.5363s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0773597\n",
      "\tspeed: 0.1190s/iter; left time: 9153.1867s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0935745\n",
      "\tspeed: 0.1169s/iter; left time: 8980.2513s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0769829\n",
      "\tspeed: 0.1498s/iter; left time: 11496.1004s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0949859\n",
      "\tspeed: 0.1122s/iter; left time: 8598.6435s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0808080\n",
      "\tspeed: 0.1210s/iter; left time: 9256.3264s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0665414\n",
      "\tspeed: 0.1203s/iter; left time: 9196.6309s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0754559\n",
      "\tspeed: 0.1588s/iter; left time: 12120.8435s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0699142\n",
      "\tspeed: 0.1162s/iter; left time: 8858.6229s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0632910\n",
      "\tspeed: 0.1161s/iter; left time: 8840.1513s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0808305\n",
      "\tspeed: 0.1171s/iter; left time: 8898.7474s\n",
      "Epoch: 3 cost time: 00h:09m:31.88s\n",
      "Epoch: 3 | Train Loss: 0.0838997 Vali Loss: 0.0905142 Test Loss: 0.1035439\n",
      "Validation loss decreased (0.092212 --> 0.090514).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0914321\n",
      "\tspeed: 1.6429s/iter; left time: 124624.1522s\n",
      "\titers: 200, epoch: 4 | loss: 0.0923484\n",
      "\tspeed: 0.1203s/iter; left time: 9116.1862s\n",
      "\titers: 300, epoch: 4 | loss: 0.0886503\n",
      "\tspeed: 0.1211s/iter; left time: 9160.7213s\n",
      "\titers: 400, epoch: 4 | loss: 0.0890385\n",
      "\tspeed: 0.1213s/iter; left time: 9167.1142s\n",
      "\titers: 500, epoch: 4 | loss: 0.0889764\n",
      "\tspeed: 0.1208s/iter; left time: 9116.7667s\n",
      "\titers: 600, epoch: 4 | loss: 0.0682504\n",
      "\tspeed: 0.1213s/iter; left time: 9141.7338s\n",
      "\titers: 700, epoch: 4 | loss: 0.0858520\n",
      "\tspeed: 0.1488s/iter; left time: 11198.2889s\n",
      "\titers: 800, epoch: 4 | loss: 0.0807579\n",
      "\tspeed: 0.1257s/iter; left time: 9450.8840s\n",
      "\titers: 900, epoch: 4 | loss: 0.0771627\n",
      "\tspeed: 0.1181s/iter; left time: 8864.3115s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0891313\n",
      "\tspeed: 0.1200s/iter; left time: 8996.8968s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0808600\n",
      "\tspeed: 0.1563s/iter; left time: 11697.9952s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1001335\n",
      "\tspeed: 0.1143s/iter; left time: 8547.2350s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1131133\n",
      "\tspeed: 0.1240s/iter; left time: 9258.4309s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0778967\n",
      "\tspeed: 0.1579s/iter; left time: 11774.3014s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0659208\n",
      "\tspeed: 0.1216s/iter; left time: 9055.3438s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0821595\n",
      "\tspeed: 0.1237s/iter; left time: 9200.6234s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0919877\n",
      "\tspeed: 0.1582s/iter; left time: 11745.4709s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0923949\n",
      "\tspeed: 0.1196s/iter; left time: 8871.0453s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0738678\n",
      "\tspeed: 0.1201s/iter; left time: 8895.2043s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0819451\n",
      "\tspeed: 0.1492s/iter; left time: 11037.6640s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0814312\n",
      "\tspeed: 0.1319s/iter; left time: 9740.8214s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0888639\n",
      "\tspeed: 0.1207s/iter; left time: 8904.7677s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1003453\n",
      "\tspeed: 0.1346s/iter; left time: 9912.0785s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0999414\n",
      "\tspeed: 0.1440s/iter; left time: 10590.6623s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0742211\n",
      "\tspeed: 0.1211s/iter; left time: 8897.1742s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0718067\n",
      "\tspeed: 0.1223s/iter; left time: 8973.5728s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0911036\n",
      "\tspeed: 0.1327s/iter; left time: 9719.6533s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877281\n",
      "\tspeed: 0.1527s/iter; left time: 11174.6643s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0797919\n",
      "\tspeed: 0.1209s/iter; left time: 8832.0852s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0775673\n",
      "\tspeed: 0.1209s/iter; left time: 8817.5974s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0879129\n",
      "\tspeed: 0.1212s/iter; left time: 8828.3852s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0780287\n",
      "\tspeed: 0.1549s/iter; left time: 11268.7987s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0876071\n",
      "\tspeed: 0.1193s/iter; left time: 8669.0646s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0757971\n",
      "\tspeed: 0.1216s/iter; left time: 8823.1363s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0894305\n",
      "\tspeed: 0.1210s/iter; left time: 8768.8092s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0851934\n",
      "\tspeed: 0.1383s/iter; left time: 10010.5680s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0877569\n",
      "\tspeed: 0.1398s/iter; left time: 10101.9393s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0781913\n",
      "\tspeed: 0.1208s/iter; left time: 8716.1955s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0874085\n",
      "\tspeed: 0.1212s/iter; left time: 8730.6403s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0781264\n",
      "\tspeed: 0.1213s/iter; left time: 8728.2010s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0745134\n",
      "\tspeed: 0.1534s/iter; left time: 11020.3506s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0732169\n",
      "\tspeed: 0.1220s/iter; left time: 8756.0525s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0699919\n",
      "\tspeed: 0.1079s/iter; left time: 7733.4768s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0951408\n",
      "\tspeed: 0.1178s/iter; left time: 8426.6895s\n",
      "Epoch: 4 cost time: 00h:09m:39.12s\n",
      "Epoch: 4 | Train Loss: 0.0825824 Vali Loss: 0.0908698 Test Loss: 0.1047861\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Capture and log output in real-time\u001b[39;00m\n\u001b[1;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 54\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Country': 'DE',\n",
       "  'Pred_len': 24,\n",
       "  'MSE': 0.02273400127887726,\n",
       "  'RMSE': 0.15077798068523407,\n",
       "  'MAE': 0.09537788480520248},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 96,\n",
       "  'MSE': 0.03578183427453041,\n",
       "  'RMSE': 0.18916086852550507,\n",
       "  'MAE': 0.12824216485023499},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 168,\n",
       "  'MSE': 0.03767301142215729,\n",
       "  'RMSE': 0.19409537315368652,\n",
       "  'MAE': 0.1336192935705185}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timellm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['GB', 'ES', 'FR', 'IT']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
