{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=10\n",
    "learning_rate=0.001 # 10^-3\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 - shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-30 23:20:13,857] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 23:20:14,955] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 23:20:14,955] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-30 23:20:14,955] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-30 23:20:15,068] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-30 23:20:15,068] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-30 23:20:15,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-30 23:20:15,725] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-30 23:20:15,725] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 23:20:15,727] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-30 23:20:15,727] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-30 23:20:15,727] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-30 23:20:15,727] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 23:20:15,727] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 23:20:15,727] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-30 23:20:15,727] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 23:20:15,998] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 23:20:15,999] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-30 23:20:15,999] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.72 GB, percent = 10.0%\n",
      "[2024-10-30 23:20:16,117] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 23:20:16,118] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.74 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-30 23:20:16,119] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.72 GB, percent = 10.0%\n",
      "[2024-10-30 23:20:16,119] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-30 23:20:16,233] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 23:20:16,234] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-30 23:20:16,234] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.72 GB, percent = 10.0%\n",
      "[2024-10-30 23:20:16,235] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-30 23:20:16,235] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-30 23:20:16,235] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-30 23:20:16,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-30 23:20:16,235] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f25824a79d0>\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 23:20:16,236] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 23:20:16,237] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 23:20:16,238] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1374278\n",
      "\tspeed: 0.2212s/iter; left time: 7885.9011s\n",
      "\titers: 200, epoch: 1 | loss: 0.1194729\n",
      "\tspeed: 0.1111s/iter; left time: 3947.9689s\n",
      "\titers: 300, epoch: 1 | loss: 0.1074283\n",
      "\tspeed: 0.1108s/iter; left time: 3928.3219s\n",
      "\titers: 400, epoch: 1 | loss: 0.1297738\n",
      "\tspeed: 0.1001s/iter; left time: 3538.2176s\n",
      "\titers: 500, epoch: 1 | loss: 0.1215319\n",
      "\tspeed: 0.1050s/iter; left time: 3700.6831s\n",
      "\titers: 600, epoch: 1 | loss: 0.1418181\n",
      "\tspeed: 0.1109s/iter; left time: 3898.3723s\n",
      "\titers: 700, epoch: 1 | loss: 0.1255434\n",
      "\tspeed: 0.1093s/iter; left time: 3831.4828s\n",
      "\titers: 800, epoch: 1 | loss: 0.1307213\n",
      "\tspeed: 0.1109s/iter; left time: 3874.3535s\n",
      "\titers: 900, epoch: 1 | loss: 0.1254326\n",
      "\tspeed: 0.1109s/iter; left time: 3866.1620s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1249642\n",
      "\tspeed: 0.1108s/iter; left time: 3851.4893s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1294816\n",
      "\tspeed: 0.1108s/iter; left time: 3840.4892s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1161849\n",
      "\tspeed: 0.1107s/iter; left time: 3823.2230s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1370030\n",
      "\tspeed: 0.1102s/iter; left time: 3797.1155s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1164477\n",
      "\tspeed: 0.1111s/iter; left time: 3816.7840s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1434606\n",
      "\tspeed: 0.1112s/iter; left time: 3809.3223s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1297308\n",
      "\tspeed: 0.1111s/iter; left time: 3794.8796s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1299761\n",
      "\tspeed: 0.1112s/iter; left time: 3787.0571s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1609686\n",
      "\tspeed: 0.1112s/iter; left time: 3776.1462s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1195945\n",
      "\tspeed: 0.1107s/iter; left time: 3748.4199s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1479447\n",
      "\tspeed: 0.1109s/iter; left time: 3742.2954s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1250950\n",
      "\tspeed: 0.1109s/iter; left time: 3730.2909s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1281286\n",
      "\tspeed: 0.1108s/iter; left time: 3718.4255s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1401293\n",
      "\tspeed: 0.1108s/iter; left time: 3706.7727s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1119573\n",
      "\tspeed: 0.1108s/iter; left time: 3696.2974s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1059771\n",
      "\tspeed: 0.0953s/iter; left time: 3168.9658s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1159924\n",
      "\tspeed: 0.1031s/iter; left time: 3417.7203s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1203575\n",
      "\tspeed: 0.1110s/iter; left time: 3669.0380s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1354804\n",
      "\tspeed: 0.1107s/iter; left time: 3648.8046s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1454908\n",
      "\tspeed: 0.1107s/iter; left time: 3636.8604s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1328376\n",
      "\tspeed: 0.1113s/iter; left time: 3646.2616s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1588222\n",
      "\tspeed: 0.1065s/iter; left time: 3476.2126s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1487896\n",
      "\tspeed: 0.0914s/iter; left time: 2974.1348s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1324607\n",
      "\tspeed: 0.0919s/iter; left time: 2982.6743s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1343060\n",
      "\tspeed: 0.0911s/iter; left time: 2948.3390s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1300933\n",
      "\tspeed: 0.0913s/iter; left time: 2945.9156s\n",
      "Epoch: 1 cost time: 00h:06m:30.70s\n",
      "Epoch: 1 | Train Loss: 0.1306244 Vali Loss: 0.1549400 Test Loss: 0.1748424\n",
      "Validation loss decreased (inf --> 0.154940).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1224152\n",
      "\tspeed: 1.1437s/iter; left time: 36685.2059s\n",
      "\titers: 200, epoch: 2 | loss: 0.1048735\n",
      "\tspeed: 0.1015s/iter; left time: 3246.5263s\n",
      "\titers: 300, epoch: 2 | loss: 0.1225947\n",
      "\tspeed: 0.1016s/iter; left time: 3237.5915s\n",
      "\titers: 400, epoch: 2 | loss: 0.1356741\n",
      "\tspeed: 0.1022s/iter; left time: 3246.7558s\n",
      "\titers: 500, epoch: 2 | loss: 0.1443259\n",
      "\tspeed: 0.1026s/iter; left time: 3249.2932s\n",
      "\titers: 600, epoch: 2 | loss: 0.1222776\n",
      "\tspeed: 0.1016s/iter; left time: 3209.5162s\n",
      "\titers: 700, epoch: 2 | loss: 0.1537299\n",
      "\tspeed: 0.1027s/iter; left time: 3232.9932s\n",
      "\titers: 800, epoch: 2 | loss: 0.1169169\n",
      "\tspeed: 0.1012s/iter; left time: 3176.0664s\n",
      "\titers: 900, epoch: 2 | loss: 0.1208766\n",
      "\tspeed: 0.1019s/iter; left time: 3186.6042s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1375516\n",
      "\tspeed: 0.1025s/iter; left time: 3194.8570s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1520113\n",
      "\tspeed: 0.1020s/iter; left time: 3168.5940s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1079463\n",
      "\tspeed: 0.1017s/iter; left time: 3149.4037s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1106950\n",
      "\tspeed: 0.1015s/iter; left time: 3133.4890s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1318861\n",
      "\tspeed: 0.1007s/iter; left time: 3100.6788s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1479900\n",
      "\tspeed: 0.1028s/iter; left time: 3152.1607s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1199978\n",
      "\tspeed: 0.1014s/iter; left time: 3099.6211s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1255758\n",
      "\tspeed: 0.1005s/iter; left time: 3063.9179s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1248307\n",
      "\tspeed: 0.1011s/iter; left time: 3069.6158s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1157912\n",
      "\tspeed: 0.1013s/iter; left time: 3066.5271s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1295319\n",
      "\tspeed: 0.1010s/iter; left time: 3048.6445s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1222536\n",
      "\tspeed: 0.1027s/iter; left time: 3090.0739s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1228313\n",
      "\tspeed: 0.1017s/iter; left time: 3047.6701s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1505517\n",
      "\tspeed: 0.1013s/iter; left time: 3025.7458s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1740039\n",
      "\tspeed: 0.0928s/iter; left time: 2762.2180s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1401507\n",
      "\tspeed: 0.0929s/iter; left time: 2755.9926s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1219478\n",
      "\tspeed: 0.1031s/iter; left time: 3048.7241s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1320422\n",
      "\tspeed: 0.1017s/iter; left time: 2997.5918s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1373524\n",
      "\tspeed: 0.0906s/iter; left time: 2660.7246s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1333678\n",
      "\tspeed: 0.0891s/iter; left time: 2608.0733s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1390935\n",
      "\tspeed: 0.1018s/iter; left time: 2969.2244s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1178290\n",
      "\tspeed: 0.1012s/iter; left time: 2941.4539s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1285579\n",
      "\tspeed: 0.1014s/iter; left time: 2937.4346s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1369879\n",
      "\tspeed: 0.1009s/iter; left time: 2914.6424s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1250566\n",
      "\tspeed: 0.1012s/iter; left time: 2911.7044s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1661724\n",
      "\tspeed: 0.1012s/iter; left time: 2900.9069s\n",
      "Epoch: 2 cost time: 00h:05m:59.67s\n",
      "Epoch: 2 | Train Loss: 0.1305167 Vali Loss: 0.1549173 Test Loss: 0.1748424\n",
      "Validation loss decreased (0.154940 --> 0.154917).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1162848\n",
      "\tspeed: 1.0213s/iter; left time: 29107.3860s\n",
      "\titers: 200, epoch: 3 | loss: 0.1249548\n",
      "\tspeed: 0.0977s/iter; left time: 2775.4738s\n",
      "\titers: 300, epoch: 3 | loss: 0.1352678\n",
      "\tspeed: 0.1015s/iter; left time: 2872.3940s\n",
      "\titers: 400, epoch: 3 | loss: 0.1372006\n",
      "\tspeed: 0.0959s/iter; left time: 2703.4797s\n",
      "\titers: 500, epoch: 3 | loss: 0.1286325\n",
      "\tspeed: 0.1006s/iter; left time: 2827.4057s\n",
      "\titers: 600, epoch: 3 | loss: 0.1480927\n",
      "\tspeed: 0.1016s/iter; left time: 2843.9705s\n",
      "\titers: 700, epoch: 3 | loss: 0.1231049\n",
      "\tspeed: 0.1013s/iter; left time: 2825.8354s\n",
      "\titers: 800, epoch: 3 | loss: 0.1073720\n",
      "\tspeed: 0.1020s/iter; left time: 2835.8501s\n",
      "\titers: 900, epoch: 3 | loss: 0.1310150\n",
      "\tspeed: 0.1005s/iter; left time: 2783.2589s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1352417\n",
      "\tspeed: 0.0841s/iter; left time: 2321.8055s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1503970\n",
      "\tspeed: 0.1049s/iter; left time: 2884.8264s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1043875\n",
      "\tspeed: 0.1036s/iter; left time: 2837.9431s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1669405\n",
      "\tspeed: 0.1024s/iter; left time: 2794.9386s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1186220\n",
      "\tspeed: 0.1017s/iter; left time: 2765.8189s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1330581\n",
      "\tspeed: 0.0936s/iter; left time: 2535.5447s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1316914\n",
      "\tspeed: 0.1029s/iter; left time: 2779.1860s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1513398\n",
      "\tspeed: 0.1033s/iter; left time: 2777.8334s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1311594\n",
      "\tspeed: 0.1039s/iter; left time: 2785.1448s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1403488\n",
      "\tspeed: 0.1025s/iter; left time: 2736.8983s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1255917\n",
      "\tspeed: 0.1015s/iter; left time: 2700.7764s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1225440\n",
      "\tspeed: 0.1027s/iter; left time: 2720.3924s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1361018\n",
      "\tspeed: 0.1021s/iter; left time: 2694.9815s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1528924\n",
      "\tspeed: 0.1017s/iter; left time: 2676.0260s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1577501\n",
      "\tspeed: 0.1011s/iter; left time: 2647.8091s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1127176\n",
      "\tspeed: 0.1027s/iter; left time: 2680.8315s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1052634\n",
      "\tspeed: 0.1021s/iter; left time: 2653.8200s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1548333\n",
      "\tspeed: 0.1013s/iter; left time: 2622.8449s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1299338\n",
      "\tspeed: 0.1013s/iter; left time: 2613.1654s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0919160\n",
      "\tspeed: 0.1010s/iter; left time: 2595.9805s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1143410\n",
      "\tspeed: 0.1020s/iter; left time: 2611.3315s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1358992\n",
      "\tspeed: 0.1039s/iter; left time: 2648.6398s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1309795\n",
      "\tspeed: 0.1034s/iter; left time: 2627.4898s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1576539\n",
      "\tspeed: 0.1023s/iter; left time: 2588.8975s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1183001\n",
      "\tspeed: 0.1000s/iter; left time: 2519.5963s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1068669\n",
      "\tspeed: 0.1033s/iter; left time: 2592.2669s\n",
      "Epoch: 3 cost time: 00h:06m:01.80s\n",
      "Epoch: 3 | Train Loss: 0.1305193 Vali Loss: 0.1549234 Test Loss: 0.1748424\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1311847\n",
      "\tspeed: 1.0067s/iter; left time: 25092.6005s\n",
      "\titers: 200, epoch: 4 | loss: 0.1289056\n",
      "\tspeed: 0.1010s/iter; left time: 2507.2924s\n",
      "\titers: 300, epoch: 4 | loss: 0.1376576\n",
      "\tspeed: 0.1016s/iter; left time: 2511.2295s\n",
      "\titers: 400, epoch: 4 | loss: 0.1221022\n",
      "\tspeed: 0.1033s/iter; left time: 2543.1665s\n",
      "\titers: 500, epoch: 4 | loss: 0.1457740\n",
      "\tspeed: 0.1030s/iter; left time: 2525.5624s\n",
      "\titers: 600, epoch: 4 | loss: 0.1467180\n",
      "\tspeed: 0.1016s/iter; left time: 2482.3758s\n",
      "\titers: 700, epoch: 4 | loss: 0.1268940\n",
      "\tspeed: 0.1029s/iter; left time: 2502.4113s\n",
      "\titers: 800, epoch: 4 | loss: 0.1044770\n",
      "\tspeed: 0.1004s/iter; left time: 2432.4211s\n",
      "\titers: 900, epoch: 4 | loss: 0.1424355\n",
      "\tspeed: 0.0967s/iter; left time: 2332.1468s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1335797\n",
      "\tspeed: 0.1014s/iter; left time: 2435.1649s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1116332\n",
      "\tspeed: 0.1009s/iter; left time: 2414.7146s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1326714\n",
      "\tspeed: 0.0851s/iter; left time: 2026.8568s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1162961\n",
      "\tspeed: 0.1013s/iter; left time: 2403.3907s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1392030\n",
      "\tspeed: 0.1033s/iter; left time: 2441.5777s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1593161\n",
      "\tspeed: 0.1020s/iter; left time: 2400.6554s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1316098\n",
      "\tspeed: 0.1020s/iter; left time: 2388.6627s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1132654\n",
      "\tspeed: 0.1025s/iter; left time: 2389.9721s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1433258\n",
      "\tspeed: 0.0996s/iter; left time: 2313.1122s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1373538\n",
      "\tspeed: 0.0965s/iter; left time: 2230.9834s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1653546\n",
      "\tspeed: 0.0958s/iter; left time: 2206.9228s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1445145\n",
      "\tspeed: 0.0976s/iter; left time: 2237.5242s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1458471\n",
      "\tspeed: 0.0961s/iter; left time: 2193.4079s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1375149\n",
      "\tspeed: 0.1023s/iter; left time: 2324.5981s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1284863\n",
      "\tspeed: 0.1015s/iter; left time: 2296.7353s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1496632\n",
      "\tspeed: 0.1016s/iter; left time: 2287.5294s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1432805\n",
      "\tspeed: 0.1032s/iter; left time: 2314.7634s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1158759\n",
      "\tspeed: 0.1016s/iter; left time: 2269.1544s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1641084\n",
      "\tspeed: 0.1039s/iter; left time: 2308.1968s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1572719\n",
      "\tspeed: 0.1023s/iter; left time: 2262.6516s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1399265\n",
      "\tspeed: 0.1029s/iter; left time: 2266.7081s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1123227\n",
      "\tspeed: 0.1015s/iter; left time: 2224.4802s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1328554\n",
      "\tspeed: 0.1024s/iter; left time: 2235.0084s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1123703\n",
      "\tspeed: 0.1039s/iter; left time: 2257.2095s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1338987\n",
      "\tspeed: 0.1039s/iter; left time: 2247.2846s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1398191\n",
      "\tspeed: 0.1017s/iter; left time: 2188.4625s\n",
      "Epoch: 4 cost time: 00h:06m:01.31s\n",
      "Epoch: 4 | Train Loss: 0.1305170 Vali Loss: 0.1549248 Test Loss: 0.1748424\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1384644\n",
      "\tspeed: 1.0038s/iter; left time: 21432.9403s\n",
      "\titers: 200, epoch: 5 | loss: 0.1454410\n",
      "\tspeed: 0.1013s/iter; left time: 2153.6467s\n",
      "\titers: 300, epoch: 5 | loss: 0.1240374\n",
      "\tspeed: 0.1039s/iter; left time: 2197.2442s\n",
      "\titers: 400, epoch: 5 | loss: 0.1268066\n",
      "\tspeed: 0.1033s/iter; left time: 2174.5451s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 232, in <module>\n",
      "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1852, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 245, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/Time-LLM/models/TimeLLM.py\", line 283, in forecast\n",
      "    prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2577, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2663, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2854, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in <genexpr>\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "\n",
      "KeyboardInterrupt\n",
      "Total time: 31.496345178286234 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"24\"]\n",
    "\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 2 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96 - shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 00:57:31,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 00:57:32,497] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 00:57:32,497] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 00:57:32,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 00:57:32,594] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 00:57:32,594] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 00:57:33,254] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 00:57:33,256] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 00:57:33,256] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 00:57:33,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 00:57:33,258] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 00:57:33,258] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 00:57:33,258] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 00:57:33,570] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 00:57:33,571] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,571] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,695] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 00:57:33,696] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,696] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,697] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 00:57:33,813] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 00:57:33,814] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 00:57:33,814] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.67 GB, percent = 10.2%\n",
      "[2024-10-31 00:57:33,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 00:57:33,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 00:57:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 00:57:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 00:57:33,815] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a0d1309d0>\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 00:57:33,816] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 00:57:33,817] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 00:57:33,818] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1332849\n",
      "\tspeed: 0.1525s/iter; left time: 5422.3506s\n",
      "\titers: 200, epoch: 1 | loss: 0.1226809\n",
      "\tspeed: 0.1111s/iter; left time: 3937.9748s\n",
      "\titers: 300, epoch: 1 | loss: 0.0829579\n",
      "\tspeed: 0.1109s/iter; left time: 3922.9468s\n",
      "\titers: 400, epoch: 1 | loss: 0.0719160\n",
      "\tspeed: 0.1002s/iter; left time: 3534.6732s\n",
      "\titers: 500, epoch: 1 | loss: 0.1208322\n",
      "\tspeed: 0.1098s/iter; left time: 3861.2302s\n",
      "\titers: 600, epoch: 1 | loss: 0.0824153\n",
      "\tspeed: 0.1110s/iter; left time: 3890.1517s\n",
      "\titers: 700, epoch: 1 | loss: 0.0565333\n",
      "\tspeed: 0.1136s/iter; left time: 3971.6596s\n",
      "\titers: 800, epoch: 1 | loss: 0.0793769\n",
      "\tspeed: 0.1146s/iter; left time: 3993.7417s\n",
      "\titers: 900, epoch: 1 | loss: 0.0705150\n",
      "\tspeed: 0.1139s/iter; left time: 3959.8387s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0586823\n",
      "\tspeed: 0.1131s/iter; left time: 3920.6815s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0585032\n",
      "\tspeed: 0.1126s/iter; left time: 3890.7956s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0598272\n",
      "\tspeed: 0.1127s/iter; left time: 3883.5807s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0872594\n",
      "\tspeed: 0.1121s/iter; left time: 3851.2699s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0588569\n",
      "\tspeed: 0.1111s/iter; left time: 3804.7949s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0763347\n",
      "\tspeed: 0.1110s/iter; left time: 3793.3456s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0903879\n",
      "\tspeed: 0.1111s/iter; left time: 3783.2268s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0801517\n",
      "\tspeed: 0.1099s/iter; left time: 3732.3117s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0630893\n",
      "\tspeed: 0.1110s/iter; left time: 3757.7620s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0696212\n",
      "\tspeed: 0.1112s/iter; left time: 3755.4751s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0660686\n",
      "\tspeed: 0.1135s/iter; left time: 3820.2270s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0738638\n",
      "\tspeed: 0.1107s/iter; left time: 3716.4403s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0562153\n",
      "\tspeed: 0.1025s/iter; left time: 3428.4043s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0773825\n",
      "\tspeed: 0.1061s/iter; left time: 3540.6957s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0946050\n",
      "\tspeed: 0.1083s/iter; left time: 3601.5181s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0675518\n",
      "\tspeed: 0.1136s/iter; left time: 3765.8907s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0667586\n",
      "\tspeed: 0.1134s/iter; left time: 3747.5797s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0709311\n",
      "\tspeed: 0.1121s/iter; left time: 3695.7801s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0718190\n",
      "\tspeed: 0.1112s/iter; left time: 3654.7393s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0784791\n",
      "\tspeed: 0.1111s/iter; left time: 3639.2803s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0715083\n",
      "\tspeed: 0.1118s/iter; left time: 3652.8302s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0626798\n",
      "\tspeed: 0.1143s/iter; left time: 3720.9594s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0780201\n",
      "\tspeed: 0.1126s/iter; left time: 3653.6404s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0764208\n",
      "\tspeed: 0.1113s/iter; left time: 3602.0657s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0627120\n",
      "\tspeed: 0.1111s/iter; left time: 3585.2850s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0708910\n",
      "\tspeed: 0.1141s/iter; left time: 3669.5756s\n",
      "Epoch: 1 cost time: 00h:06m:37.81s\n",
      "Epoch: 1 | Train Loss: 0.0782478 Vali Loss: 0.0741555 Test Loss: 0.0831796\n",
      "Validation loss decreased (inf --> 0.074155).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0658899\n",
      "\tspeed: 1.1564s/iter; left time: 36998.0622s\n",
      "\titers: 200, epoch: 2 | loss: 0.1048066\n",
      "\tspeed: 0.1083s/iter; left time: 3455.3250s\n",
      "\titers: 300, epoch: 2 | loss: 0.0702246\n",
      "\tspeed: 0.1063s/iter; left time: 3380.0659s\n",
      "\titers: 400, epoch: 2 | loss: 0.0755464\n",
      "\tspeed: 0.1084s/iter; left time: 3435.8489s\n",
      "\titers: 500, epoch: 2 | loss: 0.0727920\n",
      "\tspeed: 0.1027s/iter; left time: 3244.7229s\n",
      "\titers: 600, epoch: 2 | loss: 0.0613693\n",
      "\tspeed: 0.1009s/iter; left time: 3178.7378s\n",
      "\titers: 700, epoch: 2 | loss: 0.0999854\n",
      "\tspeed: 0.1026s/iter; left time: 3220.8845s\n",
      "\titers: 800, epoch: 2 | loss: 0.0538424\n",
      "\tspeed: 0.1045s/iter; left time: 3270.4582s\n",
      "\titers: 900, epoch: 2 | loss: 0.0550089\n",
      "\tspeed: 0.1068s/iter; left time: 3332.8771s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0533296\n",
      "\tspeed: 0.1076s/iter; left time: 3346.2745s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0814121\n",
      "\tspeed: 0.1066s/iter; left time: 3305.1335s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0633737\n",
      "\tspeed: 0.1062s/iter; left time: 3280.5800s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0798584\n",
      "\tspeed: 0.1067s/iter; left time: 3284.7576s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0698421\n",
      "\tspeed: 0.1041s/iter; left time: 3195.5801s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0627612\n",
      "\tspeed: 0.1011s/iter; left time: 3092.0542s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0672406\n",
      "\tspeed: 0.1052s/iter; left time: 3209.2672s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0693460\n",
      "\tspeed: 0.1069s/iter; left time: 3250.4720s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0610231\n",
      "\tspeed: 0.1051s/iter; left time: 3183.4856s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0713101\n",
      "\tspeed: 0.1052s/iter; left time: 3175.3243s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0816538\n",
      "\tspeed: 0.1015s/iter; left time: 3053.6582s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0808621\n",
      "\tspeed: 0.1053s/iter; left time: 3158.4254s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0554627\n",
      "\tspeed: 0.1037s/iter; left time: 3101.0450s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0546244\n",
      "\tspeed: 0.1011s/iter; left time: 3012.7038s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0925839\n",
      "\tspeed: 0.1009s/iter; left time: 2997.6863s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0774217\n",
      "\tspeed: 0.1017s/iter; left time: 3009.2214s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0685905\n",
      "\tspeed: 0.1027s/iter; left time: 3029.5529s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0614267\n",
      "\tspeed: 0.1071s/iter; left time: 3149.5926s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0766011\n",
      "\tspeed: 0.1010s/iter; left time: 2959.3594s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0837857\n",
      "\tspeed: 0.1009s/iter; left time: 2947.1969s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0537917\n",
      "\tspeed: 0.1027s/iter; left time: 2988.3968s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0577093\n",
      "\tspeed: 0.1034s/iter; left time: 2999.4673s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0555598\n",
      "\tspeed: 0.1036s/iter; left time: 2993.8941s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0752852\n",
      "\tspeed: 0.1013s/iter; left time: 2916.1508s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0711872\n",
      "\tspeed: 0.1011s/iter; left time: 2899.7425s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0683053\n",
      "\tspeed: 0.1008s/iter; left time: 2883.6118s\n",
      "Epoch: 2 cost time: 00h:06m:10.53s\n",
      "Epoch: 2 | Train Loss: 0.0687149 Vali Loss: 0.0723769 Test Loss: 0.0818021\n",
      "Validation loss decreased (0.074155 --> 0.072377).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0715792\n",
      "\tspeed: 1.0501s/iter; left time: 29852.5583s\n",
      "\titers: 200, epoch: 3 | loss: 0.0788853\n",
      "\tspeed: 0.1079s/iter; left time: 3056.2498s\n",
      "\titers: 300, epoch: 3 | loss: 0.0674003\n",
      "\tspeed: 0.1092s/iter; left time: 3081.2454s\n",
      "\titers: 400, epoch: 3 | loss: 0.0723566\n",
      "\tspeed: 0.1055s/iter; left time: 2967.7069s\n",
      "\titers: 500, epoch: 3 | loss: 0.0499699\n",
      "\tspeed: 0.1023s/iter; left time: 2868.4844s\n",
      "\titers: 600, epoch: 3 | loss: 0.0738916\n",
      "\tspeed: 0.1011s/iter; left time: 2822.6192s\n",
      "\titers: 700, epoch: 3 | loss: 0.0666234\n",
      "\tspeed: 0.1007s/iter; left time: 2801.0269s\n",
      "\titers: 800, epoch: 3 | loss: 0.0641793\n",
      "\tspeed: 0.1006s/iter; left time: 2790.9017s\n",
      "\titers: 900, epoch: 3 | loss: 0.0726167\n",
      "\tspeed: 0.1023s/iter; left time: 2825.8826s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0578101\n",
      "\tspeed: 0.1032s/iter; left time: 2841.2733s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0814463\n",
      "\tspeed: 0.1040s/iter; left time: 2853.6983s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0677088\n",
      "\tspeed: 0.1024s/iter; left time: 2798.3725s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0701640\n",
      "\tspeed: 0.1038s/iter; left time: 2827.0669s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0820722\n",
      "\tspeed: 0.1078s/iter; left time: 2924.3541s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0492643\n",
      "\tspeed: 0.1074s/iter; left time: 2903.9063s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0652639\n",
      "\tspeed: 0.1076s/iter; left time: 2897.2064s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0597799\n",
      "\tspeed: 0.1043s/iter; left time: 2799.4180s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0708796\n",
      "\tspeed: 0.1085s/iter; left time: 2901.4328s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0690726\n",
      "\tspeed: 0.1076s/iter; left time: 2865.3361s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0778807\n",
      "\tspeed: 0.1065s/iter; left time: 2826.1841s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0669764\n",
      "\tspeed: 0.1065s/iter; left time: 2815.6669s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0535536\n",
      "\tspeed: 0.1027s/iter; left time: 2703.3007s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0868591\n",
      "\tspeed: 0.1015s/iter; left time: 2662.3715s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0679954\n",
      "\tspeed: 0.0900s/iter; left time: 2351.6746s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0618816\n",
      "\tspeed: 0.0832s/iter; left time: 2165.2316s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0856984\n",
      "\tspeed: 0.0918s/iter; left time: 2379.5403s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0775335\n",
      "\tspeed: 0.0922s/iter; left time: 2380.2053s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0633479\n",
      "\tspeed: 0.1013s/iter; left time: 2605.7964s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0762474\n",
      "\tspeed: 0.1061s/iter; left time: 2720.3866s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0556791\n",
      "\tspeed: 0.1084s/iter; left time: 2766.2124s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0651474\n",
      "\tspeed: 0.1066s/iter; left time: 2711.4251s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0652379\n",
      "\tspeed: 0.1068s/iter; left time: 2704.3028s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0641431\n",
      "\tspeed: 0.1059s/iter; left time: 2671.1666s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0688879\n",
      "\tspeed: 0.1026s/iter; left time: 2578.7920s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0765588\n",
      "\tspeed: 0.1004s/iter; left time: 2513.3767s\n",
      "Epoch: 3 cost time: 00h:06m:07.63s\n",
      "Epoch: 3 | Train Loss: 0.0664232 Vali Loss: 0.0730986 Test Loss: 0.0813632\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0668811\n",
      "\tspeed: 0.9960s/iter; left time: 24764.2108s\n",
      "\titers: 200, epoch: 4 | loss: 0.0618269\n",
      "\tspeed: 0.1047s/iter; left time: 2593.6831s\n",
      "\titers: 300, epoch: 4 | loss: 0.0673992\n",
      "\tspeed: 0.1009s/iter; left time: 2487.5053s\n",
      "\titers: 400, epoch: 4 | loss: 0.0552185\n",
      "\tspeed: 0.0941s/iter; left time: 2311.3379s\n",
      "\titers: 500, epoch: 4 | loss: 0.0635973\n",
      "\tspeed: 0.1013s/iter; left time: 2478.1625s\n",
      "\titers: 600, epoch: 4 | loss: 0.0599482\n",
      "\tspeed: 0.0996s/iter; left time: 2425.6772s\n",
      "\titers: 700, epoch: 4 | loss: 0.0629725\n",
      "\tspeed: 0.1010s/iter; left time: 2451.3936s\n",
      "\titers: 800, epoch: 4 | loss: 0.0704623\n",
      "\tspeed: 0.1012s/iter; left time: 2444.8091s\n",
      "\titers: 900, epoch: 4 | loss: 0.0448214\n",
      "\tspeed: 0.1027s/iter; left time: 2470.7113s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0590776\n",
      "\tspeed: 0.1043s/iter; left time: 2498.4632s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0764958\n",
      "\tspeed: 0.1037s/iter; left time: 2474.1831s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0797139\n",
      "\tspeed: 0.1034s/iter; left time: 2456.0600s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0617398\n",
      "\tspeed: 0.1005s/iter; left time: 2377.8538s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0606667\n",
      "\tspeed: 0.0941s/iter; left time: 2218.3238s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0683119\n",
      "\tspeed: 0.0908s/iter; left time: 2130.2372s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0832711\n",
      "\tspeed: 0.1080s/iter; left time: 2523.0359s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0639146\n",
      "\tspeed: 0.1068s/iter; left time: 2484.5848s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0657482\n",
      "\tspeed: 0.1045s/iter; left time: 2419.5254s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0762344\n",
      "\tspeed: 0.1030s/iter; left time: 2376.5488s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0764676\n",
      "\tspeed: 0.0989s/iter; left time: 2271.7865s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0657827\n",
      "\tspeed: 0.1073s/iter; left time: 2452.4932s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0669597\n",
      "\tspeed: 0.1040s/iter; left time: 2367.0266s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0562412\n",
      "\tspeed: 0.1008s/iter; left time: 2285.4928s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0579680\n",
      "\tspeed: 0.1010s/iter; left time: 2278.0797s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0637907\n",
      "\tspeed: 0.1009s/iter; left time: 2265.8098s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0796028\n",
      "\tspeed: 0.1016s/iter; left time: 2271.1449s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0815872\n",
      "\tspeed: 0.1012s/iter; left time: 2252.2328s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0738015\n",
      "\tspeed: 0.1021s/iter; left time: 2262.7412s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0625055\n",
      "\tspeed: 0.1017s/iter; left time: 2243.9369s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0475844\n",
      "\tspeed: 0.1078s/iter; left time: 2368.3269s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0625621\n",
      "\tspeed: 0.1014s/iter; left time: 2216.4136s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0543961\n",
      "\tspeed: 0.1018s/iter; left time: 2215.4833s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0665312\n",
      "\tspeed: 0.1044s/iter; left time: 2262.6529s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0553873\n",
      "\tspeed: 0.1015s/iter; left time: 2187.5671s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0641216\n",
      "\tspeed: 0.1010s/iter; left time: 2167.2104s\n",
      "Epoch: 4 cost time: 00h:06m:03.64s\n",
      "Epoch: 4 | Train Loss: 0.0638932 Vali Loss: 0.0744487 Test Loss: 0.0830895\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0693577\n",
      "\tspeed: 0.9849s/iter; left time: 20976.2253s\n",
      "\titers: 200, epoch: 5 | loss: 0.0654913\n",
      "\tspeed: 0.1011s/iter; left time: 2143.0017s\n",
      "\titers: 300, epoch: 5 | loss: 0.0657027\n",
      "\tspeed: 0.1054s/iter; left time: 2222.7227s\n",
      "\titers: 400, epoch: 5 | loss: 0.0641252\n",
      "\tspeed: 0.1054s/iter; left time: 2212.3505s\n",
      "\titers: 500, epoch: 5 | loss: 0.0781336\n",
      "\tspeed: 0.1035s/iter; left time: 2163.6012s\n",
      "\titers: 600, epoch: 5 | loss: 0.0593817\n",
      "\tspeed: 0.1009s/iter; left time: 2097.3820s\n",
      "\titers: 700, epoch: 5 | loss: 0.0620136\n",
      "\tspeed: 0.1009s/iter; left time: 2089.1806s\n",
      "\titers: 800, epoch: 5 | loss: 0.0565020\n",
      "\tspeed: 0.1049s/iter; left time: 2161.0705s\n",
      "\titers: 900, epoch: 5 | loss: 0.0513705\n",
      "\tspeed: 0.1045s/iter; left time: 2142.4728s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0668830\n",
      "\tspeed: 0.1026s/iter; left time: 2092.2332s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0586694\n",
      "\tspeed: 0.1032s/iter; left time: 2095.2850s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0762751\n",
      "\tspeed: 0.1092s/iter; left time: 2205.2618s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0615793\n",
      "\tspeed: 0.1030s/iter; left time: 2069.5538s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0658676\n",
      "\tspeed: 0.1009s/iter; left time: 2017.2203s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0459896\n",
      "\tspeed: 0.1010s/iter; left time: 2008.7170s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0505971\n",
      "\tspeed: 0.1007s/iter; left time: 1993.2489s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0715075\n",
      "\tspeed: 0.1009s/iter; left time: 1986.4897s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0574153\n",
      "\tspeed: 0.1009s/iter; left time: 1976.9396s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0615106\n",
      "\tspeed: 0.1007s/iter; left time: 1962.6543s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0518319\n",
      "\tspeed: 0.1020s/iter; left time: 1977.7919s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0631665\n",
      "\tspeed: 0.1019s/iter; left time: 1966.8690s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0617508\n",
      "\tspeed: 0.1033s/iter; left time: 1983.3061s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0547799\n",
      "\tspeed: 0.1008s/iter; left time: 1925.6670s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0817395\n",
      "\tspeed: 0.1007s/iter; left time: 1913.7312s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0702271\n",
      "\tspeed: 0.1009s/iter; left time: 1905.9542s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0471027\n",
      "\tspeed: 0.1008s/iter; left time: 1895.6628s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0532453\n",
      "\tspeed: 0.1007s/iter; left time: 1883.5455s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0648276\n",
      "\tspeed: 0.1006s/iter; left time: 1870.3500s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0574297\n",
      "\tspeed: 0.1014s/iter; left time: 1875.3598s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0605013\n",
      "\tspeed: 0.1017s/iter; left time: 1871.6813s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0599723\n",
      "\tspeed: 0.1053s/iter; left time: 1926.9828s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0612394\n",
      "\tspeed: 0.1063s/iter; left time: 1934.2763s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0544313\n",
      "\tspeed: 0.1063s/iter; left time: 1923.7660s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0706279\n",
      "\tspeed: 0.1016s/iter; left time: 1828.7376s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0658244\n",
      "\tspeed: 0.1007s/iter; left time: 1801.5072s\n",
      "Epoch: 5 cost time: 00h:06m:05.66s\n",
      "Epoch: 5 | Train Loss: 0.0611344 Vali Loss: 0.0769676 Test Loss: 0.0859656\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018535790964961052, rmse:0.13614621758460999, mae:0.08180207759141922, rse:0.5266919136047363\n",
      "success delete checkpoints\n",
      "Total time: 136.75494347016016 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96, lr descrease: 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 02:07:44,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 02:07:45,926] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 02:07:45,926] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 02:07:45,926] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 02:07:46,031] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 02:07:46,032] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 02:07:46,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 02:07:46,702] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 02:07:46,702] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 02:07:46,704] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 02:07:46,704] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 02:07:46,704] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 02:07:46,704] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 02:07:46,704] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 02:07:46,705] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 02:07:46,705] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 02:07:47,035] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 02:07:47,036] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,078] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,204] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 02:07:47,205] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,205] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,205] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 02:07:47,316] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 02:07:47,317] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 02:07:47,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.26 GB, percent = 10.1%\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 02:07:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 02:07:47,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 02:07:47,319] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f734caa3990>\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 02:07:47,320] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 02:07:47,321] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 02:07:47,322] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1440575\n",
      "\tspeed: 0.1564s/iter; left time: 5562.2778s\n",
      "\titers: 200, epoch: 1 | loss: 0.1299801\n",
      "\tspeed: 0.1117s/iter; left time: 3962.5847s\n",
      "\titers: 300, epoch: 1 | loss: 0.1354454\n",
      "\tspeed: 0.1105s/iter; left time: 3907.9259s\n",
      "\titers: 400, epoch: 1 | loss: 0.1193806\n",
      "\tspeed: 0.1107s/iter; left time: 3904.6076s\n",
      "\titers: 500, epoch: 1 | loss: 0.1538889\n",
      "\tspeed: 0.1118s/iter; left time: 3930.2469s\n",
      "\titers: 600, epoch: 1 | loss: 0.1388368\n",
      "\tspeed: 0.1132s/iter; left time: 3968.5100s\n",
      "\titers: 700, epoch: 1 | loss: 0.0986667\n",
      "\tspeed: 0.1119s/iter; left time: 3913.1953s\n",
      "\titers: 800, epoch: 1 | loss: 0.1530047\n",
      "\tspeed: 0.1110s/iter; left time: 3870.9251s\n",
      "\titers: 900, epoch: 1 | loss: 0.1101738\n",
      "\tspeed: 0.1106s/iter; left time: 3844.3441s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1091491\n",
      "\tspeed: 0.1105s/iter; left time: 3831.7075s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1089654\n",
      "\tspeed: 0.1112s/iter; left time: 3843.4040s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1089270\n",
      "\tspeed: 0.1049s/iter; left time: 3613.4193s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1542389\n",
      "\tspeed: 0.1109s/iter; left time: 3811.1323s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1063095\n",
      "\tspeed: 0.1121s/iter; left time: 3839.2299s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1481955\n",
      "\tspeed: 0.1129s/iter; left time: 3856.7061s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1341569\n",
      "\tspeed: 0.1130s/iter; left time: 3849.2429s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1185152\n",
      "\tspeed: 0.1113s/iter; left time: 3780.9541s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0908604\n",
      "\tspeed: 0.1116s/iter; left time: 3780.1918s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0852027\n",
      "\tspeed: 0.1120s/iter; left time: 3779.5514s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0970242\n",
      "\tspeed: 0.1121s/iter; left time: 3771.7236s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0929401\n",
      "\tspeed: 0.1133s/iter; left time: 3804.1063s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0797752\n",
      "\tspeed: 0.0920s/iter; left time: 3078.5064s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0933316\n",
      "\tspeed: 0.0912s/iter; left time: 3043.5019s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1154153\n",
      "\tspeed: 0.0911s/iter; left time: 3031.5459s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0875616\n",
      "\tspeed: 0.1061s/iter; left time: 3518.3055s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0781510\n",
      "\tspeed: 0.1118s/iter; left time: 3696.5591s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0878930\n",
      "\tspeed: 0.0991s/iter; left time: 3268.0365s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0852987\n",
      "\tspeed: 0.0913s/iter; left time: 3001.2234s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0928331\n",
      "\tspeed: 0.1104s/iter; left time: 3617.3128s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0779679\n",
      "\tspeed: 0.1117s/iter; left time: 3649.3693s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0705386\n",
      "\tspeed: 0.1122s/iter; left time: 3654.3081s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0952635\n",
      "\tspeed: 0.1138s/iter; left time: 3694.4805s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0840503\n",
      "\tspeed: 0.1139s/iter; left time: 3684.7276s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0746481\n",
      "\tspeed: 0.1127s/iter; left time: 3637.0387s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0773572\n",
      "\tspeed: 0.1154s/iter; left time: 3710.1554s\n",
      "Epoch: 1 cost time: 00h:06m:30.02s\n",
      "Epoch: 1 | Train Loss: 0.1092172 Vali Loss: 0.0821468 Test Loss: 0.0903220\n",
      "Validation loss decreased (inf --> 0.082147).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 2 | loss: 0.0720677\n",
      "\tspeed: 1.1443s/iter; left time: 36610.7429s\n",
      "\titers: 200, epoch: 2 | loss: 0.1120776\n",
      "\tspeed: 0.1005s/iter; left time: 3206.0013s\n",
      "\titers: 300, epoch: 2 | loss: 0.0849378\n",
      "\tspeed: 0.1067s/iter; left time: 3393.0777s\n",
      "\titers: 400, epoch: 2 | loss: 0.0827252\n",
      "\tspeed: 0.0955s/iter; left time: 3025.3250s\n",
      "\titers: 500, epoch: 2 | loss: 0.0828264\n",
      "\tspeed: 0.0965s/iter; left time: 3049.7708s\n",
      "\titers: 600, epoch: 2 | loss: 0.0653720\n",
      "\tspeed: 0.1096s/iter; left time: 3452.1596s\n",
      "\titers: 700, epoch: 2 | loss: 0.0999948\n",
      "\tspeed: 0.1071s/iter; left time: 3362.1505s\n",
      "\titers: 800, epoch: 2 | loss: 0.0638315\n",
      "\tspeed: 0.1060s/iter; left time: 3317.2806s\n",
      "\titers: 900, epoch: 2 | loss: 0.0631677\n",
      "\tspeed: 0.1053s/iter; left time: 3285.9780s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0622543\n",
      "\tspeed: 0.1056s/iter; left time: 3284.6763s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0878805\n",
      "\tspeed: 0.1018s/iter; left time: 3156.1915s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0738014\n",
      "\tspeed: 0.1041s/iter; left time: 3214.8003s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0870380\n",
      "\tspeed: 0.1070s/iter; left time: 3296.2095s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0739821\n",
      "\tspeed: 0.1038s/iter; left time: 3185.2227s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0658785\n",
      "\tspeed: 0.1022s/iter; left time: 3126.3635s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0713090\n",
      "\tspeed: 0.0870s/iter; left time: 2653.1261s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0756991\n",
      "\tspeed: 0.1043s/iter; left time: 3171.3060s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0680061\n",
      "\tspeed: 0.1019s/iter; left time: 3087.8064s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0778164\n",
      "\tspeed: 0.1015s/iter; left time: 3063.8876s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0860281\n",
      "\tspeed: 0.1018s/iter; left time: 3064.3624s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0792604\n",
      "\tspeed: 0.1010s/iter; left time: 3030.9466s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0639079\n",
      "\tspeed: 0.1008s/iter; left time: 3011.9821s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0611842\n",
      "\tspeed: 0.1011s/iter; left time: 3013.6160s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0911761\n",
      "\tspeed: 0.1056s/iter; left time: 3136.2659s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0837464\n",
      "\tspeed: 0.1070s/iter; left time: 3167.2947s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0753662\n",
      "\tspeed: 0.1070s/iter; left time: 3156.0905s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0695116\n",
      "\tspeed: 0.1066s/iter; left time: 3133.4190s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0876709\n",
      "\tspeed: 0.1014s/iter; left time: 2971.5360s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0859168\n",
      "\tspeed: 0.1040s/iter; left time: 3037.4123s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0632741\n",
      "\tspeed: 0.1049s/iter; left time: 3051.3375s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0662211\n",
      "\tspeed: 0.0835s/iter; left time: 2420.8845s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0628801\n",
      "\tspeed: 0.1033s/iter; left time: 2984.6067s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0792838\n",
      "\tspeed: 0.1069s/iter; left time: 3077.0895s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0802057\n",
      "\tspeed: 0.1008s/iter; left time: 2891.3333s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0778065\n",
      "\tspeed: 0.1011s/iter; left time: 2890.2946s\n",
      "Epoch: 2 cost time: 00h:06m:03.99s\n",
      "Epoch: 2 | Train Loss: 0.0755411 Vali Loss: 0.0772852 Test Loss: 0.0860607\n",
      "Validation loss decreased (0.082147 --> 0.077285).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 3 | loss: 0.0756057\n",
      "\tspeed: 1.0066s/iter; left time: 28616.7319s\n",
      "\titers: 200, epoch: 3 | loss: 0.0905635\n",
      "\tspeed: 0.1103s/iter; left time: 3125.7538s\n",
      "\titers: 300, epoch: 3 | loss: 0.0720346\n",
      "\tspeed: 0.1094s/iter; left time: 3087.5008s\n",
      "\titers: 400, epoch: 3 | loss: 0.0809741\n",
      "\tspeed: 0.0925s/iter; left time: 2602.0558s\n",
      "\titers: 500, epoch: 3 | loss: 0.0564593\n",
      "\tspeed: 0.1021s/iter; left time: 2861.8181s\n",
      "\titers: 600, epoch: 3 | loss: 0.0814185\n",
      "\tspeed: 0.1066s/iter; left time: 2977.3762s\n",
      "\titers: 700, epoch: 3 | loss: 0.0671464\n",
      "\tspeed: 0.1026s/iter; left time: 2854.7497s\n",
      "\titers: 800, epoch: 3 | loss: 0.0729751\n",
      "\tspeed: 0.0989s/iter; left time: 2742.2934s\n",
      "\titers: 900, epoch: 3 | loss: 0.0777468\n",
      "\tspeed: 0.0857s/iter; left time: 2368.2647s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0633177\n",
      "\tspeed: 0.0856s/iter; left time: 2357.0306s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0806149\n",
      "\tspeed: 0.0858s/iter; left time: 2353.4333s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0724397\n",
      "\tspeed: 0.0874s/iter; left time: 2388.3086s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0745694\n",
      "\tspeed: 0.0962s/iter; left time: 2620.3398s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0905064\n",
      "\tspeed: 0.1044s/iter; left time: 2831.4224s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0517099\n",
      "\tspeed: 0.1022s/iter; left time: 2762.8428s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0697932\n",
      "\tspeed: 0.1043s/iter; left time: 2808.3605s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0658498\n",
      "\tspeed: 0.1041s/iter; left time: 2793.2337s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0793835\n",
      "\tspeed: 0.1052s/iter; left time: 2810.7302s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0733241\n",
      "\tspeed: 0.1075s/iter; left time: 2863.0171s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0833095\n",
      "\tspeed: 0.1007s/iter; left time: 2671.2858s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0751705\n",
      "\tspeed: 0.1053s/iter; left time: 2783.3810s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0607194\n",
      "\tspeed: 0.0893s/iter; left time: 2351.6436s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0892512\n",
      "\tspeed: 0.0863s/iter; left time: 2262.8958s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0748885\n",
      "\tspeed: 0.1071s/iter; left time: 2798.0246s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0673093\n",
      "\tspeed: 0.1075s/iter; left time: 2798.7562s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0889892\n",
      "\tspeed: 0.1014s/iter; left time: 2628.5001s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0834593\n",
      "\tspeed: 0.1016s/iter; left time: 2623.6263s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0762946\n",
      "\tspeed: 0.1037s/iter; left time: 2666.8250s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0863989\n",
      "\tspeed: 0.0864s/iter; left time: 2214.4667s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0592902\n",
      "\tspeed: 0.0947s/iter; left time: 2417.8473s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0716888\n",
      "\tspeed: 0.1078s/iter; left time: 2741.3800s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0727923\n",
      "\tspeed: 0.1088s/iter; left time: 2755.5958s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0686482\n",
      "\tspeed: 0.1091s/iter; left time: 2751.3488s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0795544\n",
      "\tspeed: 0.1074s/iter; left time: 2699.9272s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0822845\n",
      "\tspeed: 0.1069s/iter; left time: 2674.7470s\n",
      "Epoch: 3 cost time: 00h:05m:59.42s\n",
      "Epoch: 3 | Train Loss: 0.0725358 Vali Loss: 0.0751297 Test Loss: 0.0837127\n",
      "Validation loss decreased (0.077285 --> 0.075130).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 4 | loss: 0.0762659\n",
      "\tspeed: 1.0202s/iter; left time: 25364.8510s\n",
      "\titers: 200, epoch: 4 | loss: 0.0644627\n",
      "\tspeed: 0.1012s/iter; left time: 2505.8385s\n",
      "\titers: 300, epoch: 4 | loss: 0.0728707\n",
      "\tspeed: 0.1034s/iter; left time: 2550.4710s\n",
      "\titers: 400, epoch: 4 | loss: 0.0636023\n",
      "\tspeed: 0.1037s/iter; left time: 2547.2959s\n",
      "\titers: 500, epoch: 4 | loss: 0.0680254\n",
      "\tspeed: 0.1078s/iter; left time: 2637.3666s\n",
      "\titers: 600, epoch: 4 | loss: 0.0695731\n",
      "\tspeed: 0.1006s/iter; left time: 2451.2244s\n",
      "\titers: 700, epoch: 4 | loss: 0.0686705\n",
      "\tspeed: 0.0938s/iter; left time: 2275.6373s\n",
      "\titers: 800, epoch: 4 | loss: 0.0767548\n",
      "\tspeed: 0.0911s/iter; left time: 2201.5052s\n",
      "\titers: 900, epoch: 4 | loss: 0.0490004\n",
      "\tspeed: 0.1055s/iter; left time: 2538.7019s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0628900\n",
      "\tspeed: 0.1017s/iter; left time: 2438.1246s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0884633\n",
      "\tspeed: 0.1040s/iter; left time: 2482.2773s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0896056\n",
      "\tspeed: 0.1047s/iter; left time: 2488.0615s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0692411\n",
      "\tspeed: 0.1023s/iter; left time: 2420.5901s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0611375\n",
      "\tspeed: 0.1035s/iter; left time: 2439.3405s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0737480\n",
      "\tspeed: 0.0986s/iter; left time: 2312.8777s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0901903\n",
      "\tspeed: 0.1002s/iter; left time: 2341.6529s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0699836\n",
      "\tspeed: 0.1008s/iter; left time: 2344.0112s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0721211\n",
      "\tspeed: 0.1018s/iter; left time: 2358.0334s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0878837\n",
      "\tspeed: 0.1080s/iter; left time: 2490.1195s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0864065\n",
      "\tspeed: 0.1094s/iter; left time: 2511.2406s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0674135\n",
      "\tspeed: 0.1066s/iter; left time: 2438.1556s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0739628\n",
      "\tspeed: 0.1059s/iter; left time: 2411.2000s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0581623\n",
      "\tspeed: 0.0839s/iter; left time: 1901.3476s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0650130\n",
      "\tspeed: 0.1014s/iter; left time: 2288.8360s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0726438\n",
      "\tspeed: 0.1015s/iter; left time: 2279.9304s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0954770\n",
      "\tspeed: 0.0978s/iter; left time: 2186.4606s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0952775\n",
      "\tspeed: 0.1020s/iter; left time: 2270.0160s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0877698\n",
      "\tspeed: 0.1017s/iter; left time: 2253.9375s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0685904\n",
      "\tspeed: 0.1011s/iter; left time: 2231.3363s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0533736\n",
      "\tspeed: 0.1025s/iter; left time: 2251.7943s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0675482\n",
      "\tspeed: 0.1015s/iter; left time: 2218.4196s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0609828\n",
      "\tspeed: 0.1015s/iter; left time: 2207.9479s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0703463\n",
      "\tspeed: 0.0980s/iter; left time: 2123.7699s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0636662\n",
      "\tspeed: 0.1023s/iter; left time: 2204.9183s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0718687\n",
      "\tspeed: 0.1042s/iter; left time: 2236.1337s\n",
      "Epoch: 4 cost time: 00h:06m:03.04s\n",
      "Epoch: 4 | Train Loss: 0.0711842 Vali Loss: 0.0750714 Test Loss: 0.0840366\n",
      "Validation loss decreased (0.075130 --> 0.075071).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 5 | loss: 0.0734329\n",
      "\tspeed: 1.0343s/iter; left time: 22026.5958s\n",
      "\titers: 200, epoch: 5 | loss: 0.0771091\n",
      "\tspeed: 0.1095s/iter; left time: 2320.5177s\n",
      "\titers: 300, epoch: 5 | loss: 0.0714111\n",
      "\tspeed: 0.1070s/iter; left time: 2257.0338s\n",
      "\titers: 400, epoch: 5 | loss: 0.0683239\n",
      "\tspeed: 0.1071s/iter; left time: 2247.8196s\n",
      "\titers: 500, epoch: 5 | loss: 0.0850428\n",
      "\tspeed: 0.1086s/iter; left time: 2268.4453s\n",
      "\titers: 600, epoch: 5 | loss: 0.0713976\n",
      "\tspeed: 0.1097s/iter; left time: 2280.7754s\n",
      "\titers: 700, epoch: 5 | loss: 0.0713975\n",
      "\tspeed: 0.1081s/iter; left time: 2236.9963s\n",
      "\titers: 800, epoch: 5 | loss: 0.0631024\n",
      "\tspeed: 0.1077s/iter; left time: 2217.7825s\n",
      "\titers: 900, epoch: 5 | loss: 0.0584639\n",
      "\tspeed: 0.1068s/iter; left time: 2189.2806s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0775655\n",
      "\tspeed: 0.1023s/iter; left time: 2085.9071s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0632220\n",
      "\tspeed: 0.1051s/iter; left time: 2133.5271s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0836069\n",
      "\tspeed: 0.1055s/iter; left time: 2130.7154s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0737632\n",
      "\tspeed: 0.1038s/iter; left time: 2086.0883s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0728359\n",
      "\tspeed: 0.0958s/iter; left time: 1915.9544s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0541096\n",
      "\tspeed: 0.0835s/iter; left time: 1661.6913s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0603107\n",
      "\tspeed: 0.0836s/iter; left time: 1654.8538s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0888485\n",
      "\tspeed: 0.0883s/iter; left time: 1740.0984s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0655391\n",
      "\tspeed: 0.1030s/iter; left time: 2018.5900s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0643000\n",
      "\tspeed: 0.1075s/iter; left time: 2095.5647s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0545355\n",
      "\tspeed: 0.1084s/iter; left time: 2102.7656s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0691103\n",
      "\tspeed: 0.1078s/iter; left time: 2080.2286s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0727787\n",
      "\tspeed: 0.1078s/iter; left time: 2069.8696s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0637093\n",
      "\tspeed: 0.1070s/iter; left time: 2043.8875s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0946663\n",
      "\tspeed: 0.1069s/iter; left time: 2029.9371s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0769749\n",
      "\tspeed: 0.1072s/iter; left time: 2026.5928s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0556214\n",
      "\tspeed: 0.1065s/iter; left time: 2002.2442s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0592888\n",
      "\tspeed: 0.1080s/iter; left time: 2018.8116s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0776287\n",
      "\tspeed: 0.1026s/iter; left time: 1908.8558s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0624219\n",
      "\tspeed: 0.1025s/iter; left time: 1895.3924s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0679386\n",
      "\tspeed: 0.0990s/iter; left time: 1820.9712s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0714045\n",
      "\tspeed: 0.1004s/iter; left time: 1837.8219s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0672832\n",
      "\tspeed: 0.0980s/iter; left time: 1782.8004s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0646246\n",
      "\tspeed: 0.0972s/iter; left time: 1758.8962s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0775822\n",
      "\tspeed: 0.1003s/iter; left time: 1805.8413s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0754609\n",
      "\tspeed: 0.0958s/iter; left time: 1714.8076s\n",
      "Epoch: 5 cost time: 00h:06m:06.96s\n",
      "Epoch: 5 | Train Loss: 0.0702659 Vali Loss: 0.0733054 Test Loss: 0.0824435\n",
      "Validation loss decreased (0.075071 --> 0.073305).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 6 | loss: 0.0595976\n",
      "\tspeed: 1.0136s/iter; left time: 17971.8989s\n",
      "\titers: 200, epoch: 6 | loss: 0.0787350\n",
      "\tspeed: 0.1030s/iter; left time: 1815.3165s\n",
      "\titers: 300, epoch: 6 | loss: 0.0611242\n",
      "\tspeed: 0.0992s/iter; left time: 1738.6500s\n",
      "\titers: 400, epoch: 6 | loss: 0.0818065\n",
      "\tspeed: 0.1012s/iter; left time: 1764.7002s\n",
      "\titers: 500, epoch: 6 | loss: 0.0810350\n",
      "\tspeed: 0.1012s/iter; left time: 1753.4961s\n",
      "\titers: 600, epoch: 6 | loss: 0.0587074\n",
      "\tspeed: 0.1012s/iter; left time: 1743.0571s\n",
      "\titers: 700, epoch: 6 | loss: 0.0703090\n",
      "\tspeed: 0.1014s/iter; left time: 1737.5313s\n",
      "\titers: 800, epoch: 6 | loss: 0.0853346\n",
      "\tspeed: 0.1021s/iter; left time: 1738.1915s\n",
      "\titers: 900, epoch: 6 | loss: 0.0771472\n",
      "\tspeed: 0.0972s/iter; left time: 1646.5343s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0621617\n",
      "\tspeed: 0.1077s/iter; left time: 1813.1015s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0535752\n",
      "\tspeed: 0.0971s/iter; left time: 1624.8196s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0628508\n",
      "\tspeed: 0.0966s/iter; left time: 1606.6685s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0734800\n",
      "\tspeed: 0.0979s/iter; left time: 1617.5997s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0604587\n",
      "\tspeed: 0.0981s/iter; left time: 1612.6754s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0726916\n",
      "\tspeed: 0.1059s/iter; left time: 1730.2608s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0776222\n",
      "\tspeed: 0.1062s/iter; left time: 1723.5669s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0849937\n",
      "\tspeed: 0.1090s/iter; left time: 1758.2728s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0622263\n",
      "\tspeed: 0.1093s/iter; left time: 1752.2426s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0734927\n",
      "\tspeed: 0.1085s/iter; left time: 1728.1436s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0703206\n",
      "\tspeed: 0.0886s/iter; left time: 1402.3041s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0559907\n",
      "\tspeed: 0.0858s/iter; left time: 1349.7198s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0740385\n",
      "\tspeed: 0.0876s/iter; left time: 1369.5207s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0530195\n",
      "\tspeed: 0.0986s/iter; left time: 1531.0705s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0743242\n",
      "\tspeed: 0.1035s/iter; left time: 1597.4454s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0696135\n",
      "\tspeed: 0.1057s/iter; left time: 1619.7819s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0634545\n",
      "\tspeed: 0.0857s/iter; left time: 1304.7124s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0739783\n",
      "\tspeed: 0.1015s/iter; left time: 1535.6904s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0659225\n",
      "\tspeed: 0.1008s/iter; left time: 1514.4381s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0529330\n",
      "\tspeed: 0.1014s/iter; left time: 1514.4653s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0607648\n",
      "\tspeed: 0.0977s/iter; left time: 1449.1347s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0755738\n",
      "\tspeed: 0.1012s/iter; left time: 1490.5446s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0704174\n",
      "\tspeed: 0.1024s/iter; left time: 1498.5939s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0676856\n",
      "\tspeed: 0.1071s/iter; left time: 1555.6111s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0815410\n",
      "\tspeed: 0.1090s/iter; left time: 1572.9567s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0631517\n",
      "\tspeed: 0.1049s/iter; left time: 1503.1704s\n",
      "Epoch: 6 cost time: 00h:06m:00.42s\n",
      "Epoch: 6 | Train Loss: 0.0695557 Vali Loss: 0.0730720 Test Loss: 0.0821654\n",
      "Validation loss decreased (0.073305 --> 0.073072).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 7 | loss: 0.0669560\n",
      "\tspeed: 1.0079s/iter; left time: 14277.5987s\n",
      "\titers: 200, epoch: 7 | loss: 0.0734779\n",
      "\tspeed: 0.1085s/iter; left time: 1526.3299s\n",
      "\titers: 300, epoch: 7 | loss: 0.0512895\n",
      "\tspeed: 0.1079s/iter; left time: 1506.2456s\n",
      "\titers: 400, epoch: 7 | loss: 0.0642988\n",
      "\tspeed: 0.1034s/iter; left time: 1433.1695s\n",
      "\titers: 500, epoch: 7 | loss: 0.0720399\n",
      "\tspeed: 0.1009s/iter; left time: 1388.3358s\n",
      "\titers: 600, epoch: 7 | loss: 0.0552755\n",
      "\tspeed: 0.1014s/iter; left time: 1385.8126s\n",
      "\titers: 700, epoch: 7 | loss: 0.0749275\n",
      "\tspeed: 0.1084s/iter; left time: 1469.8335s\n",
      "\titers: 800, epoch: 7 | loss: 0.0594688\n",
      "\tspeed: 0.1036s/iter; left time: 1395.0888s\n",
      "\titers: 900, epoch: 7 | loss: 0.0731955\n",
      "\tspeed: 0.1055s/iter; left time: 1409.7643s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0660539\n",
      "\tspeed: 0.1033s/iter; left time: 1370.4773s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0686746\n",
      "\tspeed: 0.1036s/iter; left time: 1363.3828s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0661069\n",
      "\tspeed: 0.0963s/iter; left time: 1258.3479s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0664915\n",
      "\tspeed: 0.0985s/iter; left time: 1277.3296s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0683779\n",
      "\tspeed: 0.1088s/iter; left time: 1399.3438s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0584100\n",
      "\tspeed: 0.1097s/iter; left time: 1400.2374s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0580849\n",
      "\tspeed: 0.1103s/iter; left time: 1396.3528s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0624045\n",
      "\tspeed: 0.1062s/iter; left time: 1334.6355s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0655668\n",
      "\tspeed: 0.1077s/iter; left time: 1342.5522s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0759777\n",
      "\tspeed: 0.1017s/iter; left time: 1258.0215s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0877122\n",
      "\tspeed: 0.1001s/iter; left time: 1228.2696s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0533816\n",
      "\tspeed: 0.1088s/iter; left time: 1323.4795s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0552529\n",
      "\tspeed: 0.1099s/iter; left time: 1325.9781s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0825727\n",
      "\tspeed: 0.1073s/iter; left time: 1283.2558s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0895109\n",
      "\tspeed: 0.1095s/iter; left time: 1298.8025s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0637051\n",
      "\tspeed: 0.1096s/iter; left time: 1289.2543s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0541311\n",
      "\tspeed: 0.1091s/iter; left time: 1272.5105s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0766268\n",
      "\tspeed: 0.1084s/iter; left time: 1253.8492s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0613706\n",
      "\tspeed: 0.1067s/iter; left time: 1223.3179s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0596100\n",
      "\tspeed: 0.1058s/iter; left time: 1202.2584s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0525368\n",
      "\tspeed: 0.1028s/iter; left time: 1158.4118s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0703490\n",
      "\tspeed: 0.1026s/iter; left time: 1145.9779s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0707452\n",
      "\tspeed: 0.1021s/iter; left time: 1129.6262s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0723773\n",
      "\tspeed: 0.1099s/iter; left time: 1205.5060s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0639014\n",
      "\tspeed: 0.1091s/iter; left time: 1185.5665s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0704285\n",
      "\tspeed: 0.1102s/iter; left time: 1186.2372s\n",
      "Epoch: 7 cost time: 00h:06m:18.03s\n",
      "Epoch: 7 | Train Loss: 0.0689114 Vali Loss: 0.0735753 Test Loss: 0.0827112\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 8 | loss: 0.0563023\n",
      "\tspeed: 1.0070s/iter; left time: 10673.6317s\n",
      "\titers: 200, epoch: 8 | loss: 0.0657397\n",
      "\tspeed: 0.1096s/iter; left time: 1150.6756s\n",
      "\titers: 300, epoch: 8 | loss: 0.0645681\n",
      "\tspeed: 0.1095s/iter; left time: 1138.9919s\n",
      "\titers: 400, epoch: 8 | loss: 0.0765049\n",
      "\tspeed: 0.1046s/iter; left time: 1077.1493s\n",
      "\titers: 500, epoch: 8 | loss: 0.0836551\n",
      "\tspeed: 0.1090s/iter; left time: 1112.1008s\n",
      "\titers: 600, epoch: 8 | loss: 0.0688213\n",
      "\tspeed: 0.1095s/iter; left time: 1105.7303s\n",
      "\titers: 700, epoch: 8 | loss: 0.0578617\n",
      "\tspeed: 0.1096s/iter; left time: 1096.1797s\n",
      "\titers: 800, epoch: 8 | loss: 0.0770525\n",
      "\tspeed: 0.1098s/iter; left time: 1087.2904s\n",
      "\titers: 900, epoch: 8 | loss: 0.0914728\n",
      "\tspeed: 0.1102s/iter; left time: 1079.6141s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0698659\n",
      "\tspeed: 0.1100s/iter; left time: 1067.1698s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0682746\n",
      "\tspeed: 0.1107s/iter; left time: 1062.8348s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0841383\n",
      "\tspeed: 0.1089s/iter; left time: 1034.3435s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0946120\n",
      "\tspeed: 0.1036s/iter; left time: 973.4134s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0625262\n",
      "\tspeed: 0.1096s/iter; left time: 1018.8756s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0607226\n",
      "\tspeed: 0.1081s/iter; left time: 994.1267s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0746914\n",
      "\tspeed: 0.1109s/iter; left time: 1009.3840s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0729473\n",
      "\tspeed: 0.1104s/iter; left time: 993.7550s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0622026\n",
      "\tspeed: 0.1070s/iter; left time: 951.9821s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1025201\n",
      "\tspeed: 0.1058s/iter; left time: 930.6765s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0526033\n",
      "\tspeed: 0.1061s/iter; left time: 923.0345s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0618048\n",
      "\tspeed: 0.1074s/iter; left time: 923.2574s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0660734\n",
      "\tspeed: 0.1089s/iter; left time: 925.4242s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0746935\n",
      "\tspeed: 0.1076s/iter; left time: 903.5153s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0763705\n",
      "\tspeed: 0.1079s/iter; left time: 895.1121s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0762459\n",
      "\tspeed: 0.1072s/iter; left time: 878.8657s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0688709\n",
      "\tspeed: 0.1037s/iter; left time: 839.9614s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0732419\n",
      "\tspeed: 0.1109s/iter; left time: 886.7421s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0577185\n",
      "\tspeed: 0.1112s/iter; left time: 878.4821s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0800271\n",
      "\tspeed: 0.1089s/iter; left time: 849.3381s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0612930\n",
      "\tspeed: 0.1059s/iter; left time: 815.5166s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0570172\n",
      "\tspeed: 0.1094s/iter; left time: 831.3189s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0578851\n",
      "\tspeed: 0.1098s/iter; left time: 823.6814s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0545044\n",
      "\tspeed: 0.1106s/iter; left time: 818.1400s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0698128\n",
      "\tspeed: 0.1099s/iter; left time: 802.0030s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0574988\n",
      "\tspeed: 0.1097s/iter; left time: 790.0378s\n",
      "Epoch: 8 cost time: 00h:06m:27.75s\n",
      "Epoch: 8 | Train Loss: 0.0684275 Vali Loss: 0.0742783 Test Loss: 0.0835485\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 9 | loss: 0.0675568\n",
      "\tspeed: 1.0066s/iter; left time: 7079.1278s\n",
      "\titers: 200, epoch: 9 | loss: 0.0651613\n",
      "\tspeed: 0.1109s/iter; left time: 769.0460s\n",
      "\titers: 300, epoch: 9 | loss: 0.0539076\n",
      "\tspeed: 0.1103s/iter; left time: 753.5748s\n",
      "\titers: 400, epoch: 9 | loss: 0.0818302\n",
      "\tspeed: 0.1071s/iter; left time: 721.2492s\n",
      "\titers: 500, epoch: 9 | loss: 0.0801425\n",
      "\tspeed: 0.1069s/iter; left time: 708.8507s\n",
      "\titers: 600, epoch: 9 | loss: 0.0488272\n",
      "\tspeed: 0.1063s/iter; left time: 694.6422s\n",
      "\titers: 700, epoch: 9 | loss: 0.0668959\n",
      "\tspeed: 0.1061s/iter; left time: 682.6070s\n",
      "\titers: 800, epoch: 9 | loss: 0.0686616\n",
      "\tspeed: 0.1064s/iter; left time: 673.7968s\n",
      "\titers: 900, epoch: 9 | loss: 0.0655853\n",
      "\tspeed: 0.1048s/iter; left time: 653.4292s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0672343\n",
      "\tspeed: 0.1073s/iter; left time: 658.1628s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0556217\n",
      "\tspeed: 0.1070s/iter; left time: 645.3589s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0660226\n",
      "\tspeed: 0.1098s/iter; left time: 651.6077s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0594931\n",
      "\tspeed: 0.1099s/iter; left time: 641.2956s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0579290\n",
      "\tspeed: 0.1109s/iter; left time: 636.0000s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0606761\n",
      "\tspeed: 0.1112s/iter; left time: 626.4839s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0589912\n",
      "\tspeed: 0.1096s/iter; left time: 606.5320s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0680620\n",
      "\tspeed: 0.1111s/iter; left time: 603.6467s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0745860\n",
      "\tspeed: 0.1106s/iter; left time: 589.6628s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0524615\n",
      "\tspeed: 0.1057s/iter; left time: 553.2018s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0780820\n",
      "\tspeed: 0.1055s/iter; left time: 541.5679s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0632932\n",
      "\tspeed: 0.1073s/iter; left time: 540.2419s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0586291\n",
      "\tspeed: 0.1110s/iter; left time: 547.3800s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0707322\n",
      "\tspeed: 0.1099s/iter; left time: 531.2579s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0634186\n",
      "\tspeed: 0.1070s/iter; left time: 506.4072s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0595348\n",
      "\tspeed: 0.1074s/iter; left time: 497.6064s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0578244\n",
      "\tspeed: 0.1056s/iter; left time: 478.7451s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0661644\n",
      "\tspeed: 0.1067s/iter; left time: 473.1409s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0896046\n",
      "\tspeed: 0.1113s/iter; left time: 482.4414s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 200, in <module>\n",
      "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/data_loader.py\", line 462, in __iter__\n",
      "    next_batch = next(dataloader_iter)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1370, in _process_data\n",
      "    self._try_put_index()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1363, in _try_put_index\n",
      "    self._index_queues[worker_queue_idx].put((self._send_idx, index))\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/multiprocessing/queues.py\", line 96, in put\n",
      "    self._notempty.notify()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/threading.py\", line 375, in notify\n",
      "    waiter.release()\n",
      "KeyboardInterrupt\n",
      "Total time: 233.94862820307415 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.0001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrease complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=16\n",
    "d_ff=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 03:14:11,347] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 03:14:12,513] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 03:14:12,513] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 03:14:12,513] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 03:14:12,618] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 03:14:12,619] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 03:14:13,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 03:14:13,289] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 03:14:13,289] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 03:14:13,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 03:14:13,291] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 03:14:13,291] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 03:14:13,291] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 03:14:13,621] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 03:14:13,622] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,622] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,744] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 03:14:13,745] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,745] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,745] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 03:14:13,863] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 03:14:13,864] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 03:14:13,864] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 10.1%\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 03:14:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 03:14:13,865] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51c44ff910>\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 03:14:13,866] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 03:14:13,867] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 03:14:13,868] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1424896\n",
      "\tspeed: 0.1603s/iter; left time: 5701.1658s\n",
      "\titers: 200, epoch: 1 | loss: 0.1216655\n",
      "\tspeed: 0.1146s/iter; left time: 4064.7797s\n",
      "\titers: 300, epoch: 1 | loss: 0.1135953\n",
      "\tspeed: 0.1123s/iter; left time: 3969.4581s\n",
      "\titers: 400, epoch: 1 | loss: 0.0812779\n",
      "\tspeed: 0.1113s/iter; left time: 3923.4805s\n",
      "\titers: 500, epoch: 1 | loss: 0.0967521\n",
      "\tspeed: 0.1124s/iter; left time: 3950.9017s\n",
      "\titers: 600, epoch: 1 | loss: 0.1014863\n",
      "\tspeed: 0.1159s/iter; left time: 4062.8865s\n",
      "\titers: 700, epoch: 1 | loss: 0.0915894\n",
      "\tspeed: 0.1163s/iter; left time: 4067.3004s\n",
      "\titers: 800, epoch: 1 | loss: 0.0760651\n",
      "\tspeed: 0.1163s/iter; left time: 4055.9484s\n",
      "\titers: 900, epoch: 1 | loss: 0.0788776\n",
      "\tspeed: 0.1163s/iter; left time: 4042.5046s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0763655\n",
      "\tspeed: 0.1156s/iter; left time: 4007.0653s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0659469\n",
      "\tspeed: 0.1137s/iter; left time: 3929.2533s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0801397\n",
      "\tspeed: 0.1112s/iter; left time: 3830.5278s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0902781\n",
      "\tspeed: 0.1159s/iter; left time: 3982.1460s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0845647\n",
      "\tspeed: 0.1169s/iter; left time: 4006.0340s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0701626\n",
      "\tspeed: 0.1155s/iter; left time: 3946.4431s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0684285\n",
      "\tspeed: 0.1132s/iter; left time: 3854.3305s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0824512\n",
      "\tspeed: 0.1119s/iter; left time: 3800.7159s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0605585\n",
      "\tspeed: 0.1114s/iter; left time: 3772.5395s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0753151\n",
      "\tspeed: 0.1123s/iter; left time: 3792.8493s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0716163\n",
      "\tspeed: 0.1123s/iter; left time: 3778.8760s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0605470\n",
      "\tspeed: 0.1155s/iter; left time: 3876.3892s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0658180\n",
      "\tspeed: 0.1130s/iter; left time: 3782.5097s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0823635\n",
      "\tspeed: 0.1129s/iter; left time: 3767.1556s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0742289\n",
      "\tspeed: 0.1109s/iter; left time: 3689.3527s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0678603\n",
      "\tspeed: 0.1109s/iter; left time: 3678.4523s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0672556\n",
      "\tspeed: 0.1110s/iter; left time: 3669.3329s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0807025\n",
      "\tspeed: 0.0998s/iter; left time: 3289.3044s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0851708\n",
      "\tspeed: 0.0938s/iter; left time: 3082.1063s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0664944\n",
      "\tspeed: 0.1111s/iter; left time: 3640.1699s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0880232\n",
      "\tspeed: 0.1120s/iter; left time: 3656.4382s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0832897\n",
      "\tspeed: 0.1124s/iter; left time: 3658.7083s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0608761\n",
      "\tspeed: 0.1109s/iter; left time: 3600.5166s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0698125\n",
      "\tspeed: 0.1108s/iter; left time: 3584.4131s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0667007\n",
      "\tspeed: 0.1113s/iter; left time: 3591.6992s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0908326\n",
      "\tspeed: 0.1113s/iter; left time: 3579.9216s\n",
      "Epoch: 1 cost time: 00h:06m:41.55s\n",
      "Epoch: 1 | Train Loss: 0.0822845 Vali Loss: 0.0750525 Test Loss: 0.0838669\n",
      "Validation loss decreased (inf --> 0.075053).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0939135\n",
      "\tspeed: 1.1464s/iter; left time: 36678.9163s\n",
      "\titers: 200, epoch: 2 | loss: 0.0608462\n",
      "\tspeed: 0.1069s/iter; left time: 3410.7039s\n",
      "\titers: 300, epoch: 2 | loss: 0.0878162\n",
      "\tspeed: 0.1081s/iter; left time: 3437.1474s\n",
      "\titers: 400, epoch: 2 | loss: 0.0681369\n",
      "\tspeed: 0.1067s/iter; left time: 3380.4697s\n",
      "\titers: 500, epoch: 2 | loss: 0.0737712\n",
      "\tspeed: 0.1011s/iter; left time: 3195.6630s\n",
      "\titers: 600, epoch: 2 | loss: 0.0719987\n",
      "\tspeed: 0.1020s/iter; left time: 3210.9693s\n",
      "\titers: 700, epoch: 2 | loss: 0.0598605\n",
      "\tspeed: 0.1061s/iter; left time: 3329.8293s\n",
      "\titers: 800, epoch: 2 | loss: 0.0766290\n",
      "\tspeed: 0.1067s/iter; left time: 3338.8028s\n",
      "\titers: 900, epoch: 2 | loss: 0.0968150\n",
      "\tspeed: 0.1036s/iter; left time: 3233.0538s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0777338\n",
      "\tspeed: 0.1010s/iter; left time: 3140.0640s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0742226\n",
      "\tspeed: 0.1009s/iter; left time: 3128.9297s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0758907\n",
      "\tspeed: 0.1059s/iter; left time: 3273.1533s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0546048\n",
      "\tspeed: 0.1088s/iter; left time: 3351.3600s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0569216\n",
      "\tspeed: 0.1074s/iter; left time: 3295.8490s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0623859\n",
      "\tspeed: 0.1076s/iter; left time: 3293.1682s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0900649\n",
      "\tspeed: 0.1027s/iter; left time: 3131.5982s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0843206\n",
      "\tspeed: 0.1010s/iter; left time: 3070.1040s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0545513\n",
      "\tspeed: 0.1027s/iter; left time: 3111.2605s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0740867\n",
      "\tspeed: 0.1054s/iter; left time: 3181.2844s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0662565\n",
      "\tspeed: 0.1007s/iter; left time: 3031.9609s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0770249\n",
      "\tspeed: 0.1042s/iter; left time: 3124.9109s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0680723\n",
      "\tspeed: 0.1039s/iter; left time: 3107.0215s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0657464\n",
      "\tspeed: 0.1016s/iter; left time: 3026.7458s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0731734\n",
      "\tspeed: 0.1009s/iter; left time: 2995.8395s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0560416\n",
      "\tspeed: 0.1009s/iter; left time: 2985.1869s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0750969\n",
      "\tspeed: 0.1020s/iter; left time: 3009.5806s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0850314\n",
      "\tspeed: 0.1014s/iter; left time: 2981.7625s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0651263\n",
      "\tspeed: 0.0987s/iter; left time: 2892.1512s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0757619\n",
      "\tspeed: 0.1006s/iter; left time: 2936.1241s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0654464\n",
      "\tspeed: 0.1019s/iter; left time: 2963.4939s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0730120\n",
      "\tspeed: 0.1021s/iter; left time: 2960.6720s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0728501\n",
      "\tspeed: 0.1000s/iter; left time: 2888.3388s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0673412\n",
      "\tspeed: 0.0987s/iter; left time: 2841.7319s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0700444\n",
      "\tspeed: 0.1029s/iter; left time: 2951.3566s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0758945\n",
      "\tspeed: 0.1054s/iter; left time: 3014.3754s\n",
      "Epoch: 2 cost time: 00h:06m:09.13s\n",
      "Epoch: 2 | Train Loss: 0.0703329 Vali Loss: 0.0737976 Test Loss: 0.0828000\n",
      "Validation loss decreased (0.075053 --> 0.073798).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0808824\n",
      "\tspeed: 1.0209s/iter; left time: 29022.5604s\n",
      "\titers: 200, epoch: 3 | loss: 0.0815184\n",
      "\tspeed: 0.1076s/iter; left time: 3047.8401s\n",
      "\titers: 300, epoch: 3 | loss: 0.0870420\n",
      "\tspeed: 0.1061s/iter; left time: 2993.8540s\n",
      "\titers: 400, epoch: 3 | loss: 0.0793327\n",
      "\tspeed: 0.1017s/iter; left time: 2861.8225s\n",
      "\titers: 500, epoch: 3 | loss: 0.0821626\n",
      "\tspeed: 0.1014s/iter; left time: 2842.0881s\n",
      "\titers: 600, epoch: 3 | loss: 0.0724274\n",
      "\tspeed: 0.1076s/iter; left time: 3004.3496s\n",
      "\titers: 700, epoch: 3 | loss: 0.0620296\n",
      "\tspeed: 0.1093s/iter; left time: 3042.4611s\n",
      "\titers: 800, epoch: 3 | loss: 0.0657922\n",
      "\tspeed: 0.1088s/iter; left time: 3015.5622s\n",
      "\titers: 900, epoch: 3 | loss: 0.0808211\n",
      "\tspeed: 0.1013s/iter; left time: 2797.9522s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0673185\n",
      "\tspeed: 0.1050s/iter; left time: 2889.8331s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0721133\n",
      "\tspeed: 0.0971s/iter; left time: 2662.6821s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0694294\n",
      "\tspeed: 0.0956s/iter; left time: 2613.9948s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0946639\n",
      "\tspeed: 0.0917s/iter; left time: 2496.9612s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0837892\n",
      "\tspeed: 0.1073s/iter; left time: 2910.8547s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0588859\n",
      "\tspeed: 0.1095s/iter; left time: 2960.4854s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0794964\n",
      "\tspeed: 0.1087s/iter; left time: 2927.0299s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0612484\n",
      "\tspeed: 0.1086s/iter; left time: 2912.4573s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0708212\n",
      "\tspeed: 0.1103s/iter; left time: 2949.4036s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0597865\n",
      "\tspeed: 0.1060s/iter; left time: 2822.2823s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0816800\n",
      "\tspeed: 0.1088s/iter; left time: 2887.3360s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0771033\n",
      "\tspeed: 0.1005s/iter; left time: 2656.8888s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0774621\n",
      "\tspeed: 0.1059s/iter; left time: 2788.2415s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0717404\n",
      "\tspeed: 0.1079s/iter; left time: 2828.9438s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0534040\n",
      "\tspeed: 0.1051s/iter; left time: 2744.9886s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0659776\n",
      "\tspeed: 0.0980s/iter; left time: 2551.6795s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0575099\n",
      "\tspeed: 0.1023s/iter; left time: 2651.9897s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0676808\n",
      "\tspeed: 0.1006s/iter; left time: 2597.5943s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0968859\n",
      "\tspeed: 0.1015s/iter; left time: 2610.8087s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0860504\n",
      "\tspeed: 0.1094s/iter; left time: 2804.9158s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0572925\n",
      "\tspeed: 0.1084s/iter; left time: 2768.0889s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0730199\n",
      "\tspeed: 0.1110s/iter; left time: 2823.7652s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0750952\n",
      "\tspeed: 0.1035s/iter; left time: 2622.4682s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0757238\n",
      "\tspeed: 0.1101s/iter; left time: 2777.5383s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0755035\n",
      "\tspeed: 0.1091s/iter; left time: 2742.7412s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0583412\n",
      "\tspeed: 0.0997s/iter; left time: 2494.1664s\n",
      "Epoch: 3 cost time: 00h:06m:14.03s\n",
      "Epoch: 3 | Train Loss: 0.0681861 Vali Loss: 0.0728567 Test Loss: 0.0819308\n",
      "Validation loss decreased (0.073798 --> 0.072857).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0610442\n",
      "\tspeed: 1.0287s/iter; left time: 25576.5615s\n",
      "\titers: 200, epoch: 4 | loss: 0.0598010\n",
      "\tspeed: 0.1076s/iter; left time: 2665.0954s\n",
      "\titers: 300, epoch: 4 | loss: 0.0655513\n",
      "\tspeed: 0.1038s/iter; left time: 2559.1799s\n",
      "\titers: 400, epoch: 4 | loss: 0.0600016\n",
      "\tspeed: 0.0991s/iter; left time: 2434.3049s\n",
      "\titers: 500, epoch: 4 | loss: 0.0873438\n",
      "\tspeed: 0.1060s/iter; left time: 2593.5896s\n",
      "\titers: 600, epoch: 4 | loss: 0.0668786\n",
      "\tspeed: 0.1028s/iter; left time: 2504.2986s\n",
      "\titers: 700, epoch: 4 | loss: 0.0627359\n",
      "\tspeed: 0.1012s/iter; left time: 2454.4416s\n",
      "\titers: 800, epoch: 4 | loss: 0.0704189\n",
      "\tspeed: 0.1008s/iter; left time: 2434.4562s\n",
      "\titers: 900, epoch: 4 | loss: 0.0536439\n",
      "\tspeed: 0.1021s/iter; left time: 2456.8231s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0776928\n",
      "\tspeed: 0.1002s/iter; left time: 2400.5758s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0541186\n",
      "\tspeed: 0.0956s/iter; left time: 2281.1087s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0635243\n",
      "\tspeed: 0.0981s/iter; left time: 2331.9075s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0505764\n",
      "\tspeed: 0.1100s/iter; left time: 2604.0829s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0618619\n",
      "\tspeed: 0.1091s/iter; left time: 2570.6092s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0737609\n",
      "\tspeed: 0.1099s/iter; left time: 2578.4430s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0633889\n",
      "\tspeed: 0.1085s/iter; left time: 2535.8943s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0555208\n",
      "\tspeed: 0.1091s/iter; left time: 2537.5645s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0555905\n",
      "\tspeed: 0.1051s/iter; left time: 2434.6118s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0751671\n",
      "\tspeed: 0.1088s/iter; left time: 2510.0542s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0532623\n",
      "\tspeed: 0.1075s/iter; left time: 2469.0551s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0908642\n",
      "\tspeed: 0.1027s/iter; left time: 2348.0870s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0650437\n",
      "\tspeed: 0.1049s/iter; left time: 2386.9413s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0775387\n",
      "\tspeed: 0.1044s/iter; left time: 2365.1472s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0678262\n",
      "\tspeed: 0.1003s/iter; left time: 2263.8003s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0665249\n",
      "\tspeed: 0.1041s/iter; left time: 2339.1909s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0753792\n",
      "\tspeed: 0.0934s/iter; left time: 2088.6770s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0603315\n",
      "\tspeed: 0.0981s/iter; left time: 2185.0102s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0798816\n",
      "\tspeed: 0.1054s/iter; left time: 2334.8924s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 256, in <module>\n",
      "    accelerator.backward(loss)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1995, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 166, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1976, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2051, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Total time: 262.93945128122965 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
