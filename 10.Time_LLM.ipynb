{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=10\n",
    "learning_rate=0.001 # 10^-3\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "batch_size=128\n",
    "d_model=16\n",
    "d_ff=64\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-31 04:41:28,741] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 04:41:29,962] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 04:41:29,962] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 04:41:29,962] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 04:41:30,074] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 04:41:30,075] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 04:41:30,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 04:41:30,758] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 04:41:30,758] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 04:41:30,760] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 04:41:30,760] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 04:41:30,760] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 04:41:30,760] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 04:41:30,760] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 04:41:30,760] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 04:41:30,760] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 04:41:31,073] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 04:41:31,075] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 04:41:31,075] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.58 GB, percent = 10.3%\n",
      "[2024-10-31 04:41:31,194] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 04:41:31,195] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 04:41:31,195] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.58 GB, percent = 10.3%\n",
      "[2024-10-31 04:41:31,195] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 04:41:31,310] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 04:41:31,311] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 04:41:31,311] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.58 GB, percent = 10.3%\n",
      "[2024-10-31 04:41:31,312] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 04:41:31,312] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 04:41:31,312] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 04:41:31,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 04:41:31,313] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc45c0d5c50>\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 04:41:31,314] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 04:41:31,315] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 04:41:31,316] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 04:41:31,316] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 04:41:31,316] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 04:41:31,316] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 04:41:31,316] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1375228\n",
      "\tspeed: 0.4422s/iter; left time: 2918.7957s\n",
      "\titers: 200, epoch: 1 | loss: 0.1259014\n",
      "\tspeed: 0.3999s/iter; left time: 2599.6762s\n",
      "\titers: 300, epoch: 1 | loss: 0.0793642\n",
      "\tspeed: 0.3981s/iter; left time: 2548.5097s\n",
      "\titers: 400, epoch: 1 | loss: 0.0679271\n",
      "\tspeed: 0.3995s/iter; left time: 2517.4797s\n",
      "\titers: 500, epoch: 1 | loss: 0.0676044\n",
      "\tspeed: 0.4015s/iter; left time: 2489.4938s\n",
      "\titers: 600, epoch: 1 | loss: 0.0661930\n",
      "\tspeed: 0.4022s/iter; left time: 2453.9849s\n",
      "Epoch: 1 cost time: 00h:04m:29.29s\n",
      "Epoch: 1 | Train Loss: 0.0954351 Vali Loss: 0.0650205 Test Loss: 0.0705722\n",
      "Validation loss decreased (inf --> 0.065021).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0675441\n",
      "\tspeed: 1.5335s/iter; left time: 9094.9557s\n",
      "\titers: 200, epoch: 2 | loss: 0.0663845\n",
      "\tspeed: 0.3636s/iter; left time: 2120.2704s\n",
      "\titers: 300, epoch: 2 | loss: 0.0600896\n",
      "\tspeed: 0.3656s/iter; left time: 2095.0490s\n",
      "\titers: 400, epoch: 2 | loss: 0.0666059\n",
      "\tspeed: 0.3668s/iter; left time: 2065.2087s\n",
      "\titers: 500, epoch: 2 | loss: 0.0566242\n",
      "\tspeed: 0.3650s/iter; left time: 2019.0264s\n",
      "\titers: 600, epoch: 2 | loss: 0.0578280\n",
      "\tspeed: 0.3637s/iter; left time: 1975.1940s\n",
      "Epoch: 2 cost time: 00h:04m:04.03s\n",
      "Epoch: 2 | Train Loss: 0.0618546 Vali Loss: 0.0608053 Test Loss: 0.0656104\n",
      "Validation loss decreased (0.065021 --> 0.060805).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0696250\n",
      "\tspeed: 1.3798s/iter; left time: 7259.1396s\n",
      "\titers: 200, epoch: 3 | loss: 0.0589693\n",
      "\tspeed: 0.3646s/iter; left time: 1881.6450s\n",
      "\titers: 300, epoch: 3 | loss: 0.0617233\n",
      "\tspeed: 0.3646s/iter; left time: 1845.0451s\n",
      "\titers: 400, epoch: 3 | loss: 0.0559673\n",
      "\tspeed: 0.3638s/iter; left time: 1804.8340s\n",
      "\titers: 500, epoch: 3 | loss: 0.0572156\n",
      "\tspeed: 0.3625s/iter; left time: 1762.1780s\n",
      "\titers: 600, epoch: 3 | loss: 0.0528624\n",
      "\tspeed: 0.3654s/iter; left time: 1739.8726s\n",
      "Epoch: 3 cost time: 00h:04m:04.70s\n",
      "Epoch: 3 | Train Loss: 0.0594155 Vali Loss: 0.0605014 Test Loss: 0.0651596\n",
      "Validation loss decreased (0.060805 --> 0.060501).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0639340\n",
      "\tspeed: 1.4081s/iter; left time: 6464.4753s\n",
      "\titers: 200, epoch: 4 | loss: 0.0575449\n",
      "\tspeed: 0.3643s/iter; left time: 1636.1684s\n",
      "\titers: 300, epoch: 4 | loss: 0.0591267\n",
      "\tspeed: 0.3638s/iter; left time: 1597.6222s\n",
      "\titers: 400, epoch: 4 | loss: 0.0608316\n",
      "\tspeed: 0.3655s/iter; left time: 1568.2726s\n",
      "\titers: 500, epoch: 4 | loss: 0.0578988\n",
      "\tspeed: 0.3658s/iter; left time: 1533.2379s\n",
      "\titers: 600, epoch: 4 | loss: 0.0569705\n",
      "\tspeed: 0.3666s/iter; left time: 1499.5920s\n",
      "Epoch: 4 cost time: 00h:04m:05.00s\n",
      "Epoch: 4 | Train Loss: 0.0580582 Vali Loss: 0.0582450 Test Loss: 0.0625061\n",
      "Validation loss decreased (0.060501 --> 0.058245).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0520424\n",
      "\tspeed: 1.4191s/iter; left time: 5564.2153s\n",
      "\titers: 200, epoch: 5 | loss: 0.0503384\n",
      "\tspeed: 0.3630s/iter; left time: 1386.8703s\n",
      "\titers: 300, epoch: 5 | loss: 0.0505050\n",
      "\tspeed: 0.3629s/iter; left time: 1350.2808s\n",
      "\titers: 400, epoch: 5 | loss: 0.0544999\n",
      "\tspeed: 0.3644s/iter; left time: 1319.6400s\n",
      "\titers: 500, epoch: 5 | loss: 0.0625783\n",
      "\tspeed: 0.3642s/iter; left time: 1282.1871s\n",
      "\titers: 600, epoch: 5 | loss: 0.0623002\n",
      "\tspeed: 0.3656s/iter; left time: 1250.6001s\n",
      "Epoch: 5 cost time: 00h:04m:04.79s\n",
      "Epoch: 5 | Train Loss: 0.0569892 Vali Loss: 0.0591120 Test Loss: 0.0635082\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0576163\n",
      "\tspeed: 1.3380s/iter; left time: 4349.8218s\n",
      "\titers: 200, epoch: 6 | loss: 0.0550108\n",
      "\tspeed: 0.3624s/iter; left time: 1141.7948s\n",
      "\titers: 300, epoch: 6 | loss: 0.0538588\n",
      "\tspeed: 0.3674s/iter; left time: 1120.8131s\n",
      "\titers: 400, epoch: 6 | loss: 0.0552473\n",
      "\tspeed: 0.3630s/iter; left time: 1071.1410s\n",
      "\titers: 500, epoch: 6 | loss: 0.0594736\n",
      "\tspeed: 0.3616s/iter; left time: 1031.0241s\n",
      "\titers: 600, epoch: 6 | loss: 0.0548103\n",
      "\tspeed: 0.3638s/iter; left time: 1000.6764s\n",
      "Epoch: 6 cost time: 00h:04m:04.15s\n",
      "Epoch: 6 | Train Loss: 0.0561481 Vali Loss: 0.0572768 Test Loss: 0.0616070\n",
      "Validation loss decreased (0.058245 --> 0.057277).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0580104\n",
      "\tspeed: 1.3676s/iter; left time: 3529.8787s\n",
      "\titers: 200, epoch: 7 | loss: 0.0549729\n",
      "\tspeed: 0.3629s/iter; left time: 900.4102s\n",
      "\titers: 300, epoch: 7 | loss: 0.0618610\n",
      "\tspeed: 0.3635s/iter; left time: 865.4286s\n",
      "\titers: 400, epoch: 7 | loss: 0.0549115\n",
      "\tspeed: 0.3639s/iter; left time: 830.0195s\n",
      "\titers: 500, epoch: 7 | loss: 0.0600005\n",
      "\tspeed: 0.3637s/iter; left time: 793.2748s\n",
      "\titers: 600, epoch: 7 | loss: 0.0553411\n",
      "\tspeed: 0.3679s/iter; left time: 765.5823s\n",
      "Epoch: 7 cost time: 00h:04m:04.82s\n",
      "Epoch: 7 | Train Loss: 0.0553768 Vali Loss: 0.0581543 Test Loss: 0.0625749\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0532087\n",
      "\tspeed: 1.3342s/iter; left time: 2549.7092s\n",
      "\titers: 200, epoch: 8 | loss: 0.0515184\n",
      "\tspeed: 0.3613s/iter; left time: 654.3352s\n",
      "\titers: 300, epoch: 8 | loss: 0.0534131\n",
      "\tspeed: 0.3638s/iter; left time: 622.4741s\n",
      "\titers: 400, epoch: 8 | loss: 0.0554728\n",
      "\tspeed: 0.3644s/iter; left time: 587.1160s\n",
      "\titers: 500, epoch: 8 | loss: 0.0512503\n",
      "\tspeed: 0.3641s/iter; left time: 550.1172s\n",
      "\titers: 600, epoch: 8 | loss: 0.0480307\n",
      "\tspeed: 0.3656s/iter; left time: 515.8078s\n",
      "Epoch: 8 cost time: 00h:04m:04.09s\n",
      "Epoch: 8 | Train Loss: 0.0545969 Vali Loss: 0.0564154 Test Loss: 0.0605576\n",
      "Validation loss decreased (0.057277 --> 0.056415).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0508668\n",
      "\tspeed: 1.3891s/iter; left time: 1723.8464s\n",
      "\titers: 200, epoch: 9 | loss: 0.0509057\n",
      "\tspeed: 0.3623s/iter; left time: 413.4118s\n",
      "\titers: 300, epoch: 9 | loss: 0.0436420\n",
      "\tspeed: 0.3641s/iter; left time: 379.0123s\n",
      "\titers: 400, epoch: 9 | loss: 0.0607286\n",
      "\tspeed: 0.3642s/iter; left time: 342.6686s\n",
      "\titers: 500, epoch: 9 | loss: 0.0547512\n",
      "\tspeed: 0.3631s/iter; left time: 305.3904s\n",
      "\titers: 600, epoch: 9 | loss: 0.0493846\n",
      "\tspeed: 0.3633s/iter; left time: 269.1835s\n",
      "Epoch: 9 cost time: 00h:04m:04.35s\n",
      "Epoch: 9 | Train Loss: 0.0538918 Vali Loss: 0.0562257 Test Loss: 0.0600959\n",
      "Validation loss decreased (0.056415 --> 0.056226).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0500816\n",
      "\tspeed: 1.3836s/iter; left time: 790.0112s\n",
      "\titers: 200, epoch: 10 | loss: 0.0572331\n",
      "\tspeed: 0.3610s/iter; left time: 170.0328s\n",
      "\titers: 300, epoch: 10 | loss: 0.0518308\n",
      "\tspeed: 0.3640s/iter; left time: 135.0578s\n",
      "\titers: 400, epoch: 10 | loss: 0.0517949\n",
      "\tspeed: 0.3646s/iter; left time: 98.7963s\n",
      "\titers: 500, epoch: 10 | loss: 0.0516722\n",
      "\tspeed: 0.3653s/iter; left time: 62.4631s\n",
      "\titers: 600, epoch: 10 | loss: 0.0525138\n",
      "\tspeed: 0.3645s/iter; left time: 25.8806s\n",
      "Epoch: 10 cost time: 00h:04m:04.39s\n",
      "Epoch: 10 | Train Loss: 0.0533579 Vali Loss: 0.0552623 Test Loss: 0.0590334\n",
      "Validation loss decreased (0.056226 --> 0.055262).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "Scaled mse:0.010351366363465786, rmse:0.10174166411161423, mae:0.05903340503573418, rse:0.3940245509147644\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-31 05:36:05,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 05:36:05,970] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 05:36:05,971] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 05:36:05,971] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 05:36:05,972] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 05:36:05,972] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 05:36:05,972] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 05:36:05,972] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 05:36:05,972] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 05:36:05,973] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 05:36:05,973] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 05:36:06,325] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 05:36:06,326] [INFO] [utils.py:801:see_memory_usage] MA 1.49 GB         Max_MA 1.58 GB         CA 1.6 GB         Max_CA 2 GB \n",
      "[2024-10-31 05:36:06,326] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 78.86 GB, percent = 10.5%\n",
      "[2024-10-31 05:36:06,455] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 05:36:06,456] [INFO] [utils.py:801:see_memory_usage] MA 1.49 GB         Max_MA 1.68 GB         CA 1.79 GB         Max_CA 2 GB \n",
      "[2024-10-31 05:36:06,456] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 78.86 GB, percent = 10.5%\n",
      "[2024-10-31 05:36:06,456] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 05:36:06,582] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 05:36:06,583] [INFO] [utils.py:801:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 1.79 GB         Max_CA 2 GB \n",
      "[2024-10-31 05:36:06,583] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 78.86 GB, percent = 10.5%\n",
      "[2024-10-31 05:36:06,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 05:36:06,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 05:36:06,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 05:36:06,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc34ac898d0>\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 05:36:06,585] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 05:36:06,586] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 05:36:06,587] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 173, in <module>\n",
      "    train_loader, vali_loader, test_loader, model, model_optim, scheduler = accelerator.prepare(\n",
      "                                                                            ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1255, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1671, in _prepare_deepspeed\n",
      "    raise AssertionError(\n",
      "AssertionError: You can't use same `Accelerator()` instance with multiple models when using DeepSpeed\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1057, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 673, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/bin/python', './Time-LLM/run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './datasets/', '--data_path', 'FR_data.csv', '--model_id', '1', '--model', 'TimeLLM', '--data', 'FR', '--features', 'M', '--seq_len', '512', '--pred_len', '24', '--factor', '3', '--enc_in', '7', '--dec_in', '7', '--c_out', '7', '--des', 'Exp', '--itr', '2', '--patience', '3', '--llm_model', 'GPT2', '--llm_dim', '768', '--d_model', '16', '--d_ff', '64', '--batch_size', '128', '--learning_rate', '0.001', '--llm_layers', '12', '--train_epochs', '10', '--model_comment', 'TimeLLM-FR']' returned non-zero exit status 1.\n",
      "Total time: 55.03095538616181 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"24\"]\n",
    "\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 - shit, bs 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 14:16:36,050] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 14:16:36,071] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 14:16:37,183] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 14:16:37,183] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 14:16:37,183] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 85803\n",
      "val 18651\n",
      "train 85803\n",
      "test 18651\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-31 14:16:41,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 14:16:41,748] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 14:16:41,750] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 14:16:41,750] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 14:16:41,752] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 14:16:41,752] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 14:16:41,752] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 14:16:41,752] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 14:16:41,752] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 14:16:41,752] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 14:16:41,752] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 14:16:42,182] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 14:16:42,183] [INFO] [utils.py:801:see_memory_usage] MA 0.44 GB         Max_MA 0.49 GB         CA 0.51 GB         Max_CA 1 GB \n",
      "[2024-10-31 14:16:42,184] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 195.9 GB, percent = 26.0%\n",
      "[2024-10-31 14:16:42,333] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 14:16:42,334] [INFO] [utils.py:801:see_memory_usage] MA 0.44 GB         Max_MA 0.54 GB         CA 0.6 GB         Max_CA 1 GB \n",
      "[2024-10-31 14:16:42,335] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 195.99 GB, percent = 26.0%\n",
      "[2024-10-31 14:16:42,335] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 14:16:42,466] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 14:16:42,467] [INFO] [utils.py:801:see_memory_usage] MA 0.44 GB         Max_MA 0.44 GB         CA 0.6 GB         Max_CA 1 GB \n",
      "[2024-10-31 14:16:42,467] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 196.06 GB, percent = 26.0%\n",
      "[2024-10-31 14:16:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 14:16:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 14:16:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 14:16:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 14:16:42,468] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5c67b79e10>\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 14:16:42,469] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 14:16:42,470] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   world_size ................... 2\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 14:16:42,471] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1421028\n",
      "\tspeed: 0.3090s/iter; left time: 4110.6703s\n",
      "\titers: 200, epoch: 1 | loss: 0.1367325\n",
      "\tspeed: 0.4314s/iter; left time: 5695.3296s\n",
      "\titers: 300, epoch: 1 | loss: 0.0827058\n",
      "\tspeed: 0.4219s/iter; left time: 5527.2869s\n",
      "\titers: 400, epoch: 1 | loss: 0.0722992\n",
      "\tspeed: 0.4229s/iter; left time: 5498.4175s\n",
      "\titers: 500, epoch: 1 | loss: 0.0655111\n",
      "\tspeed: 0.4203s/iter; left time: 5422.5616s\n",
      "\titers: 600, epoch: 1 | loss: 0.0726018\n",
      "\tspeed: 0.4204s/iter; left time: 5381.7881s\n",
      "Epoch: 1 cost time: 00h:04m:31.06s\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/./Time-LLM/run_main.py\", line 268, in <module>\n",
      "    vali_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work/Time-LLM/utils/tools.py\", line 207, in vali\n",
      "    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2242, in gather_for_metrics\n",
      "    data = self.gather(input_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2205, in gather\n",
      "    return gather(tensor)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 378, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 439, in gather\n",
      "    return _gpu_gather(tensor)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 358, in _gpu_gather\n",
      "    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
      "    return honor_type(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
      "    return type(obj)(generator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
      "    recursively_apply(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 126, in recursively_apply\n",
      "    return func(data, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 348, in _gpu_gather_one\n",
      "    gather_op(output_tensors, tensor)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 72, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 2709, in all_gather_into_tensor\n",
      "    work = group._allgather_base(output_tensor, input_tensor, opts)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3\n",
      "ncclUnhandledCudaError: Call to CUDA function failed.\n",
      "Last error:\n",
      "Cuda failure 2 'out of memory'\n",
      "[2024-10-31 14:21:19,192] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 22474 closing signal SIGTERM\n",
      "[2024-10-31 14:21:19,307] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 22473) of binary: /vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./Time-LLM/run_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-10-31_14:21:19\n",
      "  host      : gruenau9.informatik.hu-berlin.de\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 22473)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "Total time: 43.043413213888805 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"24\"]\n",
    "batch_size=64\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 2\"\n",
    "\n",
    "!python -m accelerate.commands.launch --multi-gpu --mixed_precision bf16 --num_processes=2 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 87051\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-10-31 12:54:30,388] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 12:54:31,152] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 12:54:31,152] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 12:54:31,152] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 12:54:31,247] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 12:54:31,247] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 12:54:32,097] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 12:54:32,098] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 12:54:32,098] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 12:54:32,100] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 12:54:32,100] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 12:54:32,100] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 12:54:32,100] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 12:54:32,100] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 12:54:32,100] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 12:54:32,100] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 12:54:32,426] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 12:54:32,427] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 12:54:32,428] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.36 GB, percent = 10.3%\n",
      "[2024-10-31 12:54:32,549] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 12:54:32,550] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 12:54:32,550] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.36 GB, percent = 10.3%\n",
      "[2024-10-31 12:54:32,550] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 12:54:32,666] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 12:54:32,667] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 12:54:32,667] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.36 GB, percent = 10.3%\n",
      "[2024-10-31 12:54:32,668] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 12:54:32,668] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 12:54:32,668] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 12:54:32,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 12:54:32,669] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbef465dcd0>\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 12:54:32,670] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   train_batch_size ............. 64\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 12:54:32,671] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 12:54:32,672] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 12:54:32,672] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 12:54:32,672] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 12:54:32,672] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0690892\n",
      "\tspeed: 0.2359s/iter; left time: 3184.7012s\n",
      "\titers: 200, epoch: 1 | loss: 0.0716761\n",
      "\tspeed: 0.1922s/iter; left time: 2575.9246s\n",
      "\titers: 300, epoch: 1 | loss: 0.0665818\n",
      "\tspeed: 0.1943s/iter; left time: 2584.8549s\n",
      "\titers: 400, epoch: 1 | loss: 0.0671035\n",
      "\tspeed: 0.1931s/iter; left time: 2549.3320s\n",
      "\titers: 500, epoch: 1 | loss: 0.0633893\n",
      "\tspeed: 0.1893s/iter; left time: 2479.7417s\n",
      "\titers: 600, epoch: 1 | loss: 0.0688919\n",
      "\tspeed: 0.1916s/iter; left time: 2491.6156s\n",
      "\titers: 700, epoch: 1 | loss: 0.0689796\n",
      "\tspeed: 0.1915s/iter; left time: 2470.2875s\n",
      "\titers: 800, epoch: 1 | loss: 0.0678944\n",
      "\tspeed: 0.1906s/iter; left time: 2440.4418s\n",
      "\titers: 900, epoch: 1 | loss: 0.0647597\n",
      "\tspeed: 0.1917s/iter; left time: 2434.8514s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0661362\n",
      "\tspeed: 0.1909s/iter; left time: 2405.6905s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0713161\n",
      "\tspeed: 0.1908s/iter; left time: 2385.1602s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0613043\n",
      "\tspeed: 0.1898s/iter; left time: 2353.4939s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0787224\n",
      "\tspeed: 0.1904s/iter; left time: 2341.5326s\n",
      "Epoch: 1 cost time: 00h:04m:21.93s\n",
      "Epoch: 1 | Train Loss: 0.0710112 Vali Loss: 0.0688936 Test Loss: 0.0728857\n",
      "Validation loss decreased (inf --> 0.068894).  Saving model ...\n",
      "lr = 0.0004000000\n",
      "\titers: 100, epoch: 2 | loss: 0.0615751\n",
      "\tspeed: 1.0999s/iter; left time: 13353.8434s\n",
      "\titers: 200, epoch: 2 | loss: 0.0693025\n",
      "\tspeed: 0.1717s/iter; left time: 2067.3975s\n",
      "\titers: 300, epoch: 2 | loss: 0.0700133\n",
      "\tspeed: 0.1723s/iter; left time: 2057.9244s\n",
      "\titers: 400, epoch: 2 | loss: 0.0629280\n",
      "\tspeed: 0.1716s/iter; left time: 2031.5802s\n",
      "\titers: 500, epoch: 2 | loss: 0.0720696\n",
      "\tspeed: 0.1715s/iter; left time: 2013.9920s\n",
      "\titers: 600, epoch: 2 | loss: 0.0609021\n",
      "\tspeed: 0.1710s/iter; left time: 1990.3299s\n",
      "\titers: 700, epoch: 2 | loss: 0.0560531\n",
      "\tspeed: 0.1706s/iter; left time: 1969.4067s\n",
      "\titers: 800, epoch: 2 | loss: 0.0608460\n",
      "\tspeed: 0.1712s/iter; left time: 1959.2612s\n",
      "\titers: 900, epoch: 2 | loss: 0.0662128\n",
      "\tspeed: 0.1712s/iter; left time: 1941.2888s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0614207\n",
      "\tspeed: 0.1722s/iter; left time: 1935.6219s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0661629\n",
      "\tspeed: 0.1721s/iter; left time: 1917.7211s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0654974\n",
      "\tspeed: 0.1721s/iter; left time: 1900.4796s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0636230\n",
      "\tspeed: 0.1733s/iter; left time: 1896.4084s\n",
      "Epoch: 2 cost time: 00h:03m:54.01s\n",
      "Epoch: 2 | Train Loss: 0.0660190 Vali Loss: 0.0673283 Test Loss: 0.0715832\n",
      "Validation loss decreased (0.068894 --> 0.067328).  Saving model ...\n",
      "lr = 0.0004000000\n",
      "\titers: 100, epoch: 3 | loss: 0.0584215\n",
      "\tspeed: 0.9981s/iter; left time: 10760.4550s\n",
      "\titers: 200, epoch: 3 | loss: 0.0569576\n",
      "\tspeed: 0.1715s/iter; left time: 1831.7840s\n",
      "\titers: 300, epoch: 3 | loss: 0.0591433\n",
      "\tspeed: 0.1712s/iter; left time: 1811.9878s\n",
      "\titers: 400, epoch: 3 | loss: 0.0581877\n",
      "\tspeed: 0.1716s/iter; left time: 1798.9943s\n",
      "\titers: 500, epoch: 3 | loss: 0.0631398\n",
      "\tspeed: 0.1720s/iter; left time: 1785.7184s\n",
      "\titers: 600, epoch: 3 | loss: 0.0715547\n",
      "\tspeed: 0.1713s/iter; left time: 1761.4898s\n",
      "\titers: 700, epoch: 3 | loss: 0.0654168\n",
      "\tspeed: 0.1719s/iter; left time: 1750.3330s\n",
      "\titers: 800, epoch: 3 | loss: 0.0577190\n",
      "\tspeed: 0.1720s/iter; left time: 1734.1961s\n",
      "\titers: 900, epoch: 3 | loss: 0.0718568\n",
      "\tspeed: 0.1730s/iter; left time: 1727.1132s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0568200\n",
      "\tspeed: 0.1729s/iter; left time: 1708.9189s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0678742\n",
      "\tspeed: 0.1714s/iter; left time: 1676.7312s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0673134\n",
      "\tspeed: 0.1709s/iter; left time: 1654.5046s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0716605\n",
      "\tspeed: 0.1709s/iter; left time: 1637.6412s\n",
      "Epoch: 3 cost time: 00h:03m:54.01s\n",
      "Epoch: 3 | Train Loss: 0.0649025 Vali Loss: 0.0671369 Test Loss: 0.0711188\n",
      "Validation loss decreased (0.067328 --> 0.067137).  Saving model ...\n",
      "lr = 0.0004000000\n",
      "\titers: 100, epoch: 4 | loss: 0.0625824\n",
      "\tspeed: 1.0008s/iter; left time: 9428.2632s\n",
      "\titers: 200, epoch: 4 | loss: 0.0623496\n",
      "\tspeed: 0.1753s/iter; left time: 1634.1978s\n",
      "\titers: 300, epoch: 4 | loss: 0.0854841\n",
      "\tspeed: 0.1702s/iter; left time: 1569.0874s\n",
      "\titers: 400, epoch: 4 | loss: 0.0713636\n",
      "\tspeed: 0.1709s/iter; left time: 1558.9896s\n",
      "\titers: 500, epoch: 4 | loss: 0.0693465\n",
      "\tspeed: 0.1711s/iter; left time: 1543.3512s\n",
      "\titers: 600, epoch: 4 | loss: 0.0658263\n",
      "\tspeed: 0.1708s/iter; left time: 1523.3764s\n",
      "\titers: 700, epoch: 4 | loss: 0.0615876\n",
      "\tspeed: 0.1714s/iter; left time: 1511.7019s\n",
      "\titers: 800, epoch: 4 | loss: 0.0670046\n",
      "\tspeed: 0.1725s/iter; left time: 1504.0881s\n",
      "\titers: 900, epoch: 4 | loss: 0.0632667\n",
      "\tspeed: 0.1729s/iter; left time: 1490.3830s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0640918\n",
      "\tspeed: 0.1732s/iter; left time: 1475.7374s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0619269\n",
      "\tspeed: 0.1717s/iter; left time: 1445.7698s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0668435\n",
      "\tspeed: 0.1727s/iter; left time: 1436.6884s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0522618\n",
      "\tspeed: 0.1728s/iter; left time: 1420.2299s\n",
      "Epoch: 4 cost time: 00h:03m:54.73s\n",
      "Epoch: 4 | Train Loss: 0.0648874 Vali Loss: 0.0679019 Test Loss: 0.0722398\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0004000000\n",
      "\titers: 100, epoch: 5 | loss: 0.0667812\n",
      "\tspeed: 0.9526s/iter; left time: 7678.7287s\n",
      "\titers: 200, epoch: 5 | loss: 0.0598151\n",
      "\tspeed: 0.1715s/iter; left time: 1364.9938s\n",
      "\titers: 300, epoch: 5 | loss: 0.0757703\n",
      "\tspeed: 0.1711s/iter; left time: 1345.0309s\n",
      "\titers: 400, epoch: 5 | loss: 0.0527612\n",
      "\tspeed: 0.1731s/iter; left time: 1343.2468s\n",
      "\titers: 500, epoch: 5 | loss: 0.0670663\n",
      "\tspeed: 0.1731s/iter; left time: 1326.3393s\n",
      "\titers: 600, epoch: 5 | loss: 0.0713214\n",
      "\tspeed: 0.1717s/iter; left time: 1297.9591s\n",
      "\titers: 700, epoch: 5 | loss: 0.0653327\n",
      "\tspeed: 0.1713s/iter; left time: 1277.7460s\n",
      "\titers: 800, epoch: 5 | loss: 0.0621241\n",
      "\tspeed: 0.1726s/iter; left time: 1270.4638s\n",
      "\titers: 900, epoch: 5 | loss: 0.0664815\n",
      "\tspeed: 0.1713s/iter; left time: 1243.6003s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0554401\n",
      "\tspeed: 0.1728s/iter; left time: 1237.1252s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0638251\n",
      "\tspeed: 0.1721s/iter; left time: 1215.2243s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0629005\n",
      "\tspeed: 0.1717s/iter; left time: 1195.4958s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0720614\n",
      "\tspeed: 0.1725s/iter; left time: 1183.7777s\n",
      "Epoch: 5 cost time: 00h:03m:54.22s\n",
      "Epoch: 5 | Train Loss: 0.0653761 Vali Loss: 0.0674197 Test Loss: 0.0718522\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0004000000\n",
      "\titers: 100, epoch: 6 | loss: 0.0672867\n",
      "\tspeed: 0.9482s/iter; left time: 6353.8693s\n",
      "\titers: 200, epoch: 6 | loss: 0.0744749\n",
      "\tspeed: 0.1733s/iter; left time: 1144.2590s\n",
      "\titers: 300, epoch: 6 | loss: 0.0647254\n",
      "\tspeed: 0.1722s/iter; left time: 1119.4454s\n",
      "\titers: 400, epoch: 6 | loss: 0.0543054\n",
      "\tspeed: 0.1716s/iter; left time: 1098.5481s\n",
      "\titers: 500, epoch: 6 | loss: 0.0650821\n",
      "\tspeed: 0.1700s/iter; left time: 1071.1254s\n",
      "\titers: 600, epoch: 6 | loss: 0.0687747\n",
      "\tspeed: 0.1709s/iter; left time: 1059.9406s\n",
      "\titers: 700, epoch: 6 | loss: 0.0648349\n",
      "\tspeed: 0.1719s/iter; left time: 1048.6295s\n",
      "\titers: 800, epoch: 6 | loss: 0.0617772\n",
      "\tspeed: 0.1724s/iter; left time: 1034.6862s\n",
      "\titers: 900, epoch: 6 | loss: 0.0566690\n",
      "\tspeed: 0.1737s/iter; left time: 1025.2940s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0597250\n",
      "\tspeed: 0.1729s/iter; left time: 1003.1115s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0631992\n",
      "\tspeed: 0.1741s/iter; left time: 992.7170s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0572735\n",
      "\tspeed: 0.1726s/iter; left time: 966.6625s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0672784\n",
      "\tspeed: 0.1725s/iter; left time: 948.8881s\n",
      "Epoch: 6 cost time: 00h:03m:54.81s\n",
      "Epoch: 6 | Train Loss: 0.0644464 Vali Loss: 0.0671435 Test Loss: 0.0703476\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.013321156613528728, rmse:0.11541731655597687, mae:0.07111877202987671, rse:0.44527462124824524\n",
      "success delete checkpoints\n",
      "Total time: 32.09897424777349 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"24\"]\n",
    "\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96 - shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 05:36:28,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 05:36:29,679] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 05:36:29,680] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 05:36:29,680] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 05:36:29,774] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 05:36:29,774] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 05:36:30,431] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 05:36:30,432] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 05:36:30,432] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 05:36:30,434] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 05:36:30,434] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 05:36:30,434] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 05:36:30,434] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 05:36:30,434] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 05:36:30,434] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 05:36:30,434] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 05:36:30,705] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 05:36:30,706] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 05:36:30,706] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.65 GB, percent = 10.3%\n",
      "[2024-10-31 05:36:30,825] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 05:36:30,826] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 05:36:30,826] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.65 GB, percent = 10.3%\n",
      "[2024-10-31 05:36:30,826] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 05:36:30,937] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 05:36:30,938] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 05:36:30,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.65 GB, percent = 10.3%\n",
      "[2024-10-31 05:36:30,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 05:36:30,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 05:36:30,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 05:36:30,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 05:36:30,940] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f36052f4510>\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 05:36:30,941] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 05:36:30,942] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 05:36:30,943] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1301049\n",
      "\tspeed: 0.4439s/iter; left time: 2921.0342s\n",
      "\titers: 200, epoch: 1 | loss: 0.1255479\n",
      "\tspeed: 0.4009s/iter; left time: 2598.1130s\n",
      "\titers: 300, epoch: 1 | loss: 0.1306805\n",
      "\tspeed: 0.3998s/iter; left time: 2551.0624s\n",
      "\titers: 400, epoch: 1 | loss: 0.1045165\n",
      "\tspeed: 0.4001s/iter; left time: 2513.0975s\n",
      "\titers: 500, epoch: 1 | loss: 0.0872709\n",
      "\tspeed: 0.3995s/iter; left time: 2469.4686s\n",
      "\titers: 600, epoch: 1 | loss: 0.0870127\n",
      "\tspeed: 0.3983s/iter; left time: 2422.2779s\n",
      "Epoch: 1 cost time: 00h:04m:28.28s\n",
      "Epoch: 1 | Train Loss: 0.1165941 Vali Loss: 0.0803665 Test Loss: 0.0890354\n",
      "Validation loss decreased (inf --> 0.080366).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0780755\n",
      "\tspeed: 1.5897s/iter; left time: 9400.0553s\n",
      "\titers: 200, epoch: 2 | loss: 0.0843048\n",
      "\tspeed: 0.3629s/iter; left time: 2109.4851s\n",
      "\titers: 300, epoch: 2 | loss: 0.0826227\n",
      "\tspeed: 0.3682s/iter; left time: 2103.3037s\n",
      "\titers: 400, epoch: 2 | loss: 0.0715861\n",
      "\tspeed: 0.3652s/iter; left time: 2050.0721s\n",
      "\titers: 500, epoch: 2 | loss: 0.0679271\n",
      "\tspeed: 0.3643s/iter; left time: 2008.5317s\n",
      "\titers: 600, epoch: 2 | loss: 0.0699252\n",
      "\tspeed: 0.3649s/iter; left time: 1975.0662s\n",
      "Epoch: 2 cost time: 00h:04m:03.55s\n",
      "Epoch: 2 | Train Loss: 0.0752453 Vali Loss: 0.0762414 Test Loss: 0.0851909\n",
      "Validation loss decreased (0.080366 --> 0.076241).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0744916\n",
      "\tspeed: 1.3981s/iter; left time: 7333.1398s\n",
      "\titers: 200, epoch: 3 | loss: 0.0749282\n",
      "\tspeed: 0.3654s/iter; left time: 1879.9894s\n",
      "\titers: 300, epoch: 3 | loss: 0.0695439\n",
      "\tspeed: 0.3668s/iter; left time: 1850.5005s\n",
      "\titers: 400, epoch: 3 | loss: 0.0838452\n",
      "\tspeed: 0.3660s/iter; left time: 1810.0653s\n",
      "\titers: 500, epoch: 3 | loss: 0.0637714\n",
      "\tspeed: 0.3646s/iter; left time: 1766.7280s\n",
      "\titers: 600, epoch: 3 | loss: 0.0724411\n",
      "\tspeed: 0.3653s/iter; left time: 1733.3801s\n",
      "Epoch: 3 cost time: 00h:04m:04.61s\n",
      "Epoch: 3 | Train Loss: 0.0726079 Vali Loss: 0.0740470 Test Loss: 0.0830085\n",
      "Validation loss decreased (0.076241 --> 0.074047).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0697492\n",
      "\tspeed: 1.3876s/iter; left time: 6350.8813s\n",
      "\titers: 200, epoch: 4 | loss: 0.0780018\n",
      "\tspeed: 0.3661s/iter; left time: 1639.1033s\n",
      "\titers: 300, epoch: 4 | loss: 0.0656948\n",
      "\tspeed: 0.3657s/iter; left time: 1600.7615s\n",
      "\titers: 400, epoch: 4 | loss: 0.0749767\n",
      "\tspeed: 0.3654s/iter; left time: 1562.6316s\n",
      "\titers: 500, epoch: 4 | loss: 0.0665112\n",
      "\tspeed: 0.3650s/iter; left time: 1524.8072s\n",
      "\titers: 600, epoch: 4 | loss: 0.0699689\n",
      "\tspeed: 0.3636s/iter; left time: 1482.4191s\n",
      "Epoch: 4 cost time: 00h:04m:04.61s\n",
      "Epoch: 4 | Train Loss: 0.0709794 Vali Loss: 0.0745581 Test Loss: 0.0837152\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0642799\n",
      "\tspeed: 1.3294s/iter; left time: 5196.7523s\n",
      "\titers: 200, epoch: 5 | loss: 0.0639629\n",
      "\tspeed: 0.3673s/iter; left time: 1399.1068s\n",
      "\titers: 300, epoch: 5 | loss: 0.0642931\n",
      "\tspeed: 0.3635s/iter; left time: 1348.1521s\n",
      "\titers: 400, epoch: 5 | loss: 0.0697565\n",
      "\tspeed: 0.3646s/iter; left time: 1315.8302s\n",
      "\titers: 500, epoch: 5 | loss: 0.0619847\n",
      "\tspeed: 0.3646s/iter; left time: 1279.2348s\n",
      "\titers: 600, epoch: 5 | loss: 0.0771494\n",
      "\tspeed: 0.3654s/iter; left time: 1245.5121s\n",
      "Epoch: 5 cost time: 00h:04m:04.32s\n",
      "Epoch: 5 | Train Loss: 0.0700126 Vali Loss: 0.0728322 Test Loss: 0.0821955\n",
      "Validation loss decreased (0.074047 --> 0.072832).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0662860\n",
      "\tspeed: 1.3588s/iter; left time: 4403.7370s\n",
      "\titers: 200, epoch: 6 | loss: 0.0711219\n",
      "\tspeed: 0.3632s/iter; left time: 1140.8147s\n",
      "\titers: 300, epoch: 6 | loss: 0.0663035\n",
      "\tspeed: 0.3635s/iter; left time: 1105.4626s\n",
      "\titers: 400, epoch: 6 | loss: 0.0631941\n",
      "\tspeed: 0.3640s/iter; left time: 1070.5309s\n",
      "\titers: 500, epoch: 6 | loss: 0.0728785\n",
      "\tspeed: 0.3645s/iter; left time: 1035.6018s\n",
      "\titers: 600, epoch: 6 | loss: 0.0708836\n",
      "\tspeed: 0.3663s/iter; left time: 1004.0825s\n",
      "Epoch: 6 cost time: 00h:04m:04.10s\n",
      "Epoch: 6 | Train Loss: 0.0690471 Vali Loss: 0.0729586 Test Loss: 0.0825733\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0653445\n",
      "\tspeed: 1.3284s/iter; left time: 3417.8643s\n",
      "\titers: 200, epoch: 7 | loss: 0.0599990\n",
      "\tspeed: 0.3643s/iter; left time: 900.9622s\n",
      "\titers: 300, epoch: 7 | loss: 0.0609464\n",
      "\tspeed: 0.3647s/iter; left time: 865.4875s\n",
      "\titers: 400, epoch: 7 | loss: 0.0772454\n",
      "\tspeed: 0.3632s/iter; left time: 825.4669s\n",
      "\titers: 500, epoch: 7 | loss: 0.0647058\n",
      "\tspeed: 0.3637s/iter; left time: 790.3520s\n",
      "\titers: 600, epoch: 7 | loss: 0.0680623\n",
      "\tspeed: 0.3644s/iter; left time: 755.4818s\n",
      "Epoch: 7 cost time: 00h:04m:03.73s\n",
      "Epoch: 7 | Train Loss: 0.0682513 Vali Loss: 0.0729437 Test Loss: 0.0822447\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0702015\n",
      "\tspeed: 1.3264s/iter; left time: 2526.7955s\n",
      "\titers: 200, epoch: 8 | loss: 0.0654982\n",
      "\tspeed: 0.3659s/iter; left time: 660.4833s\n",
      "\titers: 300, epoch: 8 | loss: 0.0666074\n",
      "\tspeed: 0.3647s/iter; left time: 621.8592s\n",
      "\titers: 400, epoch: 8 | loss: 0.0585195\n",
      "\tspeed: 0.3636s/iter; left time: 583.5027s\n",
      "\titers: 500, epoch: 8 | loss: 0.0583894\n",
      "\tspeed: 0.3662s/iter; left time: 551.1459s\n",
      "\titers: 600, epoch: 8 | loss: 0.0714952\n",
      "\tspeed: 0.3647s/iter; left time: 512.4269s\n",
      "Epoch: 8 cost time: 00h:04m:04.22s\n",
      "Epoch: 8 | Train Loss: 0.0673482 Vali Loss: 0.0730424 Test Loss: 0.0828886\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018524011597037315, rmse:0.13610294461250305, mae:0.08219549059867859, rse:0.526524543762207\n",
      "success delete checkpoints\n",
      "Total time: 99.16904573440551 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96, lr descrease: 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 06:20:37,160] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 06:20:38,249] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 06:20:38,250] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 06:20:38,250] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 06:20:38,355] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 06:20:38,356] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 06:20:39,041] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 06:20:39,042] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 06:20:39,042] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 06:20:39,044] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 06:20:39,044] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 06:20:39,044] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 06:20:39,044] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 06:20:39,044] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 06:20:39,044] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 06:20:39,044] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 06:20:39,367] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 06:20:39,368] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-10-31 06:20:39,368] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.8 GB, percent = 10.3%\n",
      "[2024-10-31 06:20:39,494] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 06:20:39,495] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 06:20:39,496] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.8 GB, percent = 10.3%\n",
      "[2024-10-31 06:20:39,496] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 06:20:39,608] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 06:20:39,609] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-10-31 06:20:39,609] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.8 GB, percent = 10.3%\n",
      "[2024-10-31 06:20:39,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 06:20:39,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 06:20:39,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 06:20:39,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 06:20:39,611] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3c02db7590>\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 06:20:39,612] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 06:20:39,613] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 06:20:39,614] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 06:20:39,614] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 06:20:39,614] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 06:20:39,614] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 06:20:39,614] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1738079\n",
      "\tspeed: 0.4436s/iter; left time: 2919.3957s\n",
      "\titers: 200, epoch: 1 | loss: 0.1391886\n",
      "\tspeed: 0.4017s/iter; left time: 2603.3092s\n",
      "\titers: 300, epoch: 1 | loss: 0.1375185\n",
      "\tspeed: 0.4036s/iter; left time: 2575.3743s\n",
      "\titers: 400, epoch: 1 | loss: 0.1217643\n",
      "\tspeed: 0.4007s/iter; left time: 2516.9941s\n",
      "\titers: 500, epoch: 1 | loss: 0.1324808\n",
      "\tspeed: 0.4009s/iter; left time: 2477.6672s\n",
      "\titers: 600, epoch: 1 | loss: 0.1313976\n",
      "\tspeed: 0.3994s/iter; left time: 2428.8240s\n",
      "Epoch: 1 cost time: 00h:04m:29.13s\n",
      "Epoch: 1 | Train Loss: 0.1516316 Vali Loss: 0.1516823 Test Loss: 0.1694325\n",
      "Validation loss decreased (inf --> 0.151682).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 2 | loss: 0.1274338\n",
      "\tspeed: 1.5518s/iter; left time: 9175.5155s\n",
      "\titers: 200, epoch: 2 | loss: 0.1382792\n",
      "\tspeed: 0.3640s/iter; left time: 2115.8173s\n",
      "\titers: 300, epoch: 2 | loss: 0.1315838\n",
      "\tspeed: 0.3672s/iter; left time: 2097.5971s\n",
      "\titers: 400, epoch: 2 | loss: 0.1321498\n",
      "\tspeed: 0.3649s/iter; left time: 2048.0525s\n",
      "\titers: 500, epoch: 2 | loss: 0.1269793\n",
      "\tspeed: 0.3667s/iter; left time: 2021.7298s\n",
      "\titers: 600, epoch: 2 | loss: 0.1322992\n",
      "\tspeed: 0.3664s/iter; left time: 1983.3327s\n",
      "Epoch: 2 cost time: 00h:04m:04.30s\n",
      "Epoch: 2 | Train Loss: 0.1299404 Vali Loss: 0.1513301 Test Loss: 0.1691668\n",
      "Validation loss decreased (0.151682 --> 0.151330).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 3 | loss: 0.1272960\n",
      "\tspeed: 1.3597s/iter; left time: 7131.5088s\n",
      "\titers: 200, epoch: 3 | loss: 0.1355766\n",
      "\tspeed: 0.3668s/iter; left time: 1887.4072s\n",
      "\titers: 300, epoch: 3 | loss: 0.1231090\n",
      "\tspeed: 0.3650s/iter; left time: 1841.2278s\n",
      "\titers: 400, epoch: 3 | loss: 0.1292024\n",
      "\tspeed: 0.3671s/iter; left time: 1815.1934s\n",
      "\titers: 500, epoch: 3 | loss: 0.1194053\n",
      "\tspeed: 0.3637s/iter; left time: 1762.2230s\n",
      "\titers: 600, epoch: 3 | loss: 0.1300708\n",
      "\tspeed: 0.3657s/iter; left time: 1735.1498s\n",
      "Epoch: 3 cost time: 00h:04m:04.55s\n",
      "Epoch: 3 | Train Loss: 0.1279684 Vali Loss: 0.1357549 Test Loss: 0.1514821\n",
      "Validation loss decreased (0.151330 --> 0.135755).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 4 | loss: 0.1000565\n",
      "\tspeed: 1.3742s/iter; left time: 6289.7102s\n",
      "\titers: 200, epoch: 4 | loss: 0.1004288\n",
      "\tspeed: 0.3634s/iter; left time: 1626.7636s\n",
      "\titers: 300, epoch: 4 | loss: 0.0852600\n",
      "\tspeed: 0.3640s/iter; left time: 1593.0577s\n",
      "\titers: 400, epoch: 4 | loss: 0.0965838\n",
      "\tspeed: 0.3654s/iter; left time: 1562.9841s\n",
      "\titers: 500, epoch: 4 | loss: 0.0867419\n",
      "\tspeed: 0.3656s/iter; left time: 1526.9550s\n",
      "\titers: 600, epoch: 4 | loss: 0.0894511\n",
      "\tspeed: 0.3631s/iter; left time: 1480.2763s\n",
      "Epoch: 4 cost time: 00h:04m:04.05s\n",
      "Epoch: 4 | Train Loss: 0.0949176 Vali Loss: 0.0906533 Test Loss: 0.1000990\n",
      "Validation loss decreased (0.135755 --> 0.090653).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 5 | loss: 0.0775984\n",
      "\tspeed: 1.3894s/iter; left time: 5431.3129s\n",
      "\titers: 200, epoch: 5 | loss: 0.0775234\n",
      "\tspeed: 0.3660s/iter; left time: 1394.2447s\n",
      "\titers: 300, epoch: 5 | loss: 0.0766763\n",
      "\tspeed: 0.3652s/iter; left time: 1354.5339s\n",
      "\titers: 400, epoch: 5 | loss: 0.0798478\n",
      "\tspeed: 0.3643s/iter; left time: 1314.6703s\n",
      "\titers: 500, epoch: 5 | loss: 0.0745407\n",
      "\tspeed: 0.3657s/iter; left time: 1283.3125s\n",
      "\titers: 600, epoch: 5 | loss: 0.0878031\n",
      "\tspeed: 0.3645s/iter; left time: 1242.5267s\n",
      "Epoch: 5 cost time: 00h:04m:04.35s\n",
      "Epoch: 5 | Train Loss: 0.0829895 Vali Loss: 0.0820751 Test Loss: 0.0906768\n",
      "Validation loss decreased (0.090653 --> 0.082075).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 6 | loss: 0.0749179\n",
      "\tspeed: 1.3530s/iter; left time: 4384.9246s\n",
      "\titers: 200, epoch: 6 | loss: 0.0800712\n",
      "\tspeed: 0.3621s/iter; left time: 1137.4278s\n",
      "\titers: 300, epoch: 6 | loss: 0.0762458\n",
      "\tspeed: 0.3639s/iter; left time: 1106.6732s\n",
      "\titers: 400, epoch: 6 | loss: 0.0746640\n",
      "\tspeed: 0.3661s/iter; left time: 1076.6475s\n",
      "\titers: 500, epoch: 6 | loss: 0.0835361\n",
      "\tspeed: 0.3628s/iter; left time: 1030.7432s\n",
      "\titers: 600, epoch: 6 | loss: 0.0796762\n",
      "\tspeed: 0.3630s/iter; left time: 995.0703s\n",
      "Epoch: 6 cost time: 00h:04m:03.30s\n",
      "Epoch: 6 | Train Loss: 0.0784537 Vali Loss: 0.0797454 Test Loss: 0.0880287\n",
      "Validation loss decreased (0.082075 --> 0.079745).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 7 | loss: 0.0764888\n",
      "\tspeed: 1.3520s/iter; left time: 3478.6806s\n",
      "\titers: 200, epoch: 7 | loss: 0.0680704\n",
      "\tspeed: 0.3669s/iter; left time: 907.4039s\n",
      "\titers: 300, epoch: 7 | loss: 0.0679631\n",
      "\tspeed: 0.3679s/iter; left time: 873.0680s\n",
      "\titers: 400, epoch: 7 | loss: 0.0873675\n",
      "\tspeed: 0.3623s/iter; left time: 823.4612s\n",
      "\titers: 500, epoch: 7 | loss: 0.0723816\n",
      "\tspeed: 0.3638s/iter; left time: 790.5373s\n",
      "\titers: 600, epoch: 7 | loss: 0.0755982\n",
      "\tspeed: 0.3670s/iter; left time: 760.7957s\n",
      "Epoch: 7 cost time: 00h:04m:04.96s\n",
      "Epoch: 7 | Train Loss: 0.0768068 Vali Loss: 0.0789745 Test Loss: 0.0872522\n",
      "Validation loss decreased (0.079745 --> 0.078975).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 8 | loss: 0.0788021\n",
      "\tspeed: 1.3648s/iter; left time: 2600.0217s\n",
      "\titers: 200, epoch: 8 | loss: 0.0764149\n",
      "\tspeed: 0.3682s/iter; left time: 664.5922s\n",
      "\titers: 300, epoch: 8 | loss: 0.0747475\n",
      "\tspeed: 0.3665s/iter; left time: 624.9438s\n",
      "\titers: 400, epoch: 8 | loss: 0.0677170\n",
      "\tspeed: 0.3621s/iter; left time: 581.2417s\n",
      "\titers: 500, epoch: 8 | loss: 0.0684882\n",
      "\tspeed: 0.3668s/iter; left time: 552.0958s\n",
      "\titers: 600, epoch: 8 | loss: 0.0788483\n",
      "\tspeed: 0.3686s/iter; left time: 517.8328s\n",
      "Epoch: 8 cost time: 00h:04m:05.08s\n",
      "Epoch: 8 | Train Loss: 0.0758291 Vali Loss: 0.0780529 Test Loss: 0.0864035\n",
      "Validation loss decreased (0.078975 --> 0.078053).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 9 | loss: 0.0702730\n",
      "\tspeed: 1.3588s/iter; left time: 1680.8753s\n",
      "\titers: 200, epoch: 9 | loss: 0.0715103\n",
      "\tspeed: 0.3640s/iter; left time: 413.8713s\n",
      "\titers: 300, epoch: 9 | loss: 0.0795986\n",
      "\tspeed: 0.3663s/iter; left time: 379.8304s\n",
      "\titers: 400, epoch: 9 | loss: 0.0797710\n",
      "\tspeed: 0.3645s/iter; left time: 341.5181s\n",
      "\titers: 500, epoch: 9 | loss: 0.0727597\n",
      "\tspeed: 0.3659s/iter; left time: 306.2750s\n",
      "\titers: 600, epoch: 9 | loss: 0.0701457\n",
      "\tspeed: 0.3654s/iter; left time: 269.3013s\n",
      "Epoch: 9 cost time: 00h:04m:04.46s\n",
      "Epoch: 9 | Train Loss: 0.0751210 Vali Loss: 0.0775468 Test Loss: 0.0860222\n",
      "Validation loss decreased (0.078053 --> 0.077547).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "\titers: 100, epoch: 10 | loss: 0.0684217\n",
      "\tspeed: 1.3618s/iter; left time: 774.8615s\n",
      "\titers: 200, epoch: 10 | loss: 0.0674715\n",
      "\tspeed: 0.3635s/iter; left time: 170.4724s\n",
      "\titers: 300, epoch: 10 | loss: 0.0764492\n",
      "\tspeed: 0.3642s/iter; left time: 134.3911s\n",
      "\titers: 400, epoch: 10 | loss: 0.0795911\n",
      "\tspeed: 0.3636s/iter; left time: 97.7972s\n",
      "\titers: 500, epoch: 10 | loss: 0.0715406\n",
      "\tspeed: 0.3653s/iter; left time: 61.7319s\n",
      "\titers: 600, epoch: 10 | loss: 0.0724742\n",
      "\tspeed: 0.3654s/iter; left time: 25.2154s\n",
      "Epoch: 10 cost time: 00h:04m:04.01s\n",
      "Epoch: 10 | Train Loss: 0.0745486 Vali Loss: 0.0772412 Test Loss: 0.0858930\n",
      "Validation loss decreased (0.077547 --> 0.077241).  Saving model ...\n",
      "lr = 0.0000040000\n",
      "loading model...\n",
      "Scaled mse:0.019390663132071495, rmse:0.13925036787986755, mae:0.08589297533035278, rse:0.5387005805969238\n",
      "success delete checkpoints\n",
      "Total time: 154.05094581445059 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.0001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrease complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=32\n",
    "d_ff=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-31 07:15:30,267] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-31 07:15:31,478] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-31 07:15:31,479] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-31 07:15:31,479] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-10-31 07:15:31,570] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-10-31 07:15:31,570] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-31 07:15:32,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-31 07:15:32,227] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-31 07:15:32,227] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-31 07:15:32,229] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-31 07:15:32,229] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-31 07:15:32,229] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-31 07:15:32,229] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-31 07:15:32,229] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-31 07:15:32,229] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-31 07:15:32,229] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-31 07:15:32,540] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-31 07:15:32,541] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-10-31 07:15:32,541] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.73 GB, percent = 9.9%\n",
      "[2024-10-31 07:15:32,657] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-31 07:15:32,658] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.75 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 07:15:32,659] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.73 GB, percent = 9.9%\n",
      "[2024-10-31 07:15:32,659] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-31 07:15:32,771] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-31 07:15:32,772] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-10-31 07:15:32,772] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.73 GB, percent = 9.9%\n",
      "[2024-10-31 07:15:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-31 07:15:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-31 07:15:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-31 07:15:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-31 07:15:32,773] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f821c383a90>\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-31 07:15:32,774] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-31 07:15:32,775] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-31 07:15:32,776] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 128, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1300964\n",
      "\tspeed: 0.4454s/iter; left time: 2930.9608s\n",
      "\titers: 200, epoch: 1 | loss: 0.1002507\n",
      "\tspeed: 0.4015s/iter; left time: 2602.1762s\n",
      "\titers: 300, epoch: 1 | loss: 0.0847750\n",
      "\tspeed: 0.4028s/iter; left time: 2570.3132s\n",
      "\titers: 400, epoch: 1 | loss: 0.0704524\n",
      "\tspeed: 0.4016s/iter; left time: 2522.1692s\n",
      "\titers: 500, epoch: 1 | loss: 0.0785711\n",
      "\tspeed: 0.4039s/iter; left time: 2496.3696s\n",
      "\titers: 600, epoch: 1 | loss: 0.0821474\n",
      "\tspeed: 0.4032s/iter; left time: 2451.6930s\n",
      "Epoch: 1 cost time: 00h:04m:30.14s\n",
      "Epoch: 1 | Train Loss: 0.0936423 Vali Loss: 0.0776504 Test Loss: 0.0858035\n",
      "Validation loss decreased (inf --> 0.077650).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0706200\n",
      "\tspeed: 1.4987s/iter; left time: 8861.6854s\n",
      "\titers: 200, epoch: 2 | loss: 0.0717461\n",
      "\tspeed: 0.3652s/iter; left time: 2122.7770s\n",
      "\titers: 300, epoch: 2 | loss: 0.0698576\n",
      "\tspeed: 0.3663s/iter; left time: 2092.3993s\n",
      "\titers: 400, epoch: 2 | loss: 0.0752010\n",
      "\tspeed: 0.3663s/iter; left time: 2055.8612s\n",
      "\titers: 500, epoch: 2 | loss: 0.0735064\n",
      "\tspeed: 0.3673s/iter; left time: 2025.0348s\n",
      "\titers: 600, epoch: 2 | loss: 0.0743915\n",
      "\tspeed: 0.3654s/iter; left time: 1978.1472s\n",
      "Epoch: 2 cost time: 00h:04m:04.42s\n",
      "Epoch: 2 | Train Loss: 0.0725456 Vali Loss: 0.0749177 Test Loss: 0.0835638\n",
      "Validation loss decreased (0.077650 --> 0.074918).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0770939\n",
      "\tspeed: 1.3509s/iter; left time: 7085.5689s\n",
      "\titers: 200, epoch: 3 | loss: 0.0638322\n",
      "\tspeed: 0.3640s/iter; left time: 1872.9350s\n",
      "\titers: 300, epoch: 3 | loss: 0.0733752\n",
      "\tspeed: 0.3673s/iter; left time: 1852.9854s\n",
      "\titers: 400, epoch: 3 | loss: 0.0750042\n",
      "\tspeed: 0.3679s/iter; left time: 1819.3864s\n",
      "\titers: 500, epoch: 3 | loss: 0.0734662\n",
      "\tspeed: 0.3660s/iter; left time: 1773.2257s\n",
      "\titers: 600, epoch: 3 | loss: 0.0674660\n",
      "\tspeed: 0.3656s/iter; left time: 1734.5691s\n",
      "Epoch: 3 cost time: 00h:04m:05.12s\n",
      "Epoch: 3 | Train Loss: 0.0701563 Vali Loss: 0.0737785 Test Loss: 0.0829429\n",
      "Validation loss decreased (0.074918 --> 0.073778).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0735068\n",
      "\tspeed: 1.3440s/iter; left time: 6151.4946s\n",
      "\titers: 200, epoch: 4 | loss: 0.0664808\n",
      "\tspeed: 0.3631s/iter; left time: 1625.7360s\n",
      "\titers: 300, epoch: 4 | loss: 0.0709979\n",
      "\tspeed: 0.3673s/iter; left time: 1607.5447s\n",
      "\titers: 400, epoch: 4 | loss: 0.0719831\n",
      "\tspeed: 0.3658s/iter; left time: 1564.6890s\n",
      "\titers: 500, epoch: 4 | loss: 0.0677283\n",
      "\tspeed: 0.3655s/iter; left time: 1526.7765s\n",
      "\titers: 600, epoch: 4 | loss: 0.0657431\n",
      "\tspeed: 0.3694s/iter; left time: 1506.0388s\n",
      "Epoch: 4 cost time: 00h:04m:04.94s\n",
      "Epoch: 4 | Train Loss: 0.0685529 Vali Loss: 0.0734445 Test Loss: 0.0828946\n",
      "Validation loss decreased (0.073778 --> 0.073444).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0672765\n",
      "\tspeed: 1.3625s/iter; left time: 5326.0893s\n",
      "\titers: 200, epoch: 5 | loss: 0.0651767\n",
      "\tspeed: 0.3642s/iter; left time: 1387.3946s\n",
      "\titers: 300, epoch: 5 | loss: 0.0692032\n",
      "\tspeed: 0.3682s/iter; left time: 1365.5748s\n",
      "\titers: 400, epoch: 5 | loss: 0.0648893\n",
      "\tspeed: 0.3654s/iter; left time: 1318.6716s\n",
      "\titers: 500, epoch: 5 | loss: 0.0617945\n",
      "\tspeed: 0.3674s/iter; left time: 1289.0729s\n",
      "\titers: 600, epoch: 5 | loss: 0.0753025\n",
      "\tspeed: 0.3647s/iter; left time: 1243.1786s\n",
      "Epoch: 5 cost time: 00h:04m:05.15s\n",
      "Epoch: 5 | Train Loss: 0.0675518 Vali Loss: 0.0723173 Test Loss: 0.0815948\n",
      "Validation loss decreased (0.073444 --> 0.072317).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0687790\n",
      "\tspeed: 1.3530s/iter; left time: 4384.9416s\n",
      "\titers: 200, epoch: 6 | loss: 0.0682647\n",
      "\tspeed: 0.3647s/iter; left time: 1145.5875s\n",
      "\titers: 300, epoch: 6 | loss: 0.0654113\n",
      "\tspeed: 0.3689s/iter; left time: 1121.7968s\n",
      "\titers: 400, epoch: 6 | loss: 0.0627470\n",
      "\tspeed: 0.3657s/iter; left time: 1075.5405s\n",
      "\titers: 500, epoch: 6 | loss: 0.0625488\n",
      "\tspeed: 0.3648s/iter; left time: 1036.3089s\n",
      "\titers: 600, epoch: 6 | loss: 0.0634783\n",
      "\tspeed: 0.3672s/iter; left time: 1006.4710s\n",
      "Epoch: 6 cost time: 00h:04m:05.02s\n",
      "Epoch: 6 | Train Loss: 0.0663678 Vali Loss: 0.0725291 Test Loss: 0.0830315\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0645463\n",
      "\tspeed: 1.3346s/iter; left time: 3434.0297s\n",
      "\titers: 200, epoch: 7 | loss: 0.0579557\n",
      "\tspeed: 0.3670s/iter; left time: 907.4911s\n",
      "\titers: 300, epoch: 7 | loss: 0.0592161\n",
      "\tspeed: 0.3673s/iter; left time: 871.5563s\n",
      "\titers: 400, epoch: 7 | loss: 0.0680536\n",
      "\tspeed: 0.3685s/iter; left time: 837.5587s\n",
      "\titers: 500, epoch: 7 | loss: 0.0651175\n",
      "\tspeed: 0.3675s/iter; left time: 798.5756s\n",
      "\titers: 600, epoch: 7 | loss: 0.0654948\n",
      "\tspeed: 0.3637s/iter; left time: 753.9428s\n",
      "Epoch: 7 cost time: 00h:04m:05.25s\n",
      "Epoch: 7 | Train Loss: 0.0654534 Vali Loss: 0.0720752 Test Loss: 0.0806610\n",
      "Validation loss decreased (0.072317 --> 0.072075).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0708415\n",
      "\tspeed: 1.3796s/iter; left time: 2628.2157s\n",
      "\titers: 200, epoch: 8 | loss: 0.0548181\n",
      "\tspeed: 0.3641s/iter; left time: 657.1685s\n",
      "\titers: 300, epoch: 8 | loss: 0.0625901\n",
      "\tspeed: 0.3636s/iter; left time: 620.0079s\n",
      "\titers: 400, epoch: 8 | loss: 0.0593024\n",
      "\tspeed: 0.3655s/iter; left time: 586.6629s\n",
      "\titers: 500, epoch: 8 | loss: 0.0597407\n",
      "\tspeed: 0.3644s/iter; left time: 548.4733s\n",
      "\titers: 600, epoch: 8 | loss: 0.0613044\n",
      "\tspeed: 0.3670s/iter; left time: 515.5812s\n",
      "Epoch: 8 cost time: 00h:04m:04.91s\n",
      "Epoch: 8 | Train Loss: 0.0642098 Vali Loss: 0.0731629 Test Loss: 0.0809757\n",
      "EarlyStopping counter: 1 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0592178\n",
      "\tspeed: 1.3382s/iter; left time: 1655.3530s\n",
      "\titers: 200, epoch: 9 | loss: 0.0629231\n",
      "\tspeed: 0.3717s/iter; left time: 422.6448s\n",
      "\titers: 300, epoch: 9 | loss: 0.0631223\n",
      "\tspeed: 0.3693s/iter; left time: 382.9974s\n",
      "\titers: 400, epoch: 9 | loss: 0.0660379\n",
      "\tspeed: 0.3725s/iter; left time: 349.0761s\n",
      "\titers: 500, epoch: 9 | loss: 0.0608254\n",
      "\tspeed: 0.3680s/iter; left time: 307.9825s\n",
      "\titers: 600, epoch: 9 | loss: 0.0601549\n",
      "\tspeed: 0.3687s/iter; left time: 271.7186s\n",
      "Epoch: 9 cost time: 00h:04m:07.55s\n",
      "Epoch: 9 | Train Loss: 0.0630593 Vali Loss: 0.0747532 Test Loss: 0.0816336\n",
      "EarlyStopping counter: 2 out of 3\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0629280\n",
      "\tspeed: 1.3409s/iter; left time: 762.9639s\n",
      "\titers: 200, epoch: 10 | loss: 0.0546127\n",
      "\tspeed: 0.3686s/iter; left time: 172.8767s\n",
      "\titers: 300, epoch: 10 | loss: 0.0579761\n",
      "\tspeed: 0.3696s/iter; left time: 136.3914s\n",
      "\titers: 400, epoch: 10 | loss: 0.0689674\n",
      "\tspeed: 0.3690s/iter; left time: 99.2727s\n",
      "\titers: 500, epoch: 10 | loss: 0.0632121\n",
      "\tspeed: 0.3659s/iter; left time: 61.8403s\n",
      "\titers: 600, epoch: 10 | loss: 0.0626613\n",
      "\tspeed: 0.3688s/iter; left time: 25.4498s\n",
      "Epoch: 10 cost time: 00h:04m:06.45s\n",
      "Epoch: 10 | Train Loss: 0.0619098 Vali Loss: 0.0742881 Test Loss: 0.0811442\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018481871113181114, rmse:0.135948047041893, mae:0.08066105097532272, rse:0.52592533826828\n",
      "success delete checkpoints\n",
      "Total time: 208.78635589281717 min.\n"
     ]
    }
   ],
   "source": [
    "#countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "countries = ['DE']\n",
    "#pred_lens = [\"24\", \"96\", \"168\"]\n",
    "pred_lens = [\"96\"]\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id 1 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --patience 3 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
