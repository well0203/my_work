{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `python -m accelerate.commands.launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py\", line 107, in <module>\n",
      "    accelerator = Accelerator(device='cuda:1', kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Accelerator.__init__() got an unexpected keyword argument 'device'\n",
      "[2024-05-03 15:24:25,672] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 39081) of binary: /usr/local/anaconda3-2023.03/envs/python311/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/.local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-03_15:24:25\n",
      "  host      : gruenau1.informatik.hu-berlin.de\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 39081)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "train_epochs=10\n",
    "learning_rate=0.01\n",
    "llama_layers=6\n",
    "\n",
    "#master_port=00097\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --main_process_port \"00097\"\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-ETTh1'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --dynamo_backend \"no\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99645"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59647 + 19999 + 19999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428.5714285714287"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20000/100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `python -m accelerate.commands.launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-04-29 22:03:12,958] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 22:03:14,012] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-29 22:03:14,012] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 59647\n",
      "val 19999\n",
      "test 19999\n",
      "[2024-04-29 22:03:15,709] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-29 22:03:16,232] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-04-29 22:03:16,233] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-04-29 22:03:16,233] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-04-29 22:03:16,234] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-04-29 22:03:16,234] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-04-29 22:03:16,235] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-04-29 22:03:16,235] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-04-29 22:03:16,235] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-04-29 22:03:16,235] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-04-29 22:03:16,235] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-04-29 22:03:16,530] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-04-29 22:03:16,530] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB \n",
      "[2024-04-29 22:03:16,530] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.63 GB, percent = 6.0%\n",
      "[2024-04-29 22:03:16,661] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-04-29 22:03:16,662] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-04-29 22:03:16,662] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.61 GB, percent = 6.0%\n",
      "[2024-04-29 22:03:16,662] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-04-29 22:03:16,782] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-04-29 22:03:16,782] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-04-29 22:03:16,783] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.61 GB, percent = 6.0%\n",
      "[2024-04-29 22:03:16,783] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-04-29 22:03:16,783] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-04-29 22:03:16,783] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-04-29 22:03:16,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]\n",
      "[2024-04-29 22:03:16,783] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f36f03d6cd0>\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-04-29 22:03:16,784] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-04-29 22:03:16,785] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:13,  7.96it/s]\titers: 100, epoch: 1 | loss: 0.0470595\n",
      "\tspeed: 0.1461s/iter; left time: 3615.4202s\n",
      "199it [00:26,  7.96it/s]\titers: 200, epoch: 1 | loss: 1.3540492\n",
      "\tspeed: 0.1256s/iter; left time: 3096.6190s\n",
      "299it [00:38,  8.00it/s]\titers: 300, epoch: 1 | loss: 1.3124354\n",
      "\tspeed: 0.1252s/iter; left time: 3074.2403s\n",
      "399it [00:51,  7.97it/s]\titers: 400, epoch: 1 | loss: 0.1549812\n",
      "\tspeed: 0.1255s/iter; left time: 3068.8492s\n",
      "499it [01:03,  7.97it/s]\titers: 500, epoch: 1 | loss: 0.1256166\n",
      "\tspeed: 0.1255s/iter; left time: 3057.1213s\n",
      "599it [01:16,  7.98it/s]\titers: 600, epoch: 1 | loss: 0.1378662\n",
      "\tspeed: 0.1254s/iter; left time: 3041.7372s\n",
      "699it [01:28,  7.97it/s]\titers: 700, epoch: 1 | loss: 0.1962887\n",
      "\tspeed: 0.1255s/iter; left time: 3031.2328s\n",
      "799it [01:41,  7.98it/s]\titers: 800, epoch: 1 | loss: 0.0460990\n",
      "\tspeed: 0.1256s/iter; left time: 3020.2284s\n",
      "899it [01:53,  7.97it/s]\titers: 900, epoch: 1 | loss: 0.0847627\n",
      "\tspeed: 0.1256s/iter; left time: 3007.0780s\n",
      "999it [02:06,  7.99it/s]\titers: 1000, epoch: 1 | loss: 1.0234829\n",
      "\tspeed: 0.1251s/iter; left time: 2982.6435s\n",
      "1099it [02:18,  7.97it/s]\titers: 1100, epoch: 1 | loss: 0.1046316\n",
      "\tspeed: 0.1253s/iter; left time: 2974.8464s\n",
      "1199it [02:31,  7.97it/s]\titers: 1200, epoch: 1 | loss: 0.1298734\n",
      "\tspeed: 0.1255s/iter; left time: 2968.8451s\n",
      "1299it [02:43,  7.99it/s]\titers: 1300, epoch: 1 | loss: 0.5722924\n",
      "\tspeed: 0.1254s/iter; left time: 2954.0388s\n",
      "1399it [02:56,  7.76it/s]\titers: 1400, epoch: 1 | loss: 0.1640363\n",
      "\tspeed: 0.1260s/iter; left time: 2954.2331s\n",
      "1499it [03:09,  7.94it/s]\titers: 1500, epoch: 1 | loss: 0.5427644\n",
      "\tspeed: 0.1261s/iter; left time: 2945.4011s\n",
      "1599it [03:21,  7.95it/s]\titers: 1600, epoch: 1 | loss: 0.9603280\n",
      "\tspeed: 0.1258s/iter; left time: 2925.0245s\n",
      "1699it [03:34,  7.97it/s]\titers: 1700, epoch: 1 | loss: 0.6843318\n",
      "\tspeed: 0.1256s/iter; left time: 2908.5070s\n",
      "1799it [03:46,  7.96it/s]\titers: 1800, epoch: 1 | loss: 0.1612039\n",
      "\tspeed: 0.1256s/iter; left time: 2894.2642s\n",
      "1899it [03:59,  7.97it/s]\titers: 1900, epoch: 1 | loss: 0.0938611\n",
      "\tspeed: 0.1256s/iter; left time: 2881.6549s\n",
      "1999it [04:12,  7.97it/s]\titers: 2000, epoch: 1 | loss: 0.3307541\n",
      "\tspeed: 0.1256s/iter; left time: 2869.9741s\n",
      "2099it [04:24,  7.94it/s]\titers: 2100, epoch: 1 | loss: 0.1171750\n",
      "\tspeed: 0.1258s/iter; left time: 2861.1387s\n",
      "2199it [04:37,  7.97it/s]\titers: 2200, epoch: 1 | loss: 0.0460432\n",
      "\tspeed: 0.1255s/iter; left time: 2842.6793s\n",
      "2299it [04:49,  7.91it/s]\titers: 2300, epoch: 1 | loss: 0.1258454\n",
      "\tspeed: 0.1261s/iter; left time: 2844.1489s\n",
      "2399it [05:02,  7.91it/s]\titers: 2400, epoch: 1 | loss: 0.0879738\n",
      "\tspeed: 0.1264s/iter; left time: 2838.5080s\n",
      "2485it [05:13,  7.93it/s]\n",
      "Epoch: 1 cost time: 313.3299458026886\n",
      "833it [00:54, 15.39it/s]\n",
      "833it [00:53, 15.48it/s]\n",
      "Epoch: 1 | Train Loss: 0.3608817 Vali Loss: 0.5400495 Test Loss: 0.4936774 MAE Loss: 0.4573857\n",
      "lr = 0.0004000000\n",
      "Updating learning rate to 0.0003999999999999993\n",
      "99it [00:12,  8.26it/s]\titers: 100, epoch: 2 | loss: 0.0551159\n",
      "\tspeed: 1.3271s/iter; left time: 29549.6956s\n",
      "199it [00:24,  8.30it/s]\titers: 200, epoch: 2 | loss: 1.1593033\n",
      "\tspeed: 0.1208s/iter; left time: 2677.2725s\n",
      "299it [00:36,  8.31it/s]\titers: 300, epoch: 2 | loss: 0.8348759\n",
      "\tspeed: 0.1205s/iter; left time: 2658.8377s\n",
      "399it [00:48,  8.27it/s]\titers: 400, epoch: 2 | loss: 0.1319086\n",
      "\tspeed: 0.1208s/iter; left time: 2652.9695s\n",
      "499it [01:00,  8.20it/s]\titers: 500, epoch: 2 | loss: 0.1177018\n",
      "\tspeed: 0.1214s/iter; left time: 2655.0287s\n",
      "599it [01:12,  8.23it/s]\titers: 600, epoch: 2 | loss: 0.1750373\n",
      "\tspeed: 0.1218s/iter; left time: 2650.4710s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 2 | loss: 0.2026412\n",
      "\tspeed: 0.1218s/iter; left time: 2638.6186s\n",
      "799it [01:37,  8.22it/s]\titers: 800, epoch: 2 | loss: 0.0548178\n",
      "\tspeed: 0.1218s/iter; left time: 2626.8945s\n",
      "899it [01:49,  8.21it/s]\titers: 900, epoch: 2 | loss: 0.0716290\n",
      "\tspeed: 0.1220s/iter; left time: 2618.0346s\n",
      "999it [02:01,  8.23it/s]\titers: 1000, epoch: 2 | loss: 0.7185447\n",
      "\tspeed: 0.1217s/iter; left time: 2599.6089s\n",
      "1099it [02:13,  8.19it/s]\titers: 1100, epoch: 2 | loss: 0.1187836\n",
      "\tspeed: 0.1230s/iter; left time: 2614.8224s\n",
      "1199it [02:26,  8.20it/s]\titers: 1200, epoch: 2 | loss: 0.1383893\n",
      "\tspeed: 0.1220s/iter; left time: 2582.6437s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 2 | loss: 0.5992399\n",
      "\tspeed: 0.1218s/iter; left time: 2565.4793s\n",
      "1399it [02:50,  8.22it/s]\titers: 1400, epoch: 2 | loss: 0.1566758\n",
      "\tspeed: 0.1220s/iter; left time: 2557.3012s\n",
      "1499it [03:02,  8.17it/s]\titers: 1500, epoch: 2 | loss: 0.5027734\n",
      "\tspeed: 0.1221s/iter; left time: 2547.0487s\n",
      "1599it [03:14,  8.19it/s]\titers: 1600, epoch: 2 | loss: 1.1169223\n",
      "\tspeed: 0.1223s/iter; left time: 2539.4894s\n",
      "1699it [03:27,  8.20it/s]\titers: 1700, epoch: 2 | loss: 0.7417162\n",
      "\tspeed: 0.1224s/iter; left time: 2529.8394s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 2 | loss: 0.1505206\n",
      "\tspeed: 0.1220s/iter; left time: 2510.0573s\n",
      "1899it [03:51,  8.13it/s]\titers: 1900, epoch: 2 | loss: 0.1199519\n",
      "\tspeed: 0.1225s/iter; left time: 2506.9607s\n",
      "1999it [04:03,  8.18it/s]\titers: 2000, epoch: 2 | loss: 0.3742316\n",
      "\tspeed: 0.1225s/iter; left time: 2495.3896s\n",
      "2099it [04:16,  8.13it/s]\titers: 2100, epoch: 2 | loss: 0.1040087\n",
      "\tspeed: 0.1227s/iter; left time: 2486.0377s\n",
      "2199it [04:28,  8.17it/s]\titers: 2200, epoch: 2 | loss: 0.0473233\n",
      "\tspeed: 0.1227s/iter; left time: 2473.5564s\n",
      "2299it [04:40,  8.12it/s]\titers: 2300, epoch: 2 | loss: 0.0557982\n",
      "\tspeed: 0.1227s/iter; left time: 2462.4534s\n",
      "2399it [04:52,  8.18it/s]\titers: 2400, epoch: 2 | loss: 0.0900909\n",
      "\tspeed: 0.1223s/iter; left time: 2442.1621s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 2 cost time: 303.4729018211365\n",
      "833it [00:50, 16.37it/s]\n",
      "833it [00:50, 16.41it/s]\n",
      "Epoch: 2 | Train Loss: 0.3331488 Vali Loss: 0.5067128 Test Loss: 0.4618443 MAE Loss: 0.4468220\n",
      "Updating learning rate to 0.00019999999999999966\n",
      "99it [00:12,  8.20it/s]\titers: 100, epoch: 3 | loss: 0.0498120\n",
      "\tspeed: 1.3050s/iter; left time: 25814.0633s\n",
      "199it [00:24,  8.19it/s]\titers: 200, epoch: 3 | loss: 1.1408185\n",
      "\tspeed: 0.1223s/iter; left time: 2406.7851s\n",
      "299it [00:36,  8.23it/s]\titers: 300, epoch: 3 | loss: 1.0447856\n",
      "\tspeed: 0.1216s/iter; left time: 2381.6623s\n",
      "399it [00:49,  8.21it/s]\titers: 400, epoch: 3 | loss: 0.1258406\n",
      "\tspeed: 0.1217s/iter; left time: 2370.1850s\n",
      "499it [01:01,  8.16it/s]\titers: 500, epoch: 3 | loss: 0.1085179\n",
      "\tspeed: 0.1223s/iter; left time: 2371.0098s\n",
      "599it [01:13,  8.24it/s]\titers: 600, epoch: 3 | loss: 0.1653356\n",
      "\tspeed: 0.1223s/iter; left time: 2357.4448s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 3 | loss: 0.1944730\n",
      "\tspeed: 0.1218s/iter; left time: 2336.7506s\n",
      "799it [01:37,  8.23it/s]\titers: 800, epoch: 3 | loss: 0.0479954\n",
      "\tspeed: 0.1219s/iter; left time: 2325.5832s\n",
      "899it [01:50,  8.23it/s]\titers: 900, epoch: 3 | loss: 0.0759382\n",
      "\tspeed: 0.1217s/iter; left time: 2309.4276s\n",
      "999it [02:02,  8.22it/s]\titers: 1000, epoch: 3 | loss: 0.6669039\n",
      "\tspeed: 0.1216s/iter; left time: 2295.7751s\n",
      "1099it [02:14,  8.17it/s]\titers: 1100, epoch: 3 | loss: 0.1145785\n",
      "\tspeed: 0.1218s/iter; left time: 2287.2011s\n",
      "1199it [02:26,  8.20it/s]\titers: 1200, epoch: 3 | loss: 0.1272832\n",
      "\tspeed: 0.1220s/iter; left time: 2278.7618s\n",
      "1299it [02:38,  8.21it/s]\titers: 1300, epoch: 3 | loss: 0.5564218\n",
      "\tspeed: 0.1218s/iter; left time: 2263.2088s\n",
      "1399it [02:50,  8.16it/s]\titers: 1400, epoch: 3 | loss: 0.1508305\n",
      "\tspeed: 0.1223s/iter; left time: 2260.7654s\n",
      "1499it [03:03,  8.19it/s]\titers: 1500, epoch: 3 | loss: 0.5231425\n",
      "\tspeed: 0.1220s/iter; left time: 2242.2492s\n",
      "1599it [03:15,  8.22it/s]\titers: 1600, epoch: 3 | loss: 0.9066538\n",
      "\tspeed: 0.1219s/iter; left time: 2227.6054s\n",
      "1699it [03:27,  8.20it/s]\titers: 1700, epoch: 3 | loss: 0.7492739\n",
      "\tspeed: 0.1218s/iter; left time: 2214.3232s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 3 | loss: 0.1557841\n",
      "\tspeed: 0.1219s/iter; left time: 2203.9336s\n",
      "1899it [03:51,  8.20it/s]\titers: 1900, epoch: 3 | loss: 0.0978128\n",
      "\tspeed: 0.1218s/iter; left time: 2189.8844s\n",
      "1999it [04:04,  8.21it/s]\titers: 2000, epoch: 3 | loss: 0.3573691\n",
      "\tspeed: 0.1218s/iter; left time: 2178.3794s\n",
      "2099it [04:16,  8.22it/s]\titers: 2100, epoch: 3 | loss: 0.1095517\n",
      "\tspeed: 0.1219s/iter; left time: 2167.4522s\n",
      "2199it [04:28,  8.22it/s]\titers: 2200, epoch: 3 | loss: 0.0477854\n",
      "\tspeed: 0.1221s/iter; left time: 2158.9981s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 3 | loss: 0.0521798\n",
      "\tspeed: 0.1220s/iter; left time: 2144.0189s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 3 | loss: 0.0943665\n",
      "\tspeed: 0.1221s/iter; left time: 2134.4515s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 3 cost time: 303.46322226524353\n",
      "833it [00:50, 16.34it/s]\n",
      "833it [00:50, 16.36it/s]\n",
      "Epoch: 3 | Train Loss: 0.3268508 Vali Loss: 0.4916570 Test Loss: 0.4259884 MAE Loss: 0.4296986\n",
      "Updating learning rate to 9.999999999999983e-05\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 4 | loss: 0.0592945\n",
      "\tspeed: 1.2601s/iter; left time: 21794.5740s\n",
      "199it [00:24,  8.23it/s]\titers: 200, epoch: 4 | loss: 1.0796705\n",
      "\tspeed: 0.1218s/iter; left time: 2094.0758s\n",
      "299it [00:36,  8.20it/s]\titers: 300, epoch: 4 | loss: 0.8326894\n",
      "\tspeed: 0.1216s/iter; left time: 2078.9482s\n",
      "399it [00:48,  8.20it/s]\titers: 400, epoch: 4 | loss: 0.1213317\n",
      "\tspeed: 0.1217s/iter; left time: 2067.8082s\n",
      "499it [01:01,  8.21it/s]\titers: 500, epoch: 4 | loss: 0.1030726\n",
      "\tspeed: 0.1220s/iter; left time: 2060.5816s\n",
      "599it [01:13,  8.22it/s]\titers: 600, epoch: 4 | loss: 0.1408677\n",
      "\tspeed: 0.1220s/iter; left time: 2049.1836s\n",
      "699it [01:25,  8.22it/s]\titers: 700, epoch: 4 | loss: 0.1933443\n",
      "\tspeed: 0.1220s/iter; left time: 2036.4244s\n",
      "799it [01:37,  8.21it/s]\titers: 800, epoch: 4 | loss: 0.0380247\n",
      "\tspeed: 0.1218s/iter; left time: 2021.1734s\n",
      "899it [01:49,  8.22it/s]\titers: 900, epoch: 4 | loss: 0.0734085\n",
      "\tspeed: 0.1218s/iter; left time: 2008.3942s\n",
      "999it [02:02,  8.23it/s]\titers: 1000, epoch: 4 | loss: 0.6073506\n",
      "\tspeed: 0.1216s/iter; left time: 1992.9737s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 4 | loss: 0.1080227\n",
      "\tspeed: 0.1216s/iter; left time: 1981.6832s\n",
      "1199it [02:26,  8.21it/s]\titers: 1200, epoch: 4 | loss: 0.1248519\n",
      "\tspeed: 0.1219s/iter; left time: 1974.0373s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 4 | loss: 0.5179003\n",
      "\tspeed: 0.1218s/iter; left time: 1960.4568s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 4 | loss: 0.1471124\n",
      "\tspeed: 0.1221s/iter; left time: 1952.9526s\n",
      "1499it [03:02,  8.20it/s]\titers: 1500, epoch: 4 | loss: 0.5858544\n",
      "\tspeed: 0.1219s/iter; left time: 1938.2992s\n",
      "1599it [03:15,  8.20it/s]\titers: 1600, epoch: 4 | loss: 0.8666436\n",
      "\tspeed: 0.1220s/iter; left time: 1926.9083s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 4 | loss: 0.7305807\n",
      "\tspeed: 0.1220s/iter; left time: 1914.3671s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 4 | loss: 0.1490451\n",
      "\tspeed: 0.1219s/iter; left time: 1900.5243s\n",
      "1899it [03:51,  8.22it/s]\titers: 1900, epoch: 4 | loss: 0.0821003\n",
      "\tspeed: 0.1218s/iter; left time: 1888.0973s\n",
      "1999it [04:03,  8.21it/s]\titers: 2000, epoch: 4 | loss: 0.2946698\n",
      "\tspeed: 0.1219s/iter; left time: 1877.5383s\n",
      "2099it [04:16,  8.20it/s]\titers: 2100, epoch: 4 | loss: 0.1062525\n",
      "\tspeed: 0.1220s/iter; left time: 1866.2371s\n",
      "2199it [04:28,  8.22it/s]\titers: 2200, epoch: 4 | loss: 0.0428427\n",
      "\tspeed: 0.1219s/iter; left time: 1852.5333s\n",
      "2299it [04:40,  8.20it/s]\titers: 2300, epoch: 4 | loss: 0.0582951\n",
      "\tspeed: 0.1220s/iter; left time: 1840.9648s\n",
      "2399it [04:52,  8.20it/s]\titers: 2400, epoch: 4 | loss: 0.1116734\n",
      "\tspeed: 0.1219s/iter; left time: 1828.6209s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 4 cost time: 303.2673034667969\n",
      "833it [00:50, 16.38it/s]\n",
      "833it [00:51, 16.30it/s]\n",
      "Epoch: 4 | Train Loss: 0.3199663 Vali Loss: 0.4667201 Test Loss: 0.3857973 MAE Loss: 0.4072807\n",
      "Updating learning rate to 4.9999999999999914e-05\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 5 | loss: 0.0562752\n",
      "\tspeed: 1.2615s/iter; left time: 18684.2840s\n",
      "199it [00:24,  8.30it/s]\titers: 200, epoch: 5 | loss: 1.0283585\n",
      "\tspeed: 0.1214s/iter; left time: 1785.6220s\n",
      "299it [00:36,  8.31it/s]\titers: 300, epoch: 5 | loss: 0.8822472\n",
      "\tspeed: 0.1205s/iter; left time: 1760.8029s\n",
      "399it [00:48,  8.22it/s]\titers: 400, epoch: 5 | loss: 0.1255706\n",
      "\tspeed: 0.1206s/iter; left time: 1749.6344s\n",
      "499it [01:00,  8.18it/s]\titers: 500, epoch: 5 | loss: 0.1052672\n",
      "\tspeed: 0.1219s/iter; left time: 1756.0591s\n",
      "599it [01:12,  8.22it/s]\titers: 600, epoch: 5 | loss: 0.1447501\n",
      "\tspeed: 0.1218s/iter; left time: 1742.8905s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 5 | loss: 0.2029870\n",
      "\tspeed: 0.1218s/iter; left time: 1731.1232s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 5 | loss: 0.0410412\n",
      "\tspeed: 0.1219s/iter; left time: 1720.4107s\n",
      "899it [01:49,  8.23it/s]\titers: 900, epoch: 5 | loss: 0.0718827\n",
      "\tspeed: 0.1217s/iter; left time: 1705.3693s\n",
      "999it [02:01,  8.24it/s]\titers: 1000, epoch: 5 | loss: 0.6488792\n",
      "\tspeed: 0.1214s/iter; left time: 1689.2139s\n",
      "1099it [02:13,  8.22it/s]\titers: 1100, epoch: 5 | loss: 0.1062047\n",
      "\tspeed: 0.1217s/iter; left time: 1680.5559s\n",
      "1199it [02:26,  8.25it/s]\titers: 1200, epoch: 5 | loss: 0.1238012\n",
      "\tspeed: 0.1217s/iter; left time: 1668.2475s\n",
      "1299it [02:38,  8.28it/s]\titers: 1300, epoch: 5 | loss: 0.5321589\n",
      "\tspeed: 0.1210s/iter; left time: 1646.7710s\n",
      "1399it [02:50,  8.27it/s]\titers: 1400, epoch: 5 | loss: 0.1463805\n",
      "\tspeed: 0.1210s/iter; left time: 1634.9536s\n",
      "1499it [03:02,  8.28it/s]\titers: 1500, epoch: 5 | loss: 0.5825456\n",
      "\tspeed: 0.1218s/iter; left time: 1632.9749s\n",
      "1599it [03:14,  8.21it/s]\titers: 1600, epoch: 5 | loss: 0.9407220\n",
      "\tspeed: 0.1218s/iter; left time: 1621.1571s\n",
      "1699it [03:26,  8.18it/s]\titers: 1700, epoch: 5 | loss: 0.7503606\n",
      "\tspeed: 0.1225s/iter; left time: 1617.8115s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 5 | loss: 0.1520168\n",
      "\tspeed: 0.1221s/iter; left time: 1600.8745s\n",
      "1899it [03:51,  8.20it/s]\titers: 1900, epoch: 5 | loss: 0.0784954\n",
      "\tspeed: 0.1219s/iter; left time: 1585.6330s\n",
      "1999it [04:03,  8.19it/s]\titers: 2000, epoch: 5 | loss: 0.2970804\n",
      "\tspeed: 0.1219s/iter; left time: 1574.1216s\n",
      "2099it [04:15,  8.20it/s]\titers: 2100, epoch: 5 | loss: 0.1083166\n",
      "\tspeed: 0.1221s/iter; left time: 1563.7521s\n",
      "2199it [04:27,  8.21it/s]\titers: 2200, epoch: 5 | loss: 0.0375155\n",
      "\tspeed: 0.1222s/iter; left time: 1553.3165s\n",
      "2299it [04:40,  8.20it/s]\titers: 2300, epoch: 5 | loss: 0.0595233\n",
      "\tspeed: 0.1220s/iter; left time: 1538.8942s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 5 | loss: 0.1129480\n",
      "\tspeed: 0.1219s/iter; left time: 1524.6242s\n",
      "2485it [05:02,  8.21it/s]\n",
      "Epoch: 5 cost time: 302.7781140804291\n",
      "833it [00:50, 16.36it/s]\n",
      "833it [00:50, 16.36it/s]\n",
      "Epoch: 5 | Train Loss: 0.3155578 Vali Loss: 0.4612058 Test Loss: 0.3722970 MAE Loss: 0.3979324\n",
      "Updating learning rate to 2.4999999999999957e-05\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 6 | loss: 0.0559296\n",
      "\tspeed: 1.2631s/iter; left time: 15569.0250s\n",
      "199it [00:24,  8.22it/s]\titers: 200, epoch: 6 | loss: 1.0710326\n",
      "\tspeed: 0.1218s/iter; left time: 1488.6936s\n",
      "299it [00:36,  8.24it/s]\titers: 300, epoch: 6 | loss: 0.8629166\n",
      "\tspeed: 0.1215s/iter; left time: 1473.8576s\n",
      "399it [00:48,  8.22it/s]\titers: 400, epoch: 6 | loss: 0.1212952\n",
      "\tspeed: 0.1216s/iter; left time: 1462.9345s\n",
      "499it [01:01,  8.21it/s]\titers: 500, epoch: 6 | loss: 0.0964042\n",
      "\tspeed: 0.1219s/iter; left time: 1454.1985s\n",
      "599it [01:13,  8.22it/s]\titers: 600, epoch: 6 | loss: 0.1554099\n",
      "\tspeed: 0.1219s/iter; left time: 1441.0704s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 6 | loss: 0.1941764\n",
      "\tspeed: 0.1220s/iter; left time: 1431.1498s\n",
      "799it [01:37,  8.01it/s]\titers: 800, epoch: 6 | loss: 0.0313048\n",
      "\tspeed: 0.1224s/iter; left time: 1422.5592s\n",
      "899it [01:49,  8.21it/s]\titers: 900, epoch: 6 | loss: 0.0763933\n",
      "\tspeed: 0.1222s/iter; left time: 1408.7738s\n",
      "999it [02:02,  8.24it/s]\titers: 1000, epoch: 6 | loss: 0.6709769\n",
      "\tspeed: 0.1216s/iter; left time: 1388.9650s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 6 | loss: 0.1052714\n",
      "\tspeed: 0.1219s/iter; left time: 1380.1803s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 6 | loss: 0.1366353\n",
      "\tspeed: 0.1220s/iter; left time: 1369.1897s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 6 | loss: 0.4955131\n",
      "\tspeed: 0.1219s/iter; left time: 1355.7294s\n",
      "1399it [02:50,  8.22it/s]\titers: 1400, epoch: 6 | loss: 0.1363389\n",
      "\tspeed: 0.1218s/iter; left time: 1342.9020s\n",
      "1499it [03:02,  8.20it/s]\titers: 1500, epoch: 6 | loss: 0.5471390\n",
      "\tspeed: 0.1218s/iter; left time: 1330.7025s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 6 | loss: 0.9987666\n",
      "\tspeed: 0.1218s/iter; left time: 1318.4834s\n",
      "1699it [03:27,  8.18it/s]\titers: 1700, epoch: 6 | loss: 0.7934478\n",
      "\tspeed: 0.1222s/iter; left time: 1310.8920s\n",
      "1799it [03:39,  8.15it/s]\titers: 1800, epoch: 6 | loss: 0.1463301\n",
      "\tspeed: 0.1221s/iter; left time: 1297.6561s\n",
      "1899it [03:51,  8.20it/s]\titers: 1900, epoch: 6 | loss: 0.0875971\n",
      "\tspeed: 0.1219s/iter; left time: 1282.9997s\n",
      "1999it [04:03,  8.21it/s]\titers: 2000, epoch: 6 | loss: 0.3020176\n",
      "\tspeed: 0.1219s/iter; left time: 1271.3479s\n",
      "2099it [04:16,  8.21it/s]\titers: 2100, epoch: 6 | loss: 0.1063731\n",
      "\tspeed: 0.1219s/iter; left time: 1258.7386s\n",
      "2199it [04:28,  8.20it/s]\titers: 2200, epoch: 6 | loss: 0.0394702\n",
      "\tspeed: 0.1220s/iter; left time: 1247.1601s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 6 | loss: 0.0715876\n",
      "\tspeed: 0.1220s/iter; left time: 1234.8876s\n",
      "2399it [04:52,  8.20it/s]\titers: 2400, epoch: 6 | loss: 0.1067363\n",
      "\tspeed: 0.1224s/iter; left time: 1226.8838s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 6 cost time: 303.3260281085968\n",
      "833it [00:50, 16.36it/s]\n",
      "833it [00:50, 16.37it/s]\n",
      "Epoch: 6 | Train Loss: 0.3143525 Vali Loss: 0.4586123 Test Loss: 0.3619183 MAE Loss: 0.3915767\n",
      "Updating learning rate to 1.2499999999999979e-05\n",
      "99it [00:12,  8.27it/s]\titers: 100, epoch: 7 | loss: 0.0502776\n",
      "\tspeed: 1.2572s/iter; left time: 12372.5608s\n",
      "199it [00:24,  8.27it/s]\titers: 200, epoch: 7 | loss: 1.0159822\n",
      "\tspeed: 0.1208s/iter; left time: 1177.0257s\n",
      "299it [00:36,  8.30it/s]\titers: 300, epoch: 7 | loss: 0.9989552\n",
      "\tspeed: 0.1206s/iter; left time: 1162.9179s\n",
      "399it [00:48,  8.20it/s]\titers: 400, epoch: 7 | loss: 0.1263699\n",
      "\tspeed: 0.1210s/iter; left time: 1154.1795s\n",
      "499it [01:00,  8.21it/s]\titers: 500, epoch: 7 | loss: 0.0977391\n",
      "\tspeed: 0.1223s/iter; left time: 1155.0415s\n",
      "599it [01:12,  8.21it/s]\titers: 600, epoch: 7 | loss: 0.1526425\n",
      "\tspeed: 0.1219s/iter; left time: 1138.2927s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 7 | loss: 0.1918499\n",
      "\tspeed: 0.1219s/iter; left time: 1126.7615s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 7 | loss: 0.0348702\n",
      "\tspeed: 0.1219s/iter; left time: 1114.1148s\n",
      "899it [01:49,  8.20it/s]\titers: 900, epoch: 7 | loss: 0.0815229\n",
      "\tspeed: 0.1220s/iter; left time: 1103.2654s\n",
      "999it [02:01,  8.22it/s]\titers: 1000, epoch: 7 | loss: 0.6186700\n",
      "\tspeed: 0.1217s/iter; left time: 1088.5128s\n",
      "1099it [02:13,  8.21it/s]\titers: 1100, epoch: 7 | loss: 0.1126710\n",
      "\tspeed: 0.1219s/iter; left time: 1077.3722s\n",
      "1199it [02:26,  8.21it/s]\titers: 1200, epoch: 7 | loss: 0.1286798\n",
      "\tspeed: 0.1218s/iter; left time: 1065.0363s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 7 | loss: 0.5095004\n",
      "\tspeed: 0.1217s/iter; left time: 1051.7247s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 7 | loss: 0.1323458\n",
      "\tspeed: 0.1218s/iter; left time: 1040.5243s\n",
      "1499it [03:02,  8.20it/s]\titers: 1500, epoch: 7 | loss: 0.5333555\n",
      "\tspeed: 0.1220s/iter; left time: 1030.0709s\n",
      "1599it [03:14,  8.17it/s]\titers: 1600, epoch: 7 | loss: 1.0382597\n",
      "\tspeed: 0.1221s/iter; left time: 1018.7163s\n",
      "1699it [03:27,  8.19it/s]\titers: 1700, epoch: 7 | loss: 0.8116037\n",
      "\tspeed: 0.1220s/iter; left time: 1005.3346s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 7 | loss: 0.1464337\n",
      "\tspeed: 0.1220s/iter; left time: 993.0455s\n",
      "1899it [03:51,  8.21it/s]\titers: 1900, epoch: 7 | loss: 0.0841963\n",
      "\tspeed: 0.1220s/iter; left time: 980.8954s\n",
      "1999it [04:03,  8.19it/s]\titers: 2000, epoch: 7 | loss: 0.3125867\n",
      "\tspeed: 0.1220s/iter; left time: 968.8137s\n",
      "2099it [04:15,  8.18it/s]\titers: 2100, epoch: 7 | loss: 0.1100315\n",
      "\tspeed: 0.1221s/iter; left time: 957.7124s\n",
      "2199it [04:28,  8.21it/s]\titers: 2200, epoch: 7 | loss: 0.0371908\n",
      "\tspeed: 0.1220s/iter; left time: 944.4747s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 7 | loss: 0.0712200\n",
      "\tspeed: 0.1219s/iter; left time: 931.5445s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 7 | loss: 0.1051287\n",
      "\tspeed: 0.1223s/iter; left time: 922.2469s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 7 cost time: 303.04630947113037\n",
      "833it [00:50, 16.36it/s]\n",
      "833it [00:50, 16.39it/s]\n",
      "Epoch: 7 | Train Loss: 0.3141983 Vali Loss: 0.4540099 Test Loss: 0.3516092 MAE Loss: 0.3853377\n",
      "Updating learning rate to 6.249999999999989e-06\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 8 | loss: 0.0448721\n",
      "\tspeed: 1.2648s/iter; left time: 9303.8773s\n",
      "199it [00:24,  8.23it/s]\titers: 200, epoch: 8 | loss: 1.0157477\n",
      "\tspeed: 0.1217s/iter; left time: 883.2067s\n",
      "299it [00:36,  8.22it/s]\titers: 300, epoch: 8 | loss: 0.8695819\n",
      "\tspeed: 0.1215s/iter; left time: 869.7565s\n",
      "399it [00:48,  8.18it/s]\titers: 400, epoch: 8 | loss: 0.1262119\n",
      "\tspeed: 0.1218s/iter; left time: 859.0978s\n",
      "499it [01:01,  8.21it/s]\titers: 500, epoch: 8 | loss: 0.1043156\n",
      "\tspeed: 0.1220s/iter; left time: 848.3976s\n",
      "599it [01:13,  8.23it/s]\titers: 600, epoch: 8 | loss: 0.1636112\n",
      "\tspeed: 0.1219s/iter; left time: 835.5085s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 8 | loss: 0.1925516\n",
      "\tspeed: 0.1218s/iter; left time: 822.9973s\n",
      "799it [01:37,  8.23it/s]\titers: 800, epoch: 8 | loss: 0.0369022\n",
      "\tspeed: 0.1217s/iter; left time: 810.1031s\n",
      "899it [01:49,  8.11it/s]\titers: 900, epoch: 8 | loss: 0.0787857\n",
      "\tspeed: 0.1219s/iter; left time: 798.9714s\n",
      "999it [02:01,  8.24it/s]\titers: 1000, epoch: 8 | loss: 0.6531896\n",
      "\tspeed: 0.1217s/iter; left time: 785.6594s\n",
      "1099it [02:14,  8.19it/s]\titers: 1100, epoch: 8 | loss: 0.1091567\n",
      "\tspeed: 0.1216s/iter; left time: 773.0699s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 8 | loss: 0.1282128\n",
      "\tspeed: 0.1219s/iter; left time: 762.8499s\n",
      "1299it [02:38,  8.20it/s]\titers: 1300, epoch: 8 | loss: 0.4719114\n",
      "\tspeed: 0.1219s/iter; left time: 750.4140s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 8 | loss: 0.1324397\n",
      "\tspeed: 0.1220s/iter; left time: 738.6043s\n",
      "1499it [03:02,  8.21it/s]\titers: 1500, epoch: 8 | loss: 0.5679422\n",
      "\tspeed: 0.1219s/iter; left time: 725.7751s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 8 | loss: 1.0896837\n",
      "\tspeed: 0.1219s/iter; left time: 713.7919s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 8 | loss: 0.8070434\n",
      "\tspeed: 0.1219s/iter; left time: 701.3946s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 8 | loss: 0.1493082\n",
      "\tspeed: 0.1219s/iter; left time: 689.3837s\n",
      "1899it [03:51,  8.20it/s]\titers: 1900, epoch: 8 | loss: 0.0906740\n",
      "\tspeed: 0.1221s/iter; left time: 678.2054s\n",
      "1999it [04:03,  8.17it/s]\titers: 2000, epoch: 8 | loss: 0.3124506\n",
      "\tspeed: 0.1223s/iter; left time: 667.1026s\n",
      "2099it [04:16,  8.18it/s]\titers: 2100, epoch: 8 | loss: 0.1172454\n",
      "\tspeed: 0.1222s/iter; left time: 654.3815s\n",
      "2199it [04:28,  8.19it/s]\titers: 2200, epoch: 8 | loss: 0.0322632\n",
      "\tspeed: 0.1222s/iter; left time: 642.4849s\n",
      "2299it [04:40,  8.18it/s]\titers: 2300, epoch: 8 | loss: 0.0753517\n",
      "\tspeed: 0.1223s/iter; left time: 630.7119s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 8 | loss: 0.1123921\n",
      "\tspeed: 0.1219s/iter; left time: 616.4990s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 8 cost time: 303.32162833213806\n",
      "833it [00:50, 16.37it/s]\n",
      "833it [00:50, 16.35it/s]\n",
      "Epoch: 8 | Train Loss: 0.3134997 Vali Loss: 0.4481571 Test Loss: 0.3451743 MAE Loss: 0.3813489\n",
      "Updating learning rate to 3.1249999999999946e-06\n",
      "99it [00:12,  8.18it/s]\titers: 100, epoch: 9 | loss: 0.0449504\n",
      "\tspeed: 1.2644s/iter; left time: 6158.7249s\n",
      "199it [00:24,  8.23it/s]\titers: 200, epoch: 9 | loss: 1.0359209\n",
      "\tspeed: 0.1218s/iter; left time: 580.8898s\n",
      "299it [00:36,  8.25it/s]\titers: 300, epoch: 9 | loss: 0.7930785\n",
      "\tspeed: 0.1214s/iter; left time: 567.1837s\n",
      "399it [00:48,  8.23it/s]\titers: 400, epoch: 9 | loss: 0.1211611\n",
      "\tspeed: 0.1215s/iter; left time: 555.2438s\n",
      "499it [01:01,  8.22it/s]\titers: 500, epoch: 9 | loss: 0.1017774\n",
      "\tspeed: 0.1217s/iter; left time: 544.2001s\n",
      "599it [01:13,  8.20it/s]\titers: 600, epoch: 9 | loss: 0.1423902\n",
      "\tspeed: 0.1217s/iter; left time: 532.0286s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 9 | loss: 0.1923842\n",
      "\tspeed: 0.1218s/iter; left time: 520.0010s\n",
      "799it [01:37,  8.21it/s]\titers: 800, epoch: 9 | loss: 0.0363413\n",
      "\tspeed: 0.1218s/iter; left time: 507.9171s\n",
      "899it [01:49,  8.21it/s]\titers: 900, epoch: 9 | loss: 0.0740539\n",
      "\tspeed: 0.1217s/iter; left time: 495.5341s\n",
      "999it [02:01,  8.22it/s]\titers: 1000, epoch: 9 | loss: 0.5679403\n",
      "\tspeed: 0.1215s/iter; left time: 482.6462s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 9 | loss: 0.1067303\n",
      "\tspeed: 0.1216s/iter; left time: 470.8417s\n",
      "1199it [02:26,  8.20it/s]\titers: 1200, epoch: 9 | loss: 0.1311193\n",
      "\tspeed: 0.1220s/iter; left time: 460.1010s\n",
      "1299it [02:38,  8.22it/s]\titers: 1300, epoch: 9 | loss: 0.4999980\n",
      "\tspeed: 0.1219s/iter; left time: 447.4634s\n",
      "1399it [02:50,  8.22it/s]\titers: 1400, epoch: 9 | loss: 0.1305431\n",
      "\tspeed: 0.1218s/iter; left time: 434.9770s\n",
      "1499it [03:02,  8.20it/s]\titers: 1500, epoch: 9 | loss: 0.5562909\n",
      "\tspeed: 0.1218s/iter; left time: 422.9158s\n",
      "1599it [03:14,  8.19it/s]\titers: 1600, epoch: 9 | loss: 1.0555617\n",
      "\tspeed: 0.1220s/iter; left time: 411.2098s\n",
      "1699it [03:27,  8.19it/s]\titers: 1700, epoch: 9 | loss: 0.8652662\n",
      "\tspeed: 0.1220s/iter; left time: 399.0650s\n",
      "1799it [03:39,  8.22it/s]\titers: 1800, epoch: 9 | loss: 0.1552288\n",
      "\tspeed: 0.1219s/iter; left time: 386.5430s\n",
      "1899it [03:51,  8.22it/s]\titers: 1900, epoch: 9 | loss: 0.0907785\n",
      "\tspeed: 0.1217s/iter; left time: 373.8231s\n",
      "1999it [04:03,  8.22it/s]\titers: 2000, epoch: 9 | loss: 0.3048871\n",
      "\tspeed: 0.1218s/iter; left time: 361.8203s\n",
      "2099it [04:15,  8.23it/s]\titers: 2100, epoch: 9 | loss: 0.1190702\n",
      "\tspeed: 0.1218s/iter; left time: 349.6215s\n",
      "2199it [04:28,  8.22it/s]\titers: 2200, epoch: 9 | loss: 0.0377102\n",
      "\tspeed: 0.1218s/iter; left time: 337.5756s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 9 | loss: 0.0710005\n",
      "\tspeed: 0.1219s/iter; left time: 325.5766s\n",
      "2399it [04:52,  8.18it/s]\titers: 2400, epoch: 9 | loss: 0.1089400\n",
      "\tspeed: 0.1219s/iter; left time: 313.4057s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 9 cost time: 303.0115954875946\n",
      "833it [00:51, 16.32it/s]\n",
      "833it [00:51, 16.33it/s]\n",
      "Epoch: 9 | Train Loss: 0.3127405 Vali Loss: 0.4416655 Test Loss: 0.3408701 MAE Loss: 0.3786573\n",
      "Updating learning rate to 1.5624999999999973e-06\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 10 | loss: 0.0471725\n",
      "\tspeed: 1.2642s/iter; left time: 3016.3892s\n",
      "199it [00:24,  8.23it/s]\titers: 200, epoch: 10 | loss: 1.0337678\n",
      "\tspeed: 0.1218s/iter; left time: 278.5044s\n",
      "299it [00:36,  8.23it/s]\titers: 300, epoch: 10 | loss: 0.7329490\n",
      "\tspeed: 0.1216s/iter; left time: 265.7494s\n",
      "399it [00:48,  8.21it/s]\titers: 400, epoch: 10 | loss: 0.1244179\n",
      "\tspeed: 0.1216s/iter; left time: 253.6113s\n",
      "499it [01:01,  8.19it/s]\titers: 500, epoch: 10 | loss: 0.1063800\n",
      "\tspeed: 0.1220s/iter; left time: 242.3778s\n",
      "599it [01:13,  8.23it/s]\titers: 600, epoch: 10 | loss: 0.1330512\n",
      "\tspeed: 0.1218s/iter; left time: 229.7875s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 10 | loss: 0.2102944\n",
      "\tspeed: 0.1221s/iter; left time: 218.1055s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 10 | loss: 0.0308989\n",
      "\tspeed: 0.1220s/iter; left time: 205.6673s\n",
      "899it [01:49,  8.23it/s]\titers: 900, epoch: 10 | loss: 0.0726052\n",
      "\tspeed: 0.1217s/iter; left time: 193.0430s\n",
      "999it [02:02,  8.22it/s]\titers: 1000, epoch: 10 | loss: 0.5796337\n",
      "\tspeed: 0.1215s/iter; left time: 180.5555s\n",
      "1099it [02:14,  8.21it/s]\titers: 1100, epoch: 10 | loss: 0.1079719\n",
      "\tspeed: 0.1216s/iter; left time: 168.5646s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 10 | loss: 0.1322616\n",
      "\tspeed: 0.1219s/iter; left time: 156.7353s\n",
      "1299it [02:38,  8.20it/s]\titers: 1300, epoch: 10 | loss: 0.4816454\n",
      "\tspeed: 0.1221s/iter; left time: 144.7625s\n",
      "1399it [02:50,  8.20it/s]\titers: 1400, epoch: 10 | loss: 0.1339544\n",
      "\tspeed: 0.1221s/iter; left time: 132.6036s\n",
      "1499it [03:03,  8.19it/s]\titers: 1500, epoch: 10 | loss: 0.4883949\n",
      "\tspeed: 0.1221s/iter; left time: 120.3761s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 10 | loss: 1.1143407\n",
      "\tspeed: 0.1220s/iter; left time: 108.0924s\n",
      "1699it [03:27,  8.19it/s]\titers: 1700, epoch: 10 | loss: 0.8263856\n",
      "\tspeed: 0.1220s/iter; left time: 95.8610s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 10 | loss: 0.1503581\n",
      "\tspeed: 0.1220s/iter; left time: 83.6803s\n",
      "1899it [03:51,  8.21it/s]\titers: 1900, epoch: 10 | loss: 0.0890417\n",
      "\tspeed: 0.1220s/iter; left time: 71.4757s\n",
      "1999it [04:04,  8.20it/s]\titers: 2000, epoch: 10 | loss: 0.3150498\n",
      "\tspeed: 0.1219s/iter; left time: 59.2642s\n",
      "2099it [04:16,  8.21it/s]\titers: 2100, epoch: 10 | loss: 0.1246274\n",
      "\tspeed: 0.1222s/iter; left time: 47.1683s\n",
      "2199it [04:28,  8.23it/s]\titers: 2200, epoch: 10 | loss: 0.0343968\n",
      "\tspeed: 0.1221s/iter; left time: 34.9106s\n",
      "2299it [04:40,  8.20it/s]\titers: 2300, epoch: 10 | loss: 0.0714502\n",
      "\tspeed: 0.1218s/iter; left time: 22.6510s\n",
      "2399it [04:52,  8.19it/s]\titers: 2400, epoch: 10 | loss: 0.1090454\n",
      "\tspeed: 0.1220s/iter; left time: 10.4885s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 10 cost time: 303.40256667137146\n",
      "833it [00:50, 16.35it/s]\n",
      "833it [00:50, 16.34it/s]\n",
      "Epoch: 10 | Train Loss: 0.3120333 Vali Loss: 0.4381297 Test Loss: 0.3386329 MAE Loss: 0.3774307\n",
      "Updating learning rate to 7.812499999999987e-07\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "train_epochs=10\n",
    "learning_rate=0.01\n",
    "llama_layers=6\n",
    "\n",
    "#master_port=00097\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --main_process_port \"00097\"\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-ETTh1'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --dynamo_backend \"no\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `python -m accelerate.commands.launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-04-29 23:25:39,693] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 23:25:40,715] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-29 23:25:40,715] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 59647\n",
      "val 19999\n",
      "test 19999\n",
      "[2024-04-29 23:25:42,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-29 23:25:42,627] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-04-29 23:25:42,628] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-04-29 23:25:42,628] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-04-29 23:25:42,628] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-04-29 23:25:42,629] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-04-29 23:25:42,629] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-04-29 23:25:42,629] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-04-29 23:25:42,629] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-04-29 23:25:42,629] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-04-29 23:25:42,629] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-04-29 23:25:42,927] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-04-29 23:25:42,927] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB \n",
      "[2024-04-29 23:25:42,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.64 GB, percent = 6.0%\n",
      "[2024-04-29 23:25:43,057] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-04-29 23:25:43,057] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-04-29 23:25:43,058] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.64 GB, percent = 6.0%\n",
      "[2024-04-29 23:25:43,058] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-04-29 23:25:43,180] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-04-29 23:25:43,181] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-04-29 23:25:43,181] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.64 GB, percent = 6.0%\n",
      "[2024-04-29 23:25:43,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-04-29 23:25:43,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-04-29 23:25:43,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-04-29 23:25:43,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-04-29 23:25:43,182] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc6e0780710>\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-04-29 23:25:43,183] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-04-29 23:25:43,184] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-04-29 23:25:43,184] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-04-29 23:25:43,184] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-04-29 23:25:43,184] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:13,  7.95it/s]\titers: 100, epoch: 1 | loss: 0.1071717\n",
      "\tspeed: 0.1453s/iter; left time: 3596.5775s\n",
      "199it [00:25,  7.99it/s]\titers: 200, epoch: 1 | loss: 1.2981442\n",
      "\tspeed: 0.1258s/iter; left time: 3100.1690s\n",
      "299it [00:38,  8.00it/s]\titers: 300, epoch: 1 | loss: 1.1941724\n",
      "\tspeed: 0.1252s/iter; left time: 3073.6047s\n",
      "399it [00:50,  7.96it/s]\titers: 400, epoch: 1 | loss: 0.1529514\n",
      "\tspeed: 0.1253s/iter; left time: 3064.4448s\n",
      "499it [01:03,  7.96it/s]\titers: 500, epoch: 1 | loss: 0.1310877\n",
      "\tspeed: 0.1257s/iter; left time: 3061.0673s\n",
      "599it [01:16,  7.96it/s]\titers: 600, epoch: 1 | loss: 0.1540346\n",
      "\tspeed: 0.1256s/iter; left time: 3044.9371s\n",
      "699it [01:28,  7.99it/s]\titers: 700, epoch: 1 | loss: 0.2231875\n",
      "\tspeed: 0.1255s/iter; left time: 3031.1182s\n",
      "799it [01:41,  7.95it/s]\titers: 800, epoch: 1 | loss: 0.0444481\n",
      "\tspeed: 0.1257s/iter; left time: 3023.7393s\n",
      "899it [01:53,  7.97it/s]\titers: 900, epoch: 1 | loss: 0.0803411\n",
      "\tspeed: 0.1257s/iter; left time: 3009.6575s\n",
      "999it [02:06,  7.99it/s]\titers: 1000, epoch: 1 | loss: 0.7027832\n",
      "\tspeed: 0.1253s/iter; left time: 2987.8243s\n",
      "1099it [02:18,  7.50it/s]\titers: 1100, epoch: 1 | loss: 0.1127384\n",
      "\tspeed: 0.1255s/iter; left time: 2981.5734s\n",
      "1199it [02:31,  7.96it/s]\titers: 1200, epoch: 1 | loss: 0.1403320\n",
      "\tspeed: 0.1260s/iter; left time: 2979.8690s\n",
      "1299it [02:44,  7.99it/s]\titers: 1300, epoch: 1 | loss: 0.5901608\n",
      "\tspeed: 0.1258s/iter; left time: 2963.3825s\n",
      "1399it [02:56,  7.97it/s]\titers: 1400, epoch: 1 | loss: 0.1514044\n",
      "\tspeed: 0.1256s/iter; left time: 2946.0686s\n",
      "1499it [03:09,  7.95it/s]\titers: 1500, epoch: 1 | loss: 0.6951983\n",
      "\tspeed: 0.1256s/iter; left time: 2933.7510s\n",
      "1599it [03:21,  7.96it/s]\titers: 1600, epoch: 1 | loss: 0.9688895\n",
      "\tspeed: 0.1256s/iter; left time: 2921.0739s\n",
      "1699it [03:34,  7.97it/s]\titers: 1700, epoch: 1 | loss: 0.8408502\n",
      "\tspeed: 0.1256s/iter; left time: 2906.9917s\n",
      "1799it [03:46,  7.96it/s]\titers: 1800, epoch: 1 | loss: 0.1638935\n",
      "\tspeed: 0.1257s/iter; left time: 2896.4493s\n",
      "1899it [03:59,  7.97it/s]\titers: 1900, epoch: 1 | loss: 0.1150683\n",
      "\tspeed: 0.1254s/iter; left time: 2879.1909s\n",
      "1999it [04:11,  7.98it/s]\titers: 2000, epoch: 1 | loss: 0.3183035\n",
      "\tspeed: 0.1254s/iter; left time: 2864.5924s\n",
      "2099it [04:24,  7.97it/s]\titers: 2100, epoch: 1 | loss: 0.1121156\n",
      "\tspeed: 0.1256s/iter; left time: 2858.4129s\n",
      "2199it [04:37,  7.98it/s]\titers: 2200, epoch: 1 | loss: 0.0514197\n",
      "\tspeed: 0.1257s/iter; left time: 2846.5228s\n",
      "2299it [04:49,  7.96it/s]\titers: 2300, epoch: 1 | loss: 0.0595963\n",
      "\tspeed: 0.1258s/iter; left time: 2837.1187s\n",
      "2399it [05:02,  7.98it/s]\titers: 2400, epoch: 1 | loss: 0.0954722\n",
      "\tspeed: 0.1255s/iter; left time: 2818.4605s\n",
      "2485it [05:13,  7.94it/s]\n",
      "Epoch: 1 cost time: 313.04024291038513\n",
      "833it [00:54, 15.40it/s]\n",
      "833it [00:53, 15.45it/s]\n",
      "Epoch: 1 | Train Loss: 0.3682629 Vali Loss: 0.5037101 Test Loss: 0.4369820 MAE Loss: 0.4327540\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 2 | loss: 0.0542396\n",
      "\tspeed: 1.3483s/iter; left time: 30020.1835s\n",
      "199it [00:24,  8.18it/s]\titers: 200, epoch: 2 | loss: 1.0856442\n",
      "\tspeed: 0.1221s/iter; left time: 2707.3325s\n",
      "299it [00:36,  8.24it/s]\titers: 300, epoch: 2 | loss: 0.8866925\n",
      "\tspeed: 0.1216s/iter; left time: 2682.5415s\n",
      "399it [00:49,  8.21it/s]\titers: 400, epoch: 2 | loss: 0.1295584\n",
      "\tspeed: 0.1217s/iter; left time: 2672.2783s\n",
      "499it [01:01,  8.22it/s]\titers: 500, epoch: 2 | loss: 0.1112955\n",
      "\tspeed: 0.1218s/iter; left time: 2662.7499s\n",
      "599it [01:13,  8.23it/s]\titers: 600, epoch: 2 | loss: 0.1455589\n",
      "\tspeed: 0.1218s/iter; left time: 2650.9863s\n",
      "699it [01:25,  8.22it/s]\titers: 700, epoch: 2 | loss: 0.2029123\n",
      "\tspeed: 0.1220s/iter; left time: 2643.0039s\n",
      "799it [01:37,  8.21it/s]\titers: 800, epoch: 2 | loss: 0.0469525\n",
      "\tspeed: 0.1219s/iter; left time: 2627.9096s\n",
      "899it [01:49,  8.23it/s]\titers: 900, epoch: 2 | loss: 0.0710678\n",
      "\tspeed: 0.1218s/iter; left time: 2614.6154s\n",
      "999it [02:02,  8.24it/s]\titers: 1000, epoch: 2 | loss: 0.6902472\n",
      "\tspeed: 0.1214s/iter; left time: 2594.8814s\n",
      "1099it [02:14,  8.18it/s]\titers: 1100, epoch: 2 | loss: 0.1137989\n",
      "\tspeed: 0.1216s/iter; left time: 2587.0000s\n",
      "1199it [02:26,  8.22it/s]\titers: 1200, epoch: 2 | loss: 0.1286209\n",
      "\tspeed: 0.1218s/iter; left time: 2577.5493s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 2 | loss: 0.5357578\n",
      "\tspeed: 0.1216s/iter; left time: 2562.6461s\n",
      "1399it [02:50,  8.22it/s]\titers: 1400, epoch: 2 | loss: 0.1490072\n",
      "\tspeed: 0.1217s/iter; left time: 2550.9116s\n",
      "1499it [03:02,  8.19it/s]\titers: 1500, epoch: 2 | loss: 0.6192090\n",
      "\tspeed: 0.1219s/iter; left time: 2543.7885s\n",
      "1599it [03:15,  8.20it/s]\titers: 1600, epoch: 2 | loss: 0.8711715\n",
      "\tspeed: 0.1220s/iter; left time: 2533.7689s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 2 | loss: 0.7387740\n",
      "\tspeed: 0.1218s/iter; left time: 2516.1620s\n",
      "1799it [03:39,  8.21it/s]\titers: 1800, epoch: 2 | loss: 0.1564417\n",
      "\tspeed: 0.1220s/iter; left time: 2508.8020s\n",
      "1899it [03:51,  8.21it/s]\titers: 1900, epoch: 2 | loss: 0.1003245\n",
      "\tspeed: 0.1218s/iter; left time: 2492.9819s\n",
      "1999it [04:03,  8.22it/s]\titers: 2000, epoch: 2 | loss: 0.3180134\n",
      "\tspeed: 0.1219s/iter; left time: 2482.3677s\n",
      "2099it [04:16,  8.21it/s]\titers: 2100, epoch: 2 | loss: 0.1080394\n",
      "\tspeed: 0.1218s/iter; left time: 2468.5365s\n",
      "2199it [04:28,  8.24it/s]\titers: 2200, epoch: 2 | loss: 0.0471819\n",
      "\tspeed: 0.1217s/iter; left time: 2454.8020s\n",
      "2299it [04:40,  8.22it/s]\titers: 2300, epoch: 2 | loss: 0.0580913\n",
      "\tspeed: 0.1217s/iter; left time: 2442.2483s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 2 | loss: 0.1028695\n",
      "\tspeed: 0.1221s/iter; left time: 2438.2916s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 2 cost time: 303.18173933029175\n",
      "833it [00:50, 16.36it/s]\n",
      "833it [00:50, 16.37it/s]\n",
      "Epoch: 2 | Train Loss: 0.3262092 Vali Loss: 0.4778337 Test Loss: 0.4033230 MAE Loss: 0.4142924\n",
      "Updating learning rate to 1.9999999999999998e-05\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 3 | loss: 0.0481806\n",
      "\tspeed: 1.2850s/iter; left time: 25417.6763s\n",
      "199it [00:24,  8.21it/s]\titers: 200, epoch: 3 | loss: 1.0336119\n",
      "\tspeed: 0.1218s/iter; left time: 2396.4099s\n",
      "299it [00:36,  8.23it/s]\titers: 300, epoch: 3 | loss: 0.9269393\n",
      "\tspeed: 0.1216s/iter; left time: 2380.7356s\n",
      "399it [00:48,  8.20it/s]\titers: 400, epoch: 3 | loss: 0.1252273\n",
      "\tspeed: 0.1216s/iter; left time: 2369.0860s\n",
      "499it [01:01,  8.20it/s]\titers: 500, epoch: 3 | loss: 0.1100117\n",
      "\tspeed: 0.1219s/iter; left time: 2362.4239s\n",
      "599it [01:13,  8.21it/s]\titers: 600, epoch: 3 | loss: 0.1381580\n",
      "\tspeed: 0.1218s/iter; left time: 2349.1706s\n",
      "699it [01:25,  8.20it/s]\titers: 700, epoch: 3 | loss: 0.1950275\n",
      "\tspeed: 0.1219s/iter; left time: 2337.8477s\n",
      "799it [01:37,  8.22it/s]\titers: 800, epoch: 3 | loss: 0.0369016\n",
      "\tspeed: 0.1218s/iter; left time: 2323.7216s\n",
      "899it [01:49,  8.22it/s]\titers: 900, epoch: 3 | loss: 0.0741011\n",
      "\tspeed: 0.1218s/iter; left time: 2312.4617s\n",
      "999it [02:02,  8.22it/s]\titers: 1000, epoch: 3 | loss: 0.5421241\n",
      "\tspeed: 0.1216s/iter; left time: 2296.3433s\n",
      "1099it [02:14,  8.21it/s]\titers: 1100, epoch: 3 | loss: 0.1113662\n",
      "\tspeed: 0.1219s/iter; left time: 2288.5210s\n",
      "1199it [02:26,  8.18it/s]\titers: 1200, epoch: 3 | loss: 0.1243114\n",
      "\tspeed: 0.1218s/iter; left time: 2275.2439s\n",
      "1299it [02:38,  8.24it/s]\titers: 1300, epoch: 3 | loss: 0.4918513\n",
      "\tspeed: 0.1217s/iter; left time: 2260.7537s\n",
      "1399it [02:50,  8.23it/s]\titers: 1400, epoch: 3 | loss: 0.1438904\n",
      "\tspeed: 0.1217s/iter; left time: 2248.8771s\n",
      "1499it [03:02,  8.20it/s]\titers: 1500, epoch: 3 | loss: 0.5917367\n",
      "\tspeed: 0.1218s/iter; left time: 2238.2889s\n",
      "1599it [03:15,  8.22it/s]\titers: 1600, epoch: 3 | loss: 0.9037583\n",
      "\tspeed: 0.1218s/iter; left time: 2226.1921s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 3 | loss: 0.7420666\n",
      "\tspeed: 0.1218s/iter; left time: 2214.9137s\n",
      "1799it [03:39,  8.21it/s]\titers: 1800, epoch: 3 | loss: 0.1553153\n",
      "\tspeed: 0.1218s/iter; left time: 2203.1441s\n",
      "1899it [03:51,  8.02it/s]\titers: 1900, epoch: 3 | loss: 0.0871731\n",
      "\tspeed: 0.1220s/iter; left time: 2193.0690s\n",
      "1999it [04:03,  8.21it/s]\titers: 2000, epoch: 3 | loss: 0.2894790\n",
      "\tspeed: 0.1218s/iter; left time: 2178.6221s\n",
      "2099it [04:16,  8.22it/s]\titers: 2100, epoch: 3 | loss: 0.1108627\n",
      "\tspeed: 0.1220s/iter; left time: 2168.7181s\n",
      "2199it [04:28,  8.23it/s]\titers: 2200, epoch: 3 | loss: 0.0437992\n",
      "\tspeed: 0.1219s/iter; left time: 2156.1743s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 3 | loss: 0.0590112\n",
      "\tspeed: 0.1217s/iter; left time: 2139.8285s\n",
      "2399it [04:52,  8.22it/s]\titers: 2400, epoch: 3 | loss: 0.0955463\n",
      "\tspeed: 0.1218s/iter; left time: 2128.5129s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 3 cost time: 303.1128203868866\n",
      "833it [00:51, 16.33it/s]\n",
      "833it [00:51, 16.31it/s]\n",
      "Epoch: 3 | Train Loss: 0.3136620 Vali Loss: 0.4683289 Test Loss: 0.3776456 MAE Loss: 0.4000569\n",
      "Updating learning rate to 9.999999999999999e-06\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 4 | loss: 0.0526108\n",
      "\tspeed: 1.2619s/iter; left time: 21825.2084s\n",
      "199it [00:24,  8.22it/s]\titers: 200, epoch: 4 | loss: 1.0531201\n",
      "\tspeed: 0.1220s/iter; left time: 2098.2378s\n",
      "299it [00:36,  8.21it/s]\titers: 300, epoch: 4 | loss: 0.8320128\n",
      "\tspeed: 0.1218s/iter; left time: 2082.1865s\n",
      "399it [00:49,  8.22it/s]\titers: 400, epoch: 4 | loss: 0.1221905\n",
      "\tspeed: 0.1216s/iter; left time: 2066.8528s\n",
      "499it [01:01,  8.20it/s]\titers: 500, epoch: 4 | loss: 0.1080783\n",
      "\tspeed: 0.1221s/iter; left time: 2063.4844s\n",
      "599it [01:13,  8.23it/s]\titers: 600, epoch: 4 | loss: 0.1389785\n",
      "\tspeed: 0.1224s/iter; left time: 2055.0711s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 4 | loss: 0.1971639\n",
      "\tspeed: 0.1222s/iter; left time: 2039.9032s\n",
      "799it [01:37,  8.22it/s]\titers: 800, epoch: 4 | loss: 0.0344768\n",
      "\tspeed: 0.1218s/iter; left time: 2021.6553s\n",
      "899it [01:50,  8.20it/s]\titers: 900, epoch: 4 | loss: 0.0688686\n",
      "\tspeed: 0.1218s/iter; left time: 2009.4044s\n",
      "999it [02:02,  8.22it/s]\titers: 1000, epoch: 4 | loss: 0.5178009\n",
      "\tspeed: 0.1216s/iter; left time: 1993.6617s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 4 | loss: 0.1066888\n",
      "\tspeed: 0.1217s/iter; left time: 1983.2051s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 4 | loss: 0.1207483\n",
      "\tspeed: 0.1219s/iter; left time: 1974.9051s\n",
      "1299it [02:38,  8.21it/s]\titers: 1300, epoch: 4 | loss: 0.4868578\n",
      "\tspeed: 0.1224s/iter; left time: 1970.7802s\n",
      "1399it [02:51,  8.22it/s]\titers: 1400, epoch: 4 | loss: 0.1343390\n",
      "\tspeed: 0.1219s/iter; left time: 1950.6032s\n",
      "1499it [03:03,  8.18it/s]\titers: 1500, epoch: 4 | loss: 0.6056694\n",
      "\tspeed: 0.1219s/iter; left time: 1937.3218s\n",
      "1599it [03:15,  8.18it/s]\titers: 1600, epoch: 4 | loss: 0.9072855\n",
      "\tspeed: 0.1221s/iter; left time: 1928.3837s\n",
      "1699it [03:27,  8.16it/s]\titers: 1700, epoch: 4 | loss: 0.7379816\n",
      "\tspeed: 0.1223s/iter; left time: 1918.8874s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 4 | loss: 0.1555106\n",
      "\tspeed: 0.1221s/iter; left time: 1904.4939s\n",
      "1899it [03:52,  8.21it/s]\titers: 1900, epoch: 4 | loss: 0.0891850\n",
      "\tspeed: 0.1219s/iter; left time: 1889.0543s\n",
      "1999it [04:04,  8.21it/s]\titers: 2000, epoch: 4 | loss: 0.2799387\n",
      "\tspeed: 0.1226s/iter; left time: 1887.3287s\n",
      "2099it [04:16,  8.21it/s]\titers: 2100, epoch: 4 | loss: 0.1048144\n",
      "\tspeed: 0.1219s/iter; left time: 1865.2941s\n",
      "2199it [04:28,  8.21it/s]\titers: 2200, epoch: 4 | loss: 0.0461092\n",
      "\tspeed: 0.1220s/iter; left time: 1853.1968s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 4 | loss: 0.0625274\n",
      "\tspeed: 0.1219s/iter; left time: 1840.8859s\n",
      "2399it [04:53,  8.20it/s]\titers: 2400, epoch: 4 | loss: 0.1028187\n",
      "\tspeed: 0.1218s/iter; left time: 1826.2558s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 4 cost time: 303.57204270362854\n",
      "833it [00:50, 16.34it/s]\n",
      "833it [00:50, 16.37it/s]\n",
      "Epoch: 4 | Train Loss: 0.3073950 Vali Loss: 0.4613216 Test Loss: 0.3621935 MAE Loss: 0.3924860\n",
      "Updating learning rate to 4.9999999999999996e-06\n",
      "99it [00:12,  8.19it/s]\titers: 100, epoch: 5 | loss: 0.0486939\n",
      "\tspeed: 1.2617s/iter; left time: 18687.1859s\n",
      "199it [00:24,  8.22it/s]\titers: 200, epoch: 5 | loss: 0.9909213\n",
      "\tspeed: 0.1221s/iter; left time: 1796.0264s\n",
      "299it [00:36,  8.22it/s]\titers: 300, epoch: 5 | loss: 0.8808450\n",
      "\tspeed: 0.1216s/iter; left time: 1776.7272s\n",
      "399it [00:48,  8.22it/s]\titers: 400, epoch: 5 | loss: 0.1267980\n",
      "\tspeed: 0.1215s/iter; left time: 1762.4580s\n",
      "499it [01:01,  8.22it/s]\titers: 500, epoch: 5 | loss: 0.1069040\n",
      "\tspeed: 0.1217s/iter; left time: 1754.4444s\n",
      "599it [01:13,  8.24it/s]\titers: 600, epoch: 5 | loss: 0.1463281\n",
      "\tspeed: 0.1217s/iter; left time: 1741.6923s\n",
      "699it [01:25,  8.23it/s]\titers: 700, epoch: 5 | loss: 0.1973669\n",
      "\tspeed: 0.1219s/iter; left time: 1731.8831s\n",
      "799it [01:37,  8.22it/s]\titers: 800, epoch: 5 | loss: 0.0369765\n",
      "\tspeed: 0.1218s/iter; left time: 1718.2587s\n",
      "899it [01:49,  8.22it/s]\titers: 900, epoch: 5 | loss: 0.0695593\n",
      "\tspeed: 0.1218s/iter; left time: 1706.2922s\n",
      "999it [02:01,  8.24it/s]\titers: 1000, epoch: 5 | loss: 0.5806488\n",
      "\tspeed: 0.1215s/iter; left time: 1690.0630s\n",
      "1099it [02:14,  8.21it/s]\titers: 1100, epoch: 5 | loss: 0.1063451\n",
      "\tspeed: 0.1216s/iter; left time: 1679.8582s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 5 | loss: 0.1217021\n",
      "\tspeed: 0.1221s/iter; left time: 1673.9433s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 5 | loss: 0.4907848\n",
      "\tspeed: 0.1219s/iter; left time: 1659.3822s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 5 | loss: 0.1374593\n",
      "\tspeed: 0.1220s/iter; left time: 1647.7834s\n",
      "1499it [03:02,  8.19it/s]\titers: 1500, epoch: 5 | loss: 0.5757670\n",
      "\tspeed: 0.1219s/iter; left time: 1634.8681s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 5 | loss: 0.9897110\n",
      "\tspeed: 0.1219s/iter; left time: 1622.6625s\n",
      "1699it [03:27,  8.22it/s]\titers: 1700, epoch: 5 | loss: 0.7594931\n",
      "\tspeed: 0.1218s/iter; left time: 1609.2925s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 5 | loss: 0.1503337\n",
      "\tspeed: 0.1219s/iter; left time: 1597.8321s\n",
      "1899it [03:51,  8.21it/s]\titers: 1900, epoch: 5 | loss: 0.0911606\n",
      "\tspeed: 0.1219s/iter; left time: 1586.1805s\n",
      "1999it [04:03,  8.19it/s]\titers: 2000, epoch: 5 | loss: 0.2900582\n",
      "\tspeed: 0.1219s/iter; left time: 1574.3355s\n",
      "2099it [04:16,  8.22it/s]\titers: 2100, epoch: 5 | loss: 0.1169933\n",
      "\tspeed: 0.1221s/iter; left time: 1563.6213s\n",
      "2199it [04:28,  8.23it/s]\titers: 2200, epoch: 5 | loss: 0.0405672\n",
      "\tspeed: 0.1219s/iter; left time: 1550.0981s\n",
      "2299it [04:40,  8.20it/s]\titers: 2300, epoch: 5 | loss: 0.0633621\n",
      "\tspeed: 0.1218s/iter; left time: 1536.4912s\n",
      "2399it [04:52,  8.16it/s]\titers: 2400, epoch: 5 | loss: 0.0954605\n",
      "\tspeed: 0.1219s/iter; left time: 1525.1944s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 5 cost time: 303.1823182106018\n",
      "833it [00:50, 16.35it/s]\n",
      "833it [00:50, 16.39it/s]\n",
      "Epoch: 5 | Train Loss: 0.3040214 Vali Loss: 0.4505726 Test Loss: 0.3473157 MAE Loss: 0.3851942\n",
      "Updating learning rate to 2.4999999999999998e-06\n",
      "99it [00:12,  8.22it/s]\titers: 100, epoch: 6 | loss: 0.0475967\n",
      "\tspeed: 1.2725s/iter; left time: 15685.3760s\n",
      "199it [00:24,  8.23it/s]\titers: 200, epoch: 6 | loss: 1.0259718\n",
      "\tspeed: 0.1217s/iter; left time: 1488.2162s\n",
      "299it [00:36,  8.21it/s]\titers: 300, epoch: 6 | loss: 0.8127592\n",
      "\tspeed: 0.1217s/iter; left time: 1475.3137s\n",
      "399it [00:48,  8.17it/s]\titers: 400, epoch: 6 | loss: 0.1268839\n",
      "\tspeed: 0.1222s/iter; left time: 1469.8833s\n",
      "499it [01:01,  8.19it/s]\titers: 500, epoch: 6 | loss: 0.1099935\n",
      "\tspeed: 0.1221s/iter; left time: 1456.2663s\n",
      "599it [01:13,  8.21it/s]\titers: 600, epoch: 6 | loss: 0.1511280\n",
      "\tspeed: 0.1220s/iter; left time: 1443.0229s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 6 | loss: 0.1959762\n",
      "\tspeed: 0.1221s/iter; left time: 1431.5289s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 6 | loss: 0.0268386\n",
      "\tspeed: 0.1220s/iter; left time: 1418.4051s\n",
      "899it [01:50,  8.18it/s]\titers: 900, epoch: 6 | loss: 0.0733316\n",
      "\tspeed: 0.1221s/iter; left time: 1406.9826s\n",
      "999it [02:02,  8.23it/s]\titers: 1000, epoch: 6 | loss: 0.5492314\n",
      "\tspeed: 0.1217s/iter; left time: 1391.0289s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 6 | loss: 0.1071721\n",
      "\tspeed: 0.1218s/iter; left time: 1379.2063s\n",
      "1199it [02:26,  8.22it/s]\titers: 1200, epoch: 6 | loss: 0.1314935\n",
      "\tspeed: 0.1217s/iter; left time: 1366.5628s\n",
      "1299it [02:38,  8.25it/s]\titers: 1300, epoch: 6 | loss: 0.4916952\n",
      "\tspeed: 0.1217s/iter; left time: 1353.8847s\n",
      "1399it [02:50,  8.23it/s]\titers: 1400, epoch: 6 | loss: 0.1319183\n",
      "\tspeed: 0.1218s/iter; left time: 1342.8866s\n",
      "1499it [03:03,  8.22it/s]\titers: 1500, epoch: 6 | loss: 0.5113780\n",
      "\tspeed: 0.1216s/iter; left time: 1328.9524s\n",
      "1599it [03:15,  8.22it/s]\titers: 1600, epoch: 6 | loss: 1.0052153\n",
      "\tspeed: 0.1219s/iter; left time: 1319.7398s\n",
      "1699it [03:27,  8.20it/s]\titers: 1700, epoch: 6 | loss: 0.7643064\n",
      "\tspeed: 0.1219s/iter; left time: 1307.3327s\n",
      "1799it [03:39,  8.21it/s]\titers: 1800, epoch: 6 | loss: 0.1503840\n",
      "\tspeed: 0.1220s/iter; left time: 1296.8244s\n",
      "1899it [03:51,  8.24it/s]\titers: 1900, epoch: 6 | loss: 0.0941424\n",
      "\tspeed: 0.1217s/iter; left time: 1281.0168s\n",
      "1999it [04:03,  8.23it/s]\titers: 2000, epoch: 6 | loss: 0.2886471\n",
      "\tspeed: 0.1216s/iter; left time: 1267.9564s\n",
      "2099it [04:16,  8.23it/s]\titers: 2100, epoch: 6 | loss: 0.1187572\n",
      "\tspeed: 0.1217s/iter; left time: 1256.6551s\n",
      "2199it [04:28,  8.24it/s]\titers: 2200, epoch: 6 | loss: 0.0376538\n",
      "\tspeed: 0.1216s/iter; left time: 1243.2980s\n",
      "2299it [04:40,  8.22it/s]\titers: 2300, epoch: 6 | loss: 0.0706924\n",
      "\tspeed: 0.1216s/iter; left time: 1231.1781s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 6 | loss: 0.1024129\n",
      "\tspeed: 0.1216s/iter; left time: 1219.4768s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 6 cost time: 303.1713466644287\n",
      "833it [00:51, 16.32it/s]\n",
      "833it [00:50, 16.36it/s]\n",
      "Epoch: 6 | Train Loss: 0.3034263 Vali Loss: 0.4437299 Test Loss: 0.3373678 MAE Loss: 0.3788292\n",
      "Updating learning rate to 1.2499999999999999e-06\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 7 | loss: 0.0447384\n",
      "\tspeed: 1.2628s/iter; left time: 12427.1472s\n",
      "199it [00:24,  8.21it/s]\titers: 200, epoch: 7 | loss: 0.9586486\n",
      "\tspeed: 0.1218s/iter; left time: 1186.8515s\n",
      "299it [00:36,  8.22it/s]\titers: 300, epoch: 7 | loss: 0.8649328\n",
      "\tspeed: 0.1218s/iter; left time: 1174.7208s\n",
      "399it [00:48,  8.22it/s]\titers: 400, epoch: 7 | loss: 0.1271086\n",
      "\tspeed: 0.1217s/iter; left time: 1160.7474s\n",
      "499it [01:01,  8.22it/s]\titers: 500, epoch: 7 | loss: 0.1074026\n",
      "\tspeed: 0.1217s/iter; left time: 1149.1749s\n",
      "599it [01:13,  8.22it/s]\titers: 600, epoch: 7 | loss: 0.1386989\n",
      "\tspeed: 0.1218s/iter; left time: 1137.3118s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 7 | loss: 0.1901477\n",
      "\tspeed: 0.1222s/iter; left time: 1128.9278s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 7 | loss: 0.0305447\n",
      "\tspeed: 0.1220s/iter; left time: 1115.5707s\n",
      "899it [01:49,  8.18it/s]\titers: 900, epoch: 7 | loss: 0.0765108\n",
      "\tspeed: 0.1225s/iter; left time: 1107.0899s\n",
      "999it [02:02,  8.24it/s]\titers: 1000, epoch: 7 | loss: 0.5779826\n",
      "\tspeed: 0.1217s/iter; left time: 1088.4906s\n",
      "1099it [02:14,  8.23it/s]\titers: 1100, epoch: 7 | loss: 0.1079953\n",
      "\tspeed: 0.1214s/iter; left time: 1073.5889s\n",
      "1199it [02:26,  8.21it/s]\titers: 1200, epoch: 7 | loss: 0.1298250\n",
      "\tspeed: 0.1218s/iter; left time: 1064.3493s\n",
      "1299it [02:38,  8.24it/s]\titers: 1300, epoch: 7 | loss: 0.4802109\n",
      "\tspeed: 0.1217s/iter; left time: 1052.0027s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 7 | loss: 0.1333182\n",
      "\tspeed: 0.1219s/iter; left time: 1041.4917s\n",
      "1499it [03:03,  8.19it/s]\titers: 1500, epoch: 7 | loss: 0.5274199\n",
      "\tspeed: 0.1219s/iter; left time: 1029.1724s\n",
      "1599it [03:15,  8.22it/s]\titers: 1600, epoch: 7 | loss: 1.0135612\n",
      "\tspeed: 0.1219s/iter; left time: 1016.3943s\n",
      "1699it [03:27,  8.20it/s]\titers: 1700, epoch: 7 | loss: 0.8024334\n",
      "\tspeed: 0.1219s/iter; left time: 1004.6197s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 7 | loss: 0.1526403\n",
      "\tspeed: 0.1218s/iter; left time: 991.9437s\n",
      "1899it [03:51,  8.21it/s]\titers: 1900, epoch: 7 | loss: 0.0953681\n",
      "\tspeed: 0.1218s/iter; left time: 979.5017s\n",
      "1999it [04:03,  8.21it/s]\titers: 2000, epoch: 7 | loss: 0.2986341\n",
      "\tspeed: 0.1218s/iter; left time: 967.1930s\n",
      "2099it [04:16,  8.23it/s]\titers: 2100, epoch: 7 | loss: 0.1129175\n",
      "\tspeed: 0.1218s/iter; left time: 955.2964s\n",
      "2199it [04:28,  8.24it/s]\titers: 2200, epoch: 7 | loss: 0.0368344\n",
      "\tspeed: 0.1218s/iter; left time: 942.7815s\n",
      "2299it [04:40,  8.22it/s]\titers: 2300, epoch: 7 | loss: 0.0739578\n",
      "\tspeed: 0.1218s/iter; left time: 930.3931s\n",
      "2399it [04:52,  8.23it/s]\titers: 2400, epoch: 7 | loss: 0.0960623\n",
      "\tspeed: 0.1216s/iter; left time: 917.3078s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 7 cost time: 303.1741678714752\n",
      "833it [00:50, 16.34it/s]\n",
      "833it [00:51, 16.32it/s]\n",
      "Epoch: 7 | Train Loss: 0.3030645 Vali Loss: 0.4394586 Test Loss: 0.3317721 MAE Loss: 0.3750439\n",
      "Updating learning rate to 6.249999999999999e-07\n",
      "99it [00:12,  8.14it/s]\titers: 100, epoch: 8 | loss: 0.0433886\n",
      "\tspeed: 1.2625s/iter; left time: 9287.1683s\n",
      "199it [00:24,  8.21it/s]\titers: 200, epoch: 8 | loss: 0.9708326\n",
      "\tspeed: 0.1219s/iter; left time: 884.2221s\n",
      "299it [00:36,  8.24it/s]\titers: 300, epoch: 8 | loss: 0.8213887\n",
      "\tspeed: 0.1220s/iter; left time: 873.2583s\n",
      "399it [00:49,  8.22it/s]\titers: 400, epoch: 8 | loss: 0.1182711\n",
      "\tspeed: 0.1216s/iter; left time: 857.9673s\n",
      "499it [01:01,  8.21it/s]\titers: 500, epoch: 8 | loss: 0.1045281\n",
      "\tspeed: 0.1219s/iter; left time: 847.6892s\n",
      "599it [01:13,  8.16it/s]\titers: 600, epoch: 8 | loss: 0.1474651\n",
      "\tspeed: 0.1222s/iter; left time: 837.8658s\n",
      "699it [01:25,  8.18it/s]\titers: 700, epoch: 8 | loss: 0.1895068\n",
      "\tspeed: 0.1222s/iter; left time: 825.8159s\n",
      "799it [01:37,  8.18it/s]\titers: 800, epoch: 8 | loss: 0.0337249\n",
      "\tspeed: 0.1226s/iter; left time: 815.6978s\n",
      "899it [01:50,  8.20it/s]\titers: 900, epoch: 8 | loss: 0.0687144\n",
      "\tspeed: 0.1221s/iter; left time: 800.7265s\n",
      "999it [02:02,  8.22it/s]\titers: 1000, epoch: 8 | loss: 0.5797586\n",
      "\tspeed: 0.1217s/iter; left time: 785.9836s\n",
      "1099it [02:14,  8.20it/s]\titers: 1100, epoch: 8 | loss: 0.1082492\n",
      "\tspeed: 0.1217s/iter; left time: 773.4273s\n",
      "1199it [02:26,  8.19it/s]\titers: 1200, epoch: 8 | loss: 0.1282213\n",
      "\tspeed: 0.1222s/iter; left time: 764.5267s\n",
      "1299it [02:38,  8.21it/s]\titers: 1300, epoch: 8 | loss: 0.4474905\n",
      "\tspeed: 0.1220s/iter; left time: 750.8885s\n",
      "1399it [02:51,  8.21it/s]\titers: 1400, epoch: 8 | loss: 0.1278694\n",
      "\tspeed: 0.1220s/iter; left time: 738.5840s\n",
      "1499it [03:03,  8.20it/s]\titers: 1500, epoch: 8 | loss: 0.5392168\n",
      "\tspeed: 0.1218s/iter; left time: 725.5751s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 8 | loss: 1.0583874\n",
      "\tspeed: 0.1220s/iter; left time: 714.2852s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 8 | loss: 0.7860045\n",
      "\tspeed: 0.1219s/iter; left time: 701.8262s\n",
      "1799it [03:39,  8.27it/s]\titers: 1800, epoch: 8 | loss: 0.1497576\n",
      "\tspeed: 0.1217s/iter; left time: 688.3776s\n",
      "1899it [03:51,  8.17it/s]\titers: 1900, epoch: 8 | loss: 0.0947394\n",
      "\tspeed: 0.1211s/iter; left time: 672.8488s\n",
      "1999it [04:04,  8.28it/s]\titers: 2000, epoch: 8 | loss: 0.3024376\n",
      "\tspeed: 0.1217s/iter; left time: 663.7959s\n",
      "2099it [04:16,  8.29it/s]\titers: 2100, epoch: 8 | loss: 0.1251548\n",
      "\tspeed: 0.1208s/iter; left time: 647.1201s\n",
      "2199it [04:28,  8.30it/s]\titers: 2200, epoch: 8 | loss: 0.0349504\n",
      "\tspeed: 0.1207s/iter; left time: 634.5450s\n",
      "2299it [04:40,  8.27it/s]\titers: 2300, epoch: 8 | loss: 0.0794370\n",
      "\tspeed: 0.1208s/iter; left time: 622.7580s\n",
      "2399it [04:52,  8.28it/s]\titers: 2400, epoch: 8 | loss: 0.1002458\n",
      "\tspeed: 0.1209s/iter; left time: 611.2136s\n",
      "2485it [05:02,  8.20it/s]\n",
      "Epoch: 8 cost time: 302.8912079334259\n",
      "833it [00:51, 16.33it/s]\n",
      "833it [00:51, 16.27it/s]\n",
      "Epoch: 8 | Train Loss: 0.3023649 Vali Loss: 0.4328495 Test Loss: 0.3267153 MAE Loss: 0.3716697\n",
      "Updating learning rate to 3.1249999999999997e-07\n",
      "99it [00:12,  8.21it/s]\titers: 100, epoch: 9 | loss: 0.0444023\n",
      "\tspeed: 1.2622s/iter; left time: 6148.2712s\n",
      "199it [00:24,  8.21it/s]\titers: 200, epoch: 9 | loss: 0.9638591\n",
      "\tspeed: 0.1220s/iter; left time: 582.0536s\n",
      "299it [00:36,  8.20it/s]\titers: 300, epoch: 9 | loss: 0.7468925\n",
      "\tspeed: 0.1218s/iter; left time: 569.0856s\n",
      "399it [00:49,  8.21it/s]\titers: 400, epoch: 9 | loss: 0.1225721\n",
      "\tspeed: 0.1218s/iter; left time: 556.8368s\n",
      "499it [01:01,  8.20it/s]\titers: 500, epoch: 9 | loss: 0.1059525\n",
      "\tspeed: 0.1219s/iter; left time: 544.9745s\n",
      "599it [01:13,  8.22it/s]\titers: 600, epoch: 9 | loss: 0.1347970\n",
      "\tspeed: 0.1218s/iter; left time: 532.3669s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 9 | loss: 0.1915988\n",
      "\tspeed: 0.1221s/iter; left time: 521.7016s\n",
      "799it [01:37,  8.22it/s]\titers: 800, epoch: 9 | loss: 0.0320796\n",
      "\tspeed: 0.1219s/iter; left time: 508.4886s\n",
      "899it [01:49,  8.21it/s]\titers: 900, epoch: 9 | loss: 0.0717101\n",
      "\tspeed: 0.1219s/iter; left time: 496.1881s\n",
      "999it [02:02,  8.23it/s]\titers: 1000, epoch: 9 | loss: 0.5282395\n",
      "\tspeed: 0.1216s/iter; left time: 482.9526s\n",
      "1099it [02:14,  8.21it/s]\titers: 1100, epoch: 9 | loss: 0.1075804\n",
      "\tspeed: 0.1216s/iter; left time: 470.7696s\n",
      "1199it [02:26,  8.20it/s]\titers: 1200, epoch: 9 | loss: 0.1276815\n",
      "\tspeed: 0.1218s/iter; left time: 459.2079s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 9 | loss: 0.4625162\n",
      "\tspeed: 0.1216s/iter; left time: 446.4931s\n",
      "1399it [02:50,  8.18it/s]\titers: 1400, epoch: 9 | loss: 0.1349477\n",
      "\tspeed: 0.1217s/iter; left time: 434.6630s\n",
      "1499it [03:02,  8.21it/s]\titers: 1500, epoch: 9 | loss: 0.5492299\n",
      "\tspeed: 0.1217s/iter; left time: 422.5433s\n",
      "1599it [03:15,  8.20it/s]\titers: 1600, epoch: 9 | loss: 1.0781573\n",
      "\tspeed: 0.1217s/iter; left time: 410.2279s\n",
      "1699it [03:27,  8.21it/s]\titers: 1700, epoch: 9 | loss: 0.8098927\n",
      "\tspeed: 0.1219s/iter; left time: 398.6386s\n",
      "1799it [03:39,  8.18it/s]\titers: 1800, epoch: 9 | loss: 0.1540878\n",
      "\tspeed: 0.1220s/iter; left time: 386.8449s\n",
      "1899it [03:51,  8.22it/s]\titers: 1900, epoch: 9 | loss: 0.0982264\n",
      "\tspeed: 0.1218s/iter; left time: 374.1452s\n",
      "1999it [04:03,  8.21it/s]\titers: 2000, epoch: 9 | loss: 0.2938492\n",
      "\tspeed: 0.1217s/iter; left time: 361.6409s\n",
      "2099it [04:16,  8.22it/s]\titers: 2100, epoch: 9 | loss: 0.1280834\n",
      "\tspeed: 0.1218s/iter; left time: 349.7696s\n",
      "2199it [04:28,  8.23it/s]\titers: 2200, epoch: 9 | loss: 0.0399013\n",
      "\tspeed: 0.1217s/iter; left time: 337.3160s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 9 | loss: 0.0729398\n",
      "\tspeed: 0.1218s/iter; left time: 325.2092s\n",
      "2399it [04:52,  8.21it/s]\titers: 2400, epoch: 9 | loss: 0.0996839\n",
      "\tspeed: 0.1218s/iter; left time: 313.1746s\n",
      "2485it [05:03,  8.20it/s]\n",
      "Epoch: 9 cost time: 303.1444480419159\n",
      "833it [00:50, 16.37it/s]\n",
      "833it [00:50, 16.34it/s]\n",
      "Epoch: 9 | Train Loss: 0.3017788 Vali Loss: 0.4290479 Test Loss: 0.3241673 MAE Loss: 0.3699300\n",
      "Updating learning rate to 1.5624999999999999e-07\n",
      "99it [00:12,  8.18it/s]\titers: 100, epoch: 10 | loss: 0.0410205\n",
      "\tspeed: 1.2648s/iter; left time: 3017.7007s\n",
      "199it [00:24,  8.18it/s]\titers: 200, epoch: 10 | loss: 0.9657222\n",
      "\tspeed: 0.1223s/iter; left time: 279.5660s\n",
      "299it [00:36,  8.19it/s]\titers: 300, epoch: 10 | loss: 0.7291304\n",
      "\tspeed: 0.1221s/iter; left time: 266.9281s\n",
      "399it [00:49,  8.21it/s]\titers: 400, epoch: 10 | loss: 0.1225904\n",
      "\tspeed: 0.1221s/iter; left time: 254.7392s\n",
      "499it [01:01,  8.19it/s]\titers: 500, epoch: 10 | loss: 0.1116971\n",
      "\tspeed: 0.1221s/iter; left time: 242.4660s\n",
      "599it [01:13,  8.20it/s]\titers: 600, epoch: 10 | loss: 0.1294009\n",
      "\tspeed: 0.1221s/iter; left time: 230.3518s\n",
      "699it [01:25,  8.21it/s]\titers: 700, epoch: 10 | loss: 0.2082151\n",
      "\tspeed: 0.1219s/iter; left time: 217.7904s\n",
      "799it [01:37,  8.20it/s]\titers: 800, epoch: 10 | loss: 0.0319132\n",
      "\tspeed: 0.1218s/iter; left time: 205.4130s\n",
      "899it [01:50,  8.21it/s]\titers: 900, epoch: 10 | loss: 0.0717564\n",
      "\tspeed: 0.1218s/iter; left time: 193.1052s\n",
      "999it [02:02,  8.23it/s]\titers: 1000, epoch: 10 | loss: 0.4598217\n",
      "\tspeed: 0.1215s/iter; left time: 180.5914s\n",
      "1099it [02:14,  8.21it/s]\titers: 1100, epoch: 10 | loss: 0.1037919\n",
      "\tspeed: 0.1216s/iter; left time: 168.5190s\n",
      "1199it [02:26,  8.21it/s]\titers: 1200, epoch: 10 | loss: 0.1214006\n",
      "\tspeed: 0.1218s/iter; left time: 156.6072s\n",
      "1299it [02:38,  8.23it/s]\titers: 1300, epoch: 10 | loss: 0.4775082\n",
      "\tspeed: 0.1217s/iter; left time: 144.3501s\n",
      "1399it [02:50,  8.21it/s]\titers: 1400, epoch: 10 | loss: 0.1349916\n",
      "\tspeed: 0.1216s/iter; left time: 132.0891s\n",
      "1499it [03:03,  8.21it/s]\titers: 1500, epoch: 10 | loss: 0.4721676\n",
      "\tspeed: 0.1218s/iter; left time: 120.1266s\n",
      "1599it [03:15,  8.21it/s]\titers: 1600, epoch: 10 | loss: 1.0540456\n",
      "\tspeed: 0.1218s/iter; left time: 107.9421s\n",
      "1699it [03:27,  8.20it/s]\titers: 1700, epoch: 10 | loss: 0.7649369\n",
      "\tspeed: 0.1220s/iter; left time: 95.8699s\n",
      "1799it [03:39,  8.20it/s]\titers: 1800, epoch: 10 | loss: 0.1513419\n",
      "\tspeed: 0.1219s/iter; left time: 83.6440s\n",
      "1899it [03:51,  8.15it/s]\titers: 1900, epoch: 10 | loss: 0.0952678\n",
      "\tspeed: 0.1220s/iter; left time: 71.4636s\n",
      "1999it [04:04,  8.21it/s]\titers: 2000, epoch: 10 | loss: 0.2950815\n",
      "\tspeed: 0.1220s/iter; left time: 59.2846s\n",
      "2099it [04:16,  8.22it/s]\titers: 2100, epoch: 10 | loss: 0.1258142\n",
      "\tspeed: 0.1218s/iter; left time: 47.0243s\n",
      "2199it [04:28,  8.21it/s]\titers: 2200, epoch: 10 | loss: 0.0369414\n",
      "\tspeed: 0.1219s/iter; left time: 34.8740s\n",
      "2299it [04:40,  8.21it/s]\titers: 2300, epoch: 10 | loss: 0.0723521\n",
      "\tspeed: 0.1219s/iter; left time: 22.6663s\n",
      "2399it [04:52,  8.19it/s]\titers: 2400, epoch: 10 | loss: 0.0998861\n",
      "\tspeed: 0.1218s/iter; left time: 10.4729s\n",
      "2485it [05:03,  8.19it/s]\n",
      "Epoch: 10 cost time: 303.36373019218445\n",
      "833it [00:50, 16.34it/s]\n",
      "833it [00:50, 16.35it/s]\n",
      "Epoch: 10 | Train Loss: 0.3013170 Vali Loss: 0.4272505 Test Loss: 0.3230704 MAE Loss: 0.3691989\n",
      "Updating learning rate to 7.812499999999999e-08\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "train_epochs=10\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-ETTh1'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-10-27 01:29:27,686] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-27 01:29:29,285] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-27 01:29:29,285] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/./Time-LLM/run_main.py\", line 130, in <module>\n",
      "    train_data, train_loader = data_provider(args, 'train')\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/Time-LLM/data_provider/data_factory.py\", line 44, in data_provider\n",
      "    data_set = Data(\n",
      "               ^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/Time-LLM/data_provider/data_loader.py\", line 38, in __init__\n",
      "    self.__read_data__()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/my_work-1/Time-LLM/data_provider/data_loader.py\", line 51, in __read_data__\n",
      "    df_raw = pd.read_csv(os.path.join(self.root_path,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/pandas/io/common.py\", line 856, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './datasets/ETTh1.csv'\n",
      "[2024-10-27 01:29:31,958] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 69584) of binary: /vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1067, in <module>\n",
      "    main()\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1063, in main\n",
      "    launch_command(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "./Time-LLM/run_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-10-27_01:29:31\n",
      "  host      : gruenau8.informatik.hu-berlin.de\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 69584)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "Total time: 0.41490874290466306 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=25\n",
    "learning_rate=0.001\n",
    "llama_layers=12\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_512_96 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24216, 24, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.load('./results/TimeLLM/FR/pred.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8072.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24216/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25851"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8617*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "359*24*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12888, 24, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load('./results/TimeLLM/FR/true.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359.0416666666667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8617/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with parameters </start/>\n",
    "Epoch: 1 | Train Loss: 0.4043049 Vali Loss: 0.3541224 Test Loss: 0.6230705 MAE Loss: 0.4942392\n",
    "\n",
    "train 88947 (70)\n",
    "val 12687 (10)\n",
    "test 25647 (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-10-27 17:01:49,389] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-27 17:01:50,651] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-27 17:01:50,652] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 85587\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-10-27 17:01:53,558] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-27 17:01:54,174] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-27 17:01:54,175] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-27 17:01:54,175] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-27 17:01:54,177] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-27 17:01:54,177] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-27 17:01:54,177] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-27 17:01:54,177] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-27 17:01:54,177] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-27 17:01:54,177] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-27 17:01:54,177] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-27 17:01:54,521] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-27 17:01:54,522] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.58 GB         Max_CA 1 GB \n",
      "[2024-10-27 17:01:54,523] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.26 GB, percent = 6.0%\n",
      "[2024-10-27 17:01:54,675] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-27 17:01:54,676] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.78 GB         Max_CA 1 GB \n",
      "[2024-10-27 17:01:54,676] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.26 GB, percent = 6.0%\n",
      "[2024-10-27 17:01:54,676] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-27 17:01:54,808] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-27 17:01:54,809] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.78 GB         Max_CA 1 GB \n",
      "[2024-10-27 17:01:54,809] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.26 GB, percent = 6.0%\n",
      "[2024-10-27 17:01:54,809] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-27 17:01:54,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-27 17:01:54,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-27 17:01:54,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-27 17:01:54,810] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-27 17:01:54,810] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-27 17:01:54,810] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f49185b26d0>\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-27 17:01:54,811] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-27 17:01:54,812] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-27 17:01:54,813] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1296324\n",
      "\tspeed: 0.1142s/iter; left time: 395.7794s\n",
      "\titers: 200, epoch: 1 | loss: 0.1313216\n",
      "\tspeed: 0.0902s/iter; left time: 303.5537s\n",
      "\titers: 300, epoch: 1 | loss: 0.1258584\n",
      "\tspeed: 0.0908s/iter; left time: 296.7198s\n",
      "\titers: 400, epoch: 1 | loss: 0.1408084\n",
      "\tspeed: 0.0900s/iter; left time: 284.9505s\n",
      "\titers: 500, epoch: 1 | loss: 0.1265340\n",
      "\tspeed: 0.0909s/iter; left time: 278.7197s\n",
      "\titers: 600, epoch: 1 | loss: 0.1437996\n",
      "\tspeed: 0.0907s/iter; left time: 269.2096s\n",
      "\titers: 700, epoch: 1 | loss: 0.1430939\n",
      "\tspeed: 0.0907s/iter; left time: 260.0637s\n",
      "\titers: 800, epoch: 1 | loss: 0.1123035\n",
      "\tspeed: 0.0903s/iter; left time: 249.8467s\n",
      "\titers: 900, epoch: 1 | loss: 0.1291403\n",
      "\tspeed: 0.0907s/iter; left time: 241.8824s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1214111\n",
      "\tspeed: 0.0907s/iter; left time: 232.8006s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1380583\n",
      "\tspeed: 0.0907s/iter; left time: 223.6473s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1301097\n",
      "\tspeed: 0.0914s/iter; left time: 216.2581s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1332750\n",
      "\tspeed: 0.0907s/iter; left time: 205.6143s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1360297\n",
      "\tspeed: 0.0902s/iter; left time: 195.4217s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1369889\n",
      "\tspeed: 0.0907s/iter; left time: 187.4602s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1377524\n",
      "\tspeed: 0.0906s/iter; left time: 178.1807s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1278611\n",
      "\tspeed: 0.0910s/iter; left time: 169.8846s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1550148\n",
      "\tspeed: 0.0911s/iter; left time: 160.9732s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1219339\n",
      "\tspeed: 0.0909s/iter; left time: 151.5309s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1588049\n",
      "\tspeed: 0.0908s/iter; left time: 142.3542s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1039236\n",
      "\tspeed: 0.0914s/iter; left time: 134.0491s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1363883\n",
      "\tspeed: 0.0919s/iter; left time: 125.6193s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1219698\n",
      "\tspeed: 0.0913s/iter; left time: 115.6669s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1488256\n",
      "\tspeed: 0.0915s/iter; left time: 106.8334s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1233720\n",
      "\tspeed: 0.0823s/iter; left time: 87.7784s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1330394\n",
      "\tspeed: 0.0719s/iter; left time: 69.4985s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1416024\n",
      "\tspeed: 0.0831s/iter; left time: 72.0561s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1381458\n",
      "\tspeed: 0.0912s/iter; left time: 69.9146s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1213863\n",
      "\tspeed: 0.0908s/iter; left time: 60.5691s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1501067\n",
      "\tspeed: 0.0909s/iter; left time: 51.5156s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1440757\n",
      "\tspeed: 0.0904s/iter; left time: 42.2141s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1414534\n",
      "\tspeed: 0.0907s/iter; left time: 33.2746s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1520287\n",
      "\tspeed: 0.0907s/iter; left time: 24.2121s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1408513\n",
      "\tspeed: 0.0909s/iter; left time: 15.1806s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1207491\n",
      "\tspeed: 0.0911s/iter; left time: 6.1032s\n",
      "Epoch: 1 cost time: 00h:05m:21.56s\n",
      "Epoch: 1 | Train Loss: 0.1323008 Vali Loss: 0.1571093 Test Loss: 0.1764676\n",
      "Validation loss decreased (inf --> 0.157109).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "Scaled mse:0.05510799586772919, rmse:0.2347509264945984, mae:0.176467627286911, rse:0.9081517457962036\n",
      "success delete checkpoints\n",
      "Total time: 7.804715522130331 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=1\n",
    "learning_rate=0.001\n",
    "llama_layers=6\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id FR_96_24 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 88899 (70)\n",
    "# val 25707 (20)\n",
    "# test 12675 (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-10-27 02:20:10,495] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-27 02:20:12,640] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-27 02:20:12,640] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 85803\n",
      "val 18651\n",
      "test 18651\n",
      "Hourly data detailing load (electricity consumption), solar generation, and wind generation. These metrics are crucial in the electric power demand planning. \n",
      "[2024-10-27 02:20:16,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-27 02:20:17,993] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-10-27 02:20:17,995] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-10-27 02:20:17,995] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-27 02:20:17,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-10-27 02:20:17,996] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-10-27 02:20:17,996] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-10-27 02:20:17,996] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-10-27 02:20:17,997] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-10-27 02:20:17,997] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-10-27 02:20:17,997] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-27 02:20:18,491] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-27 02:20:18,492] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.56 GB         CA 0.57 GB         Max_CA 1 GB \n",
      "[2024-10-27 02:20:18,492] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 633.12 GB, percent = 62.8%\n",
      "[2024-10-27 02:20:18,719] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-27 02:20:18,720] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.66 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-10-27 02:20:18,721] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 633.12 GB, percent = 62.8%\n",
      "[2024-10-27 02:20:18,721] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-10-27 02:20:18,927] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-27 02:20:18,928] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.77 GB         Max_CA 1 GB \n",
      "[2024-10-27 02:20:18,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 633.12 GB, percent = 62.8%\n",
      "[2024-10-27 02:20:18,929] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-10-27 02:20:18,929] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-10-27 02:20:18,929] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-10-27 02:20:18,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-10-27 02:20:18,930] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-10-27 02:20:18,930] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff5a0400250>\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-27 02:20:18,931] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-10-27 02:20:18,932] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-10-27 02:20:18,933] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-10-27 02:20:18,933] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-27 02:20:18,933] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-10-27 02:20:18,933] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-27 02:20:18,933] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-27 02:20:18,937] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-27 02:20:18,938] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1309357\n",
      "\tspeed: 0.1725s/iter; left time: 599.4874s\n",
      "\titers: 200, epoch: 1 | loss: 0.1261889\n",
      "\tspeed: 0.1283s/iter; left time: 433.0936s\n",
      "\titers: 300, epoch: 1 | loss: 0.1278532\n",
      "\tspeed: 0.1235s/iter; left time: 404.6992s\n",
      "\titers: 400, epoch: 1 | loss: 0.1275618\n",
      "\tspeed: 0.1305s/iter; left time: 414.4558s\n",
      "\titers: 500, epoch: 1 | loss: 0.1467155\n",
      "\tspeed: 0.1269s/iter; left time: 390.2956s\n",
      "\titers: 600, epoch: 1 | loss: 0.1383055\n",
      "\tspeed: 0.1317s/iter; left time: 392.0371s\n",
      "\titers: 700, epoch: 1 | loss: 0.1491343\n",
      "\tspeed: 0.1236s/iter; left time: 355.5160s\n",
      "\titers: 800, epoch: 1 | loss: 0.1125599\n",
      "\tspeed: 0.1253s/iter; left time: 347.8690s\n",
      "\titers: 900, epoch: 1 | loss: 0.1253058\n",
      "\tspeed: 0.1246s/iter; left time: 333.3619s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1493505\n",
      "\tspeed: 0.1228s/iter; left time: 316.2685s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1460379\n",
      "\tspeed: 0.1246s/iter; left time: 308.6262s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1452256\n",
      "\tspeed: 0.1265s/iter; left time: 300.6324s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1289709\n",
      "\tspeed: 0.1229s/iter; left time: 279.6392s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1472139\n",
      "\tspeed: 0.1227s/iter; left time: 266.9473s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1747697\n",
      "\tspeed: 0.1241s/iter; left time: 257.5587s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1106814\n",
      "\tspeed: 0.1265s/iter; left time: 249.9130s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1252436\n",
      "\tspeed: 0.1320s/iter; left time: 247.6340s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1734609\n",
      "\tspeed: 0.1230s/iter; left time: 218.4027s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1315671\n",
      "\tspeed: 0.1225s/iter; left time: 205.2585s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0976583\n",
      "\tspeed: 0.1325s/iter; left time: 208.8949s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1237694\n",
      "\tspeed: 0.1278s/iter; left time: 188.5654s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1141509\n",
      "\tspeed: 0.1261s/iter; left time: 173.5021s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1103798\n",
      "\tspeed: 0.1276s/iter; left time: 162.7712s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1091401\n",
      "\tspeed: 0.1107s/iter; left time: 130.1801s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1145288\n",
      "\tspeed: 0.0917s/iter; left time: 98.6867s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1295653\n",
      "\tspeed: 0.0927s/iter; left time: 90.4428s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1116448\n",
      "\tspeed: 0.0922s/iter; left time: 80.7550s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1495020\n",
      "\tspeed: 0.0947s/iter; left time: 73.4939s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1228959\n",
      "\tspeed: 0.1018s/iter; left time: 68.8261s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1202834\n",
      "\tspeed: 0.1008s/iter; left time: 58.0539s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1153658\n",
      "\tspeed: 0.1052s/iter; left time: 50.0666s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1517936\n",
      "\tspeed: 0.1056s/iter; left time: 39.7056s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1132783\n",
      "\tspeed: 0.1027s/iter; left time: 28.3424s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1168138\n",
      "\tspeed: 0.1011s/iter; left time: 17.8013s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1161294\n",
      "\tspeed: 0.1024s/iter; left time: 7.7857s\n",
      "Epoch: 1 cost time: 420.39592361450195\n",
      "Epoch: 1 | Train Loss: 0.1305287 Vali Loss: 0.1549382 Test Loss: 0.1748424 MAE Loss: 0.1748424\n",
      "Validation loss decreased (inf --> 0.154938).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "loading model...\n",
      "mse:0.17484241711548704, mae:0.17484241711548704\n",
      "train_losses [0.13052865110702447]\n",
      "val_losses [0.15493824075010906]\n",
      "success delete checkpoints\n",
      "Total time: 9.785460531711578 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=1\n",
    "learning_rate=0.001\n",
    "llama_layers=6 # 626 sec on 1 epoch # 13 min\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id FR_96_24 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")\n",
    "# train 88899 # 89115\n",
    "# val 25707 # 25923\n",
    "# test 12675 # 12891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28628486 0.350173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "trues = np.load('./results/TimeLLM/FR/true.npy')[:, :, 0] # \"flatten\"\n",
    "preds = np.load('./results/TimeLLM/FR/pred.npy')[:, :, 0]\n",
    "\n",
    "mse = mean_squared_error(preds, trues)\n",
    "mae = mean_absolute_error(preds, trues)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4296.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12888/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12891/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12888 - 4297*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `python -m accelerate.commands.launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-05-01 23:41:28,579] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-01 23:41:29,614] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-01 23:41:29,614] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 89115\n",
      "val 25923\n",
      "test 12891\n",
      "[2024-05-01 23:41:31,629] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-01 23:41:32,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-01 23:41:32,267] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-01 23:41:32,267] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-01 23:41:32,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-05-01 23:41:32,268] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-05-01 23:41:32,268] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-05-01 23:41:32,268] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-05-01 23:41:32,268] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-05-01 23:41:32,268] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-05-01 23:41:32,268] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-05-01 23:41:32,490] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-01 23:41:32,490] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.65 GB         CA 0.66 GB         Max_CA 1 GB \n",
      "[2024-05-01 23:41:32,491] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.59 GB, percent = 6.3%\n",
      "[2024-05-01 23:41:32,595] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-01 23:41:32,596] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.74 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-01 23:41:32,596] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.59 GB, percent = 6.3%\n",
      "[2024-05-01 23:41:32,596] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-05-01 23:41:32,694] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-01 23:41:32,695] [INFO] [utils.py:801:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2024-05-01 23:41:32,695] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.59 GB, percent = 6.3%\n",
      "[2024-05-01 23:41:32,696] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-05-01 23:41:32,696] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-01 23:41:32,696] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-01 23:41:32,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-01 23:41:32,696] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f90996b50>\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   train_batch_size ............. 24\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24\n",
      "[2024-05-01 23:41:32,697] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-05-01 23:41:32,698] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:28,  3.54it/s]\titers: 100, epoch: 1 | loss: 0.8372728\n",
      "\tspeed: 0.3004s/iter; left time: 22280.7895s\n",
      "199it [00:57,  3.55it/s]\titers: 200, epoch: 1 | loss: 0.5694848\n",
      "\tspeed: 0.2823s/iter; left time: 20907.1646s\n",
      "299it [01:25,  3.55it/s]\titers: 300, epoch: 1 | loss: 0.9272981\n",
      "\tspeed: 0.2819s/iter; left time: 20846.9785s\n",
      "399it [01:53,  3.54it/s]\titers: 400, epoch: 1 | loss: 0.3950497\n",
      "\tspeed: 0.2821s/iter; left time: 20837.2380s\n",
      "499it [02:21,  3.54it/s]\titers: 500, epoch: 1 | loss: 0.3553948\n",
      "\tspeed: 0.2821s/iter; left time: 20807.5541s\n",
      "599it [02:49,  3.54it/s]\titers: 600, epoch: 1 | loss: 0.4466974\n",
      "\tspeed: 0.2826s/iter; left time: 20814.9448s\n",
      "699it [03:18,  3.54it/s]\titers: 700, epoch: 1 | loss: 0.3993747\n",
      "\tspeed: 0.2823s/iter; left time: 20764.7604s\n",
      "799it [03:46,  3.54it/s]\titers: 800, epoch: 1 | loss: 0.2617724\n",
      "\tspeed: 0.2824s/iter; left time: 20745.9143s\n",
      "899it [04:14,  3.55it/s]\titers: 900, epoch: 1 | loss: 0.1327625\n",
      "\tspeed: 0.2822s/iter; left time: 20702.4392s\n",
      "999it [04:42,  3.54it/s]\titers: 1000, epoch: 1 | loss: 0.5376595\n",
      "\tspeed: 0.2823s/iter; left time: 20683.3815s\n",
      "1099it [05:11,  3.54it/s]\titers: 1100, epoch: 1 | loss: 0.3515632\n",
      "\tspeed: 0.2820s/iter; left time: 20634.4854s\n",
      "1199it [05:39,  3.54it/s]\titers: 1200, epoch: 1 | loss: 0.3356682\n",
      "\tspeed: 0.2821s/iter; left time: 20610.1132s\n",
      "1299it [06:07,  3.54it/s]\titers: 1300, epoch: 1 | loss: 0.2041691\n",
      "\tspeed: 0.2823s/iter; left time: 20593.5499s\n",
      "1399it [06:35,  3.54it/s]\titers: 1400, epoch: 1 | loss: 0.2808107\n",
      "\tspeed: 0.2825s/iter; left time: 20583.1370s\n",
      "1499it [07:03,  3.54it/s]\titers: 1500, epoch: 1 | loss: 0.1883581\n",
      "\tspeed: 0.2822s/iter; left time: 20533.3839s\n",
      "1599it [07:32,  3.55it/s]\titers: 1600, epoch: 1 | loss: 0.1842742\n",
      "\tspeed: 0.2822s/iter; left time: 20504.0071s\n",
      "1699it [08:00,  3.55it/s]\titers: 1700, epoch: 1 | loss: 0.3881975\n",
      "\tspeed: 0.2820s/iter; left time: 20463.3341s\n",
      "1799it [08:28,  3.55it/s]\titers: 1800, epoch: 1 | loss: 0.5240591\n",
      "\tspeed: 0.2820s/iter; left time: 20437.4655s\n",
      "1899it [08:56,  3.45it/s]\titers: 1900, epoch: 1 | loss: 0.2318575\n",
      "\tspeed: 0.2826s/iter; left time: 20447.9138s\n",
      "1999it [09:25,  3.54it/s]\titers: 2000, epoch: 1 | loss: 0.1730266\n",
      "\tspeed: 0.2821s/iter; left time: 20382.6191s\n",
      "2099it [09:53,  3.55it/s]\titers: 2100, epoch: 1 | loss: 0.6713259\n",
      "\tspeed: 0.2821s/iter; left time: 20357.7837s\n",
      "2199it [10:21,  3.54it/s]\titers: 2200, epoch: 1 | loss: 0.1942842\n",
      "\tspeed: 0.2821s/iter; left time: 20331.7386s\n",
      "2299it [10:49,  3.54it/s]\titers: 2300, epoch: 1 | loss: 0.1913415\n",
      "\tspeed: 0.2822s/iter; left time: 20305.1517s\n",
      "2399it [11:17,  3.55it/s]\titers: 2400, epoch: 1 | loss: 0.2446006\n",
      "\tspeed: 0.2820s/iter; left time: 20262.2917s\n",
      "2499it [11:46,  3.54it/s]\titers: 2500, epoch: 1 | loss: 0.2016802\n",
      "\tspeed: 0.2822s/iter; left time: 20251.3642s\n",
      "2599it [12:14,  3.54it/s]\titers: 2600, epoch: 1 | loss: 0.1812585\n",
      "\tspeed: 0.2822s/iter; left time: 20224.0464s\n",
      "2699it [12:42,  3.54it/s]\titers: 2700, epoch: 1 | loss: 0.2225717\n",
      "\tspeed: 0.2825s/iter; left time: 20213.8879s\n",
      "2799it [13:10,  3.55it/s]\titers: 2800, epoch: 1 | loss: 0.1919975\n",
      "\tspeed: 0.2823s/iter; left time: 20171.0213s\n",
      "2899it [13:39,  3.55it/s]\titers: 2900, epoch: 1 | loss: 0.3082767\n",
      "\tspeed: 0.2823s/iter; left time: 20141.7929s\n",
      "2999it [14:07,  3.55it/s]\titers: 3000, epoch: 1 | loss: 0.2223556\n",
      "\tspeed: 0.2821s/iter; left time: 20104.4886s\n",
      "3099it [14:35,  3.53it/s]\titers: 3100, epoch: 1 | loss: 0.2487132\n",
      "\tspeed: 0.2824s/iter; left time: 20097.1842s\n",
      "3199it [15:03,  3.54it/s]\titers: 3200, epoch: 1 | loss: 0.2680546\n",
      "\tspeed: 0.2822s/iter; left time: 20055.3284s\n",
      "3299it [15:31,  3.55it/s]\titers: 3300, epoch: 1 | loss: 0.4428895\n",
      "\tspeed: 0.2823s/iter; left time: 20034.9379s\n",
      "3399it [16:00,  3.54it/s]\titers: 3400, epoch: 1 | loss: 0.2900494\n",
      "\tspeed: 0.2821s/iter; left time: 19992.0327s\n",
      "3499it [16:28,  3.54it/s]\titers: 3500, epoch: 1 | loss: 0.2503504\n",
      "\tspeed: 0.2824s/iter; left time: 19982.3153s\n",
      "3599it [16:56,  3.55it/s]\titers: 3600, epoch: 1 | loss: 0.2486453\n",
      "\tspeed: 0.2822s/iter; left time: 19940.3713s\n",
      "3699it [17:24,  3.54it/s]\titers: 3700, epoch: 1 | loss: 0.3809845\n",
      "\tspeed: 0.2822s/iter; left time: 19915.6790s\n",
      "3713it [17:28,  3.54it/s]\n",
      "Epoch: 1 cost time: 1048.8337075710297\n",
      "1080it [02:29,  7.21it/s]\n",
      "537it [01:14,  7.18it/s]\n",
      "Epoch: 1 | Train Loss: 0.3419935 Vali Loss: 0.3678522 Test Loss: 0.2862849 MAE Loss: 0.3501730\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n",
      "99it [00:28,  3.58it/s]\titers: 100, epoch: 2 | loss: 0.3549825\n",
      "\tspeed: 2.5882s/iter; left time: 182335.9293s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 2 | loss: 0.1937646\n",
      "\tspeed: 0.2790s/iter; left time: 19628.7205s\n",
      "299it [01:23,  3.58it/s]\titers: 300, epoch: 2 | loss: 0.3930861\n",
      "\tspeed: 0.2792s/iter; left time: 19614.4055s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 2 | loss: 0.0808896\n",
      "\tspeed: 0.2787s/iter; left time: 19551.1930s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 2 | loss: 0.2329840\n",
      "\tspeed: 0.2793s/iter; left time: 19564.9621s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 2 | loss: 0.3216129\n",
      "\tspeed: 0.2788s/iter; left time: 19499.0985s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 2 | loss: 0.3624127\n",
      "\tspeed: 0.2788s/iter; left time: 19470.9115s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 2 | loss: 0.3310800\n",
      "\tspeed: 0.2786s/iter; left time: 19431.6236s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 2 | loss: 0.2271818\n",
      "\tspeed: 0.2787s/iter; left time: 19411.7746s\n",
      "999it [04:39,  3.58it/s]\titers: 1000, epoch: 2 | loss: 0.2500638\n",
      "\tspeed: 0.2789s/iter; left time: 19394.8855s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 2 | loss: 0.1288883\n",
      "\tspeed: 0.2787s/iter; left time: 19355.2340s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 2 | loss: 0.2611633\n",
      "\tspeed: 0.2788s/iter; left time: 19333.4145s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 2 | loss: 0.1710465\n",
      "\tspeed: 0.2788s/iter; left time: 19304.4917s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 2 | loss: 0.2686654\n",
      "\tspeed: 0.2788s/iter; left time: 19278.5060s\n",
      "1499it [06:58,  3.58it/s]\titers: 1500, epoch: 2 | loss: 0.1587942\n",
      "\tspeed: 0.2787s/iter; left time: 19241.0934s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 2 | loss: 0.1829133\n",
      "\tspeed: 0.2787s/iter; left time: 19213.6953s\n",
      "1699it [07:54,  3.60it/s]\titers: 1700, epoch: 2 | loss: 0.2722591\n",
      "\tspeed: 0.2786s/iter; left time: 19183.9995s\n",
      "1799it [08:22,  3.59it/s]\titers: 1800, epoch: 2 | loss: 0.2882463\n",
      "\tspeed: 0.2788s/iter; left time: 19164.7319s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 2 | loss: 0.2889234\n",
      "\tspeed: 0.2786s/iter; left time: 19124.1609s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 2 | loss: 0.2084705\n",
      "\tspeed: 0.2785s/iter; left time: 19090.2296s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 2 | loss: 0.4306182\n",
      "\tspeed: 0.2785s/iter; left time: 19065.5786s\n",
      "2199it [10:13,  3.58it/s]\titers: 2200, epoch: 2 | loss: 0.2334243\n",
      "\tspeed: 0.2787s/iter; left time: 19049.2853s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 2 | loss: 0.2733656\n",
      "\tspeed: 0.2786s/iter; left time: 19017.1996s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 2 | loss: 0.0822662\n",
      "\tspeed: 0.2786s/iter; left time: 18986.7072s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 2 | loss: 0.3006004\n",
      "\tspeed: 0.2789s/iter; left time: 18977.7646s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 2 | loss: 0.3249759\n",
      "\tspeed: 0.2785s/iter; left time: 18926.0046s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 2 | loss: 0.3501224\n",
      "\tspeed: 0.2786s/iter; left time: 18905.6259s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 2 | loss: 0.2116295\n",
      "\tspeed: 0.2786s/iter; left time: 18876.5704s\n",
      "2899it [13:28,  3.58it/s]\titers: 2900, epoch: 2 | loss: 0.3304249\n",
      "\tspeed: 0.2789s/iter; left time: 18867.6795s\n",
      "2999it [13:56,  3.58it/s]\titers: 3000, epoch: 2 | loss: 0.3002547\n",
      "\tspeed: 0.2788s/iter; left time: 18829.9861s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 2 | loss: 0.2019060\n",
      "\tspeed: 0.2789s/iter; left time: 18807.9253s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 2 | loss: 0.1559609\n",
      "\tspeed: 0.2788s/iter; left time: 18775.3604s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 2 | loss: 0.2160097\n",
      "\tspeed: 0.2786s/iter; left time: 18738.4583s\n",
      "3399it [15:47,  3.58it/s]\titers: 3400, epoch: 2 | loss: 0.2168685\n",
      "\tspeed: 0.2788s/iter; left time: 18717.6081s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 2 | loss: 0.2281263\n",
      "\tspeed: 0.2786s/iter; left time: 18682.2278s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 2 | loss: 0.3565611\n",
      "\tspeed: 0.2786s/iter; left time: 18654.1446s\n",
      "3699it [17:11,  3.58it/s]\titers: 3700, epoch: 2 | loss: 0.1732474\n",
      "\tspeed: 0.2788s/iter; left time: 18635.1149s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 2 cost time: 1035.4833707809448\n",
      "1080it [02:26,  7.39it/s]\n",
      "537it [01:12,  7.37it/s]\n",
      "Epoch: 2 | Train Loss: 0.2512919 Vali Loss: 0.3599760 Test Loss: 0.2960782 MAE Loss: 0.3595319\n",
      "Updating learning rate to 1.9999999999999998e-05\n",
      "99it [00:28,  3.57it/s]\titers: 100, epoch: 3 | loss: 0.1896469\n",
      "\tspeed: 2.5362s/iter; left time: 169254.0062s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 3 | loss: 0.1609817\n",
      "\tspeed: 0.2793s/iter; left time: 18608.9832s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 3 | loss: 0.1808864\n",
      "\tspeed: 0.2788s/iter; left time: 18551.3983s\n",
      "399it [01:51,  3.58it/s]\titers: 400, epoch: 3 | loss: 0.1234762\n",
      "\tspeed: 0.2787s/iter; left time: 18515.5309s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 3 | loss: 0.2353608\n",
      "\tspeed: 0.2788s/iter; left time: 18495.1412s\n",
      "599it [02:47,  3.58it/s]\titers: 600, epoch: 3 | loss: 0.3108649\n",
      "\tspeed: 0.2788s/iter; left time: 18465.2460s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 3 | loss: 0.2086529\n",
      "\tspeed: 0.2788s/iter; left time: 18435.7339s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 3 | loss: 0.3434478\n",
      "\tspeed: 0.2790s/iter; left time: 18424.2331s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 3 | loss: 0.1913136\n",
      "\tspeed: 0.2789s/iter; left time: 18392.2167s\n",
      "999it [04:39,  3.58it/s]\titers: 1000, epoch: 3 | loss: 0.2114794\n",
      "\tspeed: 0.2794s/iter; left time: 18396.2540s\n",
      "1099it [05:07,  3.58it/s]\titers: 1100, epoch: 3 | loss: 0.1896306\n",
      "\tspeed: 0.2793s/iter; left time: 18358.0727s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 3 | loss: 0.1856081\n",
      "\tspeed: 0.2792s/iter; left time: 18326.3893s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 3 | loss: 0.2099121\n",
      "\tspeed: 0.2790s/iter; left time: 18281.9203s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 3 | loss: 0.1859097\n",
      "\tspeed: 0.2785s/iter; left time: 18224.7426s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 3 | loss: 0.2277904\n",
      "\tspeed: 0.2784s/iter; left time: 18186.7201s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 3 | loss: 0.4765970\n",
      "\tspeed: 0.2787s/iter; left time: 18179.3508s\n",
      "1699it [07:54,  3.58it/s]\titers: 1700, epoch: 3 | loss: 0.2220151\n",
      "\tspeed: 0.2786s/iter; left time: 18149.2943s\n",
      "1799it [08:22,  3.58it/s]\titers: 1800, epoch: 3 | loss: 0.2253435\n",
      "\tspeed: 0.2787s/iter; left time: 18123.7267s\n",
      "1899it [08:50,  3.59it/s]\titers: 1900, epoch: 3 | loss: 0.1961520\n",
      "\tspeed: 0.2797s/iter; left time: 18164.5461s\n",
      "1999it [09:18,  3.59it/s]\titers: 2000, epoch: 3 | loss: 0.1662754\n",
      "\tspeed: 0.2792s/iter; left time: 18101.4229s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 3 | loss: 0.3240711\n",
      "\tspeed: 0.2787s/iter; left time: 18044.5931s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 3 | loss: 0.3598391\n",
      "\tspeed: 0.2786s/iter; left time: 18008.8157s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 3 | loss: 0.1720749\n",
      "\tspeed: 0.2795s/iter; left time: 18036.2619s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 3 | loss: 0.1745865\n",
      "\tspeed: 0.2786s/iter; left time: 17954.5447s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 3 | loss: 0.1891560\n",
      "\tspeed: 0.2790s/iter; left time: 17951.6062s\n",
      "2599it [12:05,  3.59it/s]\titers: 2600, epoch: 3 | loss: 0.1243603\n",
      "\tspeed: 0.2788s/iter; left time: 17911.6100s\n",
      "2699it [12:33,  3.58it/s]\titers: 2700, epoch: 3 | loss: 0.3362224\n",
      "\tspeed: 0.2790s/iter; left time: 17894.1100s\n",
      "2799it [13:01,  3.59it/s]\titers: 2800, epoch: 3 | loss: 0.1963779\n",
      "\tspeed: 0.2788s/iter; left time: 17851.4904s\n",
      "2899it [13:29,  3.58it/s]\titers: 2900, epoch: 3 | loss: 0.2677196\n",
      "\tspeed: 0.2788s/iter; left time: 17827.3109s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 3 | loss: 0.2951477\n",
      "\tspeed: 0.2789s/iter; left time: 17804.7276s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 3 | loss: 0.3259568\n",
      "\tspeed: 0.2787s/iter; left time: 17765.1417s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 3 | loss: 0.1546946\n",
      "\tspeed: 0.2787s/iter; left time: 17735.2154s\n",
      "3299it [15:20,  3.58it/s]\titers: 3300, epoch: 3 | loss: 0.3689613\n",
      "\tspeed: 0.2788s/iter; left time: 17712.8972s\n",
      "3399it [15:48,  3.59it/s]\titers: 3400, epoch: 3 | loss: 0.4187831\n",
      "\tspeed: 0.2786s/iter; left time: 17670.7638s\n",
      "3499it [16:16,  3.59it/s]\titers: 3500, epoch: 3 | loss: 0.2477571\n",
      "\tspeed: 0.2786s/iter; left time: 17647.8215s\n",
      "3599it [16:44,  3.58it/s]\titers: 3600, epoch: 3 | loss: 0.2105096\n",
      "\tspeed: 0.2788s/iter; left time: 17626.9705s\n",
      "3699it [17:12,  3.59it/s]\titers: 3700, epoch: 3 | loss: 0.3255820\n",
      "\tspeed: 0.2789s/iter; left time: 17606.0181s\n",
      "3713it [17:16,  3.58it/s]\n",
      "Epoch: 3 cost time: 1036.0329954624176\n",
      "1080it [02:26,  7.39it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 3 | Train Loss: 0.2348192 Vali Loss: 0.3357609 Test Loss: 0.2749382 MAE Loss: 0.3293124\n",
      "Updating learning rate to 9.999999999999999e-06\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 4 | loss: 0.2562529\n",
      "\tspeed: 2.5297s/iter; left time: 159426.8458s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 4 | loss: 0.3790242\n",
      "\tspeed: 0.2789s/iter; left time: 17547.9784s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 4 | loss: 0.2625185\n",
      "\tspeed: 0.2790s/iter; left time: 17527.3986s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 4 | loss: 0.2026810\n",
      "\tspeed: 0.2789s/iter; left time: 17492.3652s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 4 | loss: 0.1990536\n",
      "\tspeed: 0.2788s/iter; left time: 17455.9845s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 4 | loss: 0.2912924\n",
      "\tspeed: 0.2790s/iter; left time: 17441.2688s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 4 | loss: 0.1408612\n",
      "\tspeed: 0.2785s/iter; left time: 17387.4060s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 4 | loss: 0.2809342\n",
      "\tspeed: 0.2787s/iter; left time: 17366.6006s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 4 | loss: 0.1805010\n",
      "\tspeed: 0.2786s/iter; left time: 17333.1205s\n",
      "999it [04:38,  3.58it/s]\titers: 1000, epoch: 4 | loss: 0.1410739\n",
      "\tspeed: 0.2786s/iter; left time: 17310.2082s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 4 | loss: 0.2367221\n",
      "\tspeed: 0.2785s/iter; left time: 17273.3946s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 4 | loss: 0.2652491\n",
      "\tspeed: 0.2787s/iter; left time: 17258.8341s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 4 | loss: 0.2226712\n",
      "\tspeed: 0.2786s/iter; left time: 17226.0304s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 4 | loss: 0.2239288\n",
      "\tspeed: 0.2788s/iter; left time: 17206.5390s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 4 | loss: 0.1873870\n",
      "\tspeed: 0.2788s/iter; left time: 17178.7749s\n",
      "1599it [07:26,  3.60it/s]\titers: 1600, epoch: 4 | loss: 0.1162323\n",
      "\tspeed: 0.2789s/iter; left time: 17158.1104s\n",
      "1699it [07:54,  3.59it/s]\titers: 1700, epoch: 4 | loss: 0.2408895\n",
      "\tspeed: 0.2787s/iter; left time: 17121.3214s\n",
      "1799it [08:21,  3.58it/s]\titers: 1800, epoch: 4 | loss: 0.2344150\n",
      "\tspeed: 0.2788s/iter; left time: 17099.4314s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 4 | loss: 0.1467646\n",
      "\tspeed: 0.2786s/iter; left time: 17059.2658s\n",
      "1999it [09:17,  3.58it/s]\titers: 2000, epoch: 4 | loss: 0.1649810\n",
      "\tspeed: 0.2788s/iter; left time: 17037.9366s\n",
      "2099it [09:45,  3.58it/s]\titers: 2100, epoch: 4 | loss: 0.1447232\n",
      "\tspeed: 0.2792s/iter; left time: 17035.4188s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 4 | loss: 0.4814666\n",
      "\tspeed: 0.2788s/iter; left time: 16987.6390s\n",
      "2299it [10:41,  3.58it/s]\titers: 2300, epoch: 4 | loss: 0.2779878\n",
      "\tspeed: 0.2787s/iter; left time: 16951.6248s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 4 | loss: 0.2569937\n",
      "\tspeed: 0.2787s/iter; left time: 16920.3632s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 4 | loss: 0.1604644\n",
      "\tspeed: 0.2786s/iter; left time: 16891.1179s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 4 | loss: 0.3601210\n",
      "\tspeed: 0.2788s/iter; left time: 16873.1823s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 4 | loss: 0.3170389\n",
      "\tspeed: 0.2787s/iter; left time: 16838.5184s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 4 | loss: 0.1155664\n",
      "\tspeed: 0.2787s/iter; left time: 16812.4377s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 4 | loss: 0.2565590\n",
      "\tspeed: 0.2786s/iter; left time: 16779.9196s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 4 | loss: 0.2762398\n",
      "\tspeed: 0.2786s/iter; left time: 16749.2645s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 4 | loss: 0.1302785\n",
      "\tspeed: 0.2785s/iter; left time: 16718.9950s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 4 | loss: 0.2635685\n",
      "\tspeed: 0.2786s/iter; left time: 16695.4818s\n",
      "3299it [15:19,  3.58it/s]\titers: 3300, epoch: 4 | loss: 0.1349494\n",
      "\tspeed: 0.2786s/iter; left time: 16668.3062s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 4 | loss: 0.2592045\n",
      "\tspeed: 0.2787s/iter; left time: 16644.5144s\n",
      "3499it [16:15,  3.58it/s]\titers: 3500, epoch: 4 | loss: 0.1583803\n",
      "\tspeed: 0.2787s/iter; left time: 16615.4904s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 4 | loss: 0.2014253\n",
      "\tspeed: 0.2789s/iter; left time: 16598.2079s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 4 | loss: 0.2643668\n",
      "\tspeed: 0.2788s/iter; left time: 16569.1733s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 4 cost time: 1035.451132774353\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.38it/s]\n",
      "Epoch: 4 | Train Loss: 0.2261532 Vali Loss: 0.3304109 Test Loss: 0.2650561 MAE Loss: 0.3207039\n",
      "Updating learning rate to 4.9999999999999996e-06\n",
      "99it [00:28,  3.60it/s]\titers: 100, epoch: 5 | loss: 0.1790921\n",
      "\tspeed: 2.5682s/iter; left time: 152319.3024s\n",
      "199it [00:55,  3.61it/s]\titers: 200, epoch: 5 | loss: 0.2022568\n",
      "\tspeed: 0.2776s/iter; left time: 16435.7922s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 5 | loss: 0.2110650\n",
      "\tspeed: 0.2786s/iter; left time: 16464.8854s\n",
      "399it [01:51,  3.56it/s]\titers: 400, epoch: 5 | loss: 0.1761048\n",
      "\tspeed: 0.2787s/iter; left time: 16443.0227s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 5 | loss: 0.2127403\n",
      "\tspeed: 0.2788s/iter; left time: 16426.5786s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 5 | loss: 0.2889642\n",
      "\tspeed: 0.2788s/iter; left time: 16395.2664s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 5 | loss: 0.1875397\n",
      "\tspeed: 0.2786s/iter; left time: 16354.7753s\n",
      "799it [03:42,  3.59it/s]\titers: 800, epoch: 5 | loss: 0.2217002\n",
      "\tspeed: 0.2787s/iter; left time: 16335.6656s\n",
      "899it [04:10,  3.59it/s]\titers: 900, epoch: 5 | loss: 0.1667274\n",
      "\tspeed: 0.2785s/iter; left time: 16294.1116s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 5 | loss: 0.1821665\n",
      "\tspeed: 0.2789s/iter; left time: 16287.6989s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 5 | loss: 0.2658961\n",
      "\tspeed: 0.2788s/iter; left time: 16255.6659s\n",
      "1199it [05:34,  3.60it/s]\titers: 1200, epoch: 5 | loss: 0.2094066\n",
      "\tspeed: 0.2784s/iter; left time: 16203.0912s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 5 | loss: 0.2052930\n",
      "\tspeed: 0.2786s/iter; left time: 16190.4365s\n",
      "1399it [06:30,  3.58it/s]\titers: 1400, epoch: 5 | loss: 0.1892228\n",
      "\tspeed: 0.2788s/iter; left time: 16175.4809s\n",
      "1499it [06:58,  3.58it/s]\titers: 1500, epoch: 5 | loss: 0.3725181\n",
      "\tspeed: 0.2791s/iter; left time: 16162.5952s\n",
      "1599it [07:25,  3.59it/s]\titers: 1600, epoch: 5 | loss: 0.2245832\n",
      "\tspeed: 0.2789s/iter; left time: 16121.5533s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 5 | loss: 0.1567070\n",
      "\tspeed: 0.2789s/iter; left time: 16092.5140s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 5 | loss: 0.1871729\n",
      "\tspeed: 0.2788s/iter; left time: 16058.8340s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 5 | loss: 0.2168327\n",
      "\tspeed: 0.2786s/iter; left time: 16022.5717s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 5 | loss: 0.2115944\n",
      "\tspeed: 0.2788s/iter; left time: 16003.5582s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 5 | loss: 0.2259027\n",
      "\tspeed: 0.2788s/iter; left time: 15978.7116s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 5 | loss: 0.1937068\n",
      "\tspeed: 0.2787s/iter; left time: 15945.6562s\n",
      "2299it [10:41,  3.58it/s]\titers: 2300, epoch: 5 | loss: 0.1766269\n",
      "\tspeed: 0.2788s/iter; left time: 15920.0905s\n",
      "2399it [11:08,  3.58it/s]\titers: 2400, epoch: 5 | loss: 0.1982124\n",
      "\tspeed: 0.2789s/iter; left time: 15899.8607s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 5 | loss: 0.3578601\n",
      "\tspeed: 0.2787s/iter; left time: 15860.4312s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 5 | loss: 0.1957287\n",
      "\tspeed: 0.2788s/iter; left time: 15839.9405s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 5 | loss: 0.3024182\n",
      "\tspeed: 0.2788s/iter; left time: 15810.2796s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 5 | loss: 0.1789113\n",
      "\tspeed: 0.2787s/iter; left time: 15775.0720s\n",
      "2899it [13:28,  3.58it/s]\titers: 2900, epoch: 5 | loss: 0.1382693\n",
      "\tspeed: 0.2791s/iter; left time: 15768.9705s\n",
      "2999it [13:56,  3.58it/s]\titers: 3000, epoch: 5 | loss: 0.4399035\n",
      "\tspeed: 0.2789s/iter; left time: 15730.0605s\n",
      "3099it [14:24,  3.58it/s]\titers: 3100, epoch: 5 | loss: 0.1720625\n",
      "\tspeed: 0.2789s/iter; left time: 15704.2469s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 5 | loss: 0.4884131\n",
      "\tspeed: 0.2786s/iter; left time: 15660.2967s\n",
      "3299it [15:19,  3.58it/s]\titers: 3300, epoch: 5 | loss: 0.1392014\n",
      "\tspeed: 0.2788s/iter; left time: 15641.5480s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 5 | loss: 0.3300296\n",
      "\tspeed: 0.2790s/iter; left time: 15627.0493s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 5 | loss: 0.3309575\n",
      "\tspeed: 0.2788s/iter; left time: 15588.3695s\n",
      "3599it [16:43,  3.58it/s]\titers: 3600, epoch: 5 | loss: 0.1950828\n",
      "\tspeed: 0.2790s/iter; left time: 15568.2503s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 5 | loss: 0.2820531\n",
      "\tspeed: 0.2790s/iter; left time: 15540.7777s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 5 cost time: 1035.4520614147186\n",
      "1080it [02:25,  7.40it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 5 | Train Loss: 0.2212898 Vali Loss: 0.3329129 Test Loss: 0.2613454 MAE Loss: 0.3126645\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.4999999999999998e-06\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 6 | loss: 0.1282861\n",
      "\tspeed: 2.5069s/iter; left time: 139372.9053s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 6 | loss: 0.2494196\n",
      "\tspeed: 0.2790s/iter; left time: 15484.5174s\n",
      "299it [01:23,  3.58it/s]\titers: 300, epoch: 6 | loss: 0.2339453\n",
      "\tspeed: 0.2793s/iter; left time: 15470.5562s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 6 | loss: 0.2137068\n",
      "\tspeed: 0.2790s/iter; left time: 15428.9702s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 6 | loss: 0.2157253\n",
      "\tspeed: 0.2785s/iter; left time: 15369.3712s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 6 | loss: 0.3673338\n",
      "\tspeed: 0.2789s/iter; left time: 15365.4185s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 6 | loss: 0.1803798\n",
      "\tspeed: 0.2786s/iter; left time: 15322.4496s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 6 | loss: 0.3463192\n",
      "\tspeed: 0.2788s/iter; left time: 15304.9381s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 6 | loss: 0.2142603\n",
      "\tspeed: 0.2788s/iter; left time: 15275.4973s\n",
      "999it [04:39,  3.59it/s]\titers: 1000, epoch: 6 | loss: 0.3134519\n",
      "\tspeed: 0.2787s/iter; left time: 15243.9380s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 6 | loss: 0.1536993\n",
      "\tspeed: 0.2787s/iter; left time: 15215.6900s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 6 | loss: 0.2473893\n",
      "\tspeed: 0.2788s/iter; left time: 15192.2196s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 6 | loss: 0.3345494\n",
      "\tspeed: 0.2785s/iter; left time: 15147.2885s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 6 | loss: 0.1569494\n",
      "\tspeed: 0.2788s/iter; left time: 15137.5923s\n",
      "1499it [06:58,  3.60it/s]\titers: 1500, epoch: 6 | loss: 0.1509610\n",
      "\tspeed: 0.2786s/iter; left time: 15098.9809s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 6 | loss: 0.1187001\n",
      "\tspeed: 0.2787s/iter; left time: 15075.5312s\n",
      "1699it [07:54,  3.59it/s]\titers: 1700, epoch: 6 | loss: 0.1893223\n",
      "\tspeed: 0.2786s/iter; left time: 15040.8368s\n",
      "1799it [08:21,  3.58it/s]\titers: 1800, epoch: 6 | loss: 0.1551444\n",
      "\tspeed: 0.2786s/iter; left time: 15016.1694s\n",
      "1899it [08:49,  3.58it/s]\titers: 1900, epoch: 6 | loss: 0.1707929\n",
      "\tspeed: 0.2787s/iter; left time: 14993.7412s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 6 | loss: 0.2386624\n",
      "\tspeed: 0.2788s/iter; left time: 14971.7078s\n",
      "2099it [09:45,  3.60it/s]\titers: 2100, epoch: 6 | loss: 0.0923626\n",
      "\tspeed: 0.2785s/iter; left time: 14926.1348s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 6 | loss: 0.3033254\n",
      "\tspeed: 0.2784s/iter; left time: 14892.9151s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 6 | loss: 0.2040737\n",
      "\tspeed: 0.2786s/iter; left time: 14873.9430s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 6 | loss: 0.1792997\n",
      "\tspeed: 0.2786s/iter; left time: 14847.7160s\n",
      "2499it [11:36,  3.60it/s]\titers: 2500, epoch: 6 | loss: 0.3716601\n",
      "\tspeed: 0.2785s/iter; left time: 14814.8985s\n",
      "2599it [12:04,  3.58it/s]\titers: 2600, epoch: 6 | loss: 0.3133176\n",
      "\tspeed: 0.2785s/iter; left time: 14788.9680s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 6 | loss: 0.2133941\n",
      "\tspeed: 0.2787s/iter; left time: 14769.5041s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 6 | loss: 0.1205921\n",
      "\tspeed: 0.2789s/iter; left time: 14752.5480s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 6 | loss: 0.2484492\n",
      "\tspeed: 0.2787s/iter; left time: 14714.6615s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 6 | loss: 0.1603760\n",
      "\tspeed: 0.2788s/iter; left time: 14691.2984s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 6 | loss: 0.1885023\n",
      "\tspeed: 0.2788s/iter; left time: 14662.6053s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 6 | loss: 0.1652133\n",
      "\tspeed: 0.2787s/iter; left time: 14628.5554s\n",
      "3299it [15:19,  3.59it/s]\titers: 3300, epoch: 6 | loss: 0.1958784\n",
      "\tspeed: 0.2785s/iter; left time: 14590.2246s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 6 | loss: 0.1950782\n",
      "\tspeed: 0.2786s/iter; left time: 14569.3289s\n",
      "3499it [16:15,  3.58it/s]\titers: 3500, epoch: 6 | loss: 0.2291469\n",
      "\tspeed: 0.2788s/iter; left time: 14553.2693s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 6 | loss: 0.1764657\n",
      "\tspeed: 0.2785s/iter; left time: 14510.0896s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 6 | loss: 0.1248449\n",
      "\tspeed: 0.2785s/iter; left time: 14483.3809s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 6 cost time: 1035.331021308899\n",
      "1080it [02:26,  7.36it/s]\n",
      "537it [01:13,  7.35it/s]\n",
      "Epoch: 6 | Train Loss: 0.2188540 Vali Loss: 0.3307064 Test Loss: 0.2634546 MAE Loss: 0.3188943\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.2499999999999999e-06\n",
      "99it [00:28,  3.58it/s]\titers: 100, epoch: 7 | loss: 0.1504427\n",
      "\tspeed: 2.5183s/iter; left time: 130656.9157s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 7 | loss: 0.2855527\n",
      "\tspeed: 0.2795s/iter; left time: 14471.2491s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 7 | loss: 0.2060968\n",
      "\tspeed: 0.2790s/iter; left time: 14419.8078s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 7 | loss: 0.2399960\n",
      "\tspeed: 0.2788s/iter; left time: 14381.2602s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 7 | loss: 0.2612259\n",
      "\tspeed: 0.2784s/iter; left time: 14335.2294s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 7 | loss: 0.1605833\n",
      "\tspeed: 0.2785s/iter; left time: 14309.4895s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 7 | loss: 0.1315918\n",
      "\tspeed: 0.2785s/iter; left time: 14281.7693s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 7 | loss: 0.1403357\n",
      "\tspeed: 0.2787s/iter; left time: 14262.2777s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 7 | loss: 0.3025744\n",
      "\tspeed: 0.2787s/iter; left time: 14235.7196s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 7 | loss: 0.2118218\n",
      "\tspeed: 0.2786s/iter; left time: 14201.6858s\n",
      "1099it [05:06,  3.60it/s]\titers: 1100, epoch: 7 | loss: 0.1808184\n",
      "\tspeed: 0.2783s/iter; left time: 14161.0671s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 7 | loss: 0.2076816\n",
      "\tspeed: 0.2784s/iter; left time: 14136.8230s\n",
      "1299it [06:02,  3.58it/s]\titers: 1300, epoch: 7 | loss: 0.3674549\n",
      "\tspeed: 0.2787s/iter; left time: 14124.2249s\n",
      "1399it [06:30,  3.58it/s]\titers: 1400, epoch: 7 | loss: 0.2770617\n",
      "\tspeed: 0.2785s/iter; left time: 14086.6303s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 7 | loss: 0.2450124\n",
      "\tspeed: 0.2786s/iter; left time: 14063.1276s\n",
      "1599it [07:25,  3.59it/s]\titers: 1600, epoch: 7 | loss: 0.2475434\n",
      "\tspeed: 0.2784s/iter; left time: 14028.4748s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 7 | loss: 0.2239520\n",
      "\tspeed: 0.2785s/iter; left time: 14002.1368s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 7 | loss: 0.1125150\n",
      "\tspeed: 0.2787s/iter; left time: 13984.9238s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 7 | loss: 0.2162626\n",
      "\tspeed: 0.2786s/iter; left time: 13952.2576s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 7 | loss: 0.2020459\n",
      "\tspeed: 0.2785s/iter; left time: 13918.8715s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 7 | loss: 0.1837399\n",
      "\tspeed: 0.2783s/iter; left time: 13883.3807s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 7 | loss: 0.2333822\n",
      "\tspeed: 0.2785s/iter; left time: 13864.9467s\n",
      "2299it [10:40,  3.58it/s]\titers: 2300, epoch: 7 | loss: 0.2186836\n",
      "\tspeed: 0.2786s/iter; left time: 13840.4633s\n",
      "2399it [11:08,  3.59it/s]\titers: 2400, epoch: 7 | loss: 0.1459916\n",
      "\tspeed: 0.2784s/iter; left time: 13805.3567s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 7 | loss: 0.1554032\n",
      "\tspeed: 0.2784s/iter; left time: 13777.9825s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 7 | loss: 0.2381754\n",
      "\tspeed: 0.2784s/iter; left time: 13746.3210s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 7 | loss: 0.1343001\n",
      "\tspeed: 0.2783s/iter; left time: 13716.5721s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 7 | loss: 0.2225294\n",
      "\tspeed: 0.2785s/iter; left time: 13697.3158s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 7 | loss: 0.1699753\n",
      "\tspeed: 0.2785s/iter; left time: 13670.8035s\n",
      "2999it [13:55,  3.59it/s]\titers: 3000, epoch: 7 | loss: 0.4446869\n",
      "\tspeed: 0.2784s/iter; left time: 13638.7177s\n",
      "3099it [14:23,  3.59it/s]\titers: 3100, epoch: 7 | loss: 0.1450774\n",
      "\tspeed: 0.2786s/iter; left time: 13618.7230s\n",
      "3199it [14:51,  3.59it/s]\titers: 3200, epoch: 7 | loss: 0.2208150\n",
      "\tspeed: 0.2784s/iter; left time: 13582.5852s\n",
      "3299it [15:19,  3.59it/s]\titers: 3300, epoch: 7 | loss: 0.2572645\n",
      "\tspeed: 0.2783s/iter; left time: 13548.1045s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 7 | loss: 0.1704672\n",
      "\tspeed: 0.2785s/iter; left time: 13530.8594s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 7 | loss: 0.2710051\n",
      "\tspeed: 0.2785s/iter; left time: 13503.0302s\n",
      "3599it [16:42,  3.59it/s]\titers: 3600, epoch: 7 | loss: 0.1845014\n",
      "\tspeed: 0.2784s/iter; left time: 13471.0058s\n",
      "3699it [17:10,  3.59it/s]\titers: 3700, epoch: 7 | loss: 0.2189014\n",
      "\tspeed: 0.2784s/iter; left time: 13440.2628s\n",
      "3713it [17:14,  3.59it/s]\n",
      "Epoch: 7 cost time: 1034.7267076969147\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.40it/s]\n",
      "Epoch: 7 | Train Loss: 0.2178144 Vali Loss: 0.3303608 Test Loss: 0.2636685 MAE Loss: 0.3179020\n",
      "Updating learning rate to 6.249999999999999e-07\n",
      "99it [00:28,  3.58it/s]\titers: 100, epoch: 8 | loss: 0.5056435\n",
      "\tspeed: 2.5231s/iter; left time: 121538.2949s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 8 | loss: 0.2309715\n",
      "\tspeed: 0.2793s/iter; left time: 13425.1785s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 8 | loss: 0.1047508\n",
      "\tspeed: 0.2791s/iter; left time: 13388.3957s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 8 | loss: 0.1928864\n",
      "\tspeed: 0.2787s/iter; left time: 13342.4704s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 8 | loss: 0.1391518\n",
      "\tspeed: 0.2786s/iter; left time: 13309.7448s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 8 | loss: 0.2154792\n",
      "\tspeed: 0.2785s/iter; left time: 13277.9259s\n",
      "699it [03:15,  3.58it/s]\titers: 700, epoch: 8 | loss: 0.1311439\n",
      "\tspeed: 0.2790s/iter; left time: 13270.9892s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 8 | loss: 0.1599038\n",
      "\tspeed: 0.2788s/iter; left time: 13234.1074s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 8 | loss: 0.2990340\n",
      "\tspeed: 0.2786s/iter; left time: 13195.8137s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 8 | loss: 0.2186944\n",
      "\tspeed: 0.2788s/iter; left time: 13178.5889s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 8 | loss: 0.2389872\n",
      "\tspeed: 0.2786s/iter; left time: 13140.9406s\n",
      "1199it [05:34,  3.58it/s]\titers: 1200, epoch: 8 | loss: 0.4347574\n",
      "\tspeed: 0.2786s/iter; left time: 13112.9382s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 8 | loss: 0.6170467\n",
      "\tspeed: 0.2785s/iter; left time: 13079.1228s\n",
      "1399it [06:30,  3.60it/s]\titers: 1400, epoch: 8 | loss: 0.1466940\n",
      "\tspeed: 0.2784s/iter; left time: 13050.7295s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 8 | loss: 0.2500080\n",
      "\tspeed: 0.2786s/iter; left time: 13029.5434s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 8 | loss: 0.2586938\n",
      "\tspeed: 0.2786s/iter; left time: 13003.7381s\n",
      "1699it [07:53,  3.58it/s]\titers: 1700, epoch: 8 | loss: 0.1993283\n",
      "\tspeed: 0.2787s/iter; left time: 12978.7371s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 8 | loss: 0.1572350\n",
      "\tspeed: 0.2785s/iter; left time: 12939.8620s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 8 | loss: 0.1844990\n",
      "\tspeed: 0.2786s/iter; left time: 12916.4772s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 8 | loss: 0.1695595\n",
      "\tspeed: 0.2786s/iter; left time: 12891.9505s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 8 | loss: 0.5200454\n",
      "\tspeed: 0.2785s/iter; left time: 12859.9175s\n",
      "2199it [10:13,  3.58it/s]\titers: 2200, epoch: 8 | loss: 0.1494411\n",
      "\tspeed: 0.2785s/iter; left time: 12831.5829s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 8 | loss: 0.1620539\n",
      "\tspeed: 0.2787s/iter; left time: 12813.6864s\n",
      "2399it [11:08,  3.59it/s]\titers: 2400, epoch: 8 | loss: 0.3097299\n",
      "\tspeed: 0.2787s/iter; left time: 12782.3645s\n",
      "2499it [11:36,  3.58it/s]\titers: 2500, epoch: 8 | loss: 0.1299736\n",
      "\tspeed: 0.2788s/iter; left time: 12761.4496s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 8 | loss: 0.1604595\n",
      "\tspeed: 0.2789s/iter; left time: 12735.2379s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 8 | loss: 0.2241838\n",
      "\tspeed: 0.2784s/iter; left time: 12686.8479s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 8 | loss: 0.2378544\n",
      "\tspeed: 0.2788s/iter; left time: 12676.9072s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 8 | loss: 0.2452403\n",
      "\tspeed: 0.2784s/iter; left time: 12630.5294s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 8 | loss: 0.3589467\n",
      "\tspeed: 0.2785s/iter; left time: 12607.8434s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 8 | loss: 0.3318018\n",
      "\tspeed: 0.2789s/iter; left time: 12596.3401s\n",
      "3199it [14:51,  3.59it/s]\titers: 3200, epoch: 8 | loss: 0.2470485\n",
      "\tspeed: 0.2786s/iter; left time: 12556.7688s\n",
      "3299it [15:19,  3.58it/s]\titers: 3300, epoch: 8 | loss: 0.2261925\n",
      "\tspeed: 0.2787s/iter; left time: 12534.5878s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 8 | loss: 0.1829678\n",
      "\tspeed: 0.2788s/iter; left time: 12509.0273s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 8 | loss: 0.2100349\n",
      "\tspeed: 0.2786s/iter; left time: 12473.2969s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 8 | loss: 0.1558320\n",
      "\tspeed: 0.2788s/iter; left time: 12451.9081s\n",
      "3699it [17:11,  3.58it/s]\titers: 3700, epoch: 8 | loss: 0.3522272\n",
      "\tspeed: 0.2788s/iter; left time: 12427.6205s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 8 cost time: 1035.230404138565\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 8 | Train Loss: 0.2166827 Vali Loss: 0.3298773 Test Loss: 0.2634017 MAE Loss: 0.3186404\n",
      "Updating learning rate to 3.1249999999999997e-07\n",
      "99it [00:27,  3.58it/s]\titers: 100, epoch: 9 | loss: 0.1341745\n",
      "\tspeed: 2.5208s/iter; left time: 112066.0787s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 9 | loss: 0.2949684\n",
      "\tspeed: 0.2792s/iter; left time: 12385.3807s\n",
      "299it [01:23,  3.58it/s]\titers: 300, epoch: 9 | loss: 0.2106320\n",
      "\tspeed: 0.2792s/iter; left time: 12355.9241s\n",
      "399it [01:51,  3.58it/s]\titers: 400, epoch: 9 | loss: 0.1520289\n",
      "\tspeed: 0.2792s/iter; left time: 12327.1683s\n",
      "499it [02:19,  3.57it/s]\titers: 500, epoch: 9 | loss: 0.2073910\n",
      "\tspeed: 0.2792s/iter; left time: 12299.3082s\n",
      "599it [02:47,  3.58it/s]\titers: 600, epoch: 9 | loss: 0.1224944\n",
      "\tspeed: 0.2792s/iter; left time: 12271.9538s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 9 | loss: 0.1404726\n",
      "\tspeed: 0.2791s/iter; left time: 12241.8582s\n",
      "799it [03:43,  3.58it/s]\titers: 800, epoch: 9 | loss: 0.1859296\n",
      "\tspeed: 0.2794s/iter; left time: 12224.7430s\n",
      "899it [04:11,  3.58it/s]\titers: 900, epoch: 9 | loss: 0.1454226\n",
      "\tspeed: 0.2793s/iter; left time: 12192.9718s\n",
      "999it [04:39,  3.59it/s]\titers: 1000, epoch: 9 | loss: 0.2518956\n",
      "\tspeed: 0.2788s/iter; left time: 12144.1962s\n",
      "1099it [05:07,  3.59it/s]\titers: 1100, epoch: 9 | loss: 0.2510198\n",
      "\tspeed: 0.2787s/iter; left time: 12111.0942s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 9 | loss: 0.1500944\n",
      "\tspeed: 0.2787s/iter; left time: 12085.4398s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 9 | loss: 0.1954053\n",
      "\tspeed: 0.2786s/iter; left time: 12052.7628s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 9 | loss: 0.4186786\n",
      "\tspeed: 0.2786s/iter; left time: 12025.3196s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 9 | loss: 0.2082435\n",
      "\tspeed: 0.2788s/iter; left time: 12003.6460s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 9 | loss: 0.2076981\n",
      "\tspeed: 0.2787s/iter; left time: 11971.3783s\n",
      "1699it [07:54,  3.58it/s]\titers: 1700, epoch: 9 | loss: 0.1377982\n",
      "\tspeed: 0.2788s/iter; left time: 11948.8655s\n",
      "1799it [08:22,  3.59it/s]\titers: 1800, epoch: 9 | loss: 0.2074758\n",
      "\tspeed: 0.2786s/iter; left time: 11913.4108s\n",
      "1899it [08:50,  3.59it/s]\titers: 1900, epoch: 9 | loss: 0.3693664\n",
      "\tspeed: 0.2787s/iter; left time: 11887.7466s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 9 | loss: 0.1934590\n",
      "\tspeed: 0.2786s/iter; left time: 11856.7713s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 9 | loss: 0.2503009\n",
      "\tspeed: 0.2786s/iter; left time: 11828.0437s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 9 | loss: 0.2185290\n",
      "\tspeed: 0.2785s/iter; left time: 11798.2536s\n",
      "2299it [10:41,  3.58it/s]\titers: 2300, epoch: 9 | loss: 0.2406732\n",
      "\tspeed: 0.2787s/iter; left time: 11778.0868s\n",
      "2399it [11:09,  3.58it/s]\titers: 2400, epoch: 9 | loss: 0.2456325\n",
      "\tspeed: 0.2787s/iter; left time: 11750.7627s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 9 | loss: 0.1962808\n",
      "\tspeed: 0.2788s/iter; left time: 11724.5092s\n",
      "2599it [12:05,  3.59it/s]\titers: 2600, epoch: 9 | loss: 0.2458303\n",
      "\tspeed: 0.2787s/iter; left time: 11691.4528s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 9 | loss: 0.2479604\n",
      "\tspeed: 0.2785s/iter; left time: 11657.1957s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 9 | loss: 0.1291510\n",
      "\tspeed: 0.2791s/iter; left time: 11655.2712s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 9 | loss: 0.3158184\n",
      "\tspeed: 0.2786s/iter; left time: 11607.2340s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 9 | loss: 0.3466226\n",
      "\tspeed: 0.2787s/iter; left time: 11583.4068s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 9 | loss: 0.3465302\n",
      "\tspeed: 0.2787s/iter; left time: 11555.0354s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 9 | loss: 0.1500640\n",
      "\tspeed: 0.2788s/iter; left time: 11528.4655s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 9 | loss: 0.1531430\n",
      "\tspeed: 0.2785s/iter; left time: 11489.9620s\n",
      "3399it [15:48,  3.59it/s]\titers: 3400, epoch: 9 | loss: 0.1198360\n",
      "\tspeed: 0.2786s/iter; left time: 11468.2552s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 9 | loss: 0.2218416\n",
      "\tspeed: 0.2786s/iter; left time: 11437.2325s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 9 | loss: 0.1375825\n",
      "\tspeed: 0.2785s/iter; left time: 11408.2014s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 9 | loss: 0.1185283\n",
      "\tspeed: 0.2787s/iter; left time: 11387.6346s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 9 cost time: 1035.6511392593384\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 9 | Train Loss: 0.2160992 Vali Loss: 0.3287215 Test Loss: 0.2625743 MAE Loss: 0.3174453\n",
      "Updating learning rate to 1.5624999999999999e-07\n",
      "99it [00:28,  3.58it/s]\titers: 100, epoch: 10 | loss: 0.3498750\n",
      "\tspeed: 2.5270s/iter; left time: 102960.2954s\n",
      "199it [00:56,  3.59it/s]\titers: 200, epoch: 10 | loss: 0.1525075\n",
      "\tspeed: 0.2792s/iter; left time: 11348.2764s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 10 | loss: 0.2369844\n",
      "\tspeed: 0.2792s/iter; left time: 11319.0782s\n",
      "399it [01:51,  3.58it/s]\titers: 400, epoch: 10 | loss: 0.1620082\n",
      "\tspeed: 0.2789s/iter; left time: 11281.8243s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 10 | loss: 0.2574057\n",
      "\tspeed: 0.2788s/iter; left time: 11248.0391s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 10 | loss: 0.1916170\n",
      "\tspeed: 0.2788s/iter; left time: 11218.8379s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 10 | loss: 0.3328696\n",
      "\tspeed: 0.2785s/iter; left time: 11180.5553s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 10 | loss: 0.2004491\n",
      "\tspeed: 0.2786s/iter; left time: 11155.2786s\n",
      "899it [04:11,  3.58it/s]\titers: 900, epoch: 10 | loss: 0.1911200\n",
      "\tspeed: 0.2787s/iter; left time: 11134.1721s\n",
      "999it [04:39,  3.59it/s]\titers: 1000, epoch: 10 | loss: 0.3797316\n",
      "\tspeed: 0.2785s/iter; left time: 11096.8635s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 10 | loss: 0.2142629\n",
      "\tspeed: 0.2788s/iter; left time: 11082.5489s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 10 | loss: 0.3560216\n",
      "\tspeed: 0.2790s/iter; left time: 11061.0496s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 10 | loss: 0.3217525\n",
      "\tspeed: 0.2787s/iter; left time: 11022.8893s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 10 | loss: 0.2555680\n",
      "\tspeed: 0.2787s/iter; left time: 10994.0562s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 10 | loss: 0.2183040\n",
      "\tspeed: 0.2786s/iter; left time: 10961.3900s\n",
      "1599it [07:26,  3.58it/s]\titers: 1600, epoch: 10 | loss: 0.3464227\n",
      "\tspeed: 0.2786s/iter; left time: 10933.7985s\n",
      "1699it [07:54,  3.59it/s]\titers: 1700, epoch: 10 | loss: 0.2732971\n",
      "\tspeed: 0.2787s/iter; left time: 10909.7361s\n",
      "1799it [08:22,  3.59it/s]\titers: 1800, epoch: 10 | loss: 0.2122099\n",
      "\tspeed: 0.2786s/iter; left time: 10877.0489s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 10 | loss: 0.1183674\n",
      "\tspeed: 0.2786s/iter; left time: 10850.9205s\n",
      "1999it [09:17,  3.58it/s]\titers: 2000, epoch: 10 | loss: 0.2794635\n",
      "\tspeed: 0.2786s/iter; left time: 10822.8056s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 10 | loss: 0.2833150\n",
      "\tspeed: 0.2786s/iter; left time: 10794.7370s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 10 | loss: 0.2838675\n",
      "\tspeed: 0.2786s/iter; left time: 10767.9665s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 10 | loss: 0.1359369\n",
      "\tspeed: 0.2787s/iter; left time: 10742.6840s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 10 | loss: 0.1835596\n",
      "\tspeed: 0.2786s/iter; left time: 10712.2217s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 10 | loss: 0.2381515\n",
      "\tspeed: 0.2787s/iter; left time: 10686.6815s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 10 | loss: 0.1174449\n",
      "\tspeed: 0.2788s/iter; left time: 10662.8656s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 10 | loss: 0.1430054\n",
      "\tspeed: 0.2787s/iter; left time: 10632.0202s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 10 | loss: 0.1241590\n",
      "\tspeed: 0.2787s/iter; left time: 10601.3191s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 10 | loss: 0.3381941\n",
      "\tspeed: 0.2786s/iter; left time: 10572.2642s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 10 | loss: 0.1644658\n",
      "\tspeed: 0.2789s/iter; left time: 10556.4366s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 10 | loss: 0.1520503\n",
      "\tspeed: 0.2787s/iter; left time: 10518.4685s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 10 | loss: 0.1075225\n",
      "\tspeed: 0.2790s/iter; left time: 10501.4271s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 10 | loss: 0.1670698\n",
      "\tspeed: 0.2789s/iter; left time: 10470.1820s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 10 | loss: 0.2242707\n",
      "\tspeed: 0.2787s/iter; left time: 10435.0316s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 10 | loss: 0.1311589\n",
      "\tspeed: 0.2786s/iter; left time: 10403.8110s\n",
      "3599it [16:43,  3.55it/s]\titers: 3600, epoch: 10 | loss: 0.2642626\n",
      "\tspeed: 0.2787s/iter; left time: 10379.0217s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 10 | loss: 0.2729248\n",
      "\tspeed: 0.2788s/iter; left time: 10355.3683s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 10 cost time: 1035.5338678359985\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.38it/s]\n",
      "Epoch: 10 | Train Loss: 0.2166528 Vali Loss: 0.3287782 Test Loss: 0.2615655 MAE Loss: 0.3159559\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.812499999999999e-08\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 11 | loss: 0.1449713\n",
      "\tspeed: 2.5068s/iter; left time: 92828.6834s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 11 | loss: 0.2340196\n",
      "\tspeed: 0.2789s/iter; left time: 10300.9636s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 11 | loss: 0.1538113\n",
      "\tspeed: 0.2787s/iter; left time: 10263.0418s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 11 | loss: 0.2850380\n",
      "\tspeed: 0.2789s/iter; left time: 10242.8678s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 11 | loss: 0.2003610\n",
      "\tspeed: 0.2786s/iter; left time: 10204.4797s\n",
      "599it [02:47,  3.58it/s]\titers: 600, epoch: 11 | loss: 0.1152752\n",
      "\tspeed: 0.2787s/iter; left time: 10180.5137s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 11 | loss: 0.2165940\n",
      "\tspeed: 0.2785s/iter; left time: 10146.8882s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 11 | loss: 0.3517349\n",
      "\tspeed: 0.2786s/iter; left time: 10122.5600s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 11 | loss: 0.1317341\n",
      "\tspeed: 0.2788s/iter; left time: 10100.2690s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 11 | loss: 0.1705614\n",
      "\tspeed: 0.2786s/iter; left time: 10067.0447s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 11 | loss: 0.1838453\n",
      "\tspeed: 0.2786s/iter; left time: 10037.0068s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 11 | loss: 0.2231478\n",
      "\tspeed: 0.2784s/iter; left time: 10003.9179s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 11 | loss: 0.1906157\n",
      "\tspeed: 0.2786s/iter; left time: 9981.0674s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 11 | loss: 0.1824067\n",
      "\tspeed: 0.2786s/iter; left time: 9954.8798s\n",
      "1499it [06:58,  3.58it/s]\titers: 1500, epoch: 11 | loss: 0.1834292\n",
      "\tspeed: 0.2789s/iter; left time: 9938.6049s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 11 | loss: 0.1393807\n",
      "\tspeed: 0.2784s/iter; left time: 9890.6913s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 11 | loss: 0.2218723\n",
      "\tspeed: 0.2785s/iter; left time: 9869.0992s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 11 | loss: 0.2590234\n",
      "\tspeed: 0.2786s/iter; left time: 9842.8119s\n",
      "1899it [08:49,  3.58it/s]\titers: 1900, epoch: 11 | loss: 0.2866240\n",
      "\tspeed: 0.2785s/iter; left time: 9812.9384s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 11 | loss: 0.1490862\n",
      "\tspeed: 0.2787s/iter; left time: 9791.7264s\n",
      "2099it [09:45,  3.58it/s]\titers: 2100, epoch: 11 | loss: 0.1247264\n",
      "\tspeed: 0.2788s/iter; left time: 9768.0118s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 11 | loss: 0.1436351\n",
      "\tspeed: 0.2788s/iter; left time: 9739.7976s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 11 | loss: 0.2891334\n",
      "\tspeed: 0.2786s/iter; left time: 9703.4536s\n",
      "2399it [11:08,  3.59it/s]\titers: 2400, epoch: 11 | loss: 0.2208642\n",
      "\tspeed: 0.2785s/iter; left time: 9673.8257s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 11 | loss: 0.1552877\n",
      "\tspeed: 0.2788s/iter; left time: 9654.5185s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 11 | loss: 0.2493455\n",
      "\tspeed: 0.2787s/iter; left time: 9623.6800s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 11 | loss: 0.2157616\n",
      "\tspeed: 0.2785s/iter; left time: 9589.3026s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 11 | loss: 0.1494562\n",
      "\tspeed: 0.2787s/iter; left time: 9568.5240s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 11 | loss: 0.2666228\n",
      "\tspeed: 0.2787s/iter; left time: 9539.3950s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 11 | loss: 0.1865970\n",
      "\tspeed: 0.2785s/iter; left time: 9504.8697s\n",
      "3099it [14:23,  3.59it/s]\titers: 3100, epoch: 11 | loss: 0.1971619\n",
      "\tspeed: 0.2788s/iter; left time: 9487.0467s\n",
      "3199it [14:51,  3.59it/s]\titers: 3200, epoch: 11 | loss: 0.1520218\n",
      "\tspeed: 0.2786s/iter; left time: 9452.2974s\n",
      "3299it [15:19,  3.60it/s]\titers: 3300, epoch: 11 | loss: 0.1809543\n",
      "\tspeed: 0.2785s/iter; left time: 9422.5658s\n",
      "3399it [15:47,  3.60it/s]\titers: 3400, epoch: 11 | loss: 0.0803915\n",
      "\tspeed: 0.2785s/iter; left time: 9392.6168s\n",
      "3499it [16:15,  3.58it/s]\titers: 3500, epoch: 11 | loss: 0.1846436\n",
      "\tspeed: 0.2786s/iter; left time: 9370.2847s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 11 | loss: 0.2701206\n",
      "\tspeed: 0.2785s/iter; left time: 9339.7625s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 11 | loss: 0.1541885\n",
      "\tspeed: 0.2787s/iter; left time: 9316.0871s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 11 cost time: 1035.09819149971\n",
      "1080it [02:25,  7.40it/s]\n",
      "537it [01:12,  7.38it/s]\n",
      "Epoch: 11 | Train Loss: 0.2161719 Vali Loss: 0.3290599 Test Loss: 0.2622392 MAE Loss: 0.3164651\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.9062499999999997e-08\n",
      "99it [00:27,  3.59it/s]\titers: 100, epoch: 12 | loss: 0.0981183\n",
      "\tspeed: 2.5068s/iter; left time: 83521.8625s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 12 | loss: 0.2782066\n",
      "\tspeed: 0.2789s/iter; left time: 9262.9634s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 12 | loss: 0.2268104\n",
      "\tspeed: 0.2786s/iter; left time: 9227.3228s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 12 | loss: 0.2051531\n",
      "\tspeed: 0.2786s/iter; left time: 9197.3395s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 12 | loss: 0.2473495\n",
      "\tspeed: 0.2786s/iter; left time: 9171.0033s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 12 | loss: 0.1485891\n",
      "\tspeed: 0.2786s/iter; left time: 9142.0623s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 12 | loss: 0.1358718\n",
      "\tspeed: 0.2787s/iter; left time: 9119.0719s\n",
      "799it [03:43,  3.58it/s]\titers: 800, epoch: 12 | loss: 0.3198900\n",
      "\tspeed: 0.2787s/iter; left time: 9091.0624s\n",
      "899it [04:10,  3.58it/s]\titers: 900, epoch: 12 | loss: 0.2260443\n",
      "\tspeed: 0.2792s/iter; left time: 9077.9826s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 12 | loss: 0.1709547\n",
      "\tspeed: 0.2788s/iter; left time: 9038.8626s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 12 | loss: 0.4471456\n",
      "\tspeed: 0.2788s/iter; left time: 9011.0746s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 12 | loss: 0.1341240\n",
      "\tspeed: 0.2784s/iter; left time: 8971.0023s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 12 | loss: 0.1956956\n",
      "\tspeed: 0.2785s/iter; left time: 8946.0693s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 12 | loss: 0.2741916\n",
      "\tspeed: 0.2787s/iter; left time: 8924.2792s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 12 | loss: 0.2103669\n",
      "\tspeed: 0.2788s/iter; left time: 8898.8838s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 12 | loss: 0.3326179\n",
      "\tspeed: 0.2786s/iter; left time: 8863.4026s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 12 | loss: 0.2679738\n",
      "\tspeed: 0.2784s/iter; left time: 8831.7188s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 12 | loss: 0.2487636\n",
      "\tspeed: 0.2785s/iter; left time: 8806.2710s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 12 | loss: 0.1206203\n",
      "\tspeed: 0.2786s/iter; left time: 8780.1392s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 12 | loss: 0.1333138\n",
      "\tspeed: 0.2787s/iter; left time: 8757.5006s\n",
      "2099it [09:45,  3.58it/s]\titers: 2100, epoch: 12 | loss: 0.1882165\n",
      "\tspeed: 0.2790s/iter; left time: 8736.6048s\n",
      "2199it [10:13,  3.58it/s]\titers: 2200, epoch: 12 | loss: 0.1144293\n",
      "\tspeed: 0.2790s/iter; left time: 8708.6607s\n",
      "2299it [10:41,  3.58it/s]\titers: 2300, epoch: 12 | loss: 0.2583632\n",
      "\tspeed: 0.2789s/iter; left time: 8679.4084s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 12 | loss: 0.3058769\n",
      "\tspeed: 0.2788s/iter; left time: 8647.8965s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 12 | loss: 0.3199786\n",
      "\tspeed: 0.2787s/iter; left time: 8615.9345s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 12 | loss: 0.1815031\n",
      "\tspeed: 0.2787s/iter; left time: 8587.7206s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 12 | loss: 0.1735200\n",
      "\tspeed: 0.2787s/iter; left time: 8559.9645s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 12 | loss: 0.1953148\n",
      "\tspeed: 0.2787s/iter; left time: 8534.1154s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 12 | loss: 0.1607758\n",
      "\tspeed: 0.2788s/iter; left time: 8508.5284s\n",
      "2999it [13:56,  3.58it/s]\titers: 3000, epoch: 12 | loss: 0.2173009\n",
      "\tspeed: 0.2789s/iter; left time: 8485.0917s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 12 | loss: 0.2588116\n",
      "\tspeed: 0.2786s/iter; left time: 8446.3288s\n",
      "3199it [14:51,  3.59it/s]\titers: 3200, epoch: 12 | loss: 0.1802034\n",
      "\tspeed: 0.2786s/iter; left time: 8418.3624s\n",
      "3299it [15:19,  3.59it/s]\titers: 3300, epoch: 12 | loss: 0.2021094\n",
      "\tspeed: 0.2788s/iter; left time: 8395.6343s\n",
      "3399it [15:47,  3.58it/s]\titers: 3400, epoch: 12 | loss: 0.1563483\n",
      "\tspeed: 0.2787s/iter; left time: 8366.1687s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 12 | loss: 0.1205895\n",
      "\tspeed: 0.2788s/iter; left time: 8340.8791s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 12 | loss: 0.1905710\n",
      "\tspeed: 0.2789s/iter; left time: 8315.4361s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 12 | loss: 0.1890188\n",
      "\tspeed: 0.2787s/iter; left time: 8282.1664s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 12 cost time: 1035.3505516052246\n",
      "1080it [02:26,  7.39it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 12 | Train Loss: 0.2172576 Vali Loss: 0.3288122 Test Loss: 0.2621949 MAE Loss: 0.3166458\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.9531249999999998e-08\n",
      "99it [00:27,  3.59it/s]\titers: 100, epoch: 13 | loss: 0.1598936\n",
      "\tspeed: 2.5081s/iter; left time: 74251.2475s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 13 | loss: 0.3535151\n",
      "\tspeed: 0.2787s/iter; left time: 8223.2977s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 13 | loss: 0.3288059\n",
      "\tspeed: 0.2786s/iter; left time: 8192.0357s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 13 | loss: 0.2943467\n",
      "\tspeed: 0.2787s/iter; left time: 8167.0797s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 13 | loss: 0.2135956\n",
      "\tspeed: 0.2787s/iter; left time: 8138.0536s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 13 | loss: 0.1159540\n",
      "\tspeed: 0.2787s/iter; left time: 8112.3615s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 13 | loss: 0.1456614\n",
      "\tspeed: 0.2787s/iter; left time: 8083.4801s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 13 | loss: 0.1935531\n",
      "\tspeed: 0.2788s/iter; left time: 8059.6108s\n",
      "899it [04:10,  3.59it/s]\titers: 900, epoch: 13 | loss: 0.2150045\n",
      "\tspeed: 0.2787s/iter; left time: 8026.5457s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 13 | loss: 0.1764426\n",
      "\tspeed: 0.2787s/iter; left time: 7998.8100s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 13 | loss: 0.1357657\n",
      "\tspeed: 0.2788s/iter; left time: 7974.5949s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 13 | loss: 0.3353128\n",
      "\tspeed: 0.2786s/iter; left time: 7941.0492s\n",
      "1299it [06:02,  3.58it/s]\titers: 1300, epoch: 13 | loss: 0.1996422\n",
      "\tspeed: 0.2786s/iter; left time: 7914.2398s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 13 | loss: 0.1475685\n",
      "\tspeed: 0.2786s/iter; left time: 7885.9517s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 13 | loss: 0.1824621\n",
      "\tspeed: 0.2785s/iter; left time: 7854.1753s\n",
      "1599it [07:25,  3.59it/s]\titers: 1600, epoch: 13 | loss: 0.1591254\n",
      "\tspeed: 0.2786s/iter; left time: 7829.2321s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 13 | loss: 0.3292871\n",
      "\tspeed: 0.2785s/iter; left time: 7798.3697s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 13 | loss: 0.2116848\n",
      "\tspeed: 0.2784s/iter; left time: 7769.0432s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 13 | loss: 0.3125138\n",
      "\tspeed: 0.2783s/iter; left time: 7736.9388s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 13 | loss: 0.1431564\n",
      "\tspeed: 0.2785s/iter; left time: 7715.2218s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 13 | loss: 0.2330172\n",
      "\tspeed: 0.2786s/iter; left time: 7690.7965s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 13 | loss: 0.1853597\n",
      "\tspeed: 0.2788s/iter; left time: 7667.3485s\n",
      "2299it [10:40,  3.59it/s]\titers: 2300, epoch: 13 | loss: 0.1942309\n",
      "\tspeed: 0.2789s/iter; left time: 7644.0821s\n",
      "2399it [11:08,  3.58it/s]\titers: 2400, epoch: 13 | loss: 0.0887064\n",
      "\tspeed: 0.2785s/iter; left time: 7605.0469s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 13 | loss: 0.3284877\n",
      "\tspeed: 0.2785s/iter; left time: 7577.5835s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 13 | loss: 0.1660588\n",
      "\tspeed: 0.2787s/iter; left time: 7553.7461s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 13 | loss: 0.3117380\n",
      "\tspeed: 0.2785s/iter; left time: 7522.0096s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 13 | loss: 0.1197880\n",
      "\tspeed: 0.2786s/iter; left time: 7496.4739s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 13 | loss: 0.4387777\n",
      "\tspeed: 0.2787s/iter; left time: 7469.3894s\n",
      "2999it [13:55,  3.59it/s]\titers: 3000, epoch: 13 | loss: 0.2206094\n",
      "\tspeed: 0.2786s/iter; left time: 7438.9589s\n",
      "3099it [14:23,  3.60it/s]\titers: 3100, epoch: 13 | loss: 0.2835032\n",
      "\tspeed: 0.2786s/iter; left time: 7411.4714s\n",
      "3199it [14:51,  3.61it/s]\titers: 3200, epoch: 13 | loss: 0.2780699\n",
      "\tspeed: 0.2775s/iter; left time: 7354.9930s\n",
      "3299it [15:19,  3.60it/s]\titers: 3300, epoch: 13 | loss: 0.2091097\n",
      "\tspeed: 0.2776s/iter; left time: 7331.1728s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 13 | loss: 0.1117227\n",
      "\tspeed: 0.2777s/iter; left time: 7304.3944s\n",
      "3499it [16:14,  3.60it/s]\titers: 3500, epoch: 13 | loss: 0.1123419\n",
      "\tspeed: 0.2787s/iter; left time: 7303.7937s\n",
      "3599it [16:42,  3.59it/s]\titers: 3600, epoch: 13 | loss: 0.1861343\n",
      "\tspeed: 0.2785s/iter; left time: 7270.2272s\n",
      "3699it [17:10,  3.59it/s]\titers: 3700, epoch: 13 | loss: 0.1750333\n",
      "\tspeed: 0.2791s/iter; left time: 7257.7285s\n",
      "3713it [17:14,  3.59it/s]\n",
      "Epoch: 13 cost time: 1034.712157011032\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.38it/s]\n",
      "Epoch: 13 | Train Loss: 0.2159967 Vali Loss: 0.3290540 Test Loss: 0.2621953 MAE Loss: 0.3165445\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.765624999999999e-09\n",
      "99it [00:28,  3.58it/s]\titers: 100, epoch: 14 | loss: 0.1171653\n",
      "\tspeed: 2.5064s/iter; left time: 64896.8808s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 14 | loss: 0.2517175\n",
      "\tspeed: 0.2792s/iter; left time: 7200.8993s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 14 | loss: 0.1823611\n",
      "\tspeed: 0.2793s/iter; left time: 7175.3323s\n",
      "399it [01:51,  3.58it/s]\titers: 400, epoch: 14 | loss: 0.1684120\n",
      "\tspeed: 0.2790s/iter; left time: 7140.2616s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 14 | loss: 0.1795499\n",
      "\tspeed: 0.2790s/iter; left time: 7112.0762s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 14 | loss: 0.1791357\n",
      "\tspeed: 0.2787s/iter; left time: 7075.7718s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 14 | loss: 0.2368675\n",
      "\tspeed: 0.2789s/iter; left time: 7053.3857s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 14 | loss: 0.1906768\n",
      "\tspeed: 0.2788s/iter; left time: 7022.6891s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 14 | loss: 0.1510700\n",
      "\tspeed: 0.2787s/iter; left time: 6993.8581s\n",
      "999it [04:39,  3.59it/s]\titers: 1000, epoch: 14 | loss: 0.3830197\n",
      "\tspeed: 0.2785s/iter; left time: 6961.3963s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 14 | loss: 0.3001775\n",
      "\tspeed: 0.2787s/iter; left time: 6936.6191s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 14 | loss: 0.2589030\n",
      "\tspeed: 0.2786s/iter; left time: 6907.2277s\n",
      "1299it [06:02,  3.58it/s]\titers: 1300, epoch: 14 | loss: 0.1419421\n",
      "\tspeed: 0.2786s/iter; left time: 6879.6447s\n",
      "1399it [06:30,  3.58it/s]\titers: 1400, epoch: 14 | loss: 0.1727114\n",
      "\tspeed: 0.2790s/iter; left time: 6860.7984s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 14 | loss: 0.0931327\n",
      "\tspeed: 0.2789s/iter; left time: 6829.6985s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 14 | loss: 0.1742762\n",
      "\tspeed: 0.2790s/iter; left time: 6804.2134s\n",
      "1699it [07:54,  3.58it/s]\titers: 1700, epoch: 14 | loss: 0.1592049\n",
      "\tspeed: 0.2789s/iter; left time: 6775.1902s\n",
      "1799it [08:22,  3.59it/s]\titers: 1800, epoch: 14 | loss: 0.2724331\n",
      "\tspeed: 0.2785s/iter; left time: 6737.6903s\n",
      "1899it [08:49,  3.58it/s]\titers: 1900, epoch: 14 | loss: 0.1787173\n",
      "\tspeed: 0.2793s/iter; left time: 6728.6304s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 14 | loss: 0.2576762\n",
      "\tspeed: 0.2787s/iter; left time: 6687.6621s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 14 | loss: 0.1281054\n",
      "\tspeed: 0.2786s/iter; left time: 6657.3727s\n",
      "2199it [10:13,  3.56it/s]\titers: 2200, epoch: 14 | loss: 0.0923741\n",
      "\tspeed: 0.2787s/iter; left time: 6629.9571s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 14 | loss: 0.2491354\n",
      "\tspeed: 0.2787s/iter; left time: 6602.0666s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 14 | loss: 0.1060119\n",
      "\tspeed: 0.2786s/iter; left time: 6572.8348s\n",
      "2499it [11:37,  3.60it/s]\titers: 2500, epoch: 14 | loss: 0.2041347\n",
      "\tspeed: 0.2787s/iter; left time: 6548.1329s\n",
      "2599it [12:05,  3.59it/s]\titers: 2600, epoch: 14 | loss: 0.2665102\n",
      "\tspeed: 0.2784s/iter; left time: 6513.2565s\n",
      "2699it [12:32,  3.58it/s]\titers: 2700, epoch: 14 | loss: 0.2887287\n",
      "\tspeed: 0.2788s/iter; left time: 6493.5401s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 14 | loss: 0.1527705\n",
      "\tspeed: 0.2788s/iter; left time: 6465.0490s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 14 | loss: 0.1344827\n",
      "\tspeed: 0.2793s/iter; left time: 6449.2340s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 14 | loss: 0.1979525\n",
      "\tspeed: 0.2787s/iter; left time: 6407.1610s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 14 | loss: 0.1349045\n",
      "\tspeed: 0.2789s/iter; left time: 6383.7545s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 14 | loss: 0.1222948\n",
      "\tspeed: 0.2787s/iter; left time: 6353.2125s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 14 | loss: 0.2408251\n",
      "\tspeed: 0.2787s/iter; left time: 6323.6611s\n",
      "3399it [15:48,  3.59it/s]\titers: 3400, epoch: 14 | loss: 0.1205181\n",
      "\tspeed: 0.2787s/iter; left time: 6296.9616s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 14 | loss: 0.2082628\n",
      "\tspeed: 0.2786s/iter; left time: 6267.1303s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 14 | loss: 0.1980462\n",
      "\tspeed: 0.2786s/iter; left time: 6238.9065s\n",
      "3699it [17:11,  3.58it/s]\titers: 3700, epoch: 14 | loss: 0.3052340\n",
      "\tspeed: 0.2789s/iter; left time: 6216.6536s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 14 cost time: 1035.6596283912659\n",
      "1080it [02:25,  7.40it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 14 | Train Loss: 0.2163281 Vali Loss: 0.3290410 Test Loss: 0.2622153 MAE Loss: 0.3165929\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.8828124999999996e-09\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 15 | loss: 0.1599563\n",
      "\tspeed: 2.5063s/iter; left time: 55588.2479s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 15 | loss: 0.1805723\n",
      "\tspeed: 0.2788s/iter; left time: 6155.4796s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 15 | loss: 0.1926850\n",
      "\tspeed: 0.2787s/iter; left time: 6126.4258s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 15 | loss: 0.1792406\n",
      "\tspeed: 0.2786s/iter; left time: 6095.8559s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 15 | loss: 0.1848410\n",
      "\tspeed: 0.2786s/iter; left time: 6067.1234s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 15 | loss: 0.1915709\n",
      "\tspeed: 0.2785s/iter; left time: 6036.5820s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 15 | loss: 0.2641656\n",
      "\tspeed: 0.2787s/iter; left time: 6013.9905s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 15 | loss: 0.1861591\n",
      "\tspeed: 0.2787s/iter; left time: 5986.5124s\n",
      "899it [04:10,  3.59it/s]\titers: 900, epoch: 15 | loss: 0.3335209\n",
      "\tspeed: 0.2788s/iter; left time: 5959.9211s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 15 | loss: 0.1317589\n",
      "\tspeed: 0.2789s/iter; left time: 5934.4172s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 15 | loss: 0.3626288\n",
      "\tspeed: 0.2788s/iter; left time: 5903.7832s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 15 | loss: 0.1947817\n",
      "\tspeed: 0.2785s/iter; left time: 5871.4220s\n",
      "1299it [06:02,  3.58it/s]\titers: 1300, epoch: 15 | loss: 0.1203282\n",
      "\tspeed: 0.2790s/iter; left time: 5852.0944s\n",
      "1399it [06:30,  3.58it/s]\titers: 1400, epoch: 15 | loss: 0.1013151\n",
      "\tspeed: 0.2788s/iter; left time: 5821.3599s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 15 | loss: 0.2908128\n",
      "\tspeed: 0.2788s/iter; left time: 5792.3800s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 15 | loss: 0.1925844\n",
      "\tspeed: 0.2786s/iter; left time: 5761.9526s\n",
      "1699it [07:53,  3.58it/s]\titers: 1700, epoch: 15 | loss: 0.3840427\n",
      "\tspeed: 0.2787s/iter; left time: 5734.7688s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 15 | loss: 0.1721246\n",
      "\tspeed: 0.2789s/iter; left time: 5711.2392s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 15 | loss: 0.1331270\n",
      "\tspeed: 0.2787s/iter; left time: 5679.3890s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 15 | loss: 0.1275507\n",
      "\tspeed: 0.2787s/iter; left time: 5652.0768s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 15 | loss: 0.1773982\n",
      "\tspeed: 0.2788s/iter; left time: 5626.8772s\n",
      "2199it [10:13,  3.58it/s]\titers: 2200, epoch: 15 | loss: 0.2554922\n",
      "\tspeed: 0.2787s/iter; left time: 5596.7165s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 15 | loss: 0.2009732\n",
      "\tspeed: 0.2790s/iter; left time: 5574.9276s\n",
      "2399it [11:09,  3.58it/s]\titers: 2400, epoch: 15 | loss: 0.2133664\n",
      "\tspeed: 0.2789s/iter; left time: 5543.4470s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 15 | loss: 0.2354217\n",
      "\tspeed: 0.2789s/iter; left time: 5516.0671s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 15 | loss: 0.1773946\n",
      "\tspeed: 0.2786s/iter; left time: 5482.5088s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 15 | loss: 0.2485515\n",
      "\tspeed: 0.2787s/iter; left time: 5457.2402s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 15 | loss: 0.1702110\n",
      "\tspeed: 0.2788s/iter; left time: 5431.4010s\n",
      "2899it [13:28,  3.60it/s]\titers: 2900, epoch: 15 | loss: 0.4179516\n",
      "\tspeed: 0.2787s/iter; left time: 5400.5911s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 15 | loss: 0.4632199\n",
      "\tspeed: 0.2790s/iter; left time: 5378.9617s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 15 | loss: 0.1610461\n",
      "\tspeed: 0.2788s/iter; left time: 5346.4772s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 15 | loss: 0.1902802\n",
      "\tspeed: 0.2786s/iter; left time: 5315.4192s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 15 | loss: 0.1854442\n",
      "\tspeed: 0.2785s/iter; left time: 5286.5307s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 15 | loss: 0.2951007\n",
      "\tspeed: 0.2785s/iter; left time: 5257.2807s\n",
      "3499it [16:15,  3.58it/s]\titers: 3500, epoch: 15 | loss: 0.1829995\n",
      "\tspeed: 0.2787s/iter; left time: 5233.9005s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 15 | loss: 0.1285038\n",
      "\tspeed: 0.2787s/iter; left time: 5204.9051s\n",
      "3699it [17:11,  3.58it/s]\titers: 3700, epoch: 15 | loss: 0.1852131\n",
      "\tspeed: 0.2786s/iter; left time: 5176.6576s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 15 cost time: 1035.4319245815277\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.40it/s]\n",
      "Epoch: 15 | Train Loss: 0.2161924 Vali Loss: 0.3290136 Test Loss: 0.2622102 MAE Loss: 0.3166045\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.4414062499999998e-09\n",
      "99it [00:27,  3.59it/s]\titers: 100, epoch: 16 | loss: 0.1378184\n",
      "\tspeed: 2.5037s/iter; left time: 46233.3383s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 16 | loss: 0.2633446\n",
      "\tspeed: 0.2787s/iter; left time: 5118.6930s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 16 | loss: 0.1798030\n",
      "\tspeed: 0.2786s/iter; left time: 5089.2025s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 16 | loss: 0.0778011\n",
      "\tspeed: 0.2788s/iter; left time: 5064.2244s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 16 | loss: 0.1918824\n",
      "\tspeed: 0.2788s/iter; left time: 5036.8741s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 16 | loss: 0.1576596\n",
      "\tspeed: 0.2787s/iter; left time: 5006.9296s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 16 | loss: 0.1658383\n",
      "\tspeed: 0.2787s/iter; left time: 4979.6853s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 16 | loss: 0.1651721\n",
      "\tspeed: 0.2788s/iter; left time: 4953.3965s\n",
      "899it [04:10,  3.59it/s]\titers: 900, epoch: 16 | loss: 0.2220348\n",
      "\tspeed: 0.2788s/iter; left time: 4924.4972s\n",
      "999it [04:38,  3.59it/s]\titers: 1000, epoch: 16 | loss: 0.2236049\n",
      "\tspeed: 0.2786s/iter; left time: 4894.0625s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 16 | loss: 0.1296080\n",
      "\tspeed: 0.2786s/iter; left time: 4865.2642s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 16 | loss: 0.2824441\n",
      "\tspeed: 0.2787s/iter; left time: 4840.0132s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 16 | loss: 0.1570976\n",
      "\tspeed: 0.2788s/iter; left time: 4813.5237s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 16 | loss: 0.1913229\n",
      "\tspeed: 0.2787s/iter; left time: 4783.8769s\n",
      "1499it [06:58,  3.60it/s]\titers: 1500, epoch: 16 | loss: 0.1516590\n",
      "\tspeed: 0.2785s/iter; left time: 4753.1117s\n",
      "1599it [07:25,  3.59it/s]\titers: 1600, epoch: 16 | loss: 0.0983954\n",
      "\tspeed: 0.2786s/iter; left time: 4726.7658s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 16 | loss: 0.2433433\n",
      "\tspeed: 0.2787s/iter; left time: 4700.7805s\n",
      "1799it [08:21,  3.58it/s]\titers: 1800, epoch: 16 | loss: 0.2575822\n",
      "\tspeed: 0.2788s/iter; left time: 4673.6669s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 16 | loss: 0.1560309\n",
      "\tspeed: 0.2787s/iter; left time: 4644.7723s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 16 | loss: 0.2112122\n",
      "\tspeed: 0.2787s/iter; left time: 4617.3897s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 16 | loss: 0.3364078\n",
      "\tspeed: 0.2785s/iter; left time: 4586.3679s\n",
      "2199it [10:13,  3.58it/s]\titers: 2200, epoch: 16 | loss: 0.1502742\n",
      "\tspeed: 0.2787s/iter; left time: 4561.2067s\n",
      "2299it [10:41,  3.58it/s]\titers: 2300, epoch: 16 | loss: 0.1546065\n",
      "\tspeed: 0.2784s/iter; left time: 4528.7117s\n",
      "2399it [11:08,  3.59it/s]\titers: 2400, epoch: 16 | loss: 0.2526341\n",
      "\tspeed: 0.2785s/iter; left time: 4502.6951s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 16 | loss: 0.2594317\n",
      "\tspeed: 0.2786s/iter; left time: 4476.3937s\n",
      "2599it [12:04,  3.58it/s]\titers: 2600, epoch: 16 | loss: 0.2179849\n",
      "\tspeed: 0.2787s/iter; left time: 4449.5112s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 16 | loss: 0.2760262\n",
      "\tspeed: 0.2788s/iter; left time: 4423.2365s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 16 | loss: 0.1324300\n",
      "\tspeed: 0.2788s/iter; left time: 4396.1680s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 16 | loss: 0.3845347\n",
      "\tspeed: 0.2788s/iter; left time: 4368.3707s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 16 | loss: 0.1330300\n",
      "\tspeed: 0.2787s/iter; left time: 4338.9219s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 16 | loss: 0.1918906\n",
      "\tspeed: 0.2787s/iter; left time: 4309.8500s\n",
      "3199it [14:51,  3.60it/s]\titers: 3200, epoch: 16 | loss: 0.3300209\n",
      "\tspeed: 0.2785s/iter; left time: 4279.5703s\n",
      "3299it [15:19,  3.58it/s]\titers: 3300, epoch: 16 | loss: 0.2066097\n",
      "\tspeed: 0.2788s/iter; left time: 4255.7761s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 16 | loss: 0.1338300\n",
      "\tspeed: 0.2786s/iter; left time: 4224.7518s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 16 | loss: 0.2895259\n",
      "\tspeed: 0.2786s/iter; left time: 4198.0144s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 16 | loss: 0.2461460\n",
      "\tspeed: 0.2787s/iter; left time: 4170.6417s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 16 | loss: 0.1839092\n",
      "\tspeed: 0.2787s/iter; left time: 4143.3159s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 16 cost time: 1035.1836245059967\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 16 | Train Loss: 0.2166212 Vali Loss: 0.3289707 Test Loss: 0.2621911 MAE Loss: 0.3165786\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.2207031249999999e-09\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 17 | loss: 0.1149908\n",
      "\tspeed: 2.5053s/iter; left time: 36960.9332s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 17 | loss: 0.1524932\n",
      "\tspeed: 0.2788s/iter; left time: 4084.6714s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 17 | loss: 0.1618537\n",
      "\tspeed: 0.2787s/iter; left time: 4056.4607s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 17 | loss: 0.1917358\n",
      "\tspeed: 0.2789s/iter; left time: 4031.6369s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 17 | loss: 0.1585454\n",
      "\tspeed: 0.2788s/iter; left time: 4001.6332s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 17 | loss: 0.2548499\n",
      "\tspeed: 0.2789s/iter; left time: 3974.4801s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 17 | loss: 0.3555305\n",
      "\tspeed: 0.2786s/iter; left time: 3943.4743s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 17 | loss: 0.3721704\n",
      "\tspeed: 0.2789s/iter; left time: 3919.1408s\n",
      "899it [04:11,  3.59it/s]\titers: 900, epoch: 17 | loss: 0.1858776\n",
      "\tspeed: 0.2787s/iter; left time: 3889.0749s\n",
      "999it [04:38,  3.58it/s]\titers: 1000, epoch: 17 | loss: 0.2311032\n",
      "\tspeed: 0.2787s/iter; left time: 3861.4662s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 17 | loss: 0.2847497\n",
      "\tspeed: 0.2787s/iter; left time: 3833.0262s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 17 | loss: 0.4550507\n",
      "\tspeed: 0.2786s/iter; left time: 3803.1672s\n",
      "1299it [06:02,  3.60it/s]\titers: 1300, epoch: 17 | loss: 0.2026780\n",
      "\tspeed: 0.2785s/iter; left time: 3774.9049s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 17 | loss: 0.4692140\n",
      "\tspeed: 0.2787s/iter; left time: 3748.9697s\n",
      "1499it [06:58,  3.59it/s]\titers: 1500, epoch: 17 | loss: 0.2174576\n",
      "\tspeed: 0.2787s/iter; left time: 3721.6849s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 17 | loss: 0.1422010\n",
      "\tspeed: 0.2785s/iter; left time: 3690.6268s\n",
      "1699it [07:53,  3.59it/s]\titers: 1700, epoch: 17 | loss: 0.3194141\n",
      "\tspeed: 0.2786s/iter; left time: 3664.3072s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 17 | loss: 0.1770387\n",
      "\tspeed: 0.2786s/iter; left time: 3636.8012s\n",
      "1899it [08:49,  3.58it/s]\titers: 1900, epoch: 17 | loss: 0.1962756\n",
      "\tspeed: 0.2786s/iter; left time: 3608.2987s\n",
      "1999it [09:17,  3.58it/s]\titers: 2000, epoch: 17 | loss: 0.1254692\n",
      "\tspeed: 0.2792s/iter; left time: 3588.2286s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 17 | loss: 0.1867508\n",
      "\tspeed: 0.2789s/iter; left time: 3556.9683s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 17 | loss: 0.3066641\n",
      "\tspeed: 0.2786s/iter; left time: 3525.1732s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 17 | loss: 0.2652248\n",
      "\tspeed: 0.2785s/iter; left time: 3496.5465s\n",
      "2399it [11:09,  3.60it/s]\titers: 2400, epoch: 17 | loss: 0.2526775\n",
      "\tspeed: 0.2785s/iter; left time: 3468.6225s\n",
      "2499it [11:36,  3.59it/s]\titers: 2500, epoch: 17 | loss: 0.2712550\n",
      "\tspeed: 0.2786s/iter; left time: 3440.9737s\n",
      "2599it [12:04,  3.59it/s]\titers: 2600, epoch: 17 | loss: 0.1602639\n",
      "\tspeed: 0.2785s/iter; left time: 3412.8100s\n",
      "2699it [12:32,  3.59it/s]\titers: 2700, epoch: 17 | loss: 0.3186671\n",
      "\tspeed: 0.2787s/iter; left time: 3387.1641s\n",
      "2799it [13:00,  3.58it/s]\titers: 2800, epoch: 17 | loss: 0.2229415\n",
      "\tspeed: 0.2788s/iter; left time: 3360.0346s\n",
      "2899it [13:28,  3.58it/s]\titers: 2900, epoch: 17 | loss: 0.2305780\n",
      "\tspeed: 0.2793s/iter; left time: 3338.4577s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 17 | loss: 0.1945155\n",
      "\tspeed: 0.2786s/iter; left time: 3302.5534s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 17 | loss: 0.2060319\n",
      "\tspeed: 0.2786s/iter; left time: 3274.4234s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 17 | loss: 0.2122407\n",
      "\tspeed: 0.2786s/iter; left time: 3246.2018s\n",
      "3299it [15:19,  3.59it/s]\titers: 3300, epoch: 17 | loss: 0.1918494\n",
      "\tspeed: 0.2786s/iter; left time: 3218.5794s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 17 | loss: 0.1702049\n",
      "\tspeed: 0.2786s/iter; left time: 3191.0081s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 17 | loss: 0.2607087\n",
      "\tspeed: 0.2785s/iter; left time: 3161.8276s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 17 | loss: 0.4418551\n",
      "\tspeed: 0.2786s/iter; left time: 3135.4675s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 17 | loss: 0.2605077\n",
      "\tspeed: 0.2786s/iter; left time: 3107.3909s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 17 cost time: 1035.2688274383545\n",
      "1080it [02:25,  7.40it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 17 | Train Loss: 0.2162176 Vali Loss: 0.3290507 Test Loss: 0.2621964 MAE Loss: 0.3165964\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.103515624999999e-10\n",
      "99it [00:27,  3.59it/s]\titers: 100, epoch: 18 | loss: 0.2545331\n",
      "\tspeed: 2.5062s/iter; left time: 27668.3301s\n",
      "199it [00:55,  3.58it/s]\titers: 200, epoch: 18 | loss: 0.1270936\n",
      "\tspeed: 0.2790s/iter; left time: 3051.8511s\n",
      "299it [01:23,  3.59it/s]\titers: 300, epoch: 18 | loss: 0.1107818\n",
      "\tspeed: 0.2790s/iter; left time: 3024.1018s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 18 | loss: 0.3091292\n",
      "\tspeed: 0.2787s/iter; left time: 2993.6068s\n",
      "499it [02:19,  3.59it/s]\titers: 500, epoch: 18 | loss: 0.1431724\n",
      "\tspeed: 0.2787s/iter; left time: 2965.7528s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 18 | loss: 0.3466567\n",
      "\tspeed: 0.2788s/iter; left time: 2938.5528s\n",
      "699it [03:15,  3.58it/s]\titers: 700, epoch: 18 | loss: 0.2541309\n",
      "\tspeed: 0.2789s/iter; left time: 2912.0491s\n",
      "799it [03:43,  3.59it/s]\titers: 800, epoch: 18 | loss: 0.1701817\n",
      "\tspeed: 0.2788s/iter; left time: 2883.1469s\n",
      "899it [04:11,  3.58it/s]\titers: 900, epoch: 18 | loss: 0.1011219\n",
      "\tspeed: 0.2788s/iter; left time: 2854.7804s\n",
      "999it [04:38,  3.60it/s]\titers: 1000, epoch: 18 | loss: 0.1920284\n",
      "\tspeed: 0.2786s/iter; left time: 2825.4391s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 18 | loss: 0.3176936\n",
      "\tspeed: 0.2786s/iter; left time: 2796.7684s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 18 | loss: 0.2115505\n",
      "\tspeed: 0.2790s/iter; left time: 2773.0065s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 18 | loss: 0.4376697\n",
      "\tspeed: 0.2786s/iter; left time: 2741.4558s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 18 | loss: 0.2142743\n",
      "\tspeed: 0.2789s/iter; left time: 2716.7368s\n",
      "1499it [06:58,  3.58it/s]\titers: 1500, epoch: 18 | loss: 0.1759028\n",
      "\tspeed: 0.2789s/iter; left time: 2689.0682s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 18 | loss: 0.3313026\n",
      "\tspeed: 0.2789s/iter; left time: 2660.3050s\n",
      "1699it [07:54,  3.58it/s]\titers: 1700, epoch: 18 | loss: 0.1783504\n",
      "\tspeed: 0.2788s/iter; left time: 2632.2683s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 18 | loss: 0.2053342\n",
      "\tspeed: 0.2789s/iter; left time: 2604.8739s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 18 | loss: 0.1407320\n",
      "\tspeed: 0.2787s/iter; left time: 2575.4734s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 18 | loss: 0.1422822\n",
      "\tspeed: 0.2784s/iter; left time: 2544.9416s\n",
      "2099it [09:45,  3.60it/s]\titers: 2100, epoch: 18 | loss: 0.1063459\n",
      "\tspeed: 0.2789s/iter; left time: 2520.8714s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 18 | loss: 0.2901277\n",
      "\tspeed: 0.2786s/iter; left time: 2490.4544s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 18 | loss: 0.1556633\n",
      "\tspeed: 0.2785s/iter; left time: 2462.2402s\n",
      "2399it [11:09,  3.58it/s]\titers: 2400, epoch: 18 | loss: 0.2813236\n",
      "\tspeed: 0.2788s/iter; left time: 2436.6443s\n",
      "2499it [11:37,  3.59it/s]\titers: 2500, epoch: 18 | loss: 0.1491094\n",
      "\tspeed: 0.2787s/iter; left time: 2408.1880s\n",
      "2599it [12:04,  3.58it/s]\titers: 2600, epoch: 18 | loss: 0.2272189\n",
      "\tspeed: 0.2790s/iter; left time: 2382.5636s\n",
      "2699it [12:32,  3.58it/s]\titers: 2700, epoch: 18 | loss: 0.4016510\n",
      "\tspeed: 0.2787s/iter; left time: 2352.3656s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 18 | loss: 0.1326070\n",
      "\tspeed: 0.2787s/iter; left time: 2324.0884s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 18 | loss: 0.1894891\n",
      "\tspeed: 0.2787s/iter; left time: 2296.6008s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 18 | loss: 0.1741673\n",
      "\tspeed: 0.2788s/iter; left time: 2269.7164s\n",
      "3099it [14:24,  3.58it/s]\titers: 3100, epoch: 18 | loss: 0.2123941\n",
      "\tspeed: 0.2788s/iter; left time: 2241.7723s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 18 | loss: 0.1308875\n",
      "\tspeed: 0.2787s/iter; left time: 2212.4971s\n",
      "3299it [15:20,  3.59it/s]\titers: 3300, epoch: 18 | loss: 0.1229706\n",
      "\tspeed: 0.2786s/iter; left time: 2184.4077s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 18 | loss: 0.2185616\n",
      "\tspeed: 0.2787s/iter; left time: 2157.4777s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 18 | loss: 0.2786092\n",
      "\tspeed: 0.2788s/iter; left time: 2129.9184s\n",
      "3599it [16:43,  3.59it/s]\titers: 3600, epoch: 18 | loss: 0.3660344\n",
      "\tspeed: 0.2788s/iter; left time: 2102.3841s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 18 | loss: 0.3047598\n",
      "\tspeed: 0.2786s/iter; left time: 2072.9924s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 18 cost time: 1035.530817747116\n",
      "1080it [02:25,  7.40it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 18 | Train Loss: 0.2160912 Vali Loss: 0.3289860 Test Loss: 0.2621921 MAE Loss: 0.3165949\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.0517578124999997e-10\n",
      "99it [00:28,  3.59it/s]\titers: 100, epoch: 19 | loss: 0.2133358\n",
      "\tspeed: 2.5064s/iter; left time: 18364.0939s\n",
      "199it [00:55,  3.59it/s]\titers: 200, epoch: 19 | loss: 0.1846379\n",
      "\tspeed: 0.2788s/iter; left time: 2014.6659s\n",
      "299it [01:23,  3.58it/s]\titers: 300, epoch: 19 | loss: 0.2655693\n",
      "\tspeed: 0.2787s/iter; left time: 1986.1148s\n",
      "399it [01:51,  3.59it/s]\titers: 400, epoch: 19 | loss: 0.1705581\n",
      "\tspeed: 0.2787s/iter; left time: 1958.4831s\n",
      "499it [02:19,  3.58it/s]\titers: 500, epoch: 19 | loss: 0.2595662\n",
      "\tspeed: 0.2785s/iter; left time: 1929.5051s\n",
      "599it [02:47,  3.59it/s]\titers: 600, epoch: 19 | loss: 0.1997813\n",
      "\tspeed: 0.2787s/iter; left time: 1902.9390s\n",
      "699it [03:15,  3.59it/s]\titers: 700, epoch: 19 | loss: 0.1872606\n",
      "\tspeed: 0.2786s/iter; left time: 1874.1635s\n",
      "799it [03:43,  3.58it/s]\titers: 800, epoch: 19 | loss: 0.3790385\n",
      "\tspeed: 0.2786s/iter; left time: 1846.1703s\n",
      "899it [04:10,  3.59it/s]\titers: 900, epoch: 19 | loss: 0.1756178\n",
      "\tspeed: 0.2786s/iter; left time: 1818.6168s\n",
      "999it [04:38,  3.58it/s]\titers: 1000, epoch: 19 | loss: 0.2920670\n",
      "\tspeed: 0.2786s/iter; left time: 1790.7702s\n",
      "1099it [05:06,  3.59it/s]\titers: 1100, epoch: 19 | loss: 0.2403958\n",
      "\tspeed: 0.2787s/iter; left time: 1763.2859s\n",
      "1199it [05:34,  3.59it/s]\titers: 1200, epoch: 19 | loss: 0.1831413\n",
      "\tspeed: 0.2787s/iter; left time: 1735.3186s\n",
      "1299it [06:02,  3.59it/s]\titers: 1300, epoch: 19 | loss: 0.2491660\n",
      "\tspeed: 0.2787s/iter; left time: 1707.3508s\n",
      "1399it [06:30,  3.59it/s]\titers: 1400, epoch: 19 | loss: 0.2255468\n",
      "\tspeed: 0.2786s/iter; left time: 1678.9041s\n",
      "1499it [06:58,  3.57it/s]\titers: 1500, epoch: 19 | loss: 0.1744705\n",
      "\tspeed: 0.2789s/iter; left time: 1653.1421s\n",
      "1599it [07:26,  3.59it/s]\titers: 1600, epoch: 19 | loss: 0.1545642\n",
      "\tspeed: 0.2786s/iter; left time: 1623.3452s\n",
      "1699it [07:53,  3.58it/s]\titers: 1700, epoch: 19 | loss: 0.1506287\n",
      "\tspeed: 0.2790s/iter; left time: 1597.7447s\n",
      "1799it [08:21,  3.59it/s]\titers: 1800, epoch: 19 | loss: 0.2690065\n",
      "\tspeed: 0.2788s/iter; left time: 1568.5626s\n",
      "1899it [08:49,  3.59it/s]\titers: 1900, epoch: 19 | loss: 0.3712234\n",
      "\tspeed: 0.2790s/iter; left time: 1542.1533s\n",
      "1999it [09:17,  3.59it/s]\titers: 2000, epoch: 19 | loss: 0.2535019\n",
      "\tspeed: 0.2787s/iter; left time: 1512.6511s\n",
      "2099it [09:45,  3.59it/s]\titers: 2100, epoch: 19 | loss: 0.2505177\n",
      "\tspeed: 0.2788s/iter; left time: 1484.9525s\n",
      "2199it [10:13,  3.59it/s]\titers: 2200, epoch: 19 | loss: 0.1084463\n",
      "\tspeed: 0.2787s/iter; left time: 1456.7224s\n",
      "2299it [10:41,  3.59it/s]\titers: 2300, epoch: 19 | loss: 0.1627678\n",
      "\tspeed: 0.2788s/iter; left time: 1429.4710s\n",
      "2399it [11:09,  3.59it/s]\titers: 2400, epoch: 19 | loss: 0.1220200\n",
      "\tspeed: 0.2787s/iter; left time: 1400.8955s\n",
      "2499it [11:36,  3.58it/s]\titers: 2500, epoch: 19 | loss: 0.2942510\n",
      "\tspeed: 0.2787s/iter; left time: 1373.3158s\n",
      "2599it [12:04,  3.58it/s]\titers: 2600, epoch: 19 | loss: 0.1508684\n",
      "\tspeed: 0.2788s/iter; left time: 1345.8984s\n",
      "2699it [12:32,  3.58it/s]\titers: 2700, epoch: 19 | loss: 0.1467731\n",
      "\tspeed: 0.2786s/iter; left time: 1316.8446s\n",
      "2799it [13:00,  3.59it/s]\titers: 2800, epoch: 19 | loss: 0.4720771\n",
      "\tspeed: 0.2788s/iter; left time: 1290.2030s\n",
      "2899it [13:28,  3.59it/s]\titers: 2900, epoch: 19 | loss: 0.1882690\n",
      "\tspeed: 0.2786s/iter; left time: 1261.3409s\n",
      "2999it [13:56,  3.59it/s]\titers: 3000, epoch: 19 | loss: 0.2225413\n",
      "\tspeed: 0.2787s/iter; left time: 1233.6514s\n",
      "3099it [14:24,  3.59it/s]\titers: 3100, epoch: 19 | loss: 0.1623758\n",
      "\tspeed: 0.2787s/iter; left time: 1206.1172s\n",
      "3199it [14:52,  3.59it/s]\titers: 3200, epoch: 19 | loss: 0.1257642\n",
      "\tspeed: 0.2786s/iter; left time: 1177.5484s\n",
      "3299it [15:19,  3.59it/s]\titers: 3300, epoch: 19 | loss: 0.2542690\n",
      "\tspeed: 0.2784s/iter; left time: 1149.1555s\n",
      "3399it [15:47,  3.59it/s]\titers: 3400, epoch: 19 | loss: 0.2843603\n",
      "\tspeed: 0.2787s/iter; left time: 1122.2556s\n",
      "3499it [16:15,  3.59it/s]\titers: 3500, epoch: 19 | loss: 0.1808718\n",
      "\tspeed: 0.2785s/iter; left time: 1093.7379s\n",
      "3599it [16:43,  3.58it/s]\titers: 3600, epoch: 19 | loss: 0.3189796\n",
      "\tspeed: 0.2786s/iter; left time: 1066.0167s\n",
      "3699it [17:11,  3.59it/s]\titers: 3700, epoch: 19 | loss: 0.1362152\n",
      "\tspeed: 0.2786s/iter; left time: 1038.1863s\n",
      "3713it [17:15,  3.59it/s]\n",
      "Epoch: 19 cost time: 1035.2947473526\n",
      "1080it [02:25,  7.41it/s]\n",
      "537it [01:12,  7.39it/s]\n",
      "Epoch: 19 | Train Loss: 0.2159840 Vali Loss: 0.3290244 Test Loss: 0.2622060 MAE Loss: 0.3166045\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Total time: 398.11671031713485 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_epochs=20\n",
    "learning_rate=0.001\n",
    "llama_layers=12 # 626 sec on 1 epoch # 13 min, # 1095 sec\n",
    "\n",
    "# num_process=1\n",
    "batch_size=24\n",
    "d_model=32\n",
    "d_ff=128\n",
    "# --num_processes 1 --num_machines 1\n",
    "comment='TimeLLM-FR'\n",
    "\n",
    "!python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./datasets/ \\\n",
    "  --data_path FR_data.csv \\\n",
    "  --model_id FR_96_24 \\\n",
    "  --model \"TimeLLM\" \\\n",
    "  --data FR \\\n",
    "  --features M \\\n",
    "  --seq_len 512 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 3 \\\n",
    "  --dec_in 3 \\\n",
    "  --c_out 3 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model \"GPT2\" \\\n",
    "  --llm_dim 768 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --llm_layers $llama_layers \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --model_comment $comment\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")\n",
    "# train 88899 # 89115\n",
    "# val 25707 # 25923\n",
    "# test 12675 # 12891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28628486 0.350173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "trues = np.load('./results/TimeLLM/FR/true.npy')[:, :, 0] # \"flatten\"\n",
    "preds = np.load('./results/TimeLLM/FR/pred.npy')[:, :, 0]\n",
    "\n",
    "mse = mean_squared_error(preds, trues)\n",
    "mae = mean_absolute_error(preds, trues)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0629387"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st col\n",
    "mean_squared_error(preds[:4297, :], trues[:4297, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06294414"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(preds[:4296, :], trues[:4296, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20941424"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd col\n",
    "mean_squared_error(preds[4297:4297+4297, :], trues[4297:4297+4297, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20946036"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(preds[4296:4296+4296, :], trues[4296:4296+4296, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58671147"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd col\n",
    "mean_squared_error(preds[-4294:, :], trues[-4294:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58645016"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(preds[-4296:, :], trues[-4296:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28635480333333335"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0629387 + 0.20941424 + 0.58671147)/3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gott sein Dank, it is equally divided!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2862848866666667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.06294414 + 0.20946036 + 0.58645016)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49835145, -0.66085815, -0.77242994, ..., -0.2461022 ,\n",
       "        -0.18869932, -0.32407108],\n",
       "       [-0.59541345, -0.5986439 , -0.6777887 , ..., -0.19645908,\n",
       "        -0.18272988, -0.3967439 ],\n",
       "       [-0.6564008 , -0.60324514, -0.49854472, ..., -0.2355855 ,\n",
       "        -0.35850787, -0.53156567],\n",
       "       ...,\n",
       "       [-0.7715905 , -0.84912944, -1.0079898 , ..., -0.474673  ,\n",
       "        -0.49547616, -0.6732484 ],\n",
       "       [-1.0477278 , -1.1990207 , -1.3011434 , ..., -0.53711456,\n",
       "        -0.63356376, -0.88177854],\n",
       "       [-1.2163655 , -1.2617521 , -1.2144743 , ..., -0.73129517,\n",
       "        -0.86698246, -1.0934432 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:4297, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4296.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12888/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12891/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12888 - 4297*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# For gruenau: For CUDA making it available this works:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'val (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cuda\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mDevice(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "mem_info = device.mem_info()\n",
    "\n",
    "print(\"Total GPU Memory:\", mem_info.total)\n",
    "print(\"Free GPU Memory:\", mem_info.free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_run_file = \"./TSLibrary/run.py\"\n",
    "# path_to_run_file = \"/Users/valentyna/Documents/Master_thesis_new/Time-LLM/run_main.py\"\n",
    "path_to_run_file = \"./Time-LLM/run_main.py\"\n",
    "\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Construct the command to execute the script with required and model arguments\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        # Execute the script and capture the output\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        # Check if there's any error in the process\n",
    "        if process.returncode != 0:\n",
    "            output = stderr.decode(\"utf-8\")\n",
    "        else:\n",
    "            output = stdout.decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        output = str(e)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"/Users/valentyna/Documents/Master_thesis_new/Time-LLM/run_main.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output  \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Check if the command that failed is hostname -I\n",
    "        if 'hostname -I' in e.cmd:\n",
    "            # Retry the hostname command with -i option instead of -I\n",
    "            try:\n",
    "                corrected_command = [\"hostname\", \"-i\"]\n",
    "                output = subprocess.check_output(corrected_command, universal_newlines=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                # If the hostname command still fails, return None\n",
    "                output = None\n",
    "        else:\n",
    "            # Re-raise the exception for other commands\n",
    "            raise e\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23it [00:04,  5.14it/s]\n",
      "2it [00:00,  3.57it/s]\n",
      "6it [00:00,  7.63it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py\", line 170, in <module>\n",
      "    train_loader, vali_loader, test_loader, model, model_optim, scheduler = accelerator.prepare(\n",
      "                                                                            ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1255, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1671, in _prepare_deepspeed\n",
      "    raise AssertionError(\n",
      "AssertionError: You can't use same `Accelerator()` instance with multiple models when using DeepSpeed\n",
      "\n",
      "23it [00:04,  5.26it/s]\n",
      "2it [00:00,  3.70it/s]\n",
      "6it [00:00,  7.67it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/cs-hu/riabchuv/hu-home/my_work/Time-LLM/run_main.py\", line 170, in <module>\n",
      "    train_loader, vali_loader, test_loader, model, model_optim, scheduler = accelerator.prepare(\n",
      "                                                                            ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1255, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1671, in _prepare_deepspeed\n",
      "    raise AssertionError(\n",
      "AssertionError: You can't use same `Accelerator()` instance with multiple models when using DeepSpeed\n",
      "\n",
      "Total time: 0.6892154971758525 min.\n"
     ]
    }
   ],
   "source": [
    "# 5.413828869660695 min. without test\n",
    "# 6.30 min with test\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "current_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "# host_address = \"MBP-Valentyna\"\n",
    "datasets = ['GB_data_small.csv', 'GB_data_small.csv']\n",
    "num_cols = [\"5\", \"5\"]\n",
    "pred_len = \"10\"\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "            \"--task_name\", \"long_term_forecast\",\n",
    "            \"--is_training\", \"1\", #True\n",
    "            \"--root_path\", current_path, #\"/content/my_work/datasets/\",\n",
    "            \"--data_path\", dataset,\n",
    "            #\"--train_epochs\", \"1\",\n",
    "            \"--model_id\", model_id,\n",
    "            \"--model\", model,\n",
    "            \"--data\", \"GB\", # This ensures a 70%,10%,20% train,val,test split see data_provider/data_loader.py\n",
    "            \"--features\", \"M\", # Multivariate\n",
    "            \"--seq_len\", \"10\",\n",
    "            \"--label_len\", \"5\",\n",
    "            \"--pred_len\", pred_len,\n",
    "            \"--e_layers\", \"3\", # Hyperparameters as in original model\n",
    "            \"--d_layers\", \"2\",\n",
    "            \"--factor\", \"3\",\n",
    "            #\"--gpu\", gpu_index,\n",
    "            # '--use_multi_gpu',\n",
    "            \"--llm_model\", \"GPT2\",\n",
    "            '--llm_dim', '768',\n",
    "            \"--llm_layers\", \"6\",\n",
    "            \"--enc_in\", num_cols[i],\n",
    "            \"--dec_in\", num_cols[i],\n",
    "            \"--c_out\", num_cols[i],\n",
    "            \"--des\", \"Exp\",\n",
    "            \"--itr\", \"2\",\n",
    "            \"--train_epochs\", \"1\",\n",
    "            \"--align_epochs\", \"1\",\n",
    "            \"--model_comment\", model\n",
    "\n",
    "\n",
    "        ]\n",
    "    int_start = time.time()\n",
    "    model_output = run_output(path_to_run_file, model_arguments)\n",
    "    print(model_output)\n",
    "    \"\"\"\n",
    "    # folder_path = f'/content/drive/MyDrive/Masterarbeit/results/{model}/'\n",
    "    folder_path = f'./results/{model}/'\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    result_file_path = os.path.join(folder_path, 'stored_model_output.txt')\n",
    "    with open(result_file_path, 'a') as f:\n",
    "\n",
    "        f.write(model_output + \"  \\n\")\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    int_end = time.time()\n",
    "    print(model_output)\n",
    "    print(f\"Time intermediate for {dataset[:2]} dataset:\", (int_end - int_start)/60, \"min.\")\n",
    "    \"\"\"\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time:\", (end - start)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deepspeed\n",
      "Version: 0.14.0\n",
      "Summary: DeepSpeed library\n",
      "Home-page: http://deepspeed.ai\n",
      "Author: DeepSpeed Team\n",
      "Author-email: deepspeed-info@microsoft.com\n",
      "License: Apache Software License 2.0\n",
      "Location: /vol/cs-hu/riabchuv/.local/lib/python3.11/site-packages\n",
      "Requires: hjson, ninja, numpy, packaging, psutil, py-cpuinfo, pydantic, pynvml, torch, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host address: gruenau1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_host_address():\n",
    "    try:\n",
    "        # Use hostname -s to get the short host name\n",
    "        host_address = subprocess.check_output([\"hostname\", \"-s\"]).strip().decode()\n",
    "        return host_address\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error retrieving host address: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "host_address = get_host_address()\n",
    "if host_address:\n",
    "    print(f\"Host address: {host_address}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve host address.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: gruenau1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the hostname command to get the hostname\n",
    "hostname_command = [\"hostname\"]\n",
    "hostname_output = subprocess.check_output(hostname_command, universal_newlines=True)\n",
    "\n",
    "# Parse the hostname from the output\n",
    "hostname = hostname_output.strip()\n",
    "\n",
    "# Now you can use the hostname variable in your script\n",
    "print(\"Hostname:\", hostname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpi4py-mpich==3.1.5\n",
      "  Downloading mpi4py_mpich-3.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Downloading mpi4py_mpich-3.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpi4py-mpich\n",
      "Successfully installed mpi4py-mpich-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mpi4py-mpich==3.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for mpi4py \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m>\u001b[0m \u001b[31m[174 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_src\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/run.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/__init__.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/bench.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/__main__.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/_base.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/server.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/__init__.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/_core.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/pool.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/aplus.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/__main__.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/_lib.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/pkl5.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/dtlib.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/__init__.py -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/__main__.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/__init__.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/run.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/bench.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/MPI.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/dl.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/py.typed -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/__init__.pxd -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/libmpi.pxd -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/MPI.pxd -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/include/mpi4py/mpi4py.MPI.h -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/include/mpi4py/mpi4py.MPI_api.h -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/include/mpi4py/mpi4py.h -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/include/mpi4py/mpi4py.i -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/include/mpi4py/mpi.pxi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/include/mpi4py\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/__main__.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/__init__.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/_core.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/aplus.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/server.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/pool.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/futures/_lib.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/futures\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/__init__.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/dtlib.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m copying src/mpi4py/util/pkl5.pyi -> build/lib.macosx-12.0-arm64-cpython-39/mpi4py/util\n",
      "  \u001b[31m   \u001b[0m running build_clib\n",
      "  \u001b[31m   \u001b[0m MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  \u001b[31m   \u001b[0m checking for library 'lmpe' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -llmpe -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'lmpe' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m building 'mpe' dylib library\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-12.0-arm64-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-12.0-arm64-cpython-39/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c src/lib-pmpi/mpe.c -o build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/mpe.o\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12.0-arm64-cpython-39/mpi4py/lib-pmpi\n",
      "  \u001b[31m   \u001b[0m clang -shared -undefined dynamic_lookup -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -install_name libmpe.dylib build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/mpe.o -o build/lib.macosx-12.0-arm64-cpython-39/mpi4py/lib-pmpi/libmpe.dylib\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/Users/valentyna/.pyenv/versions/3.9.16/lib' ignored\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/opt/homebrew/lib' ignored\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt-mpi' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt-mpi -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt-mpi' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt.mpi' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt.mpi -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt.mpi' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m building 'vt' dylib library\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c src/lib-pmpi/vt.c -o build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt.o\n",
      "  \u001b[31m   \u001b[0m clang -shared -undefined dynamic_lookup -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -install_name libvt.dylib build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt.o -o build/lib.macosx-12.0-arm64-cpython-39/mpi4py/lib-pmpi/libvt.dylib\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/Users/valentyna/.pyenv/versions/3.9.16/lib' ignored\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/opt/homebrew/lib' ignored\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt-mpi' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt-mpi -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt-mpi' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt.mpi' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt.mpi -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt.mpi' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m building 'vt-mpi' dylib library\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c src/lib-pmpi/vt-mpi.c -o build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt-mpi.o\n",
      "  \u001b[31m   \u001b[0m clang -shared -undefined dynamic_lookup -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -install_name libvt-mpi.dylib build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt-mpi.o -o build/lib.macosx-12.0-arm64-cpython-39/mpi4py/lib-pmpi/libvt-mpi.dylib\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/Users/valentyna/.pyenv/versions/3.9.16/lib' ignored\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/opt/homebrew/lib' ignored\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt-hyb' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt-hyb -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt-hyb' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m checking for library 'vt.ompi' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -lvt.ompi -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: library 'vt.ompi' not found\n",
      "  \u001b[31m   \u001b[0m clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m building 'vt-hyb' dylib library\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -c src/lib-pmpi/vt-hyb.c -o build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt-hyb.o\n",
      "  \u001b[31m   \u001b[0m clang -shared -undefined dynamic_lookup -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -install_name libvt-hyb.dylib build/temp.macosx-12.0-arm64-cpython-39/src/lib-pmpi/vt-hyb.o -o build/lib.macosx-12.0-arm64-cpython-39/mpi4py/lib-pmpi/libvt-hyb.dylib\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/Users/valentyna/.pyenv/versions/3.9.16/lib' ignored\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/opt/homebrew/lib' ignored\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m MPI configuration: [mpi] from 'mpi.cfg'\n",
      "  \u001b[31m   \u001b[0m checking for dlopen() availability ...\n",
      "  \u001b[31m   \u001b[0m checking for header 'dlfcn.h' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/valentyna/.pyenv/versions/3.9.16/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m success!\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m success!\n",
      "  \u001b[31m   \u001b[0m checking for library 'dl' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/valentyna/.pyenv/versions/3.9.16/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang -flat_namespace -undefined suppress _configtest.o -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Lbuild/temp.macosx-12.0-arm64-cpython-39 -ldl -o _configtest\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m ld: warning: -undefined suppress is deprecated\n",
      "  \u001b[31m   \u001b[0m success!\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o _configtest\n",
      "  \u001b[31m   \u001b[0m checking for function 'dlopen' ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/valentyna/.pyenv/versions/3.9.16/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m clang _configtest.o -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Lbuild/temp.macosx-12.0-arm64-cpython-39 -ldl -o _configtest\n",
      "  \u001b[31m   \u001b[0m success!\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o _configtest\n",
      "  \u001b[31m   \u001b[0m building 'mpi4py.dl' extension\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DHAVE_DLFCN_H=1 -DHAVE_DLOPEN=1 -I/Users/valentyna/.pyenv/versions/3.9.16/include/python3.9 -c src/dynload.c -o build/temp.macosx-12.0-arm64-cpython-39/src/dynload.o\n",
      "  \u001b[31m   \u001b[0m clang -bundle -undefined dynamic_lookup -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib -L/opt/homebrew/opt/readline/lib -L/opt/homebrew/opt/readline/lib -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Wl,-rpath,/Users/valentyna/.pyenv/versions/3.9.16/lib -L/opt/homebrew/lib -Wl,-rpath,/opt/homebrew/lib -L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib build/temp.macosx-12.0-arm64-cpython-39/src/dynload.o -L/Users/valentyna/.pyenv/versions/3.9.16/lib -Lbuild/temp.macosx-12.0-arm64-cpython-39 -ldl -o build/lib.macosx-12.0-arm64-cpython-39/mpi4py/dl.cpython-39-darwin.so\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/Users/valentyna/.pyenv/versions/3.9.16/lib' ignored\n",
      "  \u001b[31m   \u001b[0m ld: warning: duplicate -rpath '/opt/homebrew/lib' ignored\n",
      "  \u001b[31m   \u001b[0m checking for MPI compile and link ...\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -DOPENSSL_NO_SSL3 -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/valentyna/.pyenv/versions/3.9.16/include/python3.9 -c _configtest.c -o _configtest.o\n",
      "  \u001b[31m   \u001b[0m _configtest.c:2:10: fatal error: 'mpi.h' file not found\n",
      "  \u001b[31m   \u001b[0m #include <mpi.h>\n",
      "  \u001b[31m   \u001b[0m          ^~~~~~~\n",
      "  \u001b[31m   \u001b[0m 1 error generated.\n",
      "  \u001b[31m   \u001b[0m failure.\n",
      "  \u001b[31m   \u001b[0m removing: _configtest.c _configtest.o\n",
      "  \u001b[31m   \u001b[0m error: Cannot compile MPI programs. Check your configuration!!!\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for mpi4py\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build mpi4py\n",
      "\u001b[31mERROR: Could not build wheels for mpi4py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# On mac: first\n",
    "# brew install mpi4py\n",
    "\n",
    "# then: \n",
    "# !pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/Users/valentyna/.pyenv/versions/3.9.16/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 51, in __init__\\n    config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode(\"utf-8\")\\nUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xfd in position 0: invalid start byte\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/valentyna/Documents/Master_thesis_new/Time-LLM/run_main.py\", line 103, in <module>\\n    deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=\\'./ds_config_zero2.json\\')\\n  File \"<string>\", line 14, in __init__\\n  File \"/Users/valentyna/.pyenv/versions/3.9.16/lib/python3.9/site-packages/accelerate/utils/dataclasses.py\", line 756, in __post_init__\\n    self.hf_ds_config = HfDeepSpeedConfig(self.hf_ds_config)\\n  File \"/Users/valentyna/.pyenv/versions/3.9.16/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 54, in __init__\\n    raise ValueError(\\nValueError: Expected a string path to an existing deepspeed config, or a dictionary, or a base64 encoded string. Received: ./ds_config_zero2.json\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-9.1303e-06, -1.4021e-01, -2.0845e-01,  ..., -1.5329e-01,\n",
      "          -6.7827e-02, -1.9630e-01],\n",
      "         [ 4.1949e-01,  2.3525e-01,  3.4816e-01,  ...,  4.5321e-02,\n",
      "           1.5447e-01,  1.9546e-02],\n",
      "         [ 2.5089e-01, -3.9139e-01, -2.6851e-01,  ..., -3.5611e-01,\n",
      "          -1.5503e-01, -1.0117e-01],\n",
      "         [-3.7585e-02,  4.5083e-01, -6.3438e-02,  ..., -5.4199e-01,\n",
      "           2.9846e-01,  6.0528e-02],\n",
      "         [ 1.1579e-01, -3.7055e-01, -7.1209e-01,  ..., -8.2506e-02,\n",
      "           5.2692e-02,  1.6689e-01],\n",
      "         [ 2.8985e-01, -2.3842e-01,  5.7513e-02,  ..., -8.6427e-02,\n",
      "          -4.7248e-02,  3.3461e-01]]], grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[-1.2526e+00,  2.3200e+00,  1.7218e-01,  ..., -1.0076e+00,\n",
      "           -1.8970e-01,  1.3219e+00],\n",
      "          [-1.6482e+00,  3.0222e+00,  1.2789e+00,  ..., -9.0779e-01,\n",
      "           -1.7395e+00,  2.4237e+00],\n",
      "          [-2.0161e+00,  2.0313e+00,  2.3849e+00,  ..., -2.9251e-01,\n",
      "           -1.0717e+00,  2.1678e+00],\n",
      "          [-2.0149e+00,  2.2646e+00,  2.1904e+00,  ..., -1.2118e+00,\n",
      "           -2.6872e+00,  2.5476e+00],\n",
      "          [-2.1165e+00,  2.5639e+00,  2.9237e+00,  ..., -1.1024e+00,\n",
      "           -2.1702e+00,  1.6031e+00],\n",
      "          [-2.4981e+00,  2.8332e+00,  1.9392e+00,  ..., -1.0660e+00,\n",
      "           -2.3585e+00,  1.9409e+00]],\n",
      "\n",
      "         [[-3.9746e-01, -4.7357e-01, -8.4727e-01,  ...,  3.0718e-01,\n",
      "            2.2743e+00,  5.3708e-01],\n",
      "          [ 2.7222e-01, -1.2016e+00, -1.9081e+00,  ..., -1.3531e+00,\n",
      "            1.2823e+00, -4.3198e-01],\n",
      "          [-8.5746e-01, -1.9682e+00, -5.8241e-01,  ..., -1.2474e+00,\n",
      "            5.0094e+00, -5.0707e-02],\n",
      "          [ 1.0365e+00, -5.4134e-01,  1.6986e-01,  ..., -3.1776e+00,\n",
      "            3.7231e+00,  6.1735e-01],\n",
      "          [-1.1216e+00, -2.2523e+00, -1.1507e+00,  ..., -2.2397e+00,\n",
      "            4.9670e+00, -3.6484e-02],\n",
      "          [-3.4430e-01, -1.0480e+00, -1.0363e-01,  ..., -1.9122e+00,\n",
      "            2.5792e+00,  6.5189e-02]],\n",
      "\n",
      "         [[ 3.6703e-01, -1.0056e+00,  1.3048e+00,  ..., -1.2840e+00,\n",
      "           -2.1111e+00,  7.7345e-01],\n",
      "          [ 5.5608e-01, -4.6297e-01,  7.4483e-01,  ..., -1.8272e+00,\n",
      "            5.4572e-01,  1.0119e+00],\n",
      "          [ 7.5019e-01,  2.2083e-01,  3.3150e-01,  ..., -3.2759e+00,\n",
      "            5.0937e-01,  1.1785e+00],\n",
      "          [ 7.5790e-01,  8.9911e-01,  5.7238e-01,  ..., -2.8138e+00,\n",
      "            8.3661e-01,  1.6292e+00],\n",
      "          [ 2.4545e-01,  7.0623e-01, -6.5512e-02,  ..., -3.1818e+00,\n",
      "            1.5450e+00,  1.6885e+00],\n",
      "          [ 6.4428e-01,  6.9316e-01,  1.1326e+00,  ..., -2.0425e+00,\n",
      "            1.4424e+00,  1.8534e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.3704e-01, -6.5364e-02, -1.2984e-01,  ...,  3.8034e-01,\n",
      "            7.5885e-01,  5.1427e-01],\n",
      "          [ 1.6820e-01, -9.1829e-02, -5.0034e-02,  ...,  7.3646e-01,\n",
      "            6.1343e-01,  5.4443e-01],\n",
      "          [ 1.7551e-01,  1.2820e-01, -1.7251e-01,  ...,  1.3367e+00,\n",
      "            2.2455e-01,  3.6982e-01],\n",
      "          [ 2.7532e-01, -9.4236e-02, -5.4313e-02,  ...,  1.0819e+00,\n",
      "            2.2199e-01,  6.3474e-01],\n",
      "          [-1.5618e-01,  1.4537e-01,  1.1162e-01,  ...,  1.5673e+00,\n",
      "            4.6217e-02,  6.3518e-01],\n",
      "          [ 5.1079e-02, -1.5703e-04,  2.2588e-01,  ...,  1.0759e+00,\n",
      "            4.4574e-01,  4.6231e-01]],\n",
      "\n",
      "         [[ 8.4708e-01,  5.8427e-01, -9.9537e-02,  ..., -2.4637e-01,\n",
      "            1.6276e+00, -1.1307e+00],\n",
      "          [ 1.0113e+00,  7.0108e-01, -5.7364e-01,  ..., -7.1721e-01,\n",
      "            1.0731e+00, -1.0718e+00],\n",
      "          [ 1.3848e+00,  7.6479e-01, -2.7641e-01,  ..., -7.5346e-01,\n",
      "            7.2735e-01, -7.2647e-01],\n",
      "          [ 1.2975e+00,  1.5041e-01, -7.4808e-01,  ..., -1.0979e+00,\n",
      "            8.4370e-01, -2.0004e-01],\n",
      "          [ 1.5889e+00,  6.1555e-01, -5.5497e-01,  ..., -1.1088e+00,\n",
      "            1.1013e+00,  3.8578e-01],\n",
      "          [ 8.8399e-01,  1.3741e-01, -4.4718e-01,  ..., -6.9477e-01,\n",
      "            7.6977e-01,  3.6453e-01]],\n",
      "\n",
      "         [[ 5.8090e-01, -2.2634e-01,  3.4507e-01,  ..., -2.7921e-02,\n",
      "            5.2287e-01,  1.9549e+00],\n",
      "          [-5.6619e-01,  7.8687e-01,  2.5152e-02,  ...,  6.2100e-01,\n",
      "            4.7592e-01,  5.4321e-01],\n",
      "          [-5.8122e-01,  2.4762e-01, -7.2944e-01,  ..., -3.4239e-01,\n",
      "            2.5028e-01,  1.7674e+00],\n",
      "          [ 1.0690e+00, -3.5949e-01,  5.2123e-02,  ...,  2.1185e-01,\n",
      "            8.3326e-01,  1.4919e+00],\n",
      "          [ 1.0157e-01,  1.0315e-01, -2.5254e-01,  ..., -1.5059e-01,\n",
      "            8.0531e-01,  8.4089e-01],\n",
      "          [-1.1231e-01,  1.0899e+00,  1.4645e+00,  ..., -7.6389e-01,\n",
      "            1.2188e-01,  1.0324e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 1.6370e-01,  2.0417e-01, -4.3643e-02,  ...,  2.2554e-04,\n",
      "           -9.9765e-02,  1.4590e-02],\n",
      "          [-7.9977e-02,  1.8136e-02, -5.3423e-02,  ..., -4.1919e-02,\n",
      "           -3.6505e-02,  1.5149e-02],\n",
      "          [-1.6302e-01,  4.2686e-02,  4.1064e-02,  ...,  1.7301e-01,\n",
      "           -1.7494e-01, -1.0125e-01],\n",
      "          [ 2.8571e-01, -9.2335e-02,  2.7128e-01,  ..., -9.4528e-02,\n",
      "            1.3293e-01, -3.4941e-02],\n",
      "          [-6.5246e-03,  2.9754e-01, -5.4465e-02,  ...,  1.8581e-01,\n",
      "           -1.9225e-01,  3.4729e-02],\n",
      "          [-5.2062e-02,  6.0722e-02, -2.2644e-01,  ...,  1.5005e-01,\n",
      "           -2.8577e-01, -2.3280e-02]],\n",
      "\n",
      "         [[ 3.1018e-01,  1.1781e-01, -8.8173e-02,  ..., -6.1718e-01,\n",
      "           -4.0972e-01,  2.4498e-01],\n",
      "          [ 5.9258e-01,  1.9729e-02,  1.1069e-01,  ...,  1.2530e-01,\n",
      "            5.6754e-01, -2.6653e-01],\n",
      "          [ 4.9729e-01,  3.3188e-02, -5.8654e-02,  ...,  1.2960e-01,\n",
      "            4.8174e-01,  4.8487e-02],\n",
      "          [ 5.3861e-01,  9.5785e-02,  1.0208e-01,  ..., -6.0225e-02,\n",
      "            3.5903e-01, -2.8992e-02],\n",
      "          [ 5.2733e-01, -1.4000e-01, -1.4172e-01,  ...,  1.8146e-02,\n",
      "            2.2020e-01, -1.8636e-01],\n",
      "          [ 4.0171e-01,  3.0647e-02, -4.2317e-02,  ..., -1.8194e-01,\n",
      "            5.6371e-01, -2.4747e-01]],\n",
      "\n",
      "         [[ 2.1912e-01,  8.6628e-03,  7.3504e-02,  ...,  7.9072e-02,\n",
      "           -4.7960e-02,  9.4132e-02],\n",
      "          [-3.2565e-01,  3.9756e-02, -1.5311e-01,  ...,  4.1103e-02,\n",
      "           -4.1338e-02,  7.4518e-02],\n",
      "          [-1.2920e-01,  2.8070e-02, -3.0717e-02,  ...,  6.5625e-03,\n",
      "           -4.3846e-02,  2.0105e-02],\n",
      "          [ 1.7685e-01,  1.4861e-01,  2.9220e-01,  ..., -8.7607e-02,\n",
      "            2.2636e-02,  1.2196e-01],\n",
      "          [-3.4979e-01,  1.9469e-02, -1.3165e-01,  ..., -1.6013e-01,\n",
      "           -2.6387e-01, -1.7483e-01],\n",
      "          [-2.3146e-01,  2.0534e-01, -7.4261e-02,  ..., -9.0841e-03,\n",
      "           -6.5482e-02,  1.9889e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.4412e-02,  2.3964e-02,  3.1744e-02,  ...,  3.1589e-03,\n",
      "            1.5856e-01, -9.6892e-02],\n",
      "          [-3.0222e-01, -8.7826e-02,  1.0015e-01,  ...,  2.6181e-02,\n",
      "           -1.6474e-01,  1.6819e-01],\n",
      "          [-1.4775e-01,  5.1046e-01,  3.9396e-01,  ...,  7.3127e-02,\n",
      "            1.2692e-01,  2.8657e-01],\n",
      "          [-2.1522e-01,  3.8017e-01,  1.3313e-01,  ...,  1.0312e-01,\n",
      "           -4.1694e-01,  7.1458e-02],\n",
      "          [-3.3188e-01,  3.1851e-01,  2.3579e-01,  ..., -3.3326e-01,\n",
      "           -2.6072e-01, -4.1366e-01],\n",
      "          [ 5.2134e-03,  3.3071e-01,  9.9854e-02,  ..., -5.8137e-02,\n",
      "            2.2529e-02,  1.8651e-01]],\n",
      "\n",
      "         [[ 9.3921e-02, -2.4730e-01, -7.8721e-02,  ...,  2.7543e-02,\n",
      "            2.1350e-02,  2.5469e-02],\n",
      "          [-2.1188e-01,  5.8883e-02,  9.9730e-02,  ...,  3.8309e-03,\n",
      "            1.3309e-01,  9.2965e-02],\n",
      "          [-3.7033e-01,  1.3604e-01,  3.1448e-01,  ...,  1.1130e-01,\n",
      "            1.2622e-01, -1.7270e-01],\n",
      "          [-1.1910e-01,  9.2157e-02, -1.0433e-02,  ..., -3.3878e-01,\n",
      "           -2.0214e-01, -1.5165e-02],\n",
      "          [-5.3600e-02, -1.6348e-01, -3.5842e-01,  ...,  1.3920e-02,\n",
      "           -2.9193e-01,  3.1119e-01],\n",
      "          [-6.5717e-02,  1.2619e-01, -6.2645e-03,  ..., -2.7734e-02,\n",
      "            5.5060e-02, -8.9395e-02]],\n",
      "\n",
      "         [[ 2.9328e-02, -2.9549e-01,  1.5860e-01,  ..., -5.7903e-02,\n",
      "           -8.4012e-02,  7.8738e-02],\n",
      "          [ 5.3756e-02, -2.6476e-01, -1.4650e-02,  ...,  2.3310e-01,\n",
      "            5.1628e-02,  9.2365e-02],\n",
      "          [ 1.5824e-01,  2.9164e-03,  4.3382e-02,  ...,  1.4230e-01,\n",
      "           -3.3537e-01,  1.3201e-01],\n",
      "          [ 3.2188e-02, -1.2416e-01, -9.1354e-02,  ...,  1.7554e-01,\n",
      "            3.6295e-01,  5.3478e-02],\n",
      "          [ 3.8047e-02,  1.1388e-01, -1.8473e-01,  ...,  4.8030e-02,\n",
      "            1.0704e-01,  1.9659e-01],\n",
      "          [ 2.6312e-01, -4.0186e-02, -9.4521e-03,  ...,  5.5359e-02,\n",
      "            2.6957e-01, -2.4410e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.3402,  1.8317, -1.5569,  ...,  1.5395, -1.1796,  1.0644],\n",
      "          [ 0.7954,  1.4768, -1.5406,  ..., -0.3858, -1.6645,  0.4216],\n",
      "          [ 1.1085,  2.2798, -1.7136,  ...,  0.0640, -2.9470, -0.8405],\n",
      "          [ 0.4314,  2.7402, -0.9148,  ..., -0.2440, -1.9174, -0.2221],\n",
      "          [ 0.7380,  0.7445, -0.7913,  ..., -0.2719, -2.3468, -2.0087],\n",
      "          [ 0.6466,  1.1727, -1.0653,  ...,  0.0563, -0.7189, -0.0296]],\n",
      "\n",
      "         [[-0.9977, -0.4407, -0.6234,  ..., -0.0598,  0.7957, -0.5177],\n",
      "          [-0.4142,  0.2905, -0.8829,  ..., -0.8130,  0.1361, -0.1898],\n",
      "          [-0.4540,  0.2372, -1.4545,  ..., -0.2138,  0.0725, -0.7007],\n",
      "          [-0.8936,  1.0705, -1.7986,  ..., -0.3505,  0.3356, -1.1933],\n",
      "          [-0.4515,  0.0762, -1.6119,  ..., -0.0790, -0.5116, -0.6816],\n",
      "          [-0.8208,  1.3468, -1.4197,  ..., -0.2505, -0.1711, -1.1098]],\n",
      "\n",
      "         [[ 0.3695,  0.0797, -0.1394,  ..., -1.3083,  0.2405, -0.1721],\n",
      "          [-0.1374,  0.2415, -0.1167,  ..., -1.1328,  0.0718,  0.4941],\n",
      "          [ 0.5506, -0.3361, -0.5803,  ..., -0.9073,  0.0302,  0.0684],\n",
      "          [-0.2653, -0.0281,  0.0565,  ..., -0.9269, -0.0510, -0.0086],\n",
      "          [-0.0210,  0.2274, -0.1961,  ..., -0.6705,  0.2895,  0.4709],\n",
      "          [-0.3524,  0.0122, -0.4411,  ..., -1.2450,  0.1464,  0.2832]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1656, -0.2220, -0.6722,  ..., -0.7757,  0.4268, -0.7472],\n",
      "          [-0.3245,  1.7041,  1.7332,  ...,  0.4523, -0.4601, -1.0420],\n",
      "          [-0.2865,  0.6710,  1.6160,  ..., -0.7753, -0.2038, -1.1022],\n",
      "          [-0.2834,  2.2301,  1.2735,  ..., -0.5639, -0.6585,  0.6571],\n",
      "          [ 0.0389, -0.0634,  1.5486,  ...,  1.1367, -0.2608, -0.2710],\n",
      "          [-0.4400,  1.9759,  1.8586,  ...,  1.4860, -1.7525,  0.9642]],\n",
      "\n",
      "         [[-1.1891, -2.7560,  0.1025,  ...,  1.7293,  1.5524, -1.4357],\n",
      "          [ 0.2700,  0.7502, -0.4550,  ..., -0.8754,  0.4548, -0.3154],\n",
      "          [ 0.0994, -0.2623, -0.3735,  ..., -0.9117,  0.5579,  0.2542],\n",
      "          [-0.0551,  0.5813, -0.5940,  ..., -0.8912,  0.6713, -0.5435],\n",
      "          [ 0.0447,  0.5673, -0.6536,  ..., -0.3949,  0.7482,  0.1007],\n",
      "          [-0.1506,  0.6344, -0.6214,  ..., -0.1479,  0.6337, -0.1598]],\n",
      "\n",
      "         [[ 0.9919,  1.7035,  1.2135,  ..., -0.6806, -0.2436,  0.4080],\n",
      "          [ 0.5901,  2.1134,  0.9779,  ...,  0.1037, -0.5882, -0.2048],\n",
      "          [ 0.8782,  3.1403,  1.1713,  ...,  2.1995, -1.9104, -1.1398],\n",
      "          [ 0.6831,  2.0809, -0.7867,  ...,  0.6602,  0.1609, -1.0834],\n",
      "          [-0.2144,  3.0714,  0.6599,  ...,  0.7450, -1.1553,  0.0864],\n",
      "          [ 0.2272,  2.6181,  0.2739,  ..., -0.3740, -0.1209,  0.8294]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 4.6785e-01, -2.3693e-02,  1.2196e-02,  ...,  1.1360e-01,\n",
      "            1.7572e-02, -1.8815e-01],\n",
      "          [ 3.3626e-01,  4.8007e-01,  8.1317e-02,  ..., -1.3706e-01,\n",
      "           -2.0060e-01,  8.8067e-02],\n",
      "          [ 1.2042e+00, -5.5395e-01, -9.5782e-01,  ..., -1.8259e-01,\n",
      "            3.6053e-01,  9.2639e-01],\n",
      "          [-6.2213e-02,  1.0257e-01,  1.6138e-02,  ..., -2.2351e-01,\n",
      "           -5.5273e-01,  1.9735e-03],\n",
      "          [-1.4437e-01,  5.2892e-01,  4.8293e-01,  ...,  1.2194e-01,\n",
      "            4.8984e-01, -1.8818e-01],\n",
      "          [ 6.3028e-01,  9.3127e-02,  1.3669e-02,  ..., -3.1604e-01,\n",
      "           -7.1699e-02,  7.6013e-02]],\n",
      "\n",
      "         [[-1.1534e-01, -1.2627e-01, -3.8525e-02,  ..., -8.1216e-02,\n",
      "           -5.9342e-01,  1.0751e-01],\n",
      "          [-1.8686e-02, -1.2091e-01,  7.3735e-02,  ..., -2.3330e-01,\n",
      "            4.2565e-01, -1.7145e-01],\n",
      "          [ 4.5285e-01, -7.4433e-01, -3.6006e-01,  ..., -1.6384e-01,\n",
      "            5.7154e-02, -2.3725e-01],\n",
      "          [ 4.2463e-01,  5.8520e-01,  7.6072e-01,  ..., -2.5842e-02,\n",
      "            1.5895e-02,  2.5775e-01],\n",
      "          [ 1.8859e-01,  1.6432e-01, -2.4931e-01,  ...,  1.0404e+00,\n",
      "           -5.2631e-01, -8.0402e-01],\n",
      "          [-7.3159e-02, -8.9343e-02,  3.3729e-01,  ...,  4.2387e-01,\n",
      "            1.9203e-01, -8.0758e-02]],\n",
      "\n",
      "         [[ 6.4340e-02,  5.3017e-02, -2.3609e-01,  ..., -5.6612e-01,\n",
      "           -1.2379e-01, -6.8463e-02],\n",
      "          [ 3.5182e-01,  2.1094e-01,  3.0733e-01,  ..., -4.4855e-01,\n",
      "            4.5943e-02, -5.9989e-02],\n",
      "          [ 5.7725e-01, -3.8840e-02,  4.4132e-04,  ..., -8.4918e-01,\n",
      "           -9.4822e-02,  3.5087e-01],\n",
      "          [ 6.4304e-01,  2.2108e-01,  2.7657e-01,  ..., -7.8514e-01,\n",
      "           -5.6122e-03,  7.7701e-02],\n",
      "          [ 5.0609e-01,  5.5124e-02,  3.6511e-01,  ..., -6.1294e-01,\n",
      "            6.6360e-03, -1.5552e-01],\n",
      "          [ 5.5756e-01,  5.3863e-02,  3.3427e-01,  ..., -4.3717e-01,\n",
      "            2.7082e-01, -1.9211e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6298e-02,  6.0437e-01,  5.8276e-02,  ..., -2.2462e-02,\n",
      "           -1.0028e+00,  9.1905e-02],\n",
      "          [ 3.2396e-02, -7.1819e-01,  2.4991e-01,  ..., -5.9478e-02,\n",
      "           -9.1155e-01, -3.8558e-01],\n",
      "          [ 1.3894e-01,  2.5915e-01, -9.3391e-02,  ...,  9.6248e-02,\n",
      "           -5.1495e-01,  2.7013e-04],\n",
      "          [ 2.7589e-01, -2.2023e-01, -4.4407e-02,  ..., -8.3761e-02,\n",
      "           -2.4893e-01, -1.9303e-01],\n",
      "          [-6.7641e-02,  2.1486e-01,  3.8270e-03,  ..., -1.0838e-01,\n",
      "           -4.6731e-01, -3.7356e-01],\n",
      "          [-1.5020e-01, -8.7327e-01,  3.8257e-01,  ..., -8.2359e-02,\n",
      "           -7.1953e-01, -3.4287e-01]],\n",
      "\n",
      "         [[-1.7745e-03,  2.8265e-03, -4.0687e-03,  ...,  4.4747e-01,\n",
      "           -3.4960e+00,  5.9942e-02],\n",
      "          [ 5.9607e-01,  1.0692e-01,  1.8051e-01,  ...,  3.7004e-01,\n",
      "            8.5842e-02, -3.7256e-01],\n",
      "          [-1.1522e-01, -1.5204e-01, -2.4652e-01,  ..., -6.0661e-01,\n",
      "            1.4663e-01,  1.4084e-01],\n",
      "          [-1.0198e-01, -1.0169e-01,  1.6441e-01,  ...,  2.1809e-01,\n",
      "            9.6716e-02,  2.7747e-01],\n",
      "          [-1.1171e-01,  3.4216e-01,  3.0188e-01,  ..., -8.4586e-02,\n",
      "           -2.9355e-02,  2.3744e-01],\n",
      "          [ 7.6999e-03,  2.2737e-01,  7.6289e-03,  ...,  7.1056e-02,\n",
      "           -3.3470e-02,  2.5660e-03]],\n",
      "\n",
      "         [[ 1.5091e-01, -1.6054e-01,  5.3061e-02,  ..., -2.1581e-01,\n",
      "            2.1932e-01, -9.5636e-02],\n",
      "          [ 1.8136e-01, -3.2733e-01,  3.2247e-01,  ...,  3.8879e-02,\n",
      "            4.7582e-01,  2.7608e-01],\n",
      "          [-2.1411e-01, -3.0458e-01, -9.1755e-02,  ...,  2.7638e-02,\n",
      "            3.6478e-01,  6.3749e-02],\n",
      "          [-2.4508e-01, -5.7983e-02, -6.3300e-02,  ...,  9.5803e-04,\n",
      "            2.8208e-01,  1.7187e-01],\n",
      "          [-1.0435e-01, -5.2406e-02,  8.8303e-03,  ...,  9.8834e-02,\n",
      "            4.3689e-01,  2.6966e-01],\n",
      "          [ 3.6693e-01, -3.5106e-01,  2.2283e-01,  ...,  2.6000e-02,\n",
      "            4.8079e-01,  2.6730e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.1468, -1.1417,  0.2784,  ..., -0.6530, -0.1291,  0.0646],\n",
      "          [ 0.1918, -2.3132, -0.2449,  ...,  0.2418, -0.0920, -0.6920],\n",
      "          [-0.1616, -1.9880,  1.4026,  ..., -0.2021, -0.5790, -0.7472],\n",
      "          [ 1.0243, -1.7423, -2.1353,  ..., -0.0332, -0.0307, -2.3150],\n",
      "          [ 0.5858, -2.3361,  0.7079,  ...,  0.4739,  0.5187, -0.7109],\n",
      "          [ 0.1918, -3.3725, -0.5204,  ...,  1.6422,  0.4719, -0.8609]],\n",
      "\n",
      "         [[-0.4749,  0.3806, -0.4885,  ...,  1.1499, -0.6422, -0.5448],\n",
      "          [-1.2043, -0.3048, -1.5301,  ...,  0.4245,  0.3242, -0.5798],\n",
      "          [-1.5919, -1.8479, -0.9560,  ..., -0.2323,  1.0129, -0.6553],\n",
      "          [-1.8294, -0.5658, -2.4046,  ...,  0.3558,  0.8673,  0.2852],\n",
      "          [-2.4505, -1.6102, -1.4719,  ..., -0.9092,  1.6145, -0.2606],\n",
      "          [-1.4930, -1.1774, -2.0766,  ..., -0.3803,  2.1475, -0.6684]],\n",
      "\n",
      "         [[ 1.3267,  3.0613,  3.7465,  ...,  0.6225,  1.6899, -0.7906],\n",
      "          [-3.2611,  2.0092, -3.0073,  ..., -1.6761,  3.2044,  0.6158],\n",
      "          [-3.8642,  2.3305, -3.6757,  ..., -2.8016,  2.6351,  0.4319],\n",
      "          [-4.7247,  1.1412, -3.9936,  ..., -3.4544,  3.7409,  1.2913],\n",
      "          [-4.0154,  0.5109, -3.6685,  ..., -2.6295,  3.9950,  0.9950],\n",
      "          [-2.9584, -1.1990, -3.4255,  ..., -3.7253,  3.2595,  0.2249]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3558, -2.7361, -2.6119,  ...,  0.9535,  0.4313,  2.7028],\n",
      "          [-2.4928,  1.7280, -0.4796,  ...,  0.2630, -2.2598,  0.3448],\n",
      "          [-2.3450,  2.5602,  0.8098,  ..., -1.0825, -2.5803,  0.3561],\n",
      "          [-2.1660,  3.4065,  1.2194,  ..., -1.0278, -3.7919, -0.1340],\n",
      "          [-2.8730,  2.6903,  2.0310,  ..., -1.3000, -2.5446, -0.5607],\n",
      "          [-3.0796,  3.2369,  1.1364,  ..., -0.4490, -2.4357, -0.2307]],\n",
      "\n",
      "         [[ 1.7297,  0.4658,  0.9255,  ..., -0.0076, -0.9971, -0.2999],\n",
      "          [ 2.1909,  0.7616,  0.9752,  ...,  0.4325, -2.6768, -1.0409],\n",
      "          [ 2.1149,  0.6125,  1.4676,  ...,  0.3496, -1.8876, -1.2577],\n",
      "          [ 2.1509,  0.6281,  0.5486,  ..., -0.0920, -1.4829, -1.1661],\n",
      "          [ 2.0670,  0.6819,  1.1094,  ..., -0.1562, -1.8086, -1.0191],\n",
      "          [ 1.6784,  0.4459,  0.8882,  ..., -0.2395, -1.7092, -0.6712]],\n",
      "\n",
      "         [[-0.2337,  0.1082, -0.5605,  ...,  0.3673,  0.2845,  0.2085],\n",
      "          [-0.5986,  0.4382, -0.2240,  ...,  0.5468,  0.8883,  0.0107],\n",
      "          [-0.3466,  0.4814,  0.0826,  ..., -0.8563,  0.1976, -0.0786],\n",
      "          [-0.3881,  0.0457, -0.0929,  ..., -0.0547,  1.1418,  0.0912],\n",
      "          [ 0.0805,  0.0727, -0.5156,  ..., -0.3248,  0.2139,  0.4351],\n",
      "          [-0.9261,  0.3059, -0.7343,  ...,  0.2604,  0.7221,  1.0592]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-4.5336e-02,  9.6407e-03, -1.6183e-01,  ..., -4.4611e-02,\n",
      "            1.0405e-02, -5.8415e-01],\n",
      "          [-2.6726e-01,  3.6173e-01, -7.5621e-02,  ...,  5.7353e-01,\n",
      "           -2.9147e-01,  3.2123e-01],\n",
      "          [ 8.4974e-01, -2.4287e-01,  1.0062e-01,  ...,  5.0554e-01,\n",
      "           -8.6270e-01,  8.2158e-01],\n",
      "          [ 4.1812e-01, -3.0578e-01, -1.2153e-01,  ..., -1.2177e+00,\n",
      "           -1.8812e-01,  2.2712e-01],\n",
      "          [ 1.8474e-01,  1.3263e-01, -8.7881e-01,  ..., -5.6280e-01,\n",
      "           -1.7192e-02,  4.9957e-01],\n",
      "          [-3.6618e-03,  8.4999e-01, -1.4421e+00,  ...,  6.9051e-01,\n",
      "            3.4190e-01, -7.2155e-01]],\n",
      "\n",
      "         [[ 3.1740e-02, -2.6663e-02, -2.7226e-02,  ..., -2.3272e-02,\n",
      "            3.2426e-02, -6.5052e-03],\n",
      "          [-1.1425e-01, -1.5177e-01, -5.1865e-02,  ..., -1.6537e-01,\n",
      "           -9.5789e-02,  2.8178e-01],\n",
      "          [ 1.7299e-01,  5.8314e-01, -6.4263e-02,  ..., -7.1785e-02,\n",
      "           -9.2849e-01,  6.1726e-01],\n",
      "          [ 7.8277e-02,  1.4699e-01,  1.7325e-01,  ..., -5.6800e-03,\n",
      "            2.2045e-01,  1.3289e-01],\n",
      "          [ 8.4062e-01,  8.5272e-01,  4.2804e-01,  ..., -3.4754e-01,\n",
      "           -4.5867e-01,  7.4438e-01],\n",
      "          [ 2.6405e-01,  6.9919e-01, -6.6678e-01,  ..., -8.0274e-02,\n",
      "            1.3879e-01, -2.4597e-01]],\n",
      "\n",
      "         [[ 2.2670e-02, -7.7913e-01, -8.1938e-03,  ...,  3.0014e-02,\n",
      "            2.6048e-02, -2.9026e-02],\n",
      "          [ 2.8162e-01, -1.1074e+00,  2.0006e-01,  ...,  4.3368e-02,\n",
      "           -3.5952e-01, -8.6830e-02],\n",
      "          [ 2.8351e-02, -1.2881e+00, -8.4156e-01,  ..., -1.5629e-01,\n",
      "           -4.5506e-01, -8.9236e-02],\n",
      "          [-2.9490e-01, -1.2795e+00,  3.3148e-01,  ...,  9.0959e-01,\n",
      "           -2.9383e-01, -1.6761e-02],\n",
      "          [-1.1026e-01, -5.5193e-01,  9.0357e-01,  ..., -2.9621e-03,\n",
      "            7.6871e-01,  3.7327e-02],\n",
      "          [ 5.1721e-01, -1.5546e+00,  4.7849e-02,  ..., -3.5922e-01,\n",
      "            7.1847e-02,  8.0462e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5805e-02, -1.0197e-01,  1.3313e+00,  ..., -5.5086e-02,\n",
      "            1.5039e-01,  4.9681e-03],\n",
      "          [-3.4440e-01, -6.2666e-02,  2.4486e+00,  ..., -8.8877e-02,\n",
      "           -7.2977e-02,  1.5340e-01],\n",
      "          [ 3.9131e-01, -2.7070e-01,  2.1670e+00,  ..., -8.8468e-02,\n",
      "           -1.5152e-01,  2.3040e-01],\n",
      "          [ 5.2167e-01, -2.2402e-01,  2.0342e+00,  ...,  5.5627e-01,\n",
      "           -1.4025e-02,  2.8537e-01],\n",
      "          [-2.4821e-01, -3.8553e-01,  2.1324e+00,  ...,  2.3871e-01,\n",
      "           -4.5880e-01,  4.8515e-01],\n",
      "          [ 3.4568e-01, -7.5999e-01,  2.1024e+00,  ...,  2.8025e-01,\n",
      "           -1.0990e-02,  5.3561e-01]],\n",
      "\n",
      "         [[ 3.5587e-02, -1.0390e-01, -1.7228e-01,  ...,  1.4131e-01,\n",
      "            8.4269e-02,  1.8731e-01],\n",
      "          [ 3.5700e-01,  7.2507e-01,  6.3967e-01,  ..., -4.2103e-01,\n",
      "           -1.0334e+00, -4.6468e-01],\n",
      "          [-2.0736e-01, -2.6267e-01,  4.2389e-01,  ...,  2.1392e-01,\n",
      "           -2.6014e-01, -6.6457e-01],\n",
      "          [ 6.4488e-01, -5.2301e-01, -2.4072e-01,  ...,  2.1679e-01,\n",
      "           -1.1422e+00, -1.6608e-01],\n",
      "          [ 1.1211e+00, -1.6244e-01, -4.0167e-01,  ...,  1.9202e-01,\n",
      "           -1.6374e-01, -8.0352e-02],\n",
      "          [ 1.3408e+00,  1.0193e-02,  3.4560e-01,  ..., -5.6590e-01,\n",
      "           -6.5302e-01,  7.6592e-02]],\n",
      "\n",
      "         [[ 9.1214e-04,  1.1556e-02,  1.7982e-02,  ..., -1.8948e-02,\n",
      "            2.0190e-01, -6.2179e-03],\n",
      "          [ 2.0687e-01, -1.1427e-01,  6.3318e-01,  ..., -4.1677e-01,\n",
      "           -2.1873e+00,  1.3542e-01],\n",
      "          [-1.1043e-01, -1.0298e-01, -2.5394e-01,  ...,  1.5674e-01,\n",
      "           -1.8298e+00,  4.6668e-01],\n",
      "          [ 2.2730e-01, -7.1004e-01,  9.4812e-02,  ..., -1.1156e-01,\n",
      "           -1.9411e+00,  3.5786e-01],\n",
      "          [-2.2772e-01, -4.4977e-02, -3.1443e-01,  ...,  1.4839e-01,\n",
      "           -2.0336e+00,  2.3192e-01],\n",
      "          [ 7.0907e-01, -1.6734e-01,  3.4073e-02,  ...,  3.1513e-02,\n",
      "           -1.3117e+00, -4.8863e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0329, -0.2086,  0.1469,  ..., -0.8888,  0.7214, -1.1974],\n",
      "          [-0.3355, -0.2304, -0.2588,  ...,  0.2081, -0.2319,  0.4981],\n",
      "          [-1.6389, -1.6659, -0.5793,  ...,  1.2120,  1.2197,  2.2757],\n",
      "          [-1.4736, -0.1723, -1.3369,  ...,  0.8103, -0.3617,  0.9767],\n",
      "          [-1.5990,  0.9107,  0.3550,  ...,  1.4372, -0.1416,  0.0170],\n",
      "          [ 0.2046, -0.6748,  0.6358,  ...,  1.9473,  0.4790,  0.3683]],\n",
      "\n",
      "         [[ 0.7873,  0.1899,  0.0099,  ..., -0.1754, -1.0923, -0.2059],\n",
      "          [-1.2552, -1.6313, -0.5666,  ...,  1.5932,  4.1790,  1.0747],\n",
      "          [-0.2016, -1.8342,  1.2797,  ...,  0.3371,  5.2294,  2.1299],\n",
      "          [ 0.3575, -1.9340,  0.4470,  ...,  1.9096,  5.1970,  1.4563],\n",
      "          [-0.5657, -1.6942,  0.2235,  ...,  2.3341,  4.9721,  1.9703],\n",
      "          [-1.8255,  0.0255,  0.1650,  ..., -0.1246,  4.5301,  1.7641]],\n",
      "\n",
      "         [[ 0.3303, -0.3448, -0.3340,  ...,  0.3370,  1.4597,  0.2715],\n",
      "          [ 0.2782, -5.3686, -0.5382,  ..., -2.6507, -1.7305, -4.9019],\n",
      "          [ 0.7435, -7.1266, -2.1807,  ..., -4.7173, -3.3575, -4.1671],\n",
      "          [ 0.1379, -6.9245, -2.1891,  ..., -4.2477, -2.7924, -5.2530],\n",
      "          [-1.3687, -6.0385, -3.0723,  ..., -2.6760, -2.3656, -5.5222],\n",
      "          [-1.9541, -6.7979, -2.1729,  ..., -3.4382, -1.0491, -5.7555]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2209,  1.7799,  0.5240,  ...,  0.2592,  0.4459, -1.6863],\n",
      "          [-0.4026, -5.6205,  0.6185,  ..., -2.9235, -1.6064,  6.0061],\n",
      "          [ 0.4911, -9.1722,  1.3437,  ..., -1.9353, -2.3736,  6.7886],\n",
      "          [ 0.9032, -7.2658,  1.8981,  ..., -3.7767, -1.7222,  5.3179],\n",
      "          [-1.1552, -6.9096,  1.2491,  ..., -2.9295, -1.3703,  7.5339],\n",
      "          [ 3.0752, -8.2347,  2.0932,  ..., -1.6237, -0.8048,  4.8053]],\n",
      "\n",
      "         [[ 0.0677, -0.0320,  0.1251,  ..., -0.0986, -0.1042, -0.1573],\n",
      "          [ 0.8447, -0.6005, -0.1204,  ..., -0.5193, -0.4075, -0.2353],\n",
      "          [ 1.0912, -1.1106,  0.4242,  ..., -1.6636, -1.2502, -1.3925],\n",
      "          [-1.9216, -0.5387, -0.2596,  ..., -1.3589,  0.3904, -1.1890],\n",
      "          [-0.1333, -0.7122,  0.2963,  ..., -0.3385, -0.2019, -0.6987],\n",
      "          [-0.9832,  0.0709, -0.2282,  ..., -1.4533,  0.3499, -0.6434]],\n",
      "\n",
      "         [[ 0.3927, -0.0800,  1.9078,  ..., -0.2344, -0.2369, -0.9911],\n",
      "          [ 2.3276,  1.7537, -2.5923,  ...,  0.2305,  0.9426,  3.3243],\n",
      "          [ 3.2743,  2.9494, -3.0666,  ...,  2.6028,  1.5206,  4.5154],\n",
      "          [ 3.3012,  3.5631, -2.9283,  ...,  0.8249,  0.8828,  3.8810],\n",
      "          [ 4.6128,  1.4668, -3.3742,  ...,  3.2634, -0.3082,  4.0666],\n",
      "          [ 3.2950,  3.2178, -3.0605,  ...,  0.8046,  0.8621,  3.6154]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0430,  0.0631, -0.0082,  ...,  0.0164,  0.1050,  0.0366],\n",
      "          [ 0.3635, -0.6704,  0.3900,  ..., -0.1125, -0.6919,  0.4529],\n",
      "          [-0.5523, -0.7695, -0.2733,  ...,  0.4514, -0.5151, -0.9199],\n",
      "          [-0.1966, -0.9459, -0.6886,  ..., -0.3470, -1.0298, -0.3257],\n",
      "          [ 0.2832, -0.3113,  0.2044,  ...,  0.2456, -1.4125,  0.3405],\n",
      "          [ 0.4500, -0.6885,  0.4344,  ..., -0.0180, -0.9159,  0.6563]],\n",
      "\n",
      "         [[-0.0500, -0.0119,  0.0846,  ..., -0.0410, -0.0277, -0.0497],\n",
      "          [ 0.5273, -0.4573, -0.2496,  ...,  0.4353,  0.0435,  0.1234],\n",
      "          [ 0.8252, -0.3066, -0.0806,  ...,  0.1530,  0.1790,  0.3157],\n",
      "          [ 0.4221,  0.4986, -0.7711,  ..., -0.1317,  0.2642,  0.2447],\n",
      "          [ 0.0796,  0.0469, -0.6243,  ...,  0.0023,  0.3690, -0.1310],\n",
      "          [ 0.0116, -0.1897,  0.0503,  ...,  0.1929,  0.7005,  0.4224]],\n",
      "\n",
      "         [[ 0.0407, -0.1094, -0.0615,  ..., -0.0139,  0.0929, -0.1576],\n",
      "          [ 0.2668,  0.0511, -0.4790,  ...,  0.4212,  0.0666, -0.7940],\n",
      "          [-0.0635, -0.0942,  0.0469,  ..., -0.1478, -0.8738, -0.2883],\n",
      "          [-0.4497, -0.2151, -0.6259,  ..., -0.3970, -0.1842,  0.1433],\n",
      "          [ 0.1324,  0.2085,  0.5788,  ...,  0.2944,  0.0065,  0.3102],\n",
      "          [ 0.0109,  0.4453, -0.5656,  ..., -0.3386, -0.4160,  0.2016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0155,  0.1213, -0.0050,  ..., -0.0212,  0.0726, -0.0505],\n",
      "          [ 0.0746,  0.2854,  0.1149,  ...,  0.1233,  0.0843,  0.1644],\n",
      "          [ 0.4461,  0.2087,  0.8276,  ..., -0.0300,  0.5023,  1.0498],\n",
      "          [-0.0276, -0.3113,  1.0554,  ...,  0.5771, -0.4329,  0.1640],\n",
      "          [-0.0894,  0.4036, -0.6338,  ..., -0.7977,  0.0689, -0.1204],\n",
      "          [ 0.7815, -0.3690,  0.7982,  ...,  0.9606, -0.1556, -0.9144]],\n",
      "\n",
      "         [[-0.1744, -0.1322, -0.0783,  ..., -0.2510, -0.0232, -0.0406],\n",
      "          [ 0.2377,  0.2127,  0.0206,  ..., -0.4094,  0.3724,  0.4014],\n",
      "          [ 0.7115, -0.7792, -0.9787,  ...,  0.4738, -0.7153, -0.8276],\n",
      "          [ 0.1785,  0.5513, -0.2997,  ...,  0.9617, -0.5259, -0.1212],\n",
      "          [-0.1747, -0.3119, -0.1532,  ...,  0.3343,  0.4421,  0.5256],\n",
      "          [-0.1999,  0.4021,  0.3165,  ...,  0.3315,  0.1183,  0.2113]],\n",
      "\n",
      "         [[ 0.1164, -0.0776, -0.0223,  ..., -0.0158, -0.0992, -0.0942],\n",
      "          [ 0.7027,  0.0858,  0.0512,  ..., -0.0577, -0.1619,  0.1204],\n",
      "          [ 0.6901,  0.2302,  0.6828,  ...,  0.3321,  0.1368, -0.3858],\n",
      "          [-0.1139,  0.3702,  0.6199,  ..., -0.0670, -0.7206, -0.0014],\n",
      "          [ 0.9509,  0.7765, -0.4105,  ...,  0.2263,  0.4432, -1.0887],\n",
      "          [ 0.4989, -0.0908, -0.0564,  ...,  0.1490, -0.0376,  0.4165]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[-8.9242e-01, -1.4676e-01,  3.3058e-01,  ..., -9.6168e-01,\n",
      "            3.9738e-02, -2.9296e+00],\n",
      "          [ 1.7615e+00, -8.9097e-01, -3.3031e+00,  ..., -1.8356e+00,\n",
      "           -1.2090e+00,  7.7353e+00],\n",
      "          [ 1.4057e+00, -1.4066e+00, -2.8636e+00,  ..., -8.8692e-01,\n",
      "           -1.8838e+00,  7.8074e+00],\n",
      "          [ 6.6742e-01, -4.8253e-01, -3.6573e+00,  ..., -1.6266e-01,\n",
      "           -3.5333e+00,  7.7334e+00],\n",
      "          [ 1.8312e+00,  5.4008e-01, -3.1227e+00,  ..., -1.0031e+00,\n",
      "           -3.0040e+00,  8.8673e+00],\n",
      "          [ 9.7270e-01, -2.6792e-01, -2.8045e+00,  ..., -1.8570e+00,\n",
      "           -1.8989e+00,  7.8791e+00]],\n",
      "\n",
      "         [[ 3.5470e-01, -4.5847e-02,  4.6718e-01,  ..., -1.3655e-01,\n",
      "           -9.0256e-02, -2.2153e+00],\n",
      "          [-1.8735e+00,  1.1210e-01,  2.7458e+00,  ..., -7.2511e-01,\n",
      "           -1.7803e+00,  7.9630e+00],\n",
      "          [-1.6520e+00, -1.3125e+00,  6.9376e-01,  ..., -2.4604e+00,\n",
      "           -8.7608e-01,  5.2765e+00],\n",
      "          [-2.3409e+00, -8.7764e-01,  1.9936e+00,  ..., -1.2596e-02,\n",
      "           -1.7435e-01,  5.9098e+00],\n",
      "          [-2.0277e+00, -7.7524e-01,  2.3146e+00,  ...,  4.3002e-01,\n",
      "           -7.8186e-01,  6.5068e+00],\n",
      "          [-1.0194e+00, -3.0821e-01,  3.1228e+00,  ..., -3.9824e-01,\n",
      "           -2.6717e-01,  6.6029e+00]],\n",
      "\n",
      "         [[ 1.3502e-01, -6.5750e-01, -2.3283e-01,  ...,  1.4383e-01,\n",
      "            2.6188e-01, -1.6853e-01],\n",
      "          [ 8.0848e-01,  1.4548e+00,  8.4512e-01,  ..., -2.2409e-01,\n",
      "           -1.3566e-01,  2.5270e-01],\n",
      "          [ 1.3713e+00,  1.6380e+00,  6.8098e-01,  ...,  1.7969e-01,\n",
      "           -9.0202e-01, -6.0915e-01],\n",
      "          [-3.0293e-01,  1.8573e+00, -2.3107e-01,  ...,  1.6478e+00,\n",
      "           -8.8639e-01,  1.0788e+00],\n",
      "          [-1.3327e+00,  2.0431e+00, -1.6270e-01,  ...,  8.4891e-01,\n",
      "           -2.4871e+00, -1.5410e+00],\n",
      "          [ 4.6534e-01,  3.4011e+00,  1.8009e+00,  ..., -1.5551e+00,\n",
      "            8.6028e-02,  4.7870e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8822e-01,  9.9604e-03, -5.8665e-03,  ...,  1.2419e+00,\n",
      "            4.5654e-02,  1.7965e+00],\n",
      "          [-4.9711e-02, -1.7747e+00, -4.7276e-01,  ..., -3.3622e+00,\n",
      "           -1.8303e+00, -1.1687e+00],\n",
      "          [ 9.4328e-01, -1.5415e-01, -9.6216e-01,  ..., -1.9699e+00,\n",
      "           -1.1173e+00, -9.3605e-01],\n",
      "          [ 4.9288e-01,  2.2649e-01, -1.2146e+00,  ..., -3.6237e+00,\n",
      "           -1.0445e+00, -2.0016e+00],\n",
      "          [ 5.2528e-01, -2.1542e+00, -2.2999e-01,  ..., -2.5555e+00,\n",
      "           -1.6686e+00, -1.7201e+00],\n",
      "          [-2.7736e-02, -1.3989e+00, -2.4388e+00,  ..., -3.2265e+00,\n",
      "           -1.4686e+00, -3.9831e-01]],\n",
      "\n",
      "         [[-3.4717e-01, -1.3426e-01,  2.3333e-01,  ...,  2.5984e-01,\n",
      "           -1.1173e-02,  3.6323e-02],\n",
      "          [-5.6726e-01, -1.1477e+00,  1.2233e+00,  ...,  1.4973e+00,\n",
      "           -3.9861e-01,  1.3331e+00],\n",
      "          [-6.0740e-01, -9.2094e-01, -5.7477e-01,  ...,  2.3773e-01,\n",
      "            2.6415e-01, -2.5692e-01],\n",
      "          [ 1.7375e-01, -1.9290e-01,  1.6974e+00,  ...,  1.8208e+00,\n",
      "            1.7494e+00, -8.1957e-01],\n",
      "          [-1.4290e+00, -1.3431e+00,  6.0621e-01,  ...,  1.4638e+00,\n",
      "            1.1391e+00, -5.7803e-01],\n",
      "          [-1.2058e+00,  5.0918e-02,  1.8422e+00,  ...,  5.1216e-01,\n",
      "           -3.2252e-02,  9.2273e-01]],\n",
      "\n",
      "         [[ 3.4309e+00,  2.0956e+00, -2.0879e+00,  ..., -2.8532e+00,\n",
      "           -3.8920e+00, -1.2106e+00],\n",
      "          [-1.6784e+00, -4.2780e+00,  6.8345e+00,  ..., -2.8008e+00,\n",
      "            1.1034e+01, -3.4603e+00],\n",
      "          [-4.9596e+00,  1.1420e+00,  5.4689e+00,  ..., -1.0567e+00,\n",
      "            1.0956e+01,  3.9919e-01],\n",
      "          [-5.8813e+00, -5.8574e-01,  4.1521e+00,  ..., -4.6112e-01,\n",
      "            9.6195e+00,  1.8040e+00],\n",
      "          [-4.5521e+00, -2.6210e+00,  6.6099e+00,  ..., -2.4421e+00,\n",
      "            1.4838e+01,  8.2653e-01],\n",
      "          [-6.6592e+00, -3.9341e+00,  7.9250e+00,  ...,  4.4097e-01,\n",
      "            1.2486e+01, -2.8464e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-6.6680e-03, -4.1456e-02,  2.0246e-02,  ...,  6.1246e-02,\n",
      "            3.4407e-02,  6.8707e-02],\n",
      "          [ 4.4890e-01,  1.1458e-01,  2.7434e-01,  ...,  1.1157e-01,\n",
      "           -7.9654e-02,  8.5989e-04],\n",
      "          [-2.2197e-01, -7.3899e-01, -1.3155e-01,  ..., -3.1540e-02,\n",
      "            2.4502e-01, -5.5908e-02],\n",
      "          [-2.7859e-01,  7.4794e-01, -1.2849e+00,  ..., -6.6538e-01,\n",
      "           -4.5912e-01, -1.7728e-02],\n",
      "          [-6.5431e-01, -1.5925e-01,  3.2523e-02,  ..., -3.6311e-01,\n",
      "           -4.1633e-01, -1.2268e-02],\n",
      "          [-1.0397e+00,  8.8478e-01, -1.4895e-01,  ...,  5.0619e-01,\n",
      "           -1.0188e-01,  1.0991e-01]],\n",
      "\n",
      "         [[-6.8399e-02, -1.3111e-02, -1.4613e-01,  ..., -2.5827e-02,\n",
      "            4.6789e-02, -1.8018e-02],\n",
      "          [ 7.6337e-02,  2.4770e-01, -4.2395e-01,  ...,  3.3139e-01,\n",
      "           -1.9902e-01, -8.8937e-02],\n",
      "          [ 6.2032e-01, -1.0448e+00, -5.0396e-01,  ...,  1.5620e-01,\n",
      "           -7.8006e-01,  1.2431e+00],\n",
      "          [ 6.7333e-01,  7.6607e-01, -5.1168e-02,  ..., -4.7464e-01,\n",
      "           -4.3790e-01, -2.0519e-01],\n",
      "          [ 2.7316e-01, -2.0278e-01, -4.0181e-02,  ..., -4.0940e-01,\n",
      "            4.5384e-01, -1.6466e-01],\n",
      "          [-4.8284e-01, -1.1589e-01,  5.9937e-01,  ..., -3.3843e-01,\n",
      "           -2.3644e-01,  4.3505e-01]],\n",
      "\n",
      "         [[ 5.0535e-02,  1.0778e-01,  8.3453e-02,  ...,  2.1134e-02,\n",
      "           -8.1432e-02,  2.1831e-03],\n",
      "          [-2.4790e-01,  4.7535e-01, -1.3693e+00,  ..., -2.3040e-01,\n",
      "           -2.5561e-01,  1.9212e-01],\n",
      "          [-2.9772e-01,  5.8491e-01, -1.8932e-01,  ...,  6.8437e-01,\n",
      "           -2.7893e-02,  5.3509e-01],\n",
      "          [-2.1594e-02,  1.3217e+00,  1.0207e-01,  ...,  1.1436e-01,\n",
      "            8.5871e-02,  1.0869e-01],\n",
      "          [-6.9156e-01,  7.8010e-01,  1.0594e-01,  ...,  2.1237e-01,\n",
      "            3.8244e-01,  3.0136e-01],\n",
      "          [ 2.4652e-01,  4.8712e-01, -6.1465e-01,  ...,  4.5688e-01,\n",
      "            6.9282e-01,  8.2871e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2800e-04,  7.5450e-02, -6.1271e-02,  ...,  4.7254e-02,\n",
      "            3.5936e-02, -1.2174e-01],\n",
      "          [ 9.3303e-01,  2.2470e-01,  9.3694e-01,  ...,  2.6665e-01,\n",
      "           -1.3668e+00,  3.3838e-01],\n",
      "          [ 8.9278e-01,  4.3321e-01, -1.6248e-01,  ..., -6.0514e-02,\n",
      "           -1.7790e-01,  9.0998e-02],\n",
      "          [ 7.4851e-01,  6.0245e-01, -1.3263e-01,  ..., -7.2211e-01,\n",
      "           -1.3847e-01, -1.6386e-01],\n",
      "          [ 1.3305e-01,  6.2683e-01,  4.2357e-01,  ...,  2.8562e-01,\n",
      "           -3.8884e-01, -2.9982e-01],\n",
      "          [ 3.9254e-01,  5.5468e-01, -4.3469e-01,  ...,  3.4998e-02,\n",
      "           -6.1412e-01, -6.8290e-01]],\n",
      "\n",
      "         [[-1.3993e-01, -4.5451e-02,  1.0472e-01,  ..., -6.8383e-02,\n",
      "            5.1238e-02, -4.0748e-03],\n",
      "          [ 2.2343e-01, -6.1832e-01, -3.1974e-01,  ...,  3.3224e-01,\n",
      "           -2.2566e-01, -2.3011e-01],\n",
      "          [ 6.4836e-01, -1.6907e-01, -7.1223e-01,  ..., -4.6999e-01,\n",
      "           -1.1743e-02,  6.0893e-01],\n",
      "          [-1.0774e+00, -4.7958e-01,  1.4130e-01,  ..., -4.2860e-01,\n",
      "           -6.6182e-01,  4.4840e-01],\n",
      "          [-3.2187e-01,  4.9419e-01,  4.4285e-01,  ...,  2.9827e-01,\n",
      "            2.4095e-01,  1.0889e+00],\n",
      "          [-3.4101e-02,  9.9054e-02, -3.8500e-01,  ...,  2.8827e-01,\n",
      "           -8.8587e-02,  6.0471e-01]],\n",
      "\n",
      "         [[-1.6068e-02, -7.1866e-03, -2.6126e-02,  ..., -3.0851e-02,\n",
      "            5.5481e-03, -2.3312e-02],\n",
      "          [-1.9539e-01, -3.8325e-01, -7.7296e-01,  ..., -9.9520e-02,\n",
      "           -1.2463e-01, -6.1562e-01],\n",
      "          [-1.4505e-01, -3.1195e-01, -5.1713e-01,  ...,  1.7955e-01,\n",
      "            4.6543e-02, -9.5059e-01],\n",
      "          [-6.1059e-01, -3.2315e-01, -3.8473e-01,  ...,  4.4271e-01,\n",
      "           -3.6636e-01, -3.3035e-01],\n",
      "          [-6.1329e-01,  7.2102e-02, -1.6144e-01,  ..., -4.8824e-01,\n",
      "            2.7029e-01, -7.5064e-01],\n",
      "          [-1.5887e-02, -2.0063e-01, -2.9248e-01,  ..., -4.5681e-01,\n",
      "            1.7210e-01,  2.9558e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.1951e-02, -2.9031e-01,  2.3682e-01,  ...,  1.6976e+00,\n",
      "           -2.1622e-01, -7.0836e-02],\n",
      "          [ 1.7163e+00,  2.8447e-03, -2.7313e-01,  ..., -3.7500e+00,\n",
      "           -2.3225e-01, -6.2401e-01],\n",
      "          [-6.0339e-01, -5.6264e-01, -5.4738e-01,  ..., -3.5374e+00,\n",
      "           -5.1009e-01, -1.1121e+00],\n",
      "          [-1.2276e+00,  4.0182e-01,  1.7212e+00,  ..., -2.9034e+00,\n",
      "           -1.1610e+00, -1.6531e+00],\n",
      "          [-1.1957e+00,  2.1059e+00,  4.5209e-01,  ..., -4.0392e+00,\n",
      "           -3.2817e-01, -7.2612e-01],\n",
      "          [-5.1134e-01,  6.7422e-01, -2.5182e-01,  ..., -3.7161e+00,\n",
      "           -4.9200e-01, -1.7614e+00]],\n",
      "\n",
      "         [[ 1.7082e-01,  9.8631e-01, -1.4251e+00,  ..., -1.3545e-01,\n",
      "            2.5549e-01,  9.2218e-01],\n",
      "          [-3.8329e+00, -7.1780e+00,  7.5882e-02,  ..., -1.1822e+00,\n",
      "           -9.2771e-01, -9.7514e-01],\n",
      "          [-7.9054e-01, -4.1806e+00, -1.1806e-01,  ...,  1.2213e+00,\n",
      "           -4.9277e-01, -1.9223e+00],\n",
      "          [-1.2673e+00, -5.1812e+00,  6.8671e-01,  ..., -1.3714e+00,\n",
      "            4.9008e-01, -2.0436e+00],\n",
      "          [ 3.2552e+00, -4.2409e+00,  1.2901e+00,  ...,  1.5884e+00,\n",
      "            4.5251e-01, -2.4455e+00],\n",
      "          [ 1.8002e+00, -4.2526e+00,  1.4255e+00,  ..., -7.3833e-01,\n",
      "           -4.7044e-01, -2.7102e+00]],\n",
      "\n",
      "         [[-6.7615e-01,  2.4442e-01, -6.1178e-02,  ...,  1.7164e-01,\n",
      "            3.5102e-02, -2.9929e-01],\n",
      "          [ 1.0767e+00,  5.3217e-02, -5.1284e-01,  ..., -1.4208e+00,\n",
      "           -4.1437e-01, -5.4341e-01],\n",
      "          [ 1.3402e+00,  2.0110e-01,  8.1020e-02,  ..., -5.6560e-01,\n",
      "            6.4329e-01, -2.4767e-01],\n",
      "          [ 2.8077e-01, -8.2537e-01,  1.5243e-01,  ..., -1.2487e+00,\n",
      "            3.0471e-01, -7.4183e-01],\n",
      "          [ 1.6797e+00, -1.4154e+00,  1.3900e-01,  ..., -1.5255e+00,\n",
      "            3.3234e-01, -1.5315e+00],\n",
      "          [ 7.6684e-01, -9.5403e-01, -6.9141e-01,  ..., -1.2108e+00,\n",
      "           -1.2378e+00,  1.9260e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6829e-02,  1.1561e-01,  1.4686e-01,  ..., -1.0235e-01,\n",
      "            8.8439e-03,  1.6115e-01],\n",
      "          [ 2.5454e-01, -7.5285e-01, -1.1105e+00,  ...,  1.6497e+00,\n",
      "           -6.5482e-01,  6.2121e-01],\n",
      "          [-1.9106e-01, -3.1621e-01,  2.3864e-01,  ...,  9.6812e-01,\n",
      "           -3.7339e-01,  1.3204e-01],\n",
      "          [ 2.6018e-01, -3.7846e-01,  2.0632e-01,  ...,  1.7545e+00,\n",
      "           -2.6722e-01,  4.0025e-01],\n",
      "          [ 1.4133e+00, -1.4993e+00, -1.1973e-02,  ...,  1.4283e+00,\n",
      "           -1.1191e+00,  7.5752e-01],\n",
      "          [ 1.3568e+00, -9.8949e-01, -7.0584e-01,  ...,  8.5451e-01,\n",
      "           -2.5081e-01, -1.9863e-01]],\n",
      "\n",
      "         [[-3.0138e+00,  3.9843e-01, -1.6205e-02,  ..., -4.6883e-01,\n",
      "           -3.3862e-01,  1.2376e+00],\n",
      "          [ 4.7251e+00,  1.0700e-01, -1.2427e+00,  ..., -1.2890e+00,\n",
      "            1.6524e+00, -8.1419e-01],\n",
      "          [ 5.0123e+00, -3.6609e-01,  6.1012e-01,  ..., -5.3492e-01,\n",
      "            2.6024e+00, -5.6824e-01],\n",
      "          [ 4.8342e+00,  6.5790e-01, -2.4328e-01,  ..., -1.0042e+00,\n",
      "            1.3394e+00, -7.7723e-01],\n",
      "          [ 4.9381e+00, -5.6243e-01, -2.0867e+00,  ...,  7.4312e-01,\n",
      "            2.1419e+00,  2.4385e-01],\n",
      "          [ 4.6990e+00, -3.2900e-01, -4.6903e-01,  ..., -9.8622e-01,\n",
      "            7.7824e-01, -9.5964e-01]],\n",
      "\n",
      "         [[-1.3715e-02, -2.3592e-01,  9.2706e-03,  ..., -1.9745e-01,\n",
      "            3.4782e-01,  1.0614e-01],\n",
      "          [ 1.1252e+00, -3.3758e+00,  5.4856e-01,  ..., -6.5423e-01,\n",
      "            3.0399e-01,  6.6860e-02],\n",
      "          [ 5.3052e-01, -1.5328e+00,  2.3647e-01,  ..., -1.2837e+00,\n",
      "            1.7703e+00, -2.8148e-01],\n",
      "          [ 2.2140e-01, -2.0145e+00,  7.6141e-01,  ..., -1.8956e+00,\n",
      "            1.7021e+00, -1.3404e+00],\n",
      "          [-9.5088e-02, -1.4697e+00,  6.1475e-01,  ..., -7.0973e-01,\n",
      "            1.6957e+00, -1.0232e+00],\n",
      "          [ 1.1441e+00, -2.1796e+00,  6.0197e-01,  ...,  2.3856e-01,\n",
      "            2.7766e-01, -1.5277e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.0101e-02, -2.2483e-02,  5.6546e-04,  ...,  1.4741e-03,\n",
      "           -2.8200e-02,  3.4910e-01],\n",
      "          [ 2.3210e+00,  3.4506e-02,  1.6703e+00,  ...,  4.1030e-01,\n",
      "           -2.2479e-01, -1.5107e+00],\n",
      "          [ 2.4485e+00,  6.7195e-01, -2.0528e-04,  ..., -3.1967e-01,\n",
      "            7.1195e-01, -2.8716e-01],\n",
      "          [ 2.0135e+00,  7.1568e-01,  5.8470e-01,  ...,  5.3868e-02,\n",
      "           -9.5384e-02, -8.9054e-01],\n",
      "          [ 1.1565e+00,  4.9760e-01,  5.0370e-01,  ...,  7.6114e-01,\n",
      "            1.3777e-01, -1.8324e+00],\n",
      "          [ 1.1465e-02,  1.0697e+00, -5.7053e-01,  ...,  1.1197e+00,\n",
      "            8.1425e-01, -1.5941e+00]],\n",
      "\n",
      "         [[ 5.7400e-03, -1.3725e-02,  1.8680e-02,  ..., -2.1715e-02,\n",
      "            2.3784e-02,  6.0938e-03],\n",
      "          [ 1.9046e-01, -3.3540e-01,  5.5568e-02,  ...,  2.8069e-01,\n",
      "            1.1929e+00, -1.1521e+00],\n",
      "          [ 6.5169e-01, -1.0791e+00,  2.7086e-01,  ..., -7.1719e-01,\n",
      "            2.5290e+00, -1.4245e+00],\n",
      "          [-6.5900e-02, -7.5391e-01,  1.1856e+00,  ...,  3.3279e-01,\n",
      "            2.4733e+00, -1.4638e+00],\n",
      "          [ 5.7701e-01, -7.4556e-01, -2.0722e-01,  ...,  3.5242e-01,\n",
      "            9.1589e-01, -1.1609e+00],\n",
      "          [-1.2260e+00, -1.3338e-01, -7.9083e-02,  ..., -7.0317e-01,\n",
      "            1.1199e+00, -3.5356e-01]],\n",
      "\n",
      "         [[-5.8437e-02,  1.2950e-03, -3.8209e-02,  ..., -3.5921e-02,\n",
      "            1.2880e-02, -7.8963e-02],\n",
      "          [ 7.8388e-01,  4.0062e-02,  7.1644e-01,  ..., -8.2841e-01,\n",
      "            1.1195e+00, -2.3276e-01],\n",
      "          [ 3.9287e-01,  4.0930e-01,  1.3518e+00,  ..., -3.7697e-02,\n",
      "           -8.8867e-02, -6.1513e-01],\n",
      "          [-1.2441e-01, -1.7212e-01,  9.0978e-02,  ...,  5.3522e-01,\n",
      "           -2.5234e-01,  6.7059e-01],\n",
      "          [-6.5450e-01, -1.6786e+00, -4.4971e-01,  ...,  6.7897e-01,\n",
      "           -5.2893e-01,  1.9418e+00],\n",
      "          [-8.6826e-02,  1.3900e-01,  6.6728e-01,  ...,  7.5031e-01,\n",
      "            7.5034e-01,  1.2149e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3433e-01, -1.9210e-01, -5.9805e-02,  ..., -4.9623e-01,\n",
      "            2.1211e-01,  8.6146e-02],\n",
      "          [ 4.6437e-01, -1.0053e+00,  1.0719e+00,  ...,  1.5412e+00,\n",
      "           -2.1820e-01, -6.4617e-01],\n",
      "          [-3.5373e-01, -1.8282e+00, -2.7410e-01,  ...,  1.8658e+00,\n",
      "           -8.6923e-01,  5.3316e-01],\n",
      "          [ 7.0401e-01, -1.3571e+00, -1.9592e+00,  ...,  1.9855e+00,\n",
      "           -1.4784e+00,  5.9382e-02],\n",
      "          [ 1.8151e+00, -2.0687e+00, -6.8260e-01,  ...,  1.5192e+00,\n",
      "           -5.2038e-01,  6.1603e-01],\n",
      "          [ 2.8113e+00, -1.4863e+00, -3.5662e-01,  ...,  1.1061e+00,\n",
      "           -1.8460e-01, -2.9470e-01]],\n",
      "\n",
      "         [[-7.9508e-02, -1.4446e-01, -4.6717e-02,  ..., -1.8905e-01,\n",
      "           -1.4232e-01,  1.1325e-01],\n",
      "          [-1.2932e-01, -6.0084e-02, -1.0368e+00,  ...,  4.9461e-01,\n",
      "            4.1715e-01, -1.6332e-01],\n",
      "          [ 4.1122e-01, -9.3083e-01, -1.4137e-01,  ...,  8.7551e-01,\n",
      "            7.2926e-01, -1.6315e-01],\n",
      "          [ 6.2494e-02, -4.2804e-02,  4.9175e-01,  ..., -4.8211e-01,\n",
      "            2.7551e-01, -6.3746e-01],\n",
      "          [-3.3099e-01,  1.0014e+00, -9.6046e-01,  ...,  2.2127e-01,\n",
      "           -7.3336e-01,  4.4237e-01],\n",
      "          [-1.4111e-01, -6.0333e-01, -7.5797e-01,  ...,  2.9968e-01,\n",
      "            3.5158e-01,  1.0414e-02]],\n",
      "\n",
      "         [[-3.3629e-02, -3.3075e-02,  9.9673e-02,  ...,  6.7834e-02,\n",
      "           -4.1289e-02,  1.6266e-02],\n",
      "          [ 2.1249e-01, -2.3030e-01, -1.1335e+00,  ..., -9.8787e-01,\n",
      "           -9.2168e-01,  3.1043e-01],\n",
      "          [ 3.8342e-01,  2.2218e-01, -1.0855e+00,  ..., -2.2829e-01,\n",
      "           -7.5507e-01,  7.8662e-01],\n",
      "          [ 2.2825e-02,  1.0941e+00, -7.8950e-01,  ..., -3.2103e-01,\n",
      "           -1.3573e-01,  2.9779e-01],\n",
      "          [ 1.2123e+00,  9.8483e-01, -2.2345e-01,  ...,  6.1329e-01,\n",
      "            7.7015e-02, -1.3325e-01],\n",
      "          [-1.2234e+00, -3.0571e-01,  2.8630e-01,  ...,  6.2685e-01,\n",
      "            6.1053e-01,  5.5015e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.3408,  0.8585, -0.1599,  ...,  1.1341, -0.1732,  0.1462],\n",
      "          [-0.5527, -6.2879,  0.0307,  ..., -3.8074,  0.5053,  2.4742],\n",
      "          [ 0.1885, -4.5865,  0.5892,  ..., -4.4686,  1.0660,  2.0288],\n",
      "          [-0.1258, -4.0162, -0.0837,  ..., -3.9800,  1.9901,  1.9289],\n",
      "          [ 0.2217, -4.8761, -1.1228,  ..., -3.8617, -0.3350,  1.3587],\n",
      "          [ 0.5029, -4.6174, -0.9558,  ..., -4.1114,  1.1498,  1.1637]],\n",
      "\n",
      "         [[ 0.0537,  0.8649, -0.6301,  ..., -0.0349,  0.2855,  0.0174],\n",
      "          [ 1.1466, -0.9112,  0.0447,  ...,  2.1192, -0.5966, -0.5425],\n",
      "          [ 0.8766, -0.3043, -0.1756,  ...,  3.5147, -1.4581, -0.9111],\n",
      "          [ 1.4125,  0.9959,  2.7550,  ...,  1.7620, -2.1430,  1.0088],\n",
      "          [ 0.1106,  1.5573,  2.0884,  ...,  2.2975,  0.7909, -0.2986],\n",
      "          [ 0.2820,  1.4278,  2.8727,  ...,  1.9728, -0.7722,  0.0249]],\n",
      "\n",
      "         [[-0.3110,  0.1350, -0.9827,  ..., -0.3543, -0.0525, -0.1379],\n",
      "          [-0.2185,  0.2247,  2.6164,  ...,  0.9373,  0.6249,  0.3634],\n",
      "          [-0.3897,  0.0550,  2.6232,  ...,  0.4997,  0.2415,  0.2792],\n",
      "          [-0.3780, -0.1696,  2.7796,  ...,  0.3030,  0.4930,  0.1424],\n",
      "          [-0.0636, -1.0341,  3.4962,  ...,  0.4440, -0.4846,  0.1854],\n",
      "          [ 0.4374,  0.7576,  2.3824,  ...,  0.2295,  0.0655,  0.3687]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3680,  0.0704, -0.0714,  ..., -0.0387,  0.2336,  0.0115],\n",
      "          [-3.0412, -0.0738,  2.1857,  ..., -0.1331, -0.6098, -0.5626],\n",
      "          [-1.8702, -0.0386,  1.8052,  ..., -2.2556, -0.0815, -0.2686],\n",
      "          [-1.5968,  2.4348,  4.3501,  ..., -1.7954,  1.1750, -0.5795],\n",
      "          [-2.5137,  0.5071,  1.6048,  ..., -1.3985, -0.2136,  1.1256],\n",
      "          [-1.6600, -0.0260,  2.0814,  ..., -1.1993,  0.1347,  0.4226]],\n",
      "\n",
      "         [[ 0.2022,  0.0638,  0.3363,  ...,  0.4135,  0.0099,  0.2160],\n",
      "          [ 1.7325,  0.7064,  0.3160,  ..., -1.3130,  0.3046,  0.4912],\n",
      "          [ 2.5756, -0.3326,  1.1619,  ..., -2.2859,  0.9978,  1.0519],\n",
      "          [ 1.6059, -0.3211,  1.0875,  ..., -1.9286, -0.1870, -0.5286],\n",
      "          [ 1.6917, -1.4068,  0.2654,  ..., -0.8617, -0.3797, -1.2829],\n",
      "          [ 3.1338, -1.3283,  0.9270,  ..., -0.8453,  0.2591, -0.1297]],\n",
      "\n",
      "         [[-3.0244,  0.5300,  0.5607,  ..., -0.9401,  0.3239,  0.1843],\n",
      "          [ 8.4203, -0.9956, -2.5435,  ...,  2.6581, -0.4121, -0.0762],\n",
      "          [ 7.3758, -0.2461, -2.2521,  ...,  1.4005,  0.5201, -0.2924],\n",
      "          [ 7.4057,  1.8506, -2.2912,  ...,  0.7321, -0.4035, -0.5523],\n",
      "          [ 8.6750,  0.4892, -3.1094,  ...,  0.9440, -0.8475, -0.8583],\n",
      "          [ 8.8138, -0.3166, -1.7648,  ...,  2.9005,  0.0538, -1.2636]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 4.3166e-02, -4.7600e-02,  1.1009e-02,  ..., -7.0881e-02,\n",
      "           -4.2999e-03, -8.4636e-02],\n",
      "          [-3.5658e-01,  2.1893e-01, -9.6843e-01,  ...,  2.3265e-01,\n",
      "           -3.7752e-01,  4.5318e-01],\n",
      "          [-4.6531e-01, -6.2376e-02,  3.8188e-01,  ...,  4.0192e-01,\n",
      "           -2.7363e-01, -6.8849e-02],\n",
      "          [-9.9060e-01, -8.9789e-02, -6.5288e-01,  ..., -1.6783e+00,\n",
      "           -1.3548e-01, -7.6300e-02],\n",
      "          [-5.9470e-01,  7.7447e-02, -2.2122e-01,  ..., -5.0257e-01,\n",
      "            2.7092e-02,  8.5606e-02],\n",
      "          [-2.8252e-01,  6.4729e-01,  6.1808e-01,  ..., -5.0371e-01,\n",
      "            2.2574e-01, -7.5478e-01]],\n",
      "\n",
      "         [[ 7.1009e-02,  1.7761e-02, -2.0939e-02,  ..., -3.1767e-02,\n",
      "            1.0801e-02, -1.8365e-03],\n",
      "          [-3.1515e-01,  1.0920e+00, -3.4831e-01,  ...,  1.0768e+00,\n",
      "            5.2221e-01, -4.9559e-01],\n",
      "          [ 6.5897e-02, -2.6046e-01, -3.4204e-01,  ...,  1.1576e+00,\n",
      "           -8.3357e-02, -5.5491e-01],\n",
      "          [ 8.1049e-02, -4.6389e-01, -2.1532e+00,  ...,  1.1449e+00,\n",
      "            3.1944e-01,  3.7739e-02],\n",
      "          [ 6.5196e-01,  1.4102e-01, -1.4633e+00,  ...,  1.0389e+00,\n",
      "            7.1009e-01,  2.0635e+00],\n",
      "          [-7.6644e-04,  9.6313e-02, -4.5843e-01,  ..., -2.4473e-01,\n",
      "           -4.7606e-01,  7.0970e-01]],\n",
      "\n",
      "         [[ 8.1070e-02,  1.6610e-02, -5.7158e-03,  ...,  2.3920e-02,\n",
      "           -6.8031e-02, -6.0603e-02],\n",
      "          [ 5.7319e-01, -2.6054e-01,  3.5248e-01,  ..., -1.0454e+00,\n",
      "            6.8087e-01,  2.6963e-01],\n",
      "          [ 1.1575e+00,  6.4435e-01, -2.5158e-01,  ...,  2.6960e-02,\n",
      "            5.0273e-01, -1.5045e-01],\n",
      "          [-2.2626e-01,  5.1820e-02, -7.0766e-01,  ..., -1.0050e+00,\n",
      "            9.6869e-01,  6.3610e-01],\n",
      "          [ 9.4562e-02, -2.7122e-01, -1.1109e+00,  ..., -8.7416e-01,\n",
      "           -6.8694e-01,  2.1991e-02],\n",
      "          [ 6.5872e-01,  2.8982e-01, -1.5938e-02,  ..., -6.4122e-01,\n",
      "           -1.4362e-01,  1.4571e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3873e-03,  2.8190e-02,  1.7482e-02,  ..., -8.7293e-02,\n",
      "           -1.8433e-02,  1.6530e-02],\n",
      "          [-1.0061e+00,  4.9955e-01, -1.8535e-01,  ..., -1.4775e-01,\n",
      "           -2.0186e-01,  4.9314e-01],\n",
      "          [-4.0153e-01, -5.5506e-01,  1.7472e-01,  ..., -3.8993e-01,\n",
      "           -4.1025e-01,  7.5716e-01],\n",
      "          [-2.4429e-01, -3.4256e-01, -1.1467e-01,  ...,  1.5550e+00,\n",
      "           -2.1848e-01,  1.1315e+00],\n",
      "          [-3.1452e-01, -3.6081e-01,  2.5428e-01,  ...,  3.2410e-01,\n",
      "           -1.2063e+00,  1.8341e-01],\n",
      "          [-5.5361e-01, -7.2968e-01, -4.0351e-01,  ...,  2.2522e-01,\n",
      "            1.4998e-01,  1.5103e+00]],\n",
      "\n",
      "         [[ 4.5025e-02, -4.6938e-04,  3.0498e-02,  ...,  2.9410e-02,\n",
      "           -1.4206e-02,  1.2067e-03],\n",
      "          [ 2.7558e+00, -8.1958e-01, -1.8042e+00,  ...,  3.0614e-01,\n",
      "            6.4232e-01,  6.6298e-01],\n",
      "          [ 1.1224e+00, -3.0600e-01, -6.5503e-01,  ..., -8.6702e-01,\n",
      "           -1.1291e-01,  2.5420e-01],\n",
      "          [ 1.0578e+00,  4.3182e-01, -1.4578e+00,  ..., -2.2666e-01,\n",
      "           -1.1505e-01, -1.7406e-01],\n",
      "          [ 6.8042e-01,  6.1173e-01, -1.1378e+00,  ...,  8.0586e-01,\n",
      "            5.1096e-01,  3.6139e-01],\n",
      "          [-2.4960e-01, -1.3642e-01, -2.0846e+00,  ...,  9.1783e-01,\n",
      "            1.5916e+00,  4.8040e-01]],\n",
      "\n",
      "         [[ 7.1175e-02, -2.0185e-01, -6.6236e-02,  ..., -3.5605e-02,\n",
      "            1.9385e-01, -4.7338e-02],\n",
      "          [-3.9107e-01, -5.3681e-01,  9.7280e-02,  ..., -1.8940e-01,\n",
      "           -5.0546e-01,  7.7949e-01],\n",
      "          [ 3.7457e-01, -8.0267e-01,  7.5287e-01,  ...,  6.5750e-01,\n",
      "           -3.1562e-01, -1.9311e-01],\n",
      "          [ 1.3876e-01, -1.2481e-01, -5.2357e-01,  ..., -2.4855e-02,\n",
      "           -2.9241e-01, -6.7364e-01],\n",
      "          [ 1.8950e-01, -6.6702e-01, -1.5086e-01,  ...,  1.4189e-02,\n",
      "           -6.8680e-01,  1.9753e-01],\n",
      "          [-6.0728e-01, -1.3566e+00, -1.9423e-01,  ..., -7.4927e-01,\n",
      "            8.8708e-02,  2.4750e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0463, -0.2556, -0.1255,  ...,  0.6273,  0.7309, -0.3107],\n",
      "          [-3.8028, -1.6746,  0.6766,  ..., -0.3281, -4.9528,  0.0431],\n",
      "          [-3.1611, -1.5139,  0.4990,  ...,  0.5226, -6.2900,  1.5368],\n",
      "          [-4.7304, -3.4570,  2.1231,  ...,  1.7128, -4.2932, -0.6127],\n",
      "          [-5.0828, -2.7366,  2.4385,  ...,  2.5359, -4.9113, -0.4801],\n",
      "          [-5.0601, -2.2358,  1.4072,  ...,  0.7607, -4.0097,  0.2283]],\n",
      "\n",
      "         [[-0.1513, -0.0687,  0.1503,  ..., -0.0432, -0.8836, -0.1910],\n",
      "          [-1.9127,  0.7626, -0.2994,  ...,  0.6998, -0.1538,  0.7682],\n",
      "          [-2.3831, -0.6685, -0.5280,  ...,  0.4397,  0.3221, -0.4055],\n",
      "          [-0.1898, -0.9540, -0.1837,  ...,  0.8426,  0.0287, -2.4101],\n",
      "          [-0.7217,  1.0098,  0.0923,  ...,  1.7383,  0.6004, -0.3258],\n",
      "          [-1.2484,  1.0207,  0.1145,  ...,  0.2058, -1.0285,  0.0284]],\n",
      "\n",
      "         [[ 0.1901,  0.3029,  1.1370,  ..., -0.4679,  0.4336, -0.4995],\n",
      "          [-0.3199, -1.2099, -1.9073,  ..., -0.9362, -1.3513,  0.7871],\n",
      "          [-1.7395, -2.1578, -1.2116,  ..., -1.1218, -2.1335,  0.9619],\n",
      "          [-1.4581, -2.9776, -2.5077,  ..., -2.4675, -1.5596,  0.1780],\n",
      "          [-0.3150, -3.5276, -1.9713,  ..., -1.5520, -1.5642,  1.8438],\n",
      "          [ 0.2858, -0.9393, -1.3440,  ..., -0.0476, -2.1085,  1.4060]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1533,  0.0721, -0.2296,  ..., -0.0092,  0.1527,  0.0298],\n",
      "          [-1.6838, -0.3120, -0.0759,  ...,  0.5902, -0.6897, -0.4398],\n",
      "          [-1.2386, -0.0071, -0.5839,  ...,  2.3246, -1.4113, -0.2612],\n",
      "          [-1.1128, -0.1254, -0.3733,  ...,  0.8726, -1.6927,  0.1868],\n",
      "          [-0.9372, -0.2307, -0.2206,  ...,  0.6645, -1.5506, -0.5432],\n",
      "          [-1.6275,  1.2939, -0.3868,  ...,  0.7868, -1.3235, -1.1032]],\n",
      "\n",
      "         [[-0.3630, -2.1827,  0.1210,  ..., -0.0851, -0.0518,  0.9255],\n",
      "          [-1.1713,  3.5624,  0.1582,  ..., -1.5803, -0.0580,  0.6941],\n",
      "          [-1.1431,  4.0652, -0.5324,  ..., -0.8927, -0.7160,  0.6954],\n",
      "          [-0.6620,  4.2697, -0.7308,  ...,  0.1440, -0.9927, -0.5444],\n",
      "          [ 0.1680,  2.9536, -0.0885,  ...,  1.2356, -1.2117,  0.9059],\n",
      "          [-0.0466,  0.3041, -0.7034,  ...,  0.2900, -1.0767,  1.4770]],\n",
      "\n",
      "         [[ 0.3739,  0.0738, -0.1308,  ...,  0.6585,  0.1288,  0.2674],\n",
      "          [-1.4306,  0.4486,  0.6610,  ..., -0.1411,  0.3583, -1.1496],\n",
      "          [-0.7323,  0.2326,  0.2532,  ...,  0.2548,  1.6079, -0.1241],\n",
      "          [-1.5566, -1.2600, -1.8040,  ..., -1.4311,  1.9557, -1.0330],\n",
      "          [-1.1674, -0.3072, -1.0670,  ..., -0.6812,  0.7898, -0.3983],\n",
      "          [-0.6829,  0.0933,  0.9038,  ..., -0.7017,  1.2729,  0.3923]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-3.2081e-02,  5.2934e-02, -5.1048e-02,  ..., -1.5300e-02,\n",
      "            6.8611e-03,  1.5492e-02],\n",
      "          [-8.1912e-01,  8.4381e-01,  8.6183e-02,  ..., -4.5274e-01,\n",
      "           -6.6207e-02, -5.2101e-01],\n",
      "          [-3.9628e-01, -3.0884e-01,  2.5064e-01,  ...,  5.1090e-02,\n",
      "            2.5344e-02, -4.7552e-01],\n",
      "          [-1.1277e+00, -7.3287e-01,  8.1638e-01,  ...,  3.7446e-01,\n",
      "           -1.2896e-01, -3.3798e-01],\n",
      "          [ 2.2720e-01, -7.7322e-01,  1.3524e+00,  ...,  3.4250e-01,\n",
      "           -4.5662e-03, -1.2931e-03],\n",
      "          [ 2.0525e-01,  3.7403e-01,  7.0914e-01,  ..., -2.7239e-01,\n",
      "           -4.1847e-02, -5.4836e-02]],\n",
      "\n",
      "         [[ 1.5667e-02, -4.2659e-02,  2.0836e-02,  ...,  2.2271e-02,\n",
      "           -4.1194e-02,  2.9141e-02],\n",
      "          [-7.9369e-02, -1.1406e+00, -2.2294e-01,  ...,  1.4228e+00,\n",
      "           -3.8273e-02, -1.3321e-01],\n",
      "          [-4.3480e-01, -1.4043e+00, -1.4379e-01,  ...,  6.1232e-01,\n",
      "           -3.6775e-01, -9.3545e-01],\n",
      "          [-3.5781e-01, -1.2671e+00, -3.7432e-01,  ..., -9.2747e-01,\n",
      "           -4.4805e-01, -5.3410e-01],\n",
      "          [-6.7369e-01, -7.8454e-02, -8.8588e-01,  ...,  3.0051e-01,\n",
      "            8.1890e-01,  6.1693e-01],\n",
      "          [ 4.8307e-01, -9.6354e-02, -1.3375e+00,  ...,  7.8215e-01,\n",
      "            1.3027e+00,  3.8708e-01]],\n",
      "\n",
      "         [[ 4.2549e-02, -2.6196e-02,  4.8947e-02,  ...,  3.4685e-02,\n",
      "           -1.6215e-02,  2.2317e-02],\n",
      "          [ 9.1368e-01,  4.6674e-01, -5.9067e-01,  ...,  3.0681e-01,\n",
      "           -3.5514e-01,  8.1349e-01],\n",
      "          [ 1.3507e-01,  6.4344e-01,  8.0998e-01,  ...,  5.0471e-01,\n",
      "            7.0522e-01, -2.8860e-01],\n",
      "          [-5.1524e-01,  1.1066e+00,  7.0339e-01,  ...,  2.6563e-01,\n",
      "            4.2806e-01, -1.2521e+00],\n",
      "          [-1.2541e+00,  1.7509e+00, -6.0654e-02,  ..., -6.3092e-01,\n",
      "            4.6607e-01,  2.0558e-01],\n",
      "          [-3.6447e-01,  2.3505e-01,  2.1683e-01,  ..., -1.2098e+00,\n",
      "            6.0206e-01,  1.7884e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7778e-01,  8.9037e-02,  5.3712e-02,  ...,  4.0529e-02,\n",
      "            3.8948e-02, -1.2896e-01],\n",
      "          [-4.7232e-01, -2.2010e-01, -7.3535e-01,  ..., -3.5866e-01,\n",
      "            9.9284e-01, -4.6885e-01],\n",
      "          [-1.4980e-01, -7.4436e-03, -5.7139e-01,  ...,  2.1696e-01,\n",
      "            1.5208e+00, -1.1492e+00],\n",
      "          [ 1.3668e-01, -1.4504e-01, -1.1229e+00,  ..., -1.0185e+00,\n",
      "           -2.0993e-01,  3.2209e-01],\n",
      "          [ 8.4815e-01,  4.3023e-01, -1.3151e+00,  ...,  2.3276e-01,\n",
      "           -7.2318e-01,  9.3406e-02],\n",
      "          [-3.0509e-01,  4.9922e-02, -1.2408e+00,  ...,  1.1407e-02,\n",
      "            6.2889e-02, -8.9409e-01]],\n",
      "\n",
      "         [[-5.8261e-01, -4.6527e-03,  3.8665e-02,  ..., -7.5257e-03,\n",
      "            1.1940e-02, -5.5457e-03],\n",
      "          [-1.8308e+00,  4.2351e-01, -3.6425e-01,  ..., -3.8225e-02,\n",
      "            2.5143e-01,  3.5698e-01],\n",
      "          [-1.5268e+00,  1.5635e+00, -4.1678e-01,  ...,  6.3364e-02,\n",
      "           -1.0006e-02, -1.0256e+00],\n",
      "          [-9.3949e-01,  6.8133e-01, -7.9871e-01,  ..., -2.6659e-01,\n",
      "            5.2527e-01, -9.7037e-01],\n",
      "          [-1.2842e+00,  1.4446e-01, -2.4662e-01,  ...,  1.7356e-01,\n",
      "            1.0059e+00, -5.3077e-01],\n",
      "          [-1.9062e+00,  2.5630e-01, -1.0791e+00,  ..., -3.6148e-01,\n",
      "            1.1419e+00,  6.5466e-01]],\n",
      "\n",
      "         [[ 8.3751e-03,  8.9898e-02, -4.4026e-02,  ...,  5.8059e-02,\n",
      "            3.6096e-02, -4.5102e-02],\n",
      "          [-1.0212e+00,  8.4899e-01, -1.3188e-01,  ...,  4.2922e-02,\n",
      "           -1.2801e-01,  5.0783e-01],\n",
      "          [-2.0409e+00,  8.2631e-01,  3.5970e-01,  ..., -1.5534e-01,\n",
      "            2.5462e-01, -2.5041e-01],\n",
      "          [-1.4013e+00,  1.0909e+00, -3.6530e-02,  ...,  2.2613e-01,\n",
      "           -9.3397e-01,  3.2899e-01],\n",
      "          [-4.3364e-01,  1.4227e+00, -7.1637e-01,  ...,  2.3208e-02,\n",
      "           -6.3343e-01, -3.7189e-01],\n",
      "          [-4.4097e-01,  1.4165e-02,  9.0107e-01,  ..., -3.3245e-01,\n",
      "           -1.0353e-01,  1.2182e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.0240, -2.3477,  0.1804,  ..., -0.2122, -0.1941,  0.0690],\n",
      "          [-0.3534,  4.9962,  1.1300,  ...,  1.6244,  0.5005, -0.1245],\n",
      "          [-0.6651,  5.3500,  0.2047,  ...,  1.2072, -0.1619,  0.1184],\n",
      "          [ 0.1893,  6.2100,  2.4400,  ...,  1.6749, -1.1568, -1.0863],\n",
      "          [-0.1734,  6.0166,  2.8574,  ...,  0.3840, -1.3278, -0.8027],\n",
      "          [-1.1559,  4.8625,  1.7209,  ...,  0.2219, -0.1482, -0.5736]],\n",
      "\n",
      "         [[-0.8199,  0.2241,  0.4710,  ..., -0.5216,  1.0776,  1.1316],\n",
      "          [ 0.2212,  0.7831,  0.8161,  ..., -0.0280,  1.2480,  1.2414],\n",
      "          [ 0.8424,  0.8959, -0.4047,  ..., -0.0538,  0.6081, -1.4034],\n",
      "          [ 0.7627, -0.0231,  1.3177,  ..., -0.5827,  1.3765, -1.7436],\n",
      "          [ 1.7335,  0.2900,  1.9076,  ..., -0.6474,  1.2918, -0.9388],\n",
      "          [ 0.5420,  1.3319,  0.6490,  ..., -0.4121,  0.5131,  1.2219]],\n",
      "\n",
      "         [[-0.8492,  0.4607,  0.0322,  ...,  0.4985, -0.2432,  1.1717],\n",
      "          [ 1.1433, -1.4529,  1.8998,  ...,  1.1634,  0.5119, -0.0596],\n",
      "          [ 0.5452, -2.0937,  1.1318,  ...,  0.4223,  0.9969,  0.0807],\n",
      "          [ 1.2089, -2.2372,  1.2046,  ...,  0.2782,  0.8167,  1.3269],\n",
      "          [ 0.9392, -3.2586,  3.5340,  ..., -0.5116, -0.1587,  1.3413],\n",
      "          [ 1.1789, -0.7507,  2.6603,  ...,  1.3086,  0.3297,  1.0647]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3056, -0.1406,  0.1422,  ...,  0.1800,  1.7321, -2.8682],\n",
      "          [ 1.3774, -1.3391, -0.4987,  ..., -0.9345, -5.5728,  6.4502],\n",
      "          [-0.0619, -2.6221, -1.9724,  ..., -0.3522, -5.3927,  5.9560],\n",
      "          [-1.6977, -1.9656, -1.3106,  ..., -1.4232, -6.4935,  5.8403],\n",
      "          [-1.1043, -1.6441, -0.5474,  ..., -1.2700, -6.2532,  6.1743],\n",
      "          [-1.1537, -1.9715, -0.1237,  ..., -1.4347, -6.3413,  6.0609]],\n",
      "\n",
      "         [[ 0.1830,  0.3689,  0.2160,  ..., -0.2242,  0.0272, -0.1457],\n",
      "          [-1.1853, -0.1957,  0.2312,  ...,  0.7955, -0.7345, -0.3738],\n",
      "          [-1.5475, -0.4736,  0.1012,  ...,  0.9207, -0.5760, -0.3852],\n",
      "          [-1.7495,  0.5022, -0.7547,  ...,  1.2192,  0.0875,  0.0863],\n",
      "          [-2.1642,  0.7053, -1.1838,  ...,  0.0292, -0.5763, -0.4465],\n",
      "          [-1.4595,  0.6211, -0.5757,  ...,  0.6579, -0.2109, -0.4498]],\n",
      "\n",
      "         [[ 0.3780,  0.1189,  0.6221,  ...,  0.5265,  0.5730, -0.3332],\n",
      "          [-0.0160, -1.3816, -0.5052,  ..., -2.3754, -4.0814,  1.6947],\n",
      "          [ 0.5437, -1.9292, -2.8530,  ..., -1.9530, -3.4644, -0.6789],\n",
      "          [ 0.6687, -1.4091, -1.7914,  ..., -0.9725, -5.2165,  0.3203],\n",
      "          [ 1.3730, -1.0667, -1.0860,  ..., -2.3469, -5.0041, -0.0296],\n",
      "          [ 0.9216, -0.4546, -0.7449,  ..., -1.4488, -4.6925,  0.7318]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0561, -0.0178, -0.0192,  ...,  0.1242, -0.0726, -0.0237],\n",
      "          [-0.6572, -0.6579,  0.5664,  ...,  0.4149,  1.3191, -0.0439],\n",
      "          [-2.5691, -0.6673, -0.4724,  ..., -0.1030,  1.2223,  0.4587],\n",
      "          [-2.5473, -0.8717,  1.0827,  ...,  0.5310,  1.3350,  0.9585],\n",
      "          [-1.9153, -1.6524,  0.7554,  ...,  0.3160, -0.1277,  0.2141],\n",
      "          [-0.7397, -0.0470,  0.0695,  ..., -0.0886, -0.3021, -0.7015]],\n",
      "\n",
      "         [[ 0.0056,  0.0325,  0.0347,  ..., -0.0114,  0.0061,  0.0143],\n",
      "          [-0.3878, -0.5757, -1.1356,  ..., -0.6532,  0.6912, -0.5521],\n",
      "          [-1.9798,  0.6117, -0.0419,  ...,  0.4443,  0.4167,  0.2823],\n",
      "          [-1.5641,  0.9137, -0.3158,  ..., -0.2874,  0.6050,  0.8830],\n",
      "          [-0.9090, -0.0706, -0.2801,  ...,  0.4206, -0.0310, -1.6161],\n",
      "          [-2.0235, -0.5432, -0.0408,  ..., -0.4852,  0.1471, -0.9394]],\n",
      "\n",
      "         [[ 0.0439, -0.0428,  0.0588,  ...,  0.0525, -0.0817, -0.0600],\n",
      "          [-0.3744, -0.2124, -0.4571,  ...,  0.1068, -0.0386,  0.2504],\n",
      "          [-0.4594,  0.0077,  0.0043,  ..., -1.0568, -0.4914, -0.6356],\n",
      "          [-0.4757,  0.3194,  0.1915,  ..., -0.3166,  0.5679,  0.1694],\n",
      "          [-0.2857, -0.1861, -0.8355,  ...,  1.6137,  0.5889,  0.1262],\n",
      "          [-1.0497,  0.1706,  0.0164,  ..., -0.4034,  0.2425,  0.2583]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0991, -0.0367,  0.0492,  ..., -0.0979,  0.0381,  0.0169],\n",
      "          [ 0.2017, -0.1608,  1.6432,  ..., -1.4508,  1.2313, -0.7095],\n",
      "          [-0.2598,  0.6158,  3.5637,  ..., -1.3848,  1.4217, -2.1956],\n",
      "          [-0.1329, -0.6166,  3.8200,  ..., -0.7334,  2.5961, -0.6208],\n",
      "          [ 1.6214, -1.1706,  2.9818,  ..., -0.5916,  1.2543, -0.3802],\n",
      "          [ 0.2486, -0.3852,  2.3676,  ..., -0.9875,  0.7086, -0.7022]],\n",
      "\n",
      "         [[ 0.1602, -0.0634,  0.1319,  ...,  0.0761,  0.0238, -0.1196],\n",
      "          [-0.1758,  0.9461,  0.4425,  ...,  1.1005,  0.7562, -0.4961],\n",
      "          [-2.4865,  0.3343, -0.8997,  ..., -0.9638,  0.3108, -0.0929],\n",
      "          [-2.1872,  1.4853, -1.2856,  ...,  0.2680,  0.6452,  1.1640],\n",
      "          [-1.0929,  1.6004, -0.6213,  ...,  1.0756,  0.7809,  0.1186],\n",
      "          [-0.7848,  1.0985, -0.4224,  ...,  0.7885,  1.9400,  0.6369]],\n",
      "\n",
      "         [[ 0.2098, -0.0345, -0.0502,  ...,  0.0247,  0.0571,  0.0218],\n",
      "          [-0.3437,  0.4108,  0.6291,  ..., -1.0798,  0.4153,  0.4646],\n",
      "          [-0.5768,  0.4102,  0.5262,  ..., -1.0580,  0.1654, -0.2530],\n",
      "          [-1.5280, -0.7600, -0.0353,  ...,  0.0043,  0.5084, -0.1385],\n",
      "          [-0.5827, -0.6276,  0.6882,  ...,  0.4471, -0.3625, -0.4696],\n",
      "          [ 0.7526,  1.1238,  0.6453,  ...,  0.1450, -0.7596,  0.9990]]]],\n",
      "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0506, -0.2555, -0.4592,  ...,  0.3125,  0.3175,  0.3800],\n",
      "          [ 1.0157,  0.8256, -1.2245,  ...,  1.3237, -0.8244,  1.1907],\n",
      "          [-0.1878,  0.9763,  0.1220,  ...,  1.4052, -1.3454, -0.5984],\n",
      "          [ 0.7599,  0.7551,  1.0711,  ...,  1.2004, -1.6571, -1.1413],\n",
      "          [ 1.3565,  0.5145,  1.1566,  ...,  1.8127, -1.5385,  0.8603],\n",
      "          [ 1.5650,  0.3833,  0.2486,  ...,  0.5540, -0.7673,  2.0786]],\n",
      "\n",
      "         [[-0.2898,  0.1605,  0.1169,  ...,  0.0410, -1.1372, -0.1314],\n",
      "          [ 0.6309,  0.9716, -0.0321,  ...,  0.2026,  0.8511,  1.2967],\n",
      "          [ 0.7728,  0.5000, -0.0316,  ...,  0.3928,  0.3489,  0.1003],\n",
      "          [ 1.3931,  0.8855,  0.7990,  ...,  0.3794,  0.7033,  0.2977],\n",
      "          [ 1.3697, -0.0257, -0.5191,  ..., -0.0804, -0.8162,  1.8123],\n",
      "          [-0.0640,  1.9502,  0.4353,  ..., -0.0918,  1.7206,  0.9944]],\n",
      "\n",
      "         [[-1.2395, -0.0988,  0.5551,  ..., -0.6503,  0.4549, -0.2785],\n",
      "          [ 1.6873,  0.8381, -0.4629,  ...,  1.6695, -0.8676,  1.1380],\n",
      "          [ 2.4451,  0.4174,  0.3855,  ...,  0.0643,  0.2889,  0.2664],\n",
      "          [ 1.8754,  1.9425,  1.1713,  ...,  0.3394,  0.3721, -0.1914],\n",
      "          [ 2.5062,  0.5252,  0.2016,  ...,  0.7831,  1.0764,  0.2680],\n",
      "          [ 1.9636,  1.8460, -0.2301,  ...,  1.2732, -0.4994,  0.7709]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7925, -0.9019, -0.3986,  ..., -1.0424, -0.4156,  0.4796],\n",
      "          [ 0.9810,  0.1535, -1.3518,  ..., -0.6113, -0.0302, -0.2729],\n",
      "          [ 0.4147, -0.6516, -1.7295,  ..., -0.3744,  1.6355, -0.3944],\n",
      "          [ 0.1842,  0.4923, -0.6091,  ...,  0.8922,  1.8569, -0.1698],\n",
      "          [ 0.2531,  0.4776, -1.7653,  ...,  0.5086,  1.1851, -1.4327],\n",
      "          [ 0.9411,  0.3613, -1.4302,  ...,  0.4888, -0.2639, -0.6611]],\n",
      "\n",
      "         [[-0.9041,  2.5703,  0.3066,  ...,  0.3647,  1.9507, -0.5378],\n",
      "          [ 0.5574, -2.9114,  0.4636,  ..., -0.1938, -4.7600,  2.6124],\n",
      "          [ 0.5286, -3.5951,  1.0238,  ...,  0.2661, -4.1229,  3.0119],\n",
      "          [ 0.6746, -4.3250,  1.0893,  ...,  0.0780, -3.8145,  3.4634],\n",
      "          [ 0.9488, -3.4091,  0.9395,  ..., -1.1905, -3.9388,  3.0998],\n",
      "          [ 0.1705, -3.3215, -0.6550,  ..., -0.0852, -4.3272,  3.0601]],\n",
      "\n",
      "         [[-2.0224, -0.3568, -1.1268,  ..., -0.3994,  0.0479,  0.2443],\n",
      "          [ 1.8813,  1.0381,  0.4353,  ...,  0.7009, -0.1585,  0.1223],\n",
      "          [ 3.0428, -0.7587,  1.2729,  ...,  2.1033, -0.6861,  0.4373],\n",
      "          [ 3.7136,  0.6981,  1.1974,  ...,  0.2405, -0.0743, -0.1071],\n",
      "          [ 2.8261,  0.5276,  0.7251,  ...,  0.8735, -0.6361,  0.1888],\n",
      "          [ 2.5774,  0.1781,  2.0335,  ...,  0.3115, -1.2372,  0.0800]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-3.6111e-02, -7.4210e-02,  1.3784e-02,  ...,  1.4079e-01,\n",
      "           -3.2213e-02,  1.1696e-02],\n",
      "          [-2.6469e-01,  6.3900e-01, -9.7136e-02,  ...,  1.6929e-01,\n",
      "           -5.9649e-01,  5.8254e-01],\n",
      "          [ 4.0855e-02,  8.2690e-01,  1.3049e-01,  ..., -1.3338e+00,\n",
      "            7.4862e-02,  2.1580e-01],\n",
      "          [ 8.7299e-01,  3.7536e-01, -9.8682e-02,  ...,  1.7746e-01,\n",
      "            5.0789e-01,  1.2299e-01],\n",
      "          [ 2.9646e-01, -8.9614e-02, -4.7184e-01,  ...,  1.6908e-01,\n",
      "           -1.6346e-01, -4.2455e-01],\n",
      "          [ 4.0208e-02, -2.0449e-01, -1.5041e-01,  ...,  1.2400e+00,\n",
      "           -8.8264e-01, -7.9367e-01]],\n",
      "\n",
      "         [[ 1.0843e-02,  6.4026e-03, -4.3646e-02,  ...,  2.4077e-02,\n",
      "            9.4468e-03,  5.8973e-02],\n",
      "          [ 4.2182e-02, -1.4801e-01,  5.0387e-01,  ..., -2.9121e-01,\n",
      "            2.5832e-01, -4.9977e-02],\n",
      "          [-1.5302e+00, -8.7703e-01,  5.2978e-01,  ...,  2.3529e-01,\n",
      "            7.0955e-01, -6.2093e-02],\n",
      "          [ 1.2297e-01, -6.4723e-01,  2.0047e-01,  ...,  1.5191e-01,\n",
      "           -3.2939e-01, -1.5896e+00],\n",
      "          [ 5.8205e-01, -4.5226e-01,  9.4089e-01,  ...,  1.0672e+00,\n",
      "            9.1876e-01, -4.4069e-01],\n",
      "          [ 3.0981e-01,  8.9040e-01,  3.7806e-01,  ..., -3.7420e-01,\n",
      "            5.4073e-01, -3.7361e-01]],\n",
      "\n",
      "         [[ 4.4565e-02,  3.4269e-02, -7.4407e-02,  ...,  1.0465e-02,\n",
      "            1.5695e-03, -7.3402e-03],\n",
      "          [ 1.0064e+00,  3.3557e-02,  3.8396e-01,  ...,  1.1375e+00,\n",
      "            2.4960e-01, -9.6449e-01],\n",
      "          [ 1.3861e+00,  3.2041e-01,  8.2673e-01,  ...,  1.9781e+00,\n",
      "            4.2032e-01,  1.6241e-01],\n",
      "          [-3.6973e-01,  8.8316e-01,  1.7519e+00,  ...,  8.3878e-01,\n",
      "           -1.1294e-01, -2.6454e-01],\n",
      "          [-6.8918e-01,  5.1354e-02,  1.2453e+00,  ...,  1.4943e+00,\n",
      "            1.1799e-01, -2.2973e-01],\n",
      "          [ 2.2470e-01,  3.5262e-01,  3.7400e-01,  ...,  1.1271e+00,\n",
      "            4.1843e-01,  5.6891e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9953e-02,  2.6850e-02, -8.5255e-03,  ..., -3.1464e-02,\n",
      "           -1.9483e-02,  3.2489e-02],\n",
      "          [-6.3244e-02, -2.0811e-01, -9.0160e-03,  ...,  1.0913e+00,\n",
      "            6.0969e-01,  9.3560e-01],\n",
      "          [-4.0211e-01,  1.9002e+00, -1.1344e+00,  ..., -1.7233e-01,\n",
      "           -1.5391e+00,  8.8107e-01],\n",
      "          [ 4.6853e-01,  1.1353e+00,  1.3345e+00,  ...,  2.3791e+00,\n",
      "           -5.1093e-01,  2.1850e+00],\n",
      "          [-3.8358e-01,  1.4973e+00,  1.1492e+00,  ...,  1.4779e+00,\n",
      "           -3.6566e-01,  1.6506e+00],\n",
      "          [ 4.3739e-01, -2.6294e-02,  1.0304e-01,  ...,  7.4572e-01,\n",
      "            9.6931e-01, -9.4915e-01]],\n",
      "\n",
      "         [[-5.7977e-02, -4.1945e-02,  1.1904e-02,  ...,  2.2974e-04,\n",
      "           -5.0695e-02, -1.0177e-01],\n",
      "          [ 7.0076e-01, -2.9862e-01, -5.3334e-01,  ...,  4.8341e-01,\n",
      "            5.0447e-01, -6.6187e-01],\n",
      "          [ 6.0877e-01, -1.8534e+00, -5.0230e-01,  ..., -5.6498e-01,\n",
      "            1.1364e+00, -5.6654e-01],\n",
      "          [ 1.5720e-01, -9.3112e-01, -5.2124e-01,  ..., -3.3035e-01,\n",
      "           -4.9974e-03, -1.5219e-01],\n",
      "          [ 1.3850e+00, -1.8034e+00, -3.8061e-01,  ..., -1.0218e+00,\n",
      "            3.4655e-01, -5.1384e-01],\n",
      "          [ 1.3836e-01,  7.8253e-02, -1.3045e-01,  ...,  1.6218e-01,\n",
      "            5.8780e-01,  2.0247e-01]],\n",
      "\n",
      "         [[ 8.0821e-04,  4.4958e-02, -5.5915e-02,  ..., -2.7126e-02,\n",
      "            2.8494e-02, -5.9690e-03],\n",
      "          [ 3.7651e-01,  8.9126e-01,  6.6284e-01,  ..., -1.2527e+00,\n",
      "           -1.3042e+00, -8.0188e-01],\n",
      "          [ 7.0886e-01, -2.4365e-01, -7.9684e-01,  ..., -9.9239e-02,\n",
      "           -4.1440e-04, -1.1288e+00],\n",
      "          [ 6.0619e-01, -1.0717e+00, -3.5539e-01,  ..., -1.2357e+00,\n",
      "            9.6599e-01, -3.8069e-01],\n",
      "          [ 2.5693e-01, -5.4930e-01, -9.1037e-01,  ..., -3.2161e-01,\n",
      "            1.6185e+00, -3.5925e-01],\n",
      "          [-8.2841e-02, -1.8849e+00, -2.4126e-01,  ..., -6.3924e-01,\n",
      "            6.4325e-01, -1.2775e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5247,  0.4949, -0.8443,  ..., -1.0267, -1.3272,  0.2004],\n",
      "          [ 1.3351,  0.2915,  0.5370,  ...,  2.0823, -0.1746, -1.3214],\n",
      "          [ 0.6130, -0.2623, -0.5150,  ...,  1.7134,  1.0325, -1.3882],\n",
      "          [ 0.4956, -0.2934,  1.0520,  ...,  2.8170,  0.1840, -0.6004],\n",
      "          [ 1.0199, -0.8584,  1.2539,  ...,  3.2690,  0.2672, -0.9297],\n",
      "          [ 0.7361, -0.2945,  0.6350,  ...,  2.3141, -0.3334, -0.9642]],\n",
      "\n",
      "         [[ 0.8547, -2.0830,  0.1413,  ...,  0.2375, -2.4965, -0.4349],\n",
      "          [ 0.4831,  1.9341, -0.8300,  ..., -0.2690,  1.2970, -0.9318],\n",
      "          [ 0.8308,  3.1731, -1.4079,  ...,  0.3557,  0.8259, -0.7248],\n",
      "          [ 0.7804,  3.0297, -1.6959,  ...,  0.8058,  2.3524, -0.5103],\n",
      "          [ 0.9647,  3.7635, -1.9430,  ...,  0.4459,  1.1446, -0.4601],\n",
      "          [ 1.5187,  2.3564, -1.5870,  ...,  0.7813,  1.9170, -1.5025]],\n",
      "\n",
      "         [[ 1.0218,  0.3955, -0.1749,  ..., -0.8287, -1.4002, -0.3574],\n",
      "          [ 0.2096,  0.0317, -1.5685,  ...,  1.3762,  0.4408, -0.9132],\n",
      "          [-2.0941,  0.3492, -2.5183,  ...,  1.2349,  0.5306, -1.4540],\n",
      "          [-0.9099, -0.2332, -0.8454,  ...,  2.0371, -0.2074,  0.1125],\n",
      "          [-0.8487, -0.6331, -1.9909,  ...,  1.4657, -0.5709, -0.8473],\n",
      "          [ 0.4241,  0.2506, -0.8338,  ...,  1.6460, -0.4312, -0.6852]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2127, -0.5775,  0.4853,  ..., -0.6815,  1.0601,  0.2889],\n",
      "          [ 0.7730,  0.2692, -1.6327,  ..., -3.3057,  0.5885,  0.9874],\n",
      "          [ 0.1541,  2.5521, -0.2810,  ..., -1.5260, -2.4974, -1.3792],\n",
      "          [-1.1458,  1.7980, -1.3355,  ..., -2.2156, -0.6960,  0.7205],\n",
      "          [-1.9692,  2.4486, -3.2266,  ..., -1.0001, -1.1373,  0.3075],\n",
      "          [-0.2619, -0.2997, -1.5287,  ..., -4.0431, -0.0235,  1.6034]],\n",
      "\n",
      "         [[ 0.2434,  0.5567,  0.5359,  ...,  0.7319,  0.0880,  0.8371],\n",
      "          [ 0.8655,  2.7919,  0.3341,  ...,  1.6821, -0.7167, -0.2729],\n",
      "          [ 0.1749,  2.7262, -1.5653,  ...,  1.4221,  0.6800,  0.5448],\n",
      "          [ 1.1979,  2.7367, -0.2360,  ...,  1.5289, -0.6156,  0.6384],\n",
      "          [-0.2647,  2.8791, -0.6439,  ...,  0.6693, -1.0797,  0.1647],\n",
      "          [ 1.6332,  2.2798,  0.6610,  ...,  0.4145, -1.1649,  0.2100]],\n",
      "\n",
      "         [[-0.7243,  0.3011, -1.5991,  ..., -0.4234,  0.2265, -1.3258],\n",
      "          [-1.0928,  0.3030, -0.1954,  ...,  0.7091,  0.2618, -1.7367],\n",
      "          [-0.5196,  0.5007,  0.2720,  ...,  0.7480,  0.1141,  0.4781],\n",
      "          [-0.3182,  2.6044, -0.4324,  ...,  0.9895, -0.0471, -0.0660],\n",
      "          [-0.1938,  0.5792, -0.7385,  ..., -0.8944, -0.3173, -0.6166],\n",
      "          [ 0.1336,  0.1718,  0.8956,  ...,  0.8828,  1.7896, -1.8202]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[-3.9992e-03,  3.1354e-02, -6.2856e-02,  ...,  6.3562e-02,\n",
      "           -5.2594e-02, -1.0184e-01],\n",
      "          [-8.0971e-02,  9.4793e-01,  9.8768e-01,  ..., -9.2856e-01,\n",
      "           -1.0841e+00, -1.0147e+00],\n",
      "          [ 5.3177e-01, -5.2768e-02,  7.0404e-02,  ..., -1.4982e-01,\n",
      "            1.7254e+00, -3.2029e-01],\n",
      "          [-3.7527e-01,  6.2659e-01,  4.4375e-01,  ..., -3.0151e-01,\n",
      "            4.2112e-01,  3.8501e-01],\n",
      "          [-2.6922e-01, -1.3732e+00, -2.8914e-01,  ..., -5.3311e-01,\n",
      "            9.0174e-01,  1.1525e+00],\n",
      "          [ 2.2283e-01,  5.0835e-01,  1.5261e+00,  ..., -7.9413e-01,\n",
      "           -5.6556e-01, -7.2697e-01]],\n",
      "\n",
      "         [[ 3.8070e-02,  1.1806e-02,  3.5814e-02,  ..., -3.5340e-02,\n",
      "           -2.8176e-02,  1.4364e-03],\n",
      "          [ 1.3098e-01,  1.4555e+00,  8.9600e-01,  ...,  8.8328e-01,\n",
      "           -4.9710e-01,  9.6165e-01],\n",
      "          [ 1.2999e+00,  4.2824e-01,  1.3798e-01,  ..., -6.9389e-01,\n",
      "           -4.6948e-01, -5.7026e-01],\n",
      "          [ 6.4549e-01, -2.8031e-02, -4.6002e-01,  ..., -3.3917e-02,\n",
      "            1.9929e+00,  5.0943e-01],\n",
      "          [ 3.3120e-01,  5.6861e-01, -3.1446e-01,  ...,  6.8445e-01,\n",
      "            1.0843e+00, -5.2148e-01],\n",
      "          [-2.5186e-01,  1.4089e+00,  1.0683e-01,  ..., -3.3015e-01,\n",
      "           -5.3264e-01,  2.1154e-01]],\n",
      "\n",
      "         [[-8.6320e-03,  3.0234e-02, -1.7854e-02,  ...,  2.7625e-02,\n",
      "            2.5417e-02,  5.3863e-02],\n",
      "          [ 6.8687e-01,  2.9709e-01,  6.3241e-01,  ...,  1.2536e+00,\n",
      "           -1.1811e+00, -2.3842e-01],\n",
      "          [ 1.4733e+00,  1.1427e+00, -1.1538e+00,  ...,  1.3584e+00,\n",
      "           -2.1087e-01, -1.1615e+00],\n",
      "          [-1.2910e+00, -4.0668e-01, -3.4797e-01,  ..., -2.5707e-01,\n",
      "           -3.0803e-01, -4.0138e-01],\n",
      "          [-4.5409e-01, -9.9997e-01,  9.1260e-01,  ...,  1.0485e+00,\n",
      "           -1.1319e+00, -2.8872e-01],\n",
      "          [ 3.8852e-01, -2.1821e-01,  1.1376e+00,  ...,  2.2753e-01,\n",
      "           -3.3774e-01, -2.2651e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.4222e-02,  2.6930e-02,  5.9625e-03,  ..., -2.7150e-02,\n",
      "           -8.8860e-03,  1.1950e-02],\n",
      "          [-1.2579e+00,  1.8200e-01,  2.0702e+00,  ...,  2.0052e-02,\n",
      "           -1.2653e+00,  1.3773e+00],\n",
      "          [-1.0997e+00,  3.8824e-01,  2.4496e+00,  ..., -6.3761e-01,\n",
      "           -2.7149e-01,  9.7474e-01],\n",
      "          [-1.5979e+00,  1.3170e+00,  1.8780e+00,  ...,  6.5215e-01,\n",
      "            1.4538e+00,  5.9654e-01],\n",
      "          [-5.9805e-01,  1.7586e+00,  1.9761e+00,  ...,  1.9857e+00,\n",
      "           -2.4300e-01,  4.7130e-01],\n",
      "          [-1.0958e+00, -9.1270e-02,  1.4358e+00,  ...,  1.5645e+00,\n",
      "           -1.6633e+00,  1.4747e+00]],\n",
      "\n",
      "         [[ 8.5898e-02,  1.0410e-03,  5.7983e-02,  ...,  3.7544e-02,\n",
      "            5.2914e-02,  4.0292e-02],\n",
      "          [ 3.7628e-01,  7.2552e-01,  4.8495e-01,  ..., -6.2518e-01,\n",
      "            1.5151e+00, -5.8030e-01],\n",
      "          [ 2.7957e-01,  1.6060e+00, -1.6762e+00,  ..., -1.4493e+00,\n",
      "            1.0289e+00, -1.7817e+00],\n",
      "          [-6.4121e-01,  1.0087e+00,  5.8166e-01,  ..., -8.3526e-01,\n",
      "            1.0884e+00, -1.8756e+00],\n",
      "          [ 1.1603e+00,  8.4831e-01, -5.4076e-01,  ..., -1.2531e-01,\n",
      "            8.9639e-01,  3.1079e-01],\n",
      "          [ 4.9931e-01,  7.3069e-01,  1.1010e+00,  ...,  2.4732e-01,\n",
      "            2.5417e-01, -2.0704e+00]],\n",
      "\n",
      "         [[-1.1047e-01,  2.1945e-02, -7.2382e-02,  ..., -8.1682e-02,\n",
      "            5.8579e-02, -4.3474e-03],\n",
      "          [ 9.2852e-02,  2.1514e-01, -1.7111e-01,  ..., -7.6387e-01,\n",
      "            1.9653e-01,  3.6329e-01],\n",
      "          [ 7.4591e-01, -3.5940e-01, -1.4472e-01,  ..., -1.9708e+00,\n",
      "           -8.1406e-02, -7.9354e-01],\n",
      "          [-9.8757e-02, -8.4458e-01,  3.4920e-01,  ..., -3.4047e-02,\n",
      "           -1.8585e+00,  2.3052e-01],\n",
      "          [-1.4226e-01, -4.1820e-01, -2.7889e-01,  ..., -1.5280e-01,\n",
      "            1.1348e+00,  3.2104e-01],\n",
      "          [-5.3343e-01, -2.3753e-01,  1.2276e-01,  ..., -8.3658e-01,\n",
      "           -6.4181e-01,  6.3223e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7301, -0.3017, -0.3031,  ...,  0.1497,  0.3111, -0.5225],\n",
      "          [ 0.4479,  0.1926, -0.7345,  ...,  0.9612, -1.6255,  0.0375],\n",
      "          [ 0.4727,  0.8442, -0.1085,  ...,  2.3442, -0.8593,  0.2298],\n",
      "          [ 0.4363,  0.5042, -1.1721,  ...,  1.6572, -0.7513,  0.8146],\n",
      "          [ 0.4437,  0.0443, -1.4051,  ...,  1.5749, -0.5458, -0.0942],\n",
      "          [ 1.2962,  0.7106, -0.4133,  ...,  1.2965, -2.1147, -1.0799]],\n",
      "\n",
      "         [[ 0.1182, -0.0923,  2.2984,  ...,  0.2618,  0.0666, -0.2087],\n",
      "          [-0.2271, -0.3946, -1.3548,  ...,  1.2734, -0.7015, -0.5245],\n",
      "          [-0.6588, -1.4837, -0.7236,  ...,  0.3787, -0.0977,  0.6054],\n",
      "          [ 0.4469, -0.6170, -0.7596,  ...,  0.0038, -0.2550, -0.0224],\n",
      "          [ 1.1533, -1.2130, -0.4592,  ..., -0.6583, -0.8038,  0.0550],\n",
      "          [ 0.5038, -1.1547, -0.6742,  ...,  0.6594,  0.2296,  0.0871]],\n",
      "\n",
      "         [[-0.1914,  1.0477,  0.4763,  ..., -0.5336,  0.3140, -0.1149],\n",
      "          [-0.4442,  0.0950, -0.3881,  ...,  1.5470,  0.8762, -0.2976],\n",
      "          [-1.1832, -0.4522,  0.7553,  ...,  1.0517,  0.2123, -0.8387],\n",
      "          [-0.7818, -0.6581,  0.2901,  ...,  2.3657,  0.3350, -0.9794],\n",
      "          [-1.3319, -0.3933, -0.9592,  ...,  1.6682,  0.9889, -1.3960],\n",
      "          [-0.4641,  0.4978, -0.5712,  ...,  1.8327,  1.2782, -0.5750]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5749,  0.9934, -0.8806,  ..., -0.7286,  0.7094,  0.8631],\n",
      "          [ 0.2083,  0.3336, -1.1636,  ..., -0.3558, -0.3045,  0.3746],\n",
      "          [ 0.3569,  0.7590, -1.3219,  ..., -0.7084, -0.4914,  0.1815],\n",
      "          [-0.3331,  0.6655, -0.7047,  ...,  0.8336, -0.5042,  0.7863],\n",
      "          [-0.0660,  0.0772, -0.1955,  ...,  0.6522, -0.2581,  0.5320],\n",
      "          [-0.1715,  0.6115, -0.7641,  ..., -0.3731, -0.2593,  0.0759]],\n",
      "\n",
      "         [[-0.4152,  0.3600,  0.3492,  ...,  0.7109,  0.0221, -0.1103],\n",
      "          [-1.3041,  1.0499, -0.8384,  ...,  0.3383, -0.7068, -1.0845],\n",
      "          [ 0.1596,  0.4771,  0.5645,  ...,  1.5919, -2.0884,  0.3208],\n",
      "          [-0.6920,  1.6372,  0.3479,  ...,  1.3373, -1.3630,  0.3910],\n",
      "          [-0.5171,  0.3389,  0.0945,  ...,  1.2017, -1.1347,  0.0643],\n",
      "          [-0.9837,  1.0616, -1.3278,  ...,  0.3157, -0.7142, -0.8510]],\n",
      "\n",
      "         [[-0.7533, -0.0127,  0.4540,  ..., -0.1002,  0.0298, -0.0530],\n",
      "          [ 0.2473, -0.4783,  1.5322,  ...,  0.2154, -0.1943,  0.6086],\n",
      "          [-1.6073,  0.5077,  0.2192,  ..., -0.2576, -0.8390,  0.9065],\n",
      "          [-0.9174, -0.9268,  0.5644,  ..., -0.5462, -0.3548,  0.7615],\n",
      "          [-0.4633, -1.0399, -0.4218,  ..., -1.5781, -0.3868,  0.9603],\n",
      "          [ 0.0304, -1.0477,  2.2978,  ...,  0.2184,  1.0615,  0.7666]]]],\n",
      "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0671, -0.1002, -0.1700,  ..., -0.2930,  0.2775, -0.1651],\n",
      "          [-0.8450,  0.8397,  0.1317,  ...,  2.8629, -2.2447,  0.9398],\n",
      "          [ 0.8804,  0.4136, -0.8149,  ...,  3.4053, -1.7745, -0.5792],\n",
      "          [ 1.7001,  0.3965, -0.6537,  ...,  2.5530, -3.1879,  2.3109],\n",
      "          [ 0.2188, -1.2870,  1.3491,  ...,  2.1471, -1.1612,  0.9111],\n",
      "          [-0.8822,  0.3706,  1.4674,  ...,  2.3727, -2.2108,  0.7445]],\n",
      "\n",
      "         [[ 0.0821, -0.0340,  0.0438,  ..., -0.0579, -0.1031,  0.1804],\n",
      "          [ 0.5406,  0.3952, -0.6999,  ...,  0.2483, -1.1047, -0.3074],\n",
      "          [ 0.0100, -0.5697,  0.6873,  ..., -0.6639,  1.0166, -0.2919],\n",
      "          [ 0.5850,  0.2916,  0.1950,  ...,  0.2405, -1.2613,  0.0487],\n",
      "          [-0.3699,  0.6167, -0.3594,  ...,  1.3416, -1.2318, -0.8112],\n",
      "          [ 0.3250,  0.1369, -0.1806,  ...,  0.4198,  0.3771,  0.3064]],\n",
      "\n",
      "         [[-0.0051,  0.0322, -0.0347,  ...,  0.0763,  0.0066,  0.0721],\n",
      "          [ 0.5693, -0.1538, -0.7002,  ...,  0.5776,  0.9860, -0.4581],\n",
      "          [ 0.8550, -1.7987, -1.2918,  ...,  0.1282,  1.4262, -1.9111],\n",
      "          [-0.3586, -1.3608,  0.6638,  ..., -1.0451,  0.2963, -1.4533],\n",
      "          [-1.1271, -0.4507, -1.2523,  ..., -0.1887,  0.5243, -0.3830],\n",
      "          [ 1.4804, -0.8313, -0.9107,  ...,  0.0497,  2.0346,  0.3771]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0118, -0.0044,  0.0706,  ...,  0.0733, -0.0157,  0.0045],\n",
      "          [-0.3157, -0.1935, -0.1559,  ...,  0.4542,  0.1993, -0.8848],\n",
      "          [ 1.9094,  2.0537,  0.1424,  ...,  1.7988, -1.1404,  1.0037],\n",
      "          [ 0.9076,  0.0753, -0.9950,  ...,  1.1022, -1.1949, -1.9929],\n",
      "          [ 0.5321,  0.2315, -0.7646,  ...,  1.2609,  0.6000,  0.0812],\n",
      "          [-0.9541, -0.5959, -0.4438,  ...,  0.6838,  0.6467,  0.0297]],\n",
      "\n",
      "         [[-0.1674, -0.0717,  0.0836,  ..., -0.0684,  0.0721, -0.0918],\n",
      "          [-0.7923, -0.4966,  1.0093,  ..., -0.4697,  0.7579, -0.2786],\n",
      "          [-0.5845, -0.3398,  0.2096,  ..., -0.6172,  0.7525, -0.2205],\n",
      "          [-0.6758, -1.3223,  1.0787,  ..., -0.4043,  0.0691,  0.2043],\n",
      "          [ 0.4933,  0.2276,  0.4520,  ..., -0.1636,  1.1387, -0.5544],\n",
      "          [-1.5891,  0.3640,  0.7885,  ..., -0.5086,  0.7237, -0.1601]],\n",
      "\n",
      "         [[ 0.1095, -0.1060,  0.1534,  ..., -0.1137,  0.0091, -0.1853],\n",
      "          [-0.2533,  0.3895,  1.0629,  ..., -0.1186,  0.2770,  0.5211],\n",
      "          [ 0.7285,  0.0240,  0.5378,  ...,  0.4721, -1.4991, -0.2776],\n",
      "          [ 0.5771, -0.2084,  0.6411,  ..., -0.2638, -0.9217,  0.6920],\n",
      "          [-0.8580,  0.5946,  0.5680,  ..., -1.6925, -0.4817, -0.1798],\n",
      "          [-0.2669,  1.3080,  0.3446,  ...,  0.5596, -0.4938,  0.8683]]]],\n",
      "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def main():\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2')\n",
    "    model = GPT2Model.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_text = \"Hello, how are you?\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate output\n",
    "    output = model(input_ids)\n",
    "\n",
    "    # Print output\n",
    "    print(output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
