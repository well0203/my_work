{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. TimeLLM](#1-timellm)\n",
    "- [2. TimeLLM](#2-timellm-336)\n",
    "\n",
    "Results for TimeLLM. The first one is default input length 512, the second one: 336."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE']\n",
    "num_cols = [5]\n",
    "seq_lens = [512]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "model = \"TimeLLM\"\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_DE.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "patience = 5\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32\n",
    "\n",
    "# List to store the results\n",
    "timellm_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-16 13:35:46,795] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 13:35:46,816] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 13:35:47,677] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "train 143885\n",
      "train 143885\n",
      "val 31085\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "val 31085\n",
      "test 31085\n",
      "test 31085\n",
      "[2024-11-16 13:35:49,712] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-16 13:35:50,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-16 13:35:51,000] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-16 13:35:51,351] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-16 13:35:51,352] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-16 13:35:51,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 171.88 GB, percent = 17.1%\n",
      "[2024-11-16 13:35:51,671] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-16 13:35:51,672] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 13:35:51,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.23 GB, percent = 17.1%\n",
      "[2024-11-16 13:35:51,672] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-16 13:35:51,876] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-16 13:35:51,877] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 13:35:51,877] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.55 GB, percent = 17.1%\n",
      "[2024-11-16 13:35:51,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f53b3468590>\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-16 13:35:51,880] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0632429\n",
      "\tspeed: 0.3523s/iter; left time: 31645.8063s\n",
      "\titers: 200, epoch: 1 | loss: 0.0507106\n",
      "\tspeed: 0.3228s/iter; left time: 28965.5795s\n",
      "\titers: 300, epoch: 1 | loss: 0.0394340\n",
      "\tspeed: 0.3229s/iter; left time: 28935.6049s\n",
      "\titers: 400, epoch: 1 | loss: 0.0388287\n",
      "\tspeed: 0.3229s/iter; left time: 28909.7832s\n",
      "\titers: 500, epoch: 1 | loss: 0.0251836\n",
      "\tspeed: 0.3229s/iter; left time: 28876.4997s\n",
      "\titers: 600, epoch: 1 | loss: 0.0225685\n",
      "\tspeed: 0.3230s/iter; left time: 28851.5746s\n",
      "\titers: 700, epoch: 1 | loss: 0.0203688\n",
      "\tspeed: 0.3229s/iter; left time: 28813.8099s\n",
      "\titers: 800, epoch: 1 | loss: 0.0230455\n",
      "\tspeed: 0.3229s/iter; left time: 28778.3263s\n",
      "\titers: 900, epoch: 1 | loss: 0.0221143\n",
      "\tspeed: 0.3229s/iter; left time: 28746.0771s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0220149\n",
      "\tspeed: 0.3229s/iter; left time: 28708.3550s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0171128\n",
      "\tspeed: 0.3231s/iter; left time: 28698.6023s\n",
      "Epoch: 1 cost time: 00h:06m:04.06s\n",
      "Epoch: 1 | Train Loss: 0.0333040 Vali Loss: 0.0229411 Test Loss: 0.0251673\n",
      "Validation loss decreased (inf --> 0.022941).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0113808\n",
      "\tspeed: 1.2685s/iter; left time: 108237.0048s\n",
      "\titers: 200, epoch: 2 | loss: 0.0146446\n",
      "\tspeed: 0.3227s/iter; left time: 27499.5702s\n",
      "\titers: 300, epoch: 2 | loss: 0.0160543\n",
      "\tspeed: 0.3230s/iter; left time: 27499.5493s\n",
      "\titers: 400, epoch: 2 | loss: 0.0154046\n",
      "\tspeed: 0.3232s/iter; left time: 27476.3707s\n",
      "\titers: 500, epoch: 2 | loss: 0.0173704\n",
      "\tspeed: 0.3231s/iter; left time: 27435.1440s\n",
      "\titers: 600, epoch: 2 | loss: 0.0362225\n",
      "\tspeed: 0.3230s/iter; left time: 27397.2340s\n",
      "\titers: 700, epoch: 2 | loss: 0.0231417\n",
      "\tspeed: 0.2851s/iter; left time: 24156.7660s\n",
      "\titers: 800, epoch: 2 | loss: 0.0158735\n",
      "\tspeed: 0.2628s/iter; left time: 22235.2269s\n",
      "\titers: 900, epoch: 2 | loss: 0.0282127\n",
      "\tspeed: 0.3228s/iter; left time: 27280.8594s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0127809\n",
      "\tspeed: 0.3231s/iter; left time: 27274.1887s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0152286\n",
      "\tspeed: 0.3232s/iter; left time: 27251.1327s\n",
      "Epoch: 2 cost time: 00h:05m:53.62s\n",
      "Epoch: 2 | Train Loss: 0.0189258 Vali Loss: 0.0213288 Test Loss: 0.0228869\n",
      "Validation loss decreased (0.022941 --> 0.021329).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0147866\n",
      "\tspeed: 1.2393s/iter; left time: 100172.1175s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251924\n",
      "\tspeed: 0.3226s/iter; left time: 26042.7383s\n",
      "\titers: 300, epoch: 3 | loss: 0.0179662\n",
      "\tspeed: 0.3218s/iter; left time: 25948.7631s\n",
      "\titers: 400, epoch: 3 | loss: 0.0170750\n",
      "\tspeed: 0.3227s/iter; left time: 25983.4054s\n",
      "\titers: 500, epoch: 3 | loss: 0.0222653\n",
      "\tspeed: 0.3229s/iter; left time: 25973.3680s\n",
      "\titers: 600, epoch: 3 | loss: 0.0223925\n",
      "\tspeed: 0.3228s/iter; left time: 25928.9061s\n",
      "\titers: 700, epoch: 3 | loss: 0.0192496\n",
      "\tspeed: 0.3228s/iter; left time: 25901.0748s\n",
      "\titers: 800, epoch: 3 | loss: 0.0108626\n",
      "\tspeed: 0.3234s/iter; left time: 25916.4432s\n",
      "\titers: 900, epoch: 3 | loss: 0.0230040\n",
      "\tspeed: 0.3236s/iter; left time: 25894.3792s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0219360\n",
      "\tspeed: 0.3237s/iter; left time: 25870.1387s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0159164\n",
      "\tspeed: 0.3236s/iter; left time: 25828.7384s\n",
      "Epoch: 3 cost time: 00h:06m:03.50s\n",
      "Epoch: 3 | Train Loss: 0.0180037 Vali Loss: 0.0207199 Test Loss: 0.0221680\n",
      "Validation loss decreased (0.021329 --> 0.020720).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0195420\n",
      "\tspeed: 1.2593s/iter; left time: 96126.8123s\n",
      "\titers: 200, epoch: 4 | loss: 0.0174730\n",
      "\tspeed: 0.3234s/iter; left time: 24654.0084s\n",
      "\titers: 300, epoch: 4 | loss: 0.0160964\n",
      "\tspeed: 0.3236s/iter; left time: 24636.3340s\n",
      "\titers: 400, epoch: 4 | loss: 0.0249365\n",
      "\tspeed: 0.3238s/iter; left time: 24619.7518s\n",
      "\titers: 500, epoch: 4 | loss: 0.0150605\n",
      "\tspeed: 0.3236s/iter; left time: 24570.3229s\n",
      "\titers: 600, epoch: 4 | loss: 0.0123712\n",
      "\tspeed: 0.3235s/iter; left time: 24532.5339s\n",
      "\titers: 700, epoch: 4 | loss: 0.0285275\n",
      "\tspeed: 0.3234s/iter; left time: 24494.2035s\n",
      "\titers: 800, epoch: 4 | loss: 0.0258730\n",
      "\tspeed: 0.3235s/iter; left time: 24465.4872s\n",
      "\titers: 900, epoch: 4 | loss: 0.0145841\n",
      "\tspeed: 0.3233s/iter; left time: 24420.9361s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0179337\n",
      "\tspeed: 0.3231s/iter; left time: 24372.6274s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0190074\n",
      "\tspeed: 0.3234s/iter; left time: 24362.5242s\n",
      "Epoch: 4 cost time: 00h:06m:04.01s\n",
      "Epoch: 4 | Train Loss: 0.0173761 Vali Loss: 0.0201705 Test Loss: 0.0218201\n",
      "Validation loss decreased (0.020720 --> 0.020170).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0248572\n",
      "\tspeed: 1.2281s/iter; left time: 88221.7166s\n",
      "\titers: 200, epoch: 5 | loss: 0.0181087\n",
      "\tspeed: 0.3236s/iter; left time: 23212.2930s\n",
      "\titers: 300, epoch: 5 | loss: 0.0130525\n",
      "\tspeed: 0.3238s/iter; left time: 23198.4974s\n",
      "\titers: 400, epoch: 5 | loss: 0.0138662\n",
      "\tspeed: 0.3239s/iter; left time: 23173.0373s\n",
      "\titers: 500, epoch: 5 | loss: 0.0155967\n",
      "\tspeed: 0.3239s/iter; left time: 23135.2303s\n",
      "\titers: 600, epoch: 5 | loss: 0.0198725\n",
      "\tspeed: 0.3238s/iter; left time: 23095.7626s\n",
      "\titers: 700, epoch: 5 | loss: 0.0202901\n",
      "\tspeed: 0.3207s/iter; left time: 22844.8257s\n",
      "\titers: 800, epoch: 5 | loss: 0.0220147\n",
      "\tspeed: 0.3239s/iter; left time: 23043.2874s\n",
      "\titers: 900, epoch: 5 | loss: 0.0173096\n",
      "\tspeed: 0.3239s/iter; left time: 23006.4260s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0179298\n",
      "\tspeed: 0.3237s/iter; left time: 22962.7114s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0144313\n",
      "\tspeed: 0.3237s/iter; left time: 22930.7100s\n",
      "Epoch: 5 cost time: 00h:06m:04.12s\n",
      "Epoch: 5 | Train Loss: 0.0169353 Vali Loss: 0.0197934 Test Loss: 0.0216913\n",
      "Validation loss decreased (0.020170 --> 0.019793).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0178052\n",
      "\tspeed: 1.2141s/iter; left time: 81761.1781s\n",
      "\titers: 200, epoch: 6 | loss: 0.0166879\n",
      "\tspeed: 0.3222s/iter; left time: 21668.1074s\n",
      "\titers: 300, epoch: 6 | loss: 0.0215266\n",
      "\tspeed: 0.3225s/iter; left time: 21656.1161s\n",
      "\titers: 400, epoch: 6 | loss: 0.0158959\n",
      "\tspeed: 0.3230s/iter; left time: 21652.2129s\n",
      "\titers: 500, epoch: 6 | loss: 0.0225539\n",
      "\tspeed: 0.3235s/iter; left time: 21656.3385s\n",
      "\titers: 600, epoch: 6 | loss: 0.0142451\n",
      "\tspeed: 0.3239s/iter; left time: 21647.6084s\n",
      "\titers: 700, epoch: 6 | loss: 0.0146011\n",
      "\tspeed: 0.3237s/iter; left time: 21605.6043s\n",
      "\titers: 800, epoch: 6 | loss: 0.0156253\n",
      "\tspeed: 0.3240s/iter; left time: 21589.3386s\n",
      "\titers: 900, epoch: 6 | loss: 0.0157494\n",
      "\tspeed: 0.3240s/iter; left time: 21556.3283s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0117435\n",
      "\tspeed: 0.3239s/iter; left time: 21519.5069s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0189988\n",
      "\tspeed: 0.3229s/iter; left time: 21419.9093s\n",
      "Epoch: 6 cost time: 00h:06m:03.76s\n",
      "Epoch: 6 | Train Loss: 0.0165692 Vali Loss: 0.0196932 Test Loss: 0.0218816\n",
      "Validation loss decreased (0.019793 --> 0.019693).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0172309\n",
      "\tspeed: 1.2322s/iter; left time: 77437.5813s\n",
      "\titers: 200, epoch: 7 | loss: 0.0123486\n",
      "\tspeed: 0.3239s/iter; left time: 20322.1704s\n",
      "\titers: 300, epoch: 7 | loss: 0.0212684\n",
      "\tspeed: 0.3223s/iter; left time: 20192.9327s\n",
      "\titers: 400, epoch: 7 | loss: 0.0195458\n",
      "\tspeed: 0.3229s/iter; left time: 20195.5811s\n",
      "\titers: 500, epoch: 7 | loss: 0.0102791\n",
      "\tspeed: 0.3243s/iter; left time: 20252.8605s\n",
      "\titers: 600, epoch: 7 | loss: 0.0223481\n",
      "\tspeed: 0.3239s/iter; left time: 20191.7938s\n",
      "\titers: 700, epoch: 7 | loss: 0.0160351\n",
      "\tspeed: 0.3206s/iter; left time: 19956.1453s\n",
      "\titers: 800, epoch: 7 | loss: 0.0161180\n",
      "\tspeed: 0.3227s/iter; left time: 20056.8657s\n",
      "\titers: 900, epoch: 7 | loss: 0.0166668\n",
      "\tspeed: 0.3230s/iter; left time: 20038.1523s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0117904\n",
      "\tspeed: 0.3215s/iter; left time: 19912.8472s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0165729\n",
      "\tspeed: 0.3235s/iter; left time: 20004.3691s\n",
      "Epoch: 7 cost time: 00h:06m:03.53s\n",
      "Epoch: 7 | Train Loss: 0.0163646 Vali Loss: 0.0194863 Test Loss: 0.0215178\n",
      "Validation loss decreased (0.019693 --> 0.019486).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0151283\n",
      "\tspeed: 1.2214s/iter; left time: 71268.5130s\n",
      "\titers: 200, epoch: 8 | loss: 0.0133968\n",
      "\tspeed: 0.3230s/iter; left time: 18813.9660s\n",
      "\titers: 300, epoch: 8 | loss: 0.0154707\n",
      "\tspeed: 0.3212s/iter; left time: 18676.8243s\n",
      "\titers: 400, epoch: 8 | loss: 0.0145500\n",
      "\tspeed: 0.3225s/iter; left time: 18721.9795s\n",
      "\titers: 500, epoch: 8 | loss: 0.0161144\n",
      "\tspeed: 0.3240s/iter; left time: 18774.5140s\n",
      "\titers: 600, epoch: 8 | loss: 0.0167301\n",
      "\tspeed: 0.3225s/iter; left time: 18654.1833s\n",
      "\titers: 700, epoch: 8 | loss: 0.0157079\n",
      "\tspeed: 0.3231s/iter; left time: 18655.8284s\n",
      "\titers: 800, epoch: 8 | loss: 0.0133750\n",
      "\tspeed: 0.3224s/iter; left time: 18585.1511s\n",
      "\titers: 900, epoch: 8 | loss: 0.0157046\n",
      "\tspeed: 0.3231s/iter; left time: 18591.7227s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0103165\n",
      "\tspeed: 0.3238s/iter; left time: 18602.5103s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0139662\n",
      "\tspeed: 0.3239s/iter; left time: 18576.8395s\n",
      "Epoch: 8 cost time: 00h:06m:03.11s\n",
      "Epoch: 8 | Train Loss: 0.0162527 Vali Loss: 0.0196475 Test Loss: 0.0218817\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0137342\n",
      "\tspeed: 1.1821s/iter; left time: 63658.6641s\n",
      "\titers: 200, epoch: 9 | loss: 0.0193019\n",
      "\tspeed: 0.3220s/iter; left time: 17308.2726s\n",
      "\titers: 300, epoch: 9 | loss: 0.0143334\n",
      "\tspeed: 0.3204s/iter; left time: 17192.0739s\n",
      "\titers: 400, epoch: 9 | loss: 0.0120264\n",
      "\tspeed: 0.3234s/iter; left time: 17319.3794s\n",
      "\titers: 500, epoch: 9 | loss: 0.0147567\n",
      "\tspeed: 0.3234s/iter; left time: 17284.7771s\n",
      "\titers: 600, epoch: 9 | loss: 0.0199759\n",
      "\tspeed: 0.3231s/iter; left time: 17240.7187s\n",
      "\titers: 700, epoch: 9 | loss: 0.0136194\n",
      "\tspeed: 0.3227s/iter; left time: 17182.8881s\n",
      "\titers: 800, epoch: 9 | loss: 0.0103802\n",
      "\tspeed: 0.3214s/iter; left time: 17082.7926s\n",
      "\titers: 900, epoch: 9 | loss: 0.0166926\n",
      "\tspeed: 0.3182s/iter; left time: 16881.1642s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0198049\n",
      "\tspeed: 0.3226s/iter; left time: 17081.0444s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0279420\n",
      "\tspeed: 0.3218s/iter; left time: 17006.9184s\n",
      "Epoch: 9 cost time: 00h:06m:02.41s\n",
      "Epoch: 9 | Train Loss: 0.0158304 Vali Loss: 0.0192229 Test Loss: 0.0215448\n",
      "Validation loss decreased (0.019486 --> 0.019223).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0203107\n",
      "\tspeed: 1.2472s/iter; left time: 61555.7164s\n",
      "\titers: 200, epoch: 10 | loss: 0.0133896\n",
      "\tspeed: 0.3223s/iter; left time: 15875.1190s\n",
      "\titers: 300, epoch: 10 | loss: 0.0116833\n",
      "\tspeed: 0.3217s/iter; left time: 15813.3395s\n",
      "\titers: 400, epoch: 10 | loss: 0.0166726\n",
      "\tspeed: 0.3182s/iter; left time: 15611.8300s\n",
      "\titers: 500, epoch: 10 | loss: 0.0108126\n",
      "\tspeed: 0.3180s/iter; left time: 15567.2747s\n",
      "\titers: 600, epoch: 10 | loss: 0.0193221\n",
      "\tspeed: 0.3178s/iter; left time: 15524.8963s\n",
      "\titers: 700, epoch: 10 | loss: 0.0144544\n",
      "\tspeed: 0.3169s/iter; left time: 15452.7183s\n",
      "\titers: 800, epoch: 10 | loss: 0.0147057\n",
      "\tspeed: 0.3200s/iter; left time: 15569.0658s\n",
      "\titers: 900, epoch: 10 | loss: 0.0103039\n",
      "\tspeed: 0.3205s/iter; left time: 15564.3570s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0158291\n",
      "\tspeed: 0.3228s/iter; left time: 15641.0012s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0099726\n",
      "\tspeed: 0.3181s/iter; left time: 15384.2536s\n",
      "Epoch: 10 cost time: 00h:06m:00.19s\n",
      "Epoch: 10 | Train Loss: 0.0154770 Vali Loss: 0.0195692 Test Loss: 0.0220183\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0165156\n",
      "\tspeed: 1.1787s/iter; left time: 52878.7841s\n",
      "\titers: 200, epoch: 11 | loss: 0.0128128\n",
      "\tspeed: 0.3211s/iter; left time: 14370.7279s\n",
      "\titers: 300, epoch: 11 | loss: 0.0199038\n",
      "\tspeed: 0.3201s/iter; left time: 14297.0739s\n",
      "\titers: 400, epoch: 11 | loss: 0.0113736\n",
      "\tspeed: 0.3203s/iter; left time: 14271.6793s\n",
      "\titers: 500, epoch: 11 | loss: 0.0177854\n",
      "\tspeed: 0.3196s/iter; left time: 14208.5477s\n",
      "\titers: 600, epoch: 11 | loss: 0.0116605\n",
      "\tspeed: 0.3232s/iter; left time: 14337.9315s\n",
      "\titers: 700, epoch: 11 | loss: 0.0130346\n",
      "\tspeed: 0.3210s/iter; left time: 14208.8342s\n",
      "\titers: 800, epoch: 11 | loss: 0.0176324\n",
      "\tspeed: 0.3143s/iter; left time: 13877.8484s\n",
      "\titers: 900, epoch: 11 | loss: 0.0118758\n",
      "\tspeed: 0.3189s/iter; left time: 14051.7649s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0107230\n",
      "\tspeed: 0.3171s/iter; left time: 13941.7038s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0178455\n",
      "\tspeed: 0.3166s/iter; left time: 13886.0899s\n",
      "Epoch: 11 cost time: 00h:05m:59.43s\n",
      "Epoch: 11 | Train Loss: 0.0154121 Vali Loss: 0.0196424 Test Loss: 0.0221752\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0175937\n",
      "\tspeed: 1.1792s/iter; left time: 47596.4800s\n",
      "\titers: 200, epoch: 12 | loss: 0.0126353\n",
      "\tspeed: 0.3231s/iter; left time: 13009.9612s\n",
      "\titers: 300, epoch: 12 | loss: 0.0206016\n",
      "\tspeed: 0.3230s/iter; left time: 12972.9080s\n",
      "\titers: 400, epoch: 12 | loss: 0.0102996\n",
      "\tspeed: 0.3227s/iter; left time: 12928.5097s\n",
      "\titers: 500, epoch: 12 | loss: 0.0157677\n",
      "\tspeed: 0.3226s/iter; left time: 12894.6865s\n",
      "\titers: 600, epoch: 12 | loss: 0.0146827\n",
      "\tspeed: 0.3226s/iter; left time: 12861.4432s\n",
      "\titers: 700, epoch: 12 | loss: 0.0145983\n",
      "\tspeed: 0.3228s/iter; left time: 12835.6238s\n",
      "\titers: 800, epoch: 12 | loss: 0.0172842\n",
      "\tspeed: 0.3222s/iter; left time: 12781.1348s\n",
      "\titers: 900, epoch: 12 | loss: 0.0198558\n",
      "\tspeed: 0.3228s/iter; left time: 12771.4169s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0173477\n",
      "\tspeed: 0.3226s/iter; left time: 12732.5014s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0088205\n",
      "\tspeed: 0.3228s/iter; left time: 12705.5370s\n",
      "Epoch: 12 cost time: 00h:06m:03.24s\n",
      "Epoch: 12 | Train Loss: 0.0152097 Vali Loss: 0.0197840 Test Loss: 0.0225309\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0192575\n",
      "\tspeed: 1.1781s/iter; left time: 42257.1033s\n",
      "\titers: 200, epoch: 13 | loss: 0.0178487\n",
      "\tspeed: 0.3215s/iter; left time: 11500.5318s\n",
      "\titers: 300, epoch: 13 | loss: 0.0284491\n",
      "\tspeed: 0.3226s/iter; left time: 11507.3652s\n",
      "\titers: 400, epoch: 13 | loss: 0.0159005\n",
      "\tspeed: 0.3221s/iter; left time: 11456.8257s\n",
      "\titers: 500, epoch: 13 | loss: 0.0128495\n",
      "\tspeed: 0.3220s/iter; left time: 11421.6517s\n",
      "\titers: 600, epoch: 13 | loss: 0.0117158\n",
      "\tspeed: 0.3228s/iter; left time: 11418.6962s\n",
      "\titers: 700, epoch: 13 | loss: 0.0135272\n",
      "\tspeed: 0.3228s/iter; left time: 11383.7663s\n",
      "\titers: 800, epoch: 13 | loss: 0.0158590\n",
      "\tspeed: 0.3227s/iter; left time: 11348.2563s\n",
      "\titers: 900, epoch: 13 | loss: 0.0106493\n",
      "\tspeed: 0.3223s/iter; left time: 11301.7743s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0102547\n",
      "\tspeed: 0.3196s/iter; left time: 11176.2883s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0108302\n",
      "\tspeed: 0.3185s/iter; left time: 11107.3901s\n",
      "Epoch: 13 cost time: 00h:06m:02.09s\n",
      "Epoch: 13 | Train Loss: 0.0150879 Vali Loss: 0.0196984 Test Loss: 0.0227838\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0201575\n",
      "\tspeed: 1.1755s/iter; left time: 36877.6420s\n",
      "\titers: 200, epoch: 14 | loss: 0.0167995\n",
      "\tspeed: 0.3194s/iter; left time: 9989.9229s\n",
      "\titers: 300, epoch: 14 | loss: 0.0204233\n",
      "\tspeed: 0.3225s/iter; left time: 10052.4890s\n",
      "\titers: 400, epoch: 14 | loss: 0.0163709\n",
      "\tspeed: 0.3188s/iter; left time: 9906.4489s\n",
      "\titers: 500, epoch: 14 | loss: 0.0143940\n",
      "\tspeed: 0.3171s/iter; left time: 9821.4001s\n",
      "\titers: 600, epoch: 14 | loss: 0.0164356\n",
      "\tspeed: 0.3179s/iter; left time: 9814.6520s\n",
      "\titers: 700, epoch: 14 | loss: 0.0193533\n",
      "\tspeed: 0.3170s/iter; left time: 9756.5390s\n",
      "\titers: 800, epoch: 14 | loss: 0.0155888\n",
      "\tspeed: 0.3165s/iter; left time: 9707.3937s\n",
      "\titers: 900, epoch: 14 | loss: 0.0123037\n",
      "\tspeed: 0.3178s/iter; left time: 9716.9589s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0190936\n",
      "\tspeed: 0.3170s/iter; left time: 9660.8142s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0183230\n",
      "\tspeed: 0.3205s/iter; left time: 9733.8511s\n",
      "Epoch: 14 cost time: 00h:05m:58.40s\n",
      "Epoch: 14 | Train Loss: 0.0148230 Vali Loss: 0.0200994 Test Loss: 0.0234704\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946\n",
      "Intermediate time for DE and pred_len 24: 01h:44m:15.62s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "[2024-11-16 15:20:03,475] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 15:20:03,476] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 15:20:03,479] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 15:20:03,498] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 15:20:04,799] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 142645\n",
      "val 30725\n",
      "test 30725\n",
      "train 142645\n",
      "train 142645\n",
      "val 30725\n",
      "val 30725\n",
      "train 142645\n",
      "test 30725\n",
      "test 30725\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-16 15:20:06,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-16 15:20:08,168] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-16 15:20:08,169] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-16 15:20:08,169] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-16 15:20:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-16 15:20:08,170] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-16 15:20:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-16 15:20:08,524] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-16 15:20:08,525] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-16 15:20:08,525] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.34 GB, percent = 17.1%\n",
      "[2024-11-16 15:20:08,665] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-16 15:20:08,665] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 15:20:08,665] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.61 GB, percent = 17.1%\n",
      "[2024-11-16 15:20:08,665] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-16 15:20:08,798] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-16 15:20:08,799] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 15:20:08,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.77 GB, percent = 17.1%\n",
      "[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0350a3a310>\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-16 15:20:08,801] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0446114\n",
      "\tspeed: 0.3681s/iter; left time: 32772.2485s\n",
      "\titers: 200, epoch: 1 | loss: 0.0507687\n",
      "\tspeed: 0.3328s/iter; left time: 29601.4851s\n",
      "\titers: 300, epoch: 1 | loss: 0.0448839\n",
      "\tspeed: 0.3338s/iter; left time: 29657.3482s\n",
      "\titers: 400, epoch: 1 | loss: 0.0454857\n",
      "\tspeed: 0.3328s/iter; left time: 29529.0056s\n",
      "\titers: 500, epoch: 1 | loss: 0.0477529\n",
      "\tspeed: 0.3330s/iter; left time: 29521.1311s\n",
      "\titers: 600, epoch: 1 | loss: 0.0380543\n",
      "\tspeed: 0.3334s/iter; left time: 29519.3228s\n",
      "\titers: 700, epoch: 1 | loss: 0.0391503\n",
      "\tspeed: 0.3331s/iter; left time: 29460.7783s\n",
      "\titers: 800, epoch: 1 | loss: 0.0337921\n",
      "\tspeed: 0.3331s/iter; left time: 29430.4367s\n",
      "\titers: 900, epoch: 1 | loss: 0.0199492\n",
      "\tspeed: 0.3335s/iter; left time: 29426.9081s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0314866\n",
      "\tspeed: 0.3332s/iter; left time: 29370.0940s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0306014\n",
      "\tspeed: 0.3330s/iter; left time: 29318.7378s\n",
      "Epoch: 1 cost time: 00h:06m:12.80s\n",
      "Epoch: 1 | Train Loss: 0.0429309 Vali Loss: 0.0322865 Test Loss: 0.0373079\n",
      "Validation loss decreased (inf --> 0.032287).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0302605\n",
      "\tspeed: 1.2869s/iter; left time: 108852.5647s\n",
      "\titers: 200, epoch: 2 | loss: 0.0323583\n",
      "\tspeed: 0.3331s/iter; left time: 28140.0208s\n",
      "\titers: 300, epoch: 2 | loss: 0.0289049\n",
      "\tspeed: 0.3331s/iter; left time: 28106.3431s\n",
      "\titers: 400, epoch: 2 | loss: 0.0225016\n",
      "\tspeed: 0.3334s/iter; left time: 28101.6855s\n",
      "\titers: 500, epoch: 2 | loss: 0.0280473\n",
      "\tspeed: 0.3336s/iter; left time: 28080.0178s\n",
      "\titers: 600, epoch: 2 | loss: 0.0231738\n",
      "\tspeed: 0.3332s/iter; left time: 28014.4420s\n",
      "\titers: 700, epoch: 2 | loss: 0.0285885\n",
      "\tspeed: 0.3335s/iter; left time: 28010.8677s\n",
      "\titers: 800, epoch: 2 | loss: 0.0226014\n",
      "\tspeed: 0.3336s/iter; left time: 27987.7576s\n",
      "\titers: 900, epoch: 2 | loss: 0.0252432\n",
      "\tspeed: 0.3333s/iter; left time: 27928.8521s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0303803\n",
      "\tspeed: 0.3335s/iter; left time: 27909.7248s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0346701\n",
      "\tspeed: 0.3333s/iter; left time: 27862.2472s\n",
      "Epoch: 2 cost time: 00h:06m:11.75s\n",
      "Epoch: 2 | Train Loss: 0.0267929 Vali Loss: 0.0310637 Test Loss: 0.0355101\n",
      "Validation loss decreased (0.032287 --> 0.031064).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0272676\n",
      "\tspeed: 0.8939s/iter; left time: 71628.1422s\n",
      "\titers: 200, epoch: 3 | loss: 0.0320771\n",
      "\tspeed: 0.1432s/iter; left time: 11458.2320s\n",
      "\titers: 300, epoch: 3 | loss: 0.0189163\n",
      "\tspeed: 0.1378s/iter; left time: 11015.6804s\n",
      "\titers: 400, epoch: 3 | loss: 0.0244222\n",
      "\tspeed: 0.1350s/iter; left time: 10776.1540s\n",
      "\titers: 500, epoch: 3 | loss: 0.0268812\n",
      "\tspeed: 0.1343s/iter; left time: 10705.6543s\n",
      "\titers: 600, epoch: 3 | loss: 0.0218725\n",
      "\tspeed: 0.1341s/iter; left time: 10677.9071s\n",
      "\titers: 700, epoch: 3 | loss: 0.0201757\n",
      "\tspeed: 0.1339s/iter; left time: 10648.6167s\n",
      "\titers: 800, epoch: 3 | loss: 0.0237758\n",
      "\tspeed: 0.1344s/iter; left time: 10677.9335s\n",
      "\titers: 900, epoch: 3 | loss: 0.0272236\n",
      "\tspeed: 0.1353s/iter; left time: 10735.1516s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0253749\n",
      "\tspeed: 0.1352s/iter; left time: 10707.9453s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0313581\n",
      "\tspeed: 0.1352s/iter; left time: 10694.8882s\n",
      "Epoch: 3 cost time: 00h:02m:52.32s\n",
      "Epoch: 3 | Train Loss: 0.0256442 Vali Loss: 0.0304424 Test Loss: 0.0349693\n",
      "Validation loss decreased (0.031064 --> 0.030442).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0234055\n",
      "\tspeed: 0.4695s/iter; left time: 35523.3291s\n",
      "\titers: 200, epoch: 4 | loss: 0.0379499\n",
      "\tspeed: 0.1343s/iter; left time: 10147.7702s\n",
      "\titers: 300, epoch: 4 | loss: 0.0260023\n",
      "\tspeed: 0.1388s/iter; left time: 10476.2268s\n",
      "\titers: 400, epoch: 4 | loss: 0.0309342\n",
      "\tspeed: 0.1360s/iter; left time: 10252.8759s\n",
      "\titers: 500, epoch: 4 | loss: 0.0230162\n",
      "\tspeed: 0.1342s/iter; left time: 10103.0281s\n",
      "\titers: 600, epoch: 4 | loss: 0.0329382\n",
      "\tspeed: 0.1362s/iter; left time: 10239.6939s\n",
      "\titers: 700, epoch: 4 | loss: 0.0270466\n",
      "\tspeed: 0.1343s/iter; left time: 10082.3685s\n",
      "\titers: 800, epoch: 4 | loss: 0.0279899\n",
      "\tspeed: 0.1343s/iter; left time: 10069.3088s\n",
      "\titers: 900, epoch: 4 | loss: 0.0265847\n",
      "\tspeed: 0.1335s/iter; left time: 9997.1779s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0298538\n",
      "\tspeed: 0.1339s/iter; left time: 10009.8275s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0180375\n",
      "\tspeed: 0.1338s/iter; left time: 9989.7818s\n",
      "Epoch: 4 cost time: 00h:02m:31.06s\n",
      "Epoch: 4 | Train Loss: 0.0250050 Vali Loss: 0.0307257 Test Loss: 0.0347905\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0198935\n",
      "\tspeed: 0.4307s/iter; left time: 30671.0513s\n",
      "\titers: 200, epoch: 5 | loss: 0.0202480\n",
      "\tspeed: 0.1330s/iter; left time: 9458.7539s\n",
      "\titers: 300, epoch: 5 | loss: 0.0214222\n",
      "\tspeed: 0.1365s/iter; left time: 9694.8402s\n",
      "\titers: 400, epoch: 5 | loss: 0.0283736\n",
      "\tspeed: 0.1369s/iter; left time: 9704.8714s\n",
      "\titers: 500, epoch: 5 | loss: 0.0473016\n",
      "\tspeed: 0.1341s/iter; left time: 9493.6185s\n",
      "\titers: 600, epoch: 5 | loss: 0.0202376\n",
      "\tspeed: 0.1332s/iter; left time: 9415.4894s\n",
      "\titers: 700, epoch: 5 | loss: 0.0211350\n",
      "\tspeed: 0.1355s/iter; left time: 9571.4410s\n",
      "\titers: 800, epoch: 5 | loss: 0.0247196\n",
      "\tspeed: 0.1343s/iter; left time: 9466.5839s\n",
      "\titers: 900, epoch: 5 | loss: 0.0205404\n",
      "\tspeed: 0.1335s/iter; left time: 9403.2914s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0239363\n",
      "\tspeed: 0.1354s/iter; left time: 9521.1684s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0213725\n",
      "\tspeed: 0.1334s/iter; left time: 9367.9676s\n",
      "Epoch: 5 cost time: 00h:02m:30.29s\n",
      "Epoch: 5 | Train Loss: 0.0242875 Vali Loss: 0.0304751 Test Loss: 0.0348097\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0309512\n",
      "\tspeed: 0.4276s/iter; left time: 28547.9665s\n",
      "\titers: 200, epoch: 6 | loss: 0.0205533\n",
      "\tspeed: 0.1323s/iter; left time: 8816.1162s\n",
      "\titers: 300, epoch: 6 | loss: 0.0224231\n",
      "\tspeed: 0.1354s/iter; left time: 9014.1818s\n",
      "\titers: 400, epoch: 6 | loss: 0.0260528\n",
      "\tspeed: 0.1337s/iter; left time: 8886.6849s\n",
      "\titers: 500, epoch: 6 | loss: 0.0180285\n",
      "\tspeed: 0.1326s/iter; left time: 8797.5420s\n",
      "\titers: 600, epoch: 6 | loss: 0.0195526\n",
      "\tspeed: 0.1338s/iter; left time: 8861.9728s\n",
      "\titers: 700, epoch: 6 | loss: 0.0172730\n",
      "\tspeed: 0.1364s/iter; left time: 9020.5564s\n",
      "\titers: 800, epoch: 6 | loss: 0.0267439\n",
      "\tspeed: 0.1332s/iter; left time: 8798.6147s\n",
      "\titers: 900, epoch: 6 | loss: 0.0241353\n",
      "\tspeed: 0.1363s/iter; left time: 8990.9680s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0157862\n",
      "\tspeed: 0.1354s/iter; left time: 8914.7369s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0216071\n",
      "\tspeed: 0.1335s/iter; left time: 8777.0935s\n",
      "Epoch: 6 cost time: 00h:02m:29.93s\n",
      "Epoch: 6 | Train Loss: 0.0236184 Vali Loss: 0.0298692 Test Loss: 0.0346515\n",
      "Validation loss decreased (0.030442 --> 0.029869).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0280138\n",
      "\tspeed: 0.4666s/iter; left time: 29066.2313s\n",
      "\titers: 200, epoch: 7 | loss: 0.0166881\n",
      "\tspeed: 0.1319s/iter; left time: 8202.0174s\n",
      "\titers: 300, epoch: 7 | loss: 0.0210482\n",
      "\tspeed: 0.1357s/iter; left time: 8426.4395s\n",
      "\titers: 400, epoch: 7 | loss: 0.0262818\n",
      "\tspeed: 0.1350s/iter; left time: 8372.9613s\n",
      "\titers: 500, epoch: 7 | loss: 0.0175326\n",
      "\tspeed: 0.1328s/iter; left time: 8217.8671s\n",
      "\titers: 600, epoch: 7 | loss: 0.0228772\n",
      "\tspeed: 0.1336s/iter; left time: 8253.3771s\n",
      "\titers: 700, epoch: 7 | loss: 0.0226232\n",
      "\tspeed: 0.1354s/iter; left time: 8351.9640s\n",
      "\titers: 800, epoch: 7 | loss: 0.0138826\n",
      "\tspeed: 0.1342s/iter; left time: 8267.4929s\n",
      "\titers: 900, epoch: 7 | loss: 0.0142537\n",
      "\tspeed: 0.1330s/iter; left time: 8179.2649s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0185196\n",
      "\tspeed: 0.1323s/iter; left time: 8123.0079s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0187493\n",
      "\tspeed: 0.1329s/iter; left time: 8146.0415s\n",
      "Epoch: 7 cost time: 00h:02m:29.48s\n",
      "Epoch: 7 | Train Loss: 0.0231316 Vali Loss: 0.0302424 Test Loss: 0.0350993\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0193801\n",
      "\tspeed: 0.4266s/iter; left time: 24676.6077s\n",
      "\titers: 200, epoch: 8 | loss: 0.0185916\n",
      "\tspeed: 0.1318s/iter; left time: 7612.2115s\n",
      "\titers: 300, epoch: 8 | loss: 0.0213563\n",
      "\tspeed: 0.1351s/iter; left time: 7789.8088s\n",
      "\titers: 400, epoch: 8 | loss: 0.0218396\n",
      "\tspeed: 0.1327s/iter; left time: 7638.3653s\n",
      "\titers: 500, epoch: 8 | loss: 0.0290579\n",
      "\tspeed: 0.1324s/iter; left time: 7604.5596s\n",
      "\titers: 600, epoch: 8 | loss: 0.0331087\n",
      "\tspeed: 0.1333s/iter; left time: 7645.2016s\n",
      "\titers: 700, epoch: 8 | loss: 0.0227459\n",
      "\tspeed: 0.1377s/iter; left time: 7884.3620s\n",
      "\titers: 800, epoch: 8 | loss: 0.0252364\n",
      "\tspeed: 0.1364s/iter; left time: 7792.3467s\n",
      "\titers: 900, epoch: 8 | loss: 0.0219207\n",
      "\tspeed: 0.1332s/iter; left time: 7600.7579s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0333456\n",
      "\tspeed: 0.1346s/iter; left time: 7661.5944s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0219904\n",
      "\tspeed: 0.1332s/iter; left time: 7570.0531s\n",
      "Epoch: 8 cost time: 00h:02m:29.87s\n",
      "Epoch: 8 | Train Loss: 0.0225641 Vali Loss: 0.0309571 Test Loss: 0.0370625\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0179517\n",
      "\tspeed: 0.4295s/iter; left time: 22927.9580s\n",
      "\titers: 200, epoch: 9 | loss: 0.0256749\n",
      "\tspeed: 0.1323s/iter; left time: 7049.8919s\n",
      "\titers: 300, epoch: 9 | loss: 0.0194726\n",
      "\tspeed: 0.1347s/iter; left time: 7161.5742s\n",
      "\titers: 400, epoch: 9 | loss: 0.0239984\n",
      "\tspeed: 0.1322s/iter; left time: 7020.0342s\n",
      "\titers: 500, epoch: 9 | loss: 0.0159293\n",
      "\tspeed: 0.1324s/iter; left time: 7016.8628s\n",
      "\titers: 600, epoch: 9 | loss: 0.0192116\n",
      "\tspeed: 0.1327s/iter; left time: 7015.8684s\n",
      "\titers: 700, epoch: 9 | loss: 0.0197610\n",
      "\tspeed: 0.1331s/iter; left time: 7023.9503s\n",
      "\titers: 800, epoch: 9 | loss: 0.0186225\n",
      "\tspeed: 0.1312s/iter; left time: 6913.0395s\n",
      "\titers: 900, epoch: 9 | loss: 0.0202066\n",
      "\tspeed: 0.1312s/iter; left time: 6899.7419s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0174803\n",
      "\tspeed: 0.1317s/iter; left time: 6911.6133s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0254039\n",
      "\tspeed: 0.1317s/iter; left time: 6900.9018s\n",
      "Epoch: 9 cost time: 00h:02m:27.98s\n",
      "Epoch: 9 | Train Loss: 0.0215361 Vali Loss: 0.0309942 Test Loss: 0.0375069\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0238908\n",
      "\tspeed: 0.4197s/iter; left time: 20535.0974s\n",
      "\titers: 200, epoch: 10 | loss: 0.0189626\n",
      "\tspeed: 0.1311s/iter; left time: 6402.0846s\n",
      "\titers: 300, epoch: 10 | loss: 0.0185715\n",
      "\tspeed: 0.1344s/iter; left time: 6548.8372s\n",
      "\titers: 400, epoch: 10 | loss: 0.0208660\n",
      "\tspeed: 0.1332s/iter; left time: 6479.2278s\n",
      "\titers: 500, epoch: 10 | loss: 0.0276698\n",
      "\tspeed: 0.1312s/iter; left time: 6365.2740s\n",
      "\titers: 600, epoch: 10 | loss: 0.0254691\n",
      "\tspeed: 0.1334s/iter; left time: 6460.8830s\n",
      "\titers: 700, epoch: 10 | loss: 0.0252488\n",
      "\tspeed: 0.1311s/iter; left time: 6336.3487s\n",
      "\titers: 800, epoch: 10 | loss: 0.0194837\n",
      "\tspeed: 0.1322s/iter; left time: 6374.6949s\n",
      "\titers: 900, epoch: 10 | loss: 0.0210351\n",
      "\tspeed: 0.1334s/iter; left time: 6420.7275s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0163288\n",
      "\tspeed: 0.1333s/iter; left time: 6403.1120s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0178875\n",
      "\tspeed: 0.1332s/iter; left time: 6383.8194s\n",
      "Epoch: 10 cost time: 00h:02m:28.31s\n",
      "Epoch: 10 | Train Loss: 0.0208566 Vali Loss: 0.0317293 Test Loss: 0.0390418\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0179506\n",
      "\tspeed: 0.4286s/iter; left time: 19061.1895s\n",
      "\titers: 200, epoch: 11 | loss: 0.0186568\n",
      "\tspeed: 0.1309s/iter; left time: 5809.0643s\n",
      "\titers: 300, epoch: 11 | loss: 0.0261067\n",
      "\tspeed: 0.1322s/iter; left time: 5852.8858s\n",
      "\titers: 400, epoch: 11 | loss: 0.0201972\n",
      "\tspeed: 0.1308s/iter; left time: 5779.6304s\n",
      "\titers: 500, epoch: 11 | loss: 0.0227132\n",
      "\tspeed: 0.1325s/iter; left time: 5841.2526s\n",
      "\titers: 600, epoch: 11 | loss: 0.0168747\n",
      "\tspeed: 0.1312s/iter; left time: 5769.6729s\n",
      "\titers: 700, epoch: 11 | loss: 0.0244474\n",
      "\tspeed: 0.1350s/iter; left time: 5921.2363s\n",
      "\titers: 800, epoch: 11 | loss: 0.0158708\n",
      "\tspeed: 0.1315s/iter; left time: 5757.6713s\n",
      "\titers: 900, epoch: 11 | loss: 0.0233177\n",
      "\tspeed: 0.1314s/iter; left time: 5737.2276s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0231835\n",
      "\tspeed: 0.1333s/iter; left time: 5808.7242s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0227261\n",
      "\tspeed: 0.1328s/iter; left time: 5772.7395s\n",
      "Epoch: 11 cost time: 00h:02m:27.92s\n",
      "Epoch: 11 | Train Loss: 0.0203122 Vali Loss: 0.0328566 Test Loss: 0.0383958\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164\n",
      "Intermediate time for DE and pred_len 96: 00h:42m:21.01s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "[2024-11-16 16:02:24,986] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 16:02:24,987] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 16:02:24,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 16:02:24,991] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-16 16:02:25,922] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "train 142285\n",
      "val 30365\n",
      "test 30365\n",
      "train 142285\n",
      "train 142285\n",
      "val 30365\n",
      "val 30365\n",
      "test 30365\n",
      "train 142285\n",
      "test 30365\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-16 16:02:27,722] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-16 16:02:28,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-16 16:02:28,664] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-16 16:02:28,664] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-16 16:02:28,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-16 16:02:28,665] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-16 16:02:28,665] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-16 16:02:28,932] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-16 16:02:28,933] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-16 16:02:28,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.42 GB, percent = 16.9%\n",
      "[2024-11-16 16:02:29,093] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-16 16:02:29,094] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 16:02:29,094] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.69 GB, percent = 16.9%\n",
      "[2024-11-16 16:02:29,094] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-16 16:02:29,235] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-16 16:02:29,235] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-16 16:02:29,235] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.88 GB, percent = 17.0%\n",
      "[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-16 16:02:29,236] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8c764f7b10>\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-16 16:02:29,238] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0359368\n",
      "\tspeed: 0.1644s/iter; left time: 14606.4632s\n",
      "\titers: 200, epoch: 1 | loss: 0.0448534\n",
      "\tspeed: 0.1390s/iter; left time: 12332.1637s\n",
      "\titers: 300, epoch: 1 | loss: 0.0433941\n",
      "\tspeed: 0.1389s/iter; left time: 12309.2884s\n",
      "\titers: 400, epoch: 1 | loss: 0.0418473\n",
      "\tspeed: 0.1392s/iter; left time: 12326.1744s\n",
      "\titers: 500, epoch: 1 | loss: 0.0473000\n",
      "\tspeed: 0.1394s/iter; left time: 12324.8273s\n",
      "\titers: 600, epoch: 1 | loss: 0.0449542\n",
      "\tspeed: 0.1408s/iter; left time: 12439.1721s\n",
      "\titers: 700, epoch: 1 | loss: 0.0429755\n",
      "\tspeed: 0.1403s/iter; left time: 12374.5821s\n",
      "\titers: 800, epoch: 1 | loss: 0.0494640\n",
      "\tspeed: 0.1412s/iter; left time: 12438.3126s\n",
      "\titers: 900, epoch: 1 | loss: 0.0413226\n",
      "\tspeed: 0.1405s/iter; left time: 12365.4032s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0430699\n",
      "\tspeed: 0.1398s/iter; left time: 12289.0584s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0416704\n",
      "\tspeed: 0.1390s/iter; left time: 12204.6348s\n",
      "Epoch: 1 cost time: 00h:02m:36.39s\n",
      "Epoch: 1 | Train Loss: 0.0463432 Vali Loss: 0.0367494 Test Loss: 0.0425076\n",
      "Validation loss decreased (inf --> 0.036749).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0301424\n",
      "\tspeed: 0.4761s/iter; left time: 40174.5548s\n",
      "\titers: 200, epoch: 2 | loss: 0.0237679\n",
      "\tspeed: 0.1378s/iter; left time: 11609.3023s\n",
      "\titers: 300, epoch: 2 | loss: 0.0273275\n",
      "\tspeed: 0.1368s/iter; left time: 11511.4448s\n",
      "\titers: 400, epoch: 2 | loss: 0.0363315\n",
      "\tspeed: 0.1375s/iter; left time: 11561.1388s\n",
      "\titers: 500, epoch: 2 | loss: 0.0213703\n",
      "\tspeed: 0.1371s/iter; left time: 11509.3981s\n",
      "\titers: 600, epoch: 2 | loss: 0.0212502\n",
      "\tspeed: 0.1373s/iter; left time: 11516.1187s\n",
      "\titers: 700, epoch: 2 | loss: 0.0261255\n",
      "\tspeed: 0.1368s/iter; left time: 11459.1919s\n",
      "\titers: 800, epoch: 2 | loss: 0.0245793\n",
      "\tspeed: 0.1385s/iter; left time: 11593.1419s\n",
      "\titers: 900, epoch: 2 | loss: 0.0246260\n",
      "\tspeed: 0.1380s/iter; left time: 11533.8377s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0303873\n",
      "\tspeed: 0.1370s/iter; left time: 11432.4694s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0278004\n",
      "\tspeed: 0.1371s/iter; left time: 11430.1299s\n",
      "Epoch: 2 cost time: 00h:02m:33.24s\n",
      "Epoch: 2 | Train Loss: 0.0293619 Vali Loss: 0.0321608 Test Loss: 0.0374062\n",
      "Validation loss decreased (0.036749 --> 0.032161).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0327742\n",
      "\tspeed: 0.4578s/iter; left time: 36591.1040s\n",
      "\titers: 200, epoch: 3 | loss: 0.0426615\n",
      "\tspeed: 0.1377s/iter; left time: 10993.7534s\n",
      "\titers: 300, epoch: 3 | loss: 0.0251191\n",
      "\tspeed: 0.1379s/iter; left time: 10996.2049s\n",
      "\titers: 400, epoch: 3 | loss: 0.0265200\n",
      "\tspeed: 0.1389s/iter; left time: 11061.0320s\n",
      "\titers: 500, epoch: 3 | loss: 0.0287092\n",
      "\tspeed: 0.1366s/iter; left time: 10864.1342s\n",
      "\titers: 600, epoch: 3 | loss: 0.0326744\n",
      "\tspeed: 0.1368s/iter; left time: 10866.1062s\n",
      "\titers: 700, epoch: 3 | loss: 0.0272740\n",
      "\tspeed: 0.1371s/iter; left time: 10876.5321s\n",
      "\titers: 800, epoch: 3 | loss: 0.0219435\n",
      "\tspeed: 0.1371s/iter; left time: 10865.1690s\n",
      "\titers: 900, epoch: 3 | loss: 0.0302642\n",
      "\tspeed: 0.1367s/iter; left time: 10817.3811s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0288475\n",
      "\tspeed: 0.1369s/iter; left time: 10819.1898s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0385880\n",
      "\tspeed: 0.1369s/iter; left time: 10804.0629s\n",
      "Epoch: 3 cost time: 00h:02m:33.14s\n",
      "Epoch: 3 | Train Loss: 0.0275353 Vali Loss: 0.0315600 Test Loss: 0.0373444\n",
      "Validation loss decreased (0.032161 --> 0.031560).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0265749\n",
      "\tspeed: 0.4598s/iter; left time: 34704.8146s\n",
      "\titers: 200, epoch: 4 | loss: 0.0307901\n",
      "\tspeed: 0.1361s/iter; left time: 10263.2213s\n",
      "\titers: 300, epoch: 4 | loss: 0.0240945\n",
      "\tspeed: 0.1365s/iter; left time: 10274.5138s\n",
      "\titers: 400, epoch: 4 | loss: 0.0293844\n",
      "\tspeed: 0.1369s/iter; left time: 10294.6333s\n",
      "\titers: 500, epoch: 4 | loss: 0.0243979\n",
      "\tspeed: 0.1361s/iter; left time: 10217.3833s\n",
      "\titers: 600, epoch: 4 | loss: 0.0342317\n",
      "\tspeed: 0.1366s/iter; left time: 10240.4723s\n",
      "\titers: 700, epoch: 4 | loss: 0.0235715\n",
      "\tspeed: 0.1365s/iter; left time: 10224.0853s\n",
      "\titers: 800, epoch: 4 | loss: 0.0298171\n",
      "\tspeed: 0.1359s/iter; left time: 10164.4991s\n",
      "\titers: 900, epoch: 4 | loss: 0.0243449\n",
      "\tspeed: 0.1365s/iter; left time: 10196.7584s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0236856\n",
      "\tspeed: 0.1364s/iter; left time: 10173.1389s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0263085\n",
      "\tspeed: 0.1408s/iter; left time: 10489.8108s\n",
      "Epoch: 4 cost time: 00h:02m:32.63s\n",
      "Epoch: 4 | Train Loss: 0.0266994 Vali Loss: 0.0314476 Test Loss: 0.0374681\n",
      "Validation loss decreased (0.031560 --> 0.031448).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0282643\n",
      "\tspeed: 0.4769s/iter; left time: 33879.0590s\n",
      "\titers: 200, epoch: 5 | loss: 0.0272768\n",
      "\tspeed: 0.1355s/iter; left time: 9609.0031s\n",
      "\titers: 300, epoch: 5 | loss: 0.0217751\n",
      "\tspeed: 0.1348s/iter; left time: 9547.8683s\n",
      "\titers: 400, epoch: 5 | loss: 0.0190624\n",
      "\tspeed: 0.1345s/iter; left time: 9517.2029s\n",
      "\titers: 500, epoch: 5 | loss: 0.0376097\n",
      "\tspeed: 0.1345s/iter; left time: 9502.6648s\n",
      "\titers: 600, epoch: 5 | loss: 0.0226067\n",
      "\tspeed: 0.1347s/iter; left time: 9502.2118s\n",
      "\titers: 700, epoch: 5 | loss: 0.0193413\n",
      "\tspeed: 0.1347s/iter; left time: 9490.0790s\n",
      "\titers: 800, epoch: 5 | loss: 0.0192956\n",
      "\tspeed: 0.1354s/iter; left time: 9526.9124s\n",
      "\titers: 900, epoch: 5 | loss: 0.0285773\n",
      "\tspeed: 0.1352s/iter; left time: 9493.8396s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0276020\n",
      "\tspeed: 0.1357s/iter; left time: 9518.0815s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0244862\n",
      "\tspeed: 0.1348s/iter; left time: 9439.1368s\n",
      "Epoch: 5 cost time: 00h:02m:30.71s\n",
      "Epoch: 5 | Train Loss: 0.0259062 Vali Loss: 0.0320041 Test Loss: 0.0369165\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0214580\n",
      "\tspeed: 0.4152s/iter; left time: 27645.2994s\n",
      "\titers: 200, epoch: 6 | loss: 0.0234301\n",
      "\tspeed: 0.1349s/iter; left time: 8971.6242s\n",
      "\titers: 300, epoch: 6 | loss: 0.0200381\n",
      "\tspeed: 0.1347s/iter; left time: 8945.3894s\n",
      "\titers: 400, epoch: 6 | loss: 0.0311940\n",
      "\tspeed: 0.1353s/iter; left time: 8970.1261s\n",
      "\titers: 500, epoch: 6 | loss: 0.0301950\n",
      "\tspeed: 0.1358s/iter; left time: 8989.8726s\n",
      "\titers: 600, epoch: 6 | loss: 0.0203328\n",
      "\tspeed: 0.1352s/iter; left time: 8936.7431s\n",
      "\titers: 700, epoch: 6 | loss: 0.0225864\n",
      "\tspeed: 0.1346s/iter; left time: 8879.4130s\n",
      "\titers: 800, epoch: 6 | loss: 0.0181251\n",
      "\tspeed: 0.1347s/iter; left time: 8872.4661s\n",
      "\titers: 900, epoch: 6 | loss: 0.0198194\n",
      "\tspeed: 0.1345s/iter; left time: 8851.4498s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0240496\n",
      "\tspeed: 0.1350s/iter; left time: 8865.7421s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0215907\n",
      "\tspeed: 0.1349s/iter; left time: 8846.1939s\n",
      "Epoch: 6 cost time: 00h:02m:30.46s\n",
      "Epoch: 6 | Train Loss: 0.0249147 Vali Loss: 0.0328208 Test Loss: 0.0379578\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0212492\n",
      "\tspeed: 0.4220s/iter; left time: 26225.8119s\n",
      "\titers: 200, epoch: 7 | loss: 0.0209108\n",
      "\tspeed: 0.1354s/iter; left time: 8399.0267s\n",
      "\titers: 300, epoch: 7 | loss: 0.0251728\n",
      "\tspeed: 0.1360s/iter; left time: 8422.7246s\n",
      "\titers: 400, epoch: 7 | loss: 0.0205974\n",
      "\tspeed: 0.1354s/iter; left time: 8375.9464s\n",
      "\titers: 500, epoch: 7 | loss: 0.0166121\n",
      "\tspeed: 0.1343s/iter; left time: 8292.4816s\n",
      "\titers: 600, epoch: 7 | loss: 0.0294926\n",
      "\tspeed: 0.1360s/iter; left time: 8385.7122s\n",
      "\titers: 700, epoch: 7 | loss: 0.0198359\n",
      "\tspeed: 0.1342s/iter; left time: 8259.4761s\n",
      "\titers: 800, epoch: 7 | loss: 0.0242572\n",
      "\tspeed: 0.1343s/iter; left time: 8249.8904s\n",
      "\titers: 900, epoch: 7 | loss: 0.0213706\n",
      "\tspeed: 0.1351s/iter; left time: 8285.5735s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0236055\n",
      "\tspeed: 0.1348s/iter; left time: 8255.7980s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0265674\n",
      "\tspeed: 0.1359s/iter; left time: 8307.5910s\n",
      "Epoch: 7 cost time: 00h:02m:30.70s\n",
      "Epoch: 7 | Train Loss: 0.0237292 Vali Loss: 0.0337015 Test Loss: 0.0404406\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0256783\n",
      "\tspeed: 0.4171s/iter; left time: 24068.0077s\n",
      "\titers: 200, epoch: 8 | loss: 0.0266715\n",
      "\tspeed: 0.1349s/iter; left time: 7768.2357s\n",
      "\titers: 300, epoch: 8 | loss: 0.0207095\n",
      "\tspeed: 0.1348s/iter; left time: 7753.1162s\n",
      "\titers: 400, epoch: 8 | loss: 0.0248391\n",
      "\tspeed: 0.1345s/iter; left time: 7722.0082s\n",
      "\titers: 500, epoch: 8 | loss: 0.0196725\n",
      "\tspeed: 0.1351s/iter; left time: 7739.9736s\n",
      "\titers: 600, epoch: 8 | loss: 0.0184297\n",
      "\tspeed: 0.1347s/iter; left time: 7704.9198s\n",
      "\titers: 700, epoch: 8 | loss: 0.0277658\n",
      "\tspeed: 0.1349s/iter; left time: 7701.3372s\n",
      "\titers: 800, epoch: 8 | loss: 0.0265737\n",
      "\tspeed: 0.1350s/iter; left time: 7694.7326s\n",
      "\titers: 900, epoch: 8 | loss: 0.0243952\n",
      "\tspeed: 0.1351s/iter; left time: 7687.6184s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0238605\n",
      "\tspeed: 0.1346s/iter; left time: 7642.9981s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0290543\n",
      "\tspeed: 0.1338s/iter; left time: 7585.7476s\n",
      "Epoch: 8 cost time: 00h:02m:30.33s\n",
      "Epoch: 8 | Train Loss: 0.0225662 Vali Loss: 0.0353154 Test Loss: 0.0423327\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0227777\n",
      "\tspeed: 0.4192s/iter; left time: 22323.0045s\n",
      "\titers: 200, epoch: 9 | loss: 0.0262419\n",
      "\tspeed: 0.1338s/iter; left time: 7112.9049s\n",
      "\titers: 300, epoch: 9 | loss: 0.0207251\n",
      "\tspeed: 0.1340s/iter; left time: 7107.4673s\n",
      "\titers: 400, epoch: 9 | loss: 0.0197221\n",
      "\tspeed: 0.1347s/iter; left time: 7133.5740s\n",
      "\titers: 500, epoch: 9 | loss: 0.0167211\n",
      "\tspeed: 0.1348s/iter; left time: 7123.5262s\n",
      "\titers: 600, epoch: 9 | loss: 0.0236003\n",
      "\tspeed: 0.1346s/iter; left time: 7101.7201s\n",
      "\titers: 700, epoch: 9 | loss: 0.0170809\n",
      "\tspeed: 0.1343s/iter; left time: 7073.7127s\n",
      "\titers: 800, epoch: 9 | loss: 0.0209369\n",
      "\tspeed: 0.1345s/iter; left time: 7068.1689s\n",
      "\titers: 900, epoch: 9 | loss: 0.0263193\n",
      "\tspeed: 0.1346s/iter; left time: 7059.6231s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0252886\n",
      "\tspeed: 0.1341s/iter; left time: 7020.5467s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0198990\n",
      "\tspeed: 0.1343s/iter; left time: 7020.1685s\n",
      "Epoch: 9 cost time: 00h:02m:29.77s\n",
      "Epoch: 9 | Train Loss: 0.0214407 Vali Loss: 0.0361602 Test Loss: 0.0435897\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316\n",
      "Intermediate time for DE and pred_len 168: 00h:27m:36.38s\n",
      "\n",
      "Intermediate time for DE: 02h:54m:13.00s\n",
      "\n",
      "Total time: 02h:54m:13.01s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "            \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --multi_gpu --num_processes=4 --num_machines 1 --dynamo_backend \"no\"  --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience {patience} \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Country': 'DE',\n",
       "  'Pred_len': 24,\n",
       "  'MSE': 0.021544765681028366,\n",
       "  'RMSE': 0.14678135514259338,\n",
       "  'MAE': 0.09620458632707596},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 96,\n",
       "  'MSE': 0.0346515066921711,\n",
       "  'RMSE': 0.18614915013313293,\n",
       "  'MAE': 0.12807267904281616},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 168,\n",
       "  'MSE': 0.03746809437870979,\n",
       "  'RMSE': 0.19356676936149597,\n",
       "  'MAE': 0.1385875940322876}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timellm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [24]\n",
    "countries = ['FR']\n",
    "num_cols = [3]\n",
    "seq_lens = [168]\n",
    "train_epochs = 30\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "[2024-11-14 13:57:59,560] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 13:57:59,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 13:57:59,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 13:57:59,571] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-14 13:58:03,477] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "train 86835\n",
      "val 18651\n",
      "test 18651\n",
      "train 86835\n",
      "train 86835\n",
      "train 86835\n",
      "val 18651\n",
      "val 18651\n",
      "test 18651\n",
      "val 18651\n",
      "test 18651\n",
      "test 18651\n",
      "[2024-11-14 13:58:08,639] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-14 13:58:10,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-14 13:58:10,414] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-14 13:58:10,798] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-14 13:58:10,799] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB \n",
      "[2024-11-14 13:58:10,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.35 GB, percent = 2.6%\n",
      "[2024-11-14 13:58:11,154] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-14 13:58:11,154] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-14 13:58:11,155] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%\n",
      "[2024-11-14 13:58:11,155] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-14 13:58:11,284] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-14 13:58:11,285] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2024-11-14 13:58:11,285] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%\n",
      "[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-14 13:58:11,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32cbb78450>\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-14 13:58:11,288] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.0239559\n",
      "\tspeed: 0.2802s/iter; left time: 22775.5551s\n",
      "\titers: 200, epoch: 1 | loss: 0.0201448\n",
      "\tspeed: 0.1183s/iter; left time: 9601.5377s\n",
      "\titers: 300, epoch: 1 | loss: 0.0162691\n",
      "\tspeed: 0.1169s/iter; left time: 9480.7446s\n",
      "\titers: 400, epoch: 1 | loss: 0.0126257\n",
      "\tspeed: 0.1165s/iter; left time: 9435.1100s\n",
      "\titers: 500, epoch: 1 | loss: 0.0144120\n",
      "\tspeed: 0.1172s/iter; left time: 9483.2932s\n",
      "\titers: 600, epoch: 1 | loss: 0.0185334\n",
      "\tspeed: 0.1176s/iter; left time: 9497.9162s\n",
      "Epoch: 1 cost time: 00h:01m:33.36s\n",
      "Epoch: 1 | Train Loss: 0.0225287 Vali Loss: 0.0118601 Test Loss: 0.0136884\n",
      "Validation loss decreased (inf --> 0.011860).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0131000\n",
      "\tspeed: 0.6516s/iter; left time: 51201.0050s\n",
      "\titers: 200, epoch: 2 | loss: 0.0220157\n",
      "\tspeed: 0.1155s/iter; left time: 9064.4683s\n",
      "\titers: 300, epoch: 2 | loss: 0.0084849\n",
      "\tspeed: 0.1168s/iter; left time: 9154.1646s\n",
      "\titers: 400, epoch: 2 | loss: 0.0084919\n",
      "\tspeed: 0.1152s/iter; left time: 9015.7136s\n",
      "\titers: 500, epoch: 2 | loss: 0.0147144\n",
      "\tspeed: 0.1156s/iter; left time: 9038.7933s\n",
      "\titers: 600, epoch: 2 | loss: 0.0093752\n",
      "\tspeed: 0.1190s/iter; left time: 9288.0978s\n",
      "Epoch: 2 cost time: 00h:01m:19.40s\n",
      "Epoch: 2 | Train Loss: 0.0104779 Vali Loss: 0.0106168 Test Loss: 0.0121037\n",
      "Validation loss decreased (0.011860 --> 0.010617).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0076656\n",
      "\tspeed: 0.4371s/iter; left time: 33163.9484s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123325\n",
      "\tspeed: 0.1146s/iter; left time: 8682.3240s\n",
      "\titers: 300, epoch: 3 | loss: 0.0073657\n",
      "\tspeed: 0.1148s/iter; left time: 8682.7302s\n",
      "\titers: 400, epoch: 3 | loss: 0.0088692\n",
      "\tspeed: 0.1151s/iter; left time: 8696.7682s\n",
      "\titers: 500, epoch: 3 | loss: 0.0086650\n",
      "\tspeed: 0.1156s/iter; left time: 8727.1908s\n",
      "\titers: 600, epoch: 3 | loss: 0.0082646\n",
      "\tspeed: 0.1178s/iter; left time: 8874.5341s\n",
      "Epoch: 3 cost time: 00h:01m:20.57s\n",
      "Epoch: 3 | Train Loss: 0.0094833 Vali Loss: 0.0101822 Test Loss: 0.0114591\n",
      "Validation loss decreased (0.010617 --> 0.010182).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0109312\n",
      "\tspeed: 0.5266s/iter; left time: 38524.0249s\n",
      "\titers: 200, epoch: 4 | loss: 0.0118849\n",
      "\tspeed: 0.1167s/iter; left time: 8526.1306s\n",
      "\titers: 300, epoch: 4 | loss: 0.0099334\n",
      "\tspeed: 0.1147s/iter; left time: 8367.2881s\n",
      "\titers: 400, epoch: 4 | loss: 0.0101027\n",
      "\tspeed: 0.1146s/iter; left time: 8352.2250s\n",
      "\titers: 500, epoch: 4 | loss: 0.0118740\n",
      "\tspeed: 0.1145s/iter; left time: 8326.7028s\n",
      "\titers: 600, epoch: 4 | loss: 0.0064906\n",
      "\tspeed: 0.1145s/iter; left time: 8318.2984s\n",
      "Epoch: 4 cost time: 00h:01m:18.59s\n",
      "Epoch: 4 | Train Loss: 0.0094642 Vali Loss: 0.0098645 Test Loss: 0.0111941\n",
      "Validation loss decreased (0.010182 --> 0.009865).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0145120\n",
      "\tspeed: 0.4577s/iter; left time: 32241.6792s\n",
      "\titers: 200, epoch: 5 | loss: 0.0107904\n",
      "\tspeed: 0.1139s/iter; left time: 8014.7542s\n",
      "\titers: 300, epoch: 5 | loss: 0.0115122\n",
      "\tspeed: 0.1143s/iter; left time: 8031.6079s\n",
      "\titers: 400, epoch: 5 | loss: 0.0064738\n",
      "\tspeed: 0.1144s/iter; left time: 8023.9122s\n",
      "\titers: 500, epoch: 5 | loss: 0.0065169\n",
      "\tspeed: 0.1136s/iter; left time: 7959.0587s\n",
      "\titers: 600, epoch: 5 | loss: 0.0121063\n",
      "\tspeed: 0.1137s/iter; left time: 7949.9830s\n",
      "Epoch: 5 cost time: 00h:01m:18.09s\n",
      "Epoch: 5 | Train Loss: 0.0089185 Vali Loss: 0.0098854 Test Loss: 0.0113841\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0107378\n",
      "\tspeed: 0.3610s/iter; left time: 24448.0126s\n",
      "\titers: 200, epoch: 6 | loss: 0.0071157\n",
      "\tspeed: 0.1182s/iter; left time: 7996.0440s\n",
      "\titers: 300, epoch: 6 | loss: 0.0085659\n",
      "\tspeed: 0.1181s/iter; left time: 7973.6379s\n",
      "\titers: 400, epoch: 6 | loss: 0.0074021\n",
      "\tspeed: 0.1223s/iter; left time: 8243.7480s\n",
      "\titers: 500, epoch: 6 | loss: 0.0067580\n",
      "\tspeed: 0.1229s/iter; left time: 8272.0650s\n",
      "\titers: 600, epoch: 6 | loss: 0.0085170\n",
      "\tspeed: 0.2517s/iter; left time: 16919.5556s\n",
      "Epoch: 6 cost time: 00h:01m:45.53s\n",
      "Epoch: 6 | Train Loss: 0.0088672 Vali Loss: 0.0098212 Test Loss: 0.0112326\n",
      "Validation loss decreased (0.009865 --> 0.009821).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0133668\n",
      "\tspeed: 1.0716s/iter; left time: 69670.8480s\n",
      "\titers: 200, epoch: 7 | loss: 0.0055773\n",
      "\tspeed: 0.4422s/iter; left time: 28703.5360s\n",
      "\titers: 300, epoch: 7 | loss: 0.0110484\n",
      "\tspeed: 0.4404s/iter; left time: 28541.6124s\n",
      "\titers: 400, epoch: 7 | loss: 0.0098669\n",
      "\tspeed: 0.4496s/iter; left time: 29095.6542s\n",
      "\titers: 500, epoch: 7 | loss: 0.0105809\n",
      "\tspeed: 0.4466s/iter; left time: 28856.8058s\n",
      "\titers: 600, epoch: 7 | loss: 0.0083511\n",
      "\tspeed: 0.4468s/iter; left time: 28825.8824s\n",
      "Epoch: 7 cost time: 00h:04m:51.21s\n",
      "Epoch: 7 | Train Loss: 0.0086675 Vali Loss: 0.0096691 Test Loss: 0.0111444\n",
      "Validation loss decreased (0.009821 --> 0.009669).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0081424\n",
      "\tspeed: 1.4640s/iter; left time: 91206.3695s\n",
      "\titers: 200, epoch: 8 | loss: 0.0119270\n",
      "\tspeed: 0.4520s/iter; left time: 28111.3485s\n",
      "\titers: 300, epoch: 8 | loss: 0.0090215\n",
      "\tspeed: 0.4443s/iter; left time: 27593.2379s\n",
      "\titers: 400, epoch: 8 | loss: 0.0059014\n",
      "\tspeed: 0.1566s/iter; left time: 9707.4069s\n",
      "\titers: 500, epoch: 8 | loss: 0.0100736\n",
      "\tspeed: 0.2694s/iter; left time: 16678.4595s\n",
      "\titers: 600, epoch: 8 | loss: 0.0117893\n",
      "\tspeed: 0.2278s/iter; left time: 14078.8536s\n",
      "Epoch: 8 cost time: 00h:03m:29.50s\n",
      "Epoch: 8 | Train Loss: 0.0086480 Vali Loss: 0.0095132 Test Loss: 0.0111062\n",
      "Validation loss decreased (0.009669 --> 0.009513).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0090044\n",
      "\tspeed: 0.4103s/iter; left time: 24446.3345s\n",
      "\titers: 200, epoch: 9 | loss: 0.0094946\n",
      "\tspeed: 0.1114s/iter; left time: 6625.0114s\n",
      "\titers: 300, epoch: 9 | loss: 0.0079705\n",
      "\tspeed: 0.1121s/iter; left time: 6658.8228s\n",
      "\titers: 400, epoch: 9 | loss: 0.0099703\n",
      "\tspeed: 0.1128s/iter; left time: 6689.6377s\n",
      "\titers: 500, epoch: 9 | loss: 0.0080995\n",
      "\tspeed: 0.1125s/iter; left time: 6657.6125s\n",
      "\titers: 600, epoch: 9 | loss: 0.0063729\n",
      "\tspeed: 0.1120s/iter; left time: 6617.9407s\n",
      "Epoch: 9 cost time: 00h:01m:16.71s\n",
      "Epoch: 9 | Train Loss: 0.0085195 Vali Loss: 0.0092639 Test Loss: 0.0106769\n",
      "Validation loss decreased (0.009513 --> 0.009264).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0053214\n",
      "\tspeed: 0.4978s/iter; left time: 28311.7073s\n",
      "\titers: 200, epoch: 10 | loss: 0.0139130\n",
      "\tspeed: 0.1256s/iter; left time: 7132.7705s\n",
      "\titers: 300, epoch: 10 | loss: 0.0081261\n",
      "\tspeed: 0.2800s/iter; left time: 15868.6923s\n",
      "\titers: 400, epoch: 10 | loss: 0.0048974\n",
      "\tspeed: 0.4382s/iter; left time: 24790.6404s\n",
      "\titers: 500, epoch: 10 | loss: 0.0076337\n",
      "\tspeed: 0.4319s/iter; left time: 24392.6828s\n",
      "\titers: 600, epoch: 10 | loss: 0.0100944\n",
      "\tspeed: 0.4225s/iter; left time: 23815.2874s\n",
      "Epoch: 10 cost time: 00h:03m:34.94s\n",
      "Epoch: 10 | Train Loss: 0.0086789 Vali Loss: 0.0094028 Test Loss: 0.0108369\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0053404\n",
      "\tspeed: 1.3813s/iter; left time: 74810.0582s\n",
      "\titers: 200, epoch: 11 | loss: 0.0082792\n",
      "\tspeed: 0.4443s/iter; left time: 24020.2399s\n",
      "\titers: 300, epoch: 11 | loss: 0.0070582\n",
      "\tspeed: 0.4496s/iter; left time: 24259.9557s\n",
      "\titers: 400, epoch: 11 | loss: 0.0067524\n",
      "\tspeed: 0.4511s/iter; left time: 24294.5859s\n",
      "\titers: 500, epoch: 11 | loss: 0.0078302\n",
      "\tspeed: 0.4544s/iter; left time: 24430.0277s\n",
      "\titers: 600, epoch: 11 | loss: 0.0079525\n",
      "\tspeed: 0.3612s/iter; left time: 19382.8251s\n",
      "Epoch: 11 cost time: 00h:04m:42.41s\n",
      "Epoch: 11 | Train Loss: 0.0083401 Vali Loss: 0.0091377 Test Loss: 0.0106622\n",
      "Validation loss decreased (0.009264 --> 0.009138).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0096092\n",
      "\tspeed: 0.8587s/iter; left time: 44180.8874s\n",
      "\titers: 200, epoch: 12 | loss: 0.0080460\n",
      "\tspeed: 0.1527s/iter; left time: 7841.6917s\n",
      "\titers: 300, epoch: 12 | loss: 0.0127255\n",
      "\tspeed: 0.1116s/iter; left time: 5721.7644s\n",
      "\titers: 400, epoch: 12 | loss: 0.0106730\n",
      "\tspeed: 0.1136s/iter; left time: 5808.5216s\n",
      "\titers: 500, epoch: 12 | loss: 0.0083658\n",
      "\tspeed: 0.1125s/iter; left time: 5740.9163s\n",
      "\titers: 600, epoch: 12 | loss: 0.0132466\n",
      "\tspeed: 0.1126s/iter; left time: 5735.2817s\n",
      "Epoch: 12 cost time: 00h:01m:34.02s\n",
      "Epoch: 12 | Train Loss: 0.0082366 Vali Loss: 0.0090408 Test Loss: 0.0104260\n",
      "Validation loss decreased (0.009138 --> 0.009041).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0112591\n",
      "\tspeed: 0.6866s/iter; left time: 33461.1953s\n",
      "\titers: 200, epoch: 13 | loss: 0.0129779\n",
      "\tspeed: 0.2682s/iter; left time: 13043.7910s\n",
      "\titers: 300, epoch: 13 | loss: 0.0067636\n",
      "\tspeed: 0.2726s/iter; left time: 13232.8373s\n",
      "\titers: 400, epoch: 13 | loss: 0.0061778\n",
      "\tspeed: 0.2815s/iter; left time: 13636.8595s\n",
      "\titers: 500, epoch: 13 | loss: 0.0104312\n",
      "\tspeed: 0.2945s/iter; left time: 14233.7605s\n",
      "\titers: 600, epoch: 13 | loss: 0.0091502\n",
      "\tspeed: 0.3002s/iter; left time: 14482.2562s\n",
      "Epoch: 13 cost time: 00h:03m:13.26s\n",
      "Epoch: 13 | Train Loss: 0.0080974 Vali Loss: 0.0090926 Test Loss: 0.0107355\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0067687\n",
      "\tspeed: 0.9535s/iter; left time: 43883.0498s\n",
      "\titers: 200, epoch: 14 | loss: 0.0091137\n",
      "\tspeed: 0.3083s/iter; left time: 14156.9275s\n",
      "\titers: 300, epoch: 14 | loss: 0.0098937\n",
      "\tspeed: 0.3212s/iter; left time: 14718.1476s\n",
      "\titers: 400, epoch: 14 | loss: 0.0086841\n",
      "\tspeed: 0.3156s/iter; left time: 14431.2787s\n",
      "\titers: 500, epoch: 14 | loss: 0.0082142\n",
      "\tspeed: 0.3186s/iter; left time: 14534.1315s\n",
      "\titers: 600, epoch: 14 | loss: 0.0087004\n",
      "\tspeed: 0.2979s/iter; left time: 13560.4861s\n",
      "Epoch: 14 cost time: 00h:03m:21.18s\n",
      "Epoch: 14 | Train Loss: 0.0081368 Vali Loss: 0.0089570 Test Loss: 0.0104242\n",
      "Validation loss decreased (0.009041 --> 0.008957).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0167940\n",
      "\tspeed: 0.4613s/iter; left time: 19978.9698s\n",
      "\titers: 200, epoch: 15 | loss: 0.0050574\n",
      "\tspeed: 0.1128s/iter; left time: 4875.5426s\n",
      "\titers: 300, epoch: 15 | loss: 0.0104902\n",
      "\tspeed: 0.1127s/iter; left time: 4856.3856s\n",
      "\titers: 400, epoch: 15 | loss: 0.0089468\n",
      "\tspeed: 0.1135s/iter; left time: 4881.1831s\n",
      "\titers: 500, epoch: 15 | loss: 0.0126251\n",
      "\tspeed: 0.1118s/iter; left time: 4797.1002s\n",
      "\titers: 600, epoch: 15 | loss: 0.0059486\n",
      "\tspeed: 0.1117s/iter; left time: 4781.2579s\n",
      "Epoch: 15 cost time: 00h:01m:17.22s\n",
      "Epoch: 15 | Train Loss: 0.0081152 Vali Loss: 0.0090319 Test Loss: 0.0105617\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0057051\n",
      "\tspeed: 0.3483s/iter; left time: 14137.5610s\n",
      "\titers: 200, epoch: 16 | loss: 0.0066385\n",
      "\tspeed: 0.1124s/iter; left time: 4551.3529s\n",
      "\titers: 300, epoch: 16 | loss: 0.0073851\n",
      "\tspeed: 0.1130s/iter; left time: 4564.1924s\n",
      "\titers: 400, epoch: 16 | loss: 0.0081082\n",
      "\tspeed: 0.1127s/iter; left time: 4541.3136s\n",
      "\titers: 500, epoch: 16 | loss: 0.0123262\n",
      "\tspeed: 0.1123s/iter; left time: 4514.9041s\n",
      "\titers: 600, epoch: 16 | loss: 0.0072221\n",
      "\tspeed: 0.1126s/iter; left time: 4515.8427s\n",
      "Epoch: 16 cost time: 00h:01m:16.88s\n",
      "Epoch: 16 | Train Loss: 0.0078252 Vali Loss: 0.0091145 Test Loss: 0.0108264\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0097102\n",
      "\tspeed: 0.4192s/iter; left time: 15879.5557s\n",
      "\titers: 200, epoch: 17 | loss: 0.0064988\n",
      "\tspeed: 0.2752s/iter; left time: 10399.4114s\n",
      "\titers: 300, epoch: 17 | loss: 0.0057568\n",
      "\tspeed: 0.2698s/iter; left time: 10168.3240s\n",
      "\titers: 400, epoch: 17 | loss: 0.0065034\n",
      "\tspeed: 0.2709s/iter; left time: 10182.9524s\n",
      "\titers: 500, epoch: 17 | loss: 0.0095674\n",
      "\tspeed: 0.2763s/iter; left time: 10355.6380s\n",
      "\titers: 600, epoch: 17 | loss: 0.0062516\n",
      "\tspeed: 0.2860s/iter; left time: 10689.9752s\n",
      "Epoch: 17 cost time: 00h:02m:59.26s\n",
      "Epoch: 17 | Train Loss: 0.0078117 Vali Loss: 0.0089328 Test Loss: 0.0105026\n",
      "Validation loss decreased (0.008957 --> 0.008933).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 18 | loss: 0.0078043\n",
      "\tspeed: 1.0131s/iter; left time: 35629.1468s\n",
      "\titers: 200, epoch: 18 | loss: 0.0063590\n",
      "\tspeed: 0.3082s/iter; left time: 10810.2302s\n",
      "\titers: 300, epoch: 18 | loss: 0.0094975\n",
      "\tspeed: 0.3022s/iter; left time: 10569.3144s\n",
      "\titers: 400, epoch: 18 | loss: 0.0071088\n",
      "\tspeed: 0.3130s/iter; left time: 10913.1535s\n",
      "\titers: 500, epoch: 18 | loss: 0.0055368\n",
      "\tspeed: 0.3153s/iter; left time: 10961.2643s\n",
      "\titers: 600, epoch: 18 | loss: 0.0132828\n",
      "\tspeed: 0.3191s/iter; left time: 11063.1123s\n",
      "Epoch: 18 cost time: 00h:03m:32.03s\n",
      "Epoch: 18 | Train Loss: 0.0078160 Vali Loss: 0.0089081 Test Loss: 0.0103755\n",
      "Validation loss decreased (0.008933 --> 0.008908).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 19 | loss: 0.0115771\n",
      "\tspeed: 1.0496s/iter; left time: 34066.2837s\n",
      "\titers: 200, epoch: 19 | loss: 0.0104875\n",
      "\tspeed: 0.3142s/iter; left time: 10165.5884s\n",
      "\titers: 300, epoch: 19 | loss: 0.0096097\n",
      "\tspeed: 0.3253s/iter; left time: 10492.0188s\n",
      "\titers: 400, epoch: 19 | loss: 0.0031140\n",
      "\tspeed: 0.3250s/iter; left time: 10452.3471s\n",
      "\titers: 500, epoch: 19 | loss: 0.0102776\n",
      "\tspeed: 0.3221s/iter; left time: 10325.3806s\n",
      "\titers: 600, epoch: 19 | loss: 0.0063839\n",
      "\tspeed: 0.3233s/iter; left time: 10331.0277s\n",
      "Epoch: 19 cost time: 00h:03m:37.71s\n",
      "Epoch: 19 | Train Loss: 0.0077374 Vali Loss: 0.0088936 Test Loss: 0.0104487\n",
      "Validation loss decreased (0.008908 --> 0.008894).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 20 | loss: 0.0030808\n",
      "\tspeed: 1.0573s/iter; left time: 31449.6509s\n",
      "\titers: 200, epoch: 20 | loss: 0.0082016\n",
      "\tspeed: 0.3248s/iter; left time: 9628.7199s\n",
      "\titers: 300, epoch: 20 | loss: 0.0043443\n",
      "\tspeed: 0.3291s/iter; left time: 9723.1929s\n",
      "\titers: 400, epoch: 20 | loss: 0.0143065\n",
      "\tspeed: 0.3298s/iter; left time: 9711.0027s\n",
      "\titers: 500, epoch: 20 | loss: 0.0083869\n",
      "\tspeed: 0.3263s/iter; left time: 9575.7146s\n",
      "\titers: 600, epoch: 20 | loss: 0.0069334\n",
      "\tspeed: 0.3203s/iter; left time: 9366.3274s\n",
      "Epoch: 20 cost time: 00h:03m:41.86s\n",
      "Epoch: 20 | Train Loss: 0.0076499 Vali Loss: 0.0088768 Test Loss: 0.0103445\n",
      "Validation loss decreased (0.008894 --> 0.008877).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 21 | loss: 0.0050495\n",
      "\tspeed: 1.0619s/iter; left time: 28704.9816s\n",
      "\titers: 200, epoch: 21 | loss: 0.0056376\n",
      "\tspeed: 0.3320s/iter; left time: 8940.7912s\n",
      "\titers: 300, epoch: 21 | loss: 0.0080506\n",
      "\tspeed: 0.3343s/iter; left time: 8970.4401s\n",
      "\titers: 400, epoch: 21 | loss: 0.0066239\n",
      "\tspeed: 0.3225s/iter; left time: 8621.1311s\n",
      "\titers: 500, epoch: 21 | loss: 0.0070929\n",
      "\tspeed: 0.3161s/iter; left time: 8418.6437s\n",
      "\titers: 600, epoch: 21 | loss: 0.0058815\n",
      "\tspeed: 0.3361s/iter; left time: 8916.4134s\n",
      "Epoch: 21 cost time: 00h:03m:43.81s\n",
      "Epoch: 21 | Train Loss: 0.0076399 Vali Loss: 0.0088993 Test Loss: 0.0104224\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 22 | loss: 0.0054120\n",
      "\tspeed: 1.0256s/iter; left time: 24939.4306s\n",
      "\titers: 200, epoch: 22 | loss: 0.0076757\n",
      "\tspeed: 0.3348s/iter; left time: 8108.1047s\n",
      "\titers: 300, epoch: 22 | loss: 0.0048412\n",
      "\tspeed: 0.3362s/iter; left time: 8107.4460s\n",
      "\titers: 400, epoch: 22 | loss: 0.0044447\n",
      "\tspeed: 0.3327s/iter; left time: 7989.7958s\n",
      "\titers: 500, epoch: 22 | loss: 0.0083622\n",
      "\tspeed: 0.3392s/iter; left time: 8113.5795s\n",
      "\titers: 600, epoch: 22 | loss: 0.0153576\n",
      "\tspeed: 0.3308s/iter; left time: 7879.0503s\n",
      "Epoch: 22 cost time: 00h:03m:47.24s\n",
      "Epoch: 22 | Train Loss: 0.0075452 Vali Loss: 0.0089097 Test Loss: 0.0103669\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 23 | loss: 0.0055570\n",
      "\tspeed: 1.0418s/iter; left time: 22508.6243s\n",
      "\titers: 200, epoch: 23 | loss: 0.0043441\n",
      "\tspeed: 0.3345s/iter; left time: 7193.3601s\n",
      "\titers: 300, epoch: 23 | loss: 0.0085570\n",
      "\tspeed: 0.3377s/iter; left time: 7228.8754s\n",
      "\titers: 400, epoch: 23 | loss: 0.0088893\n",
      "\tspeed: 0.3361s/iter; left time: 7161.4683s\n",
      "\titers: 500, epoch: 23 | loss: 0.0037170\n",
      "\tspeed: 0.3218s/iter; left time: 6824.1497s\n",
      "\titers: 600, epoch: 23 | loss: 0.0061619\n",
      "\tspeed: 0.3421s/iter; left time: 7219.1257s\n",
      "Epoch: 23 cost time: 00h:03m:48.29s\n",
      "Epoch: 23 | Train Loss: 0.0075131 Vali Loss: 0.0089658 Test Loss: 0.0102591\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 24 | loss: 0.0099053\n",
      "\tspeed: 1.0770s/iter; left time: 20346.9385s\n",
      "\titers: 200, epoch: 24 | loss: 0.0091285\n",
      "\tspeed: 0.3492s/iter; left time: 6562.2366s\n",
      "\titers: 300, epoch: 24 | loss: 0.0093744\n",
      "\tspeed: 0.3467s/iter; left time: 6480.0304s\n",
      "\titers: 400, epoch: 24 | loss: 0.0068118\n",
      "\tspeed: 0.3448s/iter; left time: 6410.8172s\n",
      "\titers: 500, epoch: 24 | loss: 0.0037521\n",
      "\tspeed: 0.3279s/iter; left time: 6064.4418s\n",
      "\titers: 600, epoch: 24 | loss: 0.0060791\n",
      "\tspeed: 0.3475s/iter; left time: 6390.4704s\n",
      "Epoch: 24 cost time: 00h:03m:54.15s\n",
      "Epoch: 24 | Train Loss: 0.0075297 Vali Loss: 0.0088701 Test Loss: 0.0103922\n",
      "Validation loss decreased (0.008877 --> 0.008870).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 25 | loss: 0.0065410\n",
      "\tspeed: 1.1906s/iter; left time: 19261.9734s\n",
      "\titers: 200, epoch: 25 | loss: 0.0056375\n",
      "\tspeed: 0.3496s/iter; left time: 5621.5463s\n",
      "\titers: 300, epoch: 25 | loss: 0.0102988\n",
      "\tspeed: 0.3466s/iter; left time: 5538.5983s\n",
      "\titers: 400, epoch: 25 | loss: 0.0065078\n",
      "\tspeed: 0.3504s/iter; left time: 5563.6305s\n",
      "\titers: 500, epoch: 25 | loss: 0.0056647\n",
      "\tspeed: 0.3474s/iter; left time: 5482.1096s\n",
      "\titers: 600, epoch: 25 | loss: 0.0058788\n",
      "\tspeed: 0.3510s/iter; left time: 5504.1013s\n",
      "Epoch: 25 cost time: 00h:03m:57.45s\n",
      "Epoch: 25 | Train Loss: 0.0074501 Vali Loss: 0.0090055 Test Loss: 0.0105855\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 26 | loss: 0.0120122\n",
      "\tspeed: 1.0667s/iter; left time: 14364.3894s\n",
      "\titers: 200, epoch: 26 | loss: 0.0055594\n",
      "\tspeed: 0.3323s/iter; left time: 4441.2763s\n",
      "\titers: 300, epoch: 26 | loss: 0.0064205\n",
      "\tspeed: 0.3485s/iter; left time: 4622.8483s\n",
      "\titers: 400, epoch: 26 | loss: 0.0117745\n",
      "\tspeed: 0.3476s/iter; left time: 4576.4656s\n",
      "\titers: 500, epoch: 26 | loss: 0.0076545\n",
      "\tspeed: 0.3508s/iter; left time: 4583.1005s\n",
      "\titers: 600, epoch: 26 | loss: 0.0078711\n",
      "\tspeed: 0.3509s/iter; left time: 4550.3585s\n",
      "Epoch: 26 cost time: 00h:03m:54.73s\n",
      "Epoch: 26 | Train Loss: 0.0073374 Vali Loss: 0.0090989 Test Loss: 0.0103317\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 27 | loss: 0.0077098\n",
      "\tspeed: 1.0886s/iter; left time: 11705.9631s\n",
      "\titers: 200, epoch: 27 | loss: 0.0055892\n",
      "\tspeed: 0.3521s/iter; left time: 3751.0327s\n",
      "\titers: 300, epoch: 27 | loss: 0.0084353\n",
      "\tspeed: 0.3504s/iter; left time: 3697.2570s\n",
      "\titers: 400, epoch: 27 | loss: 0.0111528\n",
      "\tspeed: 0.3529s/iter; left time: 3689.3583s\n",
      "\titers: 500, epoch: 27 | loss: 0.0081662\n",
      "\tspeed: 0.3508s/iter; left time: 3632.0384s\n",
      "\titers: 600, epoch: 27 | loss: 0.0053349\n",
      "\tspeed: 0.3516s/iter; left time: 3605.1329s\n",
      "Epoch: 27 cost time: 00h:03m:58.54s\n",
      "Epoch: 27 | Train Loss: 0.0073602 Vali Loss: 0.0089851 Test Loss: 0.0103161\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 28 | loss: 0.0063998\n",
      "\tspeed: 1.0859s/iter; left time: 8730.6862s\n",
      "\titers: 200, epoch: 28 | loss: 0.0083144\n",
      "\tspeed: 0.3520s/iter; left time: 2795.0274s\n",
      "\titers: 300, epoch: 28 | loss: 0.0065535\n",
      "\tspeed: 0.3532s/iter; left time: 2769.0085s\n",
      "\titers: 400, epoch: 28 | loss: 0.0060916\n",
      "\tspeed: 0.3519s/iter; left time: 2723.8142s\n",
      "\titers: 500, epoch: 28 | loss: 0.0064265\n",
      "\tspeed: 0.3533s/iter; left time: 2698.9254s\n",
      "\titers: 600, epoch: 28 | loss: 0.0053284\n",
      "\tspeed: 0.3489s/iter; left time: 2630.6304s\n",
      "Epoch: 28 cost time: 00h:03m:59.29s\n",
      "Epoch: 28 | Train Loss: 0.0073103 Vali Loss: 0.0090719 Test Loss: 0.0104575\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 29 | loss: 0.0062723\n",
      "\tspeed: 1.0915s/iter; left time: 5814.3815s\n",
      "\titers: 200, epoch: 29 | loss: 0.0058362\n",
      "\tspeed: 0.3520s/iter; left time: 1839.7125s\n",
      "\titers: 300, epoch: 29 | loss: 0.0083566\n",
      "\tspeed: 0.3357s/iter; left time: 1721.2573s\n",
      "\titers: 400, epoch: 29 | loss: 0.0075264\n",
      "\tspeed: 0.3597s/iter; left time: 1807.9777s\n",
      "\titers: 500, epoch: 29 | loss: 0.0039099\n",
      "\tspeed: 0.3620s/iter; left time: 1783.6610s\n",
      "\titers: 600, epoch: 29 | loss: 0.0062451\n",
      "\tspeed: 0.3572s/iter; left time: 1724.2323s\n",
      "Epoch: 29 cost time: 00h:04m:00.51s\n",
      "Epoch: 29 | Train Loss: 0.0072024 Vali Loss: 0.0089768 Test Loss: 0.0106518\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042\n",
      "Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042\n",
      "Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042\n",
      "Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042\n",
      "Intermediate time for FR and pred_len 24: 01h:45m:51.65s\n",
      "\n",
      "Intermediate time for FR: 01h:45m:51.65s\n",
      "\n",
      "Total time: 01h:45m:51.67s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "            \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --multi_gpu --num_processes=4 --num_machines 1 --dynamo_backend \"no\"  --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience {patience} \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Country': 'FR',\n",
       "  'Pred_len': 24,\n",
       "  'MSE': 0.01039215736091137,\n",
       "  'RMSE': 0.10194192826747894,\n",
       "  'MAE': 0.05944359302520752}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timellm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [{'Country': 'DE',\n",
    "  'Pred_len': 24,\n",
    "  'MSE': 0.021544765681028366,\n",
    "  'RMSE': 0.14678135514259338,\n",
    "  'MAE': 0.09620458632707596},\n",
    " {'Country': 'DE',\n",
    "  'Pred_len': 96,\n",
    "  'MSE': 0.0346515066921711,\n",
    "  'RMSE': 0.18614915013313293,\n",
    "  'MAE': 0.12807267904281616},\n",
    " {'Country': 'DE',\n",
    "  'Pred_len': 168,\n",
    "  'MSE': 0.03746809437870979,\n",
    "  'RMSE': 0.19356676936149597,\n",
    "  'MAE': 0.1385875940322876},\n",
    " {'Country': 'GB',\n",
    "  'Pred_len': 24,\n",
    "  'MSE': 0.02595963142812252,\n",
    "  'RMSE': 0.16111992299556732,\n",
    "  'MAE': 0.10740001499652863},\n",
    " {'Country': 'GB',\n",
    "  'Pred_len': 96,\n",
    "  'MSE': 0.04068015143275261,\n",
    "  'RMSE': 0.20169320702552795,\n",
    "  'MAE': 0.14278638362884521},\n",
    " {'Country': 'GB',\n",
    "  'Pred_len': 168,\n",
    "  'MSE': 0.043486617505550385,\n",
    "  'RMSE': 0.20853444933891296,\n",
    "  'MAE': 0.14944525063037872},\n",
    " {'Country': 'ES',\n",
    "  'Pred_len': 24,\n",
    "  'MSE': 0.010894566774368286,\n",
    "  'RMSE': 0.10437703877687454,\n",
    "  'MAE': 0.0666496753692627},\n",
    " {'Country': 'ES',\n",
    "  'Pred_len': 96,\n",
    "  'MSE': 0.02007182687520981,\n",
    "  'RMSE': 0.14167506992816925,\n",
    "  'MAE': 0.09592236578464508},\n",
    " {'Country': 'ES',\n",
    "  'Pred_len': 168,\n",
    "  'MSE': 0.021907182410359383,\n",
    "  'RMSE': 0.1480107456445694,\n",
    "  'MAE': 0.10050459951162338},\n",
    " {'Country': 'FR',\n",
    "  'Pred_len': 24,\n",
    "  'MSE': 0.01039215736091137,\n",
    "  'RMSE': 0.10194192826747894,\n",
    "  'MAE': 0.05944359302520752},\n",
    " {'Country': 'FR',\n",
    "  'Pred_len': 96,\n",
    "  'MSE': 0.0196530781686306,\n",
    "  'RMSE': 0.14018943905830383,\n",
    "  'MAE': 0.08574584871530533},\n",
    " {'Country': 'FR',\n",
    "  'Pred_len': 168,\n",
    "  'MSE': 0.02125428430736065,\n",
    "  'RMSE': 0.14578849077224731,\n",
    "  'MAE': 0.09115074574947357},\n",
    " {'Country': 'IT',\n",
    "  'Pred_len': 24,\n",
    "  'MSE': 0.01045023463666439,\n",
    "  'RMSE': 0.1022263914346695,\n",
    "  'MAE': 0.06180146336555481},\n",
    " {'Country': 'IT',\n",
    "  'Pred_len': 96,\n",
    "  'MSE': 0.01832490973174572,\n",
    "  'RMSE': 0.13536952435970306,\n",
    "  'MAE': 0.08470133692026138},\n",
    " {'Country': 'IT',\n",
    "  'Pred_len': 168,\n",
    "  'MSE': 0.01952982507646084,\n",
    "  'RMSE': 0.13974915444850922,\n",
    "  'MAE': 0.08981484919786453}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>0.0962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>0.1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>0.1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.0959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.0857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.0847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0215  0.1468  0.0962\n",
       "        96        0.0347  0.1861  0.1281\n",
       "        168       0.0375  0.1936  0.1386\n",
       "GB      24        0.0260  0.1611  0.1074\n",
       "        96        0.0407  0.2017  0.1428\n",
       "        168       0.0435  0.2085  0.1494\n",
       "ES      24        0.0109  0.1044  0.0666\n",
       "        96        0.0201  0.1417  0.0959\n",
       "        168       0.0219  0.1480  0.1005\n",
       "FR      24        0.0104  0.1019  0.0594\n",
       "        96        0.0197  0.1402  0.0857\n",
       "        168       0.0213  0.1458  0.0912\n",
       "IT      24        0.0105  0.1022  0.0618\n",
       "        96        0.0183  0.1354  0.0847\n",
       "        168       0.0195  0.1397  0.0898"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(res, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm.csv'))\n",
    "timellm_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TimeLLM 336\n",
    "\n",
    "Sequence length 336."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/timellm/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Dynamic variables\n",
    "model = \"TimeLLM\"\n",
    "seq_len = 336\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_336.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.001 # 10^-3 \n",
    "train_epochs = 20\n",
    "d_model = 16\n",
    "d_ff = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-02 04:05:41,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 04:05:42,950] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 04:05:42,950] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 04:05:42,950] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 04:05:43,048] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 04:05:43,048] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 04:05:43,713] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 04:05:43,714] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 04:05:43,714] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 04:05:43,716] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 04:05:43,716] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 04:05:43,716] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 04:05:43,716] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 04:05:44,034] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 04:05:44,035] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,062] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,198] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 04:05:44,199] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,200] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,200] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 04:05:44,312] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 04:05:44,313] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 04:05:44,313] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.46 GB, percent = 9.9%\n",
      "[2024-11-02 04:05:44,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 04:05:44,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 04:05:44,314] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6394bf6dd0>\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 04:05:44,315] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 04:05:44,316] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 04:05:44,317] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1751984\n",
      "\tspeed: 0.1735s/iter; left time: 15584.1708s\n",
      "\titers: 200, epoch: 1 | loss: 0.1696132\n",
      "\tspeed: 0.1309s/iter; left time: 11748.6723s\n",
      "\titers: 300, epoch: 1 | loss: 0.1651162\n",
      "\tspeed: 0.1313s/iter; left time: 11767.3423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1374285\n",
      "\tspeed: 0.1307s/iter; left time: 11699.3748s\n",
      "\titers: 500, epoch: 1 | loss: 0.0960834\n",
      "\tspeed: 0.1272s/iter; left time: 11378.5702s\n",
      "\titers: 600, epoch: 1 | loss: 0.0871328\n",
      "\tspeed: 0.1259s/iter; left time: 11242.6723s\n",
      "\titers: 700, epoch: 1 | loss: 0.1093957\n",
      "\tspeed: 0.1296s/iter; left time: 11562.0570s\n",
      "\titers: 800, epoch: 1 | loss: 0.0864986\n",
      "\tspeed: 0.1166s/iter; left time: 10388.0754s\n",
      "\titers: 900, epoch: 1 | loss: 0.1236022\n",
      "\tspeed: 0.1189s/iter; left time: 10586.8357s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1236182\n",
      "\tspeed: 0.1289s/iter; left time: 11463.7493s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1068401\n",
      "\tspeed: 0.1264s/iter; left time: 11230.1935s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1080805\n",
      "\tspeed: 0.1299s/iter; left time: 11522.8253s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1073034\n",
      "\tspeed: 0.1300s/iter; left time: 11524.5034s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0848949\n",
      "\tspeed: 0.1256s/iter; left time: 11122.0103s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1067144\n",
      "\tspeed: 0.1307s/iter; left time: 11555.8828s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0850505\n",
      "\tspeed: 0.1298s/iter; left time: 11467.4007s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0994569\n",
      "\tspeed: 0.1308s/iter; left time: 11535.5314s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1062451\n",
      "\tspeed: 0.1303s/iter; left time: 11481.1729s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0976172\n",
      "\tspeed: 0.1307s/iter; left time: 11501.9078s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0936665\n",
      "\tspeed: 0.1300s/iter; left time: 11431.7189s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1149349\n",
      "\tspeed: 0.1103s/iter; left time: 9690.5952s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0990795\n",
      "\tspeed: 0.1246s/iter; left time: 10931.3024s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1037625\n",
      "\tspeed: 0.1204s/iter; left time: 10549.2546s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0966860\n",
      "\tspeed: 0.1097s/iter; left time: 9603.1471s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1019908\n",
      "\tspeed: 0.1092s/iter; left time: 9544.9901s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0780252\n",
      "\tspeed: 0.1089s/iter; left time: 9510.2132s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0944925\n",
      "\tspeed: 0.1090s/iter; left time: 9509.5887s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0935264\n",
      "\tspeed: 0.1088s/iter; left time: 9475.9938s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0859544\n",
      "\tspeed: 0.1091s/iter; left time: 9490.9660s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0855883\n",
      "\tspeed: 0.1229s/iter; left time: 10684.9461s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0829413\n",
      "\tspeed: 0.1277s/iter; left time: 11088.7397s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0780405\n",
      "\tspeed: 0.1285s/iter; left time: 11141.1512s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0817463\n",
      "\tspeed: 0.1259s/iter; left time: 10907.2366s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1099318\n",
      "\tspeed: 0.1302s/iter; left time: 11264.9607s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0809699\n",
      "\tspeed: 0.1291s/iter; left time: 11154.1947s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1028950\n",
      "\tspeed: 0.1301s/iter; left time: 11227.4162s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0928097\n",
      "\tspeed: 0.1261s/iter; left time: 10874.1060s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0896632\n",
      "\tspeed: 0.1254s/iter; left time: 10800.9263s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0757574\n",
      "\tspeed: 0.1244s/iter; left time: 10697.8411s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0994942\n",
      "\tspeed: 0.1246s/iter; left time: 10707.7015s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0882582\n",
      "\tspeed: 0.1291s/iter; left time: 11080.5087s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1168685\n",
      "\tspeed: 0.1298s/iter; left time: 11126.1313s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1015711\n",
      "\tspeed: 0.1307s/iter; left time: 11189.2471s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0811718\n",
      "\tspeed: 0.1312s/iter; left time: 11220.3463s\n",
      "Epoch: 1 cost time: 00h:09m:22.97s\n",
      "Epoch: 1 | Train Loss: 0.1040335 Vali Loss: 0.0961287 Test Loss: 0.0975929\n",
      "Validation loss decreased (inf --> 0.096129).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0796660\n",
      "\tspeed: 1.7371s/iter; left time: 148215.5008s\n",
      "\titers: 200, epoch: 2 | loss: 0.0931822\n",
      "\tspeed: 0.1176s/iter; left time: 10020.1966s\n",
      "\titers: 300, epoch: 2 | loss: 0.1017061\n",
      "\tspeed: 0.1195s/iter; left time: 10173.8280s\n",
      "\titers: 400, epoch: 2 | loss: 0.0949261\n",
      "\tspeed: 0.1142s/iter; left time: 9710.3122s\n",
      "\titers: 500, epoch: 2 | loss: 0.1168831\n",
      "\tspeed: 0.1137s/iter; left time: 9659.6283s\n",
      "\titers: 600, epoch: 2 | loss: 0.0772928\n",
      "\tspeed: 0.1134s/iter; left time: 9616.1355s\n",
      "\titers: 700, epoch: 2 | loss: 0.0832478\n",
      "\tspeed: 0.1129s/iter; left time: 9569.1699s\n",
      "\titers: 800, epoch: 2 | loss: 0.0981834\n",
      "\tspeed: 0.1166s/iter; left time: 9870.3455s\n",
      "\titers: 900, epoch: 2 | loss: 0.1037944\n",
      "\tspeed: 0.1193s/iter; left time: 10082.1692s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0856554\n",
      "\tspeed: 0.1135s/iter; left time: 9583.9284s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1003807\n",
      "\tspeed: 0.1193s/iter; left time: 10059.9384s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0889630\n",
      "\tspeed: 0.1208s/iter; left time: 10176.5481s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0855101\n",
      "\tspeed: 0.1167s/iter; left time: 9820.3638s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1153450\n",
      "\tspeed: 0.1152s/iter; left time: 9675.8513s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0956274\n",
      "\tspeed: 0.1067s/iter; left time: 8955.2998s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1119400\n",
      "\tspeed: 0.0961s/iter; left time: 8058.9779s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0828846\n",
      "\tspeed: 0.0962s/iter; left time: 8051.7727s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1111595\n",
      "\tspeed: 0.0961s/iter; left time: 8037.0539s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1014844\n",
      "\tspeed: 0.0963s/iter; left time: 8041.7422s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0966399\n",
      "\tspeed: 0.1111s/iter; left time: 9270.6148s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0997675\n",
      "\tspeed: 0.1125s/iter; left time: 9376.3954s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1031985\n",
      "\tspeed: 0.1125s/iter; left time: 9362.4143s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0954874\n",
      "\tspeed: 0.1132s/iter; left time: 9409.2818s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0890880\n",
      "\tspeed: 0.1094s/iter; left time: 9080.3776s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1061457\n",
      "\tspeed: 0.1010s/iter; left time: 8373.6327s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1096236\n",
      "\tspeed: 0.1133s/iter; left time: 9380.5049s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1097501\n",
      "\tspeed: 0.1127s/iter; left time: 9323.8457s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0776054\n",
      "\tspeed: 0.1133s/iter; left time: 9358.0218s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0837930\n",
      "\tspeed: 0.1125s/iter; left time: 9279.9873s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1149195\n",
      "\tspeed: 0.1127s/iter; left time: 9293.0983s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0764869\n",
      "\tspeed: 0.1148s/iter; left time: 9453.8410s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0779453\n",
      "\tspeed: 0.1141s/iter; left time: 9384.4575s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0975494\n",
      "\tspeed: 0.1144s/iter; left time: 9393.0479s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0804761\n",
      "\tspeed: 0.1153s/iter; left time: 9453.6404s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0919167\n",
      "\tspeed: 0.1151s/iter; left time: 9426.7968s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0746915\n",
      "\tspeed: 0.1150s/iter; left time: 9409.6590s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0837285\n",
      "\tspeed: 0.1133s/iter; left time: 9257.2814s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0917258\n",
      "\tspeed: 0.1142s/iter; left time: 9318.9132s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0715970\n",
      "\tspeed: 0.1145s/iter; left time: 9331.8389s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0857370\n",
      "\tspeed: 0.1136s/iter; left time: 9248.2053s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0920830\n",
      "\tspeed: 0.1140s/iter; left time: 9268.6945s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0768602\n",
      "\tspeed: 0.1140s/iter; left time: 9257.3731s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0806141\n",
      "\tspeed: 0.1144s/iter; left time: 9283.5268s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0979972\n",
      "\tspeed: 0.1138s/iter; left time: 9222.8458s\n",
      "Epoch: 2 cost time: 00h:08m:25.36s\n",
      "Epoch: 2 | Train Loss: 0.0885479 Vali Loss: 0.0917196 Test Loss: 0.0938499\n",
      "Validation loss decreased (0.096129 --> 0.091720).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0962270\n",
      "\tspeed: 1.5176s/iter; left time: 122666.1800s\n",
      "\titers: 200, epoch: 3 | loss: 0.0868109\n",
      "\tspeed: 0.1207s/iter; left time: 9740.0078s\n",
      "\titers: 300, epoch: 3 | loss: 0.0884279\n",
      "\tspeed: 0.1213s/iter; left time: 9781.4377s\n",
      "\titers: 400, epoch: 3 | loss: 0.0981889\n",
      "\tspeed: 0.1211s/iter; left time: 9751.3365s\n",
      "\titers: 500, epoch: 3 | loss: 0.0899171\n",
      "\tspeed: 0.1207s/iter; left time: 9709.9214s\n",
      "\titers: 600, epoch: 3 | loss: 0.0844401\n",
      "\tspeed: 0.1206s/iter; left time: 9689.5479s\n",
      "\titers: 700, epoch: 3 | loss: 0.0752369\n",
      "\tspeed: 0.1187s/iter; left time: 9520.6208s\n",
      "\titers: 800, epoch: 3 | loss: 0.1060660\n",
      "\tspeed: 0.1208s/iter; left time: 9680.7391s\n",
      "\titers: 900, epoch: 3 | loss: 0.0833049\n",
      "\tspeed: 0.1208s/iter; left time: 9668.7466s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0819281\n",
      "\tspeed: 0.1203s/iter; left time: 9615.2239s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0777918\n",
      "\tspeed: 0.1209s/iter; left time: 9652.0103s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0780214\n",
      "\tspeed: 0.1177s/iter; left time: 9387.0852s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0893477\n",
      "\tspeed: 0.1156s/iter; left time: 9203.5842s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1029789\n",
      "\tspeed: 0.1209s/iter; left time: 9618.1192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0834389\n",
      "\tspeed: 0.1209s/iter; left time: 9603.3986s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0866824\n",
      "\tspeed: 0.1183s/iter; left time: 9383.2547s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0818847\n",
      "\tspeed: 0.1208s/iter; left time: 9572.4971s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0906848\n",
      "\tspeed: 0.1213s/iter; left time: 9599.9935s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1146158\n",
      "\tspeed: 0.1195s/iter; left time: 9447.6256s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1075108\n",
      "\tspeed: 0.1056s/iter; left time: 8332.5540s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0804141\n",
      "\tspeed: 0.1122s/iter; left time: 8843.5249s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0912658\n",
      "\tspeed: 0.1183s/iter; left time: 9313.1841s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0801835\n",
      "\tspeed: 0.1185s/iter; left time: 9316.9041s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0916704\n",
      "\tspeed: 0.1210s/iter; left time: 9504.5358s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0994615\n",
      "\tspeed: 0.1174s/iter; left time: 9204.7229s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1034542\n",
      "\tspeed: 0.1202s/iter; left time: 9416.0022s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0897656\n",
      "\tspeed: 0.1209s/iter; left time: 9459.9946s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1089268\n",
      "\tspeed: 0.1208s/iter; left time: 9440.5900s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0789208\n",
      "\tspeed: 0.1212s/iter; left time: 9457.2291s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0762717\n",
      "\tspeed: 0.1165s/iter; left time: 9082.1749s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0869183\n",
      "\tspeed: 0.1207s/iter; left time: 9395.2146s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0751872\n",
      "\tspeed: 0.1154s/iter; left time: 8971.9236s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0878178\n",
      "\tspeed: 0.1213s/iter; left time: 9413.0328s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0656829\n",
      "\tspeed: 0.1207s/iter; left time: 9357.4105s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0847644\n",
      "\tspeed: 0.1191s/iter; left time: 9220.4443s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0789355\n",
      "\tspeed: 0.1208s/iter; left time: 9342.9600s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0871209\n",
      "\tspeed: 0.1179s/iter; left time: 9103.4060s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0978919\n",
      "\tspeed: 0.1205s/iter; left time: 9290.4399s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0924827\n",
      "\tspeed: 0.1217s/iter; left time: 9377.2864s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0813397\n",
      "\tspeed: 0.1194s/iter; left time: 9185.8177s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0784988\n",
      "\tspeed: 0.1202s/iter; left time: 9236.4669s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0991811\n",
      "\tspeed: 0.1209s/iter; left time: 9276.9542s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0861519\n",
      "\tspeed: 0.1210s/iter; left time: 9272.3883s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0760844\n",
      "\tspeed: 0.1200s/iter; left time: 9186.2755s\n",
      "Epoch: 3 cost time: 00h:08m:57.62s\n",
      "Epoch: 3 | Train Loss: 0.0849499 Vali Loss: 0.0901408 Test Loss: 0.0926513\n",
      "Validation loss decreased (0.091720 --> 0.090141).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1111427\n",
      "\tspeed: 1.5235s/iter; left time: 116290.6463s\n",
      "\titers: 200, epoch: 4 | loss: 0.0753410\n",
      "\tspeed: 0.1209s/iter; left time: 9216.7592s\n",
      "\titers: 300, epoch: 4 | loss: 0.0888413\n",
      "\tspeed: 0.1208s/iter; left time: 9197.5835s\n",
      "\titers: 400, epoch: 4 | loss: 0.0714968\n",
      "\tspeed: 0.1192s/iter; left time: 9065.1911s\n",
      "\titers: 500, epoch: 4 | loss: 0.0833365\n",
      "\tspeed: 0.1151s/iter; left time: 8739.0584s\n",
      "\titers: 600, epoch: 4 | loss: 0.0899843\n",
      "\tspeed: 0.1184s/iter; left time: 8976.1350s\n",
      "\titers: 700, epoch: 4 | loss: 0.0789592\n",
      "\tspeed: 0.1167s/iter; left time: 8841.6298s\n",
      "\titers: 800, epoch: 4 | loss: 0.0856384\n",
      "\tspeed: 0.1136s/iter; left time: 8592.4849s\n",
      "\titers: 900, epoch: 4 | loss: 0.0778323\n",
      "\tspeed: 0.1137s/iter; left time: 8585.3265s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0816742\n",
      "\tspeed: 0.1140s/iter; left time: 8599.2045s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0672043\n",
      "\tspeed: 0.1184s/iter; left time: 8920.9904s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0746364\n",
      "\tspeed: 0.1208s/iter; left time: 9087.3051s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0835876\n",
      "\tspeed: 0.1208s/iter; left time: 9074.5038s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0832614\n",
      "\tspeed: 0.1192s/iter; left time: 8945.4002s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0914626\n",
      "\tspeed: 0.1205s/iter; left time: 9032.8420s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0810632\n",
      "\tspeed: 0.1167s/iter; left time: 8731.8975s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0825682\n",
      "\tspeed: 0.1207s/iter; left time: 9019.9769s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0768290\n",
      "\tspeed: 0.1207s/iter; left time: 9009.0520s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1031834\n",
      "\tspeed: 0.1204s/iter; left time: 8974.2922s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0810642\n",
      "\tspeed: 0.1205s/iter; left time: 8968.9662s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1011447\n",
      "\tspeed: 0.1185s/iter; left time: 8809.8841s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0945584\n",
      "\tspeed: 0.1131s/iter; left time: 8393.2932s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0859294\n",
      "\tspeed: 0.1130s/iter; left time: 8373.3779s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0821534\n",
      "\tspeed: 0.1129s/iter; left time: 8356.1091s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0808725\n",
      "\tspeed: 0.1137s/iter; left time: 8405.3245s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0857516\n",
      "\tspeed: 0.1193s/iter; left time: 8808.3368s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0724460\n",
      "\tspeed: 0.1210s/iter; left time: 8918.9110s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0676562\n",
      "\tspeed: 0.1206s/iter; left time: 8881.6904s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0927680\n",
      "\tspeed: 0.1205s/iter; left time: 8858.0213s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0894543\n",
      "\tspeed: 0.1152s/iter; left time: 8461.9201s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0880243\n",
      "\tspeed: 0.1147s/iter; left time: 8409.3481s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0814270\n",
      "\tspeed: 0.1162s/iter; left time: 8507.9933s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0787635\n",
      "\tspeed: 0.1172s/iter; left time: 8570.4009s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0685097\n",
      "\tspeed: 0.1157s/iter; left time: 8452.7793s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0689826\n",
      "\tspeed: 0.1148s/iter; left time: 8372.5090s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1017262\n",
      "\tspeed: 0.1141s/iter; left time: 8312.6962s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0673666\n",
      "\tspeed: 0.1152s/iter; left time: 8376.7681s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0698210\n",
      "\tspeed: 0.1127s/iter; left time: 8184.9480s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0750384\n",
      "\tspeed: 0.1125s/iter; left time: 8157.1098s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0919478\n",
      "\tspeed: 0.1131s/iter; left time: 8189.3309s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0796407\n",
      "\tspeed: 0.0963s/iter; left time: 6968.1981s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0891761\n",
      "\tspeed: 0.1107s/iter; left time: 7993.9332s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0734849\n",
      "\tspeed: 0.1142s/iter; left time: 8236.1714s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0780938\n",
      "\tspeed: 0.1141s/iter; left time: 8221.2131s\n",
      "Epoch: 4 cost time: 00h:08m:43.54s\n",
      "Epoch: 4 | Train Loss: 0.0830537 Vali Loss: 0.0887350 Test Loss: 0.0919589\n",
      "Validation loss decreased (0.090141 --> 0.088735).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0724721\n",
      "\tspeed: 1.5060s/iter; left time: 108189.1676s\n",
      "\titers: 200, epoch: 5 | loss: 0.0816491\n",
      "\tspeed: 0.1202s/iter; left time: 8623.4440s\n",
      "\titers: 300, epoch: 5 | loss: 0.0865343\n",
      "\tspeed: 0.1167s/iter; left time: 8360.7473s\n",
      "\titers: 400, epoch: 5 | loss: 0.0748772\n",
      "\tspeed: 0.1129s/iter; left time: 8075.5640s\n",
      "\titers: 500, epoch: 5 | loss: 0.0879068\n",
      "\tspeed: 0.1137s/iter; left time: 8123.7226s\n",
      "\titers: 600, epoch: 5 | loss: 0.0776411\n",
      "\tspeed: 0.1134s/iter; left time: 8086.9740s\n",
      "\titers: 700, epoch: 5 | loss: 0.0837840\n",
      "\tspeed: 0.1134s/iter; left time: 8076.2918s\n",
      "\titers: 800, epoch: 5 | loss: 0.0696601\n",
      "\tspeed: 0.1163s/iter; left time: 8271.1694s\n",
      "\titers: 900, epoch: 5 | loss: 0.0858637\n",
      "\tspeed: 0.1159s/iter; left time: 8233.1103s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0875993\n",
      "\tspeed: 0.1137s/iter; left time: 8063.5210s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0880472\n",
      "\tspeed: 0.1159s/iter; left time: 8211.4916s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0787003\n",
      "\tspeed: 0.1140s/iter; left time: 8060.8014s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0790513\n",
      "\tspeed: 0.1149s/iter; left time: 8119.3239s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0656777\n",
      "\tspeed: 0.1213s/iter; left time: 8558.4860s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0692728\n",
      "\tspeed: 0.1212s/iter; left time: 8534.1719s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0867893\n",
      "\tspeed: 0.1185s/iter; left time: 8337.5441s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0772494\n",
      "\tspeed: 0.1151s/iter; left time: 8082.2563s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1018692\n",
      "\tspeed: 0.1172s/iter; left time: 8223.1548s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0822593\n",
      "\tspeed: 0.1211s/iter; left time: 8482.3100s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0723740\n",
      "\tspeed: 0.1204s/iter; left time: 8419.7675s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0827404\n",
      "\tspeed: 0.1177s/iter; left time: 8221.5918s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0853322\n",
      "\tspeed: 0.1207s/iter; left time: 8418.3833s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0850059\n",
      "\tspeed: 0.1212s/iter; left time: 8441.9546s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0737549\n",
      "\tspeed: 0.1212s/iter; left time: 8430.9857s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0851119\n",
      "\tspeed: 0.1207s/iter; left time: 8377.8651s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0840694\n",
      "\tspeed: 0.1202s/iter; left time: 8334.7514s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0876200\n",
      "\tspeed: 0.1209s/iter; left time: 8368.1360s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0861114\n",
      "\tspeed: 0.1200s/iter; left time: 8293.5198s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0650319\n",
      "\tspeed: 0.1171s/iter; left time: 8086.0150s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0898532\n",
      "\tspeed: 0.1208s/iter; left time: 8327.2123s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0896070\n",
      "\tspeed: 0.1181s/iter; left time: 8131.1862s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0580150\n",
      "\tspeed: 0.1154s/iter; left time: 7934.0417s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0968894\n",
      "\tspeed: 0.1192s/iter; left time: 8182.1327s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0927546\n",
      "\tspeed: 0.1209s/iter; left time: 8286.1156s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0884232\n",
      "\tspeed: 0.1204s/iter; left time: 8241.1341s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0773740\n",
      "\tspeed: 0.1149s/iter; left time: 7848.5766s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0735403\n",
      "\tspeed: 0.1157s/iter; left time: 7893.2557s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0843595\n",
      "\tspeed: 0.1185s/iter; left time: 8071.4490s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0890049\n",
      "\tspeed: 0.1212s/iter; left time: 8247.9663s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0733620\n",
      "\tspeed: 0.1180s/iter; left time: 8016.0967s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0826425\n",
      "\tspeed: 0.1195s/iter; left time: 8108.4777s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0942001\n",
      "\tspeed: 0.1175s/iter; left time: 7958.0431s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0662682\n",
      "\tspeed: 0.1208s/iter; left time: 8169.3140s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0639044\n",
      "\tspeed: 0.1202s/iter; left time: 8118.1535s\n",
      "Epoch: 5 cost time: 00h:08m:51.87s\n",
      "Epoch: 5 | Train Loss: 0.0819060 Vali Loss: 0.0899136 Test Loss: 0.0937484\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0834547\n",
      "\tspeed: 1.5025s/iter; left time: 101181.7385s\n",
      "\titers: 200, epoch: 6 | loss: 0.0788915\n",
      "\tspeed: 0.1156s/iter; left time: 7776.1217s\n",
      "\titers: 300, epoch: 6 | loss: 0.0766931\n",
      "\tspeed: 0.1185s/iter; left time: 7954.5460s\n",
      "\titers: 400, epoch: 6 | loss: 0.0621577\n",
      "\tspeed: 0.1160s/iter; left time: 7779.0340s\n",
      "\titers: 500, epoch: 6 | loss: 0.0798744\n",
      "\tspeed: 0.1150s/iter; left time: 7700.7145s\n",
      "\titers: 600, epoch: 6 | loss: 0.0737847\n",
      "\tspeed: 0.1144s/iter; left time: 7647.0265s\n",
      "\titers: 700, epoch: 6 | loss: 0.0843354\n",
      "\tspeed: 0.1210s/iter; left time: 8075.2940s\n",
      "\titers: 800, epoch: 6 | loss: 0.0722700\n",
      "\tspeed: 0.1209s/iter; left time: 8057.6134s\n",
      "\titers: 900, epoch: 6 | loss: 0.0824605\n",
      "\tspeed: 0.1207s/iter; left time: 8032.2200s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0940322\n",
      "\tspeed: 0.1201s/iter; left time: 7979.0409s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0984112\n",
      "\tspeed: 0.1182s/iter; left time: 7840.0682s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0816496\n",
      "\tspeed: 0.1212s/iter; left time: 8029.8885s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0897281\n",
      "\tspeed: 0.1208s/iter; left time: 7991.1979s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0707488\n",
      "\tspeed: 0.1212s/iter; left time: 8002.9449s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0827402\n",
      "\tspeed: 0.1209s/iter; left time: 7973.2661s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0788377\n",
      "\tspeed: 0.1208s/iter; left time: 7952.6845s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0708660\n",
      "\tspeed: 0.1208s/iter; left time: 7939.3029s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0858712\n",
      "\tspeed: 0.1144s/iter; left time: 7512.0945s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0694004\n",
      "\tspeed: 0.1157s/iter; left time: 7582.9526s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0808107\n",
      "\tspeed: 0.1208s/iter; left time: 7903.0900s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0673134\n",
      "\tspeed: 0.1139s/iter; left time: 7440.9977s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0682330\n",
      "\tspeed: 0.1151s/iter; left time: 7506.4159s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0918810\n",
      "\tspeed: 0.1209s/iter; left time: 7874.9710s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0851345\n",
      "\tspeed: 0.1208s/iter; left time: 7855.7979s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0815403\n",
      "\tspeed: 0.1209s/iter; left time: 7854.2735s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0941485\n",
      "\tspeed: 0.1208s/iter; left time: 7834.8281s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0907783\n",
      "\tspeed: 0.1209s/iter; left time: 7829.0331s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0615061\n",
      "\tspeed: 0.1212s/iter; left time: 7834.7400s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0898525\n",
      "\tspeed: 0.1187s/iter; left time: 7662.9349s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0763716\n",
      "\tspeed: 0.1052s/iter; left time: 6779.6397s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0835483\n",
      "\tspeed: 0.1076s/iter; left time: 6922.0729s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0802400\n",
      "\tspeed: 0.1186s/iter; left time: 7616.6857s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0705647\n",
      "\tspeed: 0.1210s/iter; left time: 7760.8542s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0637157\n",
      "\tspeed: 0.1209s/iter; left time: 7740.7803s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0887412\n",
      "\tspeed: 0.1211s/iter; left time: 7743.5506s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0846283\n",
      "\tspeed: 0.1158s/iter; left time: 7395.1846s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0793136\n",
      "\tspeed: 0.1203s/iter; left time: 7668.3205s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0903979\n",
      "\tspeed: 0.1206s/iter; left time: 7674.0616s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0845485\n",
      "\tspeed: 0.1208s/iter; left time: 7674.5317s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0887016\n",
      "\tspeed: 0.1209s/iter; left time: 7668.5008s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0647517\n",
      "\tspeed: 0.1188s/iter; left time: 7525.8907s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0810043\n",
      "\tspeed: 0.1141s/iter; left time: 7215.1798s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0847861\n",
      "\tspeed: 0.1206s/iter; left time: 7615.9642s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0607698\n",
      "\tspeed: 0.1209s/iter; left time: 7620.7176s\n",
      "Epoch: 6 cost time: 00h:08m:53.65s\n",
      "Epoch: 6 | Train Loss: 0.0808758 Vali Loss: 0.0904147 Test Loss: 0.0949829\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0776564\n",
      "\tspeed: 1.5165s/iter; left time: 95306.6113s\n",
      "\titers: 200, epoch: 7 | loss: 0.0784396\n",
      "\tspeed: 0.1208s/iter; left time: 7578.0014s\n",
      "\titers: 300, epoch: 7 | loss: 0.0935769\n",
      "\tspeed: 0.1209s/iter; left time: 7571.0067s\n",
      "\titers: 400, epoch: 7 | loss: 0.1011236\n",
      "\tspeed: 0.1210s/iter; left time: 7569.2014s\n",
      "\titers: 500, epoch: 7 | loss: 0.0993084\n",
      "\tspeed: 0.1199s/iter; left time: 7488.3781s\n",
      "\titers: 600, epoch: 7 | loss: 0.0678284\n",
      "\tspeed: 0.1153s/iter; left time: 7190.7073s\n",
      "\titers: 700, epoch: 7 | loss: 0.0915019\n",
      "\tspeed: 0.1158s/iter; left time: 7210.0226s\n",
      "\titers: 800, epoch: 7 | loss: 0.0908480\n",
      "\tspeed: 0.1212s/iter; left time: 7534.5951s\n",
      "\titers: 900, epoch: 7 | loss: 0.1084777\n",
      "\tspeed: 0.1211s/iter; left time: 7514.2712s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0742308\n",
      "\tspeed: 0.1202s/iter; left time: 7445.9882s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0900566\n",
      "\tspeed: 0.1200s/iter; left time: 7419.1822s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0717520\n",
      "\tspeed: 0.1169s/iter; left time: 7216.7687s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0896325\n",
      "\tspeed: 0.1197s/iter; left time: 7377.1597s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0751720\n",
      "\tspeed: 0.1205s/iter; left time: 7416.6328s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0697805\n",
      "\tspeed: 0.1168s/iter; left time: 7178.5825s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0777683\n",
      "\tspeed: 0.1154s/iter; left time: 7076.2840s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0931582\n",
      "\tspeed: 0.1211s/iter; left time: 7415.2672s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0872888\n",
      "\tspeed: 0.1206s/iter; left time: 7373.5675s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0675946\n",
      "\tspeed: 0.1210s/iter; left time: 7385.9729s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0718096\n",
      "\tspeed: 0.1211s/iter; left time: 7381.1314s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0726633\n",
      "\tspeed: 0.1209s/iter; left time: 7358.4123s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0895870\n",
      "\tspeed: 0.1173s/iter; left time: 7128.2462s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0828907\n",
      "\tspeed: 0.1143s/iter; left time: 6931.1229s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0906809\n",
      "\tspeed: 0.1088s/iter; left time: 6585.8205s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0836006\n",
      "\tspeed: 0.1187s/iter; left time: 7173.4855s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0730073\n",
      "\tspeed: 0.0993s/iter; left time: 5990.1222s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0711934\n",
      "\tspeed: 0.1122s/iter; left time: 6760.5423s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0955372\n",
      "\tspeed: 0.1142s/iter; left time: 6867.4480s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0695751\n",
      "\tspeed: 0.1138s/iter; left time: 6831.3028s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0690845\n",
      "\tspeed: 0.1166s/iter; left time: 6987.5757s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0937775\n",
      "\tspeed: 0.1209s/iter; left time: 7234.3981s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1030593\n",
      "\tspeed: 0.1212s/iter; left time: 7242.7967s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0797456\n",
      "\tspeed: 0.1211s/iter; left time: 7225.2851s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0739272\n",
      "\tspeed: 0.1209s/iter; left time: 7199.8657s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0762674\n",
      "\tspeed: 0.1210s/iter; left time: 7192.6481s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0890733\n",
      "\tspeed: 0.1211s/iter; left time: 7186.6449s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0628304\n",
      "\tspeed: 0.1172s/iter; left time: 6940.8726s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0651186\n",
      "\tspeed: 0.1178s/iter; left time: 6968.0551s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0825545\n",
      "\tspeed: 0.1209s/iter; left time: 7139.1120s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0898772\n",
      "\tspeed: 0.1208s/iter; left time: 7121.8930s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0976483\n",
      "\tspeed: 0.1194s/iter; left time: 7024.7355s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0855372\n",
      "\tspeed: 0.1156s/iter; left time: 6789.4006s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0670111\n",
      "\tspeed: 0.1152s/iter; left time: 6754.4861s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0837409\n",
      "\tspeed: 0.1206s/iter; left time: 7062.9214s\n",
      "Epoch: 7 cost time: 00h:08m:52.25s\n",
      "Epoch: 7 | Train Loss: 0.0799692 Vali Loss: 0.0892061 Test Loss: 0.0939316\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0858032\n",
      "\tspeed: 1.4986s/iter; left time: 87440.8666s\n",
      "\titers: 200, epoch: 8 | loss: 0.0891609\n",
      "\tspeed: 0.1206s/iter; left time: 7025.2453s\n",
      "\titers: 300, epoch: 8 | loss: 0.0738470\n",
      "\tspeed: 0.1218s/iter; left time: 7085.3730s\n",
      "\titers: 400, epoch: 8 | loss: 0.0654960\n",
      "\tspeed: 0.1156s/iter; left time: 6710.4905s\n",
      "\titers: 500, epoch: 8 | loss: 0.0660103\n",
      "\tspeed: 0.1153s/iter; left time: 6682.4981s\n",
      "\titers: 600, epoch: 8 | loss: 0.0857095\n",
      "\tspeed: 0.1220s/iter; left time: 7056.9123s\n",
      "\titers: 700, epoch: 8 | loss: 0.0678255\n",
      "\tspeed: 0.1206s/iter; left time: 6962.1952s\n",
      "\titers: 800, epoch: 8 | loss: 0.0832420\n",
      "\tspeed: 0.1209s/iter; left time: 6970.8135s\n",
      "\titers: 900, epoch: 8 | loss: 0.0736113\n",
      "\tspeed: 0.1211s/iter; left time: 6969.5437s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0566616\n",
      "\tspeed: 0.1201s/iter; left time: 6897.1962s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0793718\n",
      "\tspeed: 0.1152s/iter; left time: 6607.7398s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0917201\n",
      "\tspeed: 0.1205s/iter; left time: 6898.9591s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0691053\n",
      "\tspeed: 0.1183s/iter; left time: 6761.3995s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0720860\n",
      "\tspeed: 0.1199s/iter; left time: 6840.1601s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0792802\n",
      "\tspeed: 0.1200s/iter; left time: 6835.0838s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0826451\n",
      "\tspeed: 0.1216s/iter; left time: 6915.1612s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0810083\n",
      "\tspeed: 0.1210s/iter; left time: 6867.3659s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1009962\n",
      "\tspeed: 0.1204s/iter; left time: 6820.7911s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0958357\n",
      "\tspeed: 0.1183s/iter; left time: 6689.8956s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0718374\n",
      "\tspeed: 0.1189s/iter; left time: 6709.0451s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0927144\n",
      "\tspeed: 0.1200s/iter; left time: 6761.6151s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0949822\n",
      "\tspeed: 0.1210s/iter; left time: 6807.8392s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0678577\n",
      "\tspeed: 0.1204s/iter; left time: 6761.3589s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0812559\n",
      "\tspeed: 0.1168s/iter; left time: 6543.9464s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0822914\n",
      "\tspeed: 0.1195s/iter; left time: 6684.1770s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0643865\n",
      "\tspeed: 0.1171s/iter; left time: 6541.8708s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0984127\n",
      "\tspeed: 0.1209s/iter; left time: 6740.7699s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0899339\n",
      "\tspeed: 0.1208s/iter; left time: 6724.1681s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0574796\n",
      "\tspeed: 0.1213s/iter; left time: 6738.5486s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0782979\n",
      "\tspeed: 0.1184s/iter; left time: 6563.7757s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0836753\n",
      "\tspeed: 0.1175s/iter; left time: 6503.8366s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0640419\n",
      "\tspeed: 0.1172s/iter; left time: 6473.2469s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0771721\n",
      "\tspeed: 0.1215s/iter; left time: 6700.6903s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0778379\n",
      "\tspeed: 0.1222s/iter; left time: 6727.0175s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0805037\n",
      "\tspeed: 0.1208s/iter; left time: 6635.9473s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0769201\n",
      "\tspeed: 0.1209s/iter; left time: 6632.4699s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0943394\n",
      "\tspeed: 0.1207s/iter; left time: 6608.6204s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0953691\n",
      "\tspeed: 0.1208s/iter; left time: 6600.3599s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0673332\n",
      "\tspeed: 0.1209s/iter; left time: 6596.0071s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0977372\n",
      "\tspeed: 0.1215s/iter; left time: 6616.9175s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0735301\n",
      "\tspeed: 0.1205s/iter; left time: 6550.7294s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0813443\n",
      "\tspeed: 0.1207s/iter; left time: 6548.4466s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0507200\n",
      "\tspeed: 0.1207s/iter; left time: 6534.2047s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0746277\n",
      "\tspeed: 0.1213s/iter; left time: 6557.3912s\n",
      "Epoch: 8 cost time: 00h:08m:59.78s\n",
      "Epoch: 8 | Train Loss: 0.0791088 Vali Loss: 0.0891922 Test Loss: 0.0933998\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0883823\n",
      "\tspeed: 1.4926s/iter; left time: 80382.3245s\n",
      "\titers: 200, epoch: 9 | loss: 0.0742780\n",
      "\tspeed: 0.1143s/iter; left time: 6142.0410s\n",
      "\titers: 300, epoch: 9 | loss: 0.0729224\n",
      "\tspeed: 0.1201s/iter; left time: 6441.2238s\n",
      "\titers: 400, epoch: 9 | loss: 0.0771885\n",
      "\tspeed: 0.1207s/iter; left time: 6463.8236s\n",
      "\titers: 500, epoch: 9 | loss: 0.0633648\n",
      "\tspeed: 0.1197s/iter; left time: 6400.8904s\n",
      "\titers: 600, epoch: 9 | loss: 0.0877349\n",
      "\tspeed: 0.1186s/iter; left time: 6325.5573s\n",
      "\titers: 700, epoch: 9 | loss: 0.0688003\n",
      "\tspeed: 0.1209s/iter; left time: 6437.5326s\n",
      "\titers: 800, epoch: 9 | loss: 0.0829574\n",
      "\tspeed: 0.1209s/iter; left time: 6426.9728s\n",
      "\titers: 900, epoch: 9 | loss: 0.0724092\n",
      "\tspeed: 0.1204s/iter; left time: 6387.5727s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0991572\n",
      "\tspeed: 0.1208s/iter; left time: 6396.3747s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0887690\n",
      "\tspeed: 0.1202s/iter; left time: 6351.6722s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0728898\n",
      "\tspeed: 0.1207s/iter; left time: 6365.8803s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0847884\n",
      "\tspeed: 0.1209s/iter; left time: 6367.3844s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0939002\n",
      "\tspeed: 0.1167s/iter; left time: 6131.6376s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0808990\n",
      "\tspeed: 0.1090s/iter; left time: 5716.1509s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0823213\n",
      "\tspeed: 0.1135s/iter; left time: 5939.6949s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0796168\n",
      "\tspeed: 0.1195s/iter; left time: 6245.6759s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0721289\n",
      "\tspeed: 0.1208s/iter; left time: 6300.5310s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0679318\n",
      "\tspeed: 0.1194s/iter; left time: 6216.6185s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0717794\n",
      "\tspeed: 0.1184s/iter; left time: 6152.9969s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0890689\n",
      "\tspeed: 0.1140s/iter; left time: 5910.6588s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0657449\n",
      "\tspeed: 0.1195s/iter; left time: 6186.4854s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0594654\n",
      "\tspeed: 0.1209s/iter; left time: 6245.2868s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0709143\n",
      "\tspeed: 0.1209s/iter; left time: 6233.7558s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0705610\n",
      "\tspeed: 0.1191s/iter; left time: 6130.5124s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0897841\n",
      "\tspeed: 0.1159s/iter; left time: 5952.4614s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0816959\n",
      "\tspeed: 0.1198s/iter; left time: 6140.9849s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0750316\n",
      "\tspeed: 0.1207s/iter; left time: 6176.0496s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0815326\n",
      "\tspeed: 0.1208s/iter; left time: 6169.1837s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0938492\n",
      "\tspeed: 0.1186s/iter; left time: 6043.0369s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0800535\n",
      "\tspeed: 0.1145s/iter; left time: 5825.0162s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0835484\n",
      "\tspeed: 0.1131s/iter; left time: 5737.9751s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0548027\n",
      "\tspeed: 0.1160s/iter; left time: 5875.0380s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0590655\n",
      "\tspeed: 0.1189s/iter; left time: 6010.3775s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0721123\n",
      "\tspeed: 0.1180s/iter; left time: 5954.1996s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0877456\n",
      "\tspeed: 0.1141s/iter; left time: 5744.7532s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0746561\n",
      "\tspeed: 0.1183s/iter; left time: 5943.7809s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0877061\n",
      "\tspeed: 0.1192s/iter; left time: 5979.5753s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0804281\n",
      "\tspeed: 0.1201s/iter; left time: 6011.7280s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0666070\n",
      "\tspeed: 0.1166s/iter; left time: 5823.0049s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0842516\n",
      "\tspeed: 0.1181s/iter; left time: 5886.4829s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0634511\n",
      "\tspeed: 0.1156s/iter; left time: 5751.5241s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0862772\n",
      "\tspeed: 0.1136s/iter; left time: 5639.1242s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0666628\n",
      "\tspeed: 0.1130s/iter; left time: 5600.0875s\n",
      "Epoch: 9 cost time: 00h:08m:51.02s\n",
      "Epoch: 9 | Train Loss: 0.0782179 Vali Loss: 0.0899105 Test Loss: 0.0940169\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02172875590622425, rmse:0.14740677177906036, mae:0.0919589027762413, rse:0.5206128358840942\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 24: 01h:40m:54.77s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 143525\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-02 05:46:36,884] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 05:46:38,092] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 05:46:38,092] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 05:46:38,093] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 05:46:38,196] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 05:46:38,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 05:46:38,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 05:46:38,864] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 05:46:38,864] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 05:46:38,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 05:46:38,865] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 05:46:38,865] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 05:46:38,866] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 05:46:39,195] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 05:46:39,196] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,197] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,323] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 05:46:39,324] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,324] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,324] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 05:46:39,441] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 05:46:39,443] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 05:46:39,443] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.27 GB, percent = 9.8%\n",
      "[2024-11-02 05:46:39,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 05:46:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 05:46:39,444] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9778b9ae50>\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 05:46:39,445] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 05:46:39,446] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 05:46:39,447] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1796905\n",
      "\tspeed: 0.1577s/iter; left time: 14133.7593s\n",
      "\titers: 200, epoch: 1 | loss: 0.1528330\n",
      "\tspeed: 0.1278s/iter; left time: 11439.1865s\n",
      "\titers: 300, epoch: 1 | loss: 0.1636411\n",
      "\tspeed: 0.1276s/iter; left time: 11403.7132s\n",
      "\titers: 400, epoch: 1 | loss: 0.1717221\n",
      "\tspeed: 0.1243s/iter; left time: 11104.3602s\n",
      "\titers: 500, epoch: 1 | loss: 0.1625754\n",
      "\tspeed: 0.1295s/iter; left time: 11553.6505s\n",
      "\titers: 600, epoch: 1 | loss: 0.1756469\n",
      "\tspeed: 0.1281s/iter; left time: 11417.1809s\n",
      "\titers: 700, epoch: 1 | loss: 0.1374525\n",
      "\tspeed: 0.1274s/iter; left time: 11335.4492s\n",
      "\titers: 800, epoch: 1 | loss: 0.1207612\n",
      "\tspeed: 0.1274s/iter; left time: 11324.7311s\n",
      "\titers: 900, epoch: 1 | loss: 0.1471106\n",
      "\tspeed: 0.1264s/iter; left time: 11227.5124s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1346920\n",
      "\tspeed: 0.1189s/iter; left time: 10544.4150s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1261072\n",
      "\tspeed: 0.1074s/iter; left time: 9516.7964s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1214257\n",
      "\tspeed: 0.1180s/iter; left time: 10447.1125s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1278190\n",
      "\tspeed: 0.1280s/iter; left time: 11318.8812s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1183581\n",
      "\tspeed: 0.1278s/iter; left time: 11283.1554s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1059927\n",
      "\tspeed: 0.1275s/iter; left time: 11248.9772s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1225744\n",
      "\tspeed: 0.1286s/iter; left time: 11326.9879s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1184389\n",
      "\tspeed: 0.1284s/iter; left time: 11298.2002s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1273916\n",
      "\tspeed: 0.1280s/iter; left time: 11254.2115s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1305389\n",
      "\tspeed: 0.1272s/iter; left time: 11166.4987s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1092692\n",
      "\tspeed: 0.1274s/iter; left time: 11172.1297s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1127294\n",
      "\tspeed: 0.1260s/iter; left time: 11039.1152s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1192679\n",
      "\tspeed: 0.1277s/iter; left time: 11177.4816s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1197385\n",
      "\tspeed: 0.1074s/iter; left time: 9389.1119s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1230457\n",
      "\tspeed: 0.1143s/iter; left time: 9981.0137s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1455796\n",
      "\tspeed: 0.1242s/iter; left time: 10829.2640s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1400923\n",
      "\tspeed: 0.1104s/iter; left time: 9615.3961s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1138726\n",
      "\tspeed: 0.1156s/iter; left time: 10056.6128s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1174629\n",
      "\tspeed: 0.1278s/iter; left time: 11106.0899s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1184106\n",
      "\tspeed: 0.1274s/iter; left time: 11058.7338s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1312887\n",
      "\tspeed: 0.1269s/iter; left time: 11005.5283s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1324136\n",
      "\tspeed: 0.1278s/iter; left time: 11071.3846s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1211721\n",
      "\tspeed: 0.1211s/iter; left time: 10473.4759s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1178138\n",
      "\tspeed: 0.1249s/iter; left time: 10794.9082s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1224443\n",
      "\tspeed: 0.1261s/iter; left time: 10881.3066s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1183141\n",
      "\tspeed: 0.1274s/iter; left time: 10980.9751s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1242890\n",
      "\tspeed: 0.1273s/iter; left time: 10958.4074s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1144471\n",
      "\tspeed: 0.1281s/iter; left time: 11012.8003s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1015655\n",
      "\tspeed: 0.1274s/iter; left time: 10939.5922s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1111848\n",
      "\tspeed: 0.1269s/iter; left time: 10891.3807s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1316575\n",
      "\tspeed: 0.1284s/iter; left time: 11005.7130s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1088002\n",
      "\tspeed: 0.1266s/iter; left time: 10841.2923s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1164419\n",
      "\tspeed: 0.1271s/iter; left time: 10870.0765s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1259046\n",
      "\tspeed: 0.1272s/iter; left time: 10862.7851s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1085604\n",
      "\tspeed: 0.1257s/iter; left time: 10722.8313s\n",
      "Epoch: 1 cost time: 00h:09m:20.10s\n",
      "Epoch: 1 | Train Loss: 0.1268652 Vali Loss: 0.1215782 Test Loss: 0.1305961\n",
      "Validation loss decreased (inf --> 0.121578).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0937665\n",
      "\tspeed: 1.7150s/iter; left time: 145973.5313s\n",
      "\titers: 200, epoch: 2 | loss: 0.1341150\n",
      "\tspeed: 0.1166s/iter; left time: 9911.0785s\n",
      "\titers: 300, epoch: 2 | loss: 0.1132426\n",
      "\tspeed: 0.1154s/iter; left time: 9796.2151s\n",
      "\titers: 400, epoch: 2 | loss: 0.1143301\n",
      "\tspeed: 0.1156s/iter; left time: 9801.1368s\n",
      "\titers: 500, epoch: 2 | loss: 0.0936279\n",
      "\tspeed: 0.1152s/iter; left time: 9760.1670s\n",
      "\titers: 600, epoch: 2 | loss: 0.1210652\n",
      "\tspeed: 0.1165s/iter; left time: 9855.5831s\n",
      "\titers: 700, epoch: 2 | loss: 0.1088079\n",
      "\tspeed: 0.1156s/iter; left time: 9767.0333s\n",
      "\titers: 800, epoch: 2 | loss: 0.1030090\n",
      "\tspeed: 0.1160s/iter; left time: 9795.4755s\n",
      "\titers: 900, epoch: 2 | loss: 0.1108489\n",
      "\tspeed: 0.1152s/iter; left time: 9711.7447s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1083097\n",
      "\tspeed: 0.1155s/iter; left time: 9728.4191s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1051919\n",
      "\tspeed: 0.1161s/iter; left time: 9761.8822s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1058292\n",
      "\tspeed: 0.1166s/iter; left time: 9794.1186s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1199586\n",
      "\tspeed: 0.1125s/iter; left time: 9438.4597s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1058512\n",
      "\tspeed: 0.1161s/iter; left time: 9733.0043s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1033324\n",
      "\tspeed: 0.1153s/iter; left time: 9651.2989s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1068781\n",
      "\tspeed: 0.1155s/iter; left time: 9653.7709s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1302516\n",
      "\tspeed: 0.1162s/iter; left time: 9703.8041s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0881851\n",
      "\tspeed: 0.1156s/iter; left time: 9644.9497s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1368857\n",
      "\tspeed: 0.1155s/iter; left time: 9624.0595s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1057539\n",
      "\tspeed: 0.1158s/iter; left time: 9637.0728s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1266332\n",
      "\tspeed: 0.1150s/iter; left time: 9559.3286s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1175085\n",
      "\tspeed: 0.1074s/iter; left time: 8920.0190s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0942677\n",
      "\tspeed: 0.1158s/iter; left time: 9605.4521s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1120734\n",
      "\tspeed: 0.1093s/iter; left time: 9050.4594s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1228208\n",
      "\tspeed: 0.1142s/iter; left time: 9444.7890s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1139171\n",
      "\tspeed: 0.1172s/iter; left time: 9685.9896s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1344807\n",
      "\tspeed: 0.1079s/iter; left time: 8904.6353s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0970470\n",
      "\tspeed: 0.1166s/iter; left time: 9607.0546s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1221787\n",
      "\tspeed: 0.1143s/iter; left time: 9411.3897s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1317635\n",
      "\tspeed: 0.1162s/iter; left time: 9555.0655s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1116549\n",
      "\tspeed: 0.1154s/iter; left time: 9472.8397s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1177123\n",
      "\tspeed: 0.1151s/iter; left time: 9437.1712s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1062045\n",
      "\tspeed: 0.1152s/iter; left time: 9438.3001s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1368778\n",
      "\tspeed: 0.1154s/iter; left time: 9442.2239s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1050819\n",
      "\tspeed: 0.1156s/iter; left time: 9450.0167s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0943935\n",
      "\tspeed: 0.1152s/iter; left time: 9398.9143s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1096257\n",
      "\tspeed: 0.1152s/iter; left time: 9387.5670s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1289937\n",
      "\tspeed: 0.1157s/iter; left time: 9420.3089s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1044328\n",
      "\tspeed: 0.1159s/iter; left time: 9425.1984s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1009454\n",
      "\tspeed: 0.1122s/iter; left time: 9109.3106s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1349109\n",
      "\tspeed: 0.1134s/iter; left time: 9199.0742s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1130775\n",
      "\tspeed: 0.1154s/iter; left time: 9346.9434s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0991431\n",
      "\tspeed: 0.0968s/iter; left time: 7830.6714s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1101747\n",
      "\tspeed: 0.0967s/iter; left time: 7816.9526s\n",
      "Epoch: 2 cost time: 00h:08m:31.10s\n",
      "Epoch: 2 | Train Loss: 0.1110525 Vali Loss: 0.1208038 Test Loss: 0.1302489\n",
      "Validation loss decreased (0.121578 --> 0.120804).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1169660\n",
      "\tspeed: 1.4572s/iter; left time: 117493.7390s\n",
      "\titers: 200, epoch: 3 | loss: 0.1078827\n",
      "\tspeed: 0.1170s/iter; left time: 9421.6242s\n",
      "\titers: 300, epoch: 3 | loss: 0.1039631\n",
      "\tspeed: 0.1156s/iter; left time: 9301.5319s\n",
      "\titers: 400, epoch: 3 | loss: 0.1180947\n",
      "\tspeed: 0.1156s/iter; left time: 9285.7093s\n",
      "\titers: 500, epoch: 3 | loss: 0.1180327\n",
      "\tspeed: 0.1161s/iter; left time: 9313.3744s\n",
      "\titers: 600, epoch: 3 | loss: 0.0924053\n",
      "\tspeed: 0.1146s/iter; left time: 9185.8182s\n",
      "\titers: 700, epoch: 3 | loss: 0.1207208\n",
      "\tspeed: 0.1144s/iter; left time: 9155.2845s\n",
      "\titers: 800, epoch: 3 | loss: 0.1209414\n",
      "\tspeed: 0.1149s/iter; left time: 9184.5775s\n",
      "\titers: 900, epoch: 3 | loss: 0.1271856\n",
      "\tspeed: 0.1151s/iter; left time: 9192.0041s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1029982\n",
      "\tspeed: 0.1169s/iter; left time: 9321.4557s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1220193\n",
      "\tspeed: 0.1147s/iter; left time: 9136.6739s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0876944\n",
      "\tspeed: 0.1166s/iter; left time: 9272.6736s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0800447\n",
      "\tspeed: 0.1177s/iter; left time: 9346.4810s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1022078\n",
      "\tspeed: 0.1156s/iter; left time: 9167.7192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0931360\n",
      "\tspeed: 0.1172s/iter; left time: 9286.0300s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0966400\n",
      "\tspeed: 0.1152s/iter; left time: 9119.7205s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1089689\n",
      "\tspeed: 0.1145s/iter; left time: 9049.5219s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1262760\n",
      "\tspeed: 0.1156s/iter; left time: 9123.7408s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1214038\n",
      "\tspeed: 0.1169s/iter; left time: 9216.6460s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1131916\n",
      "\tspeed: 0.1180s/iter; left time: 9291.4135s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0910320\n",
      "\tspeed: 0.1178s/iter; left time: 9262.6729s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1071264\n",
      "\tspeed: 0.1164s/iter; left time: 9138.9622s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1228675\n",
      "\tspeed: 0.1169s/iter; left time: 9169.1644s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1094589\n",
      "\tspeed: 0.1165s/iter; left time: 9124.5763s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0970292\n",
      "\tspeed: 0.1196s/iter; left time: 9357.8439s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1208833\n",
      "\tspeed: 0.1177s/iter; left time: 9196.2377s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1006875\n",
      "\tspeed: 0.1178s/iter; left time: 9192.0300s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0891459\n",
      "\tspeed: 0.1157s/iter; left time: 9016.8123s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1074533\n",
      "\tspeed: 0.1165s/iter; left time: 9070.8716s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1291854\n",
      "\tspeed: 0.1162s/iter; left time: 9029.8660s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1223493\n",
      "\tspeed: 0.1171s/iter; left time: 9092.2994s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1235415\n",
      "\tspeed: 0.1169s/iter; left time: 9061.9804s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0978686\n",
      "\tspeed: 0.1164s/iter; left time: 9014.0719s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1255539\n",
      "\tspeed: 0.1168s/iter; left time: 9030.1651s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0985570\n",
      "\tspeed: 0.1172s/iter; left time: 9052.8040s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0875186\n",
      "\tspeed: 0.1137s/iter; left time: 8770.0572s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0979343\n",
      "\tspeed: 0.1147s/iter; left time: 8833.0443s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1091715\n",
      "\tspeed: 0.1158s/iter; left time: 8909.1244s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1004776\n",
      "\tspeed: 0.1161s/iter; left time: 8921.2142s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0880002\n",
      "\tspeed: 0.1155s/iter; left time: 8859.3549s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1066579\n",
      "\tspeed: 0.1155s/iter; left time: 8847.4811s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1162613\n",
      "\tspeed: 0.1103s/iter; left time: 8442.6629s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1064103\n",
      "\tspeed: 0.1093s/iter; left time: 8355.1387s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1064245\n",
      "\tspeed: 0.1126s/iter; left time: 8592.9947s\n",
      "Epoch: 3 cost time: 00h:08m:40.34s\n",
      "Epoch: 3 | Train Loss: 0.1070935 Vali Loss: 0.1214075 Test Loss: 0.1343616\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0998068\n",
      "\tspeed: 1.4499s/iter; left time: 110404.1637s\n",
      "\titers: 200, epoch: 4 | loss: 0.0908893\n",
      "\tspeed: 0.1157s/iter; left time: 8798.1010s\n",
      "\titers: 300, epoch: 4 | loss: 0.1107343\n",
      "\tspeed: 0.1172s/iter; left time: 8901.5089s\n",
      "\titers: 400, epoch: 4 | loss: 0.1227403\n",
      "\tspeed: 0.1148s/iter; left time: 8707.7615s\n",
      "\titers: 500, epoch: 4 | loss: 0.1063603\n",
      "\tspeed: 0.1164s/iter; left time: 8817.2421s\n",
      "\titers: 600, epoch: 4 | loss: 0.1203433\n",
      "\tspeed: 0.1154s/iter; left time: 8731.4206s\n",
      "\titers: 700, epoch: 4 | loss: 0.1365832\n",
      "\tspeed: 0.1154s/iter; left time: 8720.3389s\n",
      "\titers: 800, epoch: 4 | loss: 0.0913843\n",
      "\tspeed: 0.1152s/iter; left time: 8693.4082s\n",
      "\titers: 900, epoch: 4 | loss: 0.1054852\n",
      "\tspeed: 0.1156s/iter; left time: 8711.6342s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0973554\n",
      "\tspeed: 0.1146s/iter; left time: 8626.5125s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0843453\n",
      "\tspeed: 0.1155s/iter; left time: 8677.2394s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0939158\n",
      "\tspeed: 0.1066s/iter; left time: 8001.1407s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0991926\n",
      "\tspeed: 0.1078s/iter; left time: 8079.6909s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0926170\n",
      "\tspeed: 0.1164s/iter; left time: 8710.1360s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0980558\n",
      "\tspeed: 0.1150s/iter; left time: 8592.9991s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1088924\n",
      "\tspeed: 0.1165s/iter; left time: 8692.9161s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1093120\n",
      "\tspeed: 0.1152s/iter; left time: 8586.7060s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1005844\n",
      "\tspeed: 0.1150s/iter; left time: 8557.7215s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1107281\n",
      "\tspeed: 0.1146s/iter; left time: 8519.5482s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1060532\n",
      "\tspeed: 0.1163s/iter; left time: 8637.1994s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0904001\n",
      "\tspeed: 0.1151s/iter; left time: 8531.3305s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1109696\n",
      "\tspeed: 0.1162s/iter; left time: 8605.2239s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1122366\n",
      "\tspeed: 0.0992s/iter; left time: 7336.7084s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0989136\n",
      "\tspeed: 0.1142s/iter; left time: 8431.5912s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0947601\n",
      "\tspeed: 0.1094s/iter; left time: 8071.1090s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1021152\n",
      "\tspeed: 0.1161s/iter; left time: 8547.9752s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1003583\n",
      "\tspeed: 0.1147s/iter; left time: 8435.3343s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1050423\n",
      "\tspeed: 0.1150s/iter; left time: 8446.1468s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0937110\n",
      "\tspeed: 0.1147s/iter; left time: 8410.0946s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1102281\n",
      "\tspeed: 0.1137s/iter; left time: 8324.9367s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1013796\n",
      "\tspeed: 0.1164s/iter; left time: 8513.4038s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0967339\n",
      "\tspeed: 0.1167s/iter; left time: 8526.7652s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1165496\n",
      "\tspeed: 0.1159s/iter; left time: 8452.3091s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1168198\n",
      "\tspeed: 0.1157s/iter; left time: 8426.7270s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0818577\n",
      "\tspeed: 0.1156s/iter; left time: 8406.8334s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0995615\n",
      "\tspeed: 0.1147s/iter; left time: 8332.7640s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1094128\n",
      "\tspeed: 0.1161s/iter; left time: 8419.6498s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0862050\n",
      "\tspeed: 0.1141s/iter; left time: 8262.7599s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0992920\n",
      "\tspeed: 0.1144s/iter; left time: 8276.9377s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1077046\n",
      "\tspeed: 0.1143s/iter; left time: 8259.7362s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0952680\n",
      "\tspeed: 0.1147s/iter; left time: 8273.2550s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1204872\n",
      "\tspeed: 0.1153s/iter; left time: 8305.8857s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1002148\n",
      "\tspeed: 0.1159s/iter; left time: 8335.4637s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0981565\n",
      "\tspeed: 0.1051s/iter; left time: 7553.5532s\n",
      "Epoch: 4 cost time: 00h:08m:32.22s\n",
      "Epoch: 4 | Train Loss: 0.1034841 Vali Loss: 0.1205657 Test Loss: 0.1323852\n",
      "Validation loss decreased (0.120804 --> 0.120566).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1074458\n",
      "\tspeed: 1.4627s/iter; left time: 104821.1352s\n",
      "\titers: 200, epoch: 5 | loss: 0.1058937\n",
      "\tspeed: 0.1138s/iter; left time: 8143.2592s\n",
      "\titers: 300, epoch: 5 | loss: 0.1061051\n",
      "\tspeed: 0.1151s/iter; left time: 8225.7194s\n",
      "\titers: 400, epoch: 5 | loss: 0.1060566\n",
      "\tspeed: 0.1141s/iter; left time: 8142.5112s\n",
      "\titers: 500, epoch: 5 | loss: 0.1082695\n",
      "\tspeed: 0.1147s/iter; left time: 8172.9705s\n",
      "\titers: 600, epoch: 5 | loss: 0.0998992\n",
      "\tspeed: 0.1160s/iter; left time: 8253.3638s\n",
      "\titers: 700, epoch: 5 | loss: 0.0936923\n",
      "\tspeed: 0.1155s/iter; left time: 8203.9940s\n",
      "\titers: 800, epoch: 5 | loss: 0.0949209\n",
      "\tspeed: 0.1158s/iter; left time: 8217.0675s\n",
      "\titers: 900, epoch: 5 | loss: 0.1189355\n",
      "\tspeed: 0.1003s/iter; left time: 7110.8602s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1086110\n",
      "\tspeed: 0.1004s/iter; left time: 7103.0959s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1019219\n",
      "\tspeed: 0.1086s/iter; left time: 7671.5343s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0848749\n",
      "\tspeed: 0.1152s/iter; left time: 8130.4813s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0860293\n",
      "\tspeed: 0.1161s/iter; left time: 8180.9625s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0869483\n",
      "\tspeed: 0.1174s/iter; left time: 8260.7919s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0946057\n",
      "\tspeed: 0.1175s/iter; left time: 8256.7838s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0962429\n",
      "\tspeed: 0.1163s/iter; left time: 8156.7721s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0948651\n",
      "\tspeed: 0.1148s/iter; left time: 8041.5830s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0928815\n",
      "\tspeed: 0.1178s/iter; left time: 8239.3558s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1203045\n",
      "\tspeed: 0.1154s/iter; left time: 8060.0481s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0950496\n",
      "\tspeed: 0.1155s/iter; left time: 8055.3206s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1085438\n",
      "\tspeed: 0.1163s/iter; left time: 8099.8594s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0956611\n",
      "\tspeed: 0.1155s/iter; left time: 8034.8774s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1001744\n",
      "\tspeed: 0.1175s/iter; left time: 8164.6710s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0964360\n",
      "\tspeed: 0.1170s/iter; left time: 8115.6164s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0808083\n",
      "\tspeed: 0.1161s/iter; left time: 8039.4640s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1084981\n",
      "\tspeed: 0.1153s/iter; left time: 7970.9301s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0833636\n",
      "\tspeed: 0.1156s/iter; left time: 7985.1967s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0960330\n",
      "\tspeed: 0.1157s/iter; left time: 7978.9113s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1077119\n",
      "\tspeed: 0.1151s/iter; left time: 7924.2324s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1022597\n",
      "\tspeed: 0.1148s/iter; left time: 7892.3061s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0829771\n",
      "\tspeed: 0.1138s/iter; left time: 7811.7347s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1020444\n",
      "\tspeed: 0.1139s/iter; left time: 7812.5222s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0871775\n",
      "\tspeed: 0.1146s/iter; left time: 7844.7267s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0848032\n",
      "\tspeed: 0.1055s/iter; left time: 7215.2609s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1116961\n",
      "\tspeed: 0.1134s/iter; left time: 7740.9983s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0996750\n",
      "\tspeed: 0.1150s/iter; left time: 7837.7535s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0903397\n",
      "\tspeed: 0.1167s/iter; left time: 7940.6002s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0819974\n",
      "\tspeed: 0.1170s/iter; left time: 7948.9063s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1081164\n",
      "\tspeed: 0.1148s/iter; left time: 7789.3585s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0874876\n",
      "\tspeed: 0.1128s/iter; left time: 7643.4199s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1060955\n",
      "\tspeed: 0.1146s/iter; left time: 7752.6454s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1013475\n",
      "\tspeed: 0.1142s/iter; left time: 7712.8244s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0901750\n",
      "\tspeed: 0.1146s/iter; left time: 7733.9478s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0789233\n",
      "\tspeed: 0.1155s/iter; left time: 7777.1008s\n",
      "Epoch: 5 cost time: 00h:08m:33.46s\n",
      "Epoch: 5 | Train Loss: 0.1003134 Vali Loss: 0.1239620 Test Loss: 0.1393968\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1080867\n",
      "\tspeed: 1.4367s/iter; left time: 96512.7582s\n",
      "\titers: 200, epoch: 6 | loss: 0.0835378\n",
      "\tspeed: 0.0964s/iter; left time: 6466.7118s\n",
      "\titers: 300, epoch: 6 | loss: 0.0942729\n",
      "\tspeed: 0.1157s/iter; left time: 7749.9778s\n",
      "\titers: 400, epoch: 6 | loss: 0.0856797\n",
      "\tspeed: 0.1176s/iter; left time: 7867.5322s\n",
      "\titers: 500, epoch: 6 | loss: 0.1078456\n",
      "\tspeed: 0.1151s/iter; left time: 7684.6486s\n",
      "\titers: 600, epoch: 6 | loss: 0.0982793\n",
      "\tspeed: 0.1155s/iter; left time: 7701.8178s\n",
      "\titers: 700, epoch: 6 | loss: 0.0959515\n",
      "\tspeed: 0.1153s/iter; left time: 7677.6807s\n",
      "\titers: 800, epoch: 6 | loss: 0.0882172\n",
      "\tspeed: 0.1154s/iter; left time: 7670.4021s\n",
      "\titers: 900, epoch: 6 | loss: 0.1032411\n",
      "\tspeed: 0.1157s/iter; left time: 7677.2243s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0916246\n",
      "\tspeed: 0.1144s/iter; left time: 7583.0094s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1052714\n",
      "\tspeed: 0.1147s/iter; left time: 7589.3536s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1026374\n",
      "\tspeed: 0.1131s/iter; left time: 7472.7404s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0844505\n",
      "\tspeed: 0.1157s/iter; left time: 7633.2029s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1079274\n",
      "\tspeed: 0.1151s/iter; left time: 7579.2040s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1132590\n",
      "\tspeed: 0.1150s/iter; left time: 7561.5080s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0864766\n",
      "\tspeed: 0.1174s/iter; left time: 7710.4596s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0859761\n",
      "\tspeed: 0.1161s/iter; left time: 7614.6673s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0804487\n",
      "\tspeed: 0.1173s/iter; left time: 7680.2563s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1012686\n",
      "\tspeed: 0.1132s/iter; left time: 7403.0954s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1165255\n",
      "\tspeed: 0.1149s/iter; left time: 7497.9783s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1132169\n",
      "\tspeed: 0.1162s/iter; left time: 7570.7355s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0798103\n",
      "\tspeed: 0.1155s/iter; left time: 7513.9598s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0966341\n",
      "\tspeed: 0.1159s/iter; left time: 7528.6904s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1069327\n",
      "\tspeed: 0.1148s/iter; left time: 7450.4132s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0875472\n",
      "\tspeed: 0.1152s/iter; left time: 7460.0947s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1107147\n",
      "\tspeed: 0.1157s/iter; left time: 7482.2903s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1130749\n",
      "\tspeed: 0.1159s/iter; left time: 7484.2634s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0955565\n",
      "\tspeed: 0.1170s/iter; left time: 7542.6318s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1047895\n",
      "\tspeed: 0.1165s/iter; left time: 7498.4011s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1028518\n",
      "\tspeed: 0.1150s/iter; left time: 7393.8324s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0772144\n",
      "\tspeed: 0.1179s/iter; left time: 7566.3026s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1066739\n",
      "\tspeed: 0.1046s/iter; left time: 6704.5906s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1067312\n",
      "\tspeed: 0.1149s/iter; left time: 7351.1931s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0982327\n",
      "\tspeed: 0.1157s/iter; left time: 7387.9009s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1066106\n",
      "\tspeed: 0.1128s/iter; left time: 7196.3515s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1092574\n",
      "\tspeed: 0.1153s/iter; left time: 7343.8735s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1125807\n",
      "\tspeed: 0.1151s/iter; left time: 7319.3994s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0988214\n",
      "\tspeed: 0.1149s/iter; left time: 7293.2887s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0866890\n",
      "\tspeed: 0.1149s/iter; left time: 7283.4518s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0922776\n",
      "\tspeed: 0.1156s/iter; left time: 7315.6730s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1005166\n",
      "\tspeed: 0.1143s/iter; left time: 7220.2949s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0808731\n",
      "\tspeed: 0.1142s/iter; left time: 7205.7780s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0855673\n",
      "\tspeed: 0.1162s/iter; left time: 7315.5184s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1201236\n",
      "\tspeed: 0.1161s/iter; left time: 7298.1147s\n",
      "Epoch: 6 cost time: 00h:08m:34.03s\n",
      "Epoch: 6 | Train Loss: 0.0972888 Vali Loss: 0.1255503 Test Loss: 0.1417521\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0924498\n",
      "\tspeed: 1.4499s/iter; left time: 90895.6183s\n",
      "\titers: 200, epoch: 7 | loss: 0.1178812\n",
      "\tspeed: 0.1154s/iter; left time: 7223.4450s\n",
      "\titers: 300, epoch: 7 | loss: 0.0915430\n",
      "\tspeed: 0.1141s/iter; left time: 7127.9345s\n",
      "\titers: 400, epoch: 7 | loss: 0.0889588\n",
      "\tspeed: 0.1117s/iter; left time: 6967.4425s\n",
      "\titers: 500, epoch: 7 | loss: 0.0823657\n",
      "\tspeed: 0.1092s/iter; left time: 6805.2207s\n",
      "\titers: 600, epoch: 7 | loss: 0.1029791\n",
      "\tspeed: 0.1162s/iter; left time: 7227.7682s\n",
      "\titers: 700, epoch: 7 | loss: 0.0904151\n",
      "\tspeed: 0.1155s/iter; left time: 7168.4667s\n",
      "\titers: 800, epoch: 7 | loss: 0.1373906\n",
      "\tspeed: 0.1147s/iter; left time: 7108.8489s\n",
      "\titers: 900, epoch: 7 | loss: 0.0987064\n",
      "\tspeed: 0.1150s/iter; left time: 7116.0598s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0955451\n",
      "\tspeed: 0.1145s/iter; left time: 7072.5929s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0812314\n",
      "\tspeed: 0.1156s/iter; left time: 7133.7479s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1071944\n",
      "\tspeed: 0.1165s/iter; left time: 7177.3804s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1112599\n",
      "\tspeed: 0.1133s/iter; left time: 6967.3636s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0859119\n",
      "\tspeed: 0.0965s/iter; left time: 5924.9031s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0963456\n",
      "\tspeed: 0.0979s/iter; left time: 6003.2388s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0927925\n",
      "\tspeed: 0.1155s/iter; left time: 7066.2155s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1041150\n",
      "\tspeed: 0.1158s/iter; left time: 7075.9964s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0919541\n",
      "\tspeed: 0.1151s/iter; left time: 7021.1483s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0924811\n",
      "\tspeed: 0.0964s/iter; left time: 5872.0157s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0916734\n",
      "\tspeed: 0.1116s/iter; left time: 6783.0743s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0851152\n",
      "\tspeed: 0.1072s/iter; left time: 6504.1584s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0713027\n",
      "\tspeed: 0.1148s/iter; left time: 6956.5648s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0756128\n",
      "\tspeed: 0.1156s/iter; left time: 6990.4095s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0799808\n",
      "\tspeed: 0.1154s/iter; left time: 6968.6508s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0849434\n",
      "\tspeed: 0.1138s/iter; left time: 6863.1466s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0794628\n",
      "\tspeed: 0.1151s/iter; left time: 6926.4028s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0931070\n",
      "\tspeed: 0.1150s/iter; left time: 6909.9753s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1051028\n",
      "\tspeed: 0.1161s/iter; left time: 6965.8407s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1156529\n",
      "\tspeed: 0.1153s/iter; left time: 6907.9654s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0889909\n",
      "\tspeed: 0.1139s/iter; left time: 6812.9981s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0863874\n",
      "\tspeed: 0.1157s/iter; left time: 6908.3129s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0882615\n",
      "\tspeed: 0.1150s/iter; left time: 6855.6170s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0995574\n",
      "\tspeed: 0.1155s/iter; left time: 6869.2117s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0981942\n",
      "\tspeed: 0.1167s/iter; left time: 6928.9357s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1015216\n",
      "\tspeed: 0.1157s/iter; left time: 6858.1393s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0947982\n",
      "\tspeed: 0.1156s/iter; left time: 6841.4885s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1041567\n",
      "\tspeed: 0.1163s/iter; left time: 6872.8146s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0952044\n",
      "\tspeed: 0.1136s/iter; left time: 6698.5930s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0995766\n",
      "\tspeed: 0.1144s/iter; left time: 6734.4282s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0990296\n",
      "\tspeed: 0.1152s/iter; left time: 6772.2692s\n",
      "\titers: 4100, epoch: 7 | loss: 0.1016947\n",
      "\tspeed: 0.1158s/iter; left time: 6794.1004s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1040053\n",
      "\tspeed: 0.1145s/iter; left time: 6707.6129s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0988609\n",
      "\tspeed: 0.1154s/iter; left time: 6748.8651s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0869684\n",
      "\tspeed: 0.1148s/iter; left time: 6705.4233s\n",
      "Epoch: 7 cost time: 00h:08m:29.48s\n",
      "Epoch: 7 | Train Loss: 0.0946513 Vali Loss: 0.1245566 Test Loss: 0.1390361\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0918592\n",
      "\tspeed: 1.4296s/iter; left time: 83208.9369s\n",
      "\titers: 200, epoch: 8 | loss: 0.0835139\n",
      "\tspeed: 0.1151s/iter; left time: 6685.1469s\n",
      "\titers: 300, epoch: 8 | loss: 0.0830909\n",
      "\tspeed: 0.1116s/iter; left time: 6470.8564s\n",
      "\titers: 400, epoch: 8 | loss: 0.0773382\n",
      "\tspeed: 0.1140s/iter; left time: 6599.3565s\n",
      "\titers: 500, epoch: 8 | loss: 0.0999175\n",
      "\tspeed: 0.1146s/iter; left time: 6627.2344s\n",
      "\titers: 600, epoch: 8 | loss: 0.0910697\n",
      "\tspeed: 0.1148s/iter; left time: 6624.5143s\n",
      "\titers: 700, epoch: 8 | loss: 0.0925420\n",
      "\tspeed: 0.1106s/iter; left time: 6371.0272s\n",
      "\titers: 800, epoch: 8 | loss: 0.0842696\n",
      "\tspeed: 0.1029s/iter; left time: 5918.4271s\n",
      "\titers: 900, epoch: 8 | loss: 0.1030085\n",
      "\tspeed: 0.1137s/iter; left time: 6527.5101s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0857712\n",
      "\tspeed: 0.1155s/iter; left time: 6619.3193s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0929789\n",
      "\tspeed: 0.1149s/iter; left time: 6575.7293s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0848481\n",
      "\tspeed: 0.1145s/iter; left time: 6537.3795s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0990944\n",
      "\tspeed: 0.1163s/iter; left time: 6631.5699s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0971840\n",
      "\tspeed: 0.1146s/iter; left time: 6518.8950s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0843722\n",
      "\tspeed: 0.1154s/iter; left time: 6553.7758s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0780336\n",
      "\tspeed: 0.1153s/iter; left time: 6537.3273s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0831821\n",
      "\tspeed: 0.1145s/iter; left time: 6479.4710s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0875997\n",
      "\tspeed: 0.1154s/iter; left time: 6523.5061s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0869278\n",
      "\tspeed: 0.1149s/iter; left time: 6479.1692s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1146180\n",
      "\tspeed: 0.1152s/iter; left time: 6487.1368s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0729130\n",
      "\tspeed: 0.1153s/iter; left time: 6479.4428s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0905604\n",
      "\tspeed: 0.1149s/iter; left time: 6443.9184s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0900590\n",
      "\tspeed: 0.1151s/iter; left time: 6444.2595s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1031543\n",
      "\tspeed: 0.1156s/iter; left time: 6460.4231s\n",
      "\titers: 2500, epoch: 8 | loss: 0.1029483\n",
      "\tspeed: 0.1149s/iter; left time: 6414.3145s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0795364\n",
      "\tspeed: 0.1156s/iter; left time: 6440.5292s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1111625\n",
      "\tspeed: 0.1165s/iter; left time: 6477.0374s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0862386\n",
      "\tspeed: 0.1157s/iter; left time: 6422.2466s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1006412\n",
      "\tspeed: 0.1155s/iter; left time: 6401.8719s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0985600\n",
      "\tspeed: 0.1148s/iter; left time: 6347.0249s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0790469\n",
      "\tspeed: 0.1148s/iter; left time: 6335.9774s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1037440\n",
      "\tspeed: 0.1138s/iter; left time: 6272.0306s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0927985\n",
      "\tspeed: 0.1136s/iter; left time: 6250.9363s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0887386\n",
      "\tspeed: 0.1141s/iter; left time: 6266.1795s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0954302\n",
      "\tspeed: 0.1143s/iter; left time: 6263.9716s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0882672\n",
      "\tspeed: 0.1138s/iter; left time: 6224.1715s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0840252\n",
      "\tspeed: 0.1159s/iter; left time: 6328.9136s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0825819\n",
      "\tspeed: 0.1165s/iter; left time: 6349.1319s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0817300\n",
      "\tspeed: 0.1174s/iter; left time: 6386.4907s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0904208\n",
      "\tspeed: 0.1162s/iter; left time: 6312.9097s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0837452\n",
      "\tspeed: 0.1152s/iter; left time: 6244.6775s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0895528\n",
      "\tspeed: 0.1163s/iter; left time: 6290.0829s\n",
      "\titers: 4300, epoch: 8 | loss: 0.1021302\n",
      "\tspeed: 0.1163s/iter; left time: 6279.4962s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0824312\n",
      "\tspeed: 0.1157s/iter; left time: 6235.1450s\n",
      "Epoch: 8 cost time: 00h:08m:33.60s\n",
      "Epoch: 8 | Train Loss: 0.0923321 Vali Loss: 0.1244166 Test Loss: 0.1393789\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0860157\n",
      "\tspeed: 1.4456s/iter; left time: 77658.5198s\n",
      "\titers: 200, epoch: 9 | loss: 0.0827482\n",
      "\tspeed: 0.1145s/iter; left time: 6139.4815s\n",
      "\titers: 300, epoch: 9 | loss: 0.0886363\n",
      "\tspeed: 0.1163s/iter; left time: 6222.9455s\n",
      "\titers: 400, epoch: 9 | loss: 0.1117299\n",
      "\tspeed: 0.1188s/iter; left time: 6347.3701s\n",
      "\titers: 500, epoch: 9 | loss: 0.0810957\n",
      "\tspeed: 0.1137s/iter; left time: 6065.0390s\n",
      "\titers: 600, epoch: 9 | loss: 0.0957763\n",
      "\tspeed: 0.1145s/iter; left time: 6093.8977s\n",
      "\titers: 700, epoch: 9 | loss: 0.1021896\n",
      "\tspeed: 0.1150s/iter; left time: 6108.0329s\n",
      "\titers: 800, epoch: 9 | loss: 0.0900560\n",
      "\tspeed: 0.1154s/iter; left time: 6120.9355s\n",
      "\titers: 900, epoch: 9 | loss: 0.0941611\n",
      "\tspeed: 0.1150s/iter; left time: 6084.2274s\n",
      "\titers: 1000, epoch: 9 | loss: 0.1047803\n",
      "\tspeed: 0.1155s/iter; left time: 6101.7430s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0963440\n",
      "\tspeed: 0.1166s/iter; left time: 6148.0789s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0966223\n",
      "\tspeed: 0.1084s/iter; left time: 5703.9226s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0858766\n",
      "\tspeed: 0.1122s/iter; left time: 5892.2736s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0874112\n",
      "\tspeed: 0.1171s/iter; left time: 6136.1091s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0868891\n",
      "\tspeed: 0.1177s/iter; left time: 6159.6734s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0750172\n",
      "\tspeed: 0.1163s/iter; left time: 6073.6480s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0886351\n",
      "\tspeed: 0.1155s/iter; left time: 6018.5100s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0861389\n",
      "\tspeed: 0.1154s/iter; left time: 6003.2281s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0806216\n",
      "\tspeed: 0.1158s/iter; left time: 6012.8285s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0933814\n",
      "\tspeed: 0.1143s/iter; left time: 5922.7467s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0894557\n",
      "\tspeed: 0.1152s/iter; left time: 5956.7469s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0905567\n",
      "\tspeed: 0.1162s/iter; left time: 5997.0745s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0910580\n",
      "\tspeed: 0.1166s/iter; left time: 6008.7362s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0801017\n",
      "\tspeed: 0.1160s/iter; left time: 5966.8233s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0770919\n",
      "\tspeed: 0.1170s/iter; left time: 6003.3777s\n",
      "\titers: 2600, epoch: 9 | loss: 0.1063322\n",
      "\tspeed: 0.1162s/iter; left time: 5949.3625s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0792379\n",
      "\tspeed: 0.1167s/iter; left time: 5964.2041s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0964480\n",
      "\tspeed: 0.1146s/iter; left time: 5848.1760s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0888367\n",
      "\tspeed: 0.1149s/iter; left time: 5849.4877s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0799860\n",
      "\tspeed: 0.1154s/iter; left time: 5862.9364s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0811218\n",
      "\tspeed: 0.1146s/iter; left time: 5811.2544s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0989336\n",
      "\tspeed: 0.1165s/iter; left time: 5896.8224s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0911095\n",
      "\tspeed: 0.1160s/iter; left time: 5862.5736s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0657498\n",
      "\tspeed: 0.1151s/iter; left time: 5802.7048s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0899863\n",
      "\tspeed: 0.1167s/iter; left time: 5871.3044s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0988450\n",
      "\tspeed: 0.1156s/iter; left time: 5807.9330s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0745410\n",
      "\tspeed: 0.1168s/iter; left time: 5854.1877s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0895517\n",
      "\tspeed: 0.1154s/iter; left time: 5770.9914s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0853783\n",
      "\tspeed: 0.1190s/iter; left time: 5941.1843s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0899109\n",
      "\tspeed: 0.1159s/iter; left time: 5771.9665s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0919613\n",
      "\tspeed: 0.1152s/iter; left time: 5728.7361s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0816684\n",
      "\tspeed: 0.1166s/iter; left time: 5787.2196s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0924804\n",
      "\tspeed: 0.1147s/iter; left time: 5679.0589s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0909822\n",
      "\tspeed: 0.1150s/iter; left time: 5684.1695s\n",
      "Epoch: 9 cost time: 00h:08m:38.94s\n",
      "Epoch: 9 | Train Loss: 0.0901876 Vali Loss: 0.1248721 Test Loss: 0.1431583\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.039549555629491806, rmse:0.1988707035779953, mae:0.13238517940044403, rse:0.704233705997467\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 96: 01h:38m:18.51s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 143165\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-02 07:24:55,161] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 07:24:56,387] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 07:24:56,388] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 07:24:56,388] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 07:24:56,497] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 07:24:56,497] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 07:24:57,181] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 07:24:57,182] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 07:24:57,183] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 07:24:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 07:24:57,184] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 07:24:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 07:24:57,185] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 07:24:57,498] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 07:24:57,499] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,499] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,616] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 07:24:57,617] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,617] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 07:24:57,729] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 07:24:57,730] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 07:24:57,730] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.35 GB, percent = 9.9%\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 07:24:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 07:24:57,732] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe52e535310>\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 07:24:57,733] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 07:24:57,734] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 07:24:57,735] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1796879\n",
      "\tspeed: 0.1726s/iter; left time: 15419.9925s\n",
      "\titers: 200, epoch: 1 | loss: 0.1421639\n",
      "\tspeed: 0.1283s/iter; left time: 11449.1534s\n",
      "\titers: 300, epoch: 1 | loss: 0.1639913\n",
      "\tspeed: 0.1281s/iter; left time: 11420.8693s\n",
      "\titers: 400, epoch: 1 | loss: 0.1615290\n",
      "\tspeed: 0.1281s/iter; left time: 11406.5486s\n",
      "\titers: 500, epoch: 1 | loss: 0.1515372\n",
      "\tspeed: 0.1288s/iter; left time: 11457.7404s\n",
      "\titers: 600, epoch: 1 | loss: 0.1479097\n",
      "\tspeed: 0.1276s/iter; left time: 11341.7462s\n",
      "\titers: 700, epoch: 1 | loss: 0.1270908\n",
      "\tspeed: 0.1274s/iter; left time: 11312.0453s\n",
      "\titers: 800, epoch: 1 | loss: 0.1200892\n",
      "\tspeed: 0.1268s/iter; left time: 11239.8093s\n",
      "\titers: 900, epoch: 1 | loss: 0.1140285\n",
      "\tspeed: 0.1252s/iter; left time: 11088.1436s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1321990\n",
      "\tspeed: 0.1076s/iter; left time: 9519.3615s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1220576\n",
      "\tspeed: 0.1197s/iter; left time: 10577.6750s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1186985\n",
      "\tspeed: 0.1267s/iter; left time: 11179.6567s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1086995\n",
      "\tspeed: 0.1194s/iter; left time: 10525.2684s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1446381\n",
      "\tspeed: 0.1219s/iter; left time: 10730.7447s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1293692\n",
      "\tspeed: 0.1276s/iter; left time: 11220.7259s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1335815\n",
      "\tspeed: 0.1262s/iter; left time: 11084.1032s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1137727\n",
      "\tspeed: 0.1228s/iter; left time: 10779.0151s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1187546\n",
      "\tspeed: 0.1106s/iter; left time: 9695.9841s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1147917\n",
      "\tspeed: 0.1275s/iter; left time: 11166.8687s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1335327\n",
      "\tspeed: 0.1269s/iter; left time: 11098.9680s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1156783\n",
      "\tspeed: 0.1265s/iter; left time: 11051.8360s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1262950\n",
      "\tspeed: 0.1260s/iter; left time: 10998.5858s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1381016\n",
      "\tspeed: 0.1279s/iter; left time: 11152.1526s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1440842\n",
      "\tspeed: 0.1272s/iter; left time: 11073.5955s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1314677\n",
      "\tspeed: 0.1258s/iter; left time: 10941.4413s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1136199\n",
      "\tspeed: 0.1251s/iter; left time: 10868.1463s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1499220\n",
      "\tspeed: 0.1276s/iter; left time: 11072.2368s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1278764\n",
      "\tspeed: 0.1280s/iter; left time: 11093.0776s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1151534\n",
      "\tspeed: 0.1271s/iter; left time: 11001.7898s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1235821\n",
      "\tspeed: 0.1075s/iter; left time: 9291.7831s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1166754\n",
      "\tspeed: 0.1202s/iter; left time: 10382.4649s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1334251\n",
      "\tspeed: 0.1275s/iter; left time: 10999.5686s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1084857\n",
      "\tspeed: 0.1260s/iter; left time: 10859.4316s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1111453\n",
      "\tspeed: 0.1242s/iter; left time: 10693.0066s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1186036\n",
      "\tspeed: 0.1165s/iter; left time: 10018.6891s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1027479\n",
      "\tspeed: 0.1232s/iter; left time: 10577.1327s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1280373\n",
      "\tspeed: 0.1284s/iter; left time: 11015.6281s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1071356\n",
      "\tspeed: 0.1274s/iter; left time: 10911.9701s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1510007\n",
      "\tspeed: 0.1261s/iter; left time: 10787.3226s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1181215\n",
      "\tspeed: 0.1189s/iter; left time: 10163.3112s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1273197\n",
      "\tspeed: 0.1188s/iter; left time: 10142.5712s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1286651\n",
      "\tspeed: 0.1070s/iter; left time: 9124.9525s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1168264\n",
      "\tspeed: 0.1070s/iter; left time: 9108.0538s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1208781\n",
      "\tspeed: 0.1251s/iter; left time: 10639.2787s\n",
      "Epoch: 1 cost time: 00h:09m:13.64s\n",
      "Epoch: 1 | Train Loss: 0.1312128 Vali Loss: 0.1250472 Test Loss: 0.1346043\n",
      "Validation loss decreased (inf --> 0.125047).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1518959\n",
      "\tspeed: 1.6594s/iter; left time: 140860.2401s\n",
      "\titers: 200, epoch: 2 | loss: 0.0964266\n",
      "\tspeed: 0.1082s/iter; left time: 9171.5763s\n",
      "\titers: 300, epoch: 2 | loss: 0.1247060\n",
      "\tspeed: 0.1168s/iter; left time: 9893.8506s\n",
      "\titers: 400, epoch: 2 | loss: 0.1089664\n",
      "\tspeed: 0.1169s/iter; left time: 9885.9224s\n",
      "\titers: 500, epoch: 2 | loss: 0.1381617\n",
      "\tspeed: 0.1180s/iter; left time: 9972.6544s\n",
      "\titers: 600, epoch: 2 | loss: 0.1414172\n",
      "\tspeed: 0.0988s/iter; left time: 8335.2298s\n",
      "\titers: 700, epoch: 2 | loss: 0.1288400\n",
      "\tspeed: 0.1173s/iter; left time: 9884.6857s\n",
      "\titers: 800, epoch: 2 | loss: 0.1145916\n",
      "\tspeed: 0.1160s/iter; left time: 9764.8300s\n",
      "\titers: 900, epoch: 2 | loss: 0.1190638\n",
      "\tspeed: 0.1128s/iter; left time: 9486.3662s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1355297\n",
      "\tspeed: 0.1170s/iter; left time: 9824.2876s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1262201\n",
      "\tspeed: 0.1157s/iter; left time: 9704.0627s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1185752\n",
      "\tspeed: 0.1148s/iter; left time: 9615.0635s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1181296\n",
      "\tspeed: 0.1173s/iter; left time: 9820.5802s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1286746\n",
      "\tspeed: 0.1173s/iter; left time: 9803.8865s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1001056\n",
      "\tspeed: 0.1164s/iter; left time: 9715.2900s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1140635\n",
      "\tspeed: 0.1154s/iter; left time: 9626.1250s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1254133\n",
      "\tspeed: 0.1170s/iter; left time: 9746.7219s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0982282\n",
      "\tspeed: 0.1166s/iter; left time: 9701.4404s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1218288\n",
      "\tspeed: 0.1164s/iter; left time: 9674.1802s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1265479\n",
      "\tspeed: 0.1169s/iter; left time: 9700.0668s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1115575\n",
      "\tspeed: 0.1167s/iter; left time: 9671.6086s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1242154\n",
      "\tspeed: 0.1161s/iter; left time: 9610.3575s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1200421\n",
      "\tspeed: 0.1162s/iter; left time: 9611.5781s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1178377\n",
      "\tspeed: 0.1158s/iter; left time: 9561.1125s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1067296\n",
      "\tspeed: 0.1165s/iter; left time: 9613.9227s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1391483\n",
      "\tspeed: 0.1156s/iter; left time: 9527.7564s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0934009\n",
      "\tspeed: 0.1165s/iter; left time: 9590.4439s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1260687\n",
      "\tspeed: 0.1175s/iter; left time: 9660.2411s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1159549\n",
      "\tspeed: 0.1160s/iter; left time: 9524.8115s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1432488\n",
      "\tspeed: 0.1148s/iter; left time: 9414.2710s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1512252\n",
      "\tspeed: 0.1157s/iter; left time: 9477.0973s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1017725\n",
      "\tspeed: 0.1154s/iter; left time: 9435.5917s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1183511\n",
      "\tspeed: 0.1151s/iter; left time: 9401.9968s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1324611\n",
      "\tspeed: 0.1160s/iter; left time: 9464.5271s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1085706\n",
      "\tspeed: 0.1165s/iter; left time: 9490.2395s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1149569\n",
      "\tspeed: 0.1154s/iter; left time: 9388.3903s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1181808\n",
      "\tspeed: 0.1161s/iter; left time: 9435.1903s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0986252\n",
      "\tspeed: 0.1152s/iter; left time: 9354.0269s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1190111\n",
      "\tspeed: 0.1155s/iter; left time: 9365.0712s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1110677\n",
      "\tspeed: 0.1172s/iter; left time: 9489.5820s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1073243\n",
      "\tspeed: 0.1152s/iter; left time: 9322.0756s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1071260\n",
      "\tspeed: 0.1155s/iter; left time: 9328.9636s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1088651\n",
      "\tspeed: 0.1162s/iter; left time: 9378.4993s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1130303\n",
      "\tspeed: 0.1143s/iter; left time: 9207.5642s\n",
      "Epoch: 2 cost time: 00h:08m:37.30s\n",
      "Epoch: 2 | Train Loss: 0.1167669 Vali Loss: 0.1236090 Test Loss: 0.1348319\n",
      "Validation loss decreased (0.125047 --> 0.123609).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1316700\n",
      "\tspeed: 1.4469s/iter; left time: 116354.8227s\n",
      "\titers: 200, epoch: 3 | loss: 0.1039964\n",
      "\tspeed: 0.1161s/iter; left time: 9324.3219s\n",
      "\titers: 300, epoch: 3 | loss: 0.1126300\n",
      "\tspeed: 0.1174s/iter; left time: 9418.7683s\n",
      "\titers: 400, epoch: 3 | loss: 0.1131036\n",
      "\tspeed: 0.1179s/iter; left time: 9441.8684s\n",
      "\titers: 500, epoch: 3 | loss: 0.1220148\n",
      "\tspeed: 0.1157s/iter; left time: 9255.2142s\n",
      "\titers: 600, epoch: 3 | loss: 0.1027394\n",
      "\tspeed: 0.1173s/iter; left time: 9375.9775s\n",
      "\titers: 700, epoch: 3 | loss: 0.0868261\n",
      "\tspeed: 0.1194s/iter; left time: 9529.9573s\n",
      "\titers: 800, epoch: 3 | loss: 0.1178839\n",
      "\tspeed: 0.1147s/iter; left time: 9145.8728s\n",
      "\titers: 900, epoch: 3 | loss: 0.1133176\n",
      "\tspeed: 0.1156s/iter; left time: 9206.9141s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1266209\n",
      "\tspeed: 0.1094s/iter; left time: 8700.7189s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1104170\n",
      "\tspeed: 0.1161s/iter; left time: 9216.2815s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1124218\n",
      "\tspeed: 0.1152s/iter; left time: 9139.3929s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1273887\n",
      "\tspeed: 0.1155s/iter; left time: 9149.5361s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1296332\n",
      "\tspeed: 0.1165s/iter; left time: 9215.2192s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1154967\n",
      "\tspeed: 0.1157s/iter; left time: 9144.4551s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1137325\n",
      "\tspeed: 0.1173s/iter; left time: 9260.2878s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1128048\n",
      "\tspeed: 0.1148s/iter; left time: 9045.8627s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1064127\n",
      "\tspeed: 0.1156s/iter; left time: 9103.3748s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1183627\n",
      "\tspeed: 0.1174s/iter; left time: 9226.3196s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1283379\n",
      "\tspeed: 0.1165s/iter; left time: 9150.0116s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1172018\n",
      "\tspeed: 0.1149s/iter; left time: 9009.6223s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1036158\n",
      "\tspeed: 0.1161s/iter; left time: 9093.2266s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1167738\n",
      "\tspeed: 0.1157s/iter; left time: 9051.7936s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1137663\n",
      "\tspeed: 0.1147s/iter; left time: 8962.3061s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1288111\n",
      "\tspeed: 0.1158s/iter; left time: 9031.7794s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1302198\n",
      "\tspeed: 0.1154s/iter; left time: 8988.4020s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1083783\n",
      "\tspeed: 0.1160s/iter; left time: 9029.4703s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1071675\n",
      "\tspeed: 0.1170s/iter; left time: 9095.9111s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1293719\n",
      "\tspeed: 0.1189s/iter; left time: 9224.6144s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1187557\n",
      "\tspeed: 0.1176s/iter; left time: 9118.7357s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0963806\n",
      "\tspeed: 0.1170s/iter; left time: 9058.7001s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1214169\n",
      "\tspeed: 0.1165s/iter; left time: 9006.0014s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0924854\n",
      "\tspeed: 0.1175s/iter; left time: 9071.1331s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0961636\n",
      "\tspeed: 0.1161s/iter; left time: 8950.3778s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1234925\n",
      "\tspeed: 0.1174s/iter; left time: 9038.1773s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1198209\n",
      "\tspeed: 0.1161s/iter; left time: 8926.9670s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1148769\n",
      "\tspeed: 0.1172s/iter; left time: 9003.4303s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1119351\n",
      "\tspeed: 0.1161s/iter; left time: 8903.0600s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1058953\n",
      "\tspeed: 0.1188s/iter; left time: 9104.9184s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1222074\n",
      "\tspeed: 0.1186s/iter; left time: 9075.6974s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1121319\n",
      "\tspeed: 0.1168s/iter; left time: 8924.4999s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0926314\n",
      "\tspeed: 0.1174s/iter; left time: 8958.3413s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1098126\n",
      "\tspeed: 0.1200s/iter; left time: 9144.0661s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1091134\n",
      "\tspeed: 0.1167s/iter; left time: 8879.8932s\n",
      "Epoch: 3 cost time: 00h:08m:41.53s\n",
      "Epoch: 3 | Train Loss: 0.1130445 Vali Loss: 0.1239033 Test Loss: 0.1380568\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1118852\n",
      "\tspeed: 1.4256s/iter; left time: 108259.1567s\n",
      "\titers: 200, epoch: 4 | loss: 0.1162335\n",
      "\tspeed: 0.1163s/iter; left time: 8822.3548s\n",
      "\titers: 300, epoch: 4 | loss: 0.0995601\n",
      "\tspeed: 0.1157s/iter; left time: 8766.8859s\n",
      "\titers: 400, epoch: 4 | loss: 0.1132471\n",
      "\tspeed: 0.1149s/iter; left time: 8689.4645s\n",
      "\titers: 500, epoch: 4 | loss: 0.1124692\n",
      "\tspeed: 0.1151s/iter; left time: 8698.4065s\n",
      "\titers: 600, epoch: 4 | loss: 0.1041030\n",
      "\tspeed: 0.1161s/iter; left time: 8756.2922s\n",
      "\titers: 700, epoch: 4 | loss: 0.1155552\n",
      "\tspeed: 0.1168s/iter; left time: 8798.0051s\n",
      "\titers: 800, epoch: 4 | loss: 0.1389403\n",
      "\tspeed: 0.1162s/iter; left time: 8744.8484s\n",
      "\titers: 900, epoch: 4 | loss: 0.1264938\n",
      "\tspeed: 0.1158s/iter; left time: 8700.2851s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1071267\n",
      "\tspeed: 0.1144s/iter; left time: 8584.7234s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1268844\n",
      "\tspeed: 0.1149s/iter; left time: 8608.8614s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0896029\n",
      "\tspeed: 0.1144s/iter; left time: 8563.0431s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0967841\n",
      "\tspeed: 0.1154s/iter; left time: 8622.9499s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0985883\n",
      "\tspeed: 0.1175s/iter; left time: 8771.2091s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1171656\n",
      "\tspeed: 0.1145s/iter; left time: 8532.8823s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1105181\n",
      "\tspeed: 0.1167s/iter; left time: 8686.1936s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0927968\n",
      "\tspeed: 0.1149s/iter; left time: 8541.6680s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0862853\n",
      "\tspeed: 0.1159s/iter; left time: 8601.1435s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0791367\n",
      "\tspeed: 0.1150s/iter; left time: 8528.6747s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0964180\n",
      "\tspeed: 0.1163s/iter; left time: 8614.4506s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1272053\n",
      "\tspeed: 0.1150s/iter; left time: 8501.9548s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1248995\n",
      "\tspeed: 0.1149s/iter; left time: 8486.9660s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1133061\n",
      "\tspeed: 0.1163s/iter; left time: 8573.9003s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0944415\n",
      "\tspeed: 0.1160s/iter; left time: 8540.5527s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0903405\n",
      "\tspeed: 0.1155s/iter; left time: 8493.4931s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1078932\n",
      "\tspeed: 0.1163s/iter; left time: 8539.2716s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1252578\n",
      "\tspeed: 0.1151s/iter; left time: 8442.0480s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1257209\n",
      "\tspeed: 0.1174s/iter; left time: 8597.6128s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1187883\n",
      "\tspeed: 0.1158s/iter; left time: 8467.2677s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1017213\n",
      "\tspeed: 0.1168s/iter; left time: 8532.3864s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0994309\n",
      "\tspeed: 0.1160s/iter; left time: 8462.6957s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1110925\n",
      "\tspeed: 0.1155s/iter; left time: 8415.8009s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1359264\n",
      "\tspeed: 0.1148s/iter; left time: 8349.8342s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1213765\n",
      "\tspeed: 0.1159s/iter; left time: 8416.6438s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1151622\n",
      "\tspeed: 0.1152s/iter; left time: 8353.7053s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0981746\n",
      "\tspeed: 0.1146s/iter; left time: 8305.3391s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1020091\n",
      "\tspeed: 0.1042s/iter; left time: 7537.6323s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1195145\n",
      "\tspeed: 0.1161s/iter; left time: 8384.9630s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1091338\n",
      "\tspeed: 0.1151s/iter; left time: 8306.8368s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0933681\n",
      "\tspeed: 0.1152s/iter; left time: 8301.8588s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1090270\n",
      "\tspeed: 0.1157s/iter; left time: 8321.0657s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0897193\n",
      "\tspeed: 0.1150s/iter; left time: 8265.2853s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0790287\n",
      "\tspeed: 0.1148s/iter; left time: 8238.9496s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0960602\n",
      "\tspeed: 0.1144s/iter; left time: 8193.9019s\n",
      "Epoch: 4 cost time: 00h:08m:36.70s\n",
      "Epoch: 4 | Train Loss: 0.1090248 Vali Loss: 0.1242168 Test Loss: 0.1387148\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0958029\n",
      "\tspeed: 1.4214s/iter; left time: 101586.7235s\n",
      "\titers: 200, epoch: 5 | loss: 0.1198548\n",
      "\tspeed: 0.1155s/iter; left time: 8242.3255s\n",
      "\titers: 300, epoch: 5 | loss: 0.0923079\n",
      "\tspeed: 0.1149s/iter; left time: 8191.2940s\n",
      "\titers: 400, epoch: 5 | loss: 0.1043584\n",
      "\tspeed: 0.1146s/iter; left time: 8159.1562s\n",
      "\titers: 500, epoch: 5 | loss: 0.1197232\n",
      "\tspeed: 0.1137s/iter; left time: 8078.0820s\n",
      "\titers: 600, epoch: 5 | loss: 0.0903645\n",
      "\tspeed: 0.1134s/iter; left time: 8051.2072s\n",
      "\titers: 700, epoch: 5 | loss: 0.1047615\n",
      "\tspeed: 0.1144s/iter; left time: 8105.6870s\n",
      "\titers: 800, epoch: 5 | loss: 0.1033622\n",
      "\tspeed: 0.1110s/iter; left time: 7853.1599s\n",
      "\titers: 900, epoch: 5 | loss: 0.1312480\n",
      "\tspeed: 0.1155s/iter; left time: 8158.7599s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1187147\n",
      "\tspeed: 0.1154s/iter; left time: 8140.9617s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1290016\n",
      "\tspeed: 0.1163s/iter; left time: 8193.0191s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1014192\n",
      "\tspeed: 0.1159s/iter; left time: 8155.0929s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1014588\n",
      "\tspeed: 0.1163s/iter; left time: 8170.1980s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1132953\n",
      "\tspeed: 0.1156s/iter; left time: 8113.3698s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0985292\n",
      "\tspeed: 0.1143s/iter; left time: 8007.2994s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0970728\n",
      "\tspeed: 0.1145s/iter; left time: 8014.6014s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1024207\n",
      "\tspeed: 0.1070s/iter; left time: 7476.1594s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1228088\n",
      "\tspeed: 0.1087s/iter; left time: 7581.0328s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1073534\n",
      "\tspeed: 0.1151s/iter; left time: 8021.4381s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1048014\n",
      "\tspeed: 0.1160s/iter; left time: 8072.1069s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0878087\n",
      "\tspeed: 0.1153s/iter; left time: 8013.1637s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0974426\n",
      "\tspeed: 0.1161s/iter; left time: 8053.1329s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1010784\n",
      "\tspeed: 0.1145s/iter; left time: 7929.1990s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1082448\n",
      "\tspeed: 0.1146s/iter; left time: 7928.9050s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1082602\n",
      "\tspeed: 0.1134s/iter; left time: 7835.5237s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1061032\n",
      "\tspeed: 0.1153s/iter; left time: 7950.3988s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1009885\n",
      "\tspeed: 0.1140s/iter; left time: 7851.8386s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1107342\n",
      "\tspeed: 0.1146s/iter; left time: 7883.3197s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1206490\n",
      "\tspeed: 0.1171s/iter; left time: 8043.8140s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0938140\n",
      "\tspeed: 0.1124s/iter; left time: 7704.7116s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0936605\n",
      "\tspeed: 0.1148s/iter; left time: 7862.1151s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1181608\n",
      "\tspeed: 0.1047s/iter; left time: 7156.0041s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1202567\n",
      "\tspeed: 0.1147s/iter; left time: 7827.1282s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1185086\n",
      "\tspeed: 0.1150s/iter; left time: 7838.4699s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1326381\n",
      "\tspeed: 0.1151s/iter; left time: 7833.1310s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1140965\n",
      "\tspeed: 0.1146s/iter; left time: 7791.2881s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0983576\n",
      "\tspeed: 0.1150s/iter; left time: 7804.9407s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1108841\n",
      "\tspeed: 0.1152s/iter; left time: 7808.9477s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1224041\n",
      "\tspeed: 0.1163s/iter; left time: 7867.3697s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1092387\n",
      "\tspeed: 0.1151s/iter; left time: 7780.1666s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1015882\n",
      "\tspeed: 0.1163s/iter; left time: 7843.3600s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1037027\n",
      "\tspeed: 0.1158s/iter; left time: 7800.7583s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1098816\n",
      "\tspeed: 0.1146s/iter; left time: 7710.4665s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0915770\n",
      "\tspeed: 0.1142s/iter; left time: 7673.7397s\n",
      "Epoch: 5 cost time: 00h:08m:32.11s\n",
      "Epoch: 5 | Train Loss: 0.1055163 Vali Loss: 0.1278250 Test Loss: 0.1433094\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1131890\n",
      "\tspeed: 1.4163s/iter; left time: 94886.3687s\n",
      "\titers: 200, epoch: 6 | loss: 0.1149606\n",
      "\tspeed: 0.1160s/iter; left time: 7760.3897s\n",
      "\titers: 300, epoch: 6 | loss: 0.0889585\n",
      "\tspeed: 0.1161s/iter; left time: 7753.3661s\n",
      "\titers: 400, epoch: 6 | loss: 0.0879647\n",
      "\tspeed: 0.1154s/iter; left time: 7696.7286s\n",
      "\titers: 500, epoch: 6 | loss: 0.0849967\n",
      "\tspeed: 0.1151s/iter; left time: 7663.1614s\n",
      "\titers: 600, epoch: 6 | loss: 0.1211026\n",
      "\tspeed: 0.1138s/iter; left time: 7567.4627s\n",
      "\titers: 700, epoch: 6 | loss: 0.0971647\n",
      "\tspeed: 0.1136s/iter; left time: 7543.8119s\n",
      "\titers: 800, epoch: 6 | loss: 0.0980629\n",
      "\tspeed: 0.1142s/iter; left time: 7572.5191s\n",
      "\titers: 900, epoch: 6 | loss: 0.0897351\n",
      "\tspeed: 0.1146s/iter; left time: 7585.9983s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0996988\n",
      "\tspeed: 0.1164s/iter; left time: 7693.5253s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0931935\n",
      "\tspeed: 0.1145s/iter; left time: 7555.4581s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0953249\n",
      "\tspeed: 0.1157s/iter; left time: 7623.4250s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0887816\n",
      "\tspeed: 0.1163s/iter; left time: 7653.2949s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0984268\n",
      "\tspeed: 0.1158s/iter; left time: 7605.3619s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1156681\n",
      "\tspeed: 0.1156s/iter; left time: 7586.0758s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0764754\n",
      "\tspeed: 0.1149s/iter; left time: 7522.5356s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0990757\n",
      "\tspeed: 0.1140s/iter; left time: 7455.4058s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0968827\n",
      "\tspeed: 0.1153s/iter; left time: 7527.6041s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1168354\n",
      "\tspeed: 0.1150s/iter; left time: 7497.9210s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1053651\n",
      "\tspeed: 0.1154s/iter; left time: 7510.2694s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0823664\n",
      "\tspeed: 0.1151s/iter; left time: 7482.7343s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1102771\n",
      "\tspeed: 0.1166s/iter; left time: 7564.2998s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1032612\n",
      "\tspeed: 0.1114s/iter; left time: 7215.5340s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1231637\n",
      "\tspeed: 0.1150s/iter; left time: 7438.2102s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0966471\n",
      "\tspeed: 0.1137s/iter; left time: 7345.7661s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1128988\n",
      "\tspeed: 0.1171s/iter; left time: 7553.1837s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0957244\n",
      "\tspeed: 0.1161s/iter; left time: 7475.1081s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1045579\n",
      "\tspeed: 0.1151s/iter; left time: 7401.1844s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1174107\n",
      "\tspeed: 0.1138s/iter; left time: 7303.1345s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1265308\n",
      "\tspeed: 0.1158s/iter; left time: 7422.9778s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1023416\n",
      "\tspeed: 0.1147s/iter; left time: 7343.3806s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1108819\n",
      "\tspeed: 0.1153s/iter; left time: 7365.2338s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1014752\n",
      "\tspeed: 0.1148s/iter; left time: 7324.4552s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1040500\n",
      "\tspeed: 0.1133s/iter; left time: 7219.1967s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0979548\n",
      "\tspeed: 0.1142s/iter; left time: 7261.3570s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0927388\n",
      "\tspeed: 0.1071s/iter; left time: 6798.5022s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1014158\n",
      "\tspeed: 0.1152s/iter; left time: 7302.8676s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0997907\n",
      "\tspeed: 0.1131s/iter; left time: 7156.3863s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0954398\n",
      "\tspeed: 0.0966s/iter; left time: 6102.8671s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1046737\n",
      "\tspeed: 0.1147s/iter; left time: 7238.7126s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1015742\n",
      "\tspeed: 0.1153s/iter; left time: 7262.5951s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1010522\n",
      "\tspeed: 0.1145s/iter; left time: 7200.6178s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1013126\n",
      "\tspeed: 0.1159s/iter; left time: 7276.7739s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1157126\n",
      "\tspeed: 0.1157s/iter; left time: 7253.7991s\n",
      "Epoch: 6 cost time: 00h:08m:32.30s\n",
      "Epoch: 6 | Train Loss: 0.1023981 Vali Loss: 0.1287997 Test Loss: 0.1417000\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1052988\n",
      "\tspeed: 1.4231s/iter; left time: 88977.0848s\n",
      "\titers: 200, epoch: 7 | loss: 0.0873112\n",
      "\tspeed: 0.1171s/iter; left time: 7307.5313s\n",
      "\titers: 300, epoch: 7 | loss: 0.1031646\n",
      "\tspeed: 0.1071s/iter; left time: 6677.3176s\n",
      "\titers: 400, epoch: 7 | loss: 0.1072783\n",
      "\tspeed: 0.1167s/iter; left time: 7258.5221s\n",
      "\titers: 500, epoch: 7 | loss: 0.1088210\n",
      "\tspeed: 0.1060s/iter; left time: 6582.6507s\n",
      "\titers: 600, epoch: 7 | loss: 0.1105814\n",
      "\tspeed: 0.1077s/iter; left time: 6677.1701s\n",
      "\titers: 700, epoch: 7 | loss: 0.0999586\n",
      "\tspeed: 0.0995s/iter; left time: 6159.3849s\n",
      "\titers: 800, epoch: 7 | loss: 0.1067762\n",
      "\tspeed: 0.1153s/iter; left time: 7125.3779s\n",
      "\titers: 900, epoch: 7 | loss: 0.1046992\n",
      "\tspeed: 0.1151s/iter; left time: 7101.8821s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0837644\n",
      "\tspeed: 0.1085s/iter; left time: 6686.3988s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0945975\n",
      "\tspeed: 0.1145s/iter; left time: 7044.0881s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0875397\n",
      "\tspeed: 0.1145s/iter; left time: 7034.8019s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1284341\n",
      "\tspeed: 0.1173s/iter; left time: 7192.9399s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0821818\n",
      "\tspeed: 0.1187s/iter; left time: 7267.4471s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0969216\n",
      "\tspeed: 0.1176s/iter; left time: 7188.5354s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1076515\n",
      "\tspeed: 0.1160s/iter; left time: 7079.3109s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0943058\n",
      "\tspeed: 0.1171s/iter; left time: 7134.9859s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1208289\n",
      "\tspeed: 0.1172s/iter; left time: 7127.2384s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0957807\n",
      "\tspeed: 0.1169s/iter; left time: 7096.7321s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0940845\n",
      "\tspeed: 0.1146s/iter; left time: 6949.6210s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0939218\n",
      "\tspeed: 0.1155s/iter; left time: 6990.7062s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0780332\n",
      "\tspeed: 0.1163s/iter; left time: 7027.2516s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0861988\n",
      "\tspeed: 0.1159s/iter; left time: 6991.9481s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0930833\n",
      "\tspeed: 0.1150s/iter; left time: 6923.8736s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0916982\n",
      "\tspeed: 0.1156s/iter; left time: 6947.3130s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0973968\n",
      "\tspeed: 0.1179s/iter; left time: 7077.8250s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0994120\n",
      "\tspeed: 0.1165s/iter; left time: 6983.2574s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1021433\n",
      "\tspeed: 0.1164s/iter; left time: 6961.0543s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1025242\n",
      "\tspeed: 0.1172s/iter; left time: 6997.5293s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0995893\n",
      "\tspeed: 0.1159s/iter; left time: 6908.7495s\n",
      "\titers: 3100, epoch: 7 | loss: 0.1242002\n",
      "\tspeed: 0.1170s/iter; left time: 6966.5509s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0999554\n",
      "\tspeed: 0.1165s/iter; left time: 6921.0840s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0944341\n",
      "\tspeed: 0.1157s/iter; left time: 6863.4892s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0867006\n",
      "\tspeed: 0.1150s/iter; left time: 6809.7276s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0829456\n",
      "\tspeed: 0.1168s/iter; left time: 6903.4284s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1114448\n",
      "\tspeed: 0.1150s/iter; left time: 6787.7201s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1034871\n",
      "\tspeed: 0.1146s/iter; left time: 6754.5146s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0981191\n",
      "\tspeed: 0.1161s/iter; left time: 6831.5273s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0835699\n",
      "\tspeed: 0.1052s/iter; left time: 6179.2232s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0970934\n",
      "\tspeed: 0.1162s/iter; left time: 6809.7888s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0923245\n",
      "\tspeed: 0.1168s/iter; left time: 6834.4963s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0830892\n",
      "\tspeed: 0.1151s/iter; left time: 6726.9684s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1040890\n",
      "\tspeed: 0.1190s/iter; left time: 6939.1710s\n",
      "\titers: 4400, epoch: 7 | loss: 0.1054323\n",
      "\tspeed: 0.1153s/iter; left time: 6711.2376s\n",
      "Epoch: 7 cost time: 00h:08m:33.95s\n",
      "Epoch: 7 | Train Loss: 0.0994186 Vali Loss: 0.1318476 Test Loss: 0.1430013\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.039509814232587814, rmse:0.19877076148986816, mae:0.13483189046382904, rse:0.7042074799537659\n",
      "success delete checkpoints\n",
      "Intermediate time for DE and pred_len 168: 01h:16m:53.78s\n",
      "\n",
      "Intermediate time for DE: 04h:36m:07.06s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 143885\n",
      "val 31085\n",
      "test 31085\n",
      "[2024-11-02 08:41:48,941] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 08:41:50,078] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 08:41:50,079] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 08:41:50,079] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 08:41:50,183] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 08:41:50,184] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 08:41:50,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 08:41:50,864] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 08:41:50,864] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 08:41:50,866] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 08:41:50,866] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 08:41:50,866] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 08:41:50,866] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 08:41:51,181] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 08:41:51,182] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.42 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,300] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 08:41:51,301] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,302] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.43 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,302] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 08:41:51,417] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 08:41:51,418] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 08:41:51,418] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 74.43 GB, percent = 9.9%\n",
      "[2024-11-02 08:41:51,418] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 08:41:51,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 08:41:51,419] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f66915c2150>\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 08:41:51,420] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 08:41:51,421] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 08:41:51,422] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1480396\n",
      "\tspeed: 0.1702s/iter; left time: 15289.4378s\n",
      "\titers: 200, epoch: 1 | loss: 0.1359396\n",
      "\tspeed: 0.1278s/iter; left time: 11462.0213s\n",
      "\titers: 300, epoch: 1 | loss: 0.1393571\n",
      "\tspeed: 0.1281s/iter; left time: 11476.6610s\n",
      "\titers: 400, epoch: 1 | loss: 0.1382776\n",
      "\tspeed: 0.1272s/iter; left time: 11387.7617s\n",
      "\titers: 500, epoch: 1 | loss: 0.1498523\n",
      "\tspeed: 0.1274s/iter; left time: 11392.2678s\n",
      "\titers: 600, epoch: 1 | loss: 0.1618727\n",
      "\tspeed: 0.1290s/iter; left time: 11522.9930s\n",
      "\titers: 700, epoch: 1 | loss: 0.1502969\n",
      "\tspeed: 0.1276s/iter; left time: 11387.8297s\n",
      "\titers: 800, epoch: 1 | loss: 0.1506559\n",
      "\tspeed: 0.1273s/iter; left time: 11347.5577s\n",
      "\titers: 900, epoch: 1 | loss: 0.1592317\n",
      "\tspeed: 0.1270s/iter; left time: 11304.4541s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1277500\n",
      "\tspeed: 0.1272s/iter; left time: 11310.4209s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1013472\n",
      "\tspeed: 0.1176s/iter; left time: 10443.7474s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0913689\n",
      "\tspeed: 0.1276s/iter; left time: 11323.0255s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1019253\n",
      "\tspeed: 0.1258s/iter; left time: 11146.2666s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0914617\n",
      "\tspeed: 0.1257s/iter; left time: 11124.8529s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1200864\n",
      "\tspeed: 0.1285s/iter; left time: 11359.2716s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0783514\n",
      "\tspeed: 0.1288s/iter; left time: 11377.0559s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0821924\n",
      "\tspeed: 0.1281s/iter; left time: 11305.0679s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1186250\n",
      "\tspeed: 0.1286s/iter; left time: 11334.6789s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1131187\n",
      "\tspeed: 0.1290s/iter; left time: 11357.9233s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0995416\n",
      "\tspeed: 0.1274s/iter; left time: 11200.8495s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1084358\n",
      "\tspeed: 0.1263s/iter; left time: 11089.8330s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1041089\n",
      "\tspeed: 0.1216s/iter; left time: 10669.8667s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0937779\n",
      "\tspeed: 0.1275s/iter; left time: 11172.8503s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0927153\n",
      "\tspeed: 0.1284s/iter; left time: 11238.5934s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0854183\n",
      "\tspeed: 0.1272s/iter; left time: 11121.1070s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0832396\n",
      "\tspeed: 0.1262s/iter; left time: 11015.7884s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0943345\n",
      "\tspeed: 0.1268s/iter; left time: 11055.6223s\n",
      "\titers: 2800, epoch: 1 | loss: 0.0730870\n",
      "\tspeed: 0.1261s/iter; left time: 10982.9476s\n",
      "\titers: 2900, epoch: 1 | loss: 0.0838178\n",
      "\tspeed: 0.1265s/iter; left time: 11009.8294s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1037557\n",
      "\tspeed: 0.1275s/iter; left time: 11079.0149s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0776548\n",
      "\tspeed: 0.1297s/iter; left time: 11257.6756s\n",
      "\titers: 3200, epoch: 1 | loss: 0.0702538\n",
      "\tspeed: 0.1265s/iter; left time: 10968.0991s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0951389\n",
      "\tspeed: 0.1280s/iter; left time: 11083.8968s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0885212\n",
      "\tspeed: 0.1282s/iter; left time: 11091.3444s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0847573\n",
      "\tspeed: 0.1266s/iter; left time: 10939.4751s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0946836\n",
      "\tspeed: 0.1263s/iter; left time: 10904.2457s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0944981\n",
      "\tspeed: 0.1257s/iter; left time: 10838.8317s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0870032\n",
      "\tspeed: 0.1268s/iter; left time: 10923.8423s\n",
      "\titers: 3900, epoch: 1 | loss: 0.0747629\n",
      "\tspeed: 0.1260s/iter; left time: 10842.5188s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0820309\n",
      "\tspeed: 0.1268s/iter; left time: 10893.2202s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0989253\n",
      "\tspeed: 0.1259s/iter; left time: 10808.3815s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1019376\n",
      "\tspeed: 0.1260s/iter; left time: 10804.7401s\n",
      "\titers: 4300, epoch: 1 | loss: 0.0859145\n",
      "\tspeed: 0.1267s/iter; left time: 10852.4174s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0804792\n",
      "\tspeed: 0.1259s/iter; left time: 10770.4280s\n",
      "Epoch: 1 cost time: 00h:09m:31.74s\n",
      "Epoch: 1 | Train Loss: 0.1045278 Vali Loss: 0.0942639 Test Loss: 0.1063175\n",
      "Validation loss decreased (inf --> 0.094264).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0567711\n",
      "\tspeed: 1.7215s/iter; left time: 146883.7925s\n",
      "\titers: 200, epoch: 2 | loss: 0.0760146\n",
      "\tspeed: 0.1167s/iter; left time: 9949.4148s\n",
      "\titers: 300, epoch: 2 | loss: 0.0817795\n",
      "\tspeed: 0.1183s/iter; left time: 10069.3451s\n",
      "\titers: 400, epoch: 2 | loss: 0.0809524\n",
      "\tspeed: 0.1157s/iter; left time: 9835.0170s\n",
      "\titers: 500, epoch: 2 | loss: 0.0880101\n",
      "\tspeed: 0.1165s/iter; left time: 9893.0284s\n",
      "\titers: 600, epoch: 2 | loss: 0.0825832\n",
      "\tspeed: 0.1120s/iter; left time: 9503.7633s\n",
      "\titers: 700, epoch: 2 | loss: 0.0830727\n",
      "\tspeed: 0.1167s/iter; left time: 9884.5535s\n",
      "\titers: 800, epoch: 2 | loss: 0.1017664\n",
      "\tspeed: 0.1153s/iter; left time: 9755.3256s\n",
      "\titers: 900, epoch: 2 | loss: 0.0834542\n",
      "\tspeed: 0.1169s/iter; left time: 9879.2239s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1037458\n",
      "\tspeed: 0.1166s/iter; left time: 9842.4837s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0702419\n",
      "\tspeed: 0.1164s/iter; left time: 9813.7170s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0827916\n",
      "\tspeed: 0.1170s/iter; left time: 9854.5568s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0975119\n",
      "\tspeed: 0.1160s/iter; left time: 9756.9489s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0909550\n",
      "\tspeed: 0.1171s/iter; left time: 9838.3383s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0980282\n",
      "\tspeed: 0.1166s/iter; left time: 9783.2063s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0983221\n",
      "\tspeed: 0.1169s/iter; left time: 9798.2750s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0979449\n",
      "\tspeed: 0.1001s/iter; left time: 8381.6061s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0913448\n",
      "\tspeed: 0.1151s/iter; left time: 9624.4655s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1151030\n",
      "\tspeed: 0.0990s/iter; left time: 8271.8433s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0902165\n",
      "\tspeed: 0.1029s/iter; left time: 8583.1806s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1013854\n",
      "\tspeed: 0.1163s/iter; left time: 9688.5822s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0748697\n",
      "\tspeed: 0.1169s/iter; left time: 9727.6551s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0970995\n",
      "\tspeed: 0.1162s/iter; left time: 9657.8191s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0777804\n",
      "\tspeed: 0.1171s/iter; left time: 9719.2870s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0974897\n",
      "\tspeed: 0.1164s/iter; left time: 9656.5650s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0840274\n",
      "\tspeed: 0.1173s/iter; left time: 9717.8059s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0875587\n",
      "\tspeed: 0.1163s/iter; left time: 9620.4784s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0773716\n",
      "\tspeed: 0.1161s/iter; left time: 9589.8572s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0789836\n",
      "\tspeed: 0.1172s/iter; left time: 9675.8173s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1230920\n",
      "\tspeed: 0.1069s/iter; left time: 8809.7361s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0794076\n",
      "\tspeed: 0.1163s/iter; left time: 9571.7775s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0580312\n",
      "\tspeed: 0.1174s/iter; left time: 9650.7737s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0819194\n",
      "\tspeed: 0.1185s/iter; left time: 9732.8212s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0737068\n",
      "\tspeed: 0.1160s/iter; left time: 9511.0434s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0817206\n",
      "\tspeed: 0.1171s/iter; left time: 9589.7468s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0782916\n",
      "\tspeed: 0.1154s/iter; left time: 9444.9179s\n",
      "\titers: 3700, epoch: 2 | loss: 0.0830257\n",
      "\tspeed: 0.1156s/iter; left time: 9447.7819s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0800915\n",
      "\tspeed: 0.1166s/iter; left time: 9520.3294s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0705851\n",
      "\tspeed: 0.1166s/iter; left time: 9502.4624s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0799635\n",
      "\tspeed: 0.1160s/iter; left time: 9444.6185s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0793858\n",
      "\tspeed: 0.1160s/iter; left time: 9436.8695s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0901380\n",
      "\tspeed: 0.1153s/iter; left time: 9366.7847s\n",
      "\titers: 4300, epoch: 2 | loss: 0.0854981\n",
      "\tspeed: 0.1141s/iter; left time: 9252.7679s\n",
      "\titers: 4400, epoch: 2 | loss: 0.0815358\n",
      "\tspeed: 0.1147s/iter; left time: 9294.2794s\n",
      "Epoch: 2 cost time: 00h:08m:37.60s\n",
      "Epoch: 2 | Train Loss: 0.0874084 Vali Loss: 0.0943880 Test Loss: 0.1067109\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0967912\n",
      "\tspeed: 1.4692s/iter; left time: 118749.9835s\n",
      "\titers: 200, epoch: 3 | loss: 0.0968783\n",
      "\tspeed: 0.1170s/iter; left time: 9444.9843s\n",
      "\titers: 300, epoch: 3 | loss: 0.0831899\n",
      "\tspeed: 0.1130s/iter; left time: 9111.0593s\n",
      "\titers: 400, epoch: 3 | loss: 0.1071719\n",
      "\tspeed: 0.1002s/iter; left time: 8069.0538s\n",
      "\titers: 500, epoch: 3 | loss: 0.0645533\n",
      "\tspeed: 0.1145s/iter; left time: 9206.0888s\n",
      "\titers: 600, epoch: 3 | loss: 0.0737154\n",
      "\tspeed: 0.1159s/iter; left time: 9309.3799s\n",
      "\titers: 700, epoch: 3 | loss: 0.0825373\n",
      "\tspeed: 0.1158s/iter; left time: 9287.1080s\n",
      "\titers: 800, epoch: 3 | loss: 0.0860664\n",
      "\tspeed: 0.1155s/iter; left time: 9252.1500s\n",
      "\titers: 900, epoch: 3 | loss: 0.0782738\n",
      "\tspeed: 0.1165s/iter; left time: 9320.8989s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0969616\n",
      "\tspeed: 0.1144s/iter; left time: 9147.7179s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0766379\n",
      "\tspeed: 0.1160s/iter; left time: 9258.4540s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0865378\n",
      "\tspeed: 0.1171s/iter; left time: 9337.5920s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0749705\n",
      "\tspeed: 0.1167s/iter; left time: 9295.8511s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0806904\n",
      "\tspeed: 0.1173s/iter; left time: 9331.6695s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0795831\n",
      "\tspeed: 0.1159s/iter; left time: 9207.2889s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0955335\n",
      "\tspeed: 0.1153s/iter; left time: 9149.3765s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0916460\n",
      "\tspeed: 0.1150s/iter; left time: 9110.7891s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0964015\n",
      "\tspeed: 0.1154s/iter; left time: 9134.8030s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1013548\n",
      "\tspeed: 0.1146s/iter; left time: 9054.9745s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0869623\n",
      "\tspeed: 0.1157s/iter; left time: 9134.3805s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0899700\n",
      "\tspeed: 0.1149s/iter; left time: 9056.0083s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0860984\n",
      "\tspeed: 0.1153s/iter; left time: 9073.9541s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0814408\n",
      "\tspeed: 0.1150s/iter; left time: 9041.0419s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1089024\n",
      "\tspeed: 0.1159s/iter; left time: 9101.7565s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1013576\n",
      "\tspeed: 0.1147s/iter; left time: 8996.6029s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0859051\n",
      "\tspeed: 0.1149s/iter; left time: 9000.8520s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0691349\n",
      "\tspeed: 0.1153s/iter; left time: 9018.4082s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0932444\n",
      "\tspeed: 0.1172s/iter; left time: 9156.3923s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0649643\n",
      "\tspeed: 0.1152s/iter; left time: 8985.3477s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0698654\n",
      "\tspeed: 0.1167s/iter; left time: 9093.0110s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0942341\n",
      "\tspeed: 0.1162s/iter; left time: 9041.9563s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0748233\n",
      "\tspeed: 0.1176s/iter; left time: 9139.4906s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0922656\n",
      "\tspeed: 0.1159s/iter; left time: 8999.5240s\n",
      "\titers: 3400, epoch: 3 | loss: 0.0845802\n",
      "\tspeed: 0.1149s/iter; left time: 8910.5060s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0817511\n",
      "\tspeed: 0.1145s/iter; left time: 8868.7184s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0760563\n",
      "\tspeed: 0.1152s/iter; left time: 8910.8561s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1167071\n",
      "\tspeed: 0.1158s/iter; left time: 8946.6072s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0771297\n",
      "\tspeed: 0.1146s/iter; left time: 8839.2269s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0774214\n",
      "\tspeed: 0.1131s/iter; left time: 8712.6860s\n",
      "\titers: 4000, epoch: 3 | loss: 0.0806527\n",
      "\tspeed: 0.1131s/iter; left time: 8702.0080s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1028307\n",
      "\tspeed: 0.1151s/iter; left time: 8840.9925s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0881696\n",
      "\tspeed: 0.1156s/iter; left time: 8869.1441s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0913428\n",
      "\tspeed: 0.1154s/iter; left time: 8841.1989s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0837192\n",
      "\tspeed: 0.1158s/iter; left time: 8863.6025s\n",
      "Epoch: 3 cost time: 00h:08m:37.48s\n",
      "Epoch: 3 | Train Loss: 0.0860432 Vali Loss: 0.0918118 Test Loss: 0.1031213\n",
      "Validation loss decreased (0.094264 --> 0.091812).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0954896\n",
      "\tspeed: 1.5194s/iter; left time: 115982.9203s\n",
      "\titers: 200, epoch: 4 | loss: 0.0677183\n",
      "\tspeed: 0.1153s/iter; left time: 8792.1664s\n",
      "\titers: 300, epoch: 4 | loss: 0.0733540\n",
      "\tspeed: 0.1169s/iter; left time: 8903.2544s\n",
      "\titers: 400, epoch: 4 | loss: 0.0691343\n",
      "\tspeed: 0.1165s/iter; left time: 8855.6881s\n",
      "\titers: 500, epoch: 4 | loss: 0.0786122\n",
      "\tspeed: 0.1158s/iter; left time: 8793.4333s\n",
      "\titers: 600, epoch: 4 | loss: 0.0799889\n",
      "\tspeed: 0.1163s/iter; left time: 8821.3362s\n",
      "\titers: 700, epoch: 4 | loss: 0.0815678\n",
      "\tspeed: 0.1158s/iter; left time: 8770.7137s\n",
      "\titers: 800, epoch: 4 | loss: 0.0852794\n",
      "\tspeed: 0.1154s/iter; left time: 8730.4849s\n",
      "\titers: 900, epoch: 4 | loss: 0.1063726\n",
      "\tspeed: 0.1149s/iter; left time: 8676.3405s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0828676\n",
      "\tspeed: 0.1170s/iter; left time: 8826.2468s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0860450\n",
      "\tspeed: 0.1163s/iter; left time: 8757.7761s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0846064\n",
      "\tspeed: 0.1153s/iter; left time: 8677.9395s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0873416\n",
      "\tspeed: 0.1159s/iter; left time: 8705.9777s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0829189\n",
      "\tspeed: 0.1152s/iter; left time: 8641.3069s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0689046\n",
      "\tspeed: 0.1153s/iter; left time: 8636.5696s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0895178\n",
      "\tspeed: 0.1149s/iter; left time: 8601.3822s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0820116\n",
      "\tspeed: 0.1145s/iter; left time: 8555.8162s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0827952\n",
      "\tspeed: 0.1149s/iter; left time: 8571.8162s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1026970\n",
      "\tspeed: 0.1159s/iter; left time: 8640.1371s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0725404\n",
      "\tspeed: 0.1149s/iter; left time: 8555.6678s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0876920\n",
      "\tspeed: 0.1146s/iter; left time: 8520.9992s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1142400\n",
      "\tspeed: 0.1148s/iter; left time: 8518.8770s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0928669\n",
      "\tspeed: 0.1153s/iter; left time: 8550.5162s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0878178\n",
      "\tspeed: 0.1153s/iter; left time: 8533.5576s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0795425\n",
      "\tspeed: 0.1148s/iter; left time: 8490.3157s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0796704\n",
      "\tspeed: 0.1119s/iter; left time: 8262.1208s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0722193\n",
      "\tspeed: 0.1150s/iter; left time: 8476.6537s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0852612\n",
      "\tspeed: 0.1181s/iter; left time: 8693.9337s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0933903\n",
      "\tspeed: 0.1176s/iter; left time: 8645.8405s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0701374\n",
      "\tspeed: 0.1161s/iter; left time: 8525.0474s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1009073\n",
      "\tspeed: 0.1153s/iter; left time: 8457.3872s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0881226\n",
      "\tspeed: 0.1151s/iter; left time: 8425.6172s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0878723\n",
      "\tspeed: 0.1011s/iter; left time: 7397.3712s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0679823\n",
      "\tspeed: 0.1073s/iter; left time: 7839.9911s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0670110\n",
      "\tspeed: 0.1052s/iter; left time: 7669.5679s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1087624\n",
      "\tspeed: 0.1088s/iter; left time: 7922.7925s\n",
      "\titers: 3700, epoch: 4 | loss: 0.0843604\n",
      "\tspeed: 0.1083s/iter; left time: 7879.6771s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0843288\n",
      "\tspeed: 0.1079s/iter; left time: 7837.0638s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0889594\n",
      "\tspeed: 0.1070s/iter; left time: 7758.4626s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0709078\n",
      "\tspeed: 0.1089s/iter; left time: 7890.1776s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0785354\n",
      "\tspeed: 0.1110s/iter; left time: 8031.3928s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1014095\n",
      "\tspeed: 0.1110s/iter; left time: 8020.0764s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0900307\n",
      "\tspeed: 0.1059s/iter; left time: 7638.1102s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0985371\n",
      "\tspeed: 0.1094s/iter; left time: 7878.0186s\n",
      "Epoch: 4 cost time: 00h:08m:29.98s\n",
      "Epoch: 4 | Train Loss: 0.0849872 Vali Loss: 0.0910281 Test Loss: 0.1025915\n",
      "Validation loss decreased (0.091812 --> 0.091028).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0995415\n",
      "\tspeed: 1.5141s/iter; left time: 108771.3119s\n",
      "\titers: 200, epoch: 5 | loss: 0.0702179\n",
      "\tspeed: 0.1096s/iter; left time: 7862.5018s\n",
      "\titers: 300, epoch: 5 | loss: 0.0926327\n",
      "\tspeed: 0.1035s/iter; left time: 7416.0335s\n",
      "\titers: 400, epoch: 5 | loss: 0.0842026\n",
      "\tspeed: 0.1086s/iter; left time: 7768.5542s\n",
      "\titers: 500, epoch: 5 | loss: 0.0849711\n",
      "\tspeed: 0.1071s/iter; left time: 7651.4452s\n",
      "\titers: 600, epoch: 5 | loss: 0.0858472\n",
      "\tspeed: 0.1107s/iter; left time: 7895.3261s\n",
      "\titers: 700, epoch: 5 | loss: 0.0910149\n",
      "\tspeed: 0.1118s/iter; left time: 7961.6434s\n",
      "\titers: 800, epoch: 5 | loss: 0.0706106\n",
      "\tspeed: 0.1095s/iter; left time: 7791.4857s\n",
      "\titers: 900, epoch: 5 | loss: 0.0857396\n",
      "\tspeed: 0.1097s/iter; left time: 7792.7134s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0778304\n",
      "\tspeed: 0.1025s/iter; left time: 7269.0923s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0807632\n",
      "\tspeed: 0.1059s/iter; left time: 7499.7300s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0821455\n",
      "\tspeed: 0.1095s/iter; left time: 7742.7626s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0761781\n",
      "\tspeed: 0.1053s/iter; left time: 7436.2212s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0864105\n",
      "\tspeed: 0.1110s/iter; left time: 7829.8405s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0635816\n",
      "\tspeed: 0.1109s/iter; left time: 7811.5784s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0796172\n",
      "\tspeed: 0.1117s/iter; left time: 7858.3424s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0873830\n",
      "\tspeed: 0.1121s/iter; left time: 7870.4731s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0776808\n",
      "\tspeed: 0.1071s/iter; left time: 7515.0193s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0813784\n",
      "\tspeed: 0.1095s/iter; left time: 7671.4312s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0825241\n",
      "\tspeed: 0.1087s/iter; left time: 7604.7255s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0971226\n",
      "\tspeed: 0.1092s/iter; left time: 7627.9008s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0820535\n",
      "\tspeed: 0.1085s/iter; left time: 7569.6907s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0787062\n",
      "\tspeed: 0.1073s/iter; left time: 7475.1698s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0813770\n",
      "\tspeed: 0.1124s/iter; left time: 7818.5126s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0734397\n",
      "\tspeed: 0.1090s/iter; left time: 7565.5047s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0811693\n",
      "\tspeed: 0.1101s/iter; left time: 7636.4951s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0857649\n",
      "\tspeed: 0.1077s/iter; left time: 7454.6958s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0826442\n",
      "\tspeed: 0.1079s/iter; left time: 7463.0407s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0696512\n",
      "\tspeed: 0.1070s/iter; left time: 7390.0087s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0978351\n",
      "\tspeed: 0.1050s/iter; left time: 7237.8526s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0703570\n",
      "\tspeed: 0.1089s/iter; left time: 7497.3170s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0664298\n",
      "\tspeed: 0.1118s/iter; left time: 7682.5474s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0841569\n",
      "\tspeed: 0.1085s/iter; left time: 7450.3132s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0866328\n",
      "\tspeed: 0.1054s/iter; left time: 7222.5796s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0752724\n",
      "\tspeed: 0.1060s/iter; left time: 7254.2415s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0709817\n",
      "\tspeed: 0.1054s/iter; left time: 7202.8623s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0909998\n",
      "\tspeed: 0.1073s/iter; left time: 7322.7343s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0780020\n",
      "\tspeed: 0.1077s/iter; left time: 7339.9173s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0807215\n",
      "\tspeed: 0.1071s/iter; left time: 7288.4246s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0772602\n",
      "\tspeed: 0.1110s/iter; left time: 7539.4364s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0780594\n",
      "\tspeed: 0.1099s/iter; left time: 7454.3440s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0868828\n",
      "\tspeed: 0.1059s/iter; left time: 7173.3815s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0795707\n",
      "\tspeed: 0.1116s/iter; left time: 7546.4290s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0779867\n",
      "\tspeed: 0.1115s/iter; left time: 7530.1593s\n",
      "Epoch: 5 cost time: 00h:08m:08.52s\n",
      "Epoch: 5 | Train Loss: 0.0839314 Vali Loss: 0.0930410 Test Loss: 0.1055147\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0871269\n",
      "\tspeed: 1.4553s/iter; left time: 97999.8378s\n",
      "\titers: 200, epoch: 6 | loss: 0.0924439\n",
      "\tspeed: 0.1085s/iter; left time: 7298.9540s\n",
      "\titers: 300, epoch: 6 | loss: 0.0767670\n",
      "\tspeed: 0.1099s/iter; left time: 7376.6712s\n",
      "\titers: 400, epoch: 6 | loss: 0.0709401\n",
      "\tspeed: 0.1076s/iter; left time: 7210.7371s\n",
      "\titers: 500, epoch: 6 | loss: 0.0788817\n",
      "\tspeed: 0.1076s/iter; left time: 7204.0856s\n",
      "\titers: 600, epoch: 6 | loss: 0.0702770\n",
      "\tspeed: 0.1099s/iter; left time: 7345.0702s\n",
      "\titers: 700, epoch: 6 | loss: 0.0980311\n",
      "\tspeed: 0.1111s/iter; left time: 7412.3065s\n",
      "\titers: 800, epoch: 6 | loss: 0.0822494\n",
      "\tspeed: 0.1077s/iter; left time: 7176.0579s\n",
      "\titers: 900, epoch: 6 | loss: 0.0677797\n",
      "\tspeed: 0.1082s/iter; left time: 7200.6620s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0886040\n",
      "\tspeed: 0.1012s/iter; left time: 6723.6221s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0871691\n",
      "\tspeed: 0.1067s/iter; left time: 7080.8750s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0784157\n",
      "\tspeed: 0.1088s/iter; left time: 7206.3865s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0943549\n",
      "\tspeed: 0.1088s/iter; left time: 7197.4000s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0763766\n",
      "\tspeed: 0.1080s/iter; left time: 7134.1124s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0764059\n",
      "\tspeed: 0.1043s/iter; left time: 6874.9185s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0744005\n",
      "\tspeed: 0.1080s/iter; left time: 7110.8253s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0694969\n",
      "\tspeed: 0.1099s/iter; left time: 7228.1110s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0677618\n",
      "\tspeed: 0.1077s/iter; left time: 7071.5907s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0911437\n",
      "\tspeed: 0.1043s/iter; left time: 6835.3529s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0920015\n",
      "\tspeed: 0.1050s/iter; left time: 6870.2395s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0828903\n",
      "\tspeed: 0.1066s/iter; left time: 6963.6891s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0758576\n",
      "\tspeed: 0.1048s/iter; left time: 6838.2962s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0807692\n",
      "\tspeed: 0.1054s/iter; left time: 6865.9606s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0943944\n",
      "\tspeed: 0.1056s/iter; left time: 6869.0119s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0872433\n",
      "\tspeed: 0.1052s/iter; left time: 6829.1467s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0959055\n",
      "\tspeed: 0.1082s/iter; left time: 7018.1775s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0809270\n",
      "\tspeed: 0.1070s/iter; left time: 6927.1189s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0640975\n",
      "\tspeed: 0.1080s/iter; left time: 6980.9291s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0803174\n",
      "\tspeed: 0.1088s/iter; left time: 7021.9549s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0886245\n",
      "\tspeed: 0.1073s/iter; left time: 6912.7752s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0835691\n",
      "\tspeed: 0.1070s/iter; left time: 6882.9547s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0802454\n",
      "\tspeed: 0.1089s/iter; left time: 6996.6120s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0707895\n",
      "\tspeed: 0.1076s/iter; left time: 6898.5370s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0747536\n",
      "\tspeed: 0.1047s/iter; left time: 6703.4284s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1065182\n",
      "\tspeed: 0.1062s/iter; left time: 6789.0110s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0878420\n",
      "\tspeed: 0.1058s/iter; left time: 6755.5887s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0774292\n",
      "\tspeed: 0.1043s/iter; left time: 6645.3086s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1086048\n",
      "\tspeed: 0.1087s/iter; left time: 6918.0962s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0799593\n",
      "\tspeed: 0.1091s/iter; left time: 6931.3972s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1003824\n",
      "\tspeed: 0.1062s/iter; left time: 6738.8975s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0854066\n",
      "\tspeed: 0.1048s/iter; left time: 6637.6901s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0746893\n",
      "\tspeed: 0.1052s/iter; left time: 6651.1235s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0869709\n",
      "\tspeed: 0.1079s/iter; left time: 6813.7169s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0689251\n",
      "\tspeed: 0.1046s/iter; left time: 6591.0560s\n",
      "Epoch: 6 cost time: 00h:08m:02.06s\n",
      "Epoch: 6 | Train Loss: 0.0833013 Vali Loss: 0.0915693 Test Loss: 0.1043275\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0705436\n",
      "\tspeed: 1.4515s/iter; left time: 91216.5162s\n",
      "\titers: 200, epoch: 7 | loss: 0.0841148\n",
      "\tspeed: 0.1086s/iter; left time: 6812.7254s\n",
      "\titers: 300, epoch: 7 | loss: 0.0840711\n",
      "\tspeed: 0.1057s/iter; left time: 6619.8859s\n",
      "\titers: 400, epoch: 7 | loss: 0.0786994\n",
      "\tspeed: 0.1054s/iter; left time: 6594.1110s\n",
      "\titers: 500, epoch: 7 | loss: 0.0765329\n",
      "\tspeed: 0.1063s/iter; left time: 6635.6774s\n",
      "\titers: 600, epoch: 7 | loss: 0.0759210\n",
      "\tspeed: 0.1084s/iter; left time: 6759.8873s\n",
      "\titers: 700, epoch: 7 | loss: 0.1062268\n",
      "\tspeed: 0.1092s/iter; left time: 6798.3646s\n",
      "\titers: 800, epoch: 7 | loss: 0.0814249\n",
      "\tspeed: 0.1101s/iter; left time: 6839.5005s\n",
      "\titers: 900, epoch: 7 | loss: 0.0928968\n",
      "\tspeed: 0.1049s/iter; left time: 6510.9280s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0790363\n",
      "\tspeed: 0.1078s/iter; left time: 6675.1641s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0801770\n",
      "\tspeed: 0.1099s/iter; left time: 6794.7556s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0799099\n",
      "\tspeed: 0.1067s/iter; left time: 6590.6523s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1077784\n",
      "\tspeed: 0.1079s/iter; left time: 6651.2303s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0819536\n",
      "\tspeed: 0.1085s/iter; left time: 6676.5453s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0709243\n",
      "\tspeed: 0.1049s/iter; left time: 6448.0119s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0715297\n",
      "\tspeed: 0.1081s/iter; left time: 6629.1512s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0753313\n",
      "\tspeed: 0.1093s/iter; left time: 6696.5055s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0870889\n",
      "\tspeed: 0.1102s/iter; left time: 6739.3079s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0596218\n",
      "\tspeed: 0.1086s/iter; left time: 6630.2702s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0800639\n",
      "\tspeed: 0.1082s/iter; left time: 6594.2838s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0774808\n",
      "\tspeed: 0.1067s/iter; left time: 6494.8406s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0776948\n",
      "\tspeed: 0.1076s/iter; left time: 6533.1254s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0848682\n",
      "\tspeed: 0.1044s/iter; left time: 6332.0964s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0881115\n",
      "\tspeed: 0.1089s/iter; left time: 6590.3729s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0899723\n",
      "\tspeed: 0.1089s/iter; left time: 6582.7269s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0730382\n",
      "\tspeed: 0.1074s/iter; left time: 6480.5943s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0763565\n",
      "\tspeed: 0.1096s/iter; left time: 6602.4794s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0803490\n",
      "\tspeed: 0.1102s/iter; left time: 6626.0847s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0798958\n",
      "\tspeed: 0.1091s/iter; left time: 6551.9400s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0765674\n",
      "\tspeed: 0.1093s/iter; left time: 6552.8049s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0898325\n",
      "\tspeed: 0.1057s/iter; left time: 6323.5382s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0746160\n",
      "\tspeed: 0.1094s/iter; left time: 6537.2984s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0873410\n",
      "\tspeed: 0.1089s/iter; left time: 6496.4498s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0789864\n",
      "\tspeed: 0.1070s/iter; left time: 6369.8082s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0850723\n",
      "\tspeed: 0.1098s/iter; left time: 6526.5437s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0945234\n",
      "\tspeed: 0.1103s/iter; left time: 6544.1685s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0540937\n",
      "\tspeed: 0.1099s/iter; left time: 6512.3603s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0729968\n",
      "\tspeed: 0.1103s/iter; left time: 6525.2250s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0832610\n",
      "\tspeed: 0.1082s/iter; left time: 6386.7499s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0856799\n",
      "\tspeed: 0.1079s/iter; left time: 6358.0768s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0843669\n",
      "\tspeed: 0.1104s/iter; left time: 6494.3987s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0647494\n",
      "\tspeed: 0.1095s/iter; left time: 6429.6583s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0789265\n",
      "\tspeed: 0.1112s/iter; left time: 6520.6726s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0709903\n",
      "\tspeed: 0.1100s/iter; left time: 6441.0770s\n",
      "Epoch: 7 cost time: 00h:08m:08.20s\n",
      "Epoch: 7 | Train Loss: 0.0827715 Vali Loss: 0.0898659 Test Loss: 0.1015000\n",
      "Validation loss decreased (0.091028 --> 0.089866).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0960936\n",
      "\tspeed: 1.5094s/iter; left time: 88072.7849s\n",
      "\titers: 200, epoch: 8 | loss: 0.0740620\n",
      "\tspeed: 0.1050s/iter; left time: 6115.0676s\n",
      "\titers: 300, epoch: 8 | loss: 0.0876887\n",
      "\tspeed: 0.1119s/iter; left time: 6506.2064s\n",
      "\titers: 400, epoch: 8 | loss: 0.0733641\n",
      "\tspeed: 0.1081s/iter; left time: 6274.1387s\n",
      "\titers: 500, epoch: 8 | loss: 0.0889757\n",
      "\tspeed: 0.1083s/iter; left time: 6274.7419s\n",
      "\titers: 600, epoch: 8 | loss: 0.1005570\n",
      "\tspeed: 0.1054s/iter; left time: 6094.9406s\n",
      "\titers: 700, epoch: 8 | loss: 0.0792509\n",
      "\tspeed: 0.1126s/iter; left time: 6501.3859s\n",
      "\titers: 800, epoch: 8 | loss: 0.0805020\n",
      "\tspeed: 0.1068s/iter; left time: 6159.3257s\n",
      "\titers: 900, epoch: 8 | loss: 0.0761237\n",
      "\tspeed: 0.1087s/iter; left time: 6256.3690s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0728325\n",
      "\tspeed: 0.1060s/iter; left time: 6089.1869s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0897990\n",
      "\tspeed: 0.1098s/iter; left time: 6298.3499s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0857851\n",
      "\tspeed: 0.1108s/iter; left time: 6343.7974s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0711524\n",
      "\tspeed: 0.1111s/iter; left time: 6349.4167s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0887766\n",
      "\tspeed: 0.1093s/iter; left time: 6236.5120s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0786460\n",
      "\tspeed: 0.1121s/iter; left time: 6382.6439s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0860998\n",
      "\tspeed: 0.1038s/iter; left time: 5898.5450s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0714980\n",
      "\tspeed: 0.1081s/iter; left time: 6132.1933s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0794795\n",
      "\tspeed: 0.1082s/iter; left time: 6129.5536s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0873640\n",
      "\tspeed: 0.1103s/iter; left time: 6237.7933s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0895032\n",
      "\tspeed: 0.1057s/iter; left time: 5968.5749s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0804572\n",
      "\tspeed: 0.1046s/iter; left time: 5894.8727s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0963350\n",
      "\tspeed: 0.1049s/iter; left time: 5902.4230s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0714696\n",
      "\tspeed: 0.1079s/iter; left time: 6058.4630s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0926667\n",
      "\tspeed: 0.1065s/iter; left time: 5968.7561s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0717168\n",
      "\tspeed: 0.1051s/iter; left time: 5877.8041s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0704690\n",
      "\tspeed: 0.1094s/iter; left time: 6109.9223s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1047586\n",
      "\tspeed: 0.1073s/iter; left time: 5979.3563s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0844329\n",
      "\tspeed: 0.1085s/iter; left time: 6040.4353s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0658040\n",
      "\tspeed: 0.1090s/iter; left time: 6054.4304s\n",
      "\titers: 3000, epoch: 8 | loss: 0.0744485\n",
      "\tspeed: 0.1104s/iter; left time: 6120.4387s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0894325\n",
      "\tspeed: 0.1031s/iter; left time: 5706.3880s\n",
      "\titers: 3200, epoch: 8 | loss: 0.0886287\n",
      "\tspeed: 0.1051s/iter; left time: 5809.2085s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0864769\n",
      "\tspeed: 0.1071s/iter; left time: 5905.0952s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0689103\n",
      "\tspeed: 0.1057s/iter; left time: 5816.6354s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0874014\n",
      "\tspeed: 0.1106s/iter; left time: 6074.7154s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0792102\n",
      "\tspeed: 0.1088s/iter; left time: 5966.3641s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0755558\n",
      "\tspeed: 0.1102s/iter; left time: 6032.5168s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0958932\n",
      "\tspeed: 0.1094s/iter; left time: 5978.0712s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0898063\n",
      "\tspeed: 0.1048s/iter; left time: 5715.3486s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0988585\n",
      "\tspeed: 0.1072s/iter; left time: 5835.3402s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0759023\n",
      "\tspeed: 0.1122s/iter; left time: 6098.8360s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0791041\n",
      "\tspeed: 0.1084s/iter; left time: 5878.6637s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0657694\n",
      "\tspeed: 0.1105s/iter; left time: 5985.4459s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0909220\n",
      "\tspeed: 0.1087s/iter; left time: 5873.7307s\n",
      "Epoch: 8 cost time: 00h:08m:06.34s\n",
      "Epoch: 8 | Train Loss: 0.0823500 Vali Loss: 0.0910530 Test Loss: 0.1034771\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0831056\n",
      "\tspeed: 1.4514s/iter; left time: 78163.0226s\n",
      "\titers: 200, epoch: 9 | loss: 0.0952516\n",
      "\tspeed: 0.1094s/iter; left time: 5881.2074s\n",
      "\titers: 300, epoch: 9 | loss: 0.0744533\n",
      "\tspeed: 0.1068s/iter; left time: 5732.5674s\n",
      "\titers: 400, epoch: 9 | loss: 0.0850896\n",
      "\tspeed: 0.1049s/iter; left time: 5620.3388s\n",
      "\titers: 500, epoch: 9 | loss: 0.0550304\n",
      "\tspeed: 0.1058s/iter; left time: 5652.9427s\n",
      "\titers: 600, epoch: 9 | loss: 0.0789448\n",
      "\tspeed: 0.1081s/iter; left time: 5769.2775s\n",
      "\titers: 700, epoch: 9 | loss: 0.0687364\n",
      "\tspeed: 0.1090s/iter; left time: 5802.5814s\n",
      "\titers: 800, epoch: 9 | loss: 0.0792406\n",
      "\tspeed: 0.1101s/iter; left time: 5850.5956s\n",
      "\titers: 900, epoch: 9 | loss: 0.0835051\n",
      "\tspeed: 0.1086s/iter; left time: 5759.2836s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0730056\n",
      "\tspeed: 0.1064s/iter; left time: 5632.9452s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0977784\n",
      "\tspeed: 0.1065s/iter; left time: 5629.5731s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0776352\n",
      "\tspeed: 0.1103s/iter; left time: 5816.3402s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0933063\n",
      "\tspeed: 0.1051s/iter; left time: 5533.8348s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0774592\n",
      "\tspeed: 0.1051s/iter; left time: 5521.6245s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0733840\n",
      "\tspeed: 0.1095s/iter; left time: 5742.2190s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0761116\n",
      "\tspeed: 0.1027s/iter; left time: 5376.5107s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0698281\n",
      "\tspeed: 0.1076s/iter; left time: 5623.5312s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0795811\n",
      "\tspeed: 0.1061s/iter; left time: 5533.4007s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0838274\n",
      "\tspeed: 0.1069s/iter; left time: 5564.5762s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0694626\n",
      "\tspeed: 0.1096s/iter; left time: 5693.9632s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0945157\n",
      "\tspeed: 0.1065s/iter; left time: 5522.7097s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0914454\n",
      "\tspeed: 0.1068s/iter; left time: 5524.8859s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0576647\n",
      "\tspeed: 0.1093s/iter; left time: 5646.5105s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0809036\n",
      "\tspeed: 0.1080s/iter; left time: 5565.1956s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0779389\n",
      "\tspeed: 0.1088s/iter; left time: 5597.4147s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0879130\n",
      "\tspeed: 0.1061s/iter; left time: 5450.2346s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0842345\n",
      "\tspeed: 0.1090s/iter; left time: 5585.2469s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0857091\n",
      "\tspeed: 0.1087s/iter; left time: 5558.0092s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0836044\n",
      "\tspeed: 0.1111s/iter; left time: 5673.4564s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0857766\n",
      "\tspeed: 0.1081s/iter; left time: 5508.9101s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0825322\n",
      "\tspeed: 0.1088s/iter; left time: 5532.1036s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0827652\n",
      "\tspeed: 0.1090s/iter; left time: 5534.3862s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0923525\n",
      "\tspeed: 0.1109s/iter; left time: 5618.1983s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0894057\n",
      "\tspeed: 0.1113s/iter; left time: 5627.0260s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0685351\n",
      "\tspeed: 0.1100s/iter; left time: 5548.8322s\n",
      "\titers: 3600, epoch: 9 | loss: 0.1009678\n",
      "\tspeed: 0.1096s/iter; left time: 5520.7287s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0932600\n",
      "\tspeed: 0.1091s/iter; left time: 5481.8443s\n",
      "\titers: 3800, epoch: 9 | loss: 0.0711512\n",
      "\tspeed: 0.1107s/iter; left time: 5551.6595s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0905252\n",
      "\tspeed: 0.1082s/iter; left time: 5417.3414s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0613631\n",
      "\tspeed: 0.1092s/iter; left time: 5454.7988s\n",
      "\titers: 4100, epoch: 9 | loss: 0.1000974\n",
      "\tspeed: 0.1096s/iter; left time: 5464.4524s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0895644\n",
      "\tspeed: 0.1071s/iter; left time: 5329.8063s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0789975\n",
      "\tspeed: 0.1096s/iter; left time: 5440.5652s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0666418\n",
      "\tspeed: 0.1103s/iter; left time: 5466.5605s\n",
      "Epoch: 9 cost time: 00h:08m:07.22s\n",
      "Epoch: 9 | Train Loss: 0.0819719 Vali Loss: 0.0893733 Test Loss: 0.1011986\n",
      "Validation loss decreased (0.089866 --> 0.089373).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0804666\n",
      "\tspeed: 1.4863s/iter; left time: 73357.6544s\n",
      "\titers: 200, epoch: 10 | loss: 0.0945602\n",
      "\tspeed: 0.1068s/iter; left time: 5262.0357s\n",
      "\titers: 300, epoch: 10 | loss: 0.0714898\n",
      "\tspeed: 0.1039s/iter; left time: 5109.3410s\n",
      "\titers: 400, epoch: 10 | loss: 0.0826127\n",
      "\tspeed: 0.1065s/iter; left time: 5226.0831s\n",
      "\titers: 500, epoch: 10 | loss: 0.0924424\n",
      "\tspeed: 0.1054s/iter; left time: 5159.2442s\n",
      "\titers: 600, epoch: 10 | loss: 0.0706647\n",
      "\tspeed: 0.1053s/iter; left time: 5144.5541s\n",
      "\titers: 700, epoch: 10 | loss: 0.0966906\n",
      "\tspeed: 0.1032s/iter; left time: 5032.6570s\n",
      "\titers: 800, epoch: 10 | loss: 0.0714759\n",
      "\tspeed: 0.1039s/iter; left time: 5055.0802s\n",
      "\titers: 900, epoch: 10 | loss: 0.0955755\n",
      "\tspeed: 0.1062s/iter; left time: 5158.1279s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0649942\n",
      "\tspeed: 0.1036s/iter; left time: 5019.2797s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0779102\n",
      "\tspeed: 0.1044s/iter; left time: 5050.6463s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0758527\n",
      "\tspeed: 0.1072s/iter; left time: 5174.5789s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0841196\n",
      "\tspeed: 0.1065s/iter; left time: 5130.6830s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0752938\n",
      "\tspeed: 0.1070s/iter; left time: 5141.2591s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0925946\n",
      "\tspeed: 0.1082s/iter; left time: 5189.7579s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0713231\n",
      "\tspeed: 0.1038s/iter; left time: 4969.1839s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0798582\n",
      "\tspeed: 0.1127s/iter; left time: 5380.6080s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0845527\n",
      "\tspeed: 0.1071s/iter; left time: 5105.6895s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0802245\n",
      "\tspeed: 0.1076s/iter; left time: 5118.1966s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0924563\n",
      "\tspeed: 0.1104s/iter; left time: 5237.8094s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0730820\n",
      "\tspeed: 0.1012s/iter; left time: 4792.8719s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0877439\n",
      "\tspeed: 0.1074s/iter; left time: 5076.2265s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0808664\n",
      "\tspeed: 0.1048s/iter; left time: 4940.3052s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0805754\n",
      "\tspeed: 0.1059s/iter; left time: 4983.9050s\n",
      "\titers: 2500, epoch: 10 | loss: 0.1015715\n",
      "\tspeed: 0.1105s/iter; left time: 5190.6395s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0852922\n",
      "\tspeed: 0.1080s/iter; left time: 5059.2391s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0820987\n",
      "\tspeed: 0.1081s/iter; left time: 5054.6425s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0819424\n",
      "\tspeed: 0.1097s/iter; left time: 5116.0070s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0815215\n",
      "\tspeed: 0.1092s/iter; left time: 5083.0218s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0779925\n",
      "\tspeed: 0.1052s/iter; left time: 4888.9427s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0649120\n",
      "\tspeed: 0.1102s/iter; left time: 5108.8914s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0969833\n",
      "\tspeed: 0.1060s/iter; left time: 4901.9683s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0975729\n",
      "\tspeed: 0.1111s/iter; left time: 5128.5113s\n",
      "\titers: 3400, epoch: 10 | loss: 0.0759113\n",
      "\tspeed: 0.1112s/iter; left time: 5121.4709s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0715975\n",
      "\tspeed: 0.1098s/iter; left time: 5048.0698s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0765458\n",
      "\tspeed: 0.1084s/iter; left time: 4972.8561s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0926370\n",
      "\tspeed: 0.1044s/iter; left time: 4778.0693s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0954896\n",
      "\tspeed: 0.1069s/iter; left time: 4880.9876s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0901705\n",
      "\tspeed: 0.1108s/iter; left time: 5048.6446s\n",
      "\titers: 4000, epoch: 10 | loss: 0.1011319\n",
      "\tspeed: 0.1098s/iter; left time: 4990.8753s\n",
      "\titers: 4100, epoch: 10 | loss: 0.0829659\n",
      "\tspeed: 0.1103s/iter; left time: 5002.8850s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0952567\n",
      "\tspeed: 0.1095s/iter; left time: 4953.7826s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0830213\n",
      "\tspeed: 0.1058s/iter; left time: 4779.4080s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0838863\n",
      "\tspeed: 0.1099s/iter; left time: 4949.9624s\n",
      "Epoch: 10 cost time: 00h:08m:03.38s\n",
      "Epoch: 10 | Train Loss: 0.0815905 Vali Loss: 0.0900862 Test Loss: 0.1020951\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0954875\n",
      "\tspeed: 1.4550s/iter; left time: 65271.2623s\n",
      "\titers: 200, epoch: 11 | loss: 0.0963455\n",
      "\tspeed: 0.1072s/iter; left time: 4799.7547s\n",
      "\titers: 300, epoch: 11 | loss: 0.0792705\n",
      "\tspeed: 0.1024s/iter; left time: 4574.7534s\n",
      "\titers: 400, epoch: 11 | loss: 0.0801558\n",
      "\tspeed: 0.1040s/iter; left time: 4635.7560s\n",
      "\titers: 500, epoch: 11 | loss: 0.1015538\n",
      "\tspeed: 0.1042s/iter; left time: 4634.8272s\n",
      "\titers: 600, epoch: 11 | loss: 0.0732061\n",
      "\tspeed: 0.1071s/iter; left time: 4751.7992s\n",
      "\titers: 700, epoch: 11 | loss: 0.0891110\n",
      "\tspeed: 0.1045s/iter; left time: 4625.1676s\n",
      "\titers: 800, epoch: 11 | loss: 0.0999442\n",
      "\tspeed: 0.1068s/iter; left time: 4717.5128s\n",
      "\titers: 900, epoch: 11 | loss: 0.0800853\n",
      "\tspeed: 0.1101s/iter; left time: 4852.0515s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0720467\n",
      "\tspeed: 0.1090s/iter; left time: 4793.5842s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0852719\n",
      "\tspeed: 0.1118s/iter; left time: 4905.7622s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0710463\n",
      "\tspeed: 0.1097s/iter; left time: 4799.9479s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0742209\n",
      "\tspeed: 0.1043s/iter; left time: 4554.0590s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0696262\n",
      "\tspeed: 0.1074s/iter; left time: 4679.7211s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0862678\n",
      "\tspeed: 0.1109s/iter; left time: 4818.6020s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0709689\n",
      "\tspeed: 0.1081s/iter; left time: 4687.5179s\n",
      "\titers: 1700, epoch: 11 | loss: 0.1065298\n",
      "\tspeed: 0.1079s/iter; left time: 4665.8927s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0639064\n",
      "\tspeed: 0.1081s/iter; left time: 4666.2391s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0827991\n",
      "\tspeed: 0.1073s/iter; left time: 4620.0134s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0793500\n",
      "\tspeed: 0.1103s/iter; left time: 4736.9666s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0814809\n",
      "\tspeed: 0.1079s/iter; left time: 4623.5706s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0918499\n",
      "\tspeed: 0.1014s/iter; left time: 4335.5316s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0911202\n",
      "\tspeed: 0.1064s/iter; left time: 4538.3178s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0762489\n",
      "\tspeed: 0.1056s/iter; left time: 4493.1566s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0705035\n",
      "\tspeed: 0.1079s/iter; left time: 4580.0884s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0713519\n",
      "\tspeed: 0.1070s/iter; left time: 4533.2017s\n",
      "\titers: 2700, epoch: 11 | loss: 0.0821046\n",
      "\tspeed: 0.1100s/iter; left time: 4648.8504s\n",
      "\titers: 2800, epoch: 11 | loss: 0.0885580\n",
      "\tspeed: 0.1076s/iter; left time: 4538.2544s\n",
      "\titers: 2900, epoch: 11 | loss: 0.1035070\n",
      "\tspeed: 0.1077s/iter; left time: 4528.9081s\n",
      "\titers: 3000, epoch: 11 | loss: 0.0753226\n",
      "\tspeed: 0.1110s/iter; left time: 4658.2426s\n",
      "\titers: 3100, epoch: 11 | loss: 0.0714463\n",
      "\tspeed: 0.1099s/iter; left time: 4602.4249s\n",
      "\titers: 3200, epoch: 11 | loss: 0.0673974\n",
      "\tspeed: 0.1101s/iter; left time: 4595.9124s\n",
      "\titers: 3300, epoch: 11 | loss: 0.0771680\n",
      "\tspeed: 0.1084s/iter; left time: 4517.1873s\n",
      "\titers: 3400, epoch: 11 | loss: 0.1020880\n",
      "\tspeed: 0.1084s/iter; left time: 4504.3102s\n",
      "\titers: 3500, epoch: 11 | loss: 0.0695742\n",
      "\tspeed: 0.1058s/iter; left time: 4386.2004s\n",
      "\titers: 3600, epoch: 11 | loss: 0.0783400\n",
      "\tspeed: 0.1074s/iter; left time: 4440.7268s\n",
      "\titers: 3700, epoch: 11 | loss: 0.0872828\n",
      "\tspeed: 0.1115s/iter; left time: 4599.9908s\n",
      "\titers: 3800, epoch: 11 | loss: 0.1151122\n",
      "\tspeed: 0.1067s/iter; left time: 4391.6117s\n",
      "\titers: 3900, epoch: 11 | loss: 0.0821990\n",
      "\tspeed: 0.1072s/iter; left time: 4402.9774s\n",
      "\titers: 4000, epoch: 11 | loss: 0.0874506\n",
      "\tspeed: 0.1060s/iter; left time: 4343.0573s\n",
      "\titers: 4100, epoch: 11 | loss: 0.0895397\n",
      "\tspeed: 0.1070s/iter; left time: 4370.8742s\n",
      "\titers: 4200, epoch: 11 | loss: 0.0791727\n",
      "\tspeed: 0.1031s/iter; left time: 4203.4028s\n",
      "\titers: 4300, epoch: 11 | loss: 0.0826887\n",
      "\tspeed: 0.1092s/iter; left time: 4438.7178s\n",
      "\titers: 4400, epoch: 11 | loss: 0.0772736\n",
      "\tspeed: 0.1077s/iter; left time: 4369.1912s\n",
      "Epoch: 11 cost time: 00h:08m:04.10s\n",
      "Epoch: 11 | Train Loss: 0.0812005 Vali Loss: 0.0898660 Test Loss: 0.1020890\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0749976\n",
      "\tspeed: 1.4556s/iter; left time: 58756.9022s\n",
      "\titers: 200, epoch: 12 | loss: 0.0952725\n",
      "\tspeed: 0.1065s/iter; left time: 4289.6273s\n",
      "\titers: 300, epoch: 12 | loss: 0.0830348\n",
      "\tspeed: 0.1096s/iter; left time: 4400.5693s\n",
      "\titers: 400, epoch: 12 | loss: 0.0752531\n",
      "\tspeed: 0.1069s/iter; left time: 4280.9562s\n",
      "\titers: 500, epoch: 12 | loss: 0.0754286\n",
      "\tspeed: 0.1077s/iter; left time: 4304.1247s\n",
      "\titers: 600, epoch: 12 | loss: 0.0688584\n",
      "\tspeed: 0.1023s/iter; left time: 4079.0040s\n",
      "\titers: 700, epoch: 12 | loss: 0.0959318\n",
      "\tspeed: 0.1121s/iter; left time: 4458.9278s\n",
      "\titers: 800, epoch: 12 | loss: 0.0900387\n",
      "\tspeed: 0.1073s/iter; left time: 4256.0677s\n",
      "\titers: 900, epoch: 12 | loss: 0.0738986\n",
      "\tspeed: 0.1089s/iter; left time: 4308.1859s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0676233\n",
      "\tspeed: 0.1106s/iter; left time: 4365.5835s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0989304\n",
      "\tspeed: 0.1078s/iter; left time: 4244.1168s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0711347\n",
      "\tspeed: 0.1088s/iter; left time: 4271.1816s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0813206\n",
      "\tspeed: 0.1084s/iter; left time: 4243.7253s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0788712\n",
      "\tspeed: 0.1089s/iter; left time: 4254.6615s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0956492\n",
      "\tspeed: 0.1081s/iter; left time: 4213.0126s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0699590\n",
      "\tspeed: 0.1080s/iter; left time: 4196.8728s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0868768\n",
      "\tspeed: 0.1063s/iter; left time: 4120.3310s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0729411\n",
      "\tspeed: 0.1082s/iter; left time: 4181.6406s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0975851\n",
      "\tspeed: 0.1111s/iter; left time: 4283.0566s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0993183\n",
      "\tspeed: 0.1085s/iter; left time: 4171.9905s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0732370\n",
      "\tspeed: 0.1108s/iter; left time: 4251.6781s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0761602\n",
      "\tspeed: 0.1122s/iter; left time: 4293.5015s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0807065\n",
      "\tspeed: 0.1080s/iter; left time: 4121.3202s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0800232\n",
      "\tspeed: 0.1035s/iter; left time: 3939.6190s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0855232\n",
      "\tspeed: 0.1056s/iter; left time: 4009.5346s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0662207\n",
      "\tspeed: 0.1056s/iter; left time: 3997.0229s\n",
      "\titers: 2700, epoch: 12 | loss: 0.0743456\n",
      "\tspeed: 0.1110s/iter; left time: 4192.5218s\n",
      "\titers: 2800, epoch: 12 | loss: 0.0744488\n",
      "\tspeed: 0.1060s/iter; left time: 3993.9229s\n",
      "\titers: 2900, epoch: 12 | loss: 0.0875774\n",
      "\tspeed: 0.1047s/iter; left time: 3933.6907s\n",
      "\titers: 3000, epoch: 12 | loss: 0.0959495\n",
      "\tspeed: 0.1060s/iter; left time: 3971.4837s\n",
      "\titers: 3100, epoch: 12 | loss: 0.0766992\n",
      "\tspeed: 0.1080s/iter; left time: 4036.6462s\n",
      "\titers: 3200, epoch: 12 | loss: 0.1069204\n",
      "\tspeed: 0.1120s/iter; left time: 4174.6158s\n",
      "\titers: 3300, epoch: 12 | loss: 0.0903072\n",
      "\tspeed: 0.1115s/iter; left time: 4142.5623s\n",
      "\titers: 3400, epoch: 12 | loss: 0.0947618\n",
      "\tspeed: 0.1073s/iter; left time: 3978.7131s\n",
      "\titers: 3500, epoch: 12 | loss: 0.0750581\n",
      "\tspeed: 0.1036s/iter; left time: 3831.4114s\n",
      "\titers: 3600, epoch: 12 | loss: 0.0677229\n",
      "\tspeed: 0.1084s/iter; left time: 3997.1009s\n",
      "\titers: 3700, epoch: 12 | loss: 0.0825931\n",
      "\tspeed: 0.1081s/iter; left time: 3973.8821s\n",
      "\titers: 3800, epoch: 12 | loss: 0.0663846\n",
      "\tspeed: 0.1060s/iter; left time: 3884.6832s\n",
      "\titers: 3900, epoch: 12 | loss: 0.0917371\n",
      "\tspeed: 0.1075s/iter; left time: 3931.7501s\n",
      "\titers: 4000, epoch: 12 | loss: 0.0844175\n",
      "\tspeed: 0.1066s/iter; left time: 3887.4367s\n",
      "\titers: 4100, epoch: 12 | loss: 0.0886744\n",
      "\tspeed: 0.1089s/iter; left time: 3958.6021s\n",
      "\titers: 4200, epoch: 12 | loss: 0.0929067\n",
      "\tspeed: 0.1079s/iter; left time: 3912.8887s\n",
      "\titers: 4300, epoch: 12 | loss: 0.0852121\n",
      "\tspeed: 0.1067s/iter; left time: 3860.3138s\n",
      "\titers: 4400, epoch: 12 | loss: 0.0680903\n",
      "\tspeed: 0.1112s/iter; left time: 4011.7576s\n",
      "Epoch: 12 cost time: 00h:08m:05.89s\n",
      "Epoch: 12 | Train Loss: 0.0808781 Vali Loss: 0.0895198 Test Loss: 0.1023670\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0941976\n",
      "\tspeed: 1.4546s/iter; left time: 52174.8459s\n",
      "\titers: 200, epoch: 13 | loss: 0.0721684\n",
      "\tspeed: 0.1104s/iter; left time: 3947.4954s\n",
      "\titers: 300, epoch: 13 | loss: 0.0847256\n",
      "\tspeed: 0.1083s/iter; left time: 3864.1808s\n",
      "\titers: 400, epoch: 13 | loss: 0.1075728\n",
      "\tspeed: 0.1071s/iter; left time: 3810.8300s\n",
      "\titers: 500, epoch: 13 | loss: 0.0736173\n",
      "\tspeed: 0.1069s/iter; left time: 3793.3650s\n",
      "\titers: 600, epoch: 13 | loss: 0.0811842\n",
      "\tspeed: 0.1108s/iter; left time: 3918.6544s\n",
      "\titers: 700, epoch: 13 | loss: 0.0647019\n",
      "\tspeed: 0.1064s/iter; left time: 3753.0734s\n",
      "\titers: 800, epoch: 13 | loss: 0.1009579\n",
      "\tspeed: 0.1078s/iter; left time: 3790.5590s\n",
      "\titers: 900, epoch: 13 | loss: 0.0781825\n",
      "\tspeed: 0.1050s/iter; left time: 3682.7212s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0830416\n",
      "\tspeed: 0.1079s/iter; left time: 3774.3421s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0865278\n",
      "\tspeed: 0.1098s/iter; left time: 3828.0344s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0713144\n",
      "\tspeed: 0.1072s/iter; left time: 3725.7387s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0747054\n",
      "\tspeed: 0.1075s/iter; left time: 3726.8043s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0833910\n",
      "\tspeed: 0.1099s/iter; left time: 3798.9485s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0931288\n",
      "\tspeed: 0.1079s/iter; left time: 3718.0445s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0980143\n",
      "\tspeed: 0.1069s/iter; left time: 3675.7590s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0957459\n",
      "\tspeed: 0.1097s/iter; left time: 3758.7045s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0775483\n",
      "\tspeed: 0.1064s/iter; left time: 3634.4467s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0813203\n",
      "\tspeed: 0.1075s/iter; left time: 3663.9863s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0666705\n",
      "\tspeed: 0.1049s/iter; left time: 3562.7860s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0789099\n",
      "\tspeed: 0.1056s/iter; left time: 3577.9141s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0800629\n",
      "\tspeed: 0.1114s/iter; left time: 3760.1997s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0889463\n",
      "\tspeed: 0.1077s/iter; left time: 3625.9342s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0802381\n",
      "\tspeed: 0.1075s/iter; left time: 3609.9553s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0825546\n",
      "\tspeed: 0.1083s/iter; left time: 3625.2111s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0956851\n",
      "\tspeed: 0.1044s/iter; left time: 3485.0140s\n",
      "\titers: 2700, epoch: 13 | loss: 0.0856287\n",
      "\tspeed: 0.1031s/iter; left time: 3429.1250s\n",
      "\titers: 2800, epoch: 13 | loss: 0.1151457\n",
      "\tspeed: 0.1070s/iter; left time: 3549.4356s\n",
      "\titers: 2900, epoch: 13 | loss: 0.0953493\n",
      "\tspeed: 0.1076s/iter; left time: 3558.8930s\n",
      "\titers: 3000, epoch: 13 | loss: 0.0769377\n",
      "\tspeed: 0.1126s/iter; left time: 3712.9364s\n",
      "\titers: 3100, epoch: 13 | loss: 0.0727554\n",
      "\tspeed: 0.1106s/iter; left time: 3635.8976s\n",
      "\titers: 3200, epoch: 13 | loss: 0.0860205\n",
      "\tspeed: 0.1076s/iter; left time: 3526.6133s\n",
      "\titers: 3300, epoch: 13 | loss: 0.0888060\n",
      "\tspeed: 0.1038s/iter; left time: 3389.9225s\n",
      "\titers: 3400, epoch: 13 | loss: 0.0835236\n",
      "\tspeed: 0.1047s/iter; left time: 3410.0364s\n",
      "\titers: 3500, epoch: 13 | loss: 0.0703757\n",
      "\tspeed: 0.1057s/iter; left time: 3431.4315s\n",
      "\titers: 3600, epoch: 13 | loss: 0.0632925\n",
      "\tspeed: 0.1075s/iter; left time: 3478.4818s\n",
      "\titers: 3700, epoch: 13 | loss: 0.0658673\n",
      "\tspeed: 0.1066s/iter; left time: 3440.8265s\n",
      "\titers: 3800, epoch: 13 | loss: 0.0704690\n",
      "\tspeed: 0.1087s/iter; left time: 3498.0750s\n",
      "\titers: 3900, epoch: 13 | loss: 0.0829609\n",
      "\tspeed: 0.1065s/iter; left time: 3413.9607s\n",
      "\titers: 4000, epoch: 13 | loss: 0.0752248\n",
      "\tspeed: 0.1053s/iter; left time: 3365.6733s\n",
      "\titers: 4100, epoch: 13 | loss: 0.0729631\n",
      "\tspeed: 0.1063s/iter; left time: 3386.1781s\n",
      "\titers: 4200, epoch: 13 | loss: 0.0699356\n",
      "\tspeed: 0.1087s/iter; left time: 3453.0041s\n",
      "\titers: 4300, epoch: 13 | loss: 0.0826374\n",
      "\tspeed: 0.1099s/iter; left time: 3481.2287s\n",
      "\titers: 4400, epoch: 13 | loss: 0.0771110\n",
      "\tspeed: 0.1064s/iter; left time: 3359.4156s\n",
      "Epoch: 13 cost time: 00h:08m:04.11s\n",
      "Epoch: 13 | Train Loss: 0.0804917 Vali Loss: 0.0901564 Test Loss: 0.1021227\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0726042\n",
      "\tspeed: 1.4495s/iter; left time: 45476.5048s\n",
      "\titers: 200, epoch: 14 | loss: 0.0950259\n",
      "\tspeed: 0.1076s/iter; left time: 3364.6307s\n",
      "\titers: 300, epoch: 14 | loss: 0.0974067\n",
      "\tspeed: 0.1079s/iter; left time: 3363.5395s\n",
      "\titers: 400, epoch: 14 | loss: 0.0626494\n",
      "\tspeed: 0.1086s/iter; left time: 3373.3374s\n",
      "\titers: 500, epoch: 14 | loss: 0.0716593\n",
      "\tspeed: 0.1055s/iter; left time: 3266.4000s\n",
      "\titers: 600, epoch: 14 | loss: 0.0849337\n",
      "\tspeed: 0.1108s/iter; left time: 3420.4207s\n",
      "\titers: 700, epoch: 14 | loss: 0.0826007\n",
      "\tspeed: 0.1101s/iter; left time: 3387.3099s\n",
      "\titers: 800, epoch: 14 | loss: 0.0776362\n",
      "\tspeed: 0.1077s/iter; left time: 3304.1345s\n",
      "\titers: 900, epoch: 14 | loss: 0.0640049\n",
      "\tspeed: 0.1058s/iter; left time: 3235.4062s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0785243\n",
      "\tspeed: 0.1041s/iter; left time: 3171.9672s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0750521\n",
      "\tspeed: 0.1087s/iter; left time: 3300.1446s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0951579\n",
      "\tspeed: 0.1021s/iter; left time: 3089.9786s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0776445\n",
      "\tspeed: 0.1113s/iter; left time: 3357.0679s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0870477\n",
      "\tspeed: 0.1059s/iter; left time: 3184.9689s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0794556\n",
      "\tspeed: 0.1069s/iter; left time: 3203.1593s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0775292\n",
      "\tspeed: 0.1096s/iter; left time: 3273.3867s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0893918\n",
      "\tspeed: 0.1069s/iter; left time: 3183.1147s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0848042\n",
      "\tspeed: 0.1055s/iter; left time: 3129.6010s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0841384\n",
      "\tspeed: 0.1057s/iter; left time: 3126.2171s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0834215\n",
      "\tspeed: 0.1064s/iter; left time: 3136.4897s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0687327\n",
      "\tspeed: 0.1073s/iter; left time: 3151.7779s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0857227\n",
      "\tspeed: 0.1084s/iter; left time: 3172.1934s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0868264\n",
      "\tspeed: 0.1084s/iter; left time: 3161.2270s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0743876\n",
      "\tspeed: 0.1093s/iter; left time: 3179.0395s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0917093\n",
      "\tspeed: 0.1080s/iter; left time: 3129.3420s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0774212\n",
      "\tspeed: 0.1094s/iter; left time: 3157.7117s\n",
      "\titers: 2700, epoch: 14 | loss: 0.0725287\n",
      "\tspeed: 0.1075s/iter; left time: 3094.0871s\n",
      "\titers: 2800, epoch: 14 | loss: 0.0763098\n",
      "\tspeed: 0.1096s/iter; left time: 3142.6273s\n",
      "\titers: 2900, epoch: 14 | loss: 0.0796201\n",
      "\tspeed: 0.1094s/iter; left time: 3125.3975s\n",
      "\titers: 3000, epoch: 14 | loss: 0.0665898\n",
      "\tspeed: 0.1109s/iter; left time: 3158.3338s\n",
      "\titers: 3100, epoch: 14 | loss: 0.0787124\n",
      "\tspeed: 0.1109s/iter; left time: 3145.5942s\n",
      "\titers: 3200, epoch: 14 | loss: 0.1122939\n",
      "\tspeed: 0.1145s/iter; left time: 3237.6875s\n",
      "\titers: 3300, epoch: 14 | loss: 0.0970755\n",
      "\tspeed: 0.1156s/iter; left time: 3257.6814s\n",
      "\titers: 3400, epoch: 14 | loss: 0.0779423\n",
      "\tspeed: 0.1159s/iter; left time: 3253.2467s\n",
      "\titers: 3500, epoch: 14 | loss: 0.0917225\n",
      "\tspeed: 0.1117s/iter; left time: 3124.7020s\n",
      "\titers: 3600, epoch: 14 | loss: 0.1003116\n",
      "\tspeed: 0.1090s/iter; left time: 3037.3148s\n",
      "\titers: 3700, epoch: 14 | loss: 0.0843237\n",
      "\tspeed: 0.1060s/iter; left time: 2942.8000s\n",
      "\titers: 3800, epoch: 14 | loss: 0.0673942\n",
      "\tspeed: 0.1037s/iter; left time: 2868.8723s\n",
      "\titers: 3900, epoch: 14 | loss: 0.0758277\n",
      "\tspeed: 0.1104s/iter; left time: 3044.4335s\n",
      "\titers: 4000, epoch: 14 | loss: 0.0841069\n",
      "\tspeed: 0.1100s/iter; left time: 3021.1814s\n",
      "\titers: 4100, epoch: 14 | loss: 0.0972049\n",
      "\tspeed: 0.1096s/iter; left time: 2999.6882s\n",
      "\titers: 4200, epoch: 14 | loss: 0.0760971\n",
      "\tspeed: 0.1091s/iter; left time: 2974.1384s\n",
      "\titers: 4300, epoch: 14 | loss: 0.0766167\n",
      "\tspeed: 0.1087s/iter; left time: 2953.0563s\n",
      "\titers: 4400, epoch: 14 | loss: 0.0637466\n",
      "\tspeed: 0.1071s/iter; left time: 2899.3748s\n",
      "Epoch: 14 cost time: 00h:08m:08.22s\n",
      "Epoch: 14 | Train Loss: 0.0801451 Vali Loss: 0.0898751 Test Loss: 0.1022130\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.025205422192811966, rmse:0.1587621569633484, mae:0.10119861364364624, rse:0.5474064350128174\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 24: 02h:27m:18.85s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 143525\n",
      "val 30725\n",
      "test 30725\n",
      "[2024-11-02 11:09:05,393] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 11:09:06,553] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 11:09:06,553] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 11:09:06,553] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 11:09:06,653] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 11:09:06,654] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 11:09:07,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 11:09:07,352] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 11:09:07,353] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 11:09:07,354] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 11:09:07,354] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 11:09:07,354] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 11:09:07,354] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 11:09:07,670] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 11:09:07,671] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,671] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,803] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 11:09:07,804] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,804] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,804] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 11:09:07,925] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 11:09:07,926] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 11:09:07,926] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.55 GB, percent = 15.0%\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 11:09:07,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fea95d33c90>\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 11:09:07,928] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 11:09:07,929] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 11:09:07,930] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1523321\n",
      "\tspeed: 0.1642s/iter; left time: 14716.1394s\n",
      "\titers: 200, epoch: 1 | loss: 0.1570973\n",
      "\tspeed: 0.1209s/iter; left time: 10817.3486s\n",
      "\titers: 300, epoch: 1 | loss: 0.1372597\n",
      "\tspeed: 0.1232s/iter; left time: 11013.3695s\n",
      "\titers: 400, epoch: 1 | loss: 0.1611106\n",
      "\tspeed: 0.1167s/iter; left time: 10422.9751s\n",
      "\titers: 500, epoch: 1 | loss: 0.1438916\n",
      "\tspeed: 0.1254s/iter; left time: 11183.1044s\n",
      "\titers: 600, epoch: 1 | loss: 0.1517782\n",
      "\tspeed: 0.1189s/iter; left time: 10597.6413s\n",
      "\titers: 700, epoch: 1 | loss: 0.1311948\n",
      "\tspeed: 0.1241s/iter; left time: 11042.8284s\n",
      "\titers: 800, epoch: 1 | loss: 0.1038204\n",
      "\tspeed: 0.1179s/iter; left time: 10481.2214s\n",
      "\titers: 900, epoch: 1 | loss: 0.1193201\n",
      "\tspeed: 0.1216s/iter; left time: 10800.0383s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1048014\n",
      "\tspeed: 0.1208s/iter; left time: 10711.8491s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1129711\n",
      "\tspeed: 0.1191s/iter; left time: 10548.4015s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1085250\n",
      "\tspeed: 0.1219s/iter; left time: 10791.5197s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1155873\n",
      "\tspeed: 0.1198s/iter; left time: 10589.9816s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1127517\n",
      "\tspeed: 0.1150s/iter; left time: 10150.9681s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1008264\n",
      "\tspeed: 0.1230s/iter; left time: 10845.5863s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1333221\n",
      "\tspeed: 0.1181s/iter; left time: 10403.3260s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1157868\n",
      "\tspeed: 0.1178s/iter; left time: 10362.5586s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1128253\n",
      "\tspeed: 0.1150s/iter; left time: 10106.5354s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1154400\n",
      "\tspeed: 0.1197s/iter; left time: 10510.2128s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1206072\n",
      "\tspeed: 0.1213s/iter; left time: 10637.7866s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1116072\n",
      "\tspeed: 0.1179s/iter; left time: 10332.3801s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1246088\n",
      "\tspeed: 0.1199s/iter; left time: 10488.1211s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1120683\n",
      "\tspeed: 0.1125s/iter; left time: 9832.5137s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1298170\n",
      "\tspeed: 0.1194s/iter; left time: 10426.5338s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1402837\n",
      "\tspeed: 0.1181s/iter; left time: 10296.0634s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1254882\n",
      "\tspeed: 0.1183s/iter; left time: 10305.0880s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1003666\n",
      "\tspeed: 0.1171s/iter; left time: 10189.6375s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1112671\n",
      "\tspeed: 0.1172s/iter; left time: 10185.2528s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1137624\n",
      "\tspeed: 0.1198s/iter; left time: 10402.4663s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1242562\n",
      "\tspeed: 0.1185s/iter; left time: 10278.3318s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1275978\n",
      "\tspeed: 0.1142s/iter; left time: 9893.9340s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1161146\n",
      "\tspeed: 0.1141s/iter; left time: 9870.8651s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1244878\n",
      "\tspeed: 0.1178s/iter; left time: 10178.6933s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1080193\n",
      "\tspeed: 0.1216s/iter; left time: 10497.7282s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1138757\n",
      "\tspeed: 0.1196s/iter; left time: 10309.6014s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1165035\n",
      "\tspeed: 0.1162s/iter; left time: 10009.1216s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1057020\n",
      "\tspeed: 0.1226s/iter; left time: 10542.4120s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0968045\n",
      "\tspeed: 0.1225s/iter; left time: 10519.0971s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1031482\n",
      "\tspeed: 0.1164s/iter; left time: 9983.3868s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1125728\n",
      "\tspeed: 0.1196s/iter; left time: 10247.4907s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1198684\n",
      "\tspeed: 0.1194s/iter; left time: 10223.1208s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1088336\n",
      "\tspeed: 0.1180s/iter; left time: 10091.9961s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1251617\n",
      "\tspeed: 0.1194s/iter; left time: 10195.8646s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1200401\n",
      "\tspeed: 0.1176s/iter; left time: 10031.5046s\n",
      "Epoch: 1 cost time: 00h:08m:55.29s\n",
      "Epoch: 1 | Train Loss: 0.1178117 Vali Loss: 0.1184595 Test Loss: 0.1412180\n",
      "Validation loss decreased (inf --> 0.118459).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0888638\n",
      "\tspeed: 1.6645s/iter; left time: 141672.5188s\n",
      "\titers: 200, epoch: 2 | loss: 0.1137921\n",
      "\tspeed: 0.1082s/iter; left time: 9202.4579s\n",
      "\titers: 300, epoch: 2 | loss: 0.1233345\n",
      "\tspeed: 0.1080s/iter; left time: 9173.1189s\n",
      "\titers: 400, epoch: 2 | loss: 0.1065918\n",
      "\tspeed: 0.1019s/iter; left time: 8639.2576s\n",
      "\titers: 500, epoch: 2 | loss: 0.0876302\n",
      "\tspeed: 0.1067s/iter; left time: 9043.3081s\n",
      "\titers: 600, epoch: 2 | loss: 0.1139689\n",
      "\tspeed: 0.1069s/iter; left time: 9046.5598s\n",
      "\titers: 700, epoch: 2 | loss: 0.1090599\n",
      "\tspeed: 0.1102s/iter; left time: 9316.3301s\n",
      "\titers: 800, epoch: 2 | loss: 0.0918770\n",
      "\tspeed: 0.1101s/iter; left time: 9290.4094s\n",
      "\titers: 900, epoch: 2 | loss: 0.1054808\n",
      "\tspeed: 0.1096s/iter; left time: 9244.5952s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1240198\n",
      "\tspeed: 0.1094s/iter; left time: 9214.8151s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1188949\n",
      "\tspeed: 0.1060s/iter; left time: 8912.7695s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1082713\n",
      "\tspeed: 0.1068s/iter; left time: 8970.2357s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1121853\n",
      "\tspeed: 0.1107s/iter; left time: 9291.2580s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1057807\n",
      "\tspeed: 0.1094s/iter; left time: 9167.4611s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1165954\n",
      "\tspeed: 0.1027s/iter; left time: 8595.6482s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1163039\n",
      "\tspeed: 0.1082s/iter; left time: 9048.2322s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1402899\n",
      "\tspeed: 0.1080s/iter; left time: 9019.2083s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0893394\n",
      "\tspeed: 0.1091s/iter; left time: 9103.0766s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1250467\n",
      "\tspeed: 0.1082s/iter; left time: 9012.4281s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0956506\n",
      "\tspeed: 0.1055s/iter; left time: 8779.0126s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1079546\n",
      "\tspeed: 0.1074s/iter; left time: 8925.6004s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1003428\n",
      "\tspeed: 0.1034s/iter; left time: 8584.0707s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1021225\n",
      "\tspeed: 0.1088s/iter; left time: 9022.0597s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1186273\n",
      "\tspeed: 0.1034s/iter; left time: 8566.1653s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1165032\n",
      "\tspeed: 0.1061s/iter; left time: 8774.0694s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1051225\n",
      "\tspeed: 0.1063s/iter; left time: 8779.5792s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1007597\n",
      "\tspeed: 0.1062s/iter; left time: 8762.9591s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0979256\n",
      "\tspeed: 0.1066s/iter; left time: 8782.6368s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1156221\n",
      "\tspeed: 0.1103s/iter; left time: 9078.6564s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1229274\n",
      "\tspeed: 0.1086s/iter; left time: 8929.6905s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1010010\n",
      "\tspeed: 0.1101s/iter; left time: 9038.0092s\n",
      "\titers: 3200, epoch: 2 | loss: 0.0914321\n",
      "\tspeed: 0.1050s/iter; left time: 8608.4418s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0996214\n",
      "\tspeed: 0.1090s/iter; left time: 8932.6923s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1225803\n",
      "\tspeed: 0.1057s/iter; left time: 8645.2226s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1037378\n",
      "\tspeed: 0.1081s/iter; left time: 8834.8982s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0835612\n",
      "\tspeed: 0.1081s/iter; left time: 8819.2208s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1148943\n",
      "\tspeed: 0.1078s/iter; left time: 8789.5885s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1152604\n",
      "\tspeed: 0.1082s/iter; left time: 8807.0794s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1060695\n",
      "\tspeed: 0.1106s/iter; left time: 8992.8881s\n",
      "\titers: 4000, epoch: 2 | loss: 0.0958115\n",
      "\tspeed: 0.1089s/iter; left time: 8840.9779s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1165176\n",
      "\tspeed: 0.1086s/iter; left time: 8810.6341s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1040161\n",
      "\tspeed: 0.1081s/iter; left time: 8754.8615s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1187941\n",
      "\tspeed: 0.1120s/iter; left time: 9065.4638s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1056552\n",
      "\tspeed: 0.1088s/iter; left time: 8793.8298s\n",
      "Epoch: 2 cost time: 00h:08m:03.68s\n",
      "Epoch: 2 | Train Loss: 0.1077817 Vali Loss: 0.1173895 Test Loss: 0.1434370\n",
      "Validation loss decreased (0.118459 --> 0.117389).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1310282\n",
      "\tspeed: 1.4511s/iter; left time: 117006.7300s\n",
      "\titers: 200, epoch: 3 | loss: 0.1085388\n",
      "\tspeed: 0.1104s/iter; left time: 8894.4253s\n",
      "\titers: 300, epoch: 3 | loss: 0.1202963\n",
      "\tspeed: 0.1118s/iter; left time: 8993.4722s\n",
      "\titers: 400, epoch: 3 | loss: 0.1198052\n",
      "\tspeed: 0.1101s/iter; left time: 8841.1450s\n",
      "\titers: 500, epoch: 3 | loss: 0.1075269\n",
      "\tspeed: 0.1087s/iter; left time: 8723.8894s\n",
      "\titers: 600, epoch: 3 | loss: 0.0981584\n",
      "\tspeed: 0.1058s/iter; left time: 8480.3468s\n",
      "\titers: 700, epoch: 3 | loss: 0.1196058\n",
      "\tspeed: 0.1064s/iter; left time: 8511.6395s\n",
      "\titers: 800, epoch: 3 | loss: 0.1243347\n",
      "\tspeed: 0.1088s/iter; left time: 8696.4165s\n",
      "\titers: 900, epoch: 3 | loss: 0.1134376\n",
      "\tspeed: 0.1091s/iter; left time: 8712.2458s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1046471\n",
      "\tspeed: 0.1055s/iter; left time: 8408.6168s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1102815\n",
      "\tspeed: 0.1084s/iter; left time: 8632.7852s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0851324\n",
      "\tspeed: 0.1071s/iter; left time: 8521.0054s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0943000\n",
      "\tspeed: 0.1090s/iter; left time: 8655.1238s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0990528\n",
      "\tspeed: 0.1087s/iter; left time: 8621.0262s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0970403\n",
      "\tspeed: 0.1089s/iter; left time: 8625.9391s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0781570\n",
      "\tspeed: 0.1078s/iter; left time: 8528.1395s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0904602\n",
      "\tspeed: 0.1093s/iter; left time: 8634.1985s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1183398\n",
      "\tspeed: 0.1104s/iter; left time: 8716.5300s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1106287\n",
      "\tspeed: 0.1113s/iter; left time: 8772.8133s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1017435\n",
      "\tspeed: 0.1107s/iter; left time: 8712.6664s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0943806\n",
      "\tspeed: 0.1108s/iter; left time: 8715.7095s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1140733\n",
      "\tspeed: 0.1109s/iter; left time: 8710.2082s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1258210\n",
      "\tspeed: 0.1093s/iter; left time: 8571.6508s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0975943\n",
      "\tspeed: 0.1101s/iter; left time: 8623.4105s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1019286\n",
      "\tspeed: 0.1087s/iter; left time: 8507.3984s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1040863\n",
      "\tspeed: 0.1103s/iter; left time: 8615.2479s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1137355\n",
      "\tspeed: 0.1101s/iter; left time: 8591.0793s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1007091\n",
      "\tspeed: 0.1104s/iter; left time: 8604.1474s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1016200\n",
      "\tspeed: 0.1113s/iter; left time: 8660.1653s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1144505\n",
      "\tspeed: 0.1112s/iter; left time: 8642.1682s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1279829\n",
      "\tspeed: 0.1097s/iter; left time: 8513.7131s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1213689\n",
      "\tspeed: 0.1090s/iter; left time: 8451.7399s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1051345\n",
      "\tspeed: 0.1110s/iter; left time: 8597.2565s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1120226\n",
      "\tspeed: 0.1148s/iter; left time: 8874.1319s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1024189\n",
      "\tspeed: 0.1163s/iter; left time: 8980.1940s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0943960\n",
      "\tspeed: 0.1137s/iter; left time: 8772.9666s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1066416\n",
      "\tspeed: 0.1149s/iter; left time: 8854.0322s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0961487\n",
      "\tspeed: 0.1144s/iter; left time: 8803.2502s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0950556\n",
      "\tspeed: 0.1143s/iter; left time: 8778.9043s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1026345\n",
      "\tspeed: 0.1149s/iter; left time: 8814.8270s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1027451\n",
      "\tspeed: 0.1094s/iter; left time: 8386.3927s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1061701\n",
      "\tspeed: 0.1059s/iter; left time: 8105.5696s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0936683\n",
      "\tspeed: 0.1023s/iter; left time: 7818.0689s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1012166\n",
      "\tspeed: 0.1054s/iter; left time: 8046.8008s\n",
      "Epoch: 3 cost time: 00h:08m:13.06s\n",
      "Epoch: 3 | Train Loss: 0.1043771 Vali Loss: 0.1187507 Test Loss: 0.1443793\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1020158\n",
      "\tspeed: 1.4266s/iter; left time: 108629.7933s\n",
      "\titers: 200, epoch: 4 | loss: 0.1003834\n",
      "\tspeed: 0.1065s/iter; left time: 8099.1186s\n",
      "\titers: 300, epoch: 4 | loss: 0.1127172\n",
      "\tspeed: 0.1054s/iter; left time: 8005.5924s\n",
      "\titers: 400, epoch: 4 | loss: 0.1082551\n",
      "\tspeed: 0.1117s/iter; left time: 8468.3907s\n",
      "\titers: 500, epoch: 4 | loss: 0.1041521\n",
      "\tspeed: 0.1102s/iter; left time: 8350.9373s\n",
      "\titers: 600, epoch: 4 | loss: 0.1070160\n",
      "\tspeed: 0.1089s/iter; left time: 8237.1190s\n",
      "\titers: 700, epoch: 4 | loss: 0.1449660\n",
      "\tspeed: 0.1088s/iter; left time: 8222.5166s\n",
      "\titers: 800, epoch: 4 | loss: 0.0877978\n",
      "\tspeed: 0.1106s/iter; left time: 8345.7626s\n",
      "\titers: 900, epoch: 4 | loss: 0.0985263\n",
      "\tspeed: 0.1102s/iter; left time: 8304.9653s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1049943\n",
      "\tspeed: 0.1072s/iter; left time: 8069.5046s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0958291\n",
      "\tspeed: 0.1103s/iter; left time: 8288.1820s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0853065\n",
      "\tspeed: 0.1109s/iter; left time: 8324.7872s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0855185\n",
      "\tspeed: 0.1059s/iter; left time: 7939.1575s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0994194\n",
      "\tspeed: 0.1074s/iter; left time: 8038.8326s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1068409\n",
      "\tspeed: 0.1097s/iter; left time: 8196.6038s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1120063\n",
      "\tspeed: 0.1123s/iter; left time: 8379.7792s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1221094\n",
      "\tspeed: 0.1110s/iter; left time: 8274.3631s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0878371\n",
      "\tspeed: 0.1078s/iter; left time: 8023.7988s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1065904\n",
      "\tspeed: 0.1085s/iter; left time: 8069.4740s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1217954\n",
      "\tspeed: 0.1044s/iter; left time: 7747.9174s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0763485\n",
      "\tspeed: 0.1073s/iter; left time: 7958.5286s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1082068\n",
      "\tspeed: 0.1090s/iter; left time: 8070.6059s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0984239\n",
      "\tspeed: 0.1061s/iter; left time: 7844.5492s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0943878\n",
      "\tspeed: 0.1040s/iter; left time: 7681.8681s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1143633\n",
      "\tspeed: 0.1093s/iter; left time: 8060.9938s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1017567\n",
      "\tspeed: 0.1068s/iter; left time: 7866.4857s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0997897\n",
      "\tspeed: 0.1085s/iter; left time: 7979.5815s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1115867\n",
      "\tspeed: 0.1097s/iter; left time: 8059.9033s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0922957\n",
      "\tspeed: 0.1011s/iter; left time: 7417.0598s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1038117\n",
      "\tspeed: 0.1058s/iter; left time: 7746.4603s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0926916\n",
      "\tspeed: 0.1085s/iter; left time: 7933.7269s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0785196\n",
      "\tspeed: 0.1055s/iter; left time: 7706.2941s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1024314\n",
      "\tspeed: 0.1066s/iter; left time: 7777.9395s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0930109\n",
      "\tspeed: 0.0964s/iter; left time: 7019.4791s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0849443\n",
      "\tspeed: 0.1076s/iter; left time: 7830.5603s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0900325\n",
      "\tspeed: 0.1052s/iter; left time: 7642.6536s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1102086\n",
      "\tspeed: 0.1036s/iter; left time: 7515.5689s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0800017\n",
      "\tspeed: 0.1056s/iter; left time: 7651.7566s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0829896\n",
      "\tspeed: 0.1077s/iter; left time: 7789.8891s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0955721\n",
      "\tspeed: 0.1121s/iter; left time: 8097.7927s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1014783\n",
      "\tspeed: 0.1107s/iter; left time: 7987.8152s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1079314\n",
      "\tspeed: 0.1064s/iter; left time: 7664.9586s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1017345\n",
      "\tspeed: 0.1061s/iter; left time: 7632.5592s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0957486\n",
      "\tspeed: 0.1083s/iter; left time: 7778.6908s\n",
      "Epoch: 4 cost time: 00h:08m:02.79s\n",
      "Epoch: 4 | Train Loss: 0.1004036 Vali Loss: 0.1205356 Test Loss: 0.1469130\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1013267\n",
      "\tspeed: 1.4324s/iter; left time: 102648.4436s\n",
      "\titers: 200, epoch: 5 | loss: 0.1063943\n",
      "\tspeed: 0.1077s/iter; left time: 7703.5968s\n",
      "\titers: 300, epoch: 5 | loss: 0.1020837\n",
      "\tspeed: 0.1118s/iter; left time: 7991.0566s\n",
      "\titers: 400, epoch: 5 | loss: 0.1110498\n",
      "\tspeed: 0.1102s/iter; left time: 7867.0923s\n",
      "\titers: 500, epoch: 5 | loss: 0.1179720\n",
      "\tspeed: 0.1061s/iter; left time: 7559.8261s\n",
      "\titers: 600, epoch: 5 | loss: 0.1031104\n",
      "\tspeed: 0.1075s/iter; left time: 7650.7286s\n",
      "\titers: 700, epoch: 5 | loss: 0.0947505\n",
      "\tspeed: 0.1100s/iter; left time: 7815.1928s\n",
      "\titers: 800, epoch: 5 | loss: 0.0988153\n",
      "\tspeed: 0.1023s/iter; left time: 7257.7675s\n",
      "\titers: 900, epoch: 5 | loss: 0.0980647\n",
      "\tspeed: 0.1092s/iter; left time: 7736.7493s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1027513\n",
      "\tspeed: 0.1090s/iter; left time: 7711.4596s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1019618\n",
      "\tspeed: 0.1066s/iter; left time: 7531.4308s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0872446\n",
      "\tspeed: 0.1048s/iter; left time: 7393.3369s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0995278\n",
      "\tspeed: 0.1102s/iter; left time: 7767.5143s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0856419\n",
      "\tspeed: 0.1063s/iter; left time: 7480.2797s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0984738\n",
      "\tspeed: 0.1074s/iter; left time: 7547.4283s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0897058\n",
      "\tspeed: 0.1081s/iter; left time: 7581.8695s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0837954\n",
      "\tspeed: 0.1081s/iter; left time: 7573.9114s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1022424\n",
      "\tspeed: 0.1108s/iter; left time: 7751.0991s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1100879\n",
      "\tspeed: 0.1067s/iter; left time: 7457.5476s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1003026\n",
      "\tspeed: 0.1099s/iter; left time: 7669.4448s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0989896\n",
      "\tspeed: 0.1127s/iter; left time: 7853.8883s\n",
      "\titers: 2200, epoch: 5 | loss: 0.1024873\n",
      "\tspeed: 0.1112s/iter; left time: 7738.3216s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1089459\n",
      "\tspeed: 0.1093s/iter; left time: 7593.1926s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1069664\n",
      "\tspeed: 0.1097s/iter; left time: 7606.0668s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0821716\n",
      "\tspeed: 0.1092s/iter; left time: 7565.3457s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0913218\n",
      "\tspeed: 0.1044s/iter; left time: 7219.4036s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0889648\n",
      "\tspeed: 0.1084s/iter; left time: 7482.9710s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0932212\n",
      "\tspeed: 0.1138s/iter; left time: 7850.3600s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0853349\n",
      "\tspeed: 0.1050s/iter; left time: 7233.3472s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0956874\n",
      "\tspeed: 0.1033s/iter; left time: 7104.3960s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0857884\n",
      "\tspeed: 0.1133s/iter; left time: 7782.6325s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0957507\n",
      "\tspeed: 0.1091s/iter; left time: 7478.0101s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0887952\n",
      "\tspeed: 0.1111s/iter; left time: 7607.7668s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0878523\n",
      "\tspeed: 0.1104s/iter; left time: 7544.4456s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0892572\n",
      "\tspeed: 0.1069s/iter; left time: 7295.6108s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1093634\n",
      "\tspeed: 0.1074s/iter; left time: 7320.9435s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0850211\n",
      "\tspeed: 0.1037s/iter; left time: 7057.2676s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0877998\n",
      "\tspeed: 0.1081s/iter; left time: 7348.3714s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1017472\n",
      "\tspeed: 0.1090s/iter; left time: 7399.1499s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0930923\n",
      "\tspeed: 0.1088s/iter; left time: 7375.1654s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0939313\n",
      "\tspeed: 0.1073s/iter; left time: 7258.2624s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0919951\n",
      "\tspeed: 0.1029s/iter; left time: 6950.2808s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0943117\n",
      "\tspeed: 0.1106s/iter; left time: 7464.2538s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0894200\n",
      "\tspeed: 0.1101s/iter; left time: 7414.6584s\n",
      "Epoch: 5 cost time: 00h:08m:06.31s\n",
      "Epoch: 5 | Train Loss: 0.0966544 Vali Loss: 0.1223298 Test Loss: 0.1498027\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0925073\n",
      "\tspeed: 1.4349s/iter; left time: 96391.4087s\n",
      "\titers: 200, epoch: 6 | loss: 0.1033233\n",
      "\tspeed: 0.1065s/iter; left time: 7143.5485s\n",
      "\titers: 300, epoch: 6 | loss: 0.1017852\n",
      "\tspeed: 0.1045s/iter; left time: 6996.4023s\n",
      "\titers: 400, epoch: 6 | loss: 0.0813918\n",
      "\tspeed: 0.1054s/iter; left time: 7051.0602s\n",
      "\titers: 500, epoch: 6 | loss: 0.1012578\n",
      "\tspeed: 0.1046s/iter; left time: 6986.6502s\n",
      "\titers: 600, epoch: 6 | loss: 0.0925734\n",
      "\tspeed: 0.1053s/iter; left time: 7019.1729s\n",
      "\titers: 700, epoch: 6 | loss: 0.0816648\n",
      "\tspeed: 0.1059s/iter; left time: 7051.2712s\n",
      "\titers: 800, epoch: 6 | loss: 0.0903690\n",
      "\tspeed: 0.1054s/iter; left time: 7008.7325s\n",
      "\titers: 900, epoch: 6 | loss: 0.1008372\n",
      "\tspeed: 0.1073s/iter; left time: 7119.6150s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0913643\n",
      "\tspeed: 0.1080s/iter; left time: 7158.1052s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1034070\n",
      "\tspeed: 0.1102s/iter; left time: 7291.8303s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1074490\n",
      "\tspeed: 0.1054s/iter; left time: 6966.9283s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0876377\n",
      "\tspeed: 0.1064s/iter; left time: 7017.0102s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1026839\n",
      "\tspeed: 0.1084s/iter; left time: 7141.4853s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0969250\n",
      "\tspeed: 0.1130s/iter; left time: 7435.6748s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0847653\n",
      "\tspeed: 0.1105s/iter; left time: 7254.9230s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0786244\n",
      "\tspeed: 0.1098s/iter; left time: 7202.3233s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0895295\n",
      "\tspeed: 0.1110s/iter; left time: 7266.2878s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0960533\n",
      "\tspeed: 0.1112s/iter; left time: 7269.8468s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1027374\n",
      "\tspeed: 0.1042s/iter; left time: 6802.7242s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0939934\n",
      "\tspeed: 0.1096s/iter; left time: 7140.3883s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0843199\n",
      "\tspeed: 0.1080s/iter; left time: 7025.6123s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0809122\n",
      "\tspeed: 0.1056s/iter; left time: 6864.2003s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1014044\n",
      "\tspeed: 0.1072s/iter; left time: 6957.7941s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0871974\n",
      "\tspeed: 0.1046s/iter; left time: 6775.4703s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0921036\n",
      "\tspeed: 0.1055s/iter; left time: 6823.3170s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1040257\n",
      "\tspeed: 0.1067s/iter; left time: 6893.3519s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1056846\n",
      "\tspeed: 0.1114s/iter; left time: 7185.6550s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0859896\n",
      "\tspeed: 0.1093s/iter; left time: 7033.2292s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0964024\n",
      "\tspeed: 0.1030s/iter; left time: 6619.1718s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0688204\n",
      "\tspeed: 0.1062s/iter; left time: 6816.5541s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0952048\n",
      "\tspeed: 0.1090s/iter; left time: 6987.1033s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1085152\n",
      "\tspeed: 0.1089s/iter; left time: 6967.9033s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0811777\n",
      "\tspeed: 0.1089s/iter; left time: 6958.1351s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1095598\n",
      "\tspeed: 0.1048s/iter; left time: 6682.1048s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1067103\n",
      "\tspeed: 0.1071s/iter; left time: 6818.2425s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0980428\n",
      "\tspeed: 0.1116s/iter; left time: 7095.8458s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0960030\n",
      "\tspeed: 0.1074s/iter; left time: 6816.9789s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0844862\n",
      "\tspeed: 0.1094s/iter; left time: 6931.5156s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0948572\n",
      "\tspeed: 0.1122s/iter; left time: 7098.3958s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0972997\n",
      "\tspeed: 0.1041s/iter; left time: 6578.0725s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0840513\n",
      "\tspeed: 0.1005s/iter; left time: 6342.2825s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0846837\n",
      "\tspeed: 0.1068s/iter; left time: 6726.8852s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0994278\n",
      "\tspeed: 0.1067s/iter; left time: 6710.9728s\n",
      "Epoch: 6 cost time: 00h:08m:02.05s\n",
      "Epoch: 6 | Train Loss: 0.0935319 Vali Loss: 0.1227152 Test Loss: 0.1494228\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0913173\n",
      "\tspeed: 1.4293s/iter; left time: 89602.9337s\n",
      "\titers: 200, epoch: 7 | loss: 0.1163411\n",
      "\tspeed: 0.1066s/iter; left time: 6672.4964s\n",
      "\titers: 300, epoch: 7 | loss: 0.0927998\n",
      "\tspeed: 0.1023s/iter; left time: 6393.4464s\n",
      "\titers: 400, epoch: 7 | loss: 0.0917988\n",
      "\tspeed: 0.1084s/iter; left time: 6766.0541s\n",
      "\titers: 500, epoch: 7 | loss: 0.0816753\n",
      "\tspeed: 0.1080s/iter; left time: 6724.9604s\n",
      "\titers: 600, epoch: 7 | loss: 0.1129898\n",
      "\tspeed: 0.1079s/iter; left time: 6710.5650s\n",
      "\titers: 700, epoch: 7 | loss: 0.0816540\n",
      "\tspeed: 0.1090s/iter; left time: 6768.0810s\n",
      "\titers: 800, epoch: 7 | loss: 0.1145246\n",
      "\tspeed: 0.1110s/iter; left time: 6878.3435s\n",
      "\titers: 900, epoch: 7 | loss: 0.0861063\n",
      "\tspeed: 0.1078s/iter; left time: 6671.9489s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0930915\n",
      "\tspeed: 0.1067s/iter; left time: 6595.2577s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0832640\n",
      "\tspeed: 0.1060s/iter; left time: 6540.8371s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1068092\n",
      "\tspeed: 0.1047s/iter; left time: 6445.8849s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1094583\n",
      "\tspeed: 0.1059s/iter; left time: 6509.7356s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0916336\n",
      "\tspeed: 0.1056s/iter; left time: 6482.0607s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0932818\n",
      "\tspeed: 0.1042s/iter; left time: 6384.5600s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0976237\n",
      "\tspeed: 0.1079s/iter; left time: 6604.1286s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1042555\n",
      "\tspeed: 0.1116s/iter; left time: 6817.9485s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0780962\n",
      "\tspeed: 0.1084s/iter; left time: 6609.7655s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0822116\n",
      "\tspeed: 0.1047s/iter; left time: 6373.6247s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0880490\n",
      "\tspeed: 0.1074s/iter; left time: 6528.3157s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0868164\n",
      "\tspeed: 0.1072s/iter; left time: 6507.6568s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0720907\n",
      "\tspeed: 0.1071s/iter; left time: 6491.6283s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0788777\n",
      "\tspeed: 0.1096s/iter; left time: 6631.8210s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0901415\n",
      "\tspeed: 0.1063s/iter; left time: 6419.7458s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0850017\n",
      "\tspeed: 0.1067s/iter; left time: 6433.9397s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0886241\n",
      "\tspeed: 0.1072s/iter; left time: 6449.7304s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0978564\n",
      "\tspeed: 0.1041s/iter; left time: 6255.0896s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0925564\n",
      "\tspeed: 0.1065s/iter; left time: 6389.7855s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1175411\n",
      "\tspeed: 0.1077s/iter; left time: 6451.2886s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0782495\n",
      "\tspeed: 0.1077s/iter; left time: 6441.0094s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0809937\n",
      "\tspeed: 0.1041s/iter; left time: 6211.7480s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0883481\n",
      "\tspeed: 0.1054s/iter; left time: 6280.1993s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0891999\n",
      "\tspeed: 0.1065s/iter; left time: 6332.8541s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0984669\n",
      "\tspeed: 0.1078s/iter; left time: 6401.0366s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0943845\n",
      "\tspeed: 0.1074s/iter; left time: 6369.6049s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0912553\n",
      "\tspeed: 0.1056s/iter; left time: 6248.8721s\n",
      "\titers: 3700, epoch: 7 | loss: 0.0948698\n",
      "\tspeed: 0.1060s/iter; left time: 6261.7698s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0706965\n",
      "\tspeed: 0.1072s/iter; left time: 6323.6486s\n",
      "\titers: 3900, epoch: 7 | loss: 0.1006198\n",
      "\tspeed: 0.1067s/iter; left time: 6283.7804s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0881463\n",
      "\tspeed: 0.1084s/iter; left time: 6370.4621s\n",
      "\titers: 4100, epoch: 7 | loss: 0.1020899\n",
      "\tspeed: 0.1086s/iter; left time: 6371.3175s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0827523\n",
      "\tspeed: 0.1070s/iter; left time: 6269.6178s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1077781\n",
      "\tspeed: 0.1092s/iter; left time: 6388.1090s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0829007\n",
      "\tspeed: 0.1105s/iter; left time: 6453.8770s\n",
      "Epoch: 7 cost time: 00h:08m:01.26s\n",
      "Epoch: 7 | Train Loss: 0.0908818 Vali Loss: 0.1233640 Test Loss: 0.1499765\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0437089167535305, rmse:0.20906677842140198, mae:0.143436998128891, rse:0.7229486107826233\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 96: 01h:13m:33.46s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 143165\n",
      "val 30365\n",
      "test 30365\n",
      "[2024-11-02 12:22:39,024] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 12:22:39,984] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 12:22:39,985] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 12:22:39,985] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 12:22:40,074] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 12:22:40,074] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 12:22:40,708] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 12:22:40,709] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 12:22:40,709] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 12:22:40,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 12:22:40,710] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 12:22:40,711] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 12:22:40,711] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 12:22:41,051] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 12:22:41,052] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.53 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,176] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 12:22:41,177] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,177] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.53 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,177] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 12:22:41,295] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 12:22:41,296] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 12:22:41,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 113.54 GB, percent = 15.0%\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 12:22:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 12:22:41,298] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 12:22:41,299] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0822259f10>\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 12:22:41,300] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 12:22:41,301] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 12:22:41,302] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1492039\n",
      "\tspeed: 0.1558s/iter; left time: 13920.1947s\n",
      "\titers: 200, epoch: 1 | loss: 0.1571750\n",
      "\tspeed: 0.1229s/iter; left time: 10972.2522s\n",
      "\titers: 300, epoch: 1 | loss: 0.1447653\n",
      "\tspeed: 0.1187s/iter; left time: 10586.4424s\n",
      "\titers: 400, epoch: 1 | loss: 0.1463560\n",
      "\tspeed: 0.1149s/iter; left time: 10234.8297s\n",
      "\titers: 500, epoch: 1 | loss: 0.1403078\n",
      "\tspeed: 0.1148s/iter; left time: 10213.2770s\n",
      "\titers: 600, epoch: 1 | loss: 0.1426290\n",
      "\tspeed: 0.1173s/iter; left time: 10421.6835s\n",
      "\titers: 700, epoch: 1 | loss: 0.1221828\n",
      "\tspeed: 0.1194s/iter; left time: 10599.4566s\n",
      "\titers: 800, epoch: 1 | loss: 0.1188438\n",
      "\tspeed: 0.1100s/iter; left time: 9748.9378s\n",
      "\titers: 900, epoch: 1 | loss: 0.1083227\n",
      "\tspeed: 0.1164s/iter; left time: 10306.3772s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1414768\n",
      "\tspeed: 0.1181s/iter; left time: 10450.8120s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1065454\n",
      "\tspeed: 0.1194s/iter; left time: 10553.2745s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1062137\n",
      "\tspeed: 0.1160s/iter; left time: 10237.9912s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1094389\n",
      "\tspeed: 0.1209s/iter; left time: 10656.9320s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1425477\n",
      "\tspeed: 0.1170s/iter; left time: 10306.5998s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1190569\n",
      "\tspeed: 0.1203s/iter; left time: 10578.8675s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1307243\n",
      "\tspeed: 0.1175s/iter; left time: 10327.7661s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1101217\n",
      "\tspeed: 0.1158s/iter; left time: 10165.8935s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1066481\n",
      "\tspeed: 0.1168s/iter; left time: 10235.8467s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1185276\n",
      "\tspeed: 0.1188s/iter; left time: 10399.7914s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1198424\n",
      "\tspeed: 0.1118s/iter; left time: 9778.2014s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1289180\n",
      "\tspeed: 0.1156s/iter; left time: 10096.3713s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1175074\n",
      "\tspeed: 0.1172s/iter; left time: 10222.9627s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1311432\n",
      "\tspeed: 0.1216s/iter; left time: 10596.7156s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1330000\n",
      "\tspeed: 0.1184s/iter; left time: 10305.9827s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1254928\n",
      "\tspeed: 0.1184s/iter; left time: 10299.5504s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1172541\n",
      "\tspeed: 0.1183s/iter; left time: 10272.4490s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1503642\n",
      "\tspeed: 0.1170s/iter; left time: 10152.3566s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1218206\n",
      "\tspeed: 0.1193s/iter; left time: 10341.2122s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1123841\n",
      "\tspeed: 0.1196s/iter; left time: 10353.4101s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1095140\n",
      "\tspeed: 0.1198s/iter; left time: 10360.1082s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1169514\n",
      "\tspeed: 0.1222s/iter; left time: 10555.3391s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1298743\n",
      "\tspeed: 0.1197s/iter; left time: 10325.1203s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1165393\n",
      "\tspeed: 0.1233s/iter; left time: 10622.9657s\n",
      "\titers: 3400, epoch: 1 | loss: 0.0974565\n",
      "\tspeed: 0.1217s/iter; left time: 10474.5007s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1041744\n",
      "\tspeed: 0.1236s/iter; left time: 10620.6693s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0992299\n",
      "\tspeed: 0.1231s/iter; left time: 10572.3640s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1239099\n",
      "\tspeed: 0.1227s/iter; left time: 10524.0074s\n",
      "\titers: 3800, epoch: 1 | loss: 0.0990040\n",
      "\tspeed: 0.1242s/iter; left time: 10642.4687s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1218022\n",
      "\tspeed: 0.1245s/iter; left time: 10655.7635s\n",
      "\titers: 4000, epoch: 1 | loss: 0.0961539\n",
      "\tspeed: 0.1249s/iter; left time: 10676.5093s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1266004\n",
      "\tspeed: 0.1237s/iter; left time: 10561.9273s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1165152\n",
      "\tspeed: 0.1205s/iter; left time: 10275.6515s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1041826\n",
      "\tspeed: 0.1114s/iter; left time: 9484.2204s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1090055\n",
      "\tspeed: 0.1152s/iter; left time: 9801.7413s\n",
      "Epoch: 1 cost time: 00h:08m:52.60s\n",
      "Epoch: 1 | Train Loss: 0.1213396 Vali Loss: 0.1232667 Test Loss: 0.1479476\n",
      "Validation loss decreased (inf --> 0.123267).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1373354\n",
      "\tspeed: 1.6252s/iter; left time: 137957.5657s\n",
      "\titers: 200, epoch: 2 | loss: 0.1157282\n",
      "\tspeed: 0.1053s/iter; left time: 8929.5690s\n",
      "\titers: 300, epoch: 2 | loss: 0.1186823\n",
      "\tspeed: 0.1097s/iter; left time: 9286.5049s\n",
      "\titers: 400, epoch: 2 | loss: 0.1171655\n",
      "\tspeed: 0.0992s/iter; left time: 8390.7134s\n",
      "\titers: 500, epoch: 2 | loss: 0.1352277\n",
      "\tspeed: 0.1048s/iter; left time: 8850.3792s\n",
      "\titers: 600, epoch: 2 | loss: 0.1214827\n",
      "\tspeed: 0.1077s/iter; left time: 9087.7747s\n",
      "\titers: 700, epoch: 2 | loss: 0.1123141\n",
      "\tspeed: 0.1056s/iter; left time: 8898.3932s\n",
      "\titers: 800, epoch: 2 | loss: 0.1030472\n",
      "\tspeed: 0.1077s/iter; left time: 9069.4773s\n",
      "\titers: 900, epoch: 2 | loss: 0.1189815\n",
      "\tspeed: 0.1090s/iter; left time: 9169.2050s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1273633\n",
      "\tspeed: 0.1075s/iter; left time: 9030.3800s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1186239\n",
      "\tspeed: 0.1111s/iter; left time: 9321.8120s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1002740\n",
      "\tspeed: 0.1056s/iter; left time: 8848.6784s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1090776\n",
      "\tspeed: 0.1064s/iter; left time: 8904.2668s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1267764\n",
      "\tspeed: 0.1088s/iter; left time: 9095.7492s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1032072\n",
      "\tspeed: 0.1051s/iter; left time: 8772.7101s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1147069\n",
      "\tspeed: 0.1093s/iter; left time: 9112.9068s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1185646\n",
      "\tspeed: 0.1043s/iter; left time: 8686.1948s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0919052\n",
      "\tspeed: 0.1061s/iter; left time: 8827.8832s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1263514\n",
      "\tspeed: 0.1035s/iter; left time: 8597.2256s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1148299\n",
      "\tspeed: 0.1074s/iter; left time: 8916.4972s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1236556\n",
      "\tspeed: 0.1052s/iter; left time: 8716.8750s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1127978\n",
      "\tspeed: 0.1105s/iter; left time: 9150.8093s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0918464\n",
      "\tspeed: 0.1044s/iter; left time: 8632.4879s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1098604\n",
      "\tspeed: 0.1088s/iter; left time: 8988.8185s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1160145\n",
      "\tspeed: 0.1060s/iter; left time: 8745.7367s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1331852\n",
      "\tspeed: 0.1073s/iter; left time: 8842.2601s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0922735\n",
      "\tspeed: 0.1023s/iter; left time: 8418.5501s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1118323\n",
      "\tspeed: 0.1027s/iter; left time: 8438.4606s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1124189\n",
      "\tspeed: 0.1078s/iter; left time: 8851.2279s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1435691\n",
      "\tspeed: 0.1032s/iter; left time: 8459.3364s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1239352\n",
      "\tspeed: 0.1095s/iter; left time: 8969.4330s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1055086\n",
      "\tspeed: 0.1061s/iter; left time: 8675.8780s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1100568\n",
      "\tspeed: 0.1097s/iter; left time: 8965.1607s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1202246\n",
      "\tspeed: 0.1047s/iter; left time: 8541.5707s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1090674\n",
      "\tspeed: 0.1061s/iter; left time: 8642.5609s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1158136\n",
      "\tspeed: 0.1045s/iter; left time: 8508.9976s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1121945\n",
      "\tspeed: 0.1071s/iter; left time: 8705.8946s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0966658\n",
      "\tspeed: 0.1079s/iter; left time: 8758.5712s\n",
      "\titers: 3900, epoch: 2 | loss: 0.0989787\n",
      "\tspeed: 0.1048s/iter; left time: 8495.3783s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1137499\n",
      "\tspeed: 0.1074s/iter; left time: 8700.3233s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1053024\n",
      "\tspeed: 0.1056s/iter; left time: 8543.3856s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1246223\n",
      "\tspeed: 0.1080s/iter; left time: 8722.7086s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1095846\n",
      "\tspeed: 0.1019s/iter; left time: 8221.0973s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1080796\n",
      "\tspeed: 0.1051s/iter; left time: 8465.9405s\n",
      "Epoch: 2 cost time: 00h:07m:55.95s\n",
      "Epoch: 2 | Train Loss: 0.1117745 Vali Loss: 0.1218290 Test Loss: 0.1485942\n",
      "Validation loss decreased (0.123267 --> 0.121829).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1073814\n",
      "\tspeed: 1.4114s/iter; left time: 113498.1080s\n",
      "\titers: 200, epoch: 3 | loss: 0.1077686\n",
      "\tspeed: 0.1066s/iter; left time: 8560.0081s\n",
      "\titers: 300, epoch: 3 | loss: 0.1152448\n",
      "\tspeed: 0.1063s/iter; left time: 8529.2180s\n",
      "\titers: 400, epoch: 3 | loss: 0.1090948\n",
      "\tspeed: 0.1083s/iter; left time: 8674.1726s\n",
      "\titers: 500, epoch: 3 | loss: 0.1247426\n",
      "\tspeed: 0.1057s/iter; left time: 8457.5316s\n",
      "\titers: 600, epoch: 3 | loss: 0.1079610\n",
      "\tspeed: 0.1063s/iter; left time: 8495.0244s\n",
      "\titers: 700, epoch: 3 | loss: 0.0978039\n",
      "\tspeed: 0.1015s/iter; left time: 8101.7318s\n",
      "\titers: 800, epoch: 3 | loss: 0.1046700\n",
      "\tspeed: 0.1065s/iter; left time: 8491.1226s\n",
      "\titers: 900, epoch: 3 | loss: 0.1096871\n",
      "\tspeed: 0.1093s/iter; left time: 8699.4375s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1161537\n",
      "\tspeed: 0.1058s/iter; left time: 8415.1731s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1075704\n",
      "\tspeed: 0.1060s/iter; left time: 8421.7693s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0993517\n",
      "\tspeed: 0.1085s/iter; left time: 8607.9020s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1047531\n",
      "\tspeed: 0.1059s/iter; left time: 8391.0421s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1217528\n",
      "\tspeed: 0.1048s/iter; left time: 8289.6904s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1148034\n",
      "\tspeed: 0.1097s/iter; left time: 8671.7472s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1048180\n",
      "\tspeed: 0.1082s/iter; left time: 8541.0903s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1097087\n",
      "\tspeed: 0.1065s/iter; left time: 8393.4404s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1012860\n",
      "\tspeed: 0.1089s/iter; left time: 8569.7534s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1285569\n",
      "\tspeed: 0.1107s/iter; left time: 8701.9393s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1124301\n",
      "\tspeed: 0.1103s/iter; left time: 8661.1579s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1148856\n",
      "\tspeed: 0.1060s/iter; left time: 8312.6979s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0905116\n",
      "\tspeed: 0.1039s/iter; left time: 8133.1436s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1224068\n",
      "\tspeed: 0.1103s/iter; left time: 8630.6263s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1213229\n",
      "\tspeed: 0.1031s/iter; left time: 8051.0014s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1161782\n",
      "\tspeed: 0.1136s/iter; left time: 8859.4862s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1238268\n",
      "\tspeed: 0.1084s/iter; left time: 8444.4547s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1218241\n",
      "\tspeed: 0.1141s/iter; left time: 8876.6183s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1076142\n",
      "\tspeed: 0.1111s/iter; left time: 8631.2765s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1070125\n",
      "\tspeed: 0.1054s/iter; left time: 8179.2377s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1054063\n",
      "\tspeed: 0.1093s/iter; left time: 8474.4103s\n",
      "\titers: 3100, epoch: 3 | loss: 0.0906125\n",
      "\tspeed: 0.1092s/iter; left time: 8454.3057s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1038693\n",
      "\tspeed: 0.1100s/iter; left time: 8501.2242s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0932214\n",
      "\tspeed: 0.1023s/iter; left time: 7896.3574s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1064298\n",
      "\tspeed: 0.1070s/iter; left time: 8255.0368s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1178679\n",
      "\tspeed: 0.1087s/iter; left time: 8370.1033s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1058693\n",
      "\tspeed: 0.1068s/iter; left time: 8216.2819s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1053510\n",
      "\tspeed: 0.1035s/iter; left time: 7951.2590s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1111913\n",
      "\tspeed: 0.1086s/iter; left time: 8335.0260s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1219772\n",
      "\tspeed: 0.1066s/iter; left time: 8170.2211s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1169674\n",
      "\tspeed: 0.1051s/iter; left time: 8043.9718s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1220436\n",
      "\tspeed: 0.1082s/iter; left time: 8266.0103s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0994433\n",
      "\tspeed: 0.1064s/iter; left time: 8118.7653s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1174926\n",
      "\tspeed: 0.1058s/iter; left time: 8066.3382s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1029727\n",
      "\tspeed: 0.1032s/iter; left time: 7851.9695s\n",
      "Epoch: 3 cost time: 00h:08m:00.44s\n",
      "Epoch: 3 | Train Loss: 0.1101085 Vali Loss: 0.1225816 Test Loss: 0.1483277\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.1163232\n",
      "\tspeed: 1.3902s/iter; left time: 105574.8514s\n",
      "\titers: 200, epoch: 4 | loss: 0.1172400\n",
      "\tspeed: 0.1052s/iter; left time: 7979.3333s\n",
      "\titers: 300, epoch: 4 | loss: 0.1053314\n",
      "\tspeed: 0.1014s/iter; left time: 7683.8419s\n",
      "\titers: 400, epoch: 4 | loss: 0.1159020\n",
      "\tspeed: 0.1082s/iter; left time: 8184.8829s\n",
      "\titers: 500, epoch: 4 | loss: 0.1131369\n",
      "\tspeed: 0.1073s/iter; left time: 8103.7347s\n",
      "\titers: 600, epoch: 4 | loss: 0.1101619\n",
      "\tspeed: 0.1095s/iter; left time: 8259.0579s\n",
      "\titers: 700, epoch: 4 | loss: 0.1124621\n",
      "\tspeed: 0.1051s/iter; left time: 7918.2770s\n",
      "\titers: 800, epoch: 4 | loss: 0.1200308\n",
      "\tspeed: 0.1080s/iter; left time: 8122.9645s\n",
      "\titers: 900, epoch: 4 | loss: 0.1239205\n",
      "\tspeed: 0.0996s/iter; left time: 7487.8467s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1212897\n",
      "\tspeed: 0.1060s/iter; left time: 7951.4048s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1169482\n",
      "\tspeed: 0.1068s/iter; left time: 8000.2019s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0989428\n",
      "\tspeed: 0.1103s/iter; left time: 8254.4950s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1034586\n",
      "\tspeed: 0.1093s/iter; left time: 8167.2503s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1046819\n",
      "\tspeed: 0.1049s/iter; left time: 7829.5851s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1052269\n",
      "\tspeed: 0.1072s/iter; left time: 7993.7522s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1120703\n",
      "\tspeed: 0.1058s/iter; left time: 7873.0954s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1059639\n",
      "\tspeed: 0.1120s/iter; left time: 8326.0777s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0905024\n",
      "\tspeed: 0.1119s/iter; left time: 8306.9460s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0942608\n",
      "\tspeed: 0.1082s/iter; left time: 8025.5263s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1152182\n",
      "\tspeed: 0.1061s/iter; left time: 7854.8691s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1152445\n",
      "\tspeed: 0.1064s/iter; left time: 7869.0397s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1241190\n",
      "\tspeed: 0.1049s/iter; left time: 7743.5570s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1150569\n",
      "\tspeed: 0.1040s/iter; left time: 7667.1770s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0937953\n",
      "\tspeed: 0.1090s/iter; left time: 8024.5135s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1057690\n",
      "\tspeed: 0.1082s/iter; left time: 7954.1924s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1197121\n",
      "\tspeed: 0.1095s/iter; left time: 8040.6101s\n",
      "\titers: 2700, epoch: 4 | loss: 0.1218844\n",
      "\tspeed: 0.1095s/iter; left time: 8028.6505s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1118397\n",
      "\tspeed: 0.1071s/iter; left time: 7843.6559s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1214791\n",
      "\tspeed: 0.1087s/iter; left time: 7953.8045s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0951268\n",
      "\tspeed: 0.1071s/iter; left time: 7823.3377s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1038058\n",
      "\tspeed: 0.1046s/iter; left time: 7629.2405s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1209671\n",
      "\tspeed: 0.1118s/iter; left time: 8145.5158s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1161438\n",
      "\tspeed: 0.1112s/iter; left time: 8090.8911s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1275270\n",
      "\tspeed: 0.1061s/iter; left time: 7707.2413s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1004139\n",
      "\tspeed: 0.1078s/iter; left time: 7822.1911s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1117803\n",
      "\tspeed: 0.1035s/iter; left time: 7495.5683s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1009324\n",
      "\tspeed: 0.1039s/iter; left time: 7517.2787s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1207588\n",
      "\tspeed: 0.1094s/iter; left time: 7904.0515s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1000504\n",
      "\tspeed: 0.1033s/iter; left time: 7449.6913s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0920369\n",
      "\tspeed: 0.1054s/iter; left time: 7592.6974s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0991762\n",
      "\tspeed: 0.1084s/iter; left time: 7795.7800s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0991054\n",
      "\tspeed: 0.1024s/iter; left time: 7355.2127s\n",
      "\titers: 4300, epoch: 4 | loss: 0.0927669\n",
      "\tspeed: 0.1055s/iter; left time: 7566.6614s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1017505\n",
      "\tspeed: 0.1043s/iter; left time: 7472.9461s\n",
      "Epoch: 4 cost time: 00h:07m:57.80s\n",
      "Epoch: 4 | Train Loss: 0.1083292 Vali Loss: 0.1215903 Test Loss: 0.1486534\n",
      "Validation loss decreased (0.121829 --> 0.121590).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.1045664\n",
      "\tspeed: 1.4170s/iter; left time: 101270.5476s\n",
      "\titers: 200, epoch: 5 | loss: 0.1140283\n",
      "\tspeed: 0.1071s/iter; left time: 7641.6778s\n",
      "\titers: 300, epoch: 5 | loss: 0.1094048\n",
      "\tspeed: 0.1077s/iter; left time: 7674.9133s\n",
      "\titers: 400, epoch: 5 | loss: 0.1113927\n",
      "\tspeed: 0.1065s/iter; left time: 7581.8474s\n",
      "\titers: 500, epoch: 5 | loss: 0.1201597\n",
      "\tspeed: 0.1049s/iter; left time: 7452.3135s\n",
      "\titers: 600, epoch: 5 | loss: 0.0986795\n",
      "\tspeed: 0.1046s/iter; left time: 7420.5370s\n",
      "\titers: 700, epoch: 5 | loss: 0.1055745\n",
      "\tspeed: 0.1063s/iter; left time: 7535.6783s\n",
      "\titers: 800, epoch: 5 | loss: 0.1066505\n",
      "\tspeed: 0.1145s/iter; left time: 8106.1170s\n",
      "\titers: 900, epoch: 5 | loss: 0.1212705\n",
      "\tspeed: 0.1070s/iter; left time: 7560.2260s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1213971\n",
      "\tspeed: 0.1028s/iter; left time: 7254.0450s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1246218\n",
      "\tspeed: 0.1091s/iter; left time: 7685.0141s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0953918\n",
      "\tspeed: 0.1048s/iter; left time: 7374.1161s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1080353\n",
      "\tspeed: 0.1040s/iter; left time: 7305.7016s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1139053\n",
      "\tspeed: 0.1047s/iter; left time: 7347.1126s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1034893\n",
      "\tspeed: 0.1097s/iter; left time: 7683.2607s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1069212\n",
      "\tspeed: 0.1073s/iter; left time: 7506.5233s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1042794\n",
      "\tspeed: 0.1095s/iter; left time: 7650.6211s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0975064\n",
      "\tspeed: 0.1081s/iter; left time: 7545.0210s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0997893\n",
      "\tspeed: 0.1116s/iter; left time: 7772.7529s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1189045\n",
      "\tspeed: 0.1074s/iter; left time: 7475.0823s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0985344\n",
      "\tspeed: 0.1089s/iter; left time: 7566.8525s\n",
      "\titers: 2200, epoch: 5 | loss: 0.1044963\n",
      "\tspeed: 0.1090s/iter; left time: 7558.6321s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0955015\n",
      "\tspeed: 0.1061s/iter; left time: 7350.4122s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0906615\n",
      "\tspeed: 0.1091s/iter; left time: 7543.3621s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1142124\n",
      "\tspeed: 0.1090s/iter; left time: 7525.9499s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1056934\n",
      "\tspeed: 0.1079s/iter; left time: 7441.6523s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0991283\n",
      "\tspeed: 0.1083s/iter; left time: 7461.5399s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1261882\n",
      "\tspeed: 0.1089s/iter; left time: 7488.7920s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1248673\n",
      "\tspeed: 0.1096s/iter; left time: 7524.6232s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1106820\n",
      "\tspeed: 0.1042s/iter; left time: 7143.1180s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0868233\n",
      "\tspeed: 0.1076s/iter; left time: 7364.6034s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1209657\n",
      "\tspeed: 0.1048s/iter; left time: 7165.2671s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1171299\n",
      "\tspeed: 0.1077s/iter; left time: 7353.8246s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1117137\n",
      "\tspeed: 0.1045s/iter; left time: 7125.2499s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1193861\n",
      "\tspeed: 0.1083s/iter; left time: 7368.9425s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1158210\n",
      "\tspeed: 0.1083s/iter; left time: 7363.0574s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0986645\n",
      "\tspeed: 0.1099s/iter; left time: 7456.6559s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1102005\n",
      "\tspeed: 0.1085s/iter; left time: 7352.9382s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1302816\n",
      "\tspeed: 0.1057s/iter; left time: 7152.6720s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1082248\n",
      "\tspeed: 0.1079s/iter; left time: 7290.3610s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1047803\n",
      "\tspeed: 0.1089s/iter; left time: 7345.9116s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1059765\n",
      "\tspeed: 0.1053s/iter; left time: 7091.0210s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0873054\n",
      "\tspeed: 0.1079s/iter; left time: 7258.0964s\n",
      "\titers: 4400, epoch: 5 | loss: 0.1012158\n",
      "\tspeed: 0.1042s/iter; left time: 7001.3385s\n",
      "Epoch: 5 cost time: 00h:08m:01.40s\n",
      "Epoch: 5 | Train Loss: 0.1060327 Vali Loss: 0.1232200 Test Loss: 0.1516566\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1077035\n",
      "\tspeed: 1.4035s/iter; left time: 94030.2833s\n",
      "\titers: 200, epoch: 6 | loss: 0.1206247\n",
      "\tspeed: 0.1074s/iter; left time: 7184.5248s\n",
      "\titers: 300, epoch: 6 | loss: 0.1046793\n",
      "\tspeed: 0.1044s/iter; left time: 6971.7072s\n",
      "\titers: 400, epoch: 6 | loss: 0.0925871\n",
      "\tspeed: 0.1091s/iter; left time: 7278.2041s\n",
      "\titers: 500, epoch: 6 | loss: 0.0974372\n",
      "\tspeed: 0.1096s/iter; left time: 7299.0817s\n",
      "\titers: 600, epoch: 6 | loss: 0.1108949\n",
      "\tspeed: 0.1086s/iter; left time: 7223.8138s\n",
      "\titers: 700, epoch: 6 | loss: 0.1134460\n",
      "\tspeed: 0.1101s/iter; left time: 7309.6112s\n",
      "\titers: 800, epoch: 6 | loss: 0.0946199\n",
      "\tspeed: 0.1071s/iter; left time: 7103.3394s\n",
      "\titers: 900, epoch: 6 | loss: 0.0851431\n",
      "\tspeed: 0.1081s/iter; left time: 7154.9901s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1054931\n",
      "\tspeed: 0.1100s/iter; left time: 7269.8564s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1074604\n",
      "\tspeed: 0.1037s/iter; left time: 6845.8158s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0996368\n",
      "\tspeed: 0.1108s/iter; left time: 7303.6716s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1049811\n",
      "\tspeed: 0.1074s/iter; left time: 7064.6280s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1002386\n",
      "\tspeed: 0.1093s/iter; left time: 7181.5366s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1112094\n",
      "\tspeed: 0.1092s/iter; left time: 7161.1726s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0859496\n",
      "\tspeed: 0.1080s/iter; left time: 7075.3391s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1008478\n",
      "\tspeed: 0.1079s/iter; left time: 7057.9541s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1067413\n",
      "\tspeed: 0.1082s/iter; left time: 7067.3598s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1091929\n",
      "\tspeed: 0.1096s/iter; left time: 7148.6972s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1032816\n",
      "\tspeed: 0.1079s/iter; left time: 7026.5516s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0979405\n",
      "\tspeed: 0.1039s/iter; left time: 6751.1192s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1004898\n",
      "\tspeed: 0.1035s/iter; left time: 6717.4816s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1015077\n",
      "\tspeed: 0.1087s/iter; left time: 7045.5153s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1144938\n",
      "\tspeed: 0.1025s/iter; left time: 6633.0213s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1002379\n",
      "\tspeed: 0.1101s/iter; left time: 7114.1428s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1211278\n",
      "\tspeed: 0.1075s/iter; left time: 6931.0536s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0977566\n",
      "\tspeed: 0.1100s/iter; left time: 7086.3973s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1079447\n",
      "\tspeed: 0.1057s/iter; left time: 6793.6288s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1073356\n",
      "\tspeed: 0.1090s/iter; left time: 6994.3897s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1240855\n",
      "\tspeed: 0.1067s/iter; left time: 6838.9963s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1030167\n",
      "\tspeed: 0.1030s/iter; left time: 6590.1235s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1003815\n",
      "\tspeed: 0.1048s/iter; left time: 6693.7532s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0925418\n",
      "\tspeed: 0.1090s/iter; left time: 6953.9689s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1049529\n",
      "\tspeed: 0.1094s/iter; left time: 6969.3809s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1082636\n",
      "\tspeed: 0.1070s/iter; left time: 6802.9650s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0990641\n",
      "\tspeed: 0.1055s/iter; left time: 6695.8233s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1075102\n",
      "\tspeed: 0.1093s/iter; left time: 6930.3496s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0963298\n",
      "\tspeed: 0.1073s/iter; left time: 6792.3152s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0935764\n",
      "\tspeed: 0.1070s/iter; left time: 6760.9030s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1077523\n",
      "\tspeed: 0.1069s/iter; left time: 6743.8358s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1060187\n",
      "\tspeed: 0.1052s/iter; left time: 6627.6069s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0974801\n",
      "\tspeed: 0.1114s/iter; left time: 7008.6623s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1071582\n",
      "\tspeed: 0.1063s/iter; left time: 6674.0641s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1114140\n",
      "\tspeed: 0.1048s/iter; left time: 6568.6052s\n",
      "Epoch: 6 cost time: 00h:08m:01.53s\n",
      "Epoch: 6 | Train Loss: 0.1031456 Vali Loss: 0.1234408 Test Loss: 0.1535768\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.1085103\n",
      "\tspeed: 1.3971s/iter; left time: 87349.4317s\n",
      "\titers: 200, epoch: 7 | loss: 0.0954523\n",
      "\tspeed: 0.1077s/iter; left time: 6721.8729s\n",
      "\titers: 300, epoch: 7 | loss: 0.1043126\n",
      "\tspeed: 0.1081s/iter; left time: 6734.5693s\n",
      "\titers: 400, epoch: 7 | loss: 0.0980591\n",
      "\tspeed: 0.1057s/iter; left time: 6576.6214s\n",
      "\titers: 500, epoch: 7 | loss: 0.1085198\n",
      "\tspeed: 0.1074s/iter; left time: 6671.3621s\n",
      "\titers: 600, epoch: 7 | loss: 0.1165193\n",
      "\tspeed: 0.1090s/iter; left time: 6758.4967s\n",
      "\titers: 700, epoch: 7 | loss: 0.0997535\n",
      "\tspeed: 0.1054s/iter; left time: 6526.4705s\n",
      "\titers: 800, epoch: 7 | loss: 0.1118336\n",
      "\tspeed: 0.1088s/iter; left time: 6725.1179s\n",
      "\titers: 900, epoch: 7 | loss: 0.1122482\n",
      "\tspeed: 0.1045s/iter; left time: 6452.3190s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0908243\n",
      "\tspeed: 0.1102s/iter; left time: 6790.8038s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1042221\n",
      "\tspeed: 0.1037s/iter; left time: 6378.0513s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0784654\n",
      "\tspeed: 0.1106s/iter; left time: 6791.1001s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1083403\n",
      "\tspeed: 0.1066s/iter; left time: 6534.6632s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0916019\n",
      "\tspeed: 0.1115s/iter; left time: 6827.2860s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1041972\n",
      "\tspeed: 0.1087s/iter; left time: 6643.6646s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1050012\n",
      "\tspeed: 0.1072s/iter; left time: 6542.1357s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0990551\n",
      "\tspeed: 0.1104s/iter; left time: 6725.7379s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1112182\n",
      "\tspeed: 0.1071s/iter; left time: 6513.3864s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0934280\n",
      "\tspeed: 0.1108s/iter; left time: 6729.4060s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0962716\n",
      "\tspeed: 0.1102s/iter; left time: 6680.9932s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0991424\n",
      "\tspeed: 0.1107s/iter; left time: 6702.2612s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0782585\n",
      "\tspeed: 0.1090s/iter; left time: 6583.4074s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0830829\n",
      "\tspeed: 0.1090s/iter; left time: 6574.1501s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1002160\n",
      "\tspeed: 0.1094s/iter; left time: 6590.2696s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0800429\n",
      "\tspeed: 0.1082s/iter; left time: 6507.8364s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1001138\n",
      "\tspeed: 0.1092s/iter; left time: 6554.4086s\n",
      "\titers: 2700, epoch: 7 | loss: 0.1066886\n",
      "\tspeed: 0.1124s/iter; left time: 6735.6474s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1104560\n",
      "\tspeed: 0.1086s/iter; left time: 6498.8932s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1018884\n",
      "\tspeed: 0.1014s/iter; left time: 6058.3551s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1037253\n",
      "\tspeed: 0.0963s/iter; left time: 5740.6386s\n",
      "\titers: 3100, epoch: 7 | loss: 0.1261247\n",
      "\tspeed: 0.0961s/iter; left time: 5722.7381s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1097564\n",
      "\tspeed: 0.1116s/iter; left time: 6630.2530s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0908472\n",
      "\tspeed: 0.1141s/iter; left time: 6771.3907s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0831330\n",
      "\tspeed: 0.1136s/iter; left time: 6726.1647s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0842484\n",
      "\tspeed: 0.1135s/iter; left time: 6708.4212s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1124989\n",
      "\tspeed: 0.1128s/iter; left time: 6655.2255s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1083518\n",
      "\tspeed: 0.1097s/iter; left time: 6463.4231s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1133735\n",
      "\tspeed: 0.1045s/iter; left time: 6144.8654s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0848982\n",
      "\tspeed: 0.1061s/iter; left time: 6228.8058s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1067260\n",
      "\tspeed: 0.1049s/iter; left time: 6148.3412s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0981014\n",
      "\tspeed: 0.1032s/iter; left time: 6036.7361s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0832176\n",
      "\tspeed: 0.1103s/iter; left time: 6443.1575s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1142252\n",
      "\tspeed: 0.1051s/iter; left time: 6131.8096s\n",
      "\titers: 4400, epoch: 7 | loss: 0.1292363\n",
      "\tspeed: 0.1079s/iter; left time: 6283.6865s\n",
      "Epoch: 7 cost time: 00h:08m:03.49s\n",
      "Epoch: 7 | Train Loss: 0.1004053 Vali Loss: 0.1252791 Test Loss: 0.1567259\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0986653\n",
      "\tspeed: 1.3887s/iter; left time: 80615.4066s\n",
      "\titers: 200, epoch: 8 | loss: 0.0983105\n",
      "\tspeed: 0.1070s/iter; left time: 6202.1602s\n",
      "\titers: 300, epoch: 8 | loss: 0.1024780\n",
      "\tspeed: 0.1065s/iter; left time: 6162.3177s\n",
      "\titers: 400, epoch: 8 | loss: 0.1091874\n",
      "\tspeed: 0.1058s/iter; left time: 6108.2424s\n",
      "\titers: 500, epoch: 8 | loss: 0.0796972\n",
      "\tspeed: 0.1044s/iter; left time: 6021.4761s\n",
      "\titers: 600, epoch: 8 | loss: 0.1151457\n",
      "\tspeed: 0.1078s/iter; left time: 6202.3343s\n",
      "\titers: 700, epoch: 8 | loss: 0.0923439\n",
      "\tspeed: 0.1044s/iter; left time: 5998.1825s\n",
      "\titers: 800, epoch: 8 | loss: 0.0942501\n",
      "\tspeed: 0.1056s/iter; left time: 6054.1411s\n",
      "\titers: 900, epoch: 8 | loss: 0.0994631\n",
      "\tspeed: 0.1035s/iter; left time: 5925.5695s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0976768\n",
      "\tspeed: 0.1049s/iter; left time: 5994.5175s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0949048\n",
      "\tspeed: 0.1066s/iter; left time: 6083.7003s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0971709\n",
      "\tspeed: 0.1062s/iter; left time: 6049.0040s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0919779\n",
      "\tspeed: 0.1087s/iter; left time: 6177.4734s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0921907\n",
      "\tspeed: 0.1005s/iter; left time: 5705.4225s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1062825\n",
      "\tspeed: 0.1044s/iter; left time: 5916.4223s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0986134\n",
      "\tspeed: 0.1040s/iter; left time: 5881.4220s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0987850\n",
      "\tspeed: 0.1087s/iter; left time: 6137.8484s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0880046\n",
      "\tspeed: 0.1025s/iter; left time: 5777.4243s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0998610\n",
      "\tspeed: 0.1077s/iter; left time: 6060.3033s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1191346\n",
      "\tspeed: 0.1032s/iter; left time: 5796.0598s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0997336\n",
      "\tspeed: 0.1071s/iter; left time: 6003.0262s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0977681\n",
      "\tspeed: 0.1060s/iter; left time: 5929.8658s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0942919\n",
      "\tspeed: 0.1015s/iter; left time: 5668.3825s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1181585\n",
      "\tspeed: 0.1077s/iter; left time: 6002.0095s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0900528\n",
      "\tspeed: 0.1019s/iter; left time: 5669.7670s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1048671\n",
      "\tspeed: 0.1085s/iter; left time: 6028.8179s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0932050\n",
      "\tspeed: 0.1105s/iter; left time: 6127.2632s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1049393\n",
      "\tspeed: 0.1059s/iter; left time: 5859.5793s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0715457\n",
      "\tspeed: 0.1100s/iter; left time: 6076.3097s\n",
      "\titers: 3000, epoch: 8 | loss: 0.1109473\n",
      "\tspeed: 0.1064s/iter; left time: 5870.6251s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0979033\n",
      "\tspeed: 0.1070s/iter; left time: 5892.5974s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1071441\n",
      "\tspeed: 0.1064s/iter; left time: 5847.0757s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0768796\n",
      "\tspeed: 0.1068s/iter; left time: 5857.9075s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0921377\n",
      "\tspeed: 0.1109s/iter; left time: 6069.4385s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0841669\n",
      "\tspeed: 0.1046s/iter; left time: 5718.1633s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0960754\n",
      "\tspeed: 0.1100s/iter; left time: 6002.2830s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0833702\n",
      "\tspeed: 0.1091s/iter; left time: 5942.8118s\n",
      "\titers: 3800, epoch: 8 | loss: 0.1056035\n",
      "\tspeed: 0.1073s/iter; left time: 5829.5628s\n",
      "\titers: 3900, epoch: 8 | loss: 0.1034919\n",
      "\tspeed: 0.1085s/iter; left time: 5884.0160s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0962752\n",
      "\tspeed: 0.1091s/iter; left time: 5909.0832s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0970299\n",
      "\tspeed: 0.1030s/iter; left time: 5567.9175s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0906406\n",
      "\tspeed: 0.1062s/iter; left time: 5728.8146s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0950922\n",
      "\tspeed: 0.1062s/iter; left time: 5717.0580s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1170644\n",
      "\tspeed: 0.1060s/iter; left time: 5696.4968s\n",
      "Epoch: 8 cost time: 00h:07m:55.42s\n",
      "Epoch: 8 | Train Loss: 0.0976592 Vali Loss: 0.1248156 Test Loss: 0.1552406\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.1081889\n",
      "\tspeed: 1.3984s/iter; left time: 74921.6007s\n",
      "\titers: 200, epoch: 9 | loss: 0.0912347\n",
      "\tspeed: 0.1082s/iter; left time: 5787.3827s\n",
      "\titers: 300, epoch: 9 | loss: 0.0898880\n",
      "\tspeed: 0.1094s/iter; left time: 5838.8974s\n",
      "\titers: 400, epoch: 9 | loss: 0.1006834\n",
      "\tspeed: 0.1102s/iter; left time: 5872.0914s\n",
      "\titers: 500, epoch: 9 | loss: 0.1055464\n",
      "\tspeed: 0.1077s/iter; left time: 5727.9411s\n",
      "\titers: 600, epoch: 9 | loss: 0.0820070\n",
      "\tspeed: 0.1102s/iter; left time: 5849.4555s\n",
      "\titers: 700, epoch: 9 | loss: 0.1049108\n",
      "\tspeed: 0.1066s/iter; left time: 5646.1689s\n",
      "\titers: 800, epoch: 9 | loss: 0.1005554\n",
      "\tspeed: 0.1050s/iter; left time: 5552.1492s\n",
      "\titers: 900, epoch: 9 | loss: 0.0876917\n",
      "\tspeed: 0.1079s/iter; left time: 5696.0949s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0906235\n",
      "\tspeed: 0.1055s/iter; left time: 5559.9999s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0937413\n",
      "\tspeed: 0.1011s/iter; left time: 5317.8502s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1121209\n",
      "\tspeed: 0.1073s/iter; left time: 5628.7097s\n",
      "\titers: 1300, epoch: 9 | loss: 0.1008148\n",
      "\tspeed: 0.1059s/iter; left time: 5546.6378s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0889356\n",
      "\tspeed: 0.1017s/iter; left time: 5317.7008s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0898405\n",
      "\tspeed: 0.1116s/iter; left time: 5822.3724s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0989260\n",
      "\tspeed: 0.1074s/iter; left time: 5591.4003s\n",
      "\titers: 1700, epoch: 9 | loss: 0.1049883\n",
      "\tspeed: 0.1090s/iter; left time: 5663.6288s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0921862\n",
      "\tspeed: 0.1057s/iter; left time: 5481.6351s\n",
      "\titers: 1900, epoch: 9 | loss: 0.1090681\n",
      "\tspeed: 0.1083s/iter; left time: 5608.5675s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0879408\n",
      "\tspeed: 0.1095s/iter; left time: 5658.3337s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0999407\n",
      "\tspeed: 0.1078s/iter; left time: 5562.2738s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0866363\n",
      "\tspeed: 0.1113s/iter; left time: 5727.4817s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1170187\n",
      "\tspeed: 0.1026s/iter; left time: 5271.4964s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0945719\n",
      "\tspeed: 0.1057s/iter; left time: 5419.3406s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0903585\n",
      "\tspeed: 0.1045s/iter; left time: 5349.5437s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0857023\n",
      "\tspeed: 0.1057s/iter; left time: 5400.0068s\n",
      "\titers: 2700, epoch: 9 | loss: 0.1100801\n",
      "\tspeed: 0.1023s/iter; left time: 5212.4255s\n",
      "\titers: 2800, epoch: 9 | loss: 0.0914453\n",
      "\tspeed: 0.1075s/iter; left time: 5467.4288s\n",
      "\titers: 2900, epoch: 9 | loss: 0.1086308\n",
      "\tspeed: 0.1097s/iter; left time: 5567.7597s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1012945\n",
      "\tspeed: 0.1082s/iter; left time: 5481.6941s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0891736\n",
      "\tspeed: 0.1059s/iter; left time: 5353.9411s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0943430\n",
      "\tspeed: 0.1087s/iter; left time: 5485.3078s\n",
      "\titers: 3300, epoch: 9 | loss: 0.1029650\n",
      "\tspeed: 0.1107s/iter; left time: 5574.2274s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0784484\n",
      "\tspeed: 0.1102s/iter; left time: 5542.7671s\n",
      "\titers: 3500, epoch: 9 | loss: 0.1020526\n",
      "\tspeed: 0.1077s/iter; left time: 5406.4676s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0987637\n",
      "\tspeed: 0.1053s/iter; left time: 5273.0119s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0952342\n",
      "\tspeed: 0.1086s/iter; left time: 5426.0940s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1057804\n",
      "\tspeed: 0.1071s/iter; left time: 5340.4882s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0955375\n",
      "\tspeed: 0.1068s/iter; left time: 5316.8251s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0782716\n",
      "\tspeed: 0.1136s/iter; left time: 5640.9163s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0951779\n",
      "\tspeed: 0.1067s/iter; left time: 5290.7223s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0944427\n",
      "\tspeed: 0.0983s/iter; left time: 4865.4189s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0884099\n",
      "\tspeed: 0.1075s/iter; left time: 5310.1218s\n",
      "\titers: 4400, epoch: 9 | loss: 0.1025119\n",
      "\tspeed: 0.1111s/iter; left time: 5473.7040s\n",
      "Epoch: 9 cost time: 00h:08m:00.48s\n",
      "Epoch: 9 | Train Loss: 0.0950745 Vali Loss: 0.1270755 Test Loss: 0.1571813\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.04692179709672928, rmse:0.21661439538002014, mae:0.14865338802337646, rse:0.7507471442222595\n",
      "success delete checkpoints\n",
      "Intermediate time for GB and pred_len 168: 01h:32m:49.29s\n",
      "\n",
      "Intermediate time for GB: 05h:13m:41.59s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 13:55:28,723] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 13:55:29,733] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 13:55:29,733] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 13:55:29,733] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 13:55:29,819] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 13:55:29,819] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 13:55:30,452] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 13:55:30,453] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 13:55:30,453] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 13:55:30,454] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 13:55:30,454] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 13:55:30,454] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 13:55:30,455] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 13:55:30,811] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 13:55:30,812] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:30,813] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:30,939] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 13:55:30,940] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:30,940] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:30,940] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 13:55:31,057] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 13:55:31,058] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 13:55:31,058] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 114.85 GB, percent = 15.2%\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 13:55:31,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 13:55:31,060] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd48878e3d0>\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 13:55:31,061] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 13:55:31,062] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 13:55:31,063] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1712448\n",
      "\tspeed: 0.1613s/iter; left time: 8684.6738s\n",
      "\titers: 200, epoch: 1 | loss: 0.2023432\n",
      "\tspeed: 0.1201s/iter; left time: 6453.8491s\n",
      "\titers: 300, epoch: 1 | loss: 0.1408175\n",
      "\tspeed: 0.1178s/iter; left time: 6317.8376s\n",
      "\titers: 400, epoch: 1 | loss: 0.1179628\n",
      "\tspeed: 0.1235s/iter; left time: 6610.1309s\n",
      "\titers: 500, epoch: 1 | loss: 0.0901266\n",
      "\tspeed: 0.1234s/iter; left time: 6593.8340s\n",
      "\titers: 600, epoch: 1 | loss: 0.0898690\n",
      "\tspeed: 0.1213s/iter; left time: 6472.8916s\n",
      "\titers: 700, epoch: 1 | loss: 0.0787271\n",
      "\tspeed: 0.1163s/iter; left time: 6190.2511s\n",
      "\titers: 800, epoch: 1 | loss: 0.1065296\n",
      "\tspeed: 0.1186s/iter; left time: 6304.2562s\n",
      "\titers: 900, epoch: 1 | loss: 0.0820746\n",
      "\tspeed: 0.1241s/iter; left time: 6579.9415s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0925007\n",
      "\tspeed: 0.1162s/iter; left time: 6153.6498s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0864945\n",
      "\tspeed: 0.1213s/iter; left time: 6410.4135s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0853928\n",
      "\tspeed: 0.1245s/iter; left time: 6563.8128s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0853296\n",
      "\tspeed: 0.1227s/iter; left time: 6456.6093s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0815412\n",
      "\tspeed: 0.1212s/iter; left time: 6365.9414s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0878881\n",
      "\tspeed: 0.1242s/iter; left time: 6513.1136s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0781828\n",
      "\tspeed: 0.1190s/iter; left time: 6227.6744s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0933578\n",
      "\tspeed: 0.1215s/iter; left time: 6345.8813s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0840652\n",
      "\tspeed: 0.1187s/iter; left time: 6191.7086s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0744049\n",
      "\tspeed: 0.1190s/iter; left time: 6195.1130s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0831344\n",
      "\tspeed: 0.1154s/iter; left time: 5995.7890s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0764061\n",
      "\tspeed: 0.1185s/iter; left time: 6145.6551s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0826099\n",
      "\tspeed: 0.1246s/iter; left time: 6444.6799s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0772082\n",
      "\tspeed: 0.1175s/iter; left time: 6069.2087s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0781461\n",
      "\tspeed: 0.1212s/iter; left time: 6245.1723s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0887459\n",
      "\tspeed: 0.1202s/iter; left time: 6184.5045s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0926115\n",
      "\tspeed: 0.1213s/iter; left time: 6225.8487s\n",
      "Epoch: 1 cost time: 00h:05m:25.87s\n",
      "Epoch: 1 | Train Loss: 0.1003548 Vali Loss: 0.0686999 Test Loss: 0.0770876\n",
      "Validation loss decreased (inf --> 0.068700).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0797757\n",
      "\tspeed: 1.1321s/iter; left time: 57899.3057s\n",
      "\titers: 200, epoch: 2 | loss: 0.0827035\n",
      "\tspeed: 0.1055s/iter; left time: 5382.8729s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977232\n",
      "\tspeed: 0.1085s/iter; left time: 5525.9815s\n",
      "\titers: 400, epoch: 2 | loss: 0.0749713\n",
      "\tspeed: 0.1031s/iter; left time: 5244.2386s\n",
      "\titers: 500, epoch: 2 | loss: 0.0862913\n",
      "\tspeed: 0.1044s/iter; left time: 5298.2570s\n",
      "\titers: 600, epoch: 2 | loss: 0.0929019\n",
      "\tspeed: 0.1039s/iter; left time: 5260.8256s\n",
      "\titers: 700, epoch: 2 | loss: 0.0713457\n",
      "\tspeed: 0.1070s/iter; left time: 5410.1494s\n",
      "\titers: 800, epoch: 2 | loss: 0.0867402\n",
      "\tspeed: 0.1100s/iter; left time: 5549.3582s\n",
      "\titers: 900, epoch: 2 | loss: 0.0903707\n",
      "\tspeed: 0.1077s/iter; left time: 5421.1584s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0810028\n",
      "\tspeed: 0.1094s/iter; left time: 5498.9982s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0841704\n",
      "\tspeed: 0.1074s/iter; left time: 5383.8395s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0849176\n",
      "\tspeed: 0.1061s/iter; left time: 5308.9948s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0906385\n",
      "\tspeed: 0.1059s/iter; left time: 5289.1828s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0782810\n",
      "\tspeed: 0.1071s/iter; left time: 5340.6999s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0824517\n",
      "\tspeed: 0.1119s/iter; left time: 5564.5417s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0916397\n",
      "\tspeed: 0.1071s/iter; left time: 5316.0470s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0733048\n",
      "\tspeed: 0.1027s/iter; left time: 5087.5426s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0718356\n",
      "\tspeed: 0.1049s/iter; left time: 5185.6725s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0834526\n",
      "\tspeed: 0.1045s/iter; left time: 5157.8781s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0808225\n",
      "\tspeed: 0.1083s/iter; left time: 5333.7927s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0826184\n",
      "\tspeed: 0.1001s/iter; left time: 4919.7559s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0727913\n",
      "\tspeed: 0.1052s/iter; left time: 5157.3315s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0784774\n",
      "\tspeed: 0.1051s/iter; left time: 5142.4298s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0701629\n",
      "\tspeed: 0.1095s/iter; left time: 5348.5788s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0810242\n",
      "\tspeed: 0.1046s/iter; left time: 5099.7382s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0680333\n",
      "\tspeed: 0.1039s/iter; left time: 5052.3724s\n",
      "Epoch: 2 cost time: 00h:04m:47.49s\n",
      "Epoch: 2 | Train Loss: 0.0786239 Vali Loss: 0.0661815 Test Loss: 0.0743510\n",
      "Validation loss decreased (0.068700 --> 0.066182).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0761278\n",
      "\tspeed: 0.9867s/iter; left time: 47803.7469s\n",
      "\titers: 200, epoch: 3 | loss: 0.0774389\n",
      "\tspeed: 0.1055s/iter; left time: 5100.0690s\n",
      "\titers: 300, epoch: 3 | loss: 0.0796090\n",
      "\tspeed: 0.1107s/iter; left time: 5339.9299s\n",
      "\titers: 400, epoch: 3 | loss: 0.0677147\n",
      "\tspeed: 0.1045s/iter; left time: 5033.4590s\n",
      "\titers: 500, epoch: 3 | loss: 0.0734856\n",
      "\tspeed: 0.1086s/iter; left time: 5219.7809s\n",
      "\titers: 600, epoch: 3 | loss: 0.0857239\n",
      "\tspeed: 0.1067s/iter; left time: 5115.9864s\n",
      "\titers: 700, epoch: 3 | loss: 0.0681393\n",
      "\tspeed: 0.1084s/iter; left time: 5184.6248s\n",
      "\titers: 800, epoch: 3 | loss: 0.0724246\n",
      "\tspeed: 0.1068s/iter; left time: 5097.0364s\n",
      "\titers: 900, epoch: 3 | loss: 0.0910780\n",
      "\tspeed: 0.1055s/iter; left time: 5026.2883s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0728555\n",
      "\tspeed: 0.1088s/iter; left time: 5175.3988s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0870226\n",
      "\tspeed: 0.1051s/iter; left time: 4985.4615s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0779801\n",
      "\tspeed: 0.1072s/iter; left time: 5077.1261s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0614597\n",
      "\tspeed: 0.1076s/iter; left time: 5083.3829s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0864894\n",
      "\tspeed: 0.1089s/iter; left time: 5134.2734s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0696164\n",
      "\tspeed: 0.1022s/iter; left time: 4809.6830s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0833256\n",
      "\tspeed: 0.1075s/iter; left time: 5047.4433s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0790411\n",
      "\tspeed: 0.1090s/iter; left time: 5106.4661s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0904982\n",
      "\tspeed: 0.1093s/iter; left time: 5111.1672s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0717343\n",
      "\tspeed: 0.1096s/iter; left time: 5113.7256s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0635259\n",
      "\tspeed: 0.1064s/iter; left time: 4951.5607s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0628298\n",
      "\tspeed: 0.1118s/iter; left time: 5192.2139s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0658889\n",
      "\tspeed: 0.1083s/iter; left time: 5019.2886s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0762297\n",
      "\tspeed: 0.1085s/iter; left time: 5019.5641s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0619692\n",
      "\tspeed: 0.1069s/iter; left time: 4933.1645s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0707970\n",
      "\tspeed: 0.1029s/iter; left time: 4736.3141s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0759995\n",
      "\tspeed: 0.1077s/iter; left time: 4947.2761s\n",
      "Epoch: 3 cost time: 00h:04m:50.10s\n",
      "Epoch: 3 | Train Loss: 0.0745886 Vali Loss: 0.0614387 Test Loss: 0.0695850\n",
      "Validation loss decreased (0.066182 --> 0.061439).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0740035\n",
      "\tspeed: 0.9679s/iter; left time: 44283.2179s\n",
      "\titers: 200, epoch: 4 | loss: 0.0772850\n",
      "\tspeed: 0.1074s/iter; left time: 4900.6231s\n",
      "\titers: 300, epoch: 4 | loss: 0.0706134\n",
      "\tspeed: 0.1053s/iter; left time: 4795.4921s\n",
      "\titers: 400, epoch: 4 | loss: 0.0690192\n",
      "\tspeed: 0.1057s/iter; left time: 4804.5128s\n",
      "\titers: 500, epoch: 4 | loss: 0.0804873\n",
      "\tspeed: 0.1013s/iter; left time: 4594.0107s\n",
      "\titers: 600, epoch: 4 | loss: 0.0721374\n",
      "\tspeed: 0.1108s/iter; left time: 5015.3915s\n",
      "\titers: 700, epoch: 4 | loss: 0.0607685\n",
      "\tspeed: 0.1113s/iter; left time: 5023.6644s\n",
      "\titers: 800, epoch: 4 | loss: 0.0716907\n",
      "\tspeed: 0.1018s/iter; left time: 4588.0548s\n",
      "\titers: 900, epoch: 4 | loss: 0.0583386\n",
      "\tspeed: 0.1101s/iter; left time: 4949.3779s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0691884\n",
      "\tspeed: 0.1091s/iter; left time: 4894.8875s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0729936\n",
      "\tspeed: 0.1058s/iter; left time: 4735.7368s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0830592\n",
      "\tspeed: 0.1080s/iter; left time: 4820.6197s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0928226\n",
      "\tspeed: 0.1067s/iter; left time: 4751.9578s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0729482\n",
      "\tspeed: 0.1092s/iter; left time: 4853.6016s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0764900\n",
      "\tspeed: 0.1073s/iter; left time: 4759.7122s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0752271\n",
      "\tspeed: 0.1103s/iter; left time: 4882.1301s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0638424\n",
      "\tspeed: 0.1099s/iter; left time: 4851.0980s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0620330\n",
      "\tspeed: 0.1105s/iter; left time: 4866.3266s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0701117\n",
      "\tspeed: 0.1043s/iter; left time: 4581.8071s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0681855\n",
      "\tspeed: 0.1073s/iter; left time: 4706.3663s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0762056\n",
      "\tspeed: 0.1052s/iter; left time: 4600.4788s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0690991\n",
      "\tspeed: 0.1053s/iter; left time: 4596.0362s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0679242\n",
      "\tspeed: 0.1066s/iter; left time: 4643.4121s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0739057\n",
      "\tspeed: 0.1058s/iter; left time: 4597.1895s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0679740\n",
      "\tspeed: 0.1057s/iter; left time: 4584.1439s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0779216\n",
      "\tspeed: 0.1067s/iter; left time: 4616.2239s\n",
      "Epoch: 4 cost time: 00h:04m:49.39s\n",
      "Epoch: 4 | Train Loss: 0.0718865 Vali Loss: 0.0601031 Test Loss: 0.0689823\n",
      "Validation loss decreased (0.061439 --> 0.060103).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0688475\n",
      "\tspeed: 0.9751s/iter; left time: 41981.7101s\n",
      "\titers: 200, epoch: 5 | loss: 0.0738413\n",
      "\tspeed: 0.1063s/iter; left time: 4565.6181s\n",
      "\titers: 300, epoch: 5 | loss: 0.0718522\n",
      "\tspeed: 0.1099s/iter; left time: 4707.6271s\n",
      "\titers: 400, epoch: 5 | loss: 0.0661217\n",
      "\tspeed: 0.1075s/iter; left time: 4598.0354s\n",
      "\titers: 500, epoch: 5 | loss: 0.0626277\n",
      "\tspeed: 0.1086s/iter; left time: 4632.2707s\n",
      "\titers: 600, epoch: 5 | loss: 0.0671054\n",
      "\tspeed: 0.1065s/iter; left time: 4532.0856s\n",
      "\titers: 700, epoch: 5 | loss: 0.0719781\n",
      "\tspeed: 0.1053s/iter; left time: 4472.0714s\n",
      "\titers: 800, epoch: 5 | loss: 0.0713939\n",
      "\tspeed: 0.1057s/iter; left time: 4474.6205s\n",
      "\titers: 900, epoch: 5 | loss: 0.0793560\n",
      "\tspeed: 0.1075s/iter; left time: 4543.0394s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0704821\n",
      "\tspeed: 0.1057s/iter; left time: 4457.0025s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0659771\n",
      "\tspeed: 0.1056s/iter; left time: 4442.2638s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0808677\n",
      "\tspeed: 0.1117s/iter; left time: 4684.9370s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0577165\n",
      "\tspeed: 0.1059s/iter; left time: 4433.9060s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0701355\n",
      "\tspeed: 0.1056s/iter; left time: 4407.8319s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0783938\n",
      "\tspeed: 0.1061s/iter; left time: 4420.4042s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0703886\n",
      "\tspeed: 0.1086s/iter; left time: 4513.3542s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0750756\n",
      "\tspeed: 0.1106s/iter; left time: 4585.5776s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0745274\n",
      "\tspeed: 0.1045s/iter; left time: 4321.2924s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0632909\n",
      "\tspeed: 0.1065s/iter; left time: 4392.0168s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0572714\n",
      "\tspeed: 0.1027s/iter; left time: 4225.5792s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0697682\n",
      "\tspeed: 0.1047s/iter; left time: 4296.4321s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0775003\n",
      "\tspeed: 0.1066s/iter; left time: 4364.6123s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0681202\n",
      "\tspeed: 0.1059s/iter; left time: 4325.2325s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0675049\n",
      "\tspeed: 0.1107s/iter; left time: 4509.7213s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0756652\n",
      "\tspeed: 0.1064s/iter; left time: 4325.3102s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0688920\n",
      "\tspeed: 0.1081s/iter; left time: 4385.1994s\n",
      "Epoch: 5 cost time: 00h:04m:48.80s\n",
      "Epoch: 5 | Train Loss: 0.0703978 Vali Loss: 0.0614755 Test Loss: 0.0700821\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0642571\n",
      "\tspeed: 0.9516s/iter; left time: 38401.7352s\n",
      "\titers: 200, epoch: 6 | loss: 0.0717407\n",
      "\tspeed: 0.1097s/iter; left time: 4415.2552s\n",
      "\titers: 300, epoch: 6 | loss: 0.0590067\n",
      "\tspeed: 0.1089s/iter; left time: 4373.4624s\n",
      "\titers: 400, epoch: 6 | loss: 0.0612367\n",
      "\tspeed: 0.1097s/iter; left time: 4394.4029s\n",
      "\titers: 500, epoch: 6 | loss: 0.0722813\n",
      "\tspeed: 0.1082s/iter; left time: 4321.3869s\n",
      "\titers: 600, epoch: 6 | loss: 0.0766656\n",
      "\tspeed: 0.1096s/iter; left time: 4366.7746s\n",
      "\titers: 700, epoch: 6 | loss: 0.0786778\n",
      "\tspeed: 0.1089s/iter; left time: 4328.6553s\n",
      "\titers: 800, epoch: 6 | loss: 0.0702733\n",
      "\tspeed: 0.1044s/iter; left time: 4141.1896s\n",
      "\titers: 900, epoch: 6 | loss: 0.0562902\n",
      "\tspeed: 0.1085s/iter; left time: 4291.9032s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0571464\n",
      "\tspeed: 0.1066s/iter; left time: 4205.6505s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0545283\n",
      "\tspeed: 0.1085s/iter; left time: 4269.9388s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0686143\n",
      "\tspeed: 0.1105s/iter; left time: 4337.5885s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0655344\n",
      "\tspeed: 0.1104s/iter; left time: 4324.0077s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0677483\n",
      "\tspeed: 0.1075s/iter; left time: 4198.1122s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0722037\n",
      "\tspeed: 0.1065s/iter; left time: 4146.9597s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0698562\n",
      "\tspeed: 0.1077s/iter; left time: 4183.5869s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0730506\n",
      "\tspeed: 0.1072s/iter; left time: 4154.0216s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0689412\n",
      "\tspeed: 0.1093s/iter; left time: 4225.8563s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0618468\n",
      "\tspeed: 0.1071s/iter; left time: 4131.1059s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0735297\n",
      "\tspeed: 0.1109s/iter; left time: 4265.9072s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0774064\n",
      "\tspeed: 0.1104s/iter; left time: 4235.9062s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0620106\n",
      "\tspeed: 0.1078s/iter; left time: 4123.6705s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0610147\n",
      "\tspeed: 0.1113s/iter; left time: 4245.3779s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0655296\n",
      "\tspeed: 0.1088s/iter; left time: 4139.2595s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0643461\n",
      "\tspeed: 0.1184s/iter; left time: 4495.1187s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0650501\n",
      "\tspeed: 0.1208s/iter; left time: 4572.8638s\n",
      "Epoch: 6 cost time: 00h:04m:56.77s\n",
      "Epoch: 6 | Train Loss: 0.0693189 Vali Loss: 0.0591683 Test Loss: 0.0672605\n",
      "Validation loss decreased (0.060103 --> 0.059168).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0697974\n",
      "\tspeed: 0.9952s/iter; left time: 37476.9033s\n",
      "\titers: 200, epoch: 7 | loss: 0.0755775\n",
      "\tspeed: 0.1059s/iter; left time: 3977.0852s\n",
      "\titers: 300, epoch: 7 | loss: 0.0618644\n",
      "\tspeed: 0.1065s/iter; left time: 3990.2135s\n",
      "\titers: 400, epoch: 7 | loss: 0.0668553\n",
      "\tspeed: 0.1030s/iter; left time: 3846.8323s\n",
      "\titers: 500, epoch: 7 | loss: 0.0719296\n",
      "\tspeed: 0.1103s/iter; left time: 4109.9221s\n",
      "\titers: 600, epoch: 7 | loss: 0.0714099\n",
      "\tspeed: 0.1059s/iter; left time: 3934.2465s\n",
      "\titers: 700, epoch: 7 | loss: 0.0720902\n",
      "\tspeed: 0.1040s/iter; left time: 3855.2449s\n",
      "\titers: 800, epoch: 7 | loss: 0.0795342\n",
      "\tspeed: 0.1064s/iter; left time: 3932.9802s\n",
      "\titers: 900, epoch: 7 | loss: 0.0655722\n",
      "\tspeed: 0.1070s/iter; left time: 3943.9740s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0711685\n",
      "\tspeed: 0.1080s/iter; left time: 3968.4331s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0755553\n",
      "\tspeed: 0.1060s/iter; left time: 3887.0493s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0766494\n",
      "\tspeed: 0.1044s/iter; left time: 3815.7570s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0651874\n",
      "\tspeed: 0.1074s/iter; left time: 3917.2365s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0650540\n",
      "\tspeed: 0.1055s/iter; left time: 3836.7274s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0855093\n",
      "\tspeed: 0.1028s/iter; left time: 3725.6440s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0753963\n",
      "\tspeed: 0.1103s/iter; left time: 3988.5567s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0613949\n",
      "\tspeed: 0.1048s/iter; left time: 3779.8557s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0692656\n",
      "\tspeed: 0.1026s/iter; left time: 3689.8381s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0711877\n",
      "\tspeed: 0.1039s/iter; left time: 3723.9673s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0700283\n",
      "\tspeed: 0.1074s/iter; left time: 3842.0613s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0562870\n",
      "\tspeed: 0.1044s/iter; left time: 3721.6851s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0848084\n",
      "\tspeed: 0.1066s/iter; left time: 3788.9110s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0658100\n",
      "\tspeed: 0.1067s/iter; left time: 3784.0007s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0565042\n",
      "\tspeed: 0.1054s/iter; left time: 3727.2828s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0774072\n",
      "\tspeed: 0.1059s/iter; left time: 3734.5611s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0680225\n",
      "\tspeed: 0.1102s/iter; left time: 3874.4911s\n",
      "Epoch: 7 cost time: 00h:04m:46.58s\n",
      "Epoch: 7 | Train Loss: 0.0684043 Vali Loss: 0.0598554 Test Loss: 0.0685234\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0863287\n",
      "\tspeed: 0.9523s/iter; left time: 33295.2367s\n",
      "\titers: 200, epoch: 8 | loss: 0.0786934\n",
      "\tspeed: 0.1030s/iter; left time: 3590.2538s\n",
      "\titers: 300, epoch: 8 | loss: 0.0739644\n",
      "\tspeed: 0.1023s/iter; left time: 3556.8383s\n",
      "\titers: 400, epoch: 8 | loss: 0.0589355\n",
      "\tspeed: 0.1054s/iter; left time: 3652.8755s\n",
      "\titers: 500, epoch: 8 | loss: 0.0686779\n",
      "\tspeed: 0.1068s/iter; left time: 3692.0885s\n",
      "\titers: 600, epoch: 8 | loss: 0.0584960\n",
      "\tspeed: 0.1051s/iter; left time: 3621.0724s\n",
      "\titers: 700, epoch: 8 | loss: 0.0871281\n",
      "\tspeed: 0.1059s/iter; left time: 3637.8838s\n",
      "\titers: 800, epoch: 8 | loss: 0.0599374\n",
      "\tspeed: 0.1125s/iter; left time: 3855.8626s\n",
      "\titers: 900, epoch: 8 | loss: 0.0789052\n",
      "\tspeed: 0.1069s/iter; left time: 3652.5201s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0873202\n",
      "\tspeed: 0.1076s/iter; left time: 3665.3601s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0671721\n",
      "\tspeed: 0.1031s/iter; left time: 3501.1413s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0676801\n",
      "\tspeed: 0.1005s/iter; left time: 3403.1546s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0818329\n",
      "\tspeed: 0.1073s/iter; left time: 3622.8143s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0655170\n",
      "\tspeed: 0.1114s/iter; left time: 3748.8925s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0690930\n",
      "\tspeed: 0.1100s/iter; left time: 3691.2117s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0599412\n",
      "\tspeed: 0.0992s/iter; left time: 3319.3962s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0730805\n",
      "\tspeed: 0.1004s/iter; left time: 3350.1140s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0648698\n",
      "\tspeed: 0.1098s/iter; left time: 3650.7728s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0638596\n",
      "\tspeed: 0.1068s/iter; left time: 3541.0975s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0609560\n",
      "\tspeed: 0.1096s/iter; left time: 3623.0832s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0601996\n",
      "\tspeed: 0.1109s/iter; left time: 3654.6756s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0709287\n",
      "\tspeed: 0.1060s/iter; left time: 3484.5731s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0781172\n",
      "\tspeed: 0.1091s/iter; left time: 3575.7529s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0663429\n",
      "\tspeed: 0.1096s/iter; left time: 3580.6048s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0782741\n",
      "\tspeed: 0.1062s/iter; left time: 3458.0091s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0815893\n",
      "\tspeed: 0.1087s/iter; left time: 3528.7187s\n",
      "Epoch: 8 cost time: 00h:04m:47.72s\n",
      "Epoch: 8 | Train Loss: 0.0677572 Vali Loss: 0.0597144 Test Loss: 0.0685009\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0733084\n",
      "\tspeed: 0.9591s/iter; left time: 30946.8273s\n",
      "\titers: 200, epoch: 9 | loss: 0.0649616\n",
      "\tspeed: 0.1070s/iter; left time: 3441.7858s\n",
      "\titers: 300, epoch: 9 | loss: 0.0522117\n",
      "\tspeed: 0.1106s/iter; left time: 3547.7316s\n",
      "\titers: 400, epoch: 9 | loss: 0.0659156\n",
      "\tspeed: 0.1058s/iter; left time: 3381.2549s\n",
      "\titers: 500, epoch: 9 | loss: 0.0653602\n",
      "\tspeed: 0.1022s/iter; left time: 3255.7305s\n",
      "\titers: 600, epoch: 9 | loss: 0.0684405\n",
      "\tspeed: 0.1054s/iter; left time: 3348.4559s\n",
      "\titers: 700, epoch: 9 | loss: 0.0707613\n",
      "\tspeed: 0.1071s/iter; left time: 3391.6700s\n",
      "\titers: 800, epoch: 9 | loss: 0.0594048\n",
      "\tspeed: 0.1084s/iter; left time: 3421.7803s\n",
      "\titers: 900, epoch: 9 | loss: 0.0666278\n",
      "\tspeed: 0.1108s/iter; left time: 3485.6078s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0586480\n",
      "\tspeed: 0.1058s/iter; left time: 3317.3673s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0657964\n",
      "\tspeed: 0.1075s/iter; left time: 3360.6361s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0660123\n",
      "\tspeed: 0.1072s/iter; left time: 3340.2201s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0626299\n",
      "\tspeed: 0.1068s/iter; left time: 3316.2178s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0734063\n",
      "\tspeed: 0.1075s/iter; left time: 3327.8793s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0596756\n",
      "\tspeed: 0.1052s/iter; left time: 3246.9703s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0683507\n",
      "\tspeed: 0.1067s/iter; left time: 3282.1504s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0777180\n",
      "\tspeed: 0.1058s/iter; left time: 3245.2439s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0691540\n",
      "\tspeed: 0.1076s/iter; left time: 3288.3782s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0617039\n",
      "\tspeed: 0.1008s/iter; left time: 3072.3402s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0588695\n",
      "\tspeed: 0.1043s/iter; left time: 3167.3205s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0621957\n",
      "\tspeed: 0.1058s/iter; left time: 3202.2180s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0547058\n",
      "\tspeed: 0.1109s/iter; left time: 3345.2274s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0607693\n",
      "\tspeed: 0.1073s/iter; left time: 3226.4864s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0594633\n",
      "\tspeed: 0.1104s/iter; left time: 3308.0158s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0635684\n",
      "\tspeed: 0.1045s/iter; left time: 3121.6698s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0643801\n",
      "\tspeed: 0.1124s/iter; left time: 3344.5355s\n",
      "Epoch: 9 cost time: 00h:04m:49.10s\n",
      "Epoch: 9 | Train Loss: 0.0671886 Vali Loss: 0.0583229 Test Loss: 0.0672066\n",
      "Validation loss decreased (0.059168 --> 0.058323).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0552282\n",
      "\tspeed: 0.9887s/iter; left time: 29233.8330s\n",
      "\titers: 200, epoch: 10 | loss: 0.0693061\n",
      "\tspeed: 0.1088s/iter; left time: 3205.9469s\n",
      "\titers: 300, epoch: 10 | loss: 0.0732012\n",
      "\tspeed: 0.1009s/iter; left time: 2963.2220s\n",
      "\titers: 400, epoch: 10 | loss: 0.0685882\n",
      "\tspeed: 0.1064s/iter; left time: 3115.1014s\n",
      "\titers: 500, epoch: 10 | loss: 0.0663253\n",
      "\tspeed: 0.1075s/iter; left time: 3136.1761s\n",
      "\titers: 600, epoch: 10 | loss: 0.0720553\n",
      "\tspeed: 0.1085s/iter; left time: 3153.3737s\n",
      "\titers: 700, epoch: 10 | loss: 0.0778783\n",
      "\tspeed: 0.1088s/iter; left time: 3151.5514s\n",
      "\titers: 800, epoch: 10 | loss: 0.0587212\n",
      "\tspeed: 0.1074s/iter; left time: 3100.8688s\n",
      "\titers: 900, epoch: 10 | loss: 0.0744800\n",
      "\tspeed: 0.1069s/iter; left time: 3074.3845s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0681581\n",
      "\tspeed: 0.1100s/iter; left time: 3153.2325s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0630505\n",
      "\tspeed: 0.1045s/iter; left time: 2985.0956s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0582443\n",
      "\tspeed: 0.1119s/iter; left time: 3184.8198s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0649366\n",
      "\tspeed: 0.1089s/iter; left time: 3089.5797s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0721512\n",
      "\tspeed: 0.1083s/iter; left time: 3060.3826s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0701118\n",
      "\tspeed: 0.1113s/iter; left time: 3135.2027s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0635339\n",
      "\tspeed: 0.1082s/iter; left time: 3035.7514s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0698001\n",
      "\tspeed: 0.1062s/iter; left time: 2969.6689s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0668549\n",
      "\tspeed: 0.1051s/iter; left time: 2928.5506s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0737289\n",
      "\tspeed: 0.1063s/iter; left time: 2952.5134s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0622393\n",
      "\tspeed: 0.1099s/iter; left time: 3039.8494s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0650669\n",
      "\tspeed: 0.1091s/iter; left time: 3006.4543s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0562495\n",
      "\tspeed: 0.1095s/iter; left time: 3007.6895s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0678977\n",
      "\tspeed: 0.1090s/iter; left time: 2982.1215s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0679341\n",
      "\tspeed: 0.1071s/iter; left time: 2920.9894s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0645824\n",
      "\tspeed: 0.1080s/iter; left time: 2935.3384s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0802735\n",
      "\tspeed: 0.1091s/iter; left time: 2953.8478s\n",
      "Epoch: 10 cost time: 00h:04m:51.61s\n",
      "Epoch: 10 | Train Loss: 0.0665142 Vali Loss: 0.0596833 Test Loss: 0.0681801\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0599668\n",
      "\tspeed: 0.9573s/iter; left time: 25722.9568s\n",
      "\titers: 200, epoch: 11 | loss: 0.0571347\n",
      "\tspeed: 0.1065s/iter; left time: 2850.5491s\n",
      "\titers: 300, epoch: 11 | loss: 0.0555977\n",
      "\tspeed: 0.1146s/iter; left time: 3056.2351s\n",
      "\titers: 400, epoch: 11 | loss: 0.0804140\n",
      "\tspeed: 0.1061s/iter; left time: 2820.0641s\n",
      "\titers: 500, epoch: 11 | loss: 0.0598191\n",
      "\tspeed: 0.1028s/iter; left time: 2720.1557s\n",
      "\titers: 600, epoch: 11 | loss: 0.0680211\n",
      "\tspeed: 0.1070s/iter; left time: 2821.8647s\n",
      "\titers: 700, epoch: 11 | loss: 0.0765506\n",
      "\tspeed: 0.1089s/iter; left time: 2859.6527s\n",
      "\titers: 800, epoch: 11 | loss: 0.0773061\n",
      "\tspeed: 0.1080s/iter; left time: 2826.4155s\n",
      "\titers: 900, epoch: 11 | loss: 0.0626874\n",
      "\tspeed: 0.1072s/iter; left time: 2796.0956s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0522671\n",
      "\tspeed: 0.1063s/iter; left time: 2760.4022s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0617847\n",
      "\tspeed: 0.1093s/iter; left time: 2827.6076s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0724408\n",
      "\tspeed: 0.1059s/iter; left time: 2730.3436s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0688577\n",
      "\tspeed: 0.1084s/iter; left time: 2783.6595s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0556950\n",
      "\tspeed: 0.1044s/iter; left time: 2669.4810s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0723047\n",
      "\tspeed: 0.1098s/iter; left time: 2797.0139s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0631053\n",
      "\tspeed: 0.1109s/iter; left time: 2812.8506s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0670555\n",
      "\tspeed: 0.1067s/iter; left time: 2696.8812s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0663534\n",
      "\tspeed: 0.1048s/iter; left time: 2637.7162s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0744134\n",
      "\tspeed: 0.1097s/iter; left time: 2751.1074s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0686474\n",
      "\tspeed: 0.1085s/iter; left time: 2708.1896s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0585510\n",
      "\tspeed: 0.1008s/iter; left time: 2508.0445s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0590663\n",
      "\tspeed: 0.1062s/iter; left time: 2629.5855s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0612476\n",
      "\tspeed: 0.1061s/iter; left time: 2618.7157s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0564206\n",
      "\tspeed: 0.1066s/iter; left time: 2620.3427s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0649748\n",
      "\tspeed: 0.1106s/iter; left time: 2705.8915s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0657220\n",
      "\tspeed: 0.1079s/iter; left time: 2630.7462s\n",
      "Epoch: 11 cost time: 00h:04m:50.00s\n",
      "Epoch: 11 | Train Loss: 0.0660851 Vali Loss: 0.0592204 Test Loss: 0.0686712\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0633860\n",
      "\tspeed: 0.9599s/iter; left time: 23204.3836s\n",
      "\titers: 200, epoch: 12 | loss: 0.0610290\n",
      "\tspeed: 0.1125s/iter; left time: 2707.5343s\n",
      "\titers: 300, epoch: 12 | loss: 0.0708786\n",
      "\tspeed: 0.1074s/iter; left time: 2574.5213s\n",
      "\titers: 400, epoch: 12 | loss: 0.0609895\n",
      "\tspeed: 0.1086s/iter; left time: 2593.8355s\n",
      "\titers: 500, epoch: 12 | loss: 0.0662539\n",
      "\tspeed: 0.1070s/iter; left time: 2544.1196s\n",
      "\titers: 600, epoch: 12 | loss: 0.0623685\n",
      "\tspeed: 0.1097s/iter; left time: 2596.2447s\n",
      "\titers: 700, epoch: 12 | loss: 0.0687687\n",
      "\tspeed: 0.1121s/iter; left time: 2643.3199s\n",
      "\titers: 800, epoch: 12 | loss: 0.0648808\n",
      "\tspeed: 0.1087s/iter; left time: 2551.1677s\n",
      "\titers: 900, epoch: 12 | loss: 0.0647268\n",
      "\tspeed: 0.1016s/iter; left time: 2373.7196s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0696728\n",
      "\tspeed: 0.1089s/iter; left time: 2535.6462s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0658773\n",
      "\tspeed: 0.1067s/iter; left time: 2471.9854s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0531521\n",
      "\tspeed: 0.1070s/iter; left time: 2468.5060s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0591090\n",
      "\tspeed: 0.1069s/iter; left time: 2455.4522s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0587741\n",
      "\tspeed: 0.1061s/iter; left time: 2426.6691s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0690005\n",
      "\tspeed: 0.1074s/iter; left time: 2446.9495s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0695689\n",
      "\tspeed: 0.1045s/iter; left time: 2369.4013s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0818760\n",
      "\tspeed: 0.1020s/iter; left time: 2302.3399s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0798704\n",
      "\tspeed: 0.1056s/iter; left time: 2374.0073s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0663811\n",
      "\tspeed: 0.1118s/iter; left time: 2502.4097s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0659007\n",
      "\tspeed: 0.1074s/iter; left time: 2392.9768s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0658267\n",
      "\tspeed: 0.1104s/iter; left time: 2447.0488s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0753273\n",
      "\tspeed: 0.1075s/iter; left time: 2373.4862s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0546868\n",
      "\tspeed: 0.1066s/iter; left time: 2343.3964s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0648501\n",
      "\tspeed: 0.1075s/iter; left time: 2351.3421s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0702483\n",
      "\tspeed: 0.1054s/iter; left time: 2294.0471s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0676616\n",
      "\tspeed: 0.1092s/iter; left time: 2367.6167s\n",
      "Epoch: 12 cost time: 00h:04m:50.84s\n",
      "Epoch: 12 | Train Loss: 0.0656386 Vali Loss: 0.0598788 Test Loss: 0.0692823\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0604794\n",
      "\tspeed: 0.9623s/iter; left time: 20667.4025s\n",
      "\titers: 200, epoch: 13 | loss: 0.0566729\n",
      "\tspeed: 0.1115s/iter; left time: 2383.8163s\n",
      "\titers: 300, epoch: 13 | loss: 0.0683386\n",
      "\tspeed: 0.1044s/iter; left time: 2221.7334s\n",
      "\titers: 400, epoch: 13 | loss: 0.0582213\n",
      "\tspeed: 0.1044s/iter; left time: 2211.1926s\n",
      "\titers: 500, epoch: 13 | loss: 0.0658862\n",
      "\tspeed: 0.1060s/iter; left time: 2234.2187s\n",
      "\titers: 600, epoch: 13 | loss: 0.0680031\n",
      "\tspeed: 0.1073s/iter; left time: 2250.3047s\n",
      "\titers: 700, epoch: 13 | loss: 0.0549427\n",
      "\tspeed: 0.1084s/iter; left time: 2263.1544s\n",
      "\titers: 800, epoch: 13 | loss: 0.0744731\n",
      "\tspeed: 0.1090s/iter; left time: 2265.1629s\n",
      "\titers: 900, epoch: 13 | loss: 0.0574103\n",
      "\tspeed: 0.1038s/iter; left time: 2146.3706s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0571434\n",
      "\tspeed: 0.1037s/iter; left time: 2133.7964s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0576256\n",
      "\tspeed: 0.1054s/iter; left time: 2157.5566s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0628352\n",
      "\tspeed: 0.1043s/iter; left time: 2124.8254s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0693550\n",
      "\tspeed: 0.1097s/iter; left time: 2224.0056s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0574602\n",
      "\tspeed: 0.1055s/iter; left time: 2128.5007s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0820113\n",
      "\tspeed: 0.1074s/iter; left time: 2156.9764s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0689111\n",
      "\tspeed: 0.1077s/iter; left time: 2152.1251s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0636011\n",
      "\tspeed: 0.1012s/iter; left time: 2012.1593s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0583539\n",
      "\tspeed: 0.1059s/iter; left time: 2094.1268s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0606554\n",
      "\tspeed: 0.1070s/iter; left time: 2105.1503s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0585214\n",
      "\tspeed: 0.1114s/iter; left time: 2181.7210s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0706814\n",
      "\tspeed: 0.1047s/iter; left time: 2038.4392s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0654261\n",
      "\tspeed: 0.1048s/iter; left time: 2031.2516s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0581603\n",
      "\tspeed: 0.1070s/iter; left time: 2062.0388s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0690250\n",
      "\tspeed: 0.1088s/iter; left time: 2086.9024s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0769652\n",
      "\tspeed: 0.1016s/iter; left time: 1938.6240s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0761575\n",
      "\tspeed: 0.1085s/iter; left time: 2059.0608s\n",
      "Epoch: 13 cost time: 00h:04m:47.77s\n",
      "Epoch: 13 | Train Loss: 0.0652557 Vali Loss: 0.0595088 Test Loss: 0.0683032\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0597761\n",
      "\tspeed: 0.9633s/iter; left time: 18091.5826s\n",
      "\titers: 200, epoch: 14 | loss: 0.0597405\n",
      "\tspeed: 0.1083s/iter; left time: 2023.3475s\n",
      "\titers: 300, epoch: 14 | loss: 0.0717362\n",
      "\tspeed: 0.1022s/iter; left time: 1899.2130s\n",
      "\titers: 400, epoch: 14 | loss: 0.0655360\n",
      "\tspeed: 0.1077s/iter; left time: 1991.1425s\n",
      "\titers: 500, epoch: 14 | loss: 0.0798717\n",
      "\tspeed: 0.1072s/iter; left time: 1969.8460s\n",
      "\titers: 600, epoch: 14 | loss: 0.0666957\n",
      "\tspeed: 0.1046s/iter; left time: 1911.4793s\n",
      "\titers: 700, epoch: 14 | loss: 0.0554276\n",
      "\tspeed: 0.1062s/iter; left time: 1930.6350s\n",
      "\titers: 800, epoch: 14 | loss: 0.0766484\n",
      "\tspeed: 0.1046s/iter; left time: 1890.7512s\n",
      "\titers: 900, epoch: 14 | loss: 0.0795836\n",
      "\tspeed: 0.1035s/iter; left time: 1861.6078s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0669616\n",
      "\tspeed: 0.1055s/iter; left time: 1885.5383s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0561617\n",
      "\tspeed: 0.1033s/iter; left time: 1837.4406s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0759818\n",
      "\tspeed: 0.1045s/iter; left time: 1846.9357s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0588643\n",
      "\tspeed: 0.1085s/iter; left time: 1907.6299s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0568695\n",
      "\tspeed: 0.1074s/iter; left time: 1876.9303s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0583049\n",
      "\tspeed: 0.1055s/iter; left time: 1834.1887s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0541657\n",
      "\tspeed: 0.1112s/iter; left time: 1922.0183s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0580471\n",
      "\tspeed: 0.1036s/iter; left time: 1780.6303s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0683933\n",
      "\tspeed: 0.1101s/iter; left time: 1879.6820s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0691114\n",
      "\tspeed: 0.1079s/iter; left time: 1831.3454s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0739959\n",
      "\tspeed: 0.1080s/iter; left time: 1823.0459s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0626454\n",
      "\tspeed: 0.1044s/iter; left time: 1751.9195s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0539643\n",
      "\tspeed: 0.1066s/iter; left time: 1777.7894s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0740233\n",
      "\tspeed: 0.1068s/iter; left time: 1770.6359s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0800757\n",
      "\tspeed: 0.1066s/iter; left time: 1756.5512s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0604383\n",
      "\tspeed: 0.1068s/iter; left time: 1748.9560s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0544207\n",
      "\tspeed: 0.1077s/iter; left time: 1753.0148s\n",
      "Epoch: 14 cost time: 00h:04m:47.33s\n",
      "Epoch: 14 | Train Loss: 0.0648617 Vali Loss: 0.0584747 Test Loss: 0.0677103\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.0108054019510746, rmse:0.10394903272390366, mae:0.06720664352178574, rse:0.3051966428756714\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 24: 01h:26m:56.65s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 15:22:25,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 15:22:25,963] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 15:22:25,963] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 15:22:25,964] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 15:22:26,034] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 15:22:26,034] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 15:22:26,699] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 15:22:26,700] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 15:22:26,700] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 15:22:26,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 15:22:26,702] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 15:22:26,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 15:22:26,702] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 15:22:26,703] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 15:22:27,032] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 15:22:27,033] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,033] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.19 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,156] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 15:22:27,157] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,157] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.19 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,158] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 15:22:27,282] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 15:22:27,283] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 15:22:27,283] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 116.21 GB, percent = 15.4%\n",
      "[2024-11-02 15:22:27,283] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 15:22:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 15:22:27,284] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8661b2a450>\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 15:22:27,285] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 15:22:27,286] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 15:22:27,287] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1883777\n",
      "\tspeed: 0.1633s/iter; left time: 8775.2950s\n",
      "\titers: 200, epoch: 1 | loss: 0.1642785\n",
      "\tspeed: 0.1161s/iter; left time: 6223.0266s\n",
      "\titers: 300, epoch: 1 | loss: 0.1586337\n",
      "\tspeed: 0.1227s/iter; left time: 6565.8886s\n",
      "\titers: 400, epoch: 1 | loss: 0.1215111\n",
      "\tspeed: 0.1174s/iter; left time: 6270.3139s\n",
      "\titers: 500, epoch: 1 | loss: 0.1161290\n",
      "\tspeed: 0.1187s/iter; left time: 6329.3336s\n",
      "\titers: 600, epoch: 1 | loss: 0.1045108\n",
      "\tspeed: 0.1210s/iter; left time: 6440.0003s\n",
      "\titers: 700, epoch: 1 | loss: 0.1008641\n",
      "\tspeed: 0.1206s/iter; left time: 6404.1045s\n",
      "\titers: 800, epoch: 1 | loss: 0.1015716\n",
      "\tspeed: 0.1204s/iter; left time: 6382.9067s\n",
      "\titers: 900, epoch: 1 | loss: 0.1061578\n",
      "\tspeed: 0.1167s/iter; left time: 6176.6558s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1075203\n",
      "\tspeed: 0.1218s/iter; left time: 6434.9860s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0915048\n",
      "\tspeed: 0.1173s/iter; left time: 6185.6594s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1113726\n",
      "\tspeed: 0.1205s/iter; left time: 6342.6255s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1193190\n",
      "\tspeed: 0.1181s/iter; left time: 6200.5595s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1057486\n",
      "\tspeed: 0.1203s/iter; left time: 6303.7762s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1058766\n",
      "\tspeed: 0.1208s/iter; left time: 6319.5797s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0978397\n",
      "\tspeed: 0.1217s/iter; left time: 6355.8485s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0993557\n",
      "\tspeed: 0.1214s/iter; left time: 6328.9969s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1031770\n",
      "\tspeed: 0.1165s/iter; left time: 6058.6103s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0965251\n",
      "\tspeed: 0.1218s/iter; left time: 6325.7499s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0928079\n",
      "\tspeed: 0.1202s/iter; left time: 6226.6550s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0932634\n",
      "\tspeed: 0.1178s/iter; left time: 6093.9149s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1016541\n",
      "\tspeed: 0.1156s/iter; left time: 5964.9607s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0999433\n",
      "\tspeed: 0.1182s/iter; left time: 6089.1488s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0873916\n",
      "\tspeed: 0.1216s/iter; left time: 6252.8129s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0910878\n",
      "\tspeed: 0.1203s/iter; left time: 6176.1272s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1087127\n",
      "\tspeed: 0.1178s/iter; left time: 6033.2857s\n",
      "Epoch: 1 cost time: 00h:05m:23.31s\n",
      "Epoch: 1 | Train Loss: 0.1153284 Vali Loss: 0.0887686 Test Loss: 0.1010105\n",
      "Validation loss decreased (inf --> 0.088769).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0960205\n",
      "\tspeed: 1.1112s/iter; left time: 56705.4595s\n",
      "\titers: 200, epoch: 2 | loss: 0.0968005\n",
      "\tspeed: 0.1084s/iter; left time: 5522.1968s\n",
      "\titers: 300, epoch: 2 | loss: 0.1073778\n",
      "\tspeed: 0.1053s/iter; left time: 5351.6586s\n",
      "\titers: 400, epoch: 2 | loss: 0.0946367\n",
      "\tspeed: 0.1066s/iter; left time: 5407.8782s\n",
      "\titers: 500, epoch: 2 | loss: 0.1025144\n",
      "\tspeed: 0.1079s/iter; left time: 5462.6925s\n",
      "\titers: 600, epoch: 2 | loss: 0.0983518\n",
      "\tspeed: 0.1048s/iter; left time: 5297.9099s\n",
      "\titers: 700, epoch: 2 | loss: 0.0933921\n",
      "\tspeed: 0.1073s/iter; left time: 5408.6273s\n",
      "\titers: 800, epoch: 2 | loss: 0.0885800\n",
      "\tspeed: 0.1069s/iter; left time: 5378.0900s\n",
      "\titers: 900, epoch: 2 | loss: 0.0950760\n",
      "\tspeed: 0.1070s/iter; left time: 5375.5761s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0934220\n",
      "\tspeed: 0.1079s/iter; left time: 5410.7821s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1022061\n",
      "\tspeed: 0.1066s/iter; left time: 5333.7692s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0829705\n",
      "\tspeed: 0.1052s/iter; left time: 5252.5686s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1148255\n",
      "\tspeed: 0.1030s/iter; left time: 5133.3961s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0892722\n",
      "\tspeed: 0.1088s/iter; left time: 5409.7303s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0866018\n",
      "\tspeed: 0.1071s/iter; left time: 5314.3252s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0827379\n",
      "\tspeed: 0.1049s/iter; left time: 5194.5613s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1050955\n",
      "\tspeed: 0.1055s/iter; left time: 5217.1601s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1068194\n",
      "\tspeed: 0.1052s/iter; left time: 5190.2900s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0907224\n",
      "\tspeed: 0.1076s/iter; left time: 5299.5344s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0889307\n",
      "\tspeed: 0.1075s/iter; left time: 5279.8154s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0828655\n",
      "\tspeed: 0.1077s/iter; left time: 5278.4036s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1026875\n",
      "\tspeed: 0.1099s/iter; left time: 5375.0429s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0875362\n",
      "\tspeed: 0.1092s/iter; left time: 5333.6976s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0809421\n",
      "\tspeed: 0.1074s/iter; left time: 5234.9613s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0847859\n",
      "\tspeed: 0.1053s/iter; left time: 5118.4303s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0868387\n",
      "\tspeed: 0.1099s/iter; left time: 5333.3714s\n",
      "Epoch: 2 cost time: 00h:04m:48.57s\n",
      "Epoch: 2 | Train Loss: 0.0953358 Vali Loss: 0.0838980 Test Loss: 0.0960651\n",
      "Validation loss decreased (0.088769 --> 0.083898).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0928329\n",
      "\tspeed: 0.9862s/iter; left time: 47670.4618s\n",
      "\titers: 200, epoch: 3 | loss: 0.0811050\n",
      "\tspeed: 0.1161s/iter; left time: 5600.4357s\n",
      "\titers: 300, epoch: 3 | loss: 0.1074759\n",
      "\tspeed: 0.1070s/iter; left time: 5149.8389s\n",
      "\titers: 400, epoch: 3 | loss: 0.0964614\n",
      "\tspeed: 0.1077s/iter; left time: 5172.2576s\n",
      "\titers: 500, epoch: 3 | loss: 0.0867380\n",
      "\tspeed: 0.1053s/iter; left time: 5046.5864s\n",
      "\titers: 600, epoch: 3 | loss: 0.0928695\n",
      "\tspeed: 0.1092s/iter; left time: 5224.6830s\n",
      "\titers: 700, epoch: 3 | loss: 0.0858418\n",
      "\tspeed: 0.1073s/iter; left time: 5121.3753s\n",
      "\titers: 800, epoch: 3 | loss: 0.0918019\n",
      "\tspeed: 0.1065s/iter; left time: 5072.4651s\n",
      "\titers: 900, epoch: 3 | loss: 0.1130629\n",
      "\tspeed: 0.1070s/iter; left time: 5087.8674s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0996885\n",
      "\tspeed: 0.1049s/iter; left time: 4978.5365s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1002581\n",
      "\tspeed: 0.1097s/iter; left time: 5194.1672s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0801419\n",
      "\tspeed: 0.1075s/iter; left time: 5079.1085s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0852083\n",
      "\tspeed: 0.1083s/iter; left time: 5107.1702s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0802493\n",
      "\tspeed: 0.1044s/iter; left time: 4911.2287s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0922221\n",
      "\tspeed: 0.1100s/iter; left time: 5161.9898s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0829106\n",
      "\tspeed: 0.1074s/iter; left time: 5032.8094s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0909540\n",
      "\tspeed: 0.1058s/iter; left time: 4943.1901s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0992832\n",
      "\tspeed: 0.1073s/iter; left time: 5002.3535s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1036604\n",
      "\tspeed: 0.1048s/iter; left time: 4876.7098s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0947560\n",
      "\tspeed: 0.1055s/iter; left time: 4899.8111s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0935063\n",
      "\tspeed: 0.1090s/iter; left time: 5051.4298s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0912203\n",
      "\tspeed: 0.1015s/iter; left time: 4693.5850s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0946222\n",
      "\tspeed: 0.1068s/iter; left time: 4927.4407s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0792365\n",
      "\tspeed: 0.1075s/iter; left time: 4949.7346s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0930506\n",
      "\tspeed: 0.1021s/iter; left time: 4690.1926s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0854950\n",
      "\tspeed: 0.1063s/iter; left time: 4871.2306s\n",
      "Epoch: 3 cost time: 00h:04m:49.13s\n",
      "Epoch: 3 | Train Loss: 0.0915215 Vali Loss: 0.0821664 Test Loss: 0.0941079\n",
      "Validation loss decreased (0.083898 --> 0.082166).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0923687\n",
      "\tspeed: 0.9672s/iter; left time: 44149.4505s\n",
      "\titers: 200, epoch: 4 | loss: 0.0943383\n",
      "\tspeed: 0.1023s/iter; left time: 4660.4344s\n",
      "\titers: 300, epoch: 4 | loss: 0.0941553\n",
      "\tspeed: 0.1018s/iter; left time: 4625.8418s\n",
      "\titers: 400, epoch: 4 | loss: 0.0961315\n",
      "\tspeed: 0.1080s/iter; left time: 4896.0319s\n",
      "\titers: 500, epoch: 4 | loss: 0.1017793\n",
      "\tspeed: 0.1084s/iter; left time: 4904.2047s\n",
      "\titers: 600, epoch: 4 | loss: 0.0789471\n",
      "\tspeed: 0.1069s/iter; left time: 4826.5473s\n",
      "\titers: 700, epoch: 4 | loss: 0.0971971\n",
      "\tspeed: 0.1102s/iter; left time: 4962.9907s\n",
      "\titers: 800, epoch: 4 | loss: 0.0904444\n",
      "\tspeed: 0.1110s/iter; left time: 4987.5309s\n",
      "\titers: 900, epoch: 4 | loss: 0.0791268\n",
      "\tspeed: 0.1029s/iter; left time: 4615.9686s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0881244\n",
      "\tspeed: 0.1107s/iter; left time: 4953.3610s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0926313\n",
      "\tspeed: 0.0996s/iter; left time: 4444.7742s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0894173\n",
      "\tspeed: 0.1076s/iter; left time: 4791.3738s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0785045\n",
      "\tspeed: 0.1076s/iter; left time: 4781.0097s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0893424\n",
      "\tspeed: 0.1057s/iter; left time: 4688.4088s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0989641\n",
      "\tspeed: 0.1091s/iter; left time: 4828.1736s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0902890\n",
      "\tspeed: 0.1067s/iter; left time: 4709.9595s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0750174\n",
      "\tspeed: 0.1086s/iter; left time: 4785.6809s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0954725\n",
      "\tspeed: 0.1043s/iter; left time: 4582.1583s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0801812\n",
      "\tspeed: 0.1072s/iter; left time: 4700.9469s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0892521\n",
      "\tspeed: 0.1057s/iter; left time: 4624.6908s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0967448\n",
      "\tspeed: 0.1078s/iter; left time: 4704.9327s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0892473\n",
      "\tspeed: 0.1041s/iter; left time: 4531.9119s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1105002\n",
      "\tspeed: 0.1084s/iter; left time: 4709.2993s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0882045\n",
      "\tspeed: 0.1082s/iter; left time: 4689.7527s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0999998\n",
      "\tspeed: 0.1028s/iter; left time: 4443.7533s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0820032\n",
      "\tspeed: 0.1085s/iter; left time: 4680.1466s\n",
      "Epoch: 4 cost time: 00h:04m:47.46s\n",
      "Epoch: 4 | Train Loss: 0.0890123 Vali Loss: 0.0821997 Test Loss: 0.0937731\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0824345\n",
      "\tspeed: 0.9453s/iter; left time: 40605.9154s\n",
      "\titers: 200, epoch: 5 | loss: 0.0922259\n",
      "\tspeed: 0.1130s/iter; left time: 4844.8604s\n",
      "\titers: 300, epoch: 5 | loss: 0.0980015\n",
      "\tspeed: 0.1070s/iter; left time: 4576.3462s\n",
      "\titers: 400, epoch: 5 | loss: 0.0738367\n",
      "\tspeed: 0.1104s/iter; left time: 4711.3551s\n",
      "\titers: 500, epoch: 5 | loss: 0.0708809\n",
      "\tspeed: 0.1120s/iter; left time: 4767.7358s\n",
      "\titers: 600, epoch: 5 | loss: 0.0800962\n",
      "\tspeed: 0.1084s/iter; left time: 4602.6269s\n",
      "\titers: 700, epoch: 5 | loss: 0.0935463\n",
      "\tspeed: 0.1072s/iter; left time: 4542.5523s\n",
      "\titers: 800, epoch: 5 | loss: 0.0977683\n",
      "\tspeed: 0.1035s/iter; left time: 4374.0397s\n",
      "\titers: 900, epoch: 5 | loss: 0.0860935\n",
      "\tspeed: 0.1073s/iter; left time: 4523.5646s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0758314\n",
      "\tspeed: 0.1067s/iter; left time: 4489.0283s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0814082\n",
      "\tspeed: 0.1018s/iter; left time: 4270.5619s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0836338\n",
      "\tspeed: 0.1070s/iter; left time: 4479.7320s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0829336\n",
      "\tspeed: 0.1062s/iter; left time: 4435.0494s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0851112\n",
      "\tspeed: 0.1030s/iter; left time: 4289.4145s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0850937\n",
      "\tspeed: 0.1084s/iter; left time: 4506.2151s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0898389\n",
      "\tspeed: 0.1051s/iter; left time: 4357.1480s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0858165\n",
      "\tspeed: 0.1039s/iter; left time: 4298.8632s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0958296\n",
      "\tspeed: 0.1065s/iter; left time: 4392.8802s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0857323\n",
      "\tspeed: 0.1131s/iter; left time: 4655.0584s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0918692\n",
      "\tspeed: 0.1030s/iter; left time: 4228.9117s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0774475\n",
      "\tspeed: 0.1010s/iter; left time: 4136.6069s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0845494\n",
      "\tspeed: 0.1075s/iter; left time: 4392.7368s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0760283\n",
      "\tspeed: 0.1021s/iter; left time: 4161.3874s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0756378\n",
      "\tspeed: 0.1025s/iter; left time: 4167.8196s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0883363\n",
      "\tspeed: 0.1034s/iter; left time: 4195.5177s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0859664\n",
      "\tspeed: 0.1067s/iter; left time: 4316.4279s\n",
      "Epoch: 5 cost time: 00h:04m:46.37s\n",
      "Epoch: 5 | Train Loss: 0.0871692 Vali Loss: 0.0830350 Test Loss: 0.0942258\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0915383\n",
      "\tspeed: 0.9423s/iter; left time: 37943.1123s\n",
      "\titers: 200, epoch: 6 | loss: 0.0933986\n",
      "\tspeed: 0.1069s/iter; left time: 4292.3560s\n",
      "\titers: 300, epoch: 6 | loss: 0.0981537\n",
      "\tspeed: 0.1058s/iter; left time: 4237.9282s\n",
      "\titers: 400, epoch: 6 | loss: 0.0806353\n",
      "\tspeed: 0.1058s/iter; left time: 4226.9126s\n",
      "\titers: 500, epoch: 6 | loss: 0.0890182\n",
      "\tspeed: 0.1141s/iter; left time: 4546.9666s\n",
      "\titers: 600, epoch: 6 | loss: 0.0841566\n",
      "\tspeed: 0.1106s/iter; left time: 4397.1800s\n",
      "\titers: 700, epoch: 6 | loss: 0.0806254\n",
      "\tspeed: 0.1113s/iter; left time: 4416.3133s\n",
      "\titers: 800, epoch: 6 | loss: 0.0999852\n",
      "\tspeed: 0.1089s/iter; left time: 4308.5975s\n",
      "\titers: 900, epoch: 6 | loss: 0.0849152\n",
      "\tspeed: 0.1071s/iter; left time: 4227.5662s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0727589\n",
      "\tspeed: 0.1046s/iter; left time: 4117.9726s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0788582\n",
      "\tspeed: 0.1120s/iter; left time: 4399.3033s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0934843\n",
      "\tspeed: 0.1049s/iter; left time: 4108.5665s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0990512\n",
      "\tspeed: 0.1068s/iter; left time: 4174.1318s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0990836\n",
      "\tspeed: 0.1097s/iter; left time: 4276.2408s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0847571\n",
      "\tspeed: 0.1060s/iter; left time: 4120.4722s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0996485\n",
      "\tspeed: 0.1086s/iter; left time: 4211.1485s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0907493\n",
      "\tspeed: 0.1077s/iter; left time: 4162.6426s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0795749\n",
      "\tspeed: 0.1084s/iter; left time: 4182.1248s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0791156\n",
      "\tspeed: 0.1107s/iter; left time: 4257.6090s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0867164\n",
      "\tspeed: 0.1088s/iter; left time: 4174.8139s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0956665\n",
      "\tspeed: 0.1075s/iter; left time: 4115.4783s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0683716\n",
      "\tspeed: 0.1060s/iter; left time: 4043.8072s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0902782\n",
      "\tspeed: 0.1065s/iter; left time: 4052.8975s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0900548\n",
      "\tspeed: 0.1069s/iter; left time: 4057.7608s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0807459\n",
      "\tspeed: 0.1102s/iter; left time: 4172.5174s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0924551\n",
      "\tspeed: 0.1109s/iter; left time: 4187.1750s\n",
      "Epoch: 6 cost time: 00h:04m:51.55s\n",
      "Epoch: 6 | Train Loss: 0.0856582 Vali Loss: 0.0837385 Test Loss: 0.0947861\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0834660\n",
      "\tspeed: 0.9507s/iter; left time: 35723.7371s\n",
      "\titers: 200, epoch: 7 | loss: 0.0829694\n",
      "\tspeed: 0.1115s/iter; left time: 4179.7641s\n",
      "\titers: 300, epoch: 7 | loss: 0.0874605\n",
      "\tspeed: 0.1096s/iter; left time: 4096.6349s\n",
      "\titers: 400, epoch: 7 | loss: 0.0953893\n",
      "\tspeed: 0.1059s/iter; left time: 3945.9474s\n",
      "\titers: 500, epoch: 7 | loss: 0.0848485\n",
      "\tspeed: 0.1095s/iter; left time: 4069.8731s\n",
      "\titers: 600, epoch: 7 | loss: 0.0755942\n",
      "\tspeed: 0.1061s/iter; left time: 3935.3471s\n",
      "\titers: 700, epoch: 7 | loss: 0.0873404\n",
      "\tspeed: 0.1042s/iter; left time: 3852.8437s\n",
      "\titers: 800, epoch: 7 | loss: 0.0822200\n",
      "\tspeed: 0.1124s/iter; left time: 4143.6976s\n",
      "\titers: 900, epoch: 7 | loss: 0.0831545\n",
      "\tspeed: 0.1063s/iter; left time: 3908.0252s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0765932\n",
      "\tspeed: 0.1081s/iter; left time: 3962.8190s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0916426\n",
      "\tspeed: 0.1063s/iter; left time: 3889.7106s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0759756\n",
      "\tspeed: 0.1082s/iter; left time: 3945.4410s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0766117\n",
      "\tspeed: 0.1043s/iter; left time: 3794.8485s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0788070\n",
      "\tspeed: 0.1076s/iter; left time: 3904.0472s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1004317\n",
      "\tspeed: 0.1086s/iter; left time: 3930.3966s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0882952\n",
      "\tspeed: 0.1071s/iter; left time: 3864.3780s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0867677\n",
      "\tspeed: 0.1068s/iter; left time: 3842.4643s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0723154\n",
      "\tspeed: 0.1087s/iter; left time: 3900.5188s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0990128\n",
      "\tspeed: 0.1091s/iter; left time: 3902.1994s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0675883\n",
      "\tspeed: 0.1106s/iter; left time: 3945.9198s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0852043\n",
      "\tspeed: 0.1122s/iter; left time: 3990.1344s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0885752\n",
      "\tspeed: 0.1089s/iter; left time: 3864.1635s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0817467\n",
      "\tspeed: 0.1041s/iter; left time: 3681.1048s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0783509\n",
      "\tspeed: 0.1049s/iter; left time: 3701.9991s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0753475\n",
      "\tspeed: 0.1023s/iter; left time: 3597.0074s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0807764\n",
      "\tspeed: 0.1039s/iter; left time: 3642.7307s\n",
      "Epoch: 7 cost time: 00h:04m:49.85s\n",
      "Epoch: 7 | Train Loss: 0.0841171 Vali Loss: 0.0846731 Test Loss: 0.0937717\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0863863\n",
      "\tspeed: 0.9494s/iter; left time: 33117.9419s\n",
      "\titers: 200, epoch: 8 | loss: 0.0749087\n",
      "\tspeed: 0.1100s/iter; left time: 3825.2096s\n",
      "\titers: 300, epoch: 8 | loss: 0.0807150\n",
      "\tspeed: 0.1080s/iter; left time: 3745.5055s\n",
      "\titers: 400, epoch: 8 | loss: 0.0771998\n",
      "\tspeed: 0.1100s/iter; left time: 3803.8892s\n",
      "\titers: 500, epoch: 8 | loss: 0.0875563\n",
      "\tspeed: 0.1084s/iter; left time: 3738.6377s\n",
      "\titers: 600, epoch: 8 | loss: 0.0829368\n",
      "\tspeed: 0.1057s/iter; left time: 3634.9542s\n",
      "\titers: 700, epoch: 8 | loss: 0.0730132\n",
      "\tspeed: 0.1059s/iter; left time: 3631.4580s\n",
      "\titers: 800, epoch: 8 | loss: 0.0852739\n",
      "\tspeed: 0.1079s/iter; left time: 3689.0597s\n",
      "\titers: 900, epoch: 8 | loss: 0.0951376\n",
      "\tspeed: 0.1067s/iter; left time: 3637.2869s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0686005\n",
      "\tspeed: 0.1021s/iter; left time: 3468.3762s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0827262\n",
      "\tspeed: 0.1043s/iter; left time: 3535.0200s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0796252\n",
      "\tspeed: 0.1093s/iter; left time: 3691.5745s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0840250\n",
      "\tspeed: 0.1037s/iter; left time: 3491.9403s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0879034\n",
      "\tspeed: 0.1097s/iter; left time: 3684.5622s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0701827\n",
      "\tspeed: 0.1041s/iter; left time: 3485.9761s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0842006\n",
      "\tspeed: 0.1048s/iter; left time: 3497.2469s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0884516\n",
      "\tspeed: 0.1088s/iter; left time: 3619.6541s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1042492\n",
      "\tspeed: 0.1097s/iter; left time: 3640.4532s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0795427\n",
      "\tspeed: 0.1074s/iter; left time: 3552.2897s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0710123\n",
      "\tspeed: 0.1088s/iter; left time: 3589.3398s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0780427\n",
      "\tspeed: 0.1083s/iter; left time: 3562.3860s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0920559\n",
      "\tspeed: 0.1060s/iter; left time: 3473.6784s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0782730\n",
      "\tspeed: 0.1046s/iter; left time: 3418.4985s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0835265\n",
      "\tspeed: 0.1068s/iter; left time: 3478.3846s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0733754\n",
      "\tspeed: 0.1107s/iter; left time: 3596.9654s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0804105\n",
      "\tspeed: 0.1018s/iter; left time: 3295.7376s\n",
      "Epoch: 8 cost time: 00h:04m:48.48s\n",
      "Epoch: 8 | Train Loss: 0.0827061 Vali Loss: 0.0853918 Test Loss: 0.0946178\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.019848739728331566, rmse:0.14088556170463562, mae:0.09410795569419861, rse:0.41384807229042053\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 96: 00h:50m:14.97s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 16:12:39,498] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 16:12:40,572] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 16:12:40,573] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 16:12:40,573] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 16:12:40,675] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 16:12:40,676] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 16:12:41,349] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 16:12:41,350] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 16:12:41,350] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 16:12:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 16:12:41,352] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 16:12:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 16:12:41,352] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 16:12:41,689] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 16:12:41,690] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,690] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,817] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 16:12:41,818] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,818] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,818] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 16:12:41,937] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 16:12:41,938] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 16:12:41,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 117.48 GB, percent = 15.6%\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 16:12:41,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 16:12:41,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 16:12:41,940] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc2486677d0>\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 16:12:41,941] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 16:12:41,942] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 16:12:41,943] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1778164\n",
      "\tspeed: 0.1568s/iter; left time: 8400.0508s\n",
      "\titers: 200, epoch: 1 | loss: 0.1971989\n",
      "\tspeed: 0.1192s/iter; left time: 6377.3523s\n",
      "\titers: 300, epoch: 1 | loss: 0.1594778\n",
      "\tspeed: 0.1232s/iter; left time: 6575.6436s\n",
      "\titers: 400, epoch: 1 | loss: 0.1233426\n",
      "\tspeed: 0.1147s/iter; left time: 6111.6815s\n",
      "\titers: 500, epoch: 1 | loss: 0.1213352\n",
      "\tspeed: 0.1189s/iter; left time: 6324.4946s\n",
      "\titers: 600, epoch: 1 | loss: 0.1119261\n",
      "\tspeed: 0.1218s/iter; left time: 6463.4824s\n",
      "\titers: 700, epoch: 1 | loss: 0.1127991\n",
      "\tspeed: 0.1219s/iter; left time: 6458.3757s\n",
      "\titers: 800, epoch: 1 | loss: 0.1025673\n",
      "\tspeed: 0.1211s/iter; left time: 6401.9189s\n",
      "\titers: 900, epoch: 1 | loss: 0.1033708\n",
      "\tspeed: 0.1208s/iter; left time: 6373.7260s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1163776\n",
      "\tspeed: 0.1170s/iter; left time: 6163.5149s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1146140\n",
      "\tspeed: 0.1225s/iter; left time: 6440.3435s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1062645\n",
      "\tspeed: 0.1206s/iter; left time: 6330.5963s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1152460\n",
      "\tspeed: 0.1215s/iter; left time: 6363.5154s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1100958\n",
      "\tspeed: 0.1152s/iter; left time: 6025.1233s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0964408\n",
      "\tspeed: 0.1175s/iter; left time: 6133.0263s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0965621\n",
      "\tspeed: 0.1209s/iter; left time: 6297.9019s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0997088\n",
      "\tspeed: 0.1155s/iter; left time: 6005.6949s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1003732\n",
      "\tspeed: 0.1206s/iter; left time: 6257.5801s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1042660\n",
      "\tspeed: 0.1207s/iter; left time: 6248.1295s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0969521\n",
      "\tspeed: 0.1202s/iter; left time: 6210.6491s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1015071\n",
      "\tspeed: 0.1194s/iter; left time: 6159.6435s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1033745\n",
      "\tspeed: 0.1223s/iter; left time: 6297.3941s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1043368\n",
      "\tspeed: 0.1198s/iter; left time: 6157.7247s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0983482\n",
      "\tspeed: 0.1177s/iter; left time: 6037.7931s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0977514\n",
      "\tspeed: 0.1148s/iter; left time: 5873.1216s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1156735\n",
      "\tspeed: 0.1235s/iter; left time: 6309.1311s\n",
      "Epoch: 1 cost time: 00h:05m:21.95s\n",
      "Epoch: 1 | Train Loss: 0.1179913 Vali Loss: 0.0912490 Test Loss: 0.1035000\n",
      "Validation loss decreased (inf --> 0.091249).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1087977\n",
      "\tspeed: 1.0831s/iter; left time: 55126.1953s\n",
      "\titers: 200, epoch: 2 | loss: 0.1095505\n",
      "\tspeed: 0.1064s/iter; left time: 5405.0116s\n",
      "\titers: 300, epoch: 2 | loss: 0.0956247\n",
      "\tspeed: 0.1052s/iter; left time: 5331.4448s\n",
      "\titers: 400, epoch: 2 | loss: 0.1070556\n",
      "\tspeed: 0.1094s/iter; left time: 5535.6977s\n",
      "\titers: 500, epoch: 2 | loss: 0.0973769\n",
      "\tspeed: 0.1070s/iter; left time: 5405.1235s\n",
      "\titers: 600, epoch: 2 | loss: 0.0893573\n",
      "\tspeed: 0.1065s/iter; left time: 5367.3892s\n",
      "\titers: 700, epoch: 2 | loss: 0.0881983\n",
      "\tspeed: 0.1108s/iter; left time: 5572.1252s\n",
      "\titers: 800, epoch: 2 | loss: 0.0978097\n",
      "\tspeed: 0.1110s/iter; left time: 5569.7477s\n",
      "\titers: 900, epoch: 2 | loss: 0.1160224\n",
      "\tspeed: 0.1074s/iter; left time: 5379.9568s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1127868\n",
      "\tspeed: 0.1097s/iter; left time: 5485.2445s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1066502\n",
      "\tspeed: 0.1080s/iter; left time: 5387.8599s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0881434\n",
      "\tspeed: 0.1071s/iter; left time: 5334.1044s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0893611\n",
      "\tspeed: 0.1098s/iter; left time: 5454.9817s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1034243\n",
      "\tspeed: 0.1085s/iter; left time: 5379.1939s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0995729\n",
      "\tspeed: 0.1069s/iter; left time: 5289.2479s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1037162\n",
      "\tspeed: 0.1134s/iter; left time: 5601.0162s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1003603\n",
      "\tspeed: 0.1106s/iter; left time: 5453.2157s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1068703\n",
      "\tspeed: 0.1114s/iter; left time: 5480.0954s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0950982\n",
      "\tspeed: 0.1109s/iter; left time: 5444.0518s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0966142\n",
      "\tspeed: 0.1096s/iter; left time: 5367.6743s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0983411\n",
      "\tspeed: 0.1115s/iter; left time: 5452.8035s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0992421\n",
      "\tspeed: 0.1160s/iter; left time: 5660.8030s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0934880\n",
      "\tspeed: 0.1209s/iter; left time: 5886.0998s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1001284\n",
      "\tspeed: 0.1206s/iter; left time: 5862.0561s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0928106\n",
      "\tspeed: 0.1182s/iter; left time: 5733.7461s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0870252\n",
      "\tspeed: 0.1209s/iter; left time: 5852.0493s\n",
      "Epoch: 2 cost time: 00h:04m:58.85s\n",
      "Epoch: 2 | Train Loss: 0.0990444 Vali Loss: 0.0869754 Test Loss: 0.0983193\n",
      "Validation loss decreased (0.091249 --> 0.086975).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1038128\n",
      "\tspeed: 0.9964s/iter; left time: 48039.4661s\n",
      "\titers: 200, epoch: 3 | loss: 0.0941214\n",
      "\tspeed: 0.1201s/iter; left time: 5779.3333s\n",
      "\titers: 300, epoch: 3 | loss: 0.0934169\n",
      "\tspeed: 0.1213s/iter; left time: 5824.3659s\n",
      "\titers: 400, epoch: 3 | loss: 0.0778983\n",
      "\tspeed: 0.1209s/iter; left time: 5790.6581s\n",
      "\titers: 500, epoch: 3 | loss: 0.0914758\n",
      "\tspeed: 0.1210s/iter; left time: 5786.5365s\n",
      "\titers: 600, epoch: 3 | loss: 0.0881995\n",
      "\tspeed: 0.1208s/iter; left time: 5763.0747s\n",
      "\titers: 700, epoch: 3 | loss: 0.0977619\n",
      "\tspeed: 0.1166s/iter; left time: 5551.1786s\n",
      "\titers: 800, epoch: 3 | loss: 0.1015535\n",
      "\tspeed: 0.1195s/iter; left time: 5676.9406s\n",
      "\titers: 900, epoch: 3 | loss: 0.1082432\n",
      "\tspeed: 0.1198s/iter; left time: 5681.4679s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0877309\n",
      "\tspeed: 0.1202s/iter; left time: 5687.6272s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1051150\n",
      "\tspeed: 0.1181s/iter; left time: 5577.2203s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0997433\n",
      "\tspeed: 0.1216s/iter; left time: 5729.0933s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1073216\n",
      "\tspeed: 0.1207s/iter; left time: 5673.5919s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1008831\n",
      "\tspeed: 0.1204s/iter; left time: 5647.8936s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1161132\n",
      "\tspeed: 0.1190s/iter; left time: 5572.2122s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0918901\n",
      "\tspeed: 0.1205s/iter; left time: 5629.8350s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0909352\n",
      "\tspeed: 0.1207s/iter; left time: 5625.9115s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1053216\n",
      "\tspeed: 0.1179s/iter; left time: 5486.0038s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0864966\n",
      "\tspeed: 0.1188s/iter; left time: 5515.0144s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0899872\n",
      "\tspeed: 0.1170s/iter; left time: 5416.6913s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0996342\n",
      "\tspeed: 0.1210s/iter; left time: 5589.6104s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0904091\n",
      "\tspeed: 0.1197s/iter; left time: 5521.2046s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0838375\n",
      "\tspeed: 0.1196s/iter; left time: 5503.2014s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0930159\n",
      "\tspeed: 0.1201s/iter; left time: 5513.1989s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0981316\n",
      "\tspeed: 0.1194s/iter; left time: 5470.3641s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0863672\n",
      "\tspeed: 0.1208s/iter; left time: 5523.6617s\n",
      "Epoch: 3 cost time: 00h:05m:22.30s\n",
      "Epoch: 3 | Train Loss: 0.0952172 Vali Loss: 0.0876163 Test Loss: 0.0995333\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0895635\n",
      "\tspeed: 0.9674s/iter; left time: 44045.3275s\n",
      "\titers: 200, epoch: 4 | loss: 0.0948796\n",
      "\tspeed: 0.1135s/iter; left time: 5154.2111s\n",
      "\titers: 300, epoch: 4 | loss: 0.1079016\n",
      "\tspeed: 0.1192s/iter; left time: 5402.4415s\n",
      "\titers: 400, epoch: 4 | loss: 0.0807232\n",
      "\tspeed: 0.1204s/iter; left time: 5447.4501s\n",
      "\titers: 500, epoch: 4 | loss: 0.0917320\n",
      "\tspeed: 0.1206s/iter; left time: 5443.9061s\n",
      "\titers: 600, epoch: 4 | loss: 0.1008527\n",
      "\tspeed: 0.1212s/iter; left time: 5457.4455s\n",
      "\titers: 700, epoch: 4 | loss: 0.0905929\n",
      "\tspeed: 0.1211s/iter; left time: 5441.5946s\n",
      "\titers: 800, epoch: 4 | loss: 0.0817054\n",
      "\tspeed: 0.1209s/iter; left time: 5418.5595s\n",
      "\titers: 900, epoch: 4 | loss: 0.1022704\n",
      "\tspeed: 0.1202s/iter; left time: 5376.9019s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0999512\n",
      "\tspeed: 0.1211s/iter; left time: 5404.3136s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0844137\n",
      "\tspeed: 0.1180s/iter; left time: 5256.2973s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0867865\n",
      "\tspeed: 0.1206s/iter; left time: 5359.1979s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1027440\n",
      "\tspeed: 0.1200s/iter; left time: 5320.0796s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0912939\n",
      "\tspeed: 0.1153s/iter; left time: 5100.1629s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0903338\n",
      "\tspeed: 0.1172s/iter; left time: 5172.8185s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0982658\n",
      "\tspeed: 0.1160s/iter; left time: 5108.6095s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1128476\n",
      "\tspeed: 0.1203s/iter; left time: 5285.2844s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0907316\n",
      "\tspeed: 0.1210s/iter; left time: 5305.2216s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0883882\n",
      "\tspeed: 0.1180s/iter; left time: 5161.3531s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0840461\n",
      "\tspeed: 0.1159s/iter; left time: 5057.2308s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0890422\n",
      "\tspeed: 0.1211s/iter; left time: 5272.9600s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0912848\n",
      "\tspeed: 0.1211s/iter; left time: 5258.3974s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0897235\n",
      "\tspeed: 0.1185s/iter; left time: 5134.2887s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1019709\n",
      "\tspeed: 0.1184s/iter; left time: 5116.5241s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1029302\n",
      "\tspeed: 0.1200s/iter; left time: 5173.7312s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0911055\n",
      "\tspeed: 0.1146s/iter; left time: 4929.7989s\n",
      "Epoch: 4 cost time: 00h:05m:19.24s\n",
      "Epoch: 4 | Train Loss: 0.0930185 Vali Loss: 0.0847382 Test Loss: 0.0970126\n",
      "Validation loss decreased (0.086975 --> 0.084738).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0929508\n",
      "\tspeed: 1.0025s/iter; left time: 42951.2239s\n",
      "\titers: 200, epoch: 5 | loss: 0.0826883\n",
      "\tspeed: 0.1199s/iter; left time: 5124.5079s\n",
      "\titers: 300, epoch: 5 | loss: 0.0946675\n",
      "\tspeed: 0.1210s/iter; left time: 5160.7740s\n",
      "\titers: 400, epoch: 5 | loss: 0.0974359\n",
      "\tspeed: 0.1195s/iter; left time: 5082.2881s\n",
      "\titers: 500, epoch: 5 | loss: 0.0892060\n",
      "\tspeed: 0.1167s/iter; left time: 4953.6776s\n",
      "\titers: 600, epoch: 5 | loss: 0.0965789\n",
      "\tspeed: 0.1204s/iter; left time: 5096.6301s\n",
      "\titers: 700, epoch: 5 | loss: 0.0803632\n",
      "\tspeed: 0.1200s/iter; left time: 5067.3871s\n",
      "\titers: 800, epoch: 5 | loss: 0.0816639\n",
      "\tspeed: 0.1213s/iter; left time: 5113.3156s\n",
      "\titers: 900, epoch: 5 | loss: 0.0796301\n",
      "\tspeed: 0.1208s/iter; left time: 5080.1200s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0947778\n",
      "\tspeed: 0.1208s/iter; left time: 5066.0018s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0807358\n",
      "\tspeed: 0.1191s/iter; left time: 4983.2578s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0978350\n",
      "\tspeed: 0.1177s/iter; left time: 4913.5078s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0848728\n",
      "\tspeed: 0.1177s/iter; left time: 4903.4044s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0797540\n",
      "\tspeed: 0.1152s/iter; left time: 4787.1344s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0875790\n",
      "\tspeed: 0.1201s/iter; left time: 4976.1642s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0836178\n",
      "\tspeed: 0.1178s/iter; left time: 4869.7515s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0997139\n",
      "\tspeed: 0.1199s/iter; left time: 4944.8162s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0831015\n",
      "\tspeed: 0.1210s/iter; left time: 4979.7305s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0892239\n",
      "\tspeed: 0.1206s/iter; left time: 4948.1102s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0964089\n",
      "\tspeed: 0.1180s/iter; left time: 4832.3976s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0864571\n",
      "\tspeed: 0.1157s/iter; left time: 4726.1310s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0878882\n",
      "\tspeed: 0.1009s/iter; left time: 4112.1107s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0879328\n",
      "\tspeed: 0.1120s/iter; left time: 4552.9007s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0824142\n",
      "\tspeed: 0.1168s/iter; left time: 4735.5517s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0919031\n",
      "\tspeed: 0.1178s/iter; left time: 4762.9217s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0861144\n",
      "\tspeed: 0.1173s/iter; left time: 4730.6570s\n",
      "Epoch: 5 cost time: 00h:05m:17.24s\n",
      "Epoch: 5 | Train Loss: 0.0909510 Vali Loss: 0.0893654 Test Loss: 0.1023573\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0915587\n",
      "\tspeed: 0.9724s/iter; left time: 39053.6271s\n",
      "\titers: 200, epoch: 6 | loss: 0.0839357\n",
      "\tspeed: 0.1172s/iter; left time: 4696.5596s\n",
      "\titers: 300, epoch: 6 | loss: 0.0889259\n",
      "\tspeed: 0.1207s/iter; left time: 4824.6661s\n",
      "\titers: 400, epoch: 6 | loss: 0.0936077\n",
      "\tspeed: 0.1174s/iter; left time: 4681.1153s\n",
      "\titers: 500, epoch: 6 | loss: 0.0880979\n",
      "\tspeed: 0.1211s/iter; left time: 4814.4650s\n",
      "\titers: 600, epoch: 6 | loss: 0.0841514\n",
      "\tspeed: 0.1163s/iter; left time: 4613.3570s\n",
      "\titers: 700, epoch: 6 | loss: 0.0755864\n",
      "\tspeed: 0.1119s/iter; left time: 4424.9740s\n",
      "\titers: 800, epoch: 6 | loss: 0.0940484\n",
      "\tspeed: 0.1193s/iter; left time: 4709.3520s\n",
      "\titers: 900, epoch: 6 | loss: 0.0851911\n",
      "\tspeed: 0.1204s/iter; left time: 4738.1928s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0822518\n",
      "\tspeed: 0.1171s/iter; left time: 4596.1306s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0868344\n",
      "\tspeed: 0.1186s/iter; left time: 4645.2629s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0963878\n",
      "\tspeed: 0.1201s/iter; left time: 4691.8284s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0748070\n",
      "\tspeed: 0.1208s/iter; left time: 4707.5500s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0844528\n",
      "\tspeed: 0.1209s/iter; left time: 4699.4556s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1027090\n",
      "\tspeed: 0.1202s/iter; left time: 4657.6838s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0973790\n",
      "\tspeed: 0.1175s/iter; left time: 4543.9550s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0843874\n",
      "\tspeed: 0.1152s/iter; left time: 4442.2165s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0826969\n",
      "\tspeed: 0.1178s/iter; left time: 4530.4196s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0803014\n",
      "\tspeed: 0.1199s/iter; left time: 4599.0814s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0895572\n",
      "\tspeed: 0.1163s/iter; left time: 4449.4302s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0838565\n",
      "\tspeed: 0.1150s/iter; left time: 4389.7193s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0904025\n",
      "\tspeed: 0.1150s/iter; left time: 4378.3628s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0983820\n",
      "\tspeed: 0.1139s/iter; left time: 4323.7095s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0976380\n",
      "\tspeed: 0.1158s/iter; left time: 4382.8892s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0838670\n",
      "\tspeed: 0.1193s/iter; left time: 4505.8964s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0699371\n",
      "\tspeed: 0.1031s/iter; left time: 3882.8722s\n",
      "Epoch: 6 cost time: 00h:05m:13.96s\n",
      "Epoch: 6 | Train Loss: 0.0890947 Vali Loss: 0.0903814 Test Loss: 0.1015913\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0818404\n",
      "\tspeed: 0.9498s/iter; left time: 35597.2597s\n",
      "\titers: 200, epoch: 7 | loss: 0.0816711\n",
      "\tspeed: 0.1205s/iter; left time: 4505.1611s\n",
      "\titers: 300, epoch: 7 | loss: 0.0799331\n",
      "\tspeed: 0.1183s/iter; left time: 4408.2543s\n",
      "\titers: 400, epoch: 7 | loss: 0.0868699\n",
      "\tspeed: 0.1196s/iter; left time: 4447.8012s\n",
      "\titers: 500, epoch: 7 | loss: 0.0727430\n",
      "\tspeed: 0.1205s/iter; left time: 4468.0822s\n",
      "\titers: 600, epoch: 7 | loss: 0.1011259\n",
      "\tspeed: 0.1199s/iter; left time: 4433.9748s\n",
      "\titers: 700, epoch: 7 | loss: 0.0941924\n",
      "\tspeed: 0.1167s/iter; left time: 4303.5286s\n",
      "\titers: 800, epoch: 7 | loss: 0.1013498\n",
      "\tspeed: 0.1180s/iter; left time: 4338.7645s\n",
      "\titers: 900, epoch: 7 | loss: 0.0759829\n",
      "\tspeed: 0.1145s/iter; left time: 4200.3665s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0930336\n",
      "\tspeed: 0.1187s/iter; left time: 4343.0069s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0847954\n",
      "\tspeed: 0.1173s/iter; left time: 4279.1998s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0941292\n",
      "\tspeed: 0.1117s/iter; left time: 4064.6430s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0802033\n",
      "\tspeed: 0.1201s/iter; left time: 4357.1399s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0939994\n",
      "\tspeed: 0.1140s/iter; left time: 4122.7113s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0865828\n",
      "\tspeed: 0.1168s/iter; left time: 4214.2135s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1023660\n",
      "\tspeed: 0.1166s/iter; left time: 4193.9149s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0873066\n",
      "\tspeed: 0.1160s/iter; left time: 4160.8034s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0956445\n",
      "\tspeed: 0.1195s/iter; left time: 4275.3586s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0968382\n",
      "\tspeed: 0.1204s/iter; left time: 4296.9154s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0982233\n",
      "\tspeed: 0.1196s/iter; left time: 4255.4905s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0968592\n",
      "\tspeed: 0.1206s/iter; left time: 4280.2815s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0934383\n",
      "\tspeed: 0.1163s/iter; left time: 4115.5740s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0847614\n",
      "\tspeed: 0.1069s/iter; left time: 3769.4850s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0907320\n",
      "\tspeed: 0.0988s/iter; left time: 3476.2804s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0855488\n",
      "\tspeed: 0.1049s/iter; left time: 3679.9229s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0829888\n",
      "\tspeed: 0.1192s/iter; left time: 4168.3115s\n",
      "Epoch: 7 cost time: 00h:05m:12.16s\n",
      "Epoch: 7 | Train Loss: 0.0873331 Vali Loss: 0.0918732 Test Loss: 0.1017229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0874928\n",
      "\tspeed: 0.9747s/iter; left time: 33914.2749s\n",
      "\titers: 200, epoch: 8 | loss: 0.0883391\n",
      "\tspeed: 0.1208s/iter; left time: 4192.1919s\n",
      "\titers: 300, epoch: 8 | loss: 0.0920282\n",
      "\tspeed: 0.1206s/iter; left time: 4172.7244s\n",
      "\titers: 400, epoch: 8 | loss: 0.0959568\n",
      "\tspeed: 0.1160s/iter; left time: 4002.3766s\n",
      "\titers: 500, epoch: 8 | loss: 0.0956558\n",
      "\tspeed: 0.1137s/iter; left time: 3911.7858s\n",
      "\titers: 600, epoch: 8 | loss: 0.0814130\n",
      "\tspeed: 0.1138s/iter; left time: 3903.7645s\n",
      "\titers: 700, epoch: 8 | loss: 0.0857233\n",
      "\tspeed: 0.1151s/iter; left time: 3936.0245s\n",
      "\titers: 800, epoch: 8 | loss: 0.0938325\n",
      "\tspeed: 0.1149s/iter; left time: 3917.1635s\n",
      "\titers: 900, epoch: 8 | loss: 0.0937250\n",
      "\tspeed: 0.1155s/iter; left time: 3927.7969s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0781743\n",
      "\tspeed: 0.1171s/iter; left time: 3970.0446s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0944517\n",
      "\tspeed: 0.1190s/iter; left time: 4020.8408s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0741252\n",
      "\tspeed: 0.1204s/iter; left time: 4056.7614s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0715947\n",
      "\tspeed: 0.1169s/iter; left time: 3927.1916s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0883247\n",
      "\tspeed: 0.1207s/iter; left time: 4041.0368s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0868704\n",
      "\tspeed: 0.1211s/iter; left time: 4044.6174s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0939969\n",
      "\tspeed: 0.1207s/iter; left time: 4018.8690s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0877550\n",
      "\tspeed: 0.1188s/iter; left time: 3943.0573s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0865582\n",
      "\tspeed: 0.1168s/iter; left time: 3866.4770s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0877632\n",
      "\tspeed: 0.1206s/iter; left time: 3980.0795s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0904972\n",
      "\tspeed: 0.1198s/iter; left time: 3942.0435s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0918995\n",
      "\tspeed: 0.1113s/iter; left time: 3650.8766s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0880162\n",
      "\tspeed: 0.0986s/iter; left time: 3224.0149s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0901393\n",
      "\tspeed: 0.0995s/iter; left time: 3243.9052s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0813730\n",
      "\tspeed: 0.1093s/iter; left time: 3550.2013s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0902627\n",
      "\tspeed: 0.1151s/iter; left time: 3727.5932s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0779674\n",
      "\tspeed: 0.1148s/iter; left time: 3707.7360s\n",
      "Epoch: 8 cost time: 00h:05m:09.77s\n",
      "Epoch: 8 | Train Loss: 0.0859112 Vali Loss: 0.0929700 Test Loss: 0.1019867\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0787471\n",
      "\tspeed: 0.9498s/iter; left time: 30497.4128s\n",
      "\titers: 200, epoch: 9 | loss: 0.0741966\n",
      "\tspeed: 0.1207s/iter; left time: 3863.2914s\n",
      "\titers: 300, epoch: 9 | loss: 0.0855073\n",
      "\tspeed: 0.1000s/iter; left time: 3191.7296s\n",
      "\titers: 400, epoch: 9 | loss: 0.0870367\n",
      "\tspeed: 0.1007s/iter; left time: 3204.3736s\n",
      "\titers: 500, epoch: 9 | loss: 0.0798489\n",
      "\tspeed: 0.1197s/iter; left time: 3795.4704s\n",
      "\titers: 600, epoch: 9 | loss: 0.0822773\n",
      "\tspeed: 0.1157s/iter; left time: 3658.4578s\n",
      "\titers: 700, epoch: 9 | loss: 0.0890888\n",
      "\tspeed: 0.1152s/iter; left time: 3631.3840s\n",
      "\titers: 800, epoch: 9 | loss: 0.0833702\n",
      "\tspeed: 0.1192s/iter; left time: 3743.8704s\n",
      "\titers: 900, epoch: 9 | loss: 0.0905662\n",
      "\tspeed: 0.1181s/iter; left time: 3698.1330s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0836187\n",
      "\tspeed: 0.1191s/iter; left time: 3717.9772s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0860944\n",
      "\tspeed: 0.1085s/iter; left time: 3375.6543s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0905798\n",
      "\tspeed: 0.1175s/iter; left time: 3644.6725s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0945324\n",
      "\tspeed: 0.1188s/iter; left time: 3672.6808s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0950186\n",
      "\tspeed: 0.1207s/iter; left time: 3719.6932s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0807866\n",
      "\tspeed: 0.1212s/iter; left time: 3722.9836s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0886903\n",
      "\tspeed: 0.1214s/iter; left time: 3716.6798s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0865668\n",
      "\tspeed: 0.1197s/iter; left time: 3651.5093s\n",
      "\titers: 1800, epoch: 9 | loss: 0.1007445\n",
      "\tspeed: 0.1214s/iter; left time: 3692.1223s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0763340\n",
      "\tspeed: 0.1209s/iter; left time: 3664.0420s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0765861\n",
      "\tspeed: 0.1181s/iter; left time: 3566.9978s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0786577\n",
      "\tspeed: 0.1166s/iter; left time: 3510.5500s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0867473\n",
      "\tspeed: 0.1143s/iter; left time: 3429.0570s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0803090\n",
      "\tspeed: 0.1174s/iter; left time: 3510.2132s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0835162\n",
      "\tspeed: 0.1194s/iter; left time: 3559.9645s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0848668\n",
      "\tspeed: 0.1213s/iter; left time: 3604.7663s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0791643\n",
      "\tspeed: 0.1211s/iter; left time: 3586.6447s\n",
      "Epoch: 9 cost time: 00h:05m:15.40s\n",
      "Epoch: 9 | Train Loss: 0.0844346 Vali Loss: 0.0939959 Test Loss: 0.1040973\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.021669914945960045, rmse:0.14720705151557922, mae:0.09701256453990936, rse:0.4322395920753479\n",
      "success delete checkpoints\n",
      "Intermediate time for ES and pred_len 168: 00h:59m:42.76s\n",
      "\n",
      "Intermediate time for ES: 03h:16m:54.38s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 17:12:24,451] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 17:12:25,602] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 17:12:25,602] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 17:12:25,602] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 17:12:25,704] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 17:12:25,704] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 17:12:26,388] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 17:12:26,390] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 17:12:26,390] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 17:12:26,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 17:12:26,391] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 17:12:26,391] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 17:12:26,392] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 17:12:26,729] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 17:12:26,730] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,730] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,855] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 17:12:26,856] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,856] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,857] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 17:12:26,981] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 17:12:26,982] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 17:12:26,982] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.52 GB, percent = 10.0%\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 17:12:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 17:12:26,983] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f73cce4d3d0>\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 17:12:26,984] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 17:12:26,985] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 17:12:26,986] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1235450\n",
      "\tspeed: 0.1709s/iter; left time: 9199.2743s\n",
      "\titers: 200, epoch: 1 | loss: 0.1423506\n",
      "\tspeed: 0.1264s/iter; left time: 6791.4893s\n",
      "\titers: 300, epoch: 1 | loss: 0.1272179\n",
      "\tspeed: 0.1274s/iter; left time: 6835.9856s\n",
      "\titers: 400, epoch: 1 | loss: 0.0875500\n",
      "\tspeed: 0.1272s/iter; left time: 6808.7591s\n",
      "\titers: 500, epoch: 1 | loss: 0.0672416\n",
      "\tspeed: 0.1263s/iter; left time: 6751.9674s\n",
      "\titers: 600, epoch: 1 | loss: 0.0630464\n",
      "\tspeed: 0.1064s/iter; left time: 5676.5741s\n",
      "\titers: 700, epoch: 1 | loss: 0.0556630\n",
      "\tspeed: 0.1132s/iter; left time: 6025.5661s\n",
      "\titers: 800, epoch: 1 | loss: 0.0871794\n",
      "\tspeed: 0.1206s/iter; left time: 6411.3034s\n",
      "\titers: 900, epoch: 1 | loss: 0.0597571\n",
      "\tspeed: 0.1234s/iter; left time: 6543.0970s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0630214\n",
      "\tspeed: 0.1252s/iter; left time: 6630.3009s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0680356\n",
      "\tspeed: 0.1159s/iter; left time: 6121.6533s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0580192\n",
      "\tspeed: 0.1251s/iter; left time: 6595.4756s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0591872\n",
      "\tspeed: 0.1258s/iter; left time: 6620.7701s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0653023\n",
      "\tspeed: 0.1249s/iter; left time: 6559.8268s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0712432\n",
      "\tspeed: 0.1207s/iter; left time: 6328.8275s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0695876\n",
      "\tspeed: 0.1167s/iter; left time: 6108.8087s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0758628\n",
      "\tspeed: 0.1251s/iter; left time: 6537.7578s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0618466\n",
      "\tspeed: 0.1250s/iter; left time: 6517.0575s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0605258\n",
      "\tspeed: 0.1239s/iter; left time: 6450.2507s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0676748\n",
      "\tspeed: 0.1252s/iter; left time: 6502.8349s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0548497\n",
      "\tspeed: 0.1245s/iter; left time: 6453.6853s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0646279\n",
      "\tspeed: 0.1233s/iter; left time: 6379.6147s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0551258\n",
      "\tspeed: 0.1231s/iter; left time: 6357.6462s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0547290\n",
      "\tspeed: 0.1202s/iter; left time: 6193.7285s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0605357\n",
      "\tspeed: 0.1090s/iter; left time: 5608.3225s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0547643\n",
      "\tspeed: 0.1256s/iter; left time: 6447.3371s\n",
      "Epoch: 1 cost time: 00h:05m:31.08s\n",
      "Epoch: 1 | Train Loss: 0.0753777 Vali Loss: 0.0618922 Test Loss: 0.0666550\n",
      "Validation loss decreased (inf --> 0.061892).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0666260\n",
      "\tspeed: 1.1350s/iter; left time: 58048.2317s\n",
      "\titers: 200, epoch: 2 | loss: 0.0643385\n",
      "\tspeed: 0.1153s/iter; left time: 5883.9379s\n",
      "\titers: 300, epoch: 2 | loss: 0.0632871\n",
      "\tspeed: 0.1162s/iter; left time: 5920.8166s\n",
      "\titers: 400, epoch: 2 | loss: 0.0495252\n",
      "\tspeed: 0.1149s/iter; left time: 5841.7038s\n",
      "\titers: 500, epoch: 2 | loss: 0.0526561\n",
      "\tspeed: 0.1160s/iter; left time: 5884.0225s\n",
      "\titers: 600, epoch: 2 | loss: 0.0668836\n",
      "\tspeed: 0.1148s/iter; left time: 5812.2386s\n",
      "\titers: 700, epoch: 2 | loss: 0.0494954\n",
      "\tspeed: 0.1163s/iter; left time: 5877.8236s\n",
      "\titers: 800, epoch: 2 | loss: 0.0575383\n",
      "\tspeed: 0.1176s/iter; left time: 5930.1979s\n",
      "\titers: 900, epoch: 2 | loss: 0.0667061\n",
      "\tspeed: 0.1162s/iter; left time: 5851.7246s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0703753\n",
      "\tspeed: 0.1163s/iter; left time: 5842.2256s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0538771\n",
      "\tspeed: 0.1175s/iter; left time: 5891.4125s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0611253\n",
      "\tspeed: 0.1161s/iter; left time: 5811.7703s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0747418\n",
      "\tspeed: 0.1162s/iter; left time: 5801.6492s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0687371\n",
      "\tspeed: 0.1164s/iter; left time: 5799.8883s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0663108\n",
      "\tspeed: 0.1131s/iter; left time: 5625.1032s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0538801\n",
      "\tspeed: 0.1160s/iter; left time: 5757.7508s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0519770\n",
      "\tspeed: 0.1161s/iter; left time: 5750.7496s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0473880\n",
      "\tspeed: 0.1157s/iter; left time: 5720.8980s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0611360\n",
      "\tspeed: 0.1091s/iter; left time: 5382.2693s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0615671\n",
      "\tspeed: 0.1157s/iter; left time: 5697.1500s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0669719\n",
      "\tspeed: 0.1144s/iter; left time: 5622.4151s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0486104\n",
      "\tspeed: 0.1115s/iter; left time: 5466.5169s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0629132\n",
      "\tspeed: 0.0996s/iter; left time: 4874.3981s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0576007\n",
      "\tspeed: 0.1159s/iter; left time: 5658.8639s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0599707\n",
      "\tspeed: 0.1151s/iter; left time: 5612.5594s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0471631\n",
      "\tspeed: 0.1138s/iter; left time: 5536.1960s\n",
      "Epoch: 2 cost time: 00h:05m:09.65s\n",
      "Epoch: 2 | Train Loss: 0.0591769 Vali Loss: 0.0600467 Test Loss: 0.0646332\n",
      "Validation loss decreased (0.061892 --> 0.060047).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0599396\n",
      "\tspeed: 1.0017s/iter; left time: 48531.2763s\n",
      "\titers: 200, epoch: 3 | loss: 0.0531570\n",
      "\tspeed: 0.1156s/iter; left time: 5589.2050s\n",
      "\titers: 300, epoch: 3 | loss: 0.0663147\n",
      "\tspeed: 0.1158s/iter; left time: 5586.6661s\n",
      "\titers: 400, epoch: 3 | loss: 0.0573017\n",
      "\tspeed: 0.1159s/iter; left time: 5580.3935s\n",
      "\titers: 500, epoch: 3 | loss: 0.0458791\n",
      "\tspeed: 0.1154s/iter; left time: 5546.2894s\n",
      "\titers: 600, epoch: 3 | loss: 0.0676508\n",
      "\tspeed: 0.1149s/iter; left time: 5509.2006s\n",
      "\titers: 700, epoch: 3 | loss: 0.0643910\n",
      "\tspeed: 0.1139s/iter; left time: 5447.9562s\n",
      "\titers: 800, epoch: 3 | loss: 0.0421067\n",
      "\tspeed: 0.1138s/iter; left time: 5435.4103s\n",
      "\titers: 900, epoch: 3 | loss: 0.0515741\n",
      "\tspeed: 0.1167s/iter; left time: 5561.4390s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0688308\n",
      "\tspeed: 0.1143s/iter; left time: 5432.7623s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0621392\n",
      "\tspeed: 0.1157s/iter; left time: 5488.6243s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0543004\n",
      "\tspeed: 0.1139s/iter; left time: 5393.9227s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0568760\n",
      "\tspeed: 0.1123s/iter; left time: 5308.0440s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0596535\n",
      "\tspeed: 0.1156s/iter; left time: 5449.7068s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0517118\n",
      "\tspeed: 0.1175s/iter; left time: 5526.7793s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0598669\n",
      "\tspeed: 0.1141s/iter; left time: 5355.7305s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0843264\n",
      "\tspeed: 0.1161s/iter; left time: 5440.6865s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0665351\n",
      "\tspeed: 0.1159s/iter; left time: 5417.3726s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0814799\n",
      "\tspeed: 0.1170s/iter; left time: 5457.2950s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0534720\n",
      "\tspeed: 0.1156s/iter; left time: 5381.8339s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0550123\n",
      "\tspeed: 0.1146s/iter; left time: 5323.4283s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0583318\n",
      "\tspeed: 0.1139s/iter; left time: 5279.1013s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0620737\n",
      "\tspeed: 0.1148s/iter; left time: 5309.9709s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0487021\n",
      "\tspeed: 0.1148s/iter; left time: 5295.7191s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0631454\n",
      "\tspeed: 0.1149s/iter; left time: 5290.4083s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0609962\n",
      "\tspeed: 0.1161s/iter; left time: 5334.7029s\n",
      "Epoch: 3 cost time: 00h:05m:11.05s\n",
      "Epoch: 3 | Train Loss: 0.0568341 Vali Loss: 0.0582712 Test Loss: 0.0628686\n",
      "Validation loss decreased (0.060047 --> 0.058271).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0608920\n",
      "\tspeed: 0.9958s/iter; left time: 45557.7186s\n",
      "\titers: 200, epoch: 4 | loss: 0.0551032\n",
      "\tspeed: 0.1162s/iter; left time: 5303.4218s\n",
      "\titers: 300, epoch: 4 | loss: 0.0613622\n",
      "\tspeed: 0.1152s/iter; left time: 5247.1252s\n",
      "\titers: 400, epoch: 4 | loss: 0.0487876\n",
      "\tspeed: 0.1171s/iter; left time: 5320.0904s\n",
      "\titers: 500, epoch: 4 | loss: 0.0552078\n",
      "\tspeed: 0.1154s/iter; left time: 5235.0554s\n",
      "\titers: 600, epoch: 4 | loss: 0.0562445\n",
      "\tspeed: 0.1146s/iter; left time: 5183.8728s\n",
      "\titers: 700, epoch: 4 | loss: 0.0495130\n",
      "\tspeed: 0.1151s/iter; left time: 5198.9802s\n",
      "\titers: 800, epoch: 4 | loss: 0.0537369\n",
      "\tspeed: 0.1157s/iter; left time: 5212.9689s\n",
      "\titers: 900, epoch: 4 | loss: 0.0510692\n",
      "\tspeed: 0.1087s/iter; left time: 4886.4472s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0544674\n",
      "\tspeed: 0.1074s/iter; left time: 4815.6019s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0422624\n",
      "\tspeed: 0.1168s/iter; left time: 5226.3263s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0601855\n",
      "\tspeed: 0.1161s/iter; left time: 5184.8261s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0548600\n",
      "\tspeed: 0.1165s/iter; left time: 5188.9580s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0726929\n",
      "\tspeed: 0.1159s/iter; left time: 5149.5405s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0452244\n",
      "\tspeed: 0.1088s/iter; left time: 4826.8430s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0690438\n",
      "\tspeed: 0.0972s/iter; left time: 4302.4621s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0575143\n",
      "\tspeed: 0.1068s/iter; left time: 4713.2015s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0486670\n",
      "\tspeed: 0.1148s/iter; left time: 5057.0657s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0498352\n",
      "\tspeed: 0.1158s/iter; left time: 5091.6035s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0595263\n",
      "\tspeed: 0.1151s/iter; left time: 5046.4383s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0519087\n",
      "\tspeed: 0.1161s/iter; left time: 5077.3423s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0471623\n",
      "\tspeed: 0.1157s/iter; left time: 5050.6021s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0451208\n",
      "\tspeed: 0.1147s/iter; left time: 4993.9801s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0595563\n",
      "\tspeed: 0.1163s/iter; left time: 5053.4402s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0549772\n",
      "\tspeed: 0.1171s/iter; left time: 5077.2092s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0421879\n",
      "\tspeed: 0.1182s/iter; left time: 5111.2165s\n",
      "Epoch: 4 cost time: 00h:05m:07.48s\n",
      "Epoch: 4 | Train Loss: 0.0554718 Vali Loss: 0.0564575 Test Loss: 0.0613898\n",
      "Validation loss decreased (0.058271 --> 0.056457).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0640360\n",
      "\tspeed: 1.0044s/iter; left time: 43243.4201s\n",
      "\titers: 200, epoch: 5 | loss: 0.0555768\n",
      "\tspeed: 0.1147s/iter; left time: 4925.9657s\n",
      "\titers: 300, epoch: 5 | loss: 0.0411871\n",
      "\tspeed: 0.1165s/iter; left time: 4991.0211s\n",
      "\titers: 400, epoch: 5 | loss: 0.0514965\n",
      "\tspeed: 0.1150s/iter; left time: 4918.4055s\n",
      "\titers: 500, epoch: 5 | loss: 0.0501315\n",
      "\tspeed: 0.1163s/iter; left time: 4960.9477s\n",
      "\titers: 600, epoch: 5 | loss: 0.0461096\n",
      "\tspeed: 0.1147s/iter; left time: 4880.6477s\n",
      "\titers: 700, epoch: 5 | loss: 0.0579179\n",
      "\tspeed: 0.1136s/iter; left time: 4820.8346s\n",
      "\titers: 800, epoch: 5 | loss: 0.0609003\n",
      "\tspeed: 0.1150s/iter; left time: 4869.3633s\n",
      "\titers: 900, epoch: 5 | loss: 0.0671605\n",
      "\tspeed: 0.1151s/iter; left time: 4861.7235s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0632332\n",
      "\tspeed: 0.1173s/iter; left time: 4944.0046s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0637036\n",
      "\tspeed: 0.1190s/iter; left time: 5004.7178s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0558049\n",
      "\tspeed: 0.1176s/iter; left time: 4932.2707s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0510187\n",
      "\tspeed: 0.1163s/iter; left time: 4866.3413s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0609071\n",
      "\tspeed: 0.1148s/iter; left time: 4792.6565s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0545633\n",
      "\tspeed: 0.1153s/iter; left time: 4803.1647s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0504324\n",
      "\tspeed: 0.1138s/iter; left time: 4729.1776s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0481021\n",
      "\tspeed: 0.1145s/iter; left time: 4745.4862s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0610766\n",
      "\tspeed: 0.1147s/iter; left time: 4741.8071s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0627400\n",
      "\tspeed: 0.1153s/iter; left time: 4756.0239s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0500407\n",
      "\tspeed: 0.1143s/iter; left time: 4703.0922s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0583279\n",
      "\tspeed: 0.1172s/iter; left time: 4811.2763s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0541765\n",
      "\tspeed: 0.1158s/iter; left time: 4743.4718s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0596457\n",
      "\tspeed: 0.1148s/iter; left time: 4689.5396s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0590647\n",
      "\tspeed: 0.1144s/iter; left time: 4663.7620s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0656443\n",
      "\tspeed: 0.1169s/iter; left time: 4751.0732s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0455275\n",
      "\tspeed: 0.1183s/iter; left time: 4797.2323s\n",
      "Epoch: 5 cost time: 00h:05m:12.21s\n",
      "Epoch: 5 | Train Loss: 0.0542485 Vali Loss: 0.0564108 Test Loss: 0.0613056\n",
      "Validation loss decreased (0.056457 --> 0.056411).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0637889\n",
      "\tspeed: 0.9923s/iter; left time: 40045.1374s\n",
      "\titers: 200, epoch: 6 | loss: 0.0451849\n",
      "\tspeed: 0.1147s/iter; left time: 4618.4988s\n",
      "\titers: 300, epoch: 6 | loss: 0.0558336\n",
      "\tspeed: 0.1178s/iter; left time: 4731.3767s\n",
      "\titers: 400, epoch: 6 | loss: 0.0449031\n",
      "\tspeed: 0.1143s/iter; left time: 4579.4036s\n",
      "\titers: 500, epoch: 6 | loss: 0.0486762\n",
      "\tspeed: 0.1138s/iter; left time: 4548.7897s\n",
      "\titers: 600, epoch: 6 | loss: 0.0463912\n",
      "\tspeed: 0.1045s/iter; left time: 4163.3650s\n",
      "\titers: 700, epoch: 6 | loss: 0.0641680\n",
      "\tspeed: 0.1140s/iter; left time: 4531.3681s\n",
      "\titers: 800, epoch: 6 | loss: 0.0503087\n",
      "\tspeed: 0.1131s/iter; left time: 4486.3651s\n",
      "\titers: 900, epoch: 6 | loss: 0.0475201\n",
      "\tspeed: 0.1131s/iter; left time: 4472.9158s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0430374\n",
      "\tspeed: 0.1087s/iter; left time: 4289.6954s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0464682\n",
      "\tspeed: 0.1172s/iter; left time: 4612.2652s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0538542\n",
      "\tspeed: 0.1124s/iter; left time: 4413.5484s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0497831\n",
      "\tspeed: 0.1154s/iter; left time: 4516.9435s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0475077\n",
      "\tspeed: 0.1203s/iter; left time: 4696.5876s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0516047\n",
      "\tspeed: 0.1202s/iter; left time: 4684.0462s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0589632\n",
      "\tspeed: 0.1198s/iter; left time: 4654.5062s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0611102\n",
      "\tspeed: 0.1087s/iter; left time: 4214.3618s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0457706\n",
      "\tspeed: 0.1011s/iter; left time: 3907.6123s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0506376\n",
      "\tspeed: 0.1035s/iter; left time: 3989.7789s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0497848\n",
      "\tspeed: 0.1010s/iter; left time: 3884.6604s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0544278\n",
      "\tspeed: 0.1170s/iter; left time: 4486.9950s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0567052\n",
      "\tspeed: 0.1170s/iter; left time: 4475.3916s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0539384\n",
      "\tspeed: 0.1164s/iter; left time: 4440.4460s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0443573\n",
      "\tspeed: 0.1149s/iter; left time: 4372.0053s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0481149\n",
      "\tspeed: 0.1163s/iter; left time: 4415.3780s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0425841\n",
      "\tspeed: 0.1157s/iter; left time: 4379.4275s\n",
      "Epoch: 6 cost time: 00h:05m:06.57s\n",
      "Epoch: 6 | Train Loss: 0.0533678 Vali Loss: 0.0547401 Test Loss: 0.0591254\n",
      "Validation loss decreased (0.056411 --> 0.054740).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0502016\n",
      "\tspeed: 1.0296s/iter; left time: 38773.3304s\n",
      "\titers: 200, epoch: 7 | loss: 0.0561645\n",
      "\tspeed: 0.1150s/iter; left time: 4319.0582s\n",
      "\titers: 300, epoch: 7 | loss: 0.0592544\n",
      "\tspeed: 0.1166s/iter; left time: 4368.5747s\n",
      "\titers: 400, epoch: 7 | loss: 0.0523131\n",
      "\tspeed: 0.1152s/iter; left time: 4302.4687s\n",
      "\titers: 500, epoch: 7 | loss: 0.0502636\n",
      "\tspeed: 0.1170s/iter; left time: 4358.5172s\n",
      "\titers: 600, epoch: 7 | loss: 0.0607144\n",
      "\tspeed: 0.1150s/iter; left time: 4271.4519s\n",
      "\titers: 700, epoch: 7 | loss: 0.0567622\n",
      "\tspeed: 0.1152s/iter; left time: 4267.8602s\n",
      "\titers: 800, epoch: 7 | loss: 0.0611483\n",
      "\tspeed: 0.1139s/iter; left time: 4207.9981s\n",
      "\titers: 900, epoch: 7 | loss: 0.0641236\n",
      "\tspeed: 0.1155s/iter; left time: 4257.5038s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0467553\n",
      "\tspeed: 0.1152s/iter; left time: 4235.1825s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0538213\n",
      "\tspeed: 0.1148s/iter; left time: 4209.1981s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0498144\n",
      "\tspeed: 0.1148s/iter; left time: 4198.4598s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0496599\n",
      "\tspeed: 0.1161s/iter; left time: 4231.8114s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0542373\n",
      "\tspeed: 0.1173s/iter; left time: 4263.3834s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0536195\n",
      "\tspeed: 0.1177s/iter; left time: 4266.0222s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0534565\n",
      "\tspeed: 0.1186s/iter; left time: 4288.0050s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0545159\n",
      "\tspeed: 0.1170s/iter; left time: 4218.4950s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0453232\n",
      "\tspeed: 0.1156s/iter; left time: 4158.2451s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0651416\n",
      "\tspeed: 0.1162s/iter; left time: 4166.1893s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0515023\n",
      "\tspeed: 0.1163s/iter; left time: 4159.4492s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0462779\n",
      "\tspeed: 0.1159s/iter; left time: 4134.0760s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0499391\n",
      "\tspeed: 0.1149s/iter; left time: 4086.8695s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0656782\n",
      "\tspeed: 0.1120s/iter; left time: 3973.0698s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0466200\n",
      "\tspeed: 0.1168s/iter; left time: 4128.5093s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0558347\n",
      "\tspeed: 0.1064s/iter; left time: 3751.7386s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0696534\n",
      "\tspeed: 0.1163s/iter; left time: 4087.9350s\n",
      "Epoch: 7 cost time: 00h:05m:11.90s\n",
      "Epoch: 7 | Train Loss: 0.0528111 Vali Loss: 0.0540425 Test Loss: 0.0587007\n",
      "Validation loss decreased (0.054740 --> 0.054043).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0460588\n",
      "\tspeed: 0.9988s/iter; left time: 34918.3940s\n",
      "\titers: 200, epoch: 8 | loss: 0.0622025\n",
      "\tspeed: 0.1157s/iter; left time: 4032.4403s\n",
      "\titers: 300, epoch: 8 | loss: 0.0519913\n",
      "\tspeed: 0.1162s/iter; left time: 4038.4653s\n",
      "\titers: 400, epoch: 8 | loss: 0.0406249\n",
      "\tspeed: 0.1150s/iter; left time: 3987.5934s\n",
      "\titers: 500, epoch: 8 | loss: 0.0526539\n",
      "\tspeed: 0.1142s/iter; left time: 3945.7237s\n",
      "\titers: 600, epoch: 8 | loss: 0.0560093\n",
      "\tspeed: 0.1175s/iter; left time: 4048.8991s\n",
      "\titers: 700, epoch: 8 | loss: 0.0679693\n",
      "\tspeed: 0.1141s/iter; left time: 3922.2742s\n",
      "\titers: 800, epoch: 8 | loss: 0.0448523\n",
      "\tspeed: 0.1151s/iter; left time: 3944.3258s\n",
      "\titers: 900, epoch: 8 | loss: 0.0458734\n",
      "\tspeed: 0.1150s/iter; left time: 3927.7100s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0651497\n",
      "\tspeed: 0.1132s/iter; left time: 3854.4789s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0496319\n",
      "\tspeed: 0.1132s/iter; left time: 3843.3400s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0648924\n",
      "\tspeed: 0.1132s/iter; left time: 3832.8576s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0607049\n",
      "\tspeed: 0.1129s/iter; left time: 3811.3001s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0580690\n",
      "\tspeed: 0.1145s/iter; left time: 3852.7940s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0442169\n",
      "\tspeed: 0.1133s/iter; left time: 3801.9133s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0520920\n",
      "\tspeed: 0.1161s/iter; left time: 3884.0944s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0542979\n",
      "\tspeed: 0.1164s/iter; left time: 3883.1795s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0425588\n",
      "\tspeed: 0.1149s/iter; left time: 3820.4780s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0418803\n",
      "\tspeed: 0.1177s/iter; left time: 3902.8595s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0463494\n",
      "\tspeed: 0.1135s/iter; left time: 3752.5452s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0457020\n",
      "\tspeed: 0.1149s/iter; left time: 3788.0254s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0524447\n",
      "\tspeed: 0.1151s/iter; left time: 3783.4499s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0519675\n",
      "\tspeed: 0.1137s/iter; left time: 3726.6258s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0533954\n",
      "\tspeed: 0.1140s/iter; left time: 3723.8943s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0505847\n",
      "\tspeed: 0.1151s/iter; left time: 3746.7302s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0587134\n",
      "\tspeed: 0.1112s/iter; left time: 3611.1521s\n",
      "Epoch: 8 cost time: 00h:05m:09.82s\n",
      "Epoch: 8 | Train Loss: 0.0522896 Vali Loss: 0.0555167 Test Loss: 0.0603411\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0553223\n",
      "\tspeed: 0.9867s/iter; left time: 31835.2533s\n",
      "\titers: 200, epoch: 9 | loss: 0.0466191\n",
      "\tspeed: 0.1148s/iter; left time: 3691.0295s\n",
      "\titers: 300, epoch: 9 | loss: 0.0613746\n",
      "\tspeed: 0.1182s/iter; left time: 3791.6792s\n",
      "\titers: 400, epoch: 9 | loss: 0.0450902\n",
      "\tspeed: 0.1144s/iter; left time: 3656.6553s\n",
      "\titers: 500, epoch: 9 | loss: 0.0554611\n",
      "\tspeed: 0.1145s/iter; left time: 3649.9308s\n",
      "\titers: 600, epoch: 9 | loss: 0.0576737\n",
      "\tspeed: 0.1145s/iter; left time: 3638.1291s\n",
      "\titers: 700, epoch: 9 | loss: 0.0529940\n",
      "\tspeed: 0.1136s/iter; left time: 3598.6982s\n",
      "\titers: 800, epoch: 9 | loss: 0.0483810\n",
      "\tspeed: 0.1137s/iter; left time: 3590.0342s\n",
      "\titers: 900, epoch: 9 | loss: 0.0450235\n",
      "\tspeed: 0.1146s/iter; left time: 3607.4244s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0433518\n",
      "\tspeed: 0.1136s/iter; left time: 3561.6420s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0480308\n",
      "\tspeed: 0.1146s/iter; left time: 3583.4718s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0439340\n",
      "\tspeed: 0.1164s/iter; left time: 3626.4656s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0469765\n",
      "\tspeed: 0.1178s/iter; left time: 3660.5851s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0589630\n",
      "\tspeed: 0.1170s/iter; left time: 3622.9203s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0431547\n",
      "\tspeed: 0.1166s/iter; left time: 3599.4052s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0477735\n",
      "\tspeed: 0.1174s/iter; left time: 3610.7391s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0572250\n",
      "\tspeed: 0.1168s/iter; left time: 3581.7098s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0460013\n",
      "\tspeed: 0.1160s/iter; left time: 3546.0751s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0480974\n",
      "\tspeed: 0.1157s/iter; left time: 3523.3599s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0449856\n",
      "\tspeed: 0.1150s/iter; left time: 3492.2872s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0463317\n",
      "\tspeed: 0.1146s/iter; left time: 3468.2989s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0446014\n",
      "\tspeed: 0.1154s/iter; left time: 3481.5565s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0504780\n",
      "\tspeed: 0.1150s/iter; left time: 3457.8265s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0436172\n",
      "\tspeed: 0.1044s/iter; left time: 3129.5063s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0481363\n",
      "\tspeed: 0.1109s/iter; left time: 3312.7002s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0430334\n",
      "\tspeed: 0.1163s/iter; left time: 3462.6061s\n",
      "Epoch: 9 cost time: 00h:05m:10.44s\n",
      "Epoch: 9 | Train Loss: 0.0518848 Vali Loss: 0.0543241 Test Loss: 0.0591126\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0361740\n",
      "\tspeed: 0.9902s/iter; left time: 29278.6451s\n",
      "\titers: 200, epoch: 10 | loss: 0.0424532\n",
      "\tspeed: 0.1171s/iter; left time: 3451.9981s\n",
      "\titers: 300, epoch: 10 | loss: 0.0439460\n",
      "\tspeed: 0.1168s/iter; left time: 3430.5602s\n",
      "\titers: 400, epoch: 10 | loss: 0.0425408\n",
      "\tspeed: 0.1149s/iter; left time: 3363.0818s\n",
      "\titers: 500, epoch: 10 | loss: 0.0446251\n",
      "\tspeed: 0.1083s/iter; left time: 3159.9032s\n",
      "\titers: 600, epoch: 10 | loss: 0.0730994\n",
      "\tspeed: 0.0978s/iter; left time: 2842.7083s\n",
      "\titers: 700, epoch: 10 | loss: 0.0452029\n",
      "\tspeed: 0.1071s/iter; left time: 3102.6407s\n",
      "\titers: 800, epoch: 10 | loss: 0.0518145\n",
      "\tspeed: 0.1168s/iter; left time: 3371.1007s\n",
      "\titers: 900, epoch: 10 | loss: 0.0441767\n",
      "\tspeed: 0.1157s/iter; left time: 3329.4667s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0625665\n",
      "\tspeed: 0.1147s/iter; left time: 3287.9282s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0505257\n",
      "\tspeed: 0.1168s/iter; left time: 3335.7106s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0455996\n",
      "\tspeed: 0.1160s/iter; left time: 3301.2877s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0378268\n",
      "\tspeed: 0.1171s/iter; left time: 3322.3388s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0600771\n",
      "\tspeed: 0.1090s/iter; left time: 3081.8802s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0465586\n",
      "\tspeed: 0.1049s/iter; left time: 2955.8424s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0524688\n",
      "\tspeed: 0.1152s/iter; left time: 3233.3545s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0475789\n",
      "\tspeed: 0.1150s/iter; left time: 3217.2197s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0450688\n",
      "\tspeed: 0.1152s/iter; left time: 3209.4761s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0655913\n",
      "\tspeed: 0.1140s/iter; left time: 3166.8476s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0443577\n",
      "\tspeed: 0.1162s/iter; left time: 3215.0263s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0562413\n",
      "\tspeed: 0.1150s/iter; left time: 3170.1847s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0390418\n",
      "\tspeed: 0.1138s/iter; left time: 3126.7674s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0545616\n",
      "\tspeed: 0.1150s/iter; left time: 3147.1205s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0571885\n",
      "\tspeed: 0.1142s/iter; left time: 3113.9495s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0536950\n",
      "\tspeed: 0.1161s/iter; left time: 3155.2774s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0660274\n",
      "\tspeed: 0.1142s/iter; left time: 3090.6736s\n",
      "Epoch: 10 cost time: 00h:05m:06.98s\n",
      "Epoch: 10 | Train Loss: 0.0514509 Vali Loss: 0.0540158 Test Loss: 0.0590907\n",
      "Validation loss decreased (0.054043 --> 0.054016).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0486004\n",
      "\tspeed: 1.0021s/iter; left time: 26928.6929s\n",
      "\titers: 200, epoch: 11 | loss: 0.0427696\n",
      "\tspeed: 0.1153s/iter; left time: 3085.9052s\n",
      "\titers: 300, epoch: 11 | loss: 0.0502599\n",
      "\tspeed: 0.1149s/iter; left time: 3065.5980s\n",
      "\titers: 400, epoch: 11 | loss: 0.0545642\n",
      "\tspeed: 0.1134s/iter; left time: 3012.5267s\n",
      "\titers: 500, epoch: 11 | loss: 0.0415499\n",
      "\tspeed: 0.1138s/iter; left time: 3011.8174s\n",
      "\titers: 600, epoch: 11 | loss: 0.0503596\n",
      "\tspeed: 0.1146s/iter; left time: 3023.1896s\n",
      "\titers: 700, epoch: 11 | loss: 0.0587240\n",
      "\tspeed: 0.1135s/iter; left time: 2982.7512s\n",
      "\titers: 800, epoch: 11 | loss: 0.0542230\n",
      "\tspeed: 0.1123s/iter; left time: 2938.1676s\n",
      "\titers: 900, epoch: 11 | loss: 0.0419671\n",
      "\tspeed: 0.1148s/iter; left time: 2992.4918s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0539247\n",
      "\tspeed: 0.1124s/iter; left time: 2920.3983s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0388036\n",
      "\tspeed: 0.1146s/iter; left time: 2964.1080s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0615964\n",
      "\tspeed: 0.1143s/iter; left time: 2944.4680s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0468018\n",
      "\tspeed: 0.1168s/iter; left time: 2997.5277s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0443031\n",
      "\tspeed: 0.1158s/iter; left time: 2960.0505s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0490965\n",
      "\tspeed: 0.1151s/iter; left time: 2931.7566s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0545864\n",
      "\tspeed: 0.1154s/iter; left time: 2927.6585s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0517874\n",
      "\tspeed: 0.1152s/iter; left time: 2910.0282s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0556483\n",
      "\tspeed: 0.1133s/iter; left time: 2852.6975s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0528931\n",
      "\tspeed: 0.1132s/iter; left time: 2836.9183s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0465289\n",
      "\tspeed: 0.1157s/iter; left time: 2890.0792s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0451324\n",
      "\tspeed: 0.1154s/iter; left time: 2869.9303s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0447210\n",
      "\tspeed: 0.1166s/iter; left time: 2887.1053s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0517201\n",
      "\tspeed: 0.1175s/iter; left time: 2899.2541s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0628082\n",
      "\tspeed: 0.1169s/iter; left time: 2871.3439s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0555514\n",
      "\tspeed: 0.1130s/iter; left time: 2765.3817s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0564730\n",
      "\tspeed: 0.1166s/iter; left time: 2842.0555s\n",
      "Epoch: 11 cost time: 00h:05m:10.13s\n",
      "Epoch: 11 | Train Loss: 0.0511123 Vali Loss: 0.0542110 Test Loss: 0.0589370\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0413404\n",
      "\tspeed: 0.9823s/iter; left time: 23745.4161s\n",
      "\titers: 200, epoch: 12 | loss: 0.0456423\n",
      "\tspeed: 0.1154s/iter; left time: 2779.3363s\n",
      "\titers: 300, epoch: 12 | loss: 0.0599975\n",
      "\tspeed: 0.1153s/iter; left time: 2763.6161s\n",
      "\titers: 400, epoch: 12 | loss: 0.0393989\n",
      "\tspeed: 0.1138s/iter; left time: 2715.9943s\n",
      "\titers: 500, epoch: 12 | loss: 0.0631208\n",
      "\tspeed: 0.1157s/iter; left time: 2751.6474s\n",
      "\titers: 600, epoch: 12 | loss: 0.0394312\n",
      "\tspeed: 0.1140s/iter; left time: 2698.6083s\n",
      "\titers: 700, epoch: 12 | loss: 0.0610043\n",
      "\tspeed: 0.1135s/iter; left time: 2675.2827s\n",
      "\titers: 800, epoch: 12 | loss: 0.0467539\n",
      "\tspeed: 0.1157s/iter; left time: 2715.7502s\n",
      "\titers: 900, epoch: 12 | loss: 0.0553005\n",
      "\tspeed: 0.1154s/iter; left time: 2697.5072s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0448402\n",
      "\tspeed: 0.1136s/iter; left time: 2644.6336s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0574175\n",
      "\tspeed: 0.1139s/iter; left time: 2638.3887s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0437614\n",
      "\tspeed: 0.1162s/iter; left time: 2680.1898s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0455631\n",
      "\tspeed: 0.1155s/iter; left time: 2653.7600s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0532934\n",
      "\tspeed: 0.1164s/iter; left time: 2663.1238s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0407963\n",
      "\tspeed: 0.1150s/iter; left time: 2618.6813s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0568854\n",
      "\tspeed: 0.1145s/iter; left time: 2596.8789s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0648938\n",
      "\tspeed: 0.1142s/iter; left time: 2577.4574s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0681437\n",
      "\tspeed: 0.1140s/iter; left time: 2562.1909s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0537705\n",
      "\tspeed: 0.1157s/iter; left time: 2588.1640s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0641221\n",
      "\tspeed: 0.1151s/iter; left time: 2564.6658s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0408601\n",
      "\tspeed: 0.1162s/iter; left time: 2576.4156s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0592678\n",
      "\tspeed: 0.1166s/iter; left time: 2573.9433s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0429511\n",
      "\tspeed: 0.1151s/iter; left time: 2528.9800s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0535271\n",
      "\tspeed: 0.1165s/iter; left time: 2548.6800s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0569462\n",
      "\tspeed: 0.1157s/iter; left time: 2518.9259s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0574420\n",
      "\tspeed: 0.1157s/iter; left time: 2508.4919s\n",
      "Epoch: 12 cost time: 00h:05m:11.04s\n",
      "Epoch: 12 | Train Loss: 0.0506596 Vali Loss: 0.0538890 Test Loss: 0.0585830\n",
      "Validation loss decreased (0.054016 --> 0.053889).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0584482\n",
      "\tspeed: 1.0053s/iter; left time: 21590.2078s\n",
      "\titers: 200, epoch: 13 | loss: 0.0413951\n",
      "\tspeed: 0.1161s/iter; left time: 2480.8565s\n",
      "\titers: 300, epoch: 13 | loss: 0.0452191\n",
      "\tspeed: 0.1165s/iter; left time: 2478.1191s\n",
      "\titers: 400, epoch: 13 | loss: 0.0583766\n",
      "\tspeed: 0.1098s/iter; left time: 2324.9653s\n",
      "\titers: 500, epoch: 13 | loss: 0.0591615\n",
      "\tspeed: 0.1100s/iter; left time: 2318.2569s\n",
      "\titers: 600, epoch: 13 | loss: 0.0405239\n",
      "\tspeed: 0.1142s/iter; left time: 2395.0054s\n",
      "\titers: 700, epoch: 13 | loss: 0.0362805\n",
      "\tspeed: 0.1175s/iter; left time: 2452.6224s\n",
      "\titers: 800, epoch: 13 | loss: 0.0517154\n",
      "\tspeed: 0.1177s/iter; left time: 2444.6048s\n",
      "\titers: 900, epoch: 13 | loss: 0.0501910\n",
      "\tspeed: 0.1155s/iter; left time: 2387.9347s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0493395\n",
      "\tspeed: 0.1146s/iter; left time: 2357.9690s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0423344\n",
      "\tspeed: 0.1135s/iter; left time: 2324.5108s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0497453\n",
      "\tspeed: 0.1068s/iter; left time: 2177.0310s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0577142\n",
      "\tspeed: 0.1165s/iter; left time: 2361.4740s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0529139\n",
      "\tspeed: 0.1156s/iter; left time: 2332.8182s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0540026\n",
      "\tspeed: 0.1148s/iter; left time: 2304.2520s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0617525\n",
      "\tspeed: 0.1158s/iter; left time: 2312.6715s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0441613\n",
      "\tspeed: 0.1173s/iter; left time: 2332.2387s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0492811\n",
      "\tspeed: 0.1146s/iter; left time: 2265.5691s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0474770\n",
      "\tspeed: 0.1157s/iter; left time: 2276.9391s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0442020\n",
      "\tspeed: 0.1147s/iter; left time: 2245.0456s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0425807\n",
      "\tspeed: 0.1154s/iter; left time: 2248.2726s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0565319\n",
      "\tspeed: 0.1149s/iter; left time: 2226.1965s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0483340\n",
      "\tspeed: 0.1142s/iter; left time: 2201.1308s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0448032\n",
      "\tspeed: 0.1157s/iter; left time: 2219.2562s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0564242\n",
      "\tspeed: 0.1155s/iter; left time: 2204.1146s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0571262\n",
      "\tspeed: 0.1146s/iter; left time: 2174.3821s\n",
      "Epoch: 13 cost time: 00h:05m:10.22s\n",
      "Epoch: 13 | Train Loss: 0.0503306 Vali Loss: 0.0541599 Test Loss: 0.0587250\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0493650\n",
      "\tspeed: 0.9792s/iter; left time: 18389.0411s\n",
      "\titers: 200, epoch: 14 | loss: 0.0601776\n",
      "\tspeed: 0.1034s/iter; left time: 1930.8380s\n",
      "\titers: 300, epoch: 14 | loss: 0.0522846\n",
      "\tspeed: 0.1021s/iter; left time: 1896.8407s\n",
      "\titers: 400, epoch: 14 | loss: 0.0502198\n",
      "\tspeed: 0.1145s/iter; left time: 2115.8220s\n",
      "\titers: 500, epoch: 14 | loss: 0.0539697\n",
      "\tspeed: 0.1164s/iter; left time: 2140.3343s\n",
      "\titers: 600, epoch: 14 | loss: 0.0492951\n",
      "\tspeed: 0.1176s/iter; left time: 2150.5131s\n",
      "\titers: 700, epoch: 14 | loss: 0.0499482\n",
      "\tspeed: 0.1150s/iter; left time: 2090.1754s\n",
      "\titers: 800, epoch: 14 | loss: 0.0521548\n",
      "\tspeed: 0.1161s/iter; left time: 2099.8732s\n",
      "\titers: 900, epoch: 14 | loss: 0.0697306\n",
      "\tspeed: 0.1148s/iter; left time: 2063.9074s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0585489\n",
      "\tspeed: 0.1154s/iter; left time: 2063.1400s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0513945\n",
      "\tspeed: 0.1099s/iter; left time: 1953.2543s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0772938\n",
      "\tspeed: 0.0962s/iter; left time: 1700.8666s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0545680\n",
      "\tspeed: 0.1087s/iter; left time: 1911.1805s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0644709\n",
      "\tspeed: 0.1148s/iter; left time: 2006.5881s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0620324\n",
      "\tspeed: 0.1151s/iter; left time: 2000.9906s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0397591\n",
      "\tspeed: 0.1161s/iter; left time: 2007.0436s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0408592\n",
      "\tspeed: 0.1163s/iter; left time: 1998.8850s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0460187\n",
      "\tspeed: 0.1155s/iter; left time: 1973.5367s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0569062\n",
      "\tspeed: 0.1158s/iter; left time: 1966.1250s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0487930\n",
      "\tspeed: 0.1182s/iter; left time: 1995.7385s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0453887\n",
      "\tspeed: 0.1157s/iter; left time: 1942.2739s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0463870\n",
      "\tspeed: 0.1146s/iter; left time: 1910.9998s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0511975\n",
      "\tspeed: 0.1136s/iter; left time: 1882.9192s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0511051\n",
      "\tspeed: 0.1133s/iter; left time: 1867.1957s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0467189\n",
      "\tspeed: 0.1142s/iter; left time: 1871.1775s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0436856\n",
      "\tspeed: 0.1135s/iter; left time: 1847.0458s\n",
      "Epoch: 14 cost time: 00h:05m:04.65s\n",
      "Epoch: 14 | Train Loss: 0.0499867 Vali Loss: 0.0545931 Test Loss: 0.0590969\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0640254\n",
      "\tspeed: 0.9585s/iter; left time: 15414.8101s\n",
      "\titers: 200, epoch: 15 | loss: 0.0394662\n",
      "\tspeed: 0.1147s/iter; left time: 1832.6257s\n",
      "\titers: 300, epoch: 15 | loss: 0.0387993\n",
      "\tspeed: 0.1114s/iter; left time: 1768.7440s\n",
      "\titers: 400, epoch: 15 | loss: 0.0437090\n",
      "\tspeed: 0.1139s/iter; left time: 1798.2244s\n",
      "\titers: 500, epoch: 15 | loss: 0.0550465\n",
      "\tspeed: 0.1150s/iter; left time: 1803.8693s\n",
      "\titers: 600, epoch: 15 | loss: 0.0468511\n",
      "\tspeed: 0.1146s/iter; left time: 1786.1690s\n",
      "\titers: 700, epoch: 15 | loss: 0.0450512\n",
      "\tspeed: 0.1049s/iter; left time: 1624.8732s\n",
      "\titers: 800, epoch: 15 | loss: 0.0656973\n",
      "\tspeed: 0.1028s/iter; left time: 1580.9335s\n",
      "\titers: 900, epoch: 15 | loss: 0.0515748\n",
      "\tspeed: 0.1146s/iter; left time: 1751.6202s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0481743\n",
      "\tspeed: 0.1146s/iter; left time: 1740.0851s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0463150\n",
      "\tspeed: 0.1153s/iter; left time: 1738.8196s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0436716\n",
      "\tspeed: 0.1163s/iter; left time: 1742.8213s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0492802\n",
      "\tspeed: 0.1154s/iter; left time: 1717.0990s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0472708\n",
      "\tspeed: 0.1164s/iter; left time: 1721.1618s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0623606\n",
      "\tspeed: 0.1149s/iter; left time: 1686.7182s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0426730\n",
      "\tspeed: 0.1020s/iter; left time: 1487.0946s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0558270\n",
      "\tspeed: 0.1052s/iter; left time: 1523.4790s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0486433\n",
      "\tspeed: 0.1157s/iter; left time: 1664.4138s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0478737\n",
      "\tspeed: 0.1145s/iter; left time: 1634.9584s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0472583\n",
      "\tspeed: 0.1154s/iter; left time: 1636.2746s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0523396\n",
      "\tspeed: 0.1153s/iter; left time: 1624.4740s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0442008\n",
      "\tspeed: 0.1147s/iter; left time: 1604.0077s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0422843\n",
      "\tspeed: 0.1151s/iter; left time: 1598.2178s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0520833\n",
      "\tspeed: 0.1162s/iter; left time: 1601.3281s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0416124\n",
      "\tspeed: 0.1169s/iter; left time: 1599.4313s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0408221\n",
      "\tspeed: 0.1160s/iter; left time: 1575.6306s\n",
      "Epoch: 15 cost time: 00h:05m:05.30s\n",
      "Epoch: 15 | Train Loss: 0.0497556 Vali Loss: 0.0541359 Test Loss: 0.0592553\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0512954\n",
      "\tspeed: 0.9859s/iter; left time: 13197.7336s\n",
      "\titers: 200, epoch: 16 | loss: 0.0593351\n",
      "\tspeed: 0.1170s/iter; left time: 1555.0710s\n",
      "\titers: 300, epoch: 16 | loss: 0.0391670\n",
      "\tspeed: 0.1167s/iter; left time: 1538.5807s\n",
      "\titers: 400, epoch: 16 | loss: 0.0442540\n",
      "\tspeed: 0.1148s/iter; left time: 1502.5334s\n",
      "\titers: 500, epoch: 16 | loss: 0.0314678\n",
      "\tspeed: 0.1151s/iter; left time: 1494.0879s\n",
      "\titers: 600, epoch: 16 | loss: 0.0491887\n",
      "\tspeed: 0.1163s/iter; left time: 1498.0406s\n",
      "\titers: 700, epoch: 16 | loss: 0.0406393\n",
      "\tspeed: 0.1175s/iter; left time: 1502.7032s\n",
      "\titers: 800, epoch: 16 | loss: 0.0444243\n",
      "\tspeed: 0.1161s/iter; left time: 1473.1850s\n",
      "\titers: 900, epoch: 16 | loss: 0.0567451\n",
      "\tspeed: 0.1157s/iter; left time: 1456.0354s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0386371\n",
      "\tspeed: 0.1164s/iter; left time: 1452.9967s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0542442\n",
      "\tspeed: 0.1157s/iter; left time: 1433.5309s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0485616\n",
      "\tspeed: 0.1151s/iter; left time: 1413.9901s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0415913\n",
      "\tspeed: 0.1157s/iter; left time: 1409.7789s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0443841\n",
      "\tspeed: 0.1154s/iter; left time: 1395.2161s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0549853\n",
      "\tspeed: 0.1153s/iter; left time: 1381.9400s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0475429\n",
      "\tspeed: 0.1159s/iter; left time: 1377.5505s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0519379\n",
      "\tspeed: 0.1164s/iter; left time: 1372.1601s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0507676\n",
      "\tspeed: 0.1171s/iter; left time: 1368.3624s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0471109\n",
      "\tspeed: 0.1154s/iter; left time: 1337.5959s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0422025\n",
      "\tspeed: 0.1085s/iter; left time: 1245.6917s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0510330\n",
      "\tspeed: 0.1127s/iter; left time: 1283.7618s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0449592\n",
      "\tspeed: 0.1148s/iter; left time: 1296.1556s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0611455\n",
      "\tspeed: 0.1136s/iter; left time: 1271.2093s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0596026\n",
      "\tspeed: 0.1161s/iter; left time: 1286.6216s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0501522\n",
      "\tspeed: 0.1142s/iter; left time: 1254.3166s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0497214\n",
      "\tspeed: 0.1132s/iter; left time: 1231.9641s\n",
      "Epoch: 16 cost time: 00h:05m:11.56s\n",
      "Epoch: 16 | Train Loss: 0.0493574 Vali Loss: 0.0541783 Test Loss: 0.0598210\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 17 | loss: 0.0487838\n",
      "\tspeed: 0.9838s/iter; left time: 10515.7423s\n",
      "\titers: 200, epoch: 17 | loss: 0.0512044\n",
      "\tspeed: 0.1157s/iter; left time: 1225.1472s\n",
      "\titers: 300, epoch: 17 | loss: 0.0408817\n",
      "\tspeed: 0.1164s/iter; left time: 1220.6455s\n",
      "\titers: 400, epoch: 17 | loss: 0.0356466\n",
      "\tspeed: 0.1136s/iter; left time: 1180.0588s\n",
      "\titers: 500, epoch: 17 | loss: 0.0510470\n",
      "\tspeed: 0.1157s/iter; left time: 1190.0262s\n",
      "\titers: 600, epoch: 17 | loss: 0.0428915\n",
      "\tspeed: 0.1165s/iter; left time: 1186.6402s\n",
      "\titers: 700, epoch: 17 | loss: 0.0436458\n",
      "\tspeed: 0.1149s/iter; left time: 1159.5412s\n",
      "\titers: 800, epoch: 17 | loss: 0.0422825\n",
      "\tspeed: 0.1146s/iter; left time: 1144.9722s\n",
      "\titers: 900, epoch: 17 | loss: 0.0605514\n",
      "\tspeed: 0.1144s/iter; left time: 1131.1799s\n",
      "\titers: 1000, epoch: 17 | loss: 0.0491973\n",
      "\tspeed: 0.1141s/iter; left time: 1117.0568s\n",
      "\titers: 1100, epoch: 17 | loss: 0.0428665\n",
      "\tspeed: 0.1137s/iter; left time: 1102.1115s\n",
      "\titers: 1200, epoch: 17 | loss: 0.0562080\n",
      "\tspeed: 0.1141s/iter; left time: 1093.6470s\n",
      "\titers: 1300, epoch: 17 | loss: 0.0395262\n",
      "\tspeed: 0.1141s/iter; left time: 1082.2386s\n",
      "\titers: 1400, epoch: 17 | loss: 0.0628368\n",
      "\tspeed: 0.1163s/iter; left time: 1091.7784s\n",
      "\titers: 1500, epoch: 17 | loss: 0.0415979\n",
      "\tspeed: 0.1177s/iter; left time: 1093.3501s\n",
      "\titers: 1600, epoch: 17 | loss: 0.0425477\n",
      "\tspeed: 0.1169s/iter; left time: 1073.9668s\n",
      "\titers: 1700, epoch: 17 | loss: 0.0511805\n",
      "\tspeed: 0.1170s/iter; left time: 1062.9802s\n",
      "\titers: 1800, epoch: 17 | loss: 0.0501039\n",
      "\tspeed: 0.1162s/iter; left time: 1044.8256s\n",
      "\titers: 1900, epoch: 17 | loss: 0.0420724\n",
      "\tspeed: 0.1165s/iter; left time: 1035.7642s\n",
      "\titers: 2000, epoch: 17 | loss: 0.0464478\n",
      "\tspeed: 0.1044s/iter; left time: 917.5155s\n",
      "\titers: 2100, epoch: 17 | loss: 0.0633934\n",
      "\tspeed: 0.1143s/iter; left time: 993.4731s\n",
      "\titers: 2200, epoch: 17 | loss: 0.0512717\n",
      "\tspeed: 0.1152s/iter; left time: 989.2179s\n",
      "\titers: 2300, epoch: 17 | loss: 0.0531858\n",
      "\tspeed: 0.1137s/iter; left time: 965.5545s\n",
      "\titers: 2400, epoch: 17 | loss: 0.0680438\n",
      "\tspeed: 0.1137s/iter; left time: 953.4365s\n",
      "\titers: 2500, epoch: 17 | loss: 0.0617753\n",
      "\tspeed: 0.1152s/iter; left time: 955.2259s\n",
      "\titers: 2600, epoch: 17 | loss: 0.0388164\n",
      "\tspeed: 0.1147s/iter; left time: 939.5783s\n",
      "Epoch: 17 cost time: 00h:05m:10.41s\n",
      "Epoch: 17 | Train Loss: 0.0491620 Vali Loss: 0.0546305 Test Loss: 0.0602432\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010352413170039654, rmse:0.1017468124628067, mae:0.058582987636327744, rse:0.392534464597702\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 24: 01h:50m:52.56s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 19:03:17,061] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 19:03:18,263] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 19:03:18,263] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 19:03:18,263] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 19:03:18,367] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 19:03:18,367] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 19:03:19,052] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 19:03:19,054] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 19:03:19,054] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 19:03:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 19:03:19,056] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 19:03:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 19:03:19,056] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 19:03:19,385] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 19:03:19,386] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,386] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,503] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 19:03:19,504] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,504] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,504] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 19:03:19,616] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 19:03:19,617] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 19:03:19,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.09 GB, percent = 10.0%\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 19:03:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 19:03:19,619] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 19:03:19,619] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdb688760d0>\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 19:03:19,620] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 19:03:19,621] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 19:03:19,622] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1399898\n",
      "\tspeed: 0.1710s/iter; left time: 9185.5004s\n",
      "\titers: 200, epoch: 1 | loss: 0.1168020\n",
      "\tspeed: 0.1273s/iter; left time: 6823.3374s\n",
      "\titers: 300, epoch: 1 | loss: 0.1196281\n",
      "\tspeed: 0.1278s/iter; left time: 6837.3337s\n",
      "\titers: 400, epoch: 1 | loss: 0.1010908\n",
      "\tspeed: 0.1129s/iter; left time: 6032.9864s\n",
      "\titers: 500, epoch: 1 | loss: 0.0785264\n",
      "\tspeed: 0.1178s/iter; left time: 6282.5744s\n",
      "\titers: 600, epoch: 1 | loss: 0.0839203\n",
      "\tspeed: 0.1142s/iter; left time: 6079.7280s\n",
      "\titers: 700, epoch: 1 | loss: 0.0833786\n",
      "\tspeed: 0.1114s/iter; left time: 5918.1044s\n",
      "\titers: 800, epoch: 1 | loss: 0.0887009\n",
      "\tspeed: 0.1250s/iter; left time: 6629.4213s\n",
      "\titers: 900, epoch: 1 | loss: 0.0852673\n",
      "\tspeed: 0.1264s/iter; left time: 6689.9738s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0693184\n",
      "\tspeed: 0.1111s/iter; left time: 5867.2965s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0696165\n",
      "\tspeed: 0.1177s/iter; left time: 6203.3647s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0693079\n",
      "\tspeed: 0.1249s/iter; left time: 6572.6196s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0922560\n",
      "\tspeed: 0.1254s/iter; left time: 6584.3957s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0846009\n",
      "\tspeed: 0.1255s/iter; left time: 6580.5273s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0679483\n",
      "\tspeed: 0.1245s/iter; left time: 6513.0298s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0670433\n",
      "\tspeed: 0.1239s/iter; left time: 6472.0220s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0842042\n",
      "\tspeed: 0.1255s/iter; left time: 6543.4722s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0703235\n",
      "\tspeed: 0.1258s/iter; left time: 6544.0364s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0694279\n",
      "\tspeed: 0.1259s/iter; left time: 6537.2713s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0827992\n",
      "\tspeed: 0.1147s/iter; left time: 5942.9665s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0705764\n",
      "\tspeed: 0.1218s/iter; left time: 6298.3915s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0862902\n",
      "\tspeed: 0.1244s/iter; left time: 6421.4284s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0764715\n",
      "\tspeed: 0.1241s/iter; left time: 6396.1412s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0649146\n",
      "\tspeed: 0.1239s/iter; left time: 6369.6821s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0675585\n",
      "\tspeed: 0.1242s/iter; left time: 6372.0066s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0922287\n",
      "\tspeed: 0.1273s/iter; left time: 6522.9023s\n",
      "Epoch: 1 cost time: 00h:05m:31.02s\n",
      "Epoch: 1 | Train Loss: 0.0871358 Vali Loss: 0.0761358 Test Loss: 0.0846921\n",
      "Validation loss decreased (inf --> 0.076136).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0748160\n",
      "\tspeed: 1.1148s/iter; left time: 56889.9772s\n",
      "\titers: 200, epoch: 2 | loss: 0.0666597\n",
      "\tspeed: 0.1153s/iter; left time: 5870.0649s\n",
      "\titers: 300, epoch: 2 | loss: 0.0733586\n",
      "\tspeed: 0.1155s/iter; left time: 5871.3775s\n",
      "\titers: 400, epoch: 2 | loss: 0.0713845\n",
      "\tspeed: 0.1182s/iter; left time: 5998.5678s\n",
      "\titers: 500, epoch: 2 | loss: 0.0738021\n",
      "\tspeed: 0.1163s/iter; left time: 5887.6479s\n",
      "\titers: 600, epoch: 2 | loss: 0.0705914\n",
      "\tspeed: 0.1166s/iter; left time: 5889.3509s\n",
      "\titers: 700, epoch: 2 | loss: 0.0696770\n",
      "\tspeed: 0.1146s/iter; left time: 5778.8295s\n",
      "\titers: 800, epoch: 2 | loss: 0.0733058\n",
      "\tspeed: 0.1152s/iter; left time: 5800.3344s\n",
      "\titers: 900, epoch: 2 | loss: 0.0828630\n",
      "\tspeed: 0.1149s/iter; left time: 5771.4858s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0755392\n",
      "\tspeed: 0.1149s/iter; left time: 5757.6629s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0760631\n",
      "\tspeed: 0.1146s/iter; left time: 5731.0691s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0672225\n",
      "\tspeed: 0.1143s/iter; left time: 5705.2586s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0847869\n",
      "\tspeed: 0.1149s/iter; left time: 5724.5213s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0584152\n",
      "\tspeed: 0.1133s/iter; left time: 5636.7114s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0725951\n",
      "\tspeed: 0.1136s/iter; left time: 5637.1066s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0695227\n",
      "\tspeed: 0.1145s/iter; left time: 5672.3644s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0701093\n",
      "\tspeed: 0.1135s/iter; left time: 5608.9226s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0907094\n",
      "\tspeed: 0.1151s/iter; left time: 5679.9633s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0744436\n",
      "\tspeed: 0.1156s/iter; left time: 5688.6276s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0658760\n",
      "\tspeed: 0.1141s/iter; left time: 5606.2591s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0895933\n",
      "\tspeed: 0.1143s/iter; left time: 5604.1308s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0795399\n",
      "\tspeed: 0.1144s/iter; left time: 5595.9551s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0616416\n",
      "\tspeed: 0.1144s/iter; left time: 5588.0029s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0696839\n",
      "\tspeed: 0.1149s/iter; left time: 5601.2856s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0602486\n",
      "\tspeed: 0.1139s/iter; left time: 5538.9357s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0719646\n",
      "\tspeed: 0.1147s/iter; left time: 5566.5009s\n",
      "Epoch: 2 cost time: 00h:05m:09.51s\n",
      "Epoch: 2 | Train Loss: 0.0721934 Vali Loss: 0.0745545 Test Loss: 0.0839525\n",
      "Validation loss decreased (0.076136 --> 0.074555).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0690167\n",
      "\tspeed: 0.9829s/iter; left time: 47513.1729s\n",
      "\titers: 200, epoch: 3 | loss: 0.0873170\n",
      "\tspeed: 0.1153s/iter; left time: 5560.8989s\n",
      "\titers: 300, epoch: 3 | loss: 0.0652578\n",
      "\tspeed: 0.1151s/iter; left time: 5542.9431s\n",
      "\titers: 400, epoch: 3 | loss: 0.0529526\n",
      "\tspeed: 0.1165s/iter; left time: 5597.0160s\n",
      "\titers: 500, epoch: 3 | loss: 0.0714227\n",
      "\tspeed: 0.1152s/iter; left time: 5524.0865s\n",
      "\titers: 600, epoch: 3 | loss: 0.0688608\n",
      "\tspeed: 0.1155s/iter; left time: 5524.1627s\n",
      "\titers: 700, epoch: 3 | loss: 0.0620311\n",
      "\tspeed: 0.1143s/iter; left time: 5456.6919s\n",
      "\titers: 800, epoch: 3 | loss: 0.0708930\n",
      "\tspeed: 0.1151s/iter; left time: 5482.7212s\n",
      "\titers: 900, epoch: 3 | loss: 0.0866079\n",
      "\tspeed: 0.1146s/iter; left time: 5446.4498s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0677101\n",
      "\tspeed: 0.1144s/iter; left time: 5429.1977s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0758670\n",
      "\tspeed: 0.1149s/iter; left time: 5437.1999s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0556624\n",
      "\tspeed: 0.1144s/iter; left time: 5402.1315s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0775621\n",
      "\tspeed: 0.1155s/iter; left time: 5442.7883s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0611256\n",
      "\tspeed: 0.1147s/iter; left time: 5394.3616s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0686625\n",
      "\tspeed: 0.1144s/iter; left time: 5368.4786s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0653413\n",
      "\tspeed: 0.1144s/iter; left time: 5358.8470s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0682989\n",
      "\tspeed: 0.1141s/iter; left time: 5335.2256s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0813247\n",
      "\tspeed: 0.1143s/iter; left time: 5329.0297s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0699451\n",
      "\tspeed: 0.1141s/iter; left time: 5310.2608s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0698264\n",
      "\tspeed: 0.1151s/iter; left time: 5347.3520s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0730097\n",
      "\tspeed: 0.1152s/iter; left time: 5338.2925s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0531443\n",
      "\tspeed: 0.1158s/iter; left time: 5355.9318s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0865950\n",
      "\tspeed: 0.1090s/iter; left time: 5029.5136s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0673362\n",
      "\tspeed: 0.1011s/iter; left time: 4654.7382s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0694377\n",
      "\tspeed: 0.1161s/iter; left time: 5335.0049s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0681323\n",
      "\tspeed: 0.1161s/iter; left time: 5321.2158s\n",
      "Epoch: 3 cost time: 00h:05m:07.51s\n",
      "Epoch: 3 | Train Loss: 0.0699143 Vali Loss: 0.0731580 Test Loss: 0.0823368\n",
      "Validation loss decreased (0.074555 --> 0.073158).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0732795\n",
      "\tspeed: 0.9948s/iter; left time: 45411.7549s\n",
      "\titers: 200, epoch: 4 | loss: 0.0683801\n",
      "\tspeed: 0.1161s/iter; left time: 5288.1880s\n",
      "\titers: 300, epoch: 4 | loss: 0.0746370\n",
      "\tspeed: 0.1159s/iter; left time: 5265.9576s\n",
      "\titers: 400, epoch: 4 | loss: 0.0614159\n",
      "\tspeed: 0.1162s/iter; left time: 5270.5503s\n",
      "\titers: 500, epoch: 4 | loss: 0.0871528\n",
      "\tspeed: 0.1066s/iter; left time: 4822.4420s\n",
      "\titers: 600, epoch: 4 | loss: 0.0667713\n",
      "\tspeed: 0.1041s/iter; left time: 4698.7239s\n",
      "\titers: 700, epoch: 4 | loss: 0.0765538\n",
      "\tspeed: 0.1160s/iter; left time: 5225.1456s\n",
      "\titers: 800, epoch: 4 | loss: 0.0732877\n",
      "\tspeed: 0.1163s/iter; left time: 5225.4239s\n",
      "\titers: 900, epoch: 4 | loss: 0.0660509\n",
      "\tspeed: 0.1156s/iter; left time: 5183.2518s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0636234\n",
      "\tspeed: 0.1159s/iter; left time: 5184.5016s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0860807\n",
      "\tspeed: 0.1158s/iter; left time: 5171.6883s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0955131\n",
      "\tspeed: 0.1154s/iter; left time: 5140.2352s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0609814\n",
      "\tspeed: 0.1156s/iter; left time: 5137.8842s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0679537\n",
      "\tspeed: 0.1161s/iter; left time: 5149.3415s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0625240\n",
      "\tspeed: 0.1164s/iter; left time: 5150.7455s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0702323\n",
      "\tspeed: 0.1138s/iter; left time: 5025.1369s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0583599\n",
      "\tspeed: 0.1120s/iter; left time: 4934.0505s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0770510\n",
      "\tspeed: 0.1162s/iter; left time: 5104.6844s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0586945\n",
      "\tspeed: 0.1156s/iter; left time: 5070.5662s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0661756\n",
      "\tspeed: 0.1150s/iter; left time: 5030.8945s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0605308\n",
      "\tspeed: 0.1143s/iter; left time: 4990.1155s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0575532\n",
      "\tspeed: 0.1147s/iter; left time: 4993.7352s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0731880\n",
      "\tspeed: 0.1102s/iter; left time: 4787.6022s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0779390\n",
      "\tspeed: 0.1149s/iter; left time: 4979.2589s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0875420\n",
      "\tspeed: 0.1159s/iter; left time: 5013.2793s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0610501\n",
      "\tspeed: 0.1139s/iter; left time: 4914.4886s\n",
      "Epoch: 4 cost time: 00h:05m:07.85s\n",
      "Epoch: 4 | Train Loss: 0.0683495 Vali Loss: 0.0731372 Test Loss: 0.0824272\n",
      "Validation loss decreased (0.073158 --> 0.073137).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0724338\n",
      "\tspeed: 0.9850s/iter; left time: 42312.7008s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606321\n",
      "\tspeed: 0.1163s/iter; left time: 4984.2519s\n",
      "\titers: 300, epoch: 5 | loss: 0.0696154\n",
      "\tspeed: 0.1152s/iter; left time: 4927.5901s\n",
      "\titers: 400, epoch: 5 | loss: 0.0509575\n",
      "\tspeed: 0.1163s/iter; left time: 4960.8790s\n",
      "\titers: 500, epoch: 5 | loss: 0.0617031\n",
      "\tspeed: 0.1151s/iter; left time: 4898.9553s\n",
      "\titers: 600, epoch: 5 | loss: 0.0621447\n",
      "\tspeed: 0.1153s/iter; left time: 4896.5531s\n",
      "\titers: 700, epoch: 5 | loss: 0.0803559\n",
      "\tspeed: 0.1160s/iter; left time: 4913.9575s\n",
      "\titers: 800, epoch: 5 | loss: 0.0754673\n",
      "\tspeed: 0.1135s/iter; left time: 4798.2497s\n",
      "\titers: 900, epoch: 5 | loss: 0.0662868\n",
      "\tspeed: 0.1113s/iter; left time: 4692.1219s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0548034\n",
      "\tspeed: 0.1160s/iter; left time: 4879.1030s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0607634\n",
      "\tspeed: 0.1158s/iter; left time: 4860.3527s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0584400\n",
      "\tspeed: 0.1155s/iter; left time: 4833.9256s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0783436\n",
      "\tspeed: 0.1142s/iter; left time: 4768.1298s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0662102\n",
      "\tspeed: 0.1146s/iter; left time: 4773.3554s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0863430\n",
      "\tspeed: 0.1157s/iter; left time: 4808.5477s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0737604\n",
      "\tspeed: 0.1166s/iter; left time: 4834.0150s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0550957\n",
      "\tspeed: 0.1155s/iter; left time: 4777.7990s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0731752\n",
      "\tspeed: 0.1155s/iter; left time: 4766.7627s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0629882\n",
      "\tspeed: 0.1151s/iter; left time: 4735.7323s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0697896\n",
      "\tspeed: 0.1153s/iter; left time: 4734.1007s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0609421\n",
      "\tspeed: 0.1154s/iter; left time: 4727.8986s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0741776\n",
      "\tspeed: 0.1156s/iter; left time: 4724.3142s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0553646\n",
      "\tspeed: 0.1154s/iter; left time: 4704.0377s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0486240\n",
      "\tspeed: 0.1153s/iter; left time: 4686.0936s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761278\n",
      "\tspeed: 0.1151s/iter; left time: 4668.5441s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0535641\n",
      "\tspeed: 0.1158s/iter; left time: 4684.5321s\n",
      "Epoch: 5 cost time: 00h:05m:10.77s\n",
      "Epoch: 5 | Train Loss: 0.0669839 Vali Loss: 0.0734747 Test Loss: 0.0828658\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0774265\n",
      "\tspeed: 0.9689s/iter; left time: 39014.1841s\n",
      "\titers: 200, epoch: 6 | loss: 0.0675554\n",
      "\tspeed: 0.1162s/iter; left time: 4667.5723s\n",
      "\titers: 300, epoch: 6 | loss: 0.0765251\n",
      "\tspeed: 0.1162s/iter; left time: 4654.4439s\n",
      "\titers: 400, epoch: 6 | loss: 0.0700064\n",
      "\tspeed: 0.1149s/iter; left time: 4592.5471s\n",
      "\titers: 500, epoch: 6 | loss: 0.0766230\n",
      "\tspeed: 0.1147s/iter; left time: 4572.6966s\n",
      "\titers: 600, epoch: 6 | loss: 0.0631887\n",
      "\tspeed: 0.1137s/iter; left time: 4522.0342s\n",
      "\titers: 700, epoch: 6 | loss: 0.0506507\n",
      "\tspeed: 0.1151s/iter; left time: 4565.3027s\n",
      "\titers: 800, epoch: 6 | loss: 0.0790317\n",
      "\tspeed: 0.0993s/iter; left time: 3929.9614s\n",
      "\titers: 900, epoch: 6 | loss: 0.0650835\n",
      "\tspeed: 0.1154s/iter; left time: 4556.2520s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0651186\n",
      "\tspeed: 0.1161s/iter; left time: 4568.5685s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0657033\n",
      "\tspeed: 0.1155s/iter; left time: 4534.0281s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0679954\n",
      "\tspeed: 0.1150s/iter; left time: 4504.6713s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0765673\n",
      "\tspeed: 0.1143s/iter; left time: 4464.3442s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0846173\n",
      "\tspeed: 0.1152s/iter; left time: 4488.0074s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0672733\n",
      "\tspeed: 0.1143s/iter; left time: 4444.2281s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0670121\n",
      "\tspeed: 0.1143s/iter; left time: 4430.9147s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0699374\n",
      "\tspeed: 0.0974s/iter; left time: 3766.6272s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0562305\n",
      "\tspeed: 0.1068s/iter; left time: 4118.4166s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0630337\n",
      "\tspeed: 0.1155s/iter; left time: 4443.1980s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0607302\n",
      "\tspeed: 0.1163s/iter; left time: 4460.5016s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0773381\n",
      "\tspeed: 0.1163s/iter; left time: 4448.4792s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0546207\n",
      "\tspeed: 0.1052s/iter; left time: 4016.9093s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0630324\n",
      "\tspeed: 0.1025s/iter; left time: 3902.9421s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0660211\n",
      "\tspeed: 0.1168s/iter; left time: 4432.5906s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0584088\n",
      "\tspeed: 0.1152s/iter; left time: 4361.7531s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0677779\n",
      "\tspeed: 0.1154s/iter; left time: 4356.9488s\n",
      "Epoch: 6 cost time: 00h:05m:04.28s\n",
      "Epoch: 6 | Train Loss: 0.0654514 Vali Loss: 0.0743005 Test Loss: 0.0836774\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0600294\n",
      "\tspeed: 0.9650s/iter; left time: 36260.1494s\n",
      "\titers: 200, epoch: 7 | loss: 0.0696560\n",
      "\tspeed: 0.1152s/iter; left time: 4316.2745s\n",
      "\titers: 300, epoch: 7 | loss: 0.0572565\n",
      "\tspeed: 0.1141s/iter; left time: 4266.3479s\n",
      "\titers: 400, epoch: 7 | loss: 0.0701152\n",
      "\tspeed: 0.1122s/iter; left time: 4180.6558s\n",
      "\titers: 500, epoch: 7 | loss: 0.0556676\n",
      "\tspeed: 0.1147s/iter; left time: 4263.4882s\n",
      "\titers: 600, epoch: 7 | loss: 0.0589390\n",
      "\tspeed: 0.1154s/iter; left time: 4276.9752s\n",
      "\titers: 700, epoch: 7 | loss: 0.0610996\n",
      "\tspeed: 0.1163s/iter; left time: 4301.8181s\n",
      "\titers: 800, epoch: 7 | loss: 0.0592947\n",
      "\tspeed: 0.1160s/iter; left time: 4277.1571s\n",
      "\titers: 900, epoch: 7 | loss: 0.0576262\n",
      "\tspeed: 0.1150s/iter; left time: 4227.6204s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0561066\n",
      "\tspeed: 0.1167s/iter; left time: 4279.2954s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0795155\n",
      "\tspeed: 0.1170s/iter; left time: 4278.0027s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0570814\n",
      "\tspeed: 0.1154s/iter; left time: 4209.8739s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0598396\n",
      "\tspeed: 0.1157s/iter; left time: 4207.7197s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0586431\n",
      "\tspeed: 0.1158s/iter; left time: 4199.2408s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0660124\n",
      "\tspeed: 0.1152s/iter; left time: 4167.1480s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0677008\n",
      "\tspeed: 0.1155s/iter; left time: 4165.6651s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0773932\n",
      "\tspeed: 0.1156s/iter; left time: 4157.4724s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0536487\n",
      "\tspeed: 0.1158s/iter; left time: 4153.7954s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0755399\n",
      "\tspeed: 0.1153s/iter; left time: 4123.6078s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0546124\n",
      "\tspeed: 0.1160s/iter; left time: 4136.9054s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0669152\n",
      "\tspeed: 0.1186s/iter; left time: 4218.3129s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0607164\n",
      "\tspeed: 0.1161s/iter; left time: 4117.7144s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0624044\n",
      "\tspeed: 0.1165s/iter; left time: 4119.8844s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0629992\n",
      "\tspeed: 0.1165s/iter; left time: 4108.9138s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0556533\n",
      "\tspeed: 0.1157s/iter; left time: 4069.1627s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0562715\n",
      "\tspeed: 0.1158s/iter; left time: 4061.0523s\n",
      "Epoch: 7 cost time: 00h:05m:11.57s\n",
      "Epoch: 7 | Train Loss: 0.0641364 Vali Loss: 0.0751676 Test Loss: 0.0842783\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0720103\n",
      "\tspeed: 0.9617s/iter; left time: 33548.6899s\n",
      "\titers: 200, epoch: 8 | loss: 0.0527454\n",
      "\tspeed: 0.1150s/iter; left time: 3998.8838s\n",
      "\titers: 300, epoch: 8 | loss: 0.0710557\n",
      "\tspeed: 0.1152s/iter; left time: 3995.5348s\n",
      "\titers: 400, epoch: 8 | loss: 0.0677130\n",
      "\tspeed: 0.1159s/iter; left time: 4009.8367s\n",
      "\titers: 500, epoch: 8 | loss: 0.0566254\n",
      "\tspeed: 0.1158s/iter; left time: 3992.6592s\n",
      "\titers: 600, epoch: 8 | loss: 0.0638889\n",
      "\tspeed: 0.1157s/iter; left time: 3977.3375s\n",
      "\titers: 700, epoch: 8 | loss: 0.0468303\n",
      "\tspeed: 0.1154s/iter; left time: 3956.4133s\n",
      "\titers: 800, epoch: 8 | loss: 0.0620303\n",
      "\tspeed: 0.1149s/iter; left time: 3926.0927s\n",
      "\titers: 900, epoch: 8 | loss: 0.0692302\n",
      "\tspeed: 0.1164s/iter; left time: 3968.8507s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0656562\n",
      "\tspeed: 0.1169s/iter; left time: 3971.7812s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0716746\n",
      "\tspeed: 0.1156s/iter; left time: 3915.3131s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0694502\n",
      "\tspeed: 0.1167s/iter; left time: 3943.7956s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0552431\n",
      "\tspeed: 0.0993s/iter; left time: 3346.0671s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0707242\n",
      "\tspeed: 0.1162s/iter; left time: 3903.1975s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0593503\n",
      "\tspeed: 0.1173s/iter; left time: 3928.6450s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0829420\n",
      "\tspeed: 0.1163s/iter; left time: 3882.3280s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0603914\n",
      "\tspeed: 0.1167s/iter; left time: 3883.2801s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0663493\n",
      "\tspeed: 0.1126s/iter; left time: 3736.0594s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0599436\n",
      "\tspeed: 0.1153s/iter; left time: 3815.3478s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0584302\n",
      "\tspeed: 0.1163s/iter; left time: 3836.6561s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0497975\n",
      "\tspeed: 0.1155s/iter; left time: 3797.8727s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0631689\n",
      "\tspeed: 0.1159s/iter; left time: 3798.3438s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0653137\n",
      "\tspeed: 0.1156s/iter; left time: 3778.1265s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0661211\n",
      "\tspeed: 0.1162s/iter; left time: 3787.1340s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0540105\n",
      "\tspeed: 0.1158s/iter; left time: 3761.9368s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0650571\n",
      "\tspeed: 0.1157s/iter; left time: 3747.3296s\n",
      "Epoch: 8 cost time: 00h:05m:10.26s\n",
      "Epoch: 8 | Train Loss: 0.0627744 Vali Loss: 0.0746843 Test Loss: 0.0836141\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0615777\n",
      "\tspeed: 0.9610s/iter; left time: 30936.0040s\n",
      "\titers: 200, epoch: 9 | loss: 0.0627441\n",
      "\tspeed: 0.1142s/iter; left time: 3665.9066s\n",
      "\titers: 300, epoch: 9 | loss: 0.0565763\n",
      "\tspeed: 0.1150s/iter; left time: 3679.9970s\n",
      "\titers: 400, epoch: 9 | loss: 0.0663073\n",
      "\tspeed: 0.1141s/iter; left time: 3639.1125s\n",
      "\titers: 500, epoch: 9 | loss: 0.0709065\n",
      "\tspeed: 0.1152s/iter; left time: 3662.8615s\n",
      "\titers: 600, epoch: 9 | loss: 0.0613810\n",
      "\tspeed: 0.1146s/iter; left time: 3631.0668s\n",
      "\titers: 700, epoch: 9 | loss: 0.0465641\n",
      "\tspeed: 0.1144s/iter; left time: 3613.6552s\n",
      "\titers: 800, epoch: 9 | loss: 0.0625643\n",
      "\tspeed: 0.1141s/iter; left time: 3592.9230s\n",
      "\titers: 900, epoch: 9 | loss: 0.0633669\n",
      "\tspeed: 0.1130s/iter; left time: 3547.3924s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0585374\n",
      "\tspeed: 0.1156s/iter; left time: 3617.3397s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0481584\n",
      "\tspeed: 0.0971s/iter; left time: 3029.7911s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0595765\n",
      "\tspeed: 0.0963s/iter; left time: 2992.9819s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0813546\n",
      "\tspeed: 0.0964s/iter; left time: 2986.8064s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0566317\n",
      "\tspeed: 0.1011s/iter; left time: 3124.6510s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0461254\n",
      "\tspeed: 0.1145s/iter; left time: 3526.9878s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0633289\n",
      "\tspeed: 0.1148s/iter; left time: 3524.0822s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0650627\n",
      "\tspeed: 0.1126s/iter; left time: 3445.7080s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0545909\n",
      "\tspeed: 0.0963s/iter; left time: 2935.1904s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0653074\n",
      "\tspeed: 0.0962s/iter; left time: 2923.3665s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0694104\n",
      "\tspeed: 0.1085s/iter; left time: 3288.0460s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0491853\n",
      "\tspeed: 0.1142s/iter; left time: 3447.3465s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0691575\n",
      "\tspeed: 0.1148s/iter; left time: 3453.3035s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0568749\n",
      "\tspeed: 0.1152s/iter; left time: 3456.2157s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0560714\n",
      "\tspeed: 0.1096s/iter; left time: 3277.5473s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0608205\n",
      "\tspeed: 0.1150s/iter; left time: 3425.9624s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0655743\n",
      "\tspeed: 0.1154s/iter; left time: 3427.3964s\n",
      "Epoch: 9 cost time: 00h:04m:57.27s\n",
      "Epoch: 9 | Train Loss: 0.0615349 Vali Loss: 0.0750233 Test Loss: 0.0850513\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.018618784844875336, rmse:0.1364506632089615, mae:0.08242722600698471, rse:0.5278697609901428\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 96: 00h:59m:01.27s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 20:02:18,258] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 20:02:19,468] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 20:02:19,468] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 20:02:19,469] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 20:02:19,572] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 20:02:19,572] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 20:02:20,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 20:02:20,243] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 20:02:20,243] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 20:02:20,244] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 20:02:20,244] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 20:02:20,244] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 20:02:20,245] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 20:02:20,518] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 20:02:20,519] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,646] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 20:02:20,647] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,647] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,647] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 20:02:20,765] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 20:02:20,766] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 20:02:20,766] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.6 GB, percent = 10.0%\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 20:02:20,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 20:02:20,767] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa6999be090>\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 20:02:20,768] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 20:02:20,769] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 20:02:20,770] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1351442\n",
      "\tspeed: 0.1699s/iter; left time: 9105.9032s\n",
      "\titers: 200, epoch: 1 | loss: 0.1291015\n",
      "\tspeed: 0.1269s/iter; left time: 6784.2178s\n",
      "\titers: 300, epoch: 1 | loss: 0.1202403\n",
      "\tspeed: 0.1260s/iter; left time: 6727.0647s\n",
      "\titers: 400, epoch: 1 | loss: 0.0905234\n",
      "\tspeed: 0.1254s/iter; left time: 6679.5488s\n",
      "\titers: 500, epoch: 1 | loss: 0.0911944\n",
      "\tspeed: 0.1245s/iter; left time: 6618.7318s\n",
      "\titers: 600, epoch: 1 | loss: 0.0763717\n",
      "\tspeed: 0.1253s/iter; left time: 6651.2209s\n",
      "\titers: 700, epoch: 1 | loss: 0.0910461\n",
      "\tspeed: 0.1256s/iter; left time: 6653.2511s\n",
      "\titers: 800, epoch: 1 | loss: 0.0693435\n",
      "\tspeed: 0.1256s/iter; left time: 6639.6599s\n",
      "\titers: 900, epoch: 1 | loss: 0.0690839\n",
      "\tspeed: 0.1255s/iter; left time: 6622.4819s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0823524\n",
      "\tspeed: 0.1253s/iter; left time: 6602.2213s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0720417\n",
      "\tspeed: 0.1193s/iter; left time: 6273.7521s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0871387\n",
      "\tspeed: 0.1091s/iter; left time: 5725.1725s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0793843\n",
      "\tspeed: 0.1258s/iter; left time: 6587.3256s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0777027\n",
      "\tspeed: 0.1255s/iter; left time: 6560.8929s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0669637\n",
      "\tspeed: 0.1244s/iter; left time: 6493.7056s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0711201\n",
      "\tspeed: 0.1090s/iter; left time: 5679.2976s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0681182\n",
      "\tspeed: 0.1205s/iter; left time: 6266.1622s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0701449\n",
      "\tspeed: 0.1236s/iter; left time: 6414.7803s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0895861\n",
      "\tspeed: 0.1240s/iter; left time: 6422.2342s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0716423\n",
      "\tspeed: 0.1242s/iter; left time: 6420.4835s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0750876\n",
      "\tspeed: 0.1240s/iter; left time: 6396.4932s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0838144\n",
      "\tspeed: 0.1231s/iter; left time: 6339.3436s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0789797\n",
      "\tspeed: 0.1241s/iter; left time: 6376.1942s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0668762\n",
      "\tspeed: 0.1235s/iter; left time: 6334.2670s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0662106\n",
      "\tspeed: 0.1233s/iter; left time: 6311.9263s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0773352\n",
      "\tspeed: 0.1238s/iter; left time: 6326.0240s\n",
      "Epoch: 1 cost time: 00h:05m:32.23s\n",
      "Epoch: 1 | Train Loss: 0.0896614 Vali Loss: 0.0796900 Test Loss: 0.0892447\n",
      "Validation loss decreased (inf --> 0.079690).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0728463\n",
      "\tspeed: 1.0958s/iter; left time: 55774.3952s\n",
      "\titers: 200, epoch: 2 | loss: 0.0773688\n",
      "\tspeed: 0.1164s/iter; left time: 5910.8337s\n",
      "\titers: 300, epoch: 2 | loss: 0.0769546\n",
      "\tspeed: 0.1010s/iter; left time: 5121.3912s\n",
      "\titers: 400, epoch: 2 | loss: 0.0773306\n",
      "\tspeed: 0.1160s/iter; left time: 5867.6510s\n",
      "\titers: 500, epoch: 2 | loss: 0.0708913\n",
      "\tspeed: 0.1173s/iter; left time: 5923.1477s\n",
      "\titers: 600, epoch: 2 | loss: 0.0599826\n",
      "\tspeed: 0.1166s/iter; left time: 5874.3144s\n",
      "\titers: 700, epoch: 2 | loss: 0.0657055\n",
      "\tspeed: 0.1155s/iter; left time: 5808.7773s\n",
      "\titers: 800, epoch: 2 | loss: 0.0677629\n",
      "\tspeed: 0.1116s/iter; left time: 5600.0752s\n",
      "\titers: 900, epoch: 2 | loss: 0.0905147\n",
      "\tspeed: 0.1160s/iter; left time: 5809.5678s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0794925\n",
      "\tspeed: 0.1163s/iter; left time: 5816.0759s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0824175\n",
      "\tspeed: 0.1157s/iter; left time: 5771.1075s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0622338\n",
      "\tspeed: 0.1155s/iter; left time: 5750.7121s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0587609\n",
      "\tspeed: 0.1158s/iter; left time: 5756.7713s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0815516\n",
      "\tspeed: 0.1061s/iter; left time: 5264.1255s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0769047\n",
      "\tspeed: 0.1152s/iter; left time: 5702.1312s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0706145\n",
      "\tspeed: 0.1171s/iter; left time: 5784.1223s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0698010\n",
      "\tspeed: 0.1177s/iter; left time: 5803.1343s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0721849\n",
      "\tspeed: 0.1164s/iter; left time: 5724.9817s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0779953\n",
      "\tspeed: 0.1141s/iter; left time: 5599.8956s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0784639\n",
      "\tspeed: 0.1146s/iter; left time: 5613.2696s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0783538\n",
      "\tspeed: 0.1032s/iter; left time: 5047.3688s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0755614\n",
      "\tspeed: 0.0966s/iter; left time: 4714.5547s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0806916\n",
      "\tspeed: 0.0969s/iter; left time: 4716.6625s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0772925\n",
      "\tspeed: 0.0968s/iter; left time: 4703.1712s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0781110\n",
      "\tspeed: 0.1082s/iter; left time: 5249.4811s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0692590\n",
      "\tspeed: 0.1173s/iter; left time: 5676.5547s\n",
      "Epoch: 2 cost time: 00h:05m:01.36s\n",
      "Epoch: 2 | Train Loss: 0.0755227 Vali Loss: 0.0783544 Test Loss: 0.0887550\n",
      "Validation loss decreased (0.079690 --> 0.078354).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0933752\n",
      "\tspeed: 0.9840s/iter; left time: 47443.7926s\n",
      "\titers: 200, epoch: 3 | loss: 0.0752771\n",
      "\tspeed: 0.1178s/iter; left time: 5667.5518s\n",
      "\titers: 300, epoch: 3 | loss: 0.0674665\n",
      "\tspeed: 0.1180s/iter; left time: 5664.5428s\n",
      "\titers: 400, epoch: 3 | loss: 0.0666938\n",
      "\tspeed: 0.1172s/iter; left time: 5616.0198s\n",
      "\titers: 500, epoch: 3 | loss: 0.0829887\n",
      "\tspeed: 0.1172s/iter; left time: 5602.3442s\n",
      "\titers: 600, epoch: 3 | loss: 0.0702578\n",
      "\tspeed: 0.1176s/iter; left time: 5609.7557s\n",
      "\titers: 700, epoch: 3 | loss: 0.0772638\n",
      "\tspeed: 0.1186s/iter; left time: 5644.9005s\n",
      "\titers: 800, epoch: 3 | loss: 0.0794854\n",
      "\tspeed: 0.1171s/iter; left time: 5566.0472s\n",
      "\titers: 900, epoch: 3 | loss: 0.0809638\n",
      "\tspeed: 0.1166s/iter; left time: 5529.4685s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0592312\n",
      "\tspeed: 0.1179s/iter; left time: 5578.2232s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0774051\n",
      "\tspeed: 0.1166s/iter; left time: 5504.3563s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0845707\n",
      "\tspeed: 0.1157s/iter; left time: 5449.2912s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0741284\n",
      "\tspeed: 0.1033s/iter; left time: 4857.6912s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0733542\n",
      "\tspeed: 0.1050s/iter; left time: 4927.1836s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0812433\n",
      "\tspeed: 0.1170s/iter; left time: 5476.8757s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0667646\n",
      "\tspeed: 0.1167s/iter; left time: 5452.4898s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0619555\n",
      "\tspeed: 0.1165s/iter; left time: 5429.4795s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0823658\n",
      "\tspeed: 0.1170s/iter; left time: 5442.6550s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0658753\n",
      "\tspeed: 0.1163s/iter; left time: 5397.0997s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0627456\n",
      "\tspeed: 0.1153s/iter; left time: 5341.7067s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0761323\n",
      "\tspeed: 0.1153s/iter; left time: 5326.9113s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0735498\n",
      "\tspeed: 0.1153s/iter; left time: 5318.7964s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0608646\n",
      "\tspeed: 0.1022s/iter; left time: 4700.6453s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0716532\n",
      "\tspeed: 0.1103s/iter; left time: 5063.1770s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0780609\n",
      "\tspeed: 0.1170s/iter; left time: 5361.0799s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0768194\n",
      "\tspeed: 0.1170s/iter; left time: 5349.9745s\n",
      "Epoch: 3 cost time: 00h:05m:09.45s\n",
      "Epoch: 3 | Train Loss: 0.0732791 Vali Loss: 0.0775053 Test Loss: 0.0882827\n",
      "Validation loss decreased (0.078354 --> 0.077505).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0618128\n",
      "\tspeed: 0.9854s/iter; left time: 44863.6708s\n",
      "\titers: 200, epoch: 4 | loss: 0.0684583\n",
      "\tspeed: 0.1171s/iter; left time: 5319.3668s\n",
      "\titers: 300, epoch: 4 | loss: 0.0878761\n",
      "\tspeed: 0.1120s/iter; left time: 5077.5005s\n",
      "\titers: 400, epoch: 4 | loss: 0.0614962\n",
      "\tspeed: 0.1100s/iter; left time: 4973.6896s\n",
      "\titers: 500, epoch: 4 | loss: 0.0650298\n",
      "\tspeed: 0.1076s/iter; left time: 4854.9516s\n",
      "\titers: 600, epoch: 4 | loss: 0.0691537\n",
      "\tspeed: 0.1162s/iter; left time: 5231.9482s\n",
      "\titers: 700, epoch: 4 | loss: 0.0632137\n",
      "\tspeed: 0.1153s/iter; left time: 5182.1700s\n",
      "\titers: 800, epoch: 4 | loss: 0.0605262\n",
      "\tspeed: 0.1166s/iter; left time: 5227.5753s\n",
      "\titers: 900, epoch: 4 | loss: 0.0776869\n",
      "\tspeed: 0.1161s/iter; left time: 5192.7091s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0974346\n",
      "\tspeed: 0.1165s/iter; left time: 5198.8521s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0625406\n",
      "\tspeed: 0.1159s/iter; left time: 5160.0367s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0563061\n",
      "\tspeed: 0.1129s/iter; left time: 5015.2611s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0730062\n",
      "\tspeed: 0.1165s/iter; left time: 5162.6243s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0758484\n",
      "\tspeed: 0.1157s/iter; left time: 5118.4553s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0684197\n",
      "\tspeed: 0.1156s/iter; left time: 5100.2411s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0800384\n",
      "\tspeed: 0.1124s/iter; left time: 4948.6238s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1039231\n",
      "\tspeed: 0.1055s/iter; left time: 4634.5882s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0638697\n",
      "\tspeed: 0.0970s/iter; left time: 4253.5934s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0696328\n",
      "\tspeed: 0.0969s/iter; left time: 4236.3141s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0607886\n",
      "\tspeed: 0.1163s/iter; left time: 5074.3592s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0725573\n",
      "\tspeed: 0.1157s/iter; left time: 5034.8533s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0679766\n",
      "\tspeed: 0.1156s/iter; left time: 5018.6808s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0680952\n",
      "\tspeed: 0.1166s/iter; left time: 5053.9386s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0643962\n",
      "\tspeed: 0.1166s/iter; left time: 5041.6817s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0806731\n",
      "\tspeed: 0.1170s/iter; left time: 5047.9906s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0733256\n",
      "\tspeed: 0.1160s/iter; left time: 4991.8655s\n",
      "Epoch: 4 cost time: 00h:05m:05.10s\n",
      "Epoch: 4 | Train Loss: 0.0717433 Vali Loss: 0.0767714 Test Loss: 0.0877820\n",
      "Validation loss decreased (0.077505 --> 0.076771).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0772906\n",
      "\tspeed: 0.9804s/iter; left time: 42004.9112s\n",
      "\titers: 200, epoch: 5 | loss: 0.0602556\n",
      "\tspeed: 0.1152s/iter; left time: 4924.2128s\n",
      "\titers: 300, epoch: 5 | loss: 0.0761829\n",
      "\tspeed: 0.1156s/iter; left time: 4931.3701s\n",
      "\titers: 400, epoch: 5 | loss: 0.0785191\n",
      "\tspeed: 0.1158s/iter; left time: 4928.3237s\n",
      "\titers: 500, epoch: 5 | loss: 0.0726331\n",
      "\tspeed: 0.1176s/iter; left time: 4990.9619s\n",
      "\titers: 600, epoch: 5 | loss: 0.0628080\n",
      "\tspeed: 0.1190s/iter; left time: 5040.7194s\n",
      "\titers: 700, epoch: 5 | loss: 0.0705963\n",
      "\tspeed: 0.1208s/iter; left time: 5104.8287s\n",
      "\titers: 800, epoch: 5 | loss: 0.0640927\n",
      "\tspeed: 0.1170s/iter; left time: 4929.9057s\n",
      "\titers: 900, epoch: 5 | loss: 0.0642261\n",
      "\tspeed: 0.1168s/iter; left time: 4909.6580s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0796294\n",
      "\tspeed: 0.1165s/iter; left time: 4886.9596s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0674907\n",
      "\tspeed: 0.1161s/iter; left time: 4857.7713s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0745679\n",
      "\tspeed: 0.1176s/iter; left time: 4909.6260s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0665957\n",
      "\tspeed: 0.1170s/iter; left time: 4871.9910s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0686471\n",
      "\tspeed: 0.1155s/iter; left time: 4799.7413s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0801993\n",
      "\tspeed: 0.1171s/iter; left time: 4851.6019s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0743748\n",
      "\tspeed: 0.1161s/iter; left time: 4800.7026s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0830029\n",
      "\tspeed: 0.1153s/iter; left time: 4756.4299s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0718037\n",
      "\tspeed: 0.1164s/iter; left time: 4789.2782s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0723873\n",
      "\tspeed: 0.1169s/iter; left time: 4797.9684s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0700455\n",
      "\tspeed: 0.1187s/iter; left time: 4861.5283s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0692627\n",
      "\tspeed: 0.1159s/iter; left time: 4734.9627s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0740308\n",
      "\tspeed: 0.1189s/iter; left time: 4842.9737s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0640778\n",
      "\tspeed: 0.1175s/iter; left time: 4774.9139s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0611654\n",
      "\tspeed: 0.1158s/iter; left time: 4693.2176s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0829783\n",
      "\tspeed: 0.1152s/iter; left time: 4660.5101s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0644178\n",
      "\tspeed: 0.1072s/iter; left time: 4322.9743s\n",
      "Epoch: 5 cost time: 00h:05m:13.02s\n",
      "Epoch: 5 | Train Loss: 0.0704540 Vali Loss: 0.0779003 Test Loss: 0.0883271\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0766604\n",
      "\tspeed: 0.9490s/iter; left time: 38111.2026s\n",
      "\titers: 200, epoch: 6 | loss: 0.0602073\n",
      "\tspeed: 0.1150s/iter; left time: 4606.6883s\n",
      "\titers: 300, epoch: 6 | loss: 0.0693253\n",
      "\tspeed: 0.1157s/iter; left time: 4624.1599s\n",
      "\titers: 400, epoch: 6 | loss: 0.0750857\n",
      "\tspeed: 0.1155s/iter; left time: 4602.2455s\n",
      "\titers: 500, epoch: 6 | loss: 0.0740420\n",
      "\tspeed: 0.1162s/iter; left time: 4620.1758s\n",
      "\titers: 600, epoch: 6 | loss: 0.0870911\n",
      "\tspeed: 0.1158s/iter; left time: 4594.5795s\n",
      "\titers: 700, epoch: 6 | loss: 0.0626083\n",
      "\tspeed: 0.1151s/iter; left time: 4552.2886s\n",
      "\titers: 800, epoch: 6 | loss: 0.0737172\n",
      "\tspeed: 0.1159s/iter; left time: 4572.6820s\n",
      "\titers: 900, epoch: 6 | loss: 0.0666808\n",
      "\tspeed: 0.1152s/iter; left time: 4532.8060s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0632041\n",
      "\tspeed: 0.1152s/iter; left time: 4522.9705s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0679926\n",
      "\tspeed: 0.1149s/iter; left time: 4497.6465s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0704225\n",
      "\tspeed: 0.1138s/iter; left time: 4444.8616s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0641174\n",
      "\tspeed: 0.1144s/iter; left time: 4458.7806s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0752409\n",
      "\tspeed: 0.0969s/iter; left time: 3766.7865s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0732217\n",
      "\tspeed: 0.1159s/iter; left time: 4490.7261s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0692174\n",
      "\tspeed: 0.1169s/iter; left time: 4518.5567s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0633075\n",
      "\tspeed: 0.1160s/iter; left time: 4473.1074s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0659616\n",
      "\tspeed: 0.1154s/iter; left time: 4438.9299s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0701341\n",
      "\tspeed: 0.1159s/iter; left time: 4445.0044s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0799484\n",
      "\tspeed: 0.1162s/iter; left time: 4445.5652s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0665025\n",
      "\tspeed: 0.1153s/iter; left time: 4401.8152s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0605779\n",
      "\tspeed: 0.1154s/iter; left time: 4390.6827s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0754763\n",
      "\tspeed: 0.1155s/iter; left time: 4382.7074s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0764293\n",
      "\tspeed: 0.1146s/iter; left time: 4338.9996s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0653570\n",
      "\tspeed: 0.1148s/iter; left time: 4333.6815s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0497287\n",
      "\tspeed: 0.1147s/iter; left time: 4318.6174s\n",
      "Epoch: 6 cost time: 00h:05m:08.43s\n",
      "Epoch: 6 | Train Loss: 0.0689933 Vali Loss: 0.0801865 Test Loss: 0.0887019\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0567738\n",
      "\tspeed: 0.9551s/iter; left time: 35796.1148s\n",
      "\titers: 200, epoch: 7 | loss: 0.0698603\n",
      "\tspeed: 0.1164s/iter; left time: 4351.4329s\n",
      "\titers: 300, epoch: 7 | loss: 0.0627555\n",
      "\tspeed: 0.1164s/iter; left time: 4337.1812s\n",
      "\titers: 400, epoch: 7 | loss: 0.0711131\n",
      "\tspeed: 0.1148s/iter; left time: 4266.3924s\n",
      "\titers: 500, epoch: 7 | loss: 0.0619935\n",
      "\tspeed: 0.1056s/iter; left time: 3914.1867s\n",
      "\titers: 600, epoch: 7 | loss: 0.0684474\n",
      "\tspeed: 0.0993s/iter; left time: 3670.1069s\n",
      "\titers: 700, epoch: 7 | loss: 0.0584849\n",
      "\tspeed: 0.0964s/iter; left time: 3556.7805s\n",
      "\titers: 800, epoch: 7 | loss: 0.0825376\n",
      "\tspeed: 0.1175s/iter; left time: 4322.6650s\n",
      "\titers: 900, epoch: 7 | loss: 0.0589024\n",
      "\tspeed: 0.1162s/iter; left time: 4260.5552s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0760413\n",
      "\tspeed: 0.1181s/iter; left time: 4321.4916s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0752413\n",
      "\tspeed: 0.1166s/iter; left time: 4251.7001s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0719468\n",
      "\tspeed: 0.1162s/iter; left time: 4228.4449s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0687664\n",
      "\tspeed: 0.1178s/iter; left time: 4271.6289s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0635155\n",
      "\tspeed: 0.1170s/iter; left time: 4234.4860s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0563175\n",
      "\tspeed: 0.1154s/iter; left time: 4163.9422s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0767602\n",
      "\tspeed: 0.1161s/iter; left time: 4177.0578s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0655388\n",
      "\tspeed: 0.1168s/iter; left time: 4188.8851s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0644029\n",
      "\tspeed: 0.1167s/iter; left time: 4175.4584s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0621075\n",
      "\tspeed: 0.1163s/iter; left time: 4150.9808s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0698945\n",
      "\tspeed: 0.1183s/iter; left time: 4208.0165s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0594934\n",
      "\tspeed: 0.1169s/iter; left time: 4147.5437s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0764240\n",
      "\tspeed: 0.1169s/iter; left time: 4134.8430s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0701658\n",
      "\tspeed: 0.1163s/iter; left time: 4101.5739s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0640838\n",
      "\tspeed: 0.1173s/iter; left time: 4126.3804s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0787112\n",
      "\tspeed: 0.1165s/iter; left time: 4086.9096s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0690667\n",
      "\tspeed: 0.1156s/iter; left time: 4042.4559s\n",
      "Epoch: 7 cost time: 00h:05m:08.77s\n",
      "Epoch: 7 | Train Loss: 0.0675281 Vali Loss: 0.0804067 Test Loss: 0.0894281\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0717882\n",
      "\tspeed: 0.9574s/iter; left time: 33309.8724s\n",
      "\titers: 200, epoch: 8 | loss: 0.0605006\n",
      "\tspeed: 0.1160s/iter; left time: 4024.5968s\n",
      "\titers: 300, epoch: 8 | loss: 0.0679652\n",
      "\tspeed: 0.1162s/iter; left time: 4019.0646s\n",
      "\titers: 400, epoch: 8 | loss: 0.0670836\n",
      "\tspeed: 0.1138s/iter; left time: 3924.8729s\n",
      "\titers: 500, epoch: 8 | loss: 0.0609015\n",
      "\tspeed: 0.1095s/iter; left time: 3766.6694s\n",
      "\titers: 600, epoch: 8 | loss: 0.0691020\n",
      "\tspeed: 0.1154s/iter; left time: 3957.7316s\n",
      "\titers: 700, epoch: 8 | loss: 0.0870408\n",
      "\tspeed: 0.1170s/iter; left time: 4000.6373s\n",
      "\titers: 800, epoch: 8 | loss: 0.0670967\n",
      "\tspeed: 0.1153s/iter; left time: 3929.4964s\n",
      "\titers: 900, epoch: 8 | loss: 0.0613804\n",
      "\tspeed: 0.1153s/iter; left time: 3917.7232s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0556819\n",
      "\tspeed: 0.1169s/iter; left time: 3960.6988s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0724371\n",
      "\tspeed: 0.1152s/iter; left time: 3891.3479s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0520738\n",
      "\tspeed: 0.1176s/iter; left time: 3961.8690s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0637756\n",
      "\tspeed: 0.1162s/iter; left time: 3902.5430s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0791539\n",
      "\tspeed: 0.1144s/iter; left time: 3830.5374s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0764657\n",
      "\tspeed: 0.1085s/iter; left time: 3622.4277s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0693158\n",
      "\tspeed: 0.1170s/iter; left time: 3894.0243s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0667949\n",
      "\tspeed: 0.1159s/iter; left time: 3847.0470s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0755049\n",
      "\tspeed: 0.1168s/iter; left time: 3864.3251s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0684051\n",
      "\tspeed: 0.1168s/iter; left time: 3853.6416s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0655407\n",
      "\tspeed: 0.1169s/iter; left time: 3845.7445s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0763611\n",
      "\tspeed: 0.1165s/iter; left time: 3819.8486s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0691977\n",
      "\tspeed: 0.1162s/iter; left time: 3800.4033s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0659143\n",
      "\tspeed: 0.1161s/iter; left time: 3784.8566s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0583210\n",
      "\tspeed: 0.0995s/iter; left time: 3233.6551s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0685454\n",
      "\tspeed: 0.1171s/iter; left time: 3794.1018s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0590881\n",
      "\tspeed: 0.1169s/iter; left time: 3773.8764s\n",
      "Epoch: 8 cost time: 00h:05m:09.16s\n",
      "Epoch: 8 | Train Loss: 0.0660281 Vali Loss: 0.0795411 Test Loss: 0.0886591\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0612876\n",
      "\tspeed: 0.9596s/iter; left time: 30810.7018s\n",
      "\titers: 200, epoch: 9 | loss: 0.0573643\n",
      "\tspeed: 0.1176s/iter; left time: 3764.3037s\n",
      "\titers: 300, epoch: 9 | loss: 0.0823771\n",
      "\tspeed: 0.1178s/iter; left time: 3757.7956s\n",
      "\titers: 400, epoch: 9 | loss: 0.0628237\n",
      "\tspeed: 0.1158s/iter; left time: 3683.7614s\n",
      "\titers: 500, epoch: 9 | loss: 0.0592355\n",
      "\tspeed: 0.1159s/iter; left time: 3674.8911s\n",
      "\titers: 600, epoch: 9 | loss: 0.0575299\n",
      "\tspeed: 0.1155s/iter; left time: 3649.7265s\n",
      "\titers: 700, epoch: 9 | loss: 0.0713902\n",
      "\tspeed: 0.1150s/iter; left time: 3623.1765s\n",
      "\titers: 800, epoch: 9 | loss: 0.0673367\n",
      "\tspeed: 0.1168s/iter; left time: 3669.7089s\n",
      "\titers: 900, epoch: 9 | loss: 0.0654938\n",
      "\tspeed: 0.1144s/iter; left time: 3582.6519s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0654262\n",
      "\tspeed: 0.1163s/iter; left time: 3629.7195s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0678178\n",
      "\tspeed: 0.1165s/iter; left time: 3623.6421s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0703463\n",
      "\tspeed: 0.1161s/iter; left time: 3598.7283s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0616599\n",
      "\tspeed: 0.1175s/iter; left time: 3632.7587s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0652685\n",
      "\tspeed: 0.1133s/iter; left time: 3491.6282s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0672777\n",
      "\tspeed: 0.1177s/iter; left time: 3614.9463s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0607611\n",
      "\tspeed: 0.1159s/iter; left time: 3546.3244s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0589829\n",
      "\tspeed: 0.1155s/iter; left time: 3522.8006s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0738688\n",
      "\tspeed: 0.1164s/iter; left time: 3538.8598s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0663319\n",
      "\tspeed: 0.1154s/iter; left time: 3498.4936s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0749816\n",
      "\tspeed: 0.1156s/iter; left time: 3492.0085s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0585454\n",
      "\tspeed: 0.1163s/iter; left time: 3501.9838s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0625599\n",
      "\tspeed: 0.1175s/iter; left time: 3525.3204s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0571969\n",
      "\tspeed: 0.1177s/iter; left time: 3519.8619s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0624960\n",
      "\tspeed: 0.1170s/iter; left time: 3486.2738s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0616413\n",
      "\tspeed: 0.1165s/iter; left time: 3459.9627s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0632547\n",
      "\tspeed: 0.1166s/iter; left time: 3451.8509s\n",
      "Epoch: 9 cost time: 00h:05m:12.61s\n",
      "Epoch: 9 | Train Loss: 0.0648369 Vali Loss: 0.0807261 Test Loss: 0.0902847\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.020460350438952446, rmse:0.14303968846797943, mae:0.08778193593025208, rse:0.5541521310806274\n",
      "success delete checkpoints\n",
      "Intermediate time for FR and pred_len 168: 00h:59m:08.07s\n",
      "\n",
      "Intermediate time for FR: 03h:49m:01.90s\n",
      "\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "train 86331\n",
      "val 18651\n",
      "test 18651\n",
      "[2024-11-02 21:01:26,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 21:01:27,921] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 21:01:27,921] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 21:01:27,921] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 21:01:28,030] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 21:01:28,030] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 21:01:28,698] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 21:01:28,700] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 21:01:28,700] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 21:01:28,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 21:01:28,702] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 21:01:28,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 21:01:28,703] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 21:01:29,038] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 21:01:29,039] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,039] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,160] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 21:01:29,161] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,161] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,161] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 21:01:29,274] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 21:01:29,275] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 21:01:29,276] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 75.4 GB, percent = 10.0%\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 21:01:29,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 21:01:29,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 21:01:29,277] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f888c84bc50>\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 21:01:29,278] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 21:01:29,279] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 21:01:29,280] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1616891\n",
      "\tspeed: 0.1710s/iter; left time: 9204.6952s\n",
      "\titers: 200, epoch: 1 | loss: 0.1988888\n",
      "\tspeed: 0.1278s/iter; left time: 6870.6330s\n",
      "\titers: 300, epoch: 1 | loss: 0.1653045\n",
      "\tspeed: 0.1282s/iter; left time: 6874.5522s\n",
      "\titers: 400, epoch: 1 | loss: 0.1196822\n",
      "\tspeed: 0.1257s/iter; left time: 6731.4162s\n",
      "\titers: 500, epoch: 1 | loss: 0.1014044\n",
      "\tspeed: 0.1252s/iter; left time: 6691.8765s\n",
      "\titers: 600, epoch: 1 | loss: 0.0925063\n",
      "\tspeed: 0.1268s/iter; left time: 6763.2208s\n",
      "\titers: 700, epoch: 1 | loss: 0.0790739\n",
      "\tspeed: 0.1249s/iter; left time: 6651.4008s\n",
      "\titers: 800, epoch: 1 | loss: 0.0929413\n",
      "\tspeed: 0.1247s/iter; left time: 6625.0595s\n",
      "\titers: 900, epoch: 1 | loss: 0.0834920\n",
      "\tspeed: 0.1238s/iter; left time: 6567.8658s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0941050\n",
      "\tspeed: 0.1239s/iter; left time: 6557.5897s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0919473\n",
      "\tspeed: 0.1250s/iter; left time: 6605.1167s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0990285\n",
      "\tspeed: 0.1270s/iter; left time: 6699.1351s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0970086\n",
      "\tspeed: 0.1124s/iter; left time: 5916.7375s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0915016\n",
      "\tspeed: 0.1241s/iter; left time: 6521.7872s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1036599\n",
      "\tspeed: 0.1248s/iter; left time: 6546.6892s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0895585\n",
      "\tspeed: 0.1241s/iter; left time: 6496.2484s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0857308\n",
      "\tspeed: 0.1225s/iter; left time: 6400.2457s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1034492\n",
      "\tspeed: 0.1259s/iter; left time: 6562.3418s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0879421\n",
      "\tspeed: 0.1243s/iter; left time: 6467.3579s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0744498\n",
      "\tspeed: 0.1255s/iter; left time: 6520.3883s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0770138\n",
      "\tspeed: 0.1273s/iter; left time: 6598.1901s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0825696\n",
      "\tspeed: 0.1246s/iter; left time: 6447.8373s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0842841\n",
      "\tspeed: 0.1255s/iter; left time: 6478.9438s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0767634\n",
      "\tspeed: 0.1241s/iter; left time: 6397.2387s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0836823\n",
      "\tspeed: 0.1252s/iter; left time: 6441.0390s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0827758\n",
      "\tspeed: 0.1258s/iter; left time: 6459.8688s\n",
      "Epoch: 1 cost time: 00h:05m:38.23s\n",
      "Epoch: 1 | Train Loss: 0.1062889 Vali Loss: 0.0663070 Test Loss: 0.0692037\n",
      "Validation loss decreased (inf --> 0.066307).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0920735\n",
      "\tspeed: 1.1417s/iter; left time: 58390.2894s\n",
      "\titers: 200, epoch: 2 | loss: 0.0877688\n",
      "\tspeed: 0.1157s/iter; left time: 5905.6337s\n",
      "\titers: 300, epoch: 2 | loss: 0.0842819\n",
      "\tspeed: 0.1162s/iter; left time: 5920.0065s\n",
      "\titers: 400, epoch: 2 | loss: 0.0832414\n",
      "\tspeed: 0.1161s/iter; left time: 5900.5147s\n",
      "\titers: 500, epoch: 2 | loss: 0.0781539\n",
      "\tspeed: 0.1150s/iter; left time: 5835.2356s\n",
      "\titers: 600, epoch: 2 | loss: 0.0809719\n",
      "\tspeed: 0.1158s/iter; left time: 5862.3119s\n",
      "\titers: 700, epoch: 2 | loss: 0.0726647\n",
      "\tspeed: 0.1163s/iter; left time: 5877.7406s\n",
      "\titers: 800, epoch: 2 | loss: 0.0951623\n",
      "\tspeed: 0.1123s/iter; left time: 5664.2424s\n",
      "\titers: 900, epoch: 2 | loss: 0.0855009\n",
      "\tspeed: 0.1155s/iter; left time: 5813.5665s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0684977\n",
      "\tspeed: 0.1161s/iter; left time: 5835.6063s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0749037\n",
      "\tspeed: 0.1147s/iter; left time: 5751.6981s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0930617\n",
      "\tspeed: 0.1146s/iter; left time: 5735.1883s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0960172\n",
      "\tspeed: 0.1165s/iter; left time: 5819.0670s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0739193\n",
      "\tspeed: 0.1167s/iter; left time: 5819.0180s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0809535\n",
      "\tspeed: 0.1167s/iter; left time: 5803.7011s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0809979\n",
      "\tspeed: 0.1146s/iter; left time: 5689.8242s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0809475\n",
      "\tspeed: 0.1144s/iter; left time: 5666.4364s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0921727\n",
      "\tspeed: 0.1093s/iter; left time: 5404.8987s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0837119\n",
      "\tspeed: 0.1000s/iter; left time: 4935.9752s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0755220\n",
      "\tspeed: 0.1160s/iter; left time: 5711.7831s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0859457\n",
      "\tspeed: 0.1161s/iter; left time: 5703.6259s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0785623\n",
      "\tspeed: 0.1155s/iter; left time: 5662.6971s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0885806\n",
      "\tspeed: 0.1166s/iter; left time: 5707.1235s\n",
      "\titers: 2400, epoch: 2 | loss: 0.0720094\n",
      "\tspeed: 0.1148s/iter; left time: 5606.4342s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0886370\n",
      "\tspeed: 0.1121s/iter; left time: 5462.7781s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0621730\n",
      "\tspeed: 0.1173s/iter; left time: 5705.9170s\n",
      "Epoch: 2 cost time: 00h:05m:09.66s\n",
      "Epoch: 2 | Train Loss: 0.0804989 Vali Loss: 0.0635860 Test Loss: 0.0664676\n",
      "Validation loss decreased (0.066307 --> 0.063586).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0694548\n",
      "\tspeed: 0.9960s/iter; left time: 48251.9254s\n",
      "\titers: 200, epoch: 3 | loss: 0.0808334\n",
      "\tspeed: 0.1149s/iter; left time: 5554.1130s\n",
      "\titers: 300, epoch: 3 | loss: 0.0840988\n",
      "\tspeed: 0.1169s/iter; left time: 5641.2641s\n",
      "\titers: 400, epoch: 3 | loss: 0.0842470\n",
      "\tspeed: 0.1125s/iter; left time: 5417.6274s\n",
      "\titers: 500, epoch: 3 | loss: 0.0646141\n",
      "\tspeed: 0.1144s/iter; left time: 5495.3484s\n",
      "\titers: 600, epoch: 3 | loss: 0.0703670\n",
      "\tspeed: 0.1158s/iter; left time: 5551.9364s\n",
      "\titers: 700, epoch: 3 | loss: 0.1009109\n",
      "\tspeed: 0.1150s/iter; left time: 5502.4580s\n",
      "\titers: 800, epoch: 3 | loss: 0.0748487\n",
      "\tspeed: 0.1155s/iter; left time: 5516.2524s\n",
      "\titers: 900, epoch: 3 | loss: 0.0886928\n",
      "\tspeed: 0.1145s/iter; left time: 5457.6055s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0754743\n",
      "\tspeed: 0.1160s/iter; left time: 5516.3033s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0920176\n",
      "\tspeed: 0.1142s/iter; left time: 5417.5835s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0903100\n",
      "\tspeed: 0.1093s/iter; left time: 5174.3028s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0697832\n",
      "\tspeed: 0.1165s/iter; left time: 5504.6654s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0736806\n",
      "\tspeed: 0.1159s/iter; left time: 5462.9624s\n",
      "\titers: 1500, epoch: 3 | loss: 0.0663067\n",
      "\tspeed: 0.1161s/iter; left time: 5463.4055s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0637103\n",
      "\tspeed: 0.1162s/iter; left time: 5455.8326s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0779963\n",
      "\tspeed: 0.1161s/iter; left time: 5441.1227s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0827028\n",
      "\tspeed: 0.1179s/iter; left time: 5512.7869s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1057388\n",
      "\tspeed: 0.1165s/iter; left time: 5433.2936s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0641804\n",
      "\tspeed: 0.1168s/iter; left time: 5436.3407s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0772232\n",
      "\tspeed: 0.1172s/iter; left time: 5444.7137s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0680376\n",
      "\tspeed: 0.1185s/iter; left time: 5490.3259s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0713496\n",
      "\tspeed: 0.1163s/iter; left time: 5376.3683s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0559037\n",
      "\tspeed: 0.1159s/iter; left time: 5349.9769s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0826620\n",
      "\tspeed: 0.1179s/iter; left time: 5427.1389s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0766963\n",
      "\tspeed: 0.1165s/iter; left time: 5351.9525s\n",
      "Epoch: 3 cost time: 00h:05m:12.54s\n",
      "Epoch: 3 | Train Loss: 0.0764710 Vali Loss: 0.0630326 Test Loss: 0.0662999\n",
      "Validation loss decreased (0.063586 --> 0.063033).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0755529\n",
      "\tspeed: 1.0027s/iter; left time: 45872.9982s\n",
      "\titers: 200, epoch: 4 | loss: 0.0685682\n",
      "\tspeed: 0.1149s/iter; left time: 5243.1481s\n",
      "\titers: 300, epoch: 4 | loss: 0.0985739\n",
      "\tspeed: 0.1156s/iter; left time: 5266.3793s\n",
      "\titers: 400, epoch: 4 | loss: 0.0769638\n",
      "\tspeed: 0.1143s/iter; left time: 5194.2122s\n",
      "\titers: 500, epoch: 4 | loss: 0.0797975\n",
      "\tspeed: 0.1152s/iter; left time: 5224.8052s\n",
      "\titers: 600, epoch: 4 | loss: 0.0556481\n",
      "\tspeed: 0.1143s/iter; left time: 5171.0541s\n",
      "\titers: 700, epoch: 4 | loss: 0.0617126\n",
      "\tspeed: 0.1145s/iter; left time: 5170.3960s\n",
      "\titers: 800, epoch: 4 | loss: 0.0742233\n",
      "\tspeed: 0.1145s/iter; left time: 5157.4712s\n",
      "\titers: 900, epoch: 4 | loss: 0.0639015\n",
      "\tspeed: 0.1143s/iter; left time: 5135.6826s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1003068\n",
      "\tspeed: 0.1155s/iter; left time: 5180.0391s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0659704\n",
      "\tspeed: 0.1157s/iter; left time: 5176.3203s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0864620\n",
      "\tspeed: 0.1156s/iter; left time: 5162.7542s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0829317\n",
      "\tspeed: 0.1178s/iter; left time: 5249.3917s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0683253\n",
      "\tspeed: 0.1154s/iter; left time: 5129.9348s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0755579\n",
      "\tspeed: 0.1159s/iter; left time: 5142.2098s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0791848\n",
      "\tspeed: 0.1155s/iter; left time: 5112.1593s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0764505\n",
      "\tspeed: 0.1153s/iter; left time: 5089.1324s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0608163\n",
      "\tspeed: 0.1147s/iter; left time: 5050.6747s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0749248\n",
      "\tspeed: 0.1144s/iter; left time: 5028.3983s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0778898\n",
      "\tspeed: 0.1146s/iter; left time: 5025.2626s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0662812\n",
      "\tspeed: 0.1173s/iter; left time: 5131.2864s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0748827\n",
      "\tspeed: 0.1107s/iter; left time: 4833.3044s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0675273\n",
      "\tspeed: 0.0966s/iter; left time: 4208.4745s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0791746\n",
      "\tspeed: 0.1056s/iter; left time: 4589.5804s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0636560\n",
      "\tspeed: 0.1145s/iter; left time: 4965.6911s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0793808\n",
      "\tspeed: 0.1169s/iter; left time: 5055.9000s\n",
      "Epoch: 4 cost time: 00h:05m:08.24s\n",
      "Epoch: 4 | Train Loss: 0.0742877 Vali Loss: 0.0603242 Test Loss: 0.0631236\n",
      "Validation loss decreased (0.063033 --> 0.060324).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0666447\n",
      "\tspeed: 1.0072s/iter; left time: 43364.4964s\n",
      "\titers: 200, epoch: 5 | loss: 0.0850932\n",
      "\tspeed: 0.1146s/iter; left time: 4921.1163s\n",
      "\titers: 300, epoch: 5 | loss: 0.0762963\n",
      "\tspeed: 0.1154s/iter; left time: 4945.3224s\n",
      "\titers: 400, epoch: 5 | loss: 0.0729807\n",
      "\tspeed: 0.1155s/iter; left time: 4938.5099s\n",
      "\titers: 500, epoch: 5 | loss: 0.0696062\n",
      "\tspeed: 0.1161s/iter; left time: 4951.8754s\n",
      "\titers: 600, epoch: 5 | loss: 0.0712013\n",
      "\tspeed: 0.1155s/iter; left time: 4915.4375s\n",
      "\titers: 700, epoch: 5 | loss: 0.0854386\n",
      "\tspeed: 0.1150s/iter; left time: 4882.3641s\n",
      "\titers: 800, epoch: 5 | loss: 0.0720558\n",
      "\tspeed: 0.1146s/iter; left time: 4853.2534s\n",
      "\titers: 900, epoch: 5 | loss: 0.0693585\n",
      "\tspeed: 0.1140s/iter; left time: 4815.9188s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0752329\n",
      "\tspeed: 0.1149s/iter; left time: 4842.3688s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0633822\n",
      "\tspeed: 0.1150s/iter; left time: 4834.9946s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0723745\n",
      "\tspeed: 0.1152s/iter; left time: 4834.8618s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0627828\n",
      "\tspeed: 0.1158s/iter; left time: 4847.3634s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0818627\n",
      "\tspeed: 0.1171s/iter; left time: 4887.8201s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0792903\n",
      "\tspeed: 0.1173s/iter; left time: 4885.9770s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0754565\n",
      "\tspeed: 0.1153s/iter; left time: 4792.2546s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0597070\n",
      "\tspeed: 0.1140s/iter; left time: 4723.8120s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0933210\n",
      "\tspeed: 0.1154s/iter; left time: 4770.6114s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0578079\n",
      "\tspeed: 0.1153s/iter; left time: 4757.5930s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0638854\n",
      "\tspeed: 0.1153s/iter; left time: 4746.6100s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0707066\n",
      "\tspeed: 0.1152s/iter; left time: 4729.5655s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0822564\n",
      "\tspeed: 0.1157s/iter; left time: 4737.3070s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0747746\n",
      "\tspeed: 0.1167s/iter; left time: 4767.0517s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0669004\n",
      "\tspeed: 0.1163s/iter; left time: 4738.6961s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0690131\n",
      "\tspeed: 0.1156s/iter; left time: 4698.4356s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0732940\n",
      "\tspeed: 0.1160s/iter; left time: 4702.6721s\n",
      "Epoch: 5 cost time: 00h:05m:11.60s\n",
      "Epoch: 5 | Train Loss: 0.0726861 Vali Loss: 0.0594335 Test Loss: 0.0628666\n",
      "Validation loss decreased (0.060324 --> 0.059433).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0712747\n",
      "\tspeed: 1.0098s/iter; left time: 40751.4645s\n",
      "\titers: 200, epoch: 6 | loss: 0.0677786\n",
      "\tspeed: 0.1160s/iter; left time: 4669.5459s\n",
      "\titers: 300, epoch: 6 | loss: 0.0659821\n",
      "\tspeed: 0.1129s/iter; left time: 4531.9930s\n",
      "\titers: 400, epoch: 6 | loss: 0.0641622\n",
      "\tspeed: 0.0966s/iter; left time: 3869.5682s\n",
      "\titers: 500, epoch: 6 | loss: 0.0645634\n",
      "\tspeed: 0.1138s/iter; left time: 4546.2729s\n",
      "\titers: 600, epoch: 6 | loss: 0.0953365\n",
      "\tspeed: 0.1147s/iter; left time: 4570.0293s\n",
      "\titers: 700, epoch: 6 | loss: 0.0762194\n",
      "\tspeed: 0.1156s/iter; left time: 4594.3468s\n",
      "\titers: 800, epoch: 6 | loss: 0.0763267\n",
      "\tspeed: 0.1150s/iter; left time: 4558.6683s\n",
      "\titers: 900, epoch: 6 | loss: 0.0695393\n",
      "\tspeed: 0.1166s/iter; left time: 4611.9340s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0557512\n",
      "\tspeed: 0.1150s/iter; left time: 4536.4202s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0601773\n",
      "\tspeed: 0.1157s/iter; left time: 4553.5366s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0580391\n",
      "\tspeed: 0.1148s/iter; left time: 4507.3691s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0789730\n",
      "\tspeed: 0.1146s/iter; left time: 4488.8459s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0700168\n",
      "\tspeed: 0.1151s/iter; left time: 4494.8010s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0772208\n",
      "\tspeed: 0.1163s/iter; left time: 4528.7595s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0704373\n",
      "\tspeed: 0.1179s/iter; left time: 4581.1736s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0689978\n",
      "\tspeed: 0.1154s/iter; left time: 4473.1068s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0662989\n",
      "\tspeed: 0.1172s/iter; left time: 4529.5006s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0793340\n",
      "\tspeed: 0.1157s/iter; left time: 4459.1564s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0628580\n",
      "\tspeed: 0.1159s/iter; left time: 4458.5505s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0749498\n",
      "\tspeed: 0.1148s/iter; left time: 4402.8768s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0798656\n",
      "\tspeed: 0.1170s/iter; left time: 4474.1263s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0600675\n",
      "\tspeed: 0.1153s/iter; left time: 4398.9489s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0627799\n",
      "\tspeed: 0.1141s/iter; left time: 4343.0951s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0818583\n",
      "\tspeed: 0.1154s/iter; left time: 4378.3304s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0600302\n",
      "\tspeed: 0.1018s/iter; left time: 3852.0382s\n",
      "Epoch: 6 cost time: 00h:05m:06.58s\n",
      "Epoch: 6 | Train Loss: 0.0715059 Vali Loss: 0.0587914 Test Loss: 0.0618361\n",
      "Validation loss decreased (0.059433 --> 0.058791).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0741726\n",
      "\tspeed: 0.9898s/iter; left time: 37275.9424s\n",
      "\titers: 200, epoch: 7 | loss: 0.0748587\n",
      "\tspeed: 0.1156s/iter; left time: 4340.0231s\n",
      "\titers: 300, epoch: 7 | loss: 0.0780735\n",
      "\tspeed: 0.1167s/iter; left time: 4372.8185s\n",
      "\titers: 400, epoch: 7 | loss: 0.0651705\n",
      "\tspeed: 0.1146s/iter; left time: 4280.4686s\n",
      "\titers: 500, epoch: 7 | loss: 0.0722456\n",
      "\tspeed: 0.1109s/iter; left time: 4132.7038s\n",
      "\titers: 600, epoch: 7 | loss: 0.0781499\n",
      "\tspeed: 0.1054s/iter; left time: 3915.9669s\n",
      "\titers: 700, epoch: 7 | loss: 0.0795180\n",
      "\tspeed: 0.0999s/iter; left time: 3702.2645s\n",
      "\titers: 800, epoch: 7 | loss: 0.0717324\n",
      "\tspeed: 0.1112s/iter; left time: 4108.1203s\n",
      "\titers: 900, epoch: 7 | loss: 0.0713158\n",
      "\tspeed: 0.1148s/iter; left time: 4230.6673s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0723086\n",
      "\tspeed: 0.1154s/iter; left time: 4243.1740s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0738894\n",
      "\tspeed: 0.1151s/iter; left time: 4219.7402s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0729173\n",
      "\tspeed: 0.1155s/iter; left time: 4222.6904s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0678999\n",
      "\tspeed: 0.1167s/iter; left time: 4256.3744s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0747445\n",
      "\tspeed: 0.1152s/iter; left time: 4187.8987s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0734894\n",
      "\tspeed: 0.1132s/iter; left time: 4106.2845s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0665492\n",
      "\tspeed: 0.1158s/iter; left time: 4188.5237s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0577601\n",
      "\tspeed: 0.1139s/iter; left time: 4106.2015s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0670582\n",
      "\tspeed: 0.1161s/iter; left time: 4174.8886s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0767235\n",
      "\tspeed: 0.1158s/iter; left time: 4152.4629s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0678485\n",
      "\tspeed: 0.1166s/iter; left time: 4168.5701s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0605740\n",
      "\tspeed: 0.1173s/iter; left time: 4181.9423s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0644402\n",
      "\tspeed: 0.1171s/iter; left time: 4162.1812s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0643342\n",
      "\tspeed: 0.1157s/iter; left time: 4101.2533s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0800190\n",
      "\tspeed: 0.1146s/iter; left time: 4051.4881s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0799797\n",
      "\tspeed: 0.1153s/iter; left time: 4064.3580s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0731578\n",
      "\tspeed: 0.1155s/iter; left time: 4060.5084s\n",
      "Epoch: 7 cost time: 00h:05m:08.92s\n",
      "Epoch: 7 | Train Loss: 0.0705104 Vali Loss: 0.0588274 Test Loss: 0.0623315\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0723313\n",
      "\tspeed: 0.9858s/iter; left time: 34464.8020s\n",
      "\titers: 200, epoch: 8 | loss: 0.0710911\n",
      "\tspeed: 0.1143s/iter; left time: 3983.3980s\n",
      "\titers: 300, epoch: 8 | loss: 0.0671540\n",
      "\tspeed: 0.1150s/iter; left time: 3998.1304s\n",
      "\titers: 400, epoch: 8 | loss: 0.0813148\n",
      "\tspeed: 0.1145s/iter; left time: 3969.5058s\n",
      "\titers: 500, epoch: 8 | loss: 0.0627014\n",
      "\tspeed: 0.1151s/iter; left time: 3978.6193s\n",
      "\titers: 600, epoch: 8 | loss: 0.0611403\n",
      "\tspeed: 0.1141s/iter; left time: 3931.4380s\n",
      "\titers: 700, epoch: 8 | loss: 0.0833048\n",
      "\tspeed: 0.1146s/iter; left time: 3939.2204s\n",
      "\titers: 800, epoch: 8 | loss: 0.0658636\n",
      "\tspeed: 0.1141s/iter; left time: 3908.3466s\n",
      "\titers: 900, epoch: 8 | loss: 0.0716917\n",
      "\tspeed: 0.1140s/iter; left time: 3895.7594s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0793457\n",
      "\tspeed: 0.1154s/iter; left time: 3929.5973s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0693654\n",
      "\tspeed: 0.1138s/iter; left time: 3863.4662s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0896589\n",
      "\tspeed: 0.1143s/iter; left time: 3869.8106s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0923285\n",
      "\tspeed: 0.1043s/iter; left time: 3520.1031s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0691433\n",
      "\tspeed: 0.1152s/iter; left time: 3876.5073s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0635347\n",
      "\tspeed: 0.1170s/iter; left time: 3926.9469s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0756910\n",
      "\tspeed: 0.1167s/iter; left time: 3903.9346s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0683165\n",
      "\tspeed: 0.1173s/iter; left time: 3912.8210s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0640305\n",
      "\tspeed: 0.1116s/iter; left time: 3713.2979s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0605875\n",
      "\tspeed: 0.1162s/iter; left time: 3853.8823s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0707027\n",
      "\tspeed: 0.1100s/iter; left time: 3637.5042s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0703436\n",
      "\tspeed: 0.1113s/iter; left time: 3668.6883s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0644438\n",
      "\tspeed: 0.1153s/iter; left time: 3788.0077s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0740883\n",
      "\tspeed: 0.1120s/iter; left time: 3668.1759s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0688714\n",
      "\tspeed: 0.1159s/iter; left time: 3786.6674s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0841499\n",
      "\tspeed: 0.1163s/iter; left time: 3787.9676s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0752901\n",
      "\tspeed: 0.1151s/iter; left time: 3736.7459s\n",
      "Epoch: 8 cost time: 00h:05m:08.57s\n",
      "Epoch: 8 | Train Loss: 0.0698472 Vali Loss: 0.0581357 Test Loss: 0.0614739\n",
      "Validation loss decreased (0.058791 --> 0.058136).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0787969\n",
      "\tspeed: 1.0198s/iter; left time: 32905.3798s\n",
      "\titers: 200, epoch: 9 | loss: 0.0585354\n",
      "\tspeed: 0.1158s/iter; left time: 3726.0159s\n",
      "\titers: 300, epoch: 9 | loss: 0.0858474\n",
      "\tspeed: 0.1160s/iter; left time: 3720.9209s\n",
      "\titers: 400, epoch: 9 | loss: 0.0597564\n",
      "\tspeed: 0.1168s/iter; left time: 3732.4743s\n",
      "\titers: 500, epoch: 9 | loss: 0.0880816\n",
      "\tspeed: 0.1147s/iter; left time: 3656.4618s\n",
      "\titers: 600, epoch: 9 | loss: 0.0697973\n",
      "\tspeed: 0.1143s/iter; left time: 3629.9317s\n",
      "\titers: 700, epoch: 9 | loss: 0.0531395\n",
      "\tspeed: 0.1137s/iter; left time: 3601.7995s\n",
      "\titers: 800, epoch: 9 | loss: 0.0829247\n",
      "\tspeed: 0.1131s/iter; left time: 3571.4335s\n",
      "\titers: 900, epoch: 9 | loss: 0.0627392\n",
      "\tspeed: 0.1139s/iter; left time: 3583.4731s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0607399\n",
      "\tspeed: 0.1139s/iter; left time: 3573.6254s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0579930\n",
      "\tspeed: 0.1143s/iter; left time: 3573.2211s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0707458\n",
      "\tspeed: 0.1140s/iter; left time: 3551.6134s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0769623\n",
      "\tspeed: 0.0971s/iter; left time: 3015.7709s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0854174\n",
      "\tspeed: 0.0980s/iter; left time: 3033.3488s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0642810\n",
      "\tspeed: 0.1155s/iter; left time: 3565.7086s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0589194\n",
      "\tspeed: 0.1082s/iter; left time: 3329.5130s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0693697\n",
      "\tspeed: 0.1096s/iter; left time: 3360.7651s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0666921\n",
      "\tspeed: 0.1132s/iter; left time: 3459.6487s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0715291\n",
      "\tspeed: 0.1159s/iter; left time: 3529.5364s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0702265\n",
      "\tspeed: 0.1149s/iter; left time: 3490.3398s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0707251\n",
      "\tspeed: 0.1154s/iter; left time: 3493.2598s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0606815\n",
      "\tspeed: 0.1157s/iter; left time: 3490.7647s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0693892\n",
      "\tspeed: 0.1145s/iter; left time: 3441.3629s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0803939\n",
      "\tspeed: 0.1162s/iter; left time: 3481.7337s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0663976\n",
      "\tspeed: 0.1151s/iter; left time: 3438.7551s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0611548\n",
      "\tspeed: 0.1135s/iter; left time: 3377.7964s\n",
      "Epoch: 9 cost time: 00h:05m:05.59s\n",
      "Epoch: 9 | Train Loss: 0.0691710 Vali Loss: 0.0575196 Test Loss: 0.0609406\n",
      "Validation loss decreased (0.058136 --> 0.057520).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0594188\n",
      "\tspeed: 0.9952s/iter; left time: 29425.7740s\n",
      "\titers: 200, epoch: 10 | loss: 0.0590801\n",
      "\tspeed: 0.1014s/iter; left time: 2989.4648s\n",
      "\titers: 300, epoch: 10 | loss: 0.0680371\n",
      "\tspeed: 0.1140s/iter; left time: 3347.5216s\n",
      "\titers: 400, epoch: 10 | loss: 0.0673472\n",
      "\tspeed: 0.1087s/iter; left time: 3181.0288s\n",
      "\titers: 500, epoch: 10 | loss: 0.0599458\n",
      "\tspeed: 0.1069s/iter; left time: 3117.6651s\n",
      "\titers: 600, epoch: 10 | loss: 0.0861257\n",
      "\tspeed: 0.1146s/iter; left time: 3330.4469s\n",
      "\titers: 700, epoch: 10 | loss: 0.0876099\n",
      "\tspeed: 0.1142s/iter; left time: 3309.2607s\n",
      "\titers: 800, epoch: 10 | loss: 0.0699282\n",
      "\tspeed: 0.1138s/iter; left time: 3285.3269s\n",
      "\titers: 900, epoch: 10 | loss: 0.0586489\n",
      "\tspeed: 0.1147s/iter; left time: 3300.8723s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0759529\n",
      "\tspeed: 0.1148s/iter; left time: 3290.2202s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0668376\n",
      "\tspeed: 0.1092s/iter; left time: 3119.7124s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0535884\n",
      "\tspeed: 0.1031s/iter; left time: 2934.3964s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0582658\n",
      "\tspeed: 0.1138s/iter; left time: 3227.0837s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0513382\n",
      "\tspeed: 0.1130s/iter; left time: 3193.3981s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0559071\n",
      "\tspeed: 0.1137s/iter; left time: 3202.5214s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0776514\n",
      "\tspeed: 0.1151s/iter; left time: 3229.5595s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0712357\n",
      "\tspeed: 0.1041s/iter; left time: 2912.5204s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0629981\n",
      "\tspeed: 0.1156s/iter; left time: 3220.4616s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0640645\n",
      "\tspeed: 0.1089s/iter; left time: 3024.4091s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0740883\n",
      "\tspeed: 0.1007s/iter; left time: 2785.6687s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0667572\n",
      "\tspeed: 0.1143s/iter; left time: 3150.6125s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0742129\n",
      "\tspeed: 0.1027s/iter; left time: 2820.5033s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0638777\n",
      "\tspeed: 0.1020s/iter; left time: 2791.8738s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0583167\n",
      "\tspeed: 0.1136s/iter; left time: 3098.3799s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0623477\n",
      "\tspeed: 0.1136s/iter; left time: 3085.3174s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0761587\n",
      "\tspeed: 0.1009s/iter; left time: 2731.3328s\n",
      "Epoch: 10 cost time: 00h:04m:55.09s\n",
      "Epoch: 10 | Train Loss: 0.0686641 Vali Loss: 0.0578670 Test Loss: 0.0614497\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 11 | loss: 0.0688605\n",
      "\tspeed: 0.9634s/iter; left time: 25886.5138s\n",
      "\titers: 200, epoch: 11 | loss: 0.0643643\n",
      "\tspeed: 0.1157s/iter; left time: 3096.3717s\n",
      "\titers: 300, epoch: 11 | loss: 0.0519321\n",
      "\tspeed: 0.1139s/iter; left time: 3038.8410s\n",
      "\titers: 400, epoch: 11 | loss: 0.0803871\n",
      "\tspeed: 0.1149s/iter; left time: 3052.1080s\n",
      "\titers: 500, epoch: 11 | loss: 0.0732333\n",
      "\tspeed: 0.1146s/iter; left time: 3033.9379s\n",
      "\titers: 600, epoch: 11 | loss: 0.0699096\n",
      "\tspeed: 0.1143s/iter; left time: 3015.4589s\n",
      "\titers: 700, epoch: 11 | loss: 0.0627109\n",
      "\tspeed: 0.1135s/iter; left time: 2981.7984s\n",
      "\titers: 800, epoch: 11 | loss: 0.0755273\n",
      "\tspeed: 0.1133s/iter; left time: 2965.6401s\n",
      "\titers: 900, epoch: 11 | loss: 0.0590502\n",
      "\tspeed: 0.1141s/iter; left time: 2974.0376s\n",
      "\titers: 1000, epoch: 11 | loss: 0.0684834\n",
      "\tspeed: 0.0993s/iter; left time: 2578.6103s\n",
      "\titers: 1100, epoch: 11 | loss: 0.0669859\n",
      "\tspeed: 0.1155s/iter; left time: 2988.3733s\n",
      "\titers: 1200, epoch: 11 | loss: 0.0700008\n",
      "\tspeed: 0.1077s/iter; left time: 2775.5692s\n",
      "\titers: 1300, epoch: 11 | loss: 0.0586604\n",
      "\tspeed: 0.1146s/iter; left time: 2941.7344s\n",
      "\titers: 1400, epoch: 11 | loss: 0.0698778\n",
      "\tspeed: 0.1154s/iter; left time: 2951.8426s\n",
      "\titers: 1500, epoch: 11 | loss: 0.0758755\n",
      "\tspeed: 0.1156s/iter; left time: 2944.7544s\n",
      "\titers: 1600, epoch: 11 | loss: 0.0635368\n",
      "\tspeed: 0.1119s/iter; left time: 2837.8673s\n",
      "\titers: 1700, epoch: 11 | loss: 0.0804984\n",
      "\tspeed: 0.1100s/iter; left time: 2778.9005s\n",
      "\titers: 1800, epoch: 11 | loss: 0.0605070\n",
      "\tspeed: 0.1144s/iter; left time: 2878.6505s\n",
      "\titers: 1900, epoch: 11 | loss: 0.0770305\n",
      "\tspeed: 0.1148s/iter; left time: 2877.3141s\n",
      "\titers: 2000, epoch: 11 | loss: 0.0658592\n",
      "\tspeed: 0.1150s/iter; left time: 2870.8323s\n",
      "\titers: 2100, epoch: 11 | loss: 0.0650982\n",
      "\tspeed: 0.1151s/iter; left time: 2863.3911s\n",
      "\titers: 2200, epoch: 11 | loss: 0.0749165\n",
      "\tspeed: 0.1144s/iter; left time: 2832.8202s\n",
      "\titers: 2300, epoch: 11 | loss: 0.0622882\n",
      "\tspeed: 0.1140s/iter; left time: 2812.4483s\n",
      "\titers: 2400, epoch: 11 | loss: 0.0714804\n",
      "\tspeed: 0.1148s/iter; left time: 2820.8064s\n",
      "\titers: 2500, epoch: 11 | loss: 0.0614139\n",
      "\tspeed: 0.1150s/iter; left time: 2814.2477s\n",
      "\titers: 2600, epoch: 11 | loss: 0.0660085\n",
      "\tspeed: 0.1137s/iter; left time: 2769.7957s\n",
      "Epoch: 11 cost time: 00h:05m:06.71s\n",
      "Epoch: 11 | Train Loss: 0.0681724 Vali Loss: 0.0571862 Test Loss: 0.0607081\n",
      "Validation loss decreased (0.057520 --> 0.057186).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 12 | loss: 0.0519414\n",
      "\tspeed: 1.0039s/iter; left time: 24267.3512s\n",
      "\titers: 200, epoch: 12 | loss: 0.0603286\n",
      "\tspeed: 0.1136s/iter; left time: 2735.6495s\n",
      "\titers: 300, epoch: 12 | loss: 0.0825475\n",
      "\tspeed: 0.1147s/iter; left time: 2749.4585s\n",
      "\titers: 400, epoch: 12 | loss: 0.0733654\n",
      "\tspeed: 0.1163s/iter; left time: 2775.5488s\n",
      "\titers: 500, epoch: 12 | loss: 0.0657127\n",
      "\tspeed: 0.1162s/iter; left time: 2761.8917s\n",
      "\titers: 600, epoch: 12 | loss: 0.0646338\n",
      "\tspeed: 0.1151s/iter; left time: 2723.8446s\n",
      "\titers: 700, epoch: 12 | loss: 0.0897408\n",
      "\tspeed: 0.1142s/iter; left time: 2690.9921s\n",
      "\titers: 800, epoch: 12 | loss: 0.0609364\n",
      "\tspeed: 0.1145s/iter; left time: 2687.2109s\n",
      "\titers: 900, epoch: 12 | loss: 0.0622800\n",
      "\tspeed: 0.1146s/iter; left time: 2678.0021s\n",
      "\titers: 1000, epoch: 12 | loss: 0.0894761\n",
      "\tspeed: 0.1137s/iter; left time: 2645.4986s\n",
      "\titers: 1100, epoch: 12 | loss: 0.0589442\n",
      "\tspeed: 0.1055s/iter; left time: 2443.9838s\n",
      "\titers: 1200, epoch: 12 | loss: 0.0551907\n",
      "\tspeed: 0.0961s/iter; left time: 2216.2643s\n",
      "\titers: 1300, epoch: 12 | loss: 0.0603157\n",
      "\tspeed: 0.1038s/iter; left time: 2384.6036s\n",
      "\titers: 1400, epoch: 12 | loss: 0.0810170\n",
      "\tspeed: 0.1144s/iter; left time: 2617.3443s\n",
      "\titers: 1500, epoch: 12 | loss: 0.0754087\n",
      "\tspeed: 0.1142s/iter; left time: 2600.1290s\n",
      "\titers: 1600, epoch: 12 | loss: 0.0668139\n",
      "\tspeed: 0.1132s/iter; left time: 2567.3034s\n",
      "\titers: 1700, epoch: 12 | loss: 0.0780367\n",
      "\tspeed: 0.1115s/iter; left time: 2515.9899s\n",
      "\titers: 1800, epoch: 12 | loss: 0.0738580\n",
      "\tspeed: 0.1070s/iter; left time: 2405.6110s\n",
      "\titers: 1900, epoch: 12 | loss: 0.0685480\n",
      "\tspeed: 0.1146s/iter; left time: 2564.0579s\n",
      "\titers: 2000, epoch: 12 | loss: 0.0631595\n",
      "\tspeed: 0.1140s/iter; left time: 2539.9188s\n",
      "\titers: 2100, epoch: 12 | loss: 0.0612001\n",
      "\tspeed: 0.1143s/iter; left time: 2535.3791s\n",
      "\titers: 2200, epoch: 12 | loss: 0.0731911\n",
      "\tspeed: 0.1143s/iter; left time: 2523.8004s\n",
      "\titers: 2300, epoch: 12 | loss: 0.0563691\n",
      "\tspeed: 0.1154s/iter; left time: 2536.1860s\n",
      "\titers: 2400, epoch: 12 | loss: 0.0569280\n",
      "\tspeed: 0.1144s/iter; left time: 2503.0689s\n",
      "\titers: 2500, epoch: 12 | loss: 0.0736074\n",
      "\tspeed: 0.0999s/iter; left time: 2174.2873s\n",
      "\titers: 2600, epoch: 12 | loss: 0.0686894\n",
      "\tspeed: 0.1084s/iter; left time: 2348.5141s\n",
      "Epoch: 12 cost time: 00h:05m:02.47s\n",
      "Epoch: 12 | Train Loss: 0.0676054 Vali Loss: 0.0583766 Test Loss: 0.0617521\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 13 | loss: 0.0610672\n",
      "\tspeed: 0.9819s/iter; left time: 21088.5091s\n",
      "\titers: 200, epoch: 13 | loss: 0.0567970\n",
      "\tspeed: 0.1160s/iter; left time: 2479.0005s\n",
      "\titers: 300, epoch: 13 | loss: 0.0503390\n",
      "\tspeed: 0.1150s/iter; left time: 2447.6480s\n",
      "\titers: 400, epoch: 13 | loss: 0.0588858\n",
      "\tspeed: 0.1144s/iter; left time: 2422.5092s\n",
      "\titers: 500, epoch: 13 | loss: 0.0866880\n",
      "\tspeed: 0.1142s/iter; left time: 2407.5235s\n",
      "\titers: 600, epoch: 13 | loss: 0.0590931\n",
      "\tspeed: 0.1146s/iter; left time: 2403.5746s\n",
      "\titers: 700, epoch: 13 | loss: 0.0699925\n",
      "\tspeed: 0.1131s/iter; left time: 2362.1300s\n",
      "\titers: 800, epoch: 13 | loss: 0.0752468\n",
      "\tspeed: 0.1143s/iter; left time: 2373.9407s\n",
      "\titers: 900, epoch: 13 | loss: 0.0647457\n",
      "\tspeed: 0.1146s/iter; left time: 2369.8364s\n",
      "\titers: 1000, epoch: 13 | loss: 0.0609347\n",
      "\tspeed: 0.1144s/iter; left time: 2354.7522s\n",
      "\titers: 1100, epoch: 13 | loss: 0.0517241\n",
      "\tspeed: 0.1137s/iter; left time: 2328.9166s\n",
      "\titers: 1200, epoch: 13 | loss: 0.0599805\n",
      "\tspeed: 0.1142s/iter; left time: 2327.9382s\n",
      "\titers: 1300, epoch: 13 | loss: 0.0654597\n",
      "\tspeed: 0.1142s/iter; left time: 2314.9592s\n",
      "\titers: 1400, epoch: 13 | loss: 0.0604756\n",
      "\tspeed: 0.1142s/iter; left time: 2303.4778s\n",
      "\titers: 1500, epoch: 13 | loss: 0.0713967\n",
      "\tspeed: 0.1155s/iter; left time: 2319.1291s\n",
      "\titers: 1600, epoch: 13 | loss: 0.0787557\n",
      "\tspeed: 0.1158s/iter; left time: 2313.8345s\n",
      "\titers: 1700, epoch: 13 | loss: 0.0548762\n",
      "\tspeed: 0.1150s/iter; left time: 2285.9566s\n",
      "\titers: 1800, epoch: 13 | loss: 0.0690363\n",
      "\tspeed: 0.1153s/iter; left time: 2279.3040s\n",
      "\titers: 1900, epoch: 13 | loss: 0.0558236\n",
      "\tspeed: 0.1152s/iter; left time: 2265.9576s\n",
      "\titers: 2000, epoch: 13 | loss: 0.0533169\n",
      "\tspeed: 0.1135s/iter; left time: 2221.9672s\n",
      "\titers: 2100, epoch: 13 | loss: 0.0763264\n",
      "\tspeed: 0.1144s/iter; left time: 2228.5516s\n",
      "\titers: 2200, epoch: 13 | loss: 0.0767449\n",
      "\tspeed: 0.1153s/iter; left time: 2234.6715s\n",
      "\titers: 2300, epoch: 13 | loss: 0.0773105\n",
      "\tspeed: 0.1139s/iter; left time: 2195.0481s\n",
      "\titers: 2400, epoch: 13 | loss: 0.0802446\n",
      "\tspeed: 0.1137s/iter; left time: 2180.0878s\n",
      "\titers: 2500, epoch: 13 | loss: 0.0897281\n",
      "\tspeed: 0.1136s/iter; left time: 2166.2115s\n",
      "\titers: 2600, epoch: 13 | loss: 0.0587957\n",
      "\tspeed: 0.1148s/iter; left time: 2178.5897s\n",
      "Epoch: 13 cost time: 00h:05m:09.56s\n",
      "Epoch: 13 | Train Loss: 0.0671900 Vali Loss: 0.0578623 Test Loss: 0.0615834\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 14 | loss: 0.0538025\n",
      "\tspeed: 0.9785s/iter; left time: 18377.0368s\n",
      "\titers: 200, epoch: 14 | loss: 0.0614343\n",
      "\tspeed: 0.1136s/iter; left time: 2121.2640s\n",
      "\titers: 300, epoch: 14 | loss: 0.0657694\n",
      "\tspeed: 0.1144s/iter; left time: 2126.3909s\n",
      "\titers: 400, epoch: 14 | loss: 0.0752635\n",
      "\tspeed: 0.1155s/iter; left time: 2134.0448s\n",
      "\titers: 500, epoch: 14 | loss: 0.0564194\n",
      "\tspeed: 0.1159s/iter; left time: 2129.7759s\n",
      "\titers: 600, epoch: 14 | loss: 0.0707608\n",
      "\tspeed: 0.1155s/iter; left time: 2110.6360s\n",
      "\titers: 700, epoch: 14 | loss: 0.0544968\n",
      "\tspeed: 0.1148s/iter; left time: 2086.5129s\n",
      "\titers: 800, epoch: 14 | loss: 0.0735052\n",
      "\tspeed: 0.1150s/iter; left time: 2079.5721s\n",
      "\titers: 900, epoch: 14 | loss: 0.0844271\n",
      "\tspeed: 0.1140s/iter; left time: 2050.4539s\n",
      "\titers: 1000, epoch: 14 | loss: 0.0708888\n",
      "\tspeed: 0.1129s/iter; left time: 2019.2231s\n",
      "\titers: 1100, epoch: 14 | loss: 0.0629603\n",
      "\tspeed: 0.1145s/iter; left time: 2035.5142s\n",
      "\titers: 1200, epoch: 14 | loss: 0.0756200\n",
      "\tspeed: 0.1146s/iter; left time: 2026.9095s\n",
      "\titers: 1300, epoch: 14 | loss: 0.0599315\n",
      "\tspeed: 0.1149s/iter; left time: 2019.8470s\n",
      "\titers: 1400, epoch: 14 | loss: 0.0612757\n",
      "\tspeed: 0.1146s/iter; left time: 2003.0199s\n",
      "\titers: 1500, epoch: 14 | loss: 0.0702794\n",
      "\tspeed: 0.1139s/iter; left time: 1979.1056s\n",
      "\titers: 1600, epoch: 14 | loss: 0.0578222\n",
      "\tspeed: 0.1139s/iter; left time: 1968.7100s\n",
      "\titers: 1700, epoch: 14 | loss: 0.0617748\n",
      "\tspeed: 0.1166s/iter; left time: 2002.9887s\n",
      "\titers: 1800, epoch: 14 | loss: 0.0585543\n",
      "\tspeed: 0.1154s/iter; left time: 1971.6422s\n",
      "\titers: 1900, epoch: 14 | loss: 0.0675614\n",
      "\tspeed: 0.1146s/iter; left time: 1946.0450s\n",
      "\titers: 2000, epoch: 14 | loss: 0.0662966\n",
      "\tspeed: 0.1155s/iter; left time: 1949.4416s\n",
      "\titers: 2100, epoch: 14 | loss: 0.0603743\n",
      "\tspeed: 0.1146s/iter; left time: 1923.5369s\n",
      "\titers: 2200, epoch: 14 | loss: 0.0601280\n",
      "\tspeed: 0.1140s/iter; left time: 1902.0830s\n",
      "\titers: 2300, epoch: 14 | loss: 0.0591565\n",
      "\tspeed: 0.1167s/iter; left time: 1935.6409s\n",
      "\titers: 2400, epoch: 14 | loss: 0.0908771\n",
      "\tspeed: 0.1146s/iter; left time: 1888.3587s\n",
      "\titers: 2500, epoch: 14 | loss: 0.0688252\n",
      "\tspeed: 0.1142s/iter; left time: 1871.1532s\n",
      "\titers: 2600, epoch: 14 | loss: 0.0583719\n",
      "\tspeed: 0.1147s/iter; left time: 1867.9632s\n",
      "Epoch: 14 cost time: 00h:05m:10.11s\n",
      "Epoch: 14 | Train Loss: 0.0668031 Vali Loss: 0.0573795 Test Loss: 0.0608568\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 15 | loss: 0.0688373\n",
      "\tspeed: 0.9843s/iter; left time: 15830.7326s\n",
      "\titers: 200, epoch: 15 | loss: 0.0569479\n",
      "\tspeed: 0.0966s/iter; left time: 1544.1532s\n",
      "\titers: 300, epoch: 15 | loss: 0.0754607\n",
      "\tspeed: 0.1126s/iter; left time: 1787.9768s\n",
      "\titers: 400, epoch: 15 | loss: 0.0596411\n",
      "\tspeed: 0.1138s/iter; left time: 1795.5628s\n",
      "\titers: 500, epoch: 15 | loss: 0.0809594\n",
      "\tspeed: 0.1149s/iter; left time: 1801.2614s\n",
      "\titers: 600, epoch: 15 | loss: 0.0602640\n",
      "\tspeed: 0.1147s/iter; left time: 1787.3242s\n",
      "\titers: 700, epoch: 15 | loss: 0.0620752\n",
      "\tspeed: 0.1161s/iter; left time: 1796.8484s\n",
      "\titers: 800, epoch: 15 | loss: 0.0750819\n",
      "\tspeed: 0.1136s/iter; left time: 1747.5055s\n",
      "\titers: 900, epoch: 15 | loss: 0.0663498\n",
      "\tspeed: 0.1142s/iter; left time: 1745.0218s\n",
      "\titers: 1000, epoch: 15 | loss: 0.0724144\n",
      "\tspeed: 0.1118s/iter; left time: 1696.9032s\n",
      "\titers: 1100, epoch: 15 | loss: 0.0621678\n",
      "\tspeed: 0.1136s/iter; left time: 1714.0208s\n",
      "\titers: 1200, epoch: 15 | loss: 0.0722131\n",
      "\tspeed: 0.1131s/iter; left time: 1694.7276s\n",
      "\titers: 1300, epoch: 15 | loss: 0.0759164\n",
      "\tspeed: 0.1176s/iter; left time: 1749.5838s\n",
      "\titers: 1400, epoch: 15 | loss: 0.0824906\n",
      "\tspeed: 0.1154s/iter; left time: 1706.6226s\n",
      "\titers: 1500, epoch: 15 | loss: 0.0806423\n",
      "\tspeed: 0.1166s/iter; left time: 1712.4875s\n",
      "\titers: 1600, epoch: 15 | loss: 0.0617313\n",
      "\tspeed: 0.1157s/iter; left time: 1686.5711s\n",
      "\titers: 1700, epoch: 15 | loss: 0.0703903\n",
      "\tspeed: 0.1147s/iter; left time: 1661.7245s\n",
      "\titers: 1800, epoch: 15 | loss: 0.0691926\n",
      "\tspeed: 0.1157s/iter; left time: 1664.3588s\n",
      "\titers: 1900, epoch: 15 | loss: 0.0609231\n",
      "\tspeed: 0.1138s/iter; left time: 1625.7338s\n",
      "\titers: 2000, epoch: 15 | loss: 0.0674969\n",
      "\tspeed: 0.1137s/iter; left time: 1612.7425s\n",
      "\titers: 2100, epoch: 15 | loss: 0.0743056\n",
      "\tspeed: 0.1155s/iter; left time: 1626.9866s\n",
      "\titers: 2200, epoch: 15 | loss: 0.0678914\n",
      "\tspeed: 0.1144s/iter; left time: 1599.4736s\n",
      "\titers: 2300, epoch: 15 | loss: 0.0633814\n",
      "\tspeed: 0.1143s/iter; left time: 1587.3052s\n",
      "\titers: 2400, epoch: 15 | loss: 0.0677302\n",
      "\tspeed: 0.1159s/iter; left time: 1597.2985s\n",
      "\titers: 2500, epoch: 15 | loss: 0.0633345\n",
      "\tspeed: 0.1145s/iter; left time: 1567.1010s\n",
      "\titers: 2600, epoch: 15 | loss: 0.0611144\n",
      "\tspeed: 0.1171s/iter; left time: 1590.4667s\n",
      "Epoch: 15 cost time: 00h:05m:08.48s\n",
      "Epoch: 15 | Train Loss: 0.0663890 Vali Loss: 0.0575977 Test Loss: 0.0608814\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 16 | loss: 0.0731103\n",
      "\tspeed: 0.9861s/iter; left time: 13200.5695s\n",
      "\titers: 200, epoch: 16 | loss: 0.0706104\n",
      "\tspeed: 0.1138s/iter; left time: 1512.1604s\n",
      "\titers: 300, epoch: 16 | loss: 0.0581224\n",
      "\tspeed: 0.1156s/iter; left time: 1523.6973s\n",
      "\titers: 400, epoch: 16 | loss: 0.0628917\n",
      "\tspeed: 0.1159s/iter; left time: 1517.1239s\n",
      "\titers: 500, epoch: 16 | loss: 0.0536247\n",
      "\tspeed: 0.1051s/iter; left time: 1364.6688s\n",
      "\titers: 600, epoch: 16 | loss: 0.0669830\n",
      "\tspeed: 0.0966s/iter; left time: 1245.1209s\n",
      "\titers: 700, epoch: 16 | loss: 0.0593961\n",
      "\tspeed: 0.0966s/iter; left time: 1235.0858s\n",
      "\titers: 800, epoch: 16 | loss: 0.0518044\n",
      "\tspeed: 0.0966s/iter; left time: 1225.9406s\n",
      "\titers: 900, epoch: 16 | loss: 0.0667260\n",
      "\tspeed: 0.0963s/iter; left time: 1212.3648s\n",
      "\titers: 1000, epoch: 16 | loss: 0.0675460\n",
      "\tspeed: 0.1111s/iter; left time: 1387.4215s\n",
      "\titers: 1100, epoch: 16 | loss: 0.0571950\n",
      "\tspeed: 0.1155s/iter; left time: 1430.7804s\n",
      "\titers: 1200, epoch: 16 | loss: 0.0801374\n",
      "\tspeed: 0.1140s/iter; left time: 1400.3314s\n",
      "\titers: 1300, epoch: 16 | loss: 0.0643721\n",
      "\tspeed: 0.1145s/iter; left time: 1395.2823s\n",
      "\titers: 1400, epoch: 16 | loss: 0.0662640\n",
      "\tspeed: 0.1159s/iter; left time: 1400.8208s\n",
      "\titers: 1500, epoch: 16 | loss: 0.0630720\n",
      "\tspeed: 0.1149s/iter; left time: 1377.4275s\n",
      "\titers: 1600, epoch: 16 | loss: 0.0607125\n",
      "\tspeed: 0.1125s/iter; left time: 1337.7461s\n",
      "\titers: 1700, epoch: 16 | loss: 0.0707747\n",
      "\tspeed: 0.1155s/iter; left time: 1361.0946s\n",
      "\titers: 1800, epoch: 16 | loss: 0.0875769\n",
      "\tspeed: 0.1150s/iter; left time: 1343.7932s\n",
      "\titers: 1900, epoch: 16 | loss: 0.0685269\n",
      "\tspeed: 0.1181s/iter; left time: 1368.3147s\n",
      "\titers: 2000, epoch: 16 | loss: 0.0532361\n",
      "\tspeed: 0.1156s/iter; left time: 1327.9820s\n",
      "\titers: 2100, epoch: 16 | loss: 0.0648946\n",
      "\tspeed: 0.1152s/iter; left time: 1311.1761s\n",
      "\titers: 2200, epoch: 16 | loss: 0.0554299\n",
      "\tspeed: 0.1138s/iter; left time: 1284.7339s\n",
      "\titers: 2300, epoch: 16 | loss: 0.0634180\n",
      "\tspeed: 0.1142s/iter; left time: 1276.9366s\n",
      "\titers: 2400, epoch: 16 | loss: 0.0690501\n",
      "\tspeed: 0.1145s/iter; left time: 1269.7447s\n",
      "\titers: 2500, epoch: 16 | loss: 0.0580355\n",
      "\tspeed: 0.1141s/iter; left time: 1253.0987s\n",
      "\titers: 2600, epoch: 16 | loss: 0.0707478\n",
      "\tspeed: 0.1141s/iter; left time: 1241.5975s\n",
      "Epoch: 16 cost time: 00h:05m:01.80s\n",
      "Epoch: 16 | Train Loss: 0.0658955 Vali Loss: 0.0577615 Test Loss: 0.0611815\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.010438021272420883, rmse:0.10216663777828217, mae:0.060708094388246536, rse:0.3860151767730713\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 24: 01h:44m:02.92s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "train 86115\n",
      "val 18435\n",
      "test 18435\n",
      "[2024-11-02 22:45:29,624] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 22:45:30,848] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 22:45:30,848] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 22:45:30,849] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 22:45:30,950] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 22:45:30,950] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 22:45:31,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 22:45:31,619] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 22:45:31,619] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 22:45:31,621] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 22:45:31,621] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 22:45:31,621] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 22:45:31,621] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 22:45:31,622] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 22:45:31,951] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 22:45:31,952] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:31,952] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,068] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 22:45:32,069] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:32,069] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,069] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 22:45:32,182] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 22:45:32,183] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB \n",
      "[2024-11-02 22:45:32,183] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 76.14 GB, percent = 10.1%\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 22:45:32,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 22:45:32,185] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa370d9f1d0>\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 22:45:32,186] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 22:45:32,187] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 22:45:32,188] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.1934633\n",
      "\tspeed: 0.1713s/iter; left time: 9200.5937s\n",
      "\titers: 200, epoch: 1 | loss: 0.1763859\n",
      "\tspeed: 0.1289s/iter; left time: 6911.9084s\n",
      "\titers: 300, epoch: 1 | loss: 0.1493364\n",
      "\tspeed: 0.1278s/iter; left time: 6839.0948s\n",
      "\titers: 400, epoch: 1 | loss: 0.1262816\n",
      "\tspeed: 0.1280s/iter; left time: 6839.0464s\n",
      "\titers: 500, epoch: 1 | loss: 0.1171123\n",
      "\tspeed: 0.1287s/iter; left time: 6863.6276s\n",
      "\titers: 600, epoch: 1 | loss: 0.1207857\n",
      "\tspeed: 0.1256s/iter; left time: 6686.9004s\n",
      "\titers: 700, epoch: 1 | loss: 0.1195831\n",
      "\tspeed: 0.1267s/iter; left time: 6732.0809s\n",
      "\titers: 800, epoch: 1 | loss: 0.1091291\n",
      "\tspeed: 0.1166s/iter; left time: 6181.7215s\n",
      "\titers: 900, epoch: 1 | loss: 0.1172049\n",
      "\tspeed: 0.1168s/iter; left time: 6182.9086s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0856876\n",
      "\tspeed: 0.1073s/iter; left time: 5668.7135s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1010989\n",
      "\tspeed: 0.1078s/iter; left time: 5682.7896s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1015837\n",
      "\tspeed: 0.1073s/iter; left time: 5645.8179s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1383912\n",
      "\tspeed: 0.1071s/iter; left time: 5625.1548s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1047541\n",
      "\tspeed: 0.1073s/iter; left time: 5626.4303s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1040020\n",
      "\tspeed: 0.1260s/iter; left time: 6590.7064s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0965157\n",
      "\tspeed: 0.1259s/iter; left time: 6576.2054s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1022582\n",
      "\tspeed: 0.1265s/iter; left time: 6592.7620s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1087027\n",
      "\tspeed: 0.1243s/iter; left time: 6467.8052s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1015980\n",
      "\tspeed: 0.1261s/iter; left time: 6546.8714s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0960999\n",
      "\tspeed: 0.1262s/iter; left time: 6537.8271s\n",
      "\titers: 2100, epoch: 1 | loss: 0.0915355\n",
      "\tspeed: 0.1257s/iter; left time: 6502.1847s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1123403\n",
      "\tspeed: 0.1250s/iter; left time: 6453.1353s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1265393\n",
      "\tspeed: 0.1274s/iter; left time: 6563.0584s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0845973\n",
      "\tspeed: 0.1267s/iter; left time: 6514.9429s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0974592\n",
      "\tspeed: 0.1262s/iter; left time: 6478.6545s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1181518\n",
      "\tspeed: 0.1261s/iter; left time: 6457.0195s\n",
      "Epoch: 1 cost time: 00h:05m:30.32s\n",
      "Epoch: 1 | Train Loss: 0.1179917 Vali Loss: 0.0848198 Test Loss: 0.0898159\n",
      "Validation loss decreased (inf --> 0.084820).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.1002837\n",
      "\tspeed: 1.0899s/iter; left time: 55615.3095s\n",
      "\titers: 200, epoch: 2 | loss: 0.0906165\n",
      "\tspeed: 0.0962s/iter; left time: 4900.1911s\n",
      "\titers: 300, epoch: 2 | loss: 0.0938583\n",
      "\tspeed: 0.1143s/iter; left time: 5811.7729s\n",
      "\titers: 400, epoch: 2 | loss: 0.0962128\n",
      "\tspeed: 0.1157s/iter; left time: 5871.3165s\n",
      "\titers: 500, epoch: 2 | loss: 0.0942962\n",
      "\tspeed: 0.1150s/iter; left time: 5821.1903s\n",
      "\titers: 600, epoch: 2 | loss: 0.1065316\n",
      "\tspeed: 0.1148s/iter; left time: 5802.8616s\n",
      "\titers: 700, epoch: 2 | loss: 0.0971361\n",
      "\tspeed: 0.1161s/iter; left time: 5856.9167s\n",
      "\titers: 800, epoch: 2 | loss: 0.1060096\n",
      "\tspeed: 0.1163s/iter; left time: 5852.4864s\n",
      "\titers: 900, epoch: 2 | loss: 0.1037547\n",
      "\tspeed: 0.1165s/iter; left time: 5853.7284s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1060831\n",
      "\tspeed: 0.1159s/iter; left time: 5812.3636s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1103433\n",
      "\tspeed: 0.1150s/iter; left time: 5753.1402s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1000298\n",
      "\tspeed: 0.1163s/iter; left time: 5809.2005s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1118626\n",
      "\tspeed: 0.1156s/iter; left time: 5758.9532s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0781817\n",
      "\tspeed: 0.1153s/iter; left time: 5734.5130s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0979162\n",
      "\tspeed: 0.1160s/iter; left time: 5758.2457s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0903831\n",
      "\tspeed: 0.1141s/iter; left time: 5649.7216s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0924969\n",
      "\tspeed: 0.1141s/iter; left time: 5639.3169s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1178317\n",
      "\tspeed: 0.1146s/iter; left time: 5653.3441s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0728766\n",
      "\tspeed: 0.1094s/iter; left time: 5385.8710s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0894050\n",
      "\tspeed: 0.1046s/iter; left time: 5139.9066s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1005967\n",
      "\tspeed: 0.1115s/iter; left time: 5467.9781s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0853283\n",
      "\tspeed: 0.1141s/iter; left time: 5581.4293s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0847278\n",
      "\tspeed: 0.1139s/iter; left time: 5559.7386s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1019449\n",
      "\tspeed: 0.1153s/iter; left time: 5620.5961s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0932351\n",
      "\tspeed: 0.1152s/iter; left time: 5600.7076s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0873921\n",
      "\tspeed: 0.1167s/iter; left time: 5663.7633s\n",
      "Epoch: 2 cost time: 00h:05m:04.86s\n",
      "Epoch: 2 | Train Loss: 0.0962975 Vali Loss: 0.0798545 Test Loss: 0.0858662\n",
      "Validation loss decreased (0.084820 --> 0.079855).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.0884367\n",
      "\tspeed: 0.9828s/iter; left time: 47505.5496s\n",
      "\titers: 200, epoch: 3 | loss: 0.1001280\n",
      "\tspeed: 0.1150s/iter; left time: 5549.5756s\n",
      "\titers: 300, epoch: 3 | loss: 0.1046731\n",
      "\tspeed: 0.1074s/iter; left time: 5171.4595s\n",
      "\titers: 400, epoch: 3 | loss: 0.0806120\n",
      "\tspeed: 0.0970s/iter; left time: 4660.2030s\n",
      "\titers: 500, epoch: 3 | loss: 0.0833375\n",
      "\tspeed: 0.0965s/iter; left time: 4624.8956s\n",
      "\titers: 600, epoch: 3 | loss: 0.0915050\n",
      "\tspeed: 0.1107s/iter; left time: 5293.4542s\n",
      "\titers: 700, epoch: 3 | loss: 0.0814567\n",
      "\tspeed: 0.1144s/iter; left time: 5462.7055s\n",
      "\titers: 800, epoch: 3 | loss: 0.0916979\n",
      "\tspeed: 0.1150s/iter; left time: 5477.3659s\n",
      "\titers: 900, epoch: 3 | loss: 0.1094405\n",
      "\tspeed: 0.1170s/iter; left time: 5561.3051s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1011684\n",
      "\tspeed: 0.1181s/iter; left time: 5603.5740s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0865153\n",
      "\tspeed: 0.1163s/iter; left time: 5503.3759s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0769959\n",
      "\tspeed: 0.1185s/iter; left time: 5596.0877s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0918234\n",
      "\tspeed: 0.1152s/iter; left time: 5429.9668s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0899273\n",
      "\tspeed: 0.1158s/iter; left time: 5447.0141s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1062002\n",
      "\tspeed: 0.1168s/iter; left time: 5483.4192s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0708728\n",
      "\tspeed: 0.1159s/iter; left time: 5429.9981s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1091485\n",
      "\tspeed: 0.1166s/iter; left time: 5451.7992s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1023156\n",
      "\tspeed: 0.1149s/iter; left time: 5359.9744s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0731412\n",
      "\tspeed: 0.1085s/iter; left time: 5050.1919s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1124088\n",
      "\tspeed: 0.1142s/iter; left time: 5303.9769s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1252687\n",
      "\tspeed: 0.1169s/iter; left time: 5416.1146s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0836434\n",
      "\tspeed: 0.1153s/iter; left time: 5333.4217s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1059322\n",
      "\tspeed: 0.1165s/iter; left time: 5373.8691s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0889617\n",
      "\tspeed: 0.1159s/iter; left time: 5335.2100s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0884438\n",
      "\tspeed: 0.1146s/iter; left time: 5262.9297s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0789040\n",
      "\tspeed: 0.1152s/iter; left time: 5278.6567s\n",
      "Epoch: 3 cost time: 00h:05m:06.51s\n",
      "Epoch: 3 | Train Loss: 0.0925695 Vali Loss: 0.0794393 Test Loss: 0.0851859\n",
      "Validation loss decreased (0.079855 --> 0.079439).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0944193\n",
      "\tspeed: 0.9894s/iter; left time: 45165.9990s\n",
      "\titers: 200, epoch: 4 | loss: 0.1009860\n",
      "\tspeed: 0.1164s/iter; left time: 5300.7634s\n",
      "\titers: 300, epoch: 4 | loss: 0.1014991\n",
      "\tspeed: 0.1175s/iter; left time: 5338.4891s\n",
      "\titers: 400, epoch: 4 | loss: 0.0945554\n",
      "\tspeed: 0.1069s/iter; left time: 4848.2545s\n",
      "\titers: 500, epoch: 4 | loss: 0.0920349\n",
      "\tspeed: 0.1099s/iter; left time: 4973.2031s\n",
      "\titers: 600, epoch: 4 | loss: 0.0887014\n",
      "\tspeed: 0.1151s/iter; left time: 5194.5193s\n",
      "\titers: 700, epoch: 4 | loss: 0.0846027\n",
      "\tspeed: 0.1151s/iter; left time: 5183.7344s\n",
      "\titers: 800, epoch: 4 | loss: 0.0905010\n",
      "\tspeed: 0.1151s/iter; left time: 5174.5785s\n",
      "\titers: 900, epoch: 4 | loss: 0.0935379\n",
      "\tspeed: 0.1148s/iter; left time: 5148.6376s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0826019\n",
      "\tspeed: 0.1158s/iter; left time: 5181.6581s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1028748\n",
      "\tspeed: 0.1149s/iter; left time: 5131.9953s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1137985\n",
      "\tspeed: 0.1143s/iter; left time: 5090.8303s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0763816\n",
      "\tspeed: 0.1143s/iter; left time: 5078.2930s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0880500\n",
      "\tspeed: 0.1147s/iter; left time: 5085.6903s\n",
      "\titers: 1500, epoch: 4 | loss: 0.0818828\n",
      "\tspeed: 0.1151s/iter; left time: 5093.7656s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0753789\n",
      "\tspeed: 0.1153s/iter; left time: 5089.6542s\n",
      "\titers: 1700, epoch: 4 | loss: 0.0667379\n",
      "\tspeed: 0.1164s/iter; left time: 5126.1695s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0773144\n",
      "\tspeed: 0.1149s/iter; left time: 5048.0574s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0894531\n",
      "\tspeed: 0.1153s/iter; left time: 5057.1730s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0876464\n",
      "\tspeed: 0.1143s/iter; left time: 5000.3282s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0817939\n",
      "\tspeed: 0.1148s/iter; left time: 5010.6727s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0817806\n",
      "\tspeed: 0.1145s/iter; left time: 4988.0128s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1111523\n",
      "\tspeed: 0.1145s/iter; left time: 4974.3493s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1141342\n",
      "\tspeed: 0.1150s/iter; left time: 4986.0679s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1129447\n",
      "\tspeed: 0.1145s/iter; left time: 4952.6372s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0830951\n",
      "\tspeed: 0.1113s/iter; left time: 4803.8555s\n",
      "Epoch: 4 cost time: 00h:05m:08.73s\n",
      "Epoch: 4 | Train Loss: 0.0904820 Vali Loss: 0.0780338 Test Loss: 0.0831292\n",
      "Validation loss decreased (0.079439 --> 0.078034).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0972390\n",
      "\tspeed: 0.9804s/iter; left time: 42113.7149s\n",
      "\titers: 200, epoch: 5 | loss: 0.0866233\n",
      "\tspeed: 0.1155s/iter; left time: 4948.1146s\n",
      "\titers: 300, epoch: 5 | loss: 0.0958727\n",
      "\tspeed: 0.1177s/iter; left time: 5032.4006s\n",
      "\titers: 400, epoch: 5 | loss: 0.0693469\n",
      "\tspeed: 0.1165s/iter; left time: 4968.6235s\n",
      "\titers: 500, epoch: 5 | loss: 0.0849624\n",
      "\tspeed: 0.1164s/iter; left time: 4953.0272s\n",
      "\titers: 600, epoch: 5 | loss: 0.1001985\n",
      "\tspeed: 0.1186s/iter; left time: 5034.6916s\n",
      "\titers: 700, epoch: 5 | loss: 0.1063398\n",
      "\tspeed: 0.1176s/iter; left time: 4980.0291s\n",
      "\titers: 800, epoch: 5 | loss: 0.0999221\n",
      "\tspeed: 0.1158s/iter; left time: 4892.5268s\n",
      "\titers: 900, epoch: 5 | loss: 0.0816023\n",
      "\tspeed: 0.1173s/iter; left time: 4944.0625s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0799536\n",
      "\tspeed: 0.1156s/iter; left time: 4863.7731s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0782519\n",
      "\tspeed: 0.1180s/iter; left time: 4951.3473s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0764363\n",
      "\tspeed: 0.1154s/iter; left time: 4829.2510s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0967658\n",
      "\tspeed: 0.1170s/iter; left time: 4886.6172s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0868528\n",
      "\tspeed: 0.1162s/iter; left time: 4838.9733s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0770575\n",
      "\tspeed: 0.1179s/iter; left time: 4900.6489s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0936656\n",
      "\tspeed: 0.1191s/iter; left time: 4938.2399s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0781666\n",
      "\tspeed: 0.1202s/iter; left time: 4973.1603s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0932333\n",
      "\tspeed: 0.1178s/iter; left time: 4860.0693s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1001875\n",
      "\tspeed: 0.1168s/iter; left time: 4809.1613s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1147297\n",
      "\tspeed: 0.1168s/iter; left time: 4796.2472s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0869296\n",
      "\tspeed: 0.1206s/iter; left time: 4938.1741s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0745400\n",
      "\tspeed: 0.1186s/iter; left time: 4844.6170s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0715365\n",
      "\tspeed: 0.1161s/iter; left time: 4731.8313s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0643543\n",
      "\tspeed: 0.1185s/iter; left time: 4816.8680s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0852049\n",
      "\tspeed: 0.1155s/iter; left time: 4685.3851s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0833014\n",
      "\tspeed: 0.1183s/iter; left time: 4787.8768s\n",
      "Epoch: 5 cost time: 00h:05m:15.74s\n",
      "Epoch: 5 | Train Loss: 0.0887613 Vali Loss: 0.0775704 Test Loss: 0.0839218\n",
      "Validation loss decreased (0.078034 --> 0.077570).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.0947636\n",
      "\tspeed: 0.9904s/iter; left time: 39880.4076s\n",
      "\titers: 200, epoch: 6 | loss: 0.0871146\n",
      "\tspeed: 0.1148s/iter; left time: 4609.5723s\n",
      "\titers: 300, epoch: 6 | loss: 0.1186312\n",
      "\tspeed: 0.1145s/iter; left time: 4588.0703s\n",
      "\titers: 400, epoch: 6 | loss: 0.1023487\n",
      "\tspeed: 0.1146s/iter; left time: 4581.3191s\n",
      "\titers: 500, epoch: 6 | loss: 0.0941400\n",
      "\tspeed: 0.1160s/iter; left time: 4623.0951s\n",
      "\titers: 600, epoch: 6 | loss: 0.0893563\n",
      "\tspeed: 0.1142s/iter; left time: 4541.1525s\n",
      "\titers: 700, epoch: 6 | loss: 0.0804608\n",
      "\tspeed: 0.1157s/iter; left time: 4587.4114s\n",
      "\titers: 800, epoch: 6 | loss: 0.0967790\n",
      "\tspeed: 0.1140s/iter; left time: 4510.0502s\n",
      "\titers: 900, epoch: 6 | loss: 0.0773378\n",
      "\tspeed: 0.1147s/iter; left time: 4527.2170s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0813385\n",
      "\tspeed: 0.1153s/iter; left time: 4538.8424s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0976809\n",
      "\tspeed: 0.1153s/iter; left time: 4528.6515s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0809603\n",
      "\tspeed: 0.1159s/iter; left time: 4538.5710s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0903448\n",
      "\tspeed: 0.1150s/iter; left time: 4493.0150s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0978441\n",
      "\tspeed: 0.1152s/iter; left time: 4489.8987s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0846561\n",
      "\tspeed: 0.1174s/iter; left time: 4563.0950s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1038152\n",
      "\tspeed: 0.1135s/iter; left time: 4400.6125s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0767741\n",
      "\tspeed: 0.1166s/iter; left time: 4508.1000s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0908609\n",
      "\tspeed: 0.1169s/iter; left time: 4510.1077s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0820962\n",
      "\tspeed: 0.1176s/iter; left time: 4522.2339s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0735893\n",
      "\tspeed: 0.1173s/iter; left time: 4500.7639s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0998428\n",
      "\tspeed: 0.1150s/iter; left time: 4399.5530s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0660093\n",
      "\tspeed: 0.1152s/iter; left time: 4398.4991s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0850836\n",
      "\tspeed: 0.1113s/iter; left time: 4234.8741s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0883901\n",
      "\tspeed: 0.1006s/iter; left time: 3820.2006s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0745388\n",
      "\tspeed: 0.1022s/iter; left time: 3868.0306s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0925138\n",
      "\tspeed: 0.1153s/iter; left time: 4355.8140s\n",
      "Epoch: 6 cost time: 00h:05m:08.02s\n",
      "Epoch: 6 | Train Loss: 0.0873064 Vali Loss: 0.0791609 Test Loss: 0.0851259\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0738214\n",
      "\tspeed: 0.9711s/iter; left time: 36489.8400s\n",
      "\titers: 200, epoch: 7 | loss: 0.0840051\n",
      "\tspeed: 0.1161s/iter; left time: 4352.2370s\n",
      "\titers: 300, epoch: 7 | loss: 0.0792297\n",
      "\tspeed: 0.1152s/iter; left time: 4305.8477s\n",
      "\titers: 400, epoch: 7 | loss: 0.1042238\n",
      "\tspeed: 0.1162s/iter; left time: 4333.1863s\n",
      "\titers: 500, epoch: 7 | loss: 0.0820721\n",
      "\tspeed: 0.1171s/iter; left time: 4352.7926s\n",
      "\titers: 600, epoch: 7 | loss: 0.0845355\n",
      "\tspeed: 0.1178s/iter; left time: 4366.5657s\n",
      "\titers: 700, epoch: 7 | loss: 0.0905107\n",
      "\tspeed: 0.1159s/iter; left time: 4285.8246s\n",
      "\titers: 800, epoch: 7 | loss: 0.0772271\n",
      "\tspeed: 0.1149s/iter; left time: 4236.7052s\n",
      "\titers: 900, epoch: 7 | loss: 0.0770174\n",
      "\tspeed: 0.1168s/iter; left time: 4294.1460s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0694755\n",
      "\tspeed: 0.1169s/iter; left time: 4287.4194s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1038114\n",
      "\tspeed: 0.1172s/iter; left time: 4287.6980s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0788102\n",
      "\tspeed: 0.1171s/iter; left time: 4270.7940s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0758535\n",
      "\tspeed: 0.1167s/iter; left time: 4246.4545s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0813657\n",
      "\tspeed: 0.1152s/iter; left time: 4177.8630s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0944329\n",
      "\tspeed: 0.1164s/iter; left time: 4211.2387s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1017237\n",
      "\tspeed: 0.1156s/iter; left time: 4171.1375s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1062258\n",
      "\tspeed: 0.1157s/iter; left time: 4162.8680s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1014802\n",
      "\tspeed: 0.1167s/iter; left time: 4187.8144s\n",
      "\titers: 1900, epoch: 7 | loss: 0.1010558\n",
      "\tspeed: 0.1167s/iter; left time: 4174.3972s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0732227\n",
      "\tspeed: 0.1169s/iter; left time: 4169.7044s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0915565\n",
      "\tspeed: 0.1173s/iter; left time: 4174.4953s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0892187\n",
      "\tspeed: 0.1153s/iter; left time: 4090.8475s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0861450\n",
      "\tspeed: 0.1153s/iter; left time: 4077.3121s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0746982\n",
      "\tspeed: 0.1160s/iter; left time: 4093.0898s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0731142\n",
      "\tspeed: 0.1162s/iter; left time: 4088.0478s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0744261\n",
      "\tspeed: 0.1155s/iter; left time: 4049.5048s\n",
      "Epoch: 7 cost time: 00h:05m:13.39s\n",
      "Epoch: 7 | Train Loss: 0.0857488 Vali Loss: 0.0787276 Test Loss: 0.0848344\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0803231\n",
      "\tspeed: 0.9810s/iter; left time: 34221.1282s\n",
      "\titers: 200, epoch: 8 | loss: 0.0757215\n",
      "\tspeed: 0.1165s/iter; left time: 4050.9094s\n",
      "\titers: 300, epoch: 8 | loss: 0.0945297\n",
      "\tspeed: 0.1174s/iter; left time: 4071.7276s\n",
      "\titers: 400, epoch: 8 | loss: 0.0887359\n",
      "\tspeed: 0.1170s/iter; left time: 4046.3619s\n",
      "\titers: 500, epoch: 8 | loss: 0.0828975\n",
      "\tspeed: 0.1153s/iter; left time: 3975.1436s\n",
      "\titers: 600, epoch: 8 | loss: 0.0813282\n",
      "\tspeed: 0.1162s/iter; left time: 3995.7034s\n",
      "\titers: 700, epoch: 8 | loss: 0.0668938\n",
      "\tspeed: 0.1176s/iter; left time: 4032.2929s\n",
      "\titers: 800, epoch: 8 | loss: 0.0837017\n",
      "\tspeed: 0.1063s/iter; left time: 3633.6560s\n",
      "\titers: 900, epoch: 8 | loss: 0.0803642\n",
      "\tspeed: 0.1150s/iter; left time: 3918.1419s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0760200\n",
      "\tspeed: 0.1106s/iter; left time: 3759.4089s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0895748\n",
      "\tspeed: 0.1079s/iter; left time: 3655.4745s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0938020\n",
      "\tspeed: 0.1076s/iter; left time: 3636.5085s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0925988\n",
      "\tspeed: 0.1089s/iter; left time: 3669.5521s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0767561\n",
      "\tspeed: 0.1087s/iter; left time: 3651.4342s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0654879\n",
      "\tspeed: 0.1088s/iter; left time: 3642.3707s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0950684\n",
      "\tspeed: 0.1099s/iter; left time: 3669.1597s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0819798\n",
      "\tspeed: 0.1085s/iter; left time: 3610.1746s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1015619\n",
      "\tspeed: 0.1104s/iter; left time: 3662.4440s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0803827\n",
      "\tspeed: 0.1075s/iter; left time: 3555.6806s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0669943\n",
      "\tspeed: 0.1076s/iter; left time: 3548.2569s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0584329\n",
      "\tspeed: 0.1100s/iter; left time: 3617.0933s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0806590\n",
      "\tspeed: 0.1085s/iter; left time: 3556.5976s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0929671\n",
      "\tspeed: 0.1069s/iter; left time: 3493.5199s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0913270\n",
      "\tspeed: 0.1105s/iter; left time: 3600.6363s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0718101\n",
      "\tspeed: 0.1065s/iter; left time: 3459.0762s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0859066\n",
      "\tspeed: 0.1086s/iter; left time: 3518.3297s\n",
      "Epoch: 8 cost time: 00h:04m:58.79s\n",
      "Epoch: 8 | Train Loss: 0.0842752 Vali Loss: 0.0798619 Test Loss: 0.0841101\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0922106\n",
      "\tspeed: 0.9995s/iter; left time: 32176.1556s\n",
      "\titers: 200, epoch: 9 | loss: 0.0897571\n",
      "\tspeed: 0.1089s/iter; left time: 3495.7113s\n",
      "\titers: 300, epoch: 9 | loss: 0.0623380\n",
      "\tspeed: 0.1080s/iter; left time: 3456.0926s\n",
      "\titers: 400, epoch: 9 | loss: 0.0733511\n",
      "\tspeed: 0.1100s/iter; left time: 3509.7160s\n",
      "\titers: 500, epoch: 9 | loss: 0.0863153\n",
      "\tspeed: 0.1096s/iter; left time: 3485.3193s\n",
      "\titers: 600, epoch: 9 | loss: 0.0784250\n",
      "\tspeed: 0.1099s/iter; left time: 3482.5028s\n",
      "\titers: 700, epoch: 9 | loss: 0.0621964\n",
      "\tspeed: 0.1095s/iter; left time: 3460.7887s\n",
      "\titers: 800, epoch: 9 | loss: 0.0698314\n",
      "\tspeed: 0.1091s/iter; left time: 3436.6810s\n",
      "\titers: 900, epoch: 9 | loss: 0.0820047\n",
      "\tspeed: 0.1083s/iter; left time: 3401.3189s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0763479\n",
      "\tspeed: 0.1087s/iter; left time: 3400.6183s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0723443\n",
      "\tspeed: 0.1089s/iter; left time: 3398.3996s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0933339\n",
      "\tspeed: 0.1107s/iter; left time: 3440.4772s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0803285\n",
      "\tspeed: 0.1108s/iter; left time: 3433.4594s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0697617\n",
      "\tspeed: 0.1117s/iter; left time: 3451.2412s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0668261\n",
      "\tspeed: 0.1106s/iter; left time: 3404.3545s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0736176\n",
      "\tspeed: 0.1111s/iter; left time: 3409.7163s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0924216\n",
      "\tspeed: 0.1106s/iter; left time: 3384.7932s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0700373\n",
      "\tspeed: 0.1110s/iter; left time: 3386.1760s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0905273\n",
      "\tspeed: 0.1121s/iter; left time: 3407.0341s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0873711\n",
      "\tspeed: 0.1104s/iter; left time: 3343.4414s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0690359\n",
      "\tspeed: 0.1115s/iter; left time: 3365.0326s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0764138\n",
      "\tspeed: 0.1107s/iter; left time: 3330.2051s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0736902\n",
      "\tspeed: 0.1111s/iter; left time: 3331.5544s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0738030\n",
      "\tspeed: 0.1109s/iter; left time: 3314.5451s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0723926\n",
      "\tspeed: 0.1106s/iter; left time: 3295.7242s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0774040\n",
      "\tspeed: 0.1127s/iter; left time: 3345.2743s\n",
      "Epoch: 9 cost time: 00h:04m:57.58s\n",
      "Epoch: 9 | Train Loss: 0.0826723 Vali Loss: 0.0802053 Test Loss: 0.0843679\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0718719\n",
      "\tspeed: 1.0037s/iter; left time: 29611.6652s\n",
      "\titers: 200, epoch: 10 | loss: 0.0820134\n",
      "\tspeed: 0.1085s/iter; left time: 3191.3399s\n",
      "\titers: 300, epoch: 10 | loss: 0.0871429\n",
      "\tspeed: 0.1121s/iter; left time: 3285.7988s\n",
      "\titers: 400, epoch: 10 | loss: 0.0750499\n",
      "\tspeed: 0.1103s/iter; left time: 3221.5376s\n",
      "\titers: 500, epoch: 10 | loss: 0.0743526\n",
      "\tspeed: 0.1100s/iter; left time: 3202.5129s\n",
      "\titers: 600, epoch: 10 | loss: 0.0716704\n",
      "\tspeed: 0.1070s/iter; left time: 3103.4502s\n",
      "\titers: 700, epoch: 10 | loss: 0.0730481\n",
      "\tspeed: 0.1096s/iter; left time: 3166.3384s\n",
      "\titers: 800, epoch: 10 | loss: 0.0884776\n",
      "\tspeed: 0.1093s/iter; left time: 3148.1204s\n",
      "\titers: 900, epoch: 10 | loss: 0.0730145\n",
      "\tspeed: 0.1098s/iter; left time: 3150.3721s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0823452\n",
      "\tspeed: 0.1074s/iter; left time: 3072.0429s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0792662\n",
      "\tspeed: 0.1076s/iter; left time: 3067.9559s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0772781\n",
      "\tspeed: 0.1071s/iter; left time: 3041.7340s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0990755\n",
      "\tspeed: 0.1093s/iter; left time: 3093.3337s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0935434\n",
      "\tspeed: 0.1111s/iter; left time: 3132.4183s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0814993\n",
      "\tspeed: 0.1073s/iter; left time: 3016.1721s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0930371\n",
      "\tspeed: 0.1092s/iter; left time: 3057.1117s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0896463\n",
      "\tspeed: 0.1093s/iter; left time: 3049.7165s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0843857\n",
      "\tspeed: 0.1118s/iter; left time: 3109.5160s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0857551\n",
      "\tspeed: 0.1115s/iter; left time: 3088.6796s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0770558\n",
      "\tspeed: 0.1127s/iter; left time: 3110.4487s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0894686\n",
      "\tspeed: 0.1135s/iter; left time: 3121.6679s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0692895\n",
      "\tspeed: 0.1125s/iter; left time: 3082.0599s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0728806\n",
      "\tspeed: 0.1132s/iter; left time: 3090.6093s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0917956\n",
      "\tspeed: 0.1126s/iter; left time: 3064.0648s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0742273\n",
      "\tspeed: 0.1116s/iter; left time: 3025.0136s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0754556\n",
      "\tspeed: 0.1127s/iter; left time: 3044.0736s\n",
      "Epoch: 10 cost time: 00h:04m:57.45s\n",
      "Epoch: 10 | Train Loss: 0.0813775 Vali Loss: 0.0823561 Test Loss: 0.0861748\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.01877460442483425, rmse:0.13702045381069183, mae:0.08392173796892166, rse:0.5181057453155518\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 96: 01h:05m:25.55s\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "train 85899\n",
      "val 18219\n",
      "test 18219\n",
      "[2024-11-02 23:50:53,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-02 23:50:54,668] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-02 23:50:54,668] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-11-02 23:50:54,668] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-11-02 23:50:54,764] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.43, master_port=29500\n",
      "[2024-11-02 23:50:54,764] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-02 23:50:55,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-02 23:50:55,479] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-02 23:50:55,479] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-02 23:50:55,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-11-02 23:50:55,480] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-11-02 23:50:55,480] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-11-02 23:50:55,480] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-11-02 23:50:55,481] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-11-02 23:50:55,878] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-11-02 23:50:55,879] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:55,879] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.54 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,065] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-11-02 23:50:56,066] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:56,066] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.69 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,066] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-11-02 23:50:56,224] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-11-02 23:50:56,225] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB \n",
      "[2024-11-02 23:50:56,225] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 77.72 GB, percent = 10.3%\n",
      "[2024-11-02 23:50:56,226] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-11-02 23:50:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-11-02 23:50:56,228] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-02 23:50:56,229] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efc80676d90>\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-11-02 23:50:56,230] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-11-02 23:50:56,231] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-02 23:50:56,232] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-11-02 23:50:56,233] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 0.2029339\n",
      "\tspeed: 0.1679s/iter; left time: 8995.0314s\n",
      "\titers: 200, epoch: 1 | loss: 0.1965789\n",
      "\tspeed: 0.1201s/iter; left time: 6421.9761s\n",
      "\titers: 300, epoch: 1 | loss: 0.1898626\n",
      "\tspeed: 0.1198s/iter; left time: 6392.5897s\n",
      "\titers: 400, epoch: 1 | loss: 0.1325201\n",
      "\tspeed: 0.1219s/iter; left time: 6497.6080s\n",
      "\titers: 500, epoch: 1 | loss: 0.1311700\n",
      "\tspeed: 0.1221s/iter; left time: 6492.0127s\n",
      "\titers: 600, epoch: 1 | loss: 0.1017136\n",
      "\tspeed: 0.1197s/iter; left time: 6353.0440s\n",
      "\titers: 700, epoch: 1 | loss: 0.1076386\n",
      "\tspeed: 0.1220s/iter; left time: 6462.5694s\n",
      "\titers: 800, epoch: 1 | loss: 0.0973543\n",
      "\tspeed: 0.1208s/iter; left time: 6389.9462s\n",
      "\titers: 900, epoch: 1 | loss: 0.1088642\n",
      "\tspeed: 0.1220s/iter; left time: 6437.7781s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1006573\n",
      "\tspeed: 0.1170s/iter; left time: 6163.3873s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1109908\n",
      "\tspeed: 0.1200s/iter; left time: 6309.1371s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1184647\n",
      "\tspeed: 0.1189s/iter; left time: 6240.6206s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1054379\n",
      "\tspeed: 0.1211s/iter; left time: 6343.0842s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0932986\n",
      "\tspeed: 0.1217s/iter; left time: 6362.1241s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0875861\n",
      "\tspeed: 0.1198s/iter; left time: 6252.8626s\n",
      "\titers: 1600, epoch: 1 | loss: 0.0992577\n",
      "\tspeed: 0.1198s/iter; left time: 6240.0593s\n",
      "\titers: 1700, epoch: 1 | loss: 0.0822084\n",
      "\tspeed: 0.1217s/iter; left time: 6327.8348s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0944820\n",
      "\tspeed: 0.1208s/iter; left time: 6268.0620s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1164758\n",
      "\tspeed: 0.1205s/iter; left time: 6241.0843s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0924384\n",
      "\tspeed: 0.1191s/iter; left time: 6155.0111s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1107640\n",
      "\tspeed: 0.1185s/iter; left time: 6110.7540s\n",
      "\titers: 2200, epoch: 1 | loss: 0.0971796\n",
      "\tspeed: 0.1189s/iter; left time: 6122.0563s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1212062\n",
      "\tspeed: 0.1190s/iter; left time: 6114.2571s\n",
      "\titers: 2400, epoch: 1 | loss: 0.0958548\n",
      "\tspeed: 0.1187s/iter; left time: 6088.0092s\n",
      "\titers: 2500, epoch: 1 | loss: 0.0936789\n",
      "\tspeed: 0.1194s/iter; left time: 6108.7054s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1049049\n",
      "\tspeed: 0.1187s/iter; left time: 6061.0823s\n",
      "Epoch: 1 cost time: 00h:05m:23.87s\n",
      "Epoch: 1 | Train Loss: 0.1210713 Vali Loss: 0.0890277 Test Loss: 0.0935509\n",
      "Validation loss decreased (inf --> 0.089028).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 2 | loss: 0.0989368\n",
      "\tspeed: 1.1391s/iter; left time: 57975.4121s\n",
      "\titers: 200, epoch: 2 | loss: 0.1063188\n",
      "\tspeed: 0.1097s/iter; left time: 5572.4976s\n",
      "\titers: 300, epoch: 2 | loss: 0.0887055\n",
      "\tspeed: 0.1095s/iter; left time: 5550.5036s\n",
      "\titers: 400, epoch: 2 | loss: 0.1021468\n",
      "\tspeed: 0.1079s/iter; left time: 5457.7022s\n",
      "\titers: 500, epoch: 2 | loss: 0.1028672\n",
      "\tspeed: 0.1096s/iter; left time: 5532.1899s\n",
      "\titers: 600, epoch: 2 | loss: 0.0918319\n",
      "\tspeed: 0.1079s/iter; left time: 5439.5561s\n",
      "\titers: 700, epoch: 2 | loss: 0.0951936\n",
      "\tspeed: 0.1054s/iter; left time: 5299.3630s\n",
      "\titers: 800, epoch: 2 | loss: 0.1038750\n",
      "\tspeed: 0.1104s/iter; left time: 5544.2517s\n",
      "\titers: 900, epoch: 2 | loss: 0.1151560\n",
      "\tspeed: 0.1108s/iter; left time: 5550.2593s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1127429\n",
      "\tspeed: 0.1094s/iter; left time: 5469.5303s\n",
      "\titers: 1100, epoch: 2 | loss: 0.0969984\n",
      "\tspeed: 0.1083s/iter; left time: 5403.1301s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0921559\n",
      "\tspeed: 0.1101s/iter; left time: 5484.1706s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0809502\n",
      "\tspeed: 0.1082s/iter; left time: 5376.5853s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1063600\n",
      "\tspeed: 0.1123s/iter; left time: 5570.1085s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1049415\n",
      "\tspeed: 0.1081s/iter; left time: 5350.1810s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0913269\n",
      "\tspeed: 0.1085s/iter; left time: 5358.6075s\n",
      "\titers: 1700, epoch: 2 | loss: 0.0902280\n",
      "\tspeed: 0.1107s/iter; left time: 5457.1329s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1115005\n",
      "\tspeed: 0.1093s/iter; left time: 5377.3792s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1071176\n",
      "\tspeed: 0.1084s/iter; left time: 5323.9618s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1019441\n",
      "\tspeed: 0.1092s/iter; left time: 5348.9450s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1077473\n",
      "\tspeed: 0.1128s/iter; left time: 5513.3037s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1180863\n",
      "\tspeed: 0.1093s/iter; left time: 5335.5151s\n",
      "\titers: 2300, epoch: 2 | loss: 0.0979958\n",
      "\tspeed: 0.1077s/iter; left time: 5242.6888s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1100279\n",
      "\tspeed: 0.1086s/iter; left time: 5277.2700s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1068703\n",
      "\tspeed: 0.1106s/iter; left time: 5366.0161s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0836630\n",
      "\tspeed: 0.1099s/iter; left time: 5316.6257s\n",
      "Epoch: 2 cost time: 00h:04m:53.91s\n",
      "Epoch: 2 | Train Loss: 0.0996675 Vali Loss: 0.0841658 Test Loss: 0.0895244\n",
      "Validation loss decreased (0.089028 --> 0.084166).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 3 | loss: 0.1142319\n",
      "\tspeed: 1.0354s/iter; left time: 49920.7792s\n",
      "\titers: 200, epoch: 3 | loss: 0.0889279\n",
      "\tspeed: 0.1108s/iter; left time: 5332.4357s\n",
      "\titers: 300, epoch: 3 | loss: 0.0985157\n",
      "\tspeed: 0.1093s/iter; left time: 5245.7327s\n",
      "\titers: 400, epoch: 3 | loss: 0.0925059\n",
      "\tspeed: 0.1078s/iter; left time: 5166.7550s\n",
      "\titers: 500, epoch: 3 | loss: 0.0979997\n",
      "\tspeed: 0.1105s/iter; left time: 5284.2447s\n",
      "\titers: 600, epoch: 3 | loss: 0.1002590\n",
      "\tspeed: 0.1085s/iter; left time: 5176.8450s\n",
      "\titers: 700, epoch: 3 | loss: 0.0918859\n",
      "\tspeed: 0.1088s/iter; left time: 5181.8800s\n",
      "\titers: 800, epoch: 3 | loss: 0.0898818\n",
      "\tspeed: 0.1092s/iter; left time: 5186.8335s\n",
      "\titers: 900, epoch: 3 | loss: 0.1080294\n",
      "\tspeed: 0.1077s/iter; left time: 5107.2974s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0840585\n",
      "\tspeed: 0.1109s/iter; left time: 5248.6033s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1036685\n",
      "\tspeed: 0.1080s/iter; left time: 5099.2819s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0952610\n",
      "\tspeed: 0.1097s/iter; left time: 5170.3811s\n",
      "\titers: 1300, epoch: 3 | loss: 0.0965679\n",
      "\tspeed: 0.1097s/iter; left time: 5157.9908s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0959029\n",
      "\tspeed: 0.1093s/iter; left time: 5128.9773s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1035789\n",
      "\tspeed: 0.1090s/iter; left time: 5103.8582s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1028642\n",
      "\tspeed: 0.1100s/iter; left time: 5137.9338s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0882429\n",
      "\tspeed: 0.1096s/iter; left time: 5110.8954s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0945800\n",
      "\tspeed: 0.1099s/iter; left time: 5112.2348s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0928347\n",
      "\tspeed: 0.1104s/iter; left time: 5124.0375s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1003603\n",
      "\tspeed: 0.1096s/iter; left time: 5077.2074s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0984533\n",
      "\tspeed: 0.1095s/iter; left time: 5062.3104s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1025412\n",
      "\tspeed: 0.1075s/iter; left time: 4955.1975s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0893745\n",
      "\tspeed: 0.1096s/iter; left time: 5041.8927s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0895675\n",
      "\tspeed: 0.1104s/iter; left time: 5068.4222s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1030405\n",
      "\tspeed: 0.1123s/iter; left time: 5143.2645s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0951473\n",
      "\tspeed: 0.1126s/iter; left time: 5145.1640s\n",
      "Epoch: 3 cost time: 00h:04m:54.67s\n",
      "Epoch: 3 | Train Loss: 0.0963206 Vali Loss: 0.0841027 Test Loss: 0.0897672\n",
      "Validation loss decreased (0.084166 --> 0.084103).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 4 | loss: 0.0876906\n",
      "\tspeed: 1.0156s/iter; left time: 46239.8347s\n",
      "\titers: 200, epoch: 4 | loss: 0.0991883\n",
      "\tspeed: 0.1097s/iter; left time: 4984.6479s\n",
      "\titers: 300, epoch: 4 | loss: 0.1039904\n",
      "\tspeed: 0.1111s/iter; left time: 5037.4476s\n",
      "\titers: 400, epoch: 4 | loss: 0.0816384\n",
      "\tspeed: 0.1112s/iter; left time: 5029.5138s\n",
      "\titers: 500, epoch: 4 | loss: 0.0884582\n",
      "\tspeed: 0.1091s/iter; left time: 4923.2094s\n",
      "\titers: 600, epoch: 4 | loss: 0.0939277\n",
      "\tspeed: 0.1102s/iter; left time: 4961.8169s\n",
      "\titers: 700, epoch: 4 | loss: 0.0897342\n",
      "\tspeed: 0.1086s/iter; left time: 4879.0680s\n",
      "\titers: 800, epoch: 4 | loss: 0.0904002\n",
      "\tspeed: 0.1136s/iter; left time: 5092.9051s\n",
      "\titers: 900, epoch: 4 | loss: 0.1019345\n",
      "\tspeed: 0.1094s/iter; left time: 4895.5678s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1152800\n",
      "\tspeed: 0.1103s/iter; left time: 4923.4616s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0801582\n",
      "\tspeed: 0.1103s/iter; left time: 4912.7238s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0705594\n",
      "\tspeed: 0.1108s/iter; left time: 4922.2872s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1035900\n",
      "\tspeed: 0.1114s/iter; left time: 4938.2054s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1084320\n",
      "\tspeed: 0.1099s/iter; left time: 4858.8270s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1027107\n",
      "\tspeed: 0.1103s/iter; left time: 4868.2863s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1098636\n",
      "\tspeed: 0.1125s/iter; left time: 4953.1752s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1155795\n",
      "\tspeed: 0.1101s/iter; left time: 4836.7290s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0852332\n",
      "\tspeed: 0.1088s/iter; left time: 4766.7043s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0975535\n",
      "\tspeed: 0.1080s/iter; left time: 4722.3323s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0907878\n",
      "\tspeed: 0.1094s/iter; left time: 4774.2535s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1058796\n",
      "\tspeed: 0.1083s/iter; left time: 4714.2260s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0916528\n",
      "\tspeed: 0.1085s/iter; left time: 4713.9516s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0722211\n",
      "\tspeed: 0.1091s/iter; left time: 4726.8356s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0976648\n",
      "\tspeed: 0.1091s/iter; left time: 4717.7498s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0973345\n",
      "\tspeed: 0.1098s/iter; left time: 4734.1440s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1003160\n",
      "\tspeed: 0.1075s/iter; left time: 4626.5454s\n",
      "Epoch: 4 cost time: 00h:04m:55.51s\n",
      "Epoch: 4 | Train Loss: 0.0942669 Vali Loss: 0.0841781 Test Loss: 0.0898632\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 5 | loss: 0.0956298\n",
      "\tspeed: 0.9791s/iter; left time: 41947.9687s\n",
      "\titers: 200, epoch: 5 | loss: 0.0926843\n",
      "\tspeed: 0.1115s/iter; left time: 4768.1909s\n",
      "\titers: 300, epoch: 5 | loss: 0.0960669\n",
      "\tspeed: 0.1119s/iter; left time: 4771.1583s\n",
      "\titers: 400, epoch: 5 | loss: 0.1028672\n",
      "\tspeed: 0.1133s/iter; left time: 4818.9602s\n",
      "\titers: 500, epoch: 5 | loss: 0.0889630\n",
      "\tspeed: 0.1136s/iter; left time: 4822.7283s\n",
      "\titers: 600, epoch: 5 | loss: 0.0886828\n",
      "\tspeed: 0.1118s/iter; left time: 4734.4450s\n",
      "\titers: 700, epoch: 5 | loss: 0.0865297\n",
      "\tspeed: 0.1123s/iter; left time: 4742.8606s\n",
      "\titers: 800, epoch: 5 | loss: 0.0793266\n",
      "\tspeed: 0.1123s/iter; left time: 4732.9254s\n",
      "\titers: 900, epoch: 5 | loss: 0.0875158\n",
      "\tspeed: 0.1100s/iter; left time: 4626.6067s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1002013\n",
      "\tspeed: 0.1128s/iter; left time: 4730.8102s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0797455\n",
      "\tspeed: 0.1109s/iter; left time: 4640.6352s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1008162\n",
      "\tspeed: 0.1114s/iter; left time: 4650.6303s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0942667\n",
      "\tspeed: 0.1117s/iter; left time: 4651.4745s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0891483\n",
      "\tspeed: 0.1108s/iter; left time: 4602.0999s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0945314\n",
      "\tspeed: 0.1104s/iter; left time: 4574.6913s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0918318\n",
      "\tspeed: 0.1082s/iter; left time: 4474.3307s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0960498\n",
      "\tspeed: 0.1101s/iter; left time: 4539.8121s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0857012\n",
      "\tspeed: 0.1114s/iter; left time: 4585.0055s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1000285\n",
      "\tspeed: 0.1093s/iter; left time: 4484.9406s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0954931\n",
      "\tspeed: 0.1111s/iter; left time: 4549.3282s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0967594\n",
      "\tspeed: 0.1115s/iter; left time: 4555.8473s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0867127\n",
      "\tspeed: 0.1100s/iter; left time: 4483.8720s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0886693\n",
      "\tspeed: 0.1081s/iter; left time: 4391.9438s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0934356\n",
      "\tspeed: 0.1099s/iter; left time: 4455.3731s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1050704\n",
      "\tspeed: 0.1047s/iter; left time: 4235.5352s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0817107\n",
      "\tspeed: 0.1089s/iter; left time: 4394.9630s\n",
      "Epoch: 5 cost time: 00h:04m:57.74s\n",
      "Epoch: 5 | Train Loss: 0.0925586 Vali Loss: 0.0840413 Test Loss: 0.0891893\n",
      "Validation loss decreased (0.084103 --> 0.084041).  Saving model ...\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 6 | loss: 0.1023156\n",
      "\tspeed: 0.9987s/iter; left time: 40109.5961s\n",
      "\titers: 200, epoch: 6 | loss: 0.0814353\n",
      "\tspeed: 0.1091s/iter; left time: 4372.3853s\n",
      "\titers: 300, epoch: 6 | loss: 0.0786190\n",
      "\tspeed: 0.1127s/iter; left time: 4505.0348s\n",
      "\titers: 400, epoch: 6 | loss: 0.0850952\n",
      "\tspeed: 0.1095s/iter; left time: 4364.9110s\n",
      "\titers: 500, epoch: 6 | loss: 0.0954753\n",
      "\tspeed: 0.1084s/iter; left time: 4310.2970s\n",
      "\titers: 600, epoch: 6 | loss: 0.0951714\n",
      "\tspeed: 0.1075s/iter; left time: 4263.4483s\n",
      "\titers: 700, epoch: 6 | loss: 0.0956363\n",
      "\tspeed: 0.1088s/iter; left time: 4304.6142s\n",
      "\titers: 800, epoch: 6 | loss: 0.0957980\n",
      "\tspeed: 0.1116s/iter; left time: 4403.2993s\n",
      "\titers: 900, epoch: 6 | loss: 0.0858036\n",
      "\tspeed: 0.1084s/iter; left time: 4266.2953s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0840625\n",
      "\tspeed: 0.1095s/iter; left time: 4299.1964s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1052077\n",
      "\tspeed: 0.1113s/iter; left time: 4360.4634s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0996516\n",
      "\tspeed: 0.1107s/iter; left time: 4323.7721s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0890602\n",
      "\tspeed: 0.1103s/iter; left time: 4299.3119s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0871324\n",
      "\tspeed: 0.1134s/iter; left time: 4405.4610s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1050522\n",
      "\tspeed: 0.1097s/iter; left time: 4251.4229s\n",
      "\titers: 1600, epoch: 6 | loss: 0.0922716\n",
      "\tspeed: 0.1076s/iter; left time: 4160.7940s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0856184\n",
      "\tspeed: 0.1090s/iter; left time: 4201.4655s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0855285\n",
      "\tspeed: 0.1098s/iter; left time: 4223.7127s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0947269\n",
      "\tspeed: 0.1088s/iter; left time: 4174.5388s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0936098\n",
      "\tspeed: 0.1098s/iter; left time: 4200.9011s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0826341\n",
      "\tspeed: 0.1077s/iter; left time: 4111.5602s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0854143\n",
      "\tspeed: 0.1091s/iter; left time: 4153.8185s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1005903\n",
      "\tspeed: 0.1087s/iter; left time: 4125.2825s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1012520\n",
      "\tspeed: 0.1090s/iter; left time: 4125.6942s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0836399\n",
      "\tspeed: 0.1077s/iter; left time: 4068.0019s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0693969\n",
      "\tspeed: 0.1071s/iter; left time: 4033.4910s\n",
      "Epoch: 6 cost time: 00h:04m:54.35s\n",
      "Epoch: 6 | Train Loss: 0.0908017 Vali Loss: 0.0846587 Test Loss: 0.0896558\n",
      "EarlyStopping counter: 1 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 7 | loss: 0.0822827\n",
      "\tspeed: 0.9899s/iter; left time: 37100.0563s\n",
      "\titers: 200, epoch: 7 | loss: 0.0926162\n",
      "\tspeed: 0.1114s/iter; left time: 4164.7326s\n",
      "\titers: 300, epoch: 7 | loss: 0.0814312\n",
      "\tspeed: 0.1117s/iter; left time: 4164.3827s\n",
      "\titers: 400, epoch: 7 | loss: 0.0927256\n",
      "\tspeed: 0.1096s/iter; left time: 4074.4149s\n",
      "\titers: 500, epoch: 7 | loss: 0.0889228\n",
      "\tspeed: 0.1091s/iter; left time: 4045.6678s\n",
      "\titers: 600, epoch: 7 | loss: 0.1010134\n",
      "\tspeed: 0.1092s/iter; left time: 4039.2271s\n",
      "\titers: 700, epoch: 7 | loss: 0.0931574\n",
      "\tspeed: 0.1082s/iter; left time: 3988.9460s\n",
      "\titers: 800, epoch: 7 | loss: 0.1155873\n",
      "\tspeed: 0.1110s/iter; left time: 4080.5043s\n",
      "\titers: 900, epoch: 7 | loss: 0.0780460\n",
      "\tspeed: 0.1100s/iter; left time: 4035.3996s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0952020\n",
      "\tspeed: 0.1084s/iter; left time: 3963.3575s\n",
      "\titers: 1100, epoch: 7 | loss: 0.0811136\n",
      "\tspeed: 0.1111s/iter; left time: 4052.5775s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0879815\n",
      "\tspeed: 0.1089s/iter; left time: 3961.4461s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0844414\n",
      "\tspeed: 0.1090s/iter; left time: 3956.0002s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0822187\n",
      "\tspeed: 0.1082s/iter; left time: 3915.6191s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0836276\n",
      "\tspeed: 0.1106s/iter; left time: 3991.4405s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0950310\n",
      "\tspeed: 0.1099s/iter; left time: 3952.8517s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0898525\n",
      "\tspeed: 0.1119s/iter; left time: 4015.9486s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1007908\n",
      "\tspeed: 0.1098s/iter; left time: 3928.4173s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0798731\n",
      "\tspeed: 0.1129s/iter; left time: 4028.6812s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1034588\n",
      "\tspeed: 0.1093s/iter; left time: 3887.3728s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0921248\n",
      "\tspeed: 0.1107s/iter; left time: 3926.8929s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1086720\n",
      "\tspeed: 0.1100s/iter; left time: 3890.3396s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0822674\n",
      "\tspeed: 0.1113s/iter; left time: 3926.1189s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0884877\n",
      "\tspeed: 0.1077s/iter; left time: 3790.1335s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1050238\n",
      "\tspeed: 0.1150s/iter; left time: 4034.0993s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0835463\n",
      "\tspeed: 0.1124s/iter; left time: 3930.4840s\n",
      "Epoch: 7 cost time: 00h:04m:56.70s\n",
      "Epoch: 7 | Train Loss: 0.0888387 Vali Loss: 0.0842900 Test Loss: 0.0893077\n",
      "EarlyStopping counter: 2 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 8 | loss: 0.0919155\n",
      "\tspeed: 0.9824s/iter; left time: 34179.8800s\n",
      "\titers: 200, epoch: 8 | loss: 0.0877724\n",
      "\tspeed: 0.1100s/iter; left time: 3816.5570s\n",
      "\titers: 300, epoch: 8 | loss: 0.0862236\n",
      "\tspeed: 0.1089s/iter; left time: 3768.4950s\n",
      "\titers: 400, epoch: 8 | loss: 0.0993591\n",
      "\tspeed: 0.1077s/iter; left time: 3716.4490s\n",
      "\titers: 500, epoch: 8 | loss: 0.1030686\n",
      "\tspeed: 0.1112s/iter; left time: 3823.2374s\n",
      "\titers: 600, epoch: 8 | loss: 0.0945140\n",
      "\tspeed: 0.1119s/iter; left time: 3838.9451s\n",
      "\titers: 700, epoch: 8 | loss: 0.1027590\n",
      "\tspeed: 0.1092s/iter; left time: 3732.5127s\n",
      "\titers: 800, epoch: 8 | loss: 0.1001980\n",
      "\tspeed: 0.1121s/iter; left time: 3822.6252s\n",
      "\titers: 900, epoch: 8 | loss: 0.0884102\n",
      "\tspeed: 0.1090s/iter; left time: 3705.9591s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0649632\n",
      "\tspeed: 0.1099s/iter; left time: 3723.3871s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0900450\n",
      "\tspeed: 0.1090s/iter; left time: 3683.1089s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0741200\n",
      "\tspeed: 0.1112s/iter; left time: 3746.8564s\n",
      "\titers: 1300, epoch: 8 | loss: 0.0829209\n",
      "\tspeed: 0.1088s/iter; left time: 3654.2949s\n",
      "\titers: 1400, epoch: 8 | loss: 0.1002313\n",
      "\tspeed: 0.1093s/iter; left time: 3661.3948s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0962400\n",
      "\tspeed: 0.1087s/iter; left time: 3629.8484s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0859644\n",
      "\tspeed: 0.1107s/iter; left time: 3684.4940s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0893864\n",
      "\tspeed: 0.1133s/iter; left time: 3761.5151s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0875129\n",
      "\tspeed: 0.1101s/iter; left time: 3643.0258s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0895706\n",
      "\tspeed: 0.1103s/iter; left time: 3637.8111s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0810007\n",
      "\tspeed: 0.1099s/iter; left time: 3615.4657s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0934768\n",
      "\tspeed: 0.1105s/iter; left time: 3624.0294s\n",
      "\titers: 2200, epoch: 8 | loss: 0.1046799\n",
      "\tspeed: 0.1108s/iter; left time: 3623.2802s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0742580\n",
      "\tspeed: 0.1118s/iter; left time: 3643.2920s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0805892\n",
      "\tspeed: 0.1101s/iter; left time: 3575.9694s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0943531\n",
      "\tspeed: 0.1133s/iter; left time: 3669.6143s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0835769\n",
      "\tspeed: 0.1103s/iter; left time: 3563.4025s\n",
      "Epoch: 8 cost time: 00h:04m:56.76s\n",
      "Epoch: 8 | Train Loss: 0.0870042 Vali Loss: 0.0843130 Test Loss: 0.0891844\n",
      "EarlyStopping counter: 3 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 9 | loss: 0.0846436\n",
      "\tspeed: 0.9931s/iter; left time: 31886.2504s\n",
      "\titers: 200, epoch: 9 | loss: 0.0627204\n",
      "\tspeed: 0.1136s/iter; left time: 3635.0689s\n",
      "\titers: 300, epoch: 9 | loss: 0.0839949\n",
      "\tspeed: 0.1118s/iter; left time: 3566.4207s\n",
      "\titers: 400, epoch: 9 | loss: 0.0899176\n",
      "\tspeed: 0.1117s/iter; left time: 3554.0239s\n",
      "\titers: 500, epoch: 9 | loss: 0.0827703\n",
      "\tspeed: 0.1087s/iter; left time: 3448.3166s\n",
      "\titers: 600, epoch: 9 | loss: 0.0757234\n",
      "\tspeed: 0.1079s/iter; left time: 3410.3451s\n",
      "\titers: 700, epoch: 9 | loss: 0.0842483\n",
      "\tspeed: 0.1056s/iter; left time: 3328.5301s\n",
      "\titers: 800, epoch: 9 | loss: 0.0728695\n",
      "\tspeed: 0.1088s/iter; left time: 3417.5830s\n",
      "\titers: 900, epoch: 9 | loss: 0.0946383\n",
      "\tspeed: 0.1088s/iter; left time: 3407.5581s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0801099\n",
      "\tspeed: 0.1090s/iter; left time: 3403.3083s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0760204\n",
      "\tspeed: 0.1110s/iter; left time: 3452.1874s\n",
      "\titers: 1200, epoch: 9 | loss: 0.0898418\n",
      "\tspeed: 0.1095s/iter; left time: 3395.2446s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0854249\n",
      "\tspeed: 0.1069s/iter; left time: 3304.6754s\n",
      "\titers: 1400, epoch: 9 | loss: 0.0841468\n",
      "\tspeed: 0.1094s/iter; left time: 3371.7299s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0759359\n",
      "\tspeed: 0.1100s/iter; left time: 3376.7436s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0933973\n",
      "\tspeed: 0.1102s/iter; left time: 3374.0485s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0746514\n",
      "\tspeed: 0.1120s/iter; left time: 3416.6345s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0891489\n",
      "\tspeed: 0.1091s/iter; left time: 3316.1871s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0825078\n",
      "\tspeed: 0.1085s/iter; left time: 3289.5723s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0846198\n",
      "\tspeed: 0.1104s/iter; left time: 3335.3857s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0837791\n",
      "\tspeed: 0.1095s/iter; left time: 3296.0821s\n",
      "\titers: 2200, epoch: 9 | loss: 0.0860811\n",
      "\tspeed: 0.1096s/iter; left time: 3287.7848s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0792212\n",
      "\tspeed: 0.1103s/iter; left time: 3298.4558s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0837938\n",
      "\tspeed: 0.1080s/iter; left time: 3219.4986s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0834742\n",
      "\tspeed: 0.1098s/iter; left time: 3263.2591s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0836421\n",
      "\tspeed: 0.1088s/iter; left time: 3220.5760s\n",
      "Epoch: 9 cost time: 00h:04m:54.56s\n",
      "Epoch: 9 | Train Loss: 0.0853277 Vali Loss: 0.0862787 Test Loss: 0.0900003\n",
      "EarlyStopping counter: 4 out of 5\n",
      "lr = 0.0000400000\n",
      "\titers: 100, epoch: 10 | loss: 0.0887046\n",
      "\tspeed: 0.9788s/iter; left time: 28801.2164s\n",
      "\titers: 200, epoch: 10 | loss: 0.0646053\n",
      "\tspeed: 0.1075s/iter; left time: 3152.2188s\n",
      "\titers: 300, epoch: 10 | loss: 0.0941833\n",
      "\tspeed: 0.1090s/iter; left time: 3186.2389s\n",
      "\titers: 400, epoch: 10 | loss: 0.0726987\n",
      "\tspeed: 0.1123s/iter; left time: 3271.5557s\n",
      "\titers: 500, epoch: 10 | loss: 0.0876720\n",
      "\tspeed: 0.1126s/iter; left time: 3269.1611s\n",
      "\titers: 600, epoch: 10 | loss: 0.0904420\n",
      "\tspeed: 0.1142s/iter; left time: 3303.6465s\n",
      "\titers: 700, epoch: 10 | loss: 0.0787993\n",
      "\tspeed: 0.1109s/iter; left time: 3197.0932s\n",
      "\titers: 800, epoch: 10 | loss: 0.0708453\n",
      "\tspeed: 0.1118s/iter; left time: 3211.4367s\n",
      "\titers: 900, epoch: 10 | loss: 0.0839387\n",
      "\tspeed: 0.1111s/iter; left time: 3180.7091s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0783891\n",
      "\tspeed: 0.1096s/iter; left time: 3127.5211s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0766438\n",
      "\tspeed: 0.1103s/iter; left time: 3134.5486s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0895397\n",
      "\tspeed: 0.1103s/iter; left time: 3123.7572s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0842266\n",
      "\tspeed: 0.1087s/iter; left time: 3068.8769s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0860861\n",
      "\tspeed: 0.1102s/iter; left time: 3098.9001s\n",
      "\titers: 1500, epoch: 10 | loss: 0.0650968\n",
      "\tspeed: 0.1095s/iter; left time: 3068.1646s\n",
      "\titers: 1600, epoch: 10 | loss: 0.1023239\n",
      "\tspeed: 0.1092s/iter; left time: 3048.2008s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0784739\n",
      "\tspeed: 0.1103s/iter; left time: 3069.1363s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0877584\n",
      "\tspeed: 0.1096s/iter; left time: 3038.4388s\n",
      "\titers: 1900, epoch: 10 | loss: 0.0746399\n",
      "\tspeed: 0.1106s/iter; left time: 3056.2888s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0881622\n",
      "\tspeed: 0.1076s/iter; left time: 2962.8424s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0905812\n",
      "\tspeed: 0.1096s/iter; left time: 3006.3810s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0936635\n",
      "\tspeed: 0.1130s/iter; left time: 3087.8383s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0850890\n",
      "\tspeed: 0.1102s/iter; left time: 2999.2200s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0925531\n",
      "\tspeed: 0.1115s/iter; left time: 3025.3495s\n",
      "\titers: 2500, epoch: 10 | loss: 0.0950423\n",
      "\tspeed: 0.1093s/iter; left time: 2952.7029s\n",
      "\titers: 2600, epoch: 10 | loss: 0.0860755\n",
      "\tspeed: 0.1111s/iter; left time: 2992.5685s\n",
      "Epoch: 10 cost time: 00h:04m:56.60s\n",
      "Epoch: 10 | Train Loss: 0.0836082 Vali Loss: 0.0852293 Test Loss: 0.0903601\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "loading model...\n",
      "Scaled mse:0.02039830945432186, rmse:0.14282265305519104, mae:0.0891893059015274, rse:0.5403580069541931\n",
      "success delete checkpoints\n",
      "Intermediate time for IT and pred_len 168: 01h:04m:11.86s\n",
      "\n",
      "Intermediate time for IT: 03h:53m:40.34s\n",
      "\n",
      "Total time: 20h:49m:25.28s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "timellm_results = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Open log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2)\n",
    "\n",
    "            # Command to run script with parameters\n",
    "            command = f\"\"\"\n",
    "            python -m accelerate.commands.launch --mixed_precision bf16 --num_processes=1 --num_machines 1 --dynamo_backend \"no\" --main_process_port \"01025\" ./Time-LLM/run_main.py \\\n",
    "              --task_name long_term_forecast \\\n",
    "              --is_training 1 \\\n",
    "              --root_path ./datasets/ \\\n",
    "              --data_path {country}_data.csv \\\n",
    "              --model_id {i+1} \\\n",
    "              --model {model} \\\n",
    "              --data {country} \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --factor 3 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --itr 1 \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --batch_size {batch_size} \\\n",
    "              --learning_rate {lr} \\\n",
    "              --llm_model \"GPT2\" \\\n",
    "              --llm_dim 768 \\\n",
    "              --llm_layers 12 \\\n",
    "              --train_epochs {train_epochs} \\\n",
    "              --patience 5 \\\n",
    "              --model_comment {model}+{country}\n",
    "            \"\"\"\n",
    "\n",
    "            # Run command and log output\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture and log output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')\n",
    "                log_file.write(line)\n",
    "\n",
    "            process.wait()  # Wait for process to finish\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr=1)[0]\n",
    "            mse, rmse, mae, _ = iteration_metrics\n",
    "            timellm_results.append({\n",
    "                'Country': country,\n",
    "                'Pred_len': pred_len,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae\n",
    "                })\n",
    "\n",
    "            # Time tracking for pred_len\n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = f\"Intermediate time for {country} and pred_len {pred_len}: {hours_int:0>2}h:{mins_int:0>2}m:{secs_int:05.2f}s\\n\"\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        # Time tracking for each country\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = f\"Intermediate time for {country}: {hours_c:0>2}h:{mins_c:0>2}m:{secs_c:05.2f}s\\n\"\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    # Total time\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = f\"Total time: {hours:0>2}h:{mins:0>2}m:{secs:05.2f}s\\n\"\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TimeLLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>0.0920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>0.1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0.1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>0.1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.0941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.0824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            TimeLLM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0217  0.1474  0.0920\n",
       "        96        0.0395  0.1989  0.1324\n",
       "        168       0.0395  0.1988  0.1348\n",
       "GB      24        0.0252  0.1588  0.1012\n",
       "        96        0.0437  0.2091  0.1434\n",
       "        168       0.0469  0.2166  0.1487\n",
       "ES      24        0.0108  0.1039  0.0672\n",
       "        96        0.0198  0.1409  0.0941\n",
       "        168       0.0217  0.1472  0.0970\n",
       "FR      24        0.0104  0.1017  0.0586\n",
       "        96        0.0186  0.1365  0.0824\n",
       "        168       0.0205  0.1430  0.0878\n",
       "IT      24        0.0104  0.1022  0.0607\n",
       "        96        0.0188  0.1370  0.0839\n",
       "        168       0.0204  0.1428  0.0892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/timellm'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "timellm_df = convert_results_into_df(timellm_results, if_loss_fnc=False, itr=1)\n",
    "\n",
    "# Final DF\n",
    "timellm_df.columns = pd.MultiIndex.from_product([['TimeLLM/336'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "timellm_df.to_csv(os.path.join(path, 'timellm_336.csv'))\n",
    "timellm_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
